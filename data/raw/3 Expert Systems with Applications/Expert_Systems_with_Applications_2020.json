[
  {
    "title": "Efficient scheduling of a stochastic no-wait job shop with controllable processing times",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113879",
    "abstract": "This work derives a novel effective and efficient algorithm for a stochastic no-wait job-shop scheduling problem with controllable processing times. Some of the processing times are stochastic and the proposed solution effectively minimizes the makespan and increases the robustness of the makespan against deviating processing times. Therefore, a no-wait job shop with controllable deterministic processing times is solved by a decomposition into timetabling and sequencing. During timetabling, extra safety margins are added to the scheduled processing times without delaying jobs. In the sequence optimization subproblem, an extra penalty term is added to the cost function which punishes uncertain tasks at positions that have an impact on the makespan. Simulation results based on real plant data and tailor-made benchmark problems show that these measures can reduce the standard deviation of the makespan dramatically. This significantly improves the prediction accuracy of the scheduling method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306849",
    "keywords": [
      "Benchmark (surveying)",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Flow shop scheduling",
      "Gene",
      "Geodesy",
      "Geography",
      "Job shop",
      "Job shop scheduling",
      "Mathematical optimization",
      "Mathematics",
      "Operating system",
      "Robustness (evolution)",
      "Schedule",
      "Scheduling (production processes)"
    ],
    "authors": [
      {
        "surname": "Aschauer",
        "given_name": "Alexander"
      },
      {
        "surname": "Roetzer",
        "given_name": "Florian"
      },
      {
        "surname": "Steinboeck",
        "given_name": "Andreas"
      },
      {
        "surname": "Kugi",
        "given_name": "Andreas"
      }
    ]
  },
  {
    "title": "Local search metaheuristic for solving hybrid flow shop problem in slabs and beams manufacturing",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113716",
    "abstract": "Solving optimization problems in the modern manufacturing industry is one of the most attractive areas of operational research. In this paper, we propose an industrial optimization case of slabs and beams production problem. The suggested approach consists in modeling the process as a hybrid flow shop with unrelated parallel machines under constraints of sequence dependent setup time. Jobs launched in production are of different characteristics and require a machine preparation time at each phase during their production cycle. The goal is to minimize the total tardiness of all jobs when they are completed on the last stage. The hybrid flow shop problem is considered as one of the NP-hard issues since it consists of at least two stages. To solve numerically this problem, we propose two methods, the iterative local search (ILS) and the iterated greedy (IG) metaheuristics. In addition, we suggest two new improvements, the first relates to the initial step while the second concerns the steps of the neighborhood exploration. In the initial phase, we consider an initial solution set generated from the priority rules whose scheduling is ensured by the Nawaz-Enscore-Ham (NEH) algorithm and the greedy randomized adaptive search procedure (GRASP). In the second phase, we suggest a new version of exploration of the neighborhood. In fact, from a single solution, we will generate a set of neighboring solutions and we will choose the best solution. In this new resolution approach, we develop a total of twelve algorithms based on neighborhood exploration. We note that the proposed algorithms are hybrid metaheuristics and are among the most powerful optimization algorithms in local search. A simulation study is conducted to verify the effectiveness of the suggested algorithms.This simulation is performed on a set of instances by varying the number of jobs, the number of machines per stage and the number of stages. We find that the IG algorithm based on NEH initialization heuristic gives good results in terms of quality and convergence time to the best solution.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305406",
    "keywords": [
      "Algorithm",
      "Computer science",
      "Flow shop scheduling",
      "GRASP",
      "Greedy algorithm",
      "Greedy randomized adaptive search procedure",
      "Heuristic",
      "Iterated function",
      "Iterated local search",
      "Job shop scheduling",
      "Local search (optimization)",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Metaheuristic",
      "Operating system",
      "Programming language",
      "Schedule",
      "Scheduling (production processes)",
      "Set (abstract data type)",
      "Tabu search",
      "Tardiness"
    ],
    "authors": [
      {
        "surname": "Aqil",
        "given_name": "Said"
      },
      {
        "surname": "Allali",
        "given_name": "Karam"
      }
    ]
  },
  {
    "title": "Node Importance based Label Propagation Algorithm for overlapping community detection in networks",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113020",
    "abstract": "The enormous growth of the Web led to the birth of different network structures. Therefore, one of the important issues in the field of complex network analysis is to find and exploit the structure. Many studies have been carried out in this sense. The Label Propagation Algorithm (LPA) is among the most recognized approaches to detect disjointed communities. It is a simple and fast method, but its major disadvantage lies in its instability due to a random update. In this paper, we introduce a Node Importance based Label Propagation Algorithm (NI-LPA), a new algorithm for detecting overlapping communities in networks. As indicated in its name, NI-LPA is an improved version of LPA which maintains its simplicity and enhances its accuracy. In fact, we adopt the LPA strategy to allow a node to contain a set of labels. Moreover, the algorithm simulates a special propagation and filtering process using information deduced from the properties of nodes. Experimental results on artificial and real-world networks with different sizes, complexities and densities show the efficiency of our approach to detect overlapping communities.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419307377",
    "keywords": [
      "Algorithm",
      "Computer science",
      "Computer security",
      "Engineering",
      "Epistemology",
      "Exploit",
      "Field (mathematics)",
      "Mathematics",
      "Node (physics)",
      "Operating system",
      "Philosophy",
      "Process (computing)",
      "Programming language",
      "Pure mathematics",
      "Set (abstract data type)",
      "Simple (philosophy)",
      "Simplicity",
      "Structural engineering"
    ],
    "authors": [
      {
        "surname": "Ben El Kouni",
        "given_name": "Imen"
      },
      {
        "surname": "Karoui",
        "given_name": "Wafa"
      },
      {
        "surname": "Romdhane",
        "given_name": "Lotfi Ben"
      }
    ]
  },
  {
    "title": "Tens-embedding: A Tensor-based document embedding method",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113770",
    "abstract": "A human is capable of understanding and classifying a text but a computer can understand the underlying semantics of a text when texts are represented in a way comprehensible by computers. The text representation is a fundamental stage in natural language processing (NLP). One of the main drawbacks of existing text representation approaches is that they only utilize one aspect or view of a text e.g. They only consider texts by their words while the topic information can be extracted from text as well. The term-document and document-topic matrix are two views of a text and contain complementary information. We use the strength of both views to extract a richer representation. In this paper, we propose three different text representation methods with the help of these two matrices and tensor factorization to utilize the power of both views. The proposed approach (Tens-Embedding) was applied in the tasks of text classification, sentence-level and document-level sentiment analysis and text clustering wherein the conducted experiments on 20newsgroups, R52, R8, MR and IMDB datasets indicated the superiority of the proposed method in comparison with other document embedding techniques.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305947",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Document clustering",
      "Embedding",
      "Information retrieval",
      "Law",
      "Mathematics",
      "Natural language processing",
      "Political science",
      "Politics",
      "Programming language",
      "Pure mathematics",
      "Representation (politics)",
      "Semantics (computer science)",
      "Sentence",
      "Tensor (intrinsic definition)",
      "Text graph",
      "Text mining",
      "Word embedding"
    ],
    "authors": [
      {
        "surname": "Rahimi",
        "given_name": "Zahra"
      },
      {
        "surname": "Homayounpour",
        "given_name": "Mohammad Mehdi"
      }
    ]
  },
  {
    "title": "A systematic evaluation of filter Unsupervised Feature Selection methods",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113745",
    "abstract": "Unsupervised Feature Selection (UFS) has aroused great interest in the last years because of its practical significance and application on a large variety of problems in expert and intelligent systems where unlabeled data appear. Specifically, Unsupervised Feature Selection methods based on the filter approach have received more attention due to their efficiency, scalability, and simplicity. However, in the literature, there are no comprehensive studies for assessing such UFS methods when they are applied, under the same conditions, to a wide variety of real-world data. To fill this gap, in this paper, we present a comprehensive empirical and systematic evaluation of the most popular and recent filter UFS methods, evaluating their performance in terms of clustering, classification, and runtime. The filter methods used in our study were applied on 50 datasets from the UCI Machine Learning Repository and 25 high dimensional datasets from the ASU Feature Selection Repository. To evaluate if the outcomes obtained by the assessed methods are statistically significant, the Friedman test and Holm post hoc procedure were applied in the clustering and classification results. From our experiments, we provide some practical guidelines and insights for the use of the filter UFS methods analyzed in our study.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305698",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Database",
      "Feature (linguistics)",
      "Feature selection",
      "Filter (signal processing)",
      "Linguistics",
      "Machine learning",
      "Philosophy",
      "Scalability",
      "Selection (genetic algorithm)",
      "Unsupervised learning",
      "Variety (cybernetics)"
    ],
    "authors": [
      {
        "surname": "Solorio-Fernández",
        "given_name": "Saúl"
      },
      {
        "surname": "Ariel Carrasco-Ochoa",
        "given_name": "J."
      },
      {
        "surname": "Martínez-Trinidad",
        "given_name": "José Fco."
      }
    ]
  },
  {
    "title": "Applying deep neural networks for multi-level classification of driver drowsiness using Vehicle-based measures",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113778",
    "abstract": "Drowsy driving is one of the main causes of road accidents. Accurate and reliable detection of drivers' drowsiness is significantly important to prevent drowsiness-related accidents. In the context of automated vehicle driving, it is important for intelligent systems to know the current state of the driver to prepare handover maneuvers. Previous studies are mostly based on manually extracted features from either driving performance or driver physiological data. This methodology of a priori defined features can lead to losing valuable information of input signals that are significant to classify drowsiness levels in individual drivers because generally, it is not known which features are suitable for drowsiness prediction before classification. By using deep neural networks, features can be extracted automatically from preprocessed data. This paper presents a new non-obtrusive drowsiness detection system based on deep neural networks using vehicle-based measures. The proposed method is based on a combination of convolutional neural networks (CNN) and recurrent neural networks (RNN). Five vehicle-based measures, including lateral deviation from road centerline, lateral acceleration, yaw rate, steering wheel angle, and steering wheel velocity, are exploited as network inputs. The level of drowsiness is classified into three different classes. Long-short term memory (LSTM) and gated recurrent unit (GRU) layers are used as RNN in the structure of the designed deep network. The performance of the proposed method is evaluated on experimental data that were collected from 44 sessions in a fixed-base driving simulator simulating monotonous night-time highway drives. Results show that the classification accuracy of the designed deep networks outperforms traditional classifiers like support vector machine and k-nearest neighbors. The highest accuracy of 96.0% has been achieved with a combination of CNN and LSTM (CNN-LSTM). Further research should include more signal sources, including unobtrusively taken physiological signals, and test the system in real-world conditions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306023",
    "keywords": [
      "Acceleration",
      "Artificial intelligence",
      "Artificial neural network",
      "Automotive engineering",
      "Biology",
      "Classical mechanics",
      "Computer science",
      "Context (archaeology)",
      "Convolutional neural network",
      "Deep learning",
      "Engineering",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Physics",
      "Real-time computing",
      "Recurrent neural network",
      "Simulation",
      "Steering wheel"
    ],
    "authors": [
      {
        "surname": "Arefnezhad",
        "given_name": "Sadegh"
      },
      {
        "surname": "Samiee",
        "given_name": "Sajjad"
      },
      {
        "surname": "Eichberger",
        "given_name": "Arno"
      },
      {
        "surname": "Frühwirth",
        "given_name": "Matthias"
      },
      {
        "surname": "Kaufmann",
        "given_name": "Clemens"
      },
      {
        "surname": "Klotz",
        "given_name": "Emma"
      }
    ]
  },
  {
    "title": "Predicting trust in online advertising with an SEM-artificial neural network approach",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113849",
    "abstract": "Trust has an imperative role in online advertising because the effectiveness of the adverts will be greatly affected when consumers distrust online adverts. Currently, the level of consumers' trust in online advertising remains low. The current study will assess the drivers of trust by integrating the Trust Building Model and the ADTRUST scale. Unlike present literature that utilized linear models, a Structural Equation Modelling-Artificial Neural Network (SEM-ANN) approach was used. This is because consumers’ trust-building is a complex process and linear models will over-simplify the complexity in the decision-making processes. Thus, the outcomes from linear models are inadequate and inaccurate to explicate the mechanism of trust creation in online advertising. Data were gathered from 500 online consumers using a mall intercept technique. The outcomes from the sensitivity analysis show that reliability is the most imperative antecedent of trust followed by website quality, willingness to rely on, reputation, and hours spent. The model predicts 76.14% trust in online advertising. The theoretical implication is the integration of the ADTRUST scale with the Trust Building Model. The methodological implication is the use of the SEM-ANN approach that captured both linear-nonlinear and compensatory-non-compensatory associations. The findings provide some useful practical implications for online advertisers, service providers, and retailers. The study has contributed useful theoretical and practical implications to the online marketing literature.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306618",
    "keywords": [
      "Advertising",
      "Antecedent (behavioral psychology)",
      "Business",
      "Computer science",
      "Developmental psychology",
      "Distrust",
      "Epistemology",
      "Online advertising",
      "Operating system",
      "Philosophy",
      "Process (computing)",
      "Psychology",
      "Psychotherapist",
      "Quality (philosophy)",
      "Reputation",
      "Social science",
      "Sociology",
      "The Internet",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Leong",
        "given_name": "Lai-Ying"
      },
      {
        "surname": "Hew",
        "given_name": "Teck-Soon"
      },
      {
        "surname": "Ooi",
        "given_name": "Keng-Boon"
      },
      {
        "surname": "Dwivedi",
        "given_name": "Yogesh K."
      }
    ]
  },
  {
    "title": "Design of an evolving Fuzzy-PID controller for optimal trajectory control of a 7-DOF redundant manipulator with prioritized sub-tasks",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113021",
    "abstract": "Articulated manipulators constitute a major fraction of industrial robots. With the increase in the need to perform complex tasks requiring a higher degree of accuracy, such manipulators require a more robust and efficient control. In this paper, an evolving Fuzzy-PID control design for the trajectory tracking problem of a redundant 7-DOF serial manipulator is proposed. The scaling factors of the Fuzzy controller are evolved over time by using recent meta-heuristic techniques, namely, Moth-Flame Optimisation (MFO), Novel Bat Algorithm (NBA) and Sine Cosine Algorithm (SCA). The proposed technique which utilizes Intelligent control methods combined with meta-heuristics provides a promising hybrid control design, which has been found to be more efficient in our software implementation, in terms of reduced tracking error of the manipulator for its kinematic control in the joint space as compared to other controllers of the same nature that are generally used in practise. The performance of the controller is compared in the time-domain responses under external disturbances and have been found to outperform standard controllers without the need of any additional training.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419307389",
    "keywords": [
      "Agronomy",
      "Artificial intelligence",
      "Astronomy",
      "Biology",
      "Classical mechanics",
      "Computer science",
      "Control (management)",
      "Control engineering",
      "Control theory (sociology)",
      "Controller (irrigation)",
      "Engineering",
      "Fuzzy control system",
      "Fuzzy logic",
      "Heuristic",
      "Kinematics",
      "PID controller",
      "Parallel manipulator",
      "Physics",
      "Robot",
      "Serial manipulator",
      "Temperature control",
      "Trajectory"
    ],
    "authors": [
      {
        "surname": "Dhyani",
        "given_name": "Abhishek"
      },
      {
        "surname": "Panda",
        "given_name": "Manoj Kumar"
      },
      {
        "surname": "Jha",
        "given_name": "Bhola"
      }
    ]
  },
  {
    "title": "Fault Matters: Sensor data fusion for detection of faults using Dempster–Shafer theory of evidence in IoT-based applications",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113887",
    "abstract": "Fault detection in sensor nodes is a pertinent issue that has been an important area of research for a very long time. But it is not explored much as yet in the context of Internet of Things. Internet of Things work with a massive amount of data so the responsibility for guaranteeing the accuracy of the data also lies with it. Moreover, a lot of important and critical decisions are made based on these data, so ensuring its correctness and accuracy is also very important. Also, the detection needs to be as precise as possible to avoid negative alerts. For this purpose, this work has adopted Dempster–Shafer theory of evidence to collate the information from sensors to come up with a decision regarding the faulty status of a sensor node from a data-centric perspective. To verify the validity of the proposed method, simulations have been performed on a benchmark data set and data collected through a test bed in a laboratory set-up. For the different types of faults, the proposed method shows very high accuracy for both the benchmark (99.8%) and laboratory data sets (99.9%) when compared to the other state-of-the-art machine learning techniques.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306898",
    "keywords": [
      "Actuator",
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Computer network",
      "Computer science",
      "Context (archaeology)",
      "Correctness",
      "Data mining",
      "Data set",
      "Dempster–Shafer theory",
      "Engineering",
      "Fault (geology)",
      "Fault detection and isolation",
      "Geodesy",
      "Geography",
      "Geology",
      "Machine learning",
      "Node (physics)",
      "Paleontology",
      "Programming language",
      "Seismology",
      "Sensor fusion",
      "Set (abstract data type)",
      "Structural engineering",
      "Wireless sensor network"
    ],
    "authors": [
      {
        "surname": "Ghosh",
        "given_name": "Nimisha"
      },
      {
        "surname": "Paul",
        "given_name": "Rourab"
      },
      {
        "surname": "Maity",
        "given_name": "Satyabrata"
      },
      {
        "surname": "Maity",
        "given_name": "Krishanu"
      },
      {
        "surname": "Saha",
        "given_name": "Sayantan"
      }
    ]
  },
  {
    "title": "Using VADER sentiment and SVM for predicting customer response sentiment",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113746",
    "abstract": "Customer support is important to corporate operations, which involves dealing with disgruntled customer and content customers that can have different requirements. As such, it is important to quickly extract the sentiment of support errands. In this study we investigate sentiment analysis in customer support for a large Swedish Telecom corporation. The data set consists of 168,010 e-mails divided into 69,900 conversation threads without any sentiment information available. Therefore, VADER sentiment is used together with a Swedish sentiment lexicon in order to provide initial labeling of the e-mails. The e-mail content and sentiment labels are then used to train two Support Vector Machine models in extracting/classifying the sentiment of e-mails. Further, the ability to predict sentiment of not-yet-seen e-mail responses is investigated. Experimental results show that the LinearSVM model was able to extract sentiment with a mean F1-score of 0.834 and mean AUC of 0.896. Moreover, the LinearSVM algorithm was also able to predict the sentiment of an e-mail one step ahead in the thread (based on the text in the an already sent e-mail) with a mean F1-score of 0.688 and the mean AUC of 0.805. The results indicate a predictable pattern in e-mail conversation that enables predicting the sentiment of a not-yet-seen e-mail. This can be used e.g. to prepare particular actions for customers that are likely to have a negative response. It can also provide feedback on possible sentiment reactions to customer support e-mails.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305704",
    "keywords": [
      "Artificial intelligence",
      "Communication",
      "Computer science",
      "Conversation",
      "Economics",
      "Lexicon",
      "Machine learning",
      "Mean opinion score",
      "Metric (unit)",
      "Natural language processing",
      "Operations management",
      "Psychology",
      "Sentiment analysis",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Borg",
        "given_name": "Anton"
      },
      {
        "surname": "Boldt",
        "given_name": "Martin"
      }
    ]
  },
  {
    "title": "NSGA-II variants for solving a social-conscious dual resource-constrained scheduling problem",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113754",
    "abstract": "This present study compares various non-dominated sorting genetic algorithm II (NSGA-II) variants to deal with multi-task simultaneous supervision dual resource-constrained (MTSSDRC) scheduling to minimize the makespan and workload unbalance among operators. MTSSDRC is a complex problem that needs to integrate two types of scheduling, i.e., job sequencing on the machines and task sequencing, which includes setup, unloading, and moving, by the operators. Since there are two resources, one may be busier than the other. Task sequencing should be prioritized if the operator is very busy, otherwise, we should accentuate the job sequencing. The existing NSGA-II variant prioritizes the machine assignment over the operator. Therefore, its performance is not so well when the operator-to-machine ratio is small. Thus, this proposed research develops two new NSGA-II variants identified by their modified decoding schemes. They posit the operator more important or at least equal to the machine. To the best of our knowledge, this research is the first to analyze MTSDDRC based on the operator-to-machine ratio. The essential finding insists that each variant fits the specific ratio. The new ones are promising for cases with a ratio ⩽ 0.5 . Otherwise, the existing variant is still recommended. Besides, one of the new decoding schemes performs well for all ratio, but not the best.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305789",
    "keywords": [
      "Algorithm",
      "Art",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Dual (grammatical number)",
      "Economics",
      "Gene",
      "Job shop scheduling",
      "Literature",
      "Management",
      "Mathematical optimization",
      "Mathematics",
      "Operating system",
      "Operator (biology)",
      "Repressor",
      "Schedule",
      "Scheduling (production processes)",
      "Sorting",
      "Task (project management)",
      "Transcription factor",
      "Workload"
    ],
    "authors": [
      {
        "surname": "Akbar",
        "given_name": "Muhammad"
      },
      {
        "surname": "Irohara",
        "given_name": "Takashi"
      }
    ]
  },
  {
    "title": "Breast tumor segmentation in ultrasound images using contextual-information-aware deep adversarial learning framework",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113870",
    "abstract": "Automatic tumor segmentation in breast ultrasound (BUS) images is still a challenging task because of many sources of uncertainty, such as speckle noise, very low signal-to-noise ratio, shadows that make the anatomical boundaries of tumors ambiguous, as well as the highly variable tumor sizes and shapes. This article proposes an efficient automated method for tumor segmentation in BUS images based on a contextual information-aware conditional generative adversarial learning framework. Specifically, we exploit several enhancements on a deep adversarial learning framework to capture both texture features and contextual dependencies in the BUS images that facilitate beating the challenges mentioned above. First, we adopt atrous convolution (AC) to capture spatial and scale context (i.e., position and size of tumors) to handle very different tumor sizes and shapes. Second, we propose the use of channel attention along with channel weighting (CAW) mechanisms to promote the tumor-relevant features (without extra supervision) and mitigate the effects of artifacts. Third, we propose to integrate the structural similarity index metric (SSIM) and L1-norm in the loss function of the adversarial learning framework to capture the local context information derived from the area surrounding the tumors. We used two BUS image datasets to assess the efficiency of the proposed model. The experimental results show that the proposed model achieves competitive results compared with state-of-the-art segmentation models in terms of Dice and IoU metrics. The source code of the proposed model is publicly available at https://github.com/vivek231/Breast-US-project.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306771",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Deep learning",
      "Economics",
      "Image (mathematics)",
      "Machine learning",
      "Metric (unit)",
      "Operations management",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Segmentation",
      "Speckle noise"
    ],
    "authors": [
      {
        "surname": "Singh",
        "given_name": "Vivek Kumar"
      },
      {
        "surname": "Abdel-Nasser",
        "given_name": "Mohamed"
      },
      {
        "surname": "Akram",
        "given_name": "Farhan"
      },
      {
        "surname": "Rashwan",
        "given_name": "Hatem A."
      },
      {
        "surname": "Sarker",
        "given_name": "Md. Mostafa Kamal"
      },
      {
        "surname": "Pandey",
        "given_name": "Nidhi"
      },
      {
        "surname": "Romani",
        "given_name": "Santiago"
      },
      {
        "surname": "Puig",
        "given_name": "Domenec"
      }
    ]
  },
  {
    "title": "SDT: An integrated model for open-world knowledge graph reasoning",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113889",
    "abstract": "Knowledge graphs (KGs) have a wide range of applications, such as recommender systems, relation extraction, and intelligent question answering systems. However, existing KGs are far from complete. Knowledge graph reasoning (KGR) has been studied to complete KGs by inferring missing entities or relations. But most previous methods require that all entities should be seen during training, which is impractical for real-world KGs with new entities emerging daily. In this paper, we address the open-world KGR task: how to perform reasoning when entities are not observed at training time. The description-embodied knowledge representation learning (DKRL) model attempts to study the open-world KGR task. We find that DKRL does not consider hierarchical type information contained in entities when learning entities and relations representations, which results in poor performance. To address this problem, we propose a novel model, SDT, that incorporates the structural information, entity descriptions, and hierarchical type information of entities into a unified framework to learn more representative embeddings for KGs. Specifically, for entity descriptions, we explore continuous bag-of-words and convolutional neural networks models to encode the semantics of entity representations. For hierarchical types, we utilize a recursive hierarchy encoder and a weighted hierarchy encoder to construct the projection matrices of hierarchical types. We evaluate the SDT model on both open-world and closed-world reasoning tasks, including entity prediction and relation prediction. Experimental results on large-scale datasets show that SDT achieves a lower mean rank and higher Hits@10 than the baseline methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306904",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Graph",
      "Knowledge graph",
      "Knowledge management",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Xiaojun"
      },
      {
        "surname": "Jia",
        "given_name": "Shengbin"
      },
      {
        "surname": "Ding",
        "given_name": "Ling"
      },
      {
        "surname": "Shen",
        "given_name": "Hong"
      },
      {
        "surname": "Xiang",
        "given_name": "Yang"
      }
    ]
  },
  {
    "title": "Lessons learned from data stream classification applied to credit scoring",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113899",
    "abstract": "The financial credibility of a person is a factor used to determine whether a loan should be approved or not, and this is quantified by a ‘credit score,’ which is calculated using a variety of factors, including past performance on debt obligations, profiling, amongst others. Machine learning has been widely applied to automate the development of effective credit scoring models over the years. Yet, studies show that the development of robust credit scoring models may take longer than a year, and thus, if the behavior of customers changes over time, the model will be outdated even before its deployment. In this paper, we made 3 anonymized real-world credit scoring datasets available alongside the results obtained. In each of these datasets, we verify whether the credit scoring task should be thought as an ephemeral scenario since many of the variables may drift over time, and thus, data stream mining techniques should be used since they were tailored for incremental learning and to detect and adapt to changes in the data distribution. Therefore, we compare both traditional batch machine learning algorithms with data stream algorithms in different validation schemes using both Kolmogorov–Smirnov and Population Stability Index metrics. Furthermore, we also provide insights on the importance of features according to their Information Value, Mean Decrease Impurity, and Mean Positional Gain metrics, such that the last depicts changes in the importance of features over time. For 2 of the 3 tested datasets, the results obtained by data stream learners are comparable to predictive models currently in use, thus showing the efficiency of data stream classification for the credit scoring task.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306928",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Data stream",
      "Machine learning",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Barddal",
        "given_name": "Jean Paul"
      },
      {
        "surname": "Loezer",
        "given_name": "Lucas"
      },
      {
        "surname": "Enembreck",
        "given_name": "Fabrício"
      },
      {
        "surname": "Lanzuolo",
        "given_name": "Riccardo"
      }
    ]
  },
  {
    "title": "STCCD: Semantic trajectory clustering based on community detection in networks",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113689",
    "abstract": "Most of traditional trajectory clustering algorithms often cluster similar trajectories from a temporal or spatial perspective. One weak point is that the semantic relationship between the trajectories is ignored. In some cases, trajectories with spatio-temporal similarities may be semantically related, and the negligence of semantic information may result in unreasonable trajectory clustering results. In addition, the existing semantic trajectory clustering algorithms only consider the local semantic relationship between adjacent spatio-temporal trajectories, and the overall global semantic relationship between trajectories is still unknown. Considering the disadvantages of the current trajectory clustering methods, we proposed a novel algorithm for semantic trajectory clustering based on community detection (STCCD) in networks, which can better measure the semantic similarity of trajectories and capture global relationship among trajectories from the perspective of the network, and can get better trajectory clustering results compared to some traditional and recently proposed methods. Experimental results demonstrate that the proposed method can effectively mine the trajectory clustering information and related knowledge from the semantic trajectory data.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305133",
    "keywords": [
      "Artificial intelligence",
      "Astronomy",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Image (mathematics)",
      "Perspective (graphical)",
      "Physics",
      "Semantic similarity",
      "Similarity (geometry)",
      "Trajectory"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Caihong"
      },
      {
        "surname": "Guo",
        "given_name": "Chonghui"
      }
    ]
  },
  {
    "title": "LsRec: Large-scale social recommendation with online update",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113739",
    "abstract": "With the ever-increasing scale and complexity of social network and online business, Recommender Systems (RS) have played crucial roles in information processing and filtering in various online applications, although suffering from such as data sparsity and low accuracy problems. Meanwhile, recent researches try to enhance the performance of RS through such social network and clustering algorithms, however, they may fail to achieve further improvement in large-scale online recommendation due to the serious information overload. In this article, a novel social recommendation approach with online update referred to as LsRec is proposed, which generally contains offline computation and online incremental update. More precisely, LsRec not only takes account of user’s social relationship, but also clusters items according to the similarity degree, furthermore, LsRec performs recommendation in each generated cluster respectively. In practice, LsRec could be capable of exploiting user-level social influence, and capturing the intricate relationship between items. In addition, theoretical proof could provide convergence guarantee for the model. Specifically, with the appealing merit of flexible online update scenario, LsRec could yield high performance in large-scale online recommendation with low computational complexity. Extensive experimental analysis over four real world datasets demonstrate the effectiveness and efficiency of LsRec, which indicates that LsRec could significantly outperform state-of-the-art recommender approaches, especially in large-scale online recommendation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305637",
    "keywords": [
      "Computer science",
      "Data science",
      "Information retrieval",
      "Physics",
      "Quantum mechanics",
      "Scale (ratio)"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Wang"
      },
      {
        "surname": "Zhou",
        "given_name": "Yongluan"
      },
      {
        "surname": "Li",
        "given_name": "Jianping"
      },
      {
        "surname": "Memon",
        "given_name": "Muhammad Hammad"
      }
    ]
  },
  {
    "title": "Deep adaptive feature enrichment",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113780",
    "abstract": "Features play an important role in the performance of machine learning and classification applications. Usually, separability of classes by using raw or original features are so low, and it is necessary to use complex classifiers with high computational costs or use enrichment modules to increase distinctiveness of features. In this paper, a deep feature enrichment method is proposed to increase the distinguishing power of features using an adaptive neural network-based structure. Proposed method adaptively uses linear/non-linear activation functions for coding, and the dimension of the coding space adaptively adjusted to be lower, the same, or higher than the original feature space. Then the best neural network structure (number of layers and neurons per layers) and the optimum weights for the proposed neural structure are optimized using an evolutionary optimization algorithm. Optimized modules can map/code raw input features into an enriched feature space that can increase the separability of the data points among classes. In fact, our obtained enriched features can adapt themselves to the nature of the training data and they can improve the generalization power also the performance of conventional classifiers. Experimental results on popular UCI datasets such as Glass, Liver, Iris, Wine, Breast cancer and seeds show increase of significant correct recognition rates (11.63% for Glass, 4.35% for Liver, 13.34% for Iris, 27.78% for Wine, 0.72% for Breast cancer and 11.9% for seeds) and also improvement of more than 1.5% of verification rate and 2% of Identification rate for the Yale face database.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306047",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Coding (social sciences)",
      "Computer science",
      "Feature (linguistics)",
      "Feature vector",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Taghipour-Gorjikolaie",
        "given_name": "Mehran"
      },
      {
        "surname": "Sadri",
        "given_name": "Javad"
      },
      {
        "surname": "Razavi",
        "given_name": "Seyed Mohammad"
      }
    ]
  },
  {
    "title": "Recent trends in multicue based visual tracking: A review",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113711",
    "abstract": "In the recent years, multicue visual tracking frameworks have been preferred over single cue visual tracking approaches to address critical environmental challenges. In literature, it has been well accepted that combining multiple complementary cues extracted from single sensor or multiple sensors, deep features and features extracted from different layers of deep learning architecture enhance tracking performance and accuracy. In this paper, we have categorized the multi-cue object tracking work based on the exploited appearance model into traditional architecture and deep learning based trackers. The categorized work have been tabulated to provide detailed overview of the representative work and to list out the new trends in the domain. Also, we have briefly analyzed the various tracking benchmark and tabulated their substantial parameters. Our review work analyze the recent trends in the field of object tracking alongwith the latest tracking benchmark to indicate the future directions to the researchers. In addition, we have experimentally evaluated the state-of-the-arts on OTB-15, UAV123, VOT2017 and LaSOT datasets under various tracking challenges.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305352",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "BitTorrent tracker",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Domain (mathematical analysis)",
      "Eye tracking",
      "Field (mathematics)",
      "Geodesy",
      "Geography",
      "Kalman filter",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Pedagogy",
      "Psychology",
      "Pure mathematics",
      "Tracking (education)",
      "Tracking system",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Kumar",
        "given_name": "Ashish"
      },
      {
        "surname": "Walia",
        "given_name": "Gurjit Singh"
      },
      {
        "surname": "Sharma",
        "given_name": "Kapil"
      }
    ]
  },
  {
    "title": "Rotation invariant angle-density based features for an ice image classification system",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113744",
    "abstract": "One of the natural disasters which cause economic loss and are a serious threat to society are ice covering phenomena for overhead transmission power lines. This paper presents a new method for ice and non-ice image classification to improve ice detection results. The proposed method uses wavelet decomposition to extract robust features, such as those invariant to rotation, scaling and thickness of ice for classification. The proposed approach estimates the average of the high frequency sub-bands for each level. Then it obtains Canny edge components for the average wavelet image at each level. Our approach studies shape of edge components to identify the presence of ice. To achieve this, the proposed method finds the major and minor axes for each edge component, and then draws parallel lines to the major and minor axes over the edge components. For each parallel line to the major and minor axes, the proposed approach further extracts angle and density-based features for pixels that fall on the parallel lines to the major and minor axes. Next, our method selects features from each average wavelet image and further calculates the mean for the feature vectors corresponding to the level, which results in a feature matrix. Finally, the feature matrix is fed to a Multi-Layer Perceptron Neural Network for classifying ice and non-ice images. Experimental results on a diversified dataset and comparative study with an existing method show that the proposed method is useful for accurate ice detection with better accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305686",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Feature extraction",
      "Geology",
      "Invariant (physics)",
      "Linguistics",
      "Mathematical physics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Perceptron",
      "Philosophy",
      "Pixel",
      "Remote sensing",
      "Wavelet"
    ],
    "authors": [
      {
        "surname": "Yue",
        "given_name": "Shengkai"
      },
      {
        "surname": "Yuan",
        "given_name": "Minglei"
      },
      {
        "surname": "Lu",
        "given_name": "Tong"
      },
      {
        "surname": "Shivakumara",
        "given_name": "Palaiahnakote"
      },
      {
        "surname": "Blumenstein",
        "given_name": "Michael"
      },
      {
        "surname": "Shi",
        "given_name": "Jie"
      },
      {
        "surname": "Kumar",
        "given_name": "G. Hemantha"
      }
    ]
  },
  {
    "title": "Feedback-based metric learning for activity recognition",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2018.09.021",
    "abstract": "Mobile activity recognition is an effective approach to understanding human context in real time. Existing methods based on supervised learning that require a large amount of training samples for building activity recognition models. The collection of labeled training samples is a boring process and most users are reluctant to get involved. Crowdsourcing is a simple and potential approach to collecting the training samples and building accurate activity recognizers. Since different people usually have different physical features and behavior patterns, an accurate activity recognition model cannot be constructed directly from the training samples collected by crowdsourcing. In this paper, we have proposed a Mixture Expert Model for Activity Recognition (MEMAR) based on feedback and crowdsourcing samples. The proposed model can continuously discover the difference between user activity and crowdsourcing samples. Then we update activity recognition models with the discovered differences. A mobile can correctly utilize crowdsourcing samples for recognition model construction with MEMAR and can also track and recognize mobile users’ behavior dynamics. The experiments based on a smartphone dataset verify the validity of MEMAR. We believe MEMAR provides a basis for context-aware mobile applications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417418305980",
    "keywords": [
      "Activity recognition",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Crowdsourcing",
      "Economics",
      "Human–computer interaction",
      "Machine learning",
      "Metric (unit)",
      "Mobile device",
      "Operating system",
      "Operations management",
      "Paleontology",
      "Process (computing)",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Xiaofeng"
      },
      {
        "surname": "Xu",
        "given_name": "Yang"
      },
      {
        "surname": "Hu",
        "given_name": "Haixiao"
      },
      {
        "surname": "Liu",
        "given_name": "Ming"
      },
      {
        "surname": "Li",
        "given_name": "Gang"
      }
    ]
  },
  {
    "title": "Towards integrated dialogue policy learning for multiple domains and intents using Hierarchical Deep Reinforcement Learning",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113650",
    "abstract": "Creation of Expert and Intelligent Dialogue/Virtual Agent (VA) that can serve complicated and intricate tasks (need) of the user related to multiple domains and its various intents is indeed quite challenging as it necessitates the agent to concurrently handle multiple subtasks in different domains. This paper presents an expert, unified and a generic Deep Reinforcement Learning (DRL) framework that creates dialogue managers competent for managing task-oriented conversations embodying multiple domains along with their various intents and provide the user with an expert system which is a one stop for all queries. In order to address these multiple aspects, the dialogue exchange between the user and the VA is split into hierarchies, so as to isolate and identify subtasks belonging to different domains. The notion of Hierarchical Reinforcement Learning (HRL) specifically options is employed to learn optimal policies in these hierarchies that operate at varying time steps to accomplish the user goal. The dialogue manager encompasses a top-level domain meta-policy, intermediate-level intent meta-policies in order to select amongst varied and multiple subtasks or options and low-level controller policies to select primitive actions to complete the subtask given by the higher-level meta-policies in varying intents and domains. Sharing of controller policies among overlapping subtasks enables the meta-policies to be generic. The proposed expert framework has been demonstrated in the domains of “Air Travel” and “Restaurant”. Experiments as compared to several strong baselines and a state of the art model establish the efficiency of the learned policies and the need for such expert models capable of handling complex and composite tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420304747",
    "keywords": [
      "Agronomy",
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Control (management)",
      "Controller (irrigation)",
      "Domain (mathematical analysis)",
      "Economics",
      "Finance",
      "Human–computer interaction",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Order (exchange)",
      "Reinforcement learning",
      "State (computer science)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Saha",
        "given_name": "Tulika"
      },
      {
        "surname": "Gupta",
        "given_name": "Dhawal"
      },
      {
        "surname": "Saha",
        "given_name": "Sriparna"
      },
      {
        "surname": "Bhattacharyya",
        "given_name": "Pushpak"
      }
    ]
  },
  {
    "title": "Value of fuzzy logic for data mining and machine learning: A case study",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113781",
    "abstract": "In this paper, a case study on the role of fuzzy logic (FL) in data mining and machine learning is carried out. It is outlined that, in order to draw more attention of data-mining and machine-learning communities to FL, studies on FL could be more focused not on the activities that fuzzy methods can perform better but rather on the activities that fuzzy methods can perform and the non-fuzzy ones can’t. Such approach takes us away from discussing quantitative differences between fuzzy and non-fuzzy methods to discussing qualitative differences, which are possibly more favorable objects of scientific curiosity. Following the outlined suggestion, a novel speed-up technique is proposed in this paper to support association rule mining (ARM). The proposed technique is a clustering-based one and provides fusion of clustering and ARM. The catchy feature of this technique is that it works well if applied in fuzzy ARM and doesn’t work well if applied in non-fuzzy ARM. The proposed technique is put through experimental verification involving several real-world datasets, and the results substantiate its effectiveness.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306059",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Defuzzification",
      "Fuzzy classification",
      "Fuzzy clustering",
      "Fuzzy control system",
      "Fuzzy logic",
      "Fuzzy number",
      "Fuzzy set",
      "Fuzzy set operations",
      "Machine learning",
      "Neuro-fuzzy",
      "Value (mathematics)"
    ],
    "authors": [
      {
        "surname": "Mirzakhanov",
        "given_name": "Vugar E."
      }
    ]
  },
  {
    "title": "Locally robust EEG feature selection for individual-independent emotion recognition",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113768",
    "abstract": "Brain computer interface (BCI) systems can decode brain affective activities into interpretable features and facilitate emotional human–computer interaction. However, individual differences of neurophysiological responses from BCI subjects constitute a stumbling block in individual-independent emotion recognition. In this study, we propose a new locally-robust feature selection (LRFS) method to determine generalizable features of electroencephalography (EEG) within several subsets of accessible subjects. In the LRFS framework, extracted EEG features are first modeled with probability densities. By evaluating the similarity of all density functions between each two subjects, inter-individual consistency of the EEG features is described. The derived consistency determines locally-robust EEG features, wherein importance of each feature is examined according to margin loss between emotions. To fuse selected features from multiple subsets of subjects, we employ ensemble learning principle and build an emotion classifier committee. Based on public DEAP and MAHNOB-HCI databases, individual-independent classification accuracy of the LRFS-based classifier is achieved by 0.65–0.68 (DEAP) and 0.67–0.70 (MAHNOB-HCI) for arousal and valence domains, respectively. Competitiveness of the LRFS has been validated when compared with several existing feature selection methods and emotion recognition systems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305923",
    "keywords": [
      "Affective computing",
      "Arousal",
      "Artificial intelligence",
      "Brain–computer interface",
      "Classifier (UML)",
      "Computer science",
      "Electroencephalography",
      "Emotion classification",
      "Emotion recognition",
      "Feature (linguistics)",
      "Feature extraction",
      "Feature selection",
      "Linguistics",
      "Machine learning",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Psychology",
      "Quantum mechanics",
      "Support vector machine",
      "Valence (chemistry)"
    ],
    "authors": [
      {
        "surname": "Yin",
        "given_name": "Zhong"
      },
      {
        "surname": "Liu",
        "given_name": "Lei"
      },
      {
        "surname": "Chen",
        "given_name": "Jianing"
      },
      {
        "surname": "Zhao",
        "given_name": "Boxi"
      },
      {
        "surname": "Wang",
        "given_name": "Yongxiong"
      }
    ]
  },
  {
    "title": "A new investment method with AutoEncoder: Applications to crypto currencies",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113730",
    "abstract": "This paper proposes a novel approach to the portfolio management using an AutoEncoder. In particular, features learned by an AutoEncoder with ReLU are directly exploited to portfolio constructions. Since the AutoEncoder extracts characteristics of data through a non-linear activation function ReLU, its realization is generally difficult due to the non-linear transformation procedure. In the current paper, we solve this problem by taking full advantage of the similarity of ReLU and an option payoff. Especially, this paper shows that the features are successfully replicated by applying so-called dynamic delta hedging strategy. An out of sample simulation with crypto currency dataset shows the effectiveness of our proposed strategy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305546",
    "keywords": [
      "Artificial intelligence",
      "Autoencoder",
      "Biochemistry",
      "Biology",
      "Chemistry",
      "Computer science",
      "Computer security",
      "Cryptocurrency",
      "Currency",
      "Data mining",
      "Deep learning",
      "Economics",
      "Engineering",
      "Evolutionary biology",
      "Finance",
      "Financial economics",
      "Function (biology)",
      "Gene",
      "Image (mathematics)",
      "Investment strategy",
      "Market liquidity",
      "Mathematical optimization",
      "Mathematics",
      "Monetary economics",
      "Portfolio",
      "Project management",
      "Project portfolio management",
      "Realization (probability)",
      "Similarity (geometry)",
      "Statistics",
      "Systems engineering",
      "Transformation (genetics)"
    ],
    "authors": [
      {
        "surname": "Nakano",
        "given_name": "Masafumi"
      },
      {
        "surname": "Takahashi",
        "given_name": "Akihiko"
      }
    ]
  },
  {
    "title": "A hybrid fuzzy feature selection algorithm for high-dimensional regression problems: An mRMR-based framework",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113859",
    "abstract": "One of the most important factors affecting the interpretability of Fuzzy Rule-Based Systems (FRBSs) is the number of features used Indeed, the employment of a large number of features could be problematic for the components of an FRBS. In these cases,feature selection approaches can be employed to discover the most beneficial features. In this study, we present HFFS a Hybrid Fuzzy Feature Selection algorithm for high-dimensional regression problems. It benefits from both filter and wrapper methods. HFFS is composed of two main components: a filter-based Selector and a wrapper-based Modifier. Selector is an mRMR-based framework that sequentially selects the most informative features and adds them to a candidate subset. However, since these features may not result in an efficient FRBS, they are evaluated by Modifier, which fine-tunes them whenever their overall performance declines. This procedure is repeated until no further progress is possible within the candidate subset, acting as the stopping point for the algorithm. The effectiveness of HFFS was evaluated using twenty-eight real-world regression datasets and four different types of FRBSs. The experimental results and statistical tests confirmed that HFFS was able to improve the estimation accuracy compared to the filter methods and reduce the computation times compared to the wrapper methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306692",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computation",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Feature (linguistics)",
      "Feature selection",
      "Filter (signal processing)",
      "Fuzzy logic",
      "Fuzzy rule",
      "Fuzzy set",
      "Interpretability",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Regression",
      "Selection (genetic algorithm)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Aghaeipoor",
        "given_name": "Fatemeh"
      },
      {
        "surname": "Javidi",
        "given_name": "Mohammad Masoud"
      }
    ]
  },
  {
    "title": "M-AdaBoost-A based ensemble system for network intrusion detection",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113864",
    "abstract": "Network intrusion detection remains a challenging research area as it involves learning from large-scale imbalanced multiclass datasets. While machine learning algorithms have been widely used for network intrusion detection, most standard techniques cannot achieve consistent good performance across multiple classes. In this paper we proposed a novel ensemble system based on the modified adaptive boosting with area under the curve (M-AdaBoost-A) algorithm to detect network intrusions more effectively. We combined multiple M-AdaBoost-A-based classifiers into an ensemble by employing various strategies, including particle swarm optimization. To the best of our knowledge, this study is the first to utilize the M-AdaBoost-A algorithm, which incorporates the area under the curve into the boosting process for addressing class imbalance in network intrusion detection. Compared with existing standard techniques, our proposed ensemble system achieved superior performance across multiple classes in both 802.11 wireless intrusion detection and traditional enterprise intrusion detection.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306746",
    "keywords": [
      "AdaBoost",
      "Artificial intelligence",
      "Boosting (machine learning)",
      "Computer science",
      "Data mining",
      "Ensemble learning",
      "Intrusion detection system",
      "Machine learning",
      "Particle swarm optimization",
      "Pattern recognition (psychology)",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Ying"
      },
      {
        "surname": "Mazzuchi",
        "given_name": "Thomas A."
      },
      {
        "surname": "Sarkani",
        "given_name": "Shahram"
      }
    ]
  },
  {
    "title": "A framework for multi-objective optimization of virtual tree pruning based on growth simulation",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113792",
    "abstract": "We present a framework for multi-objective optimization of fruit tree pruning within a simulated environment, where pruning is performed on a virtual tree model, and its effects on tree growth are observed. The proposed framework uses quantitative measures to express the short-term and long-term effects of pruning, for which potentially conflicting optimization objectives can be defined. The short-term objectives are evaluated on the pruned tree model directly, while the values of long-term objectives are estimated by executing a tree growth simulation. We demonstrate the concept by using a bi-objective case, where the estimated light interceptions of the pruned tree in the current and the next season are used to define separate optimization objectives. We compare the performance of the multi-objective simulated annealing and the NSGA-II method in building the sets of non-dominated pruning solutions. The obtained Pareto front approximations correspond to diverse pruning solutions that balance between optimizing either objective to different extents, which indicates a potential for new applications of the multi-objective pruning optimization concept.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306138",
    "keywords": [
      "Agronomy",
      "Biology",
      "Computer science",
      "Machine learning",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Multi-objective optimization",
      "Pareto principle",
      "Physics",
      "Pruning",
      "Quantum mechanics",
      "Simulated annealing",
      "Term (time)",
      "Tree (set theory)"
    ],
    "authors": [
      {
        "surname": "Strnad",
        "given_name": "Damjan"
      },
      {
        "surname": "Kohek",
        "given_name": "Štefan"
      },
      {
        "surname": "Benes",
        "given_name": "Bedrich"
      },
      {
        "surname": "Kolmanič",
        "given_name": "Simon"
      },
      {
        "surname": "Žalik",
        "given_name": "Borut"
      }
    ]
  },
  {
    "title": "Deep learning in citation recommendation models survey",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113790",
    "abstract": "The huge amount of research papers on the web makes finding a relevant manuscript a difficult task. In recent years many models were introduced to support researchers by providing personalized citation recommendations. Moreover, deep learning methods have been employed in this domain to improve the quality of the final recommendations. However, a thorough study that classifies citation recommendation models and examines their (a) strengths and weaknesses, (b) evaluation metrics used, (c) popular datasets, and challenges faced is missing. Therefore, with this survey, we present a new classification approach for deep learning models that provide citation recommendation. Our approach uses the following six criteria: data factors, data representation methods, methodologies, types of recommendations used, problems addressed, and personalization. Additionally, we present a comparative analysis of those models that use the same set of evaluation metrics and datasets. Moreover, we examine hot upcoming issues and solutions in light of explored literature. Also, the survey discusses and analyzes the evaluation metrics and datasets adopted by the explored models. Finally, we conclude our survey with trends and future directions to further assist research on that domain.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306126",
    "keywords": [
      "Artificial intelligence",
      "Citation",
      "Computer science",
      "Data science",
      "Deep learning",
      "Domain (mathematical analysis)",
      "Epistemology",
      "Information retrieval",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Personalization",
      "Philosophy",
      "Programming language",
      "Quality (philosophy)",
      "Set (abstract data type)",
      "Strengths and weaknesses",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Ali",
        "given_name": "Zafar"
      },
      {
        "surname": "Kefalas",
        "given_name": "Pavlos"
      },
      {
        "surname": "Muhammad",
        "given_name": "Khan"
      },
      {
        "surname": "Ali",
        "given_name": "Bahadar"
      },
      {
        "surname": "Imran",
        "given_name": "Muhammad"
      }
    ]
  },
  {
    "title": "Landscape-assisted multi-operator differential evolution for solving constrained optimization problems",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113033",
    "abstract": "Over time, many differential evolution (DE) algorithms have been proposed for solving constrained optimization problems (COPs). However, no single DE algorithm was found to be the best for many types of COPs. Although researchers tried to mitigate this shortcoming by using multiple DE algorithms under a single algorithm structure, while putting more emphasis on the best-performing one, the use of landscape information in such designs has not been fully explored yet. Therefore, in this research, a multi-operator DE algorithm is developed, which uses a landscape-based indicator to choose the best-performing DE operator throughout the evolutionary process. The performance of the proposed algorithm was tested by solving a set of constrained optimization problems, 22 from CEC2006, 36 test problems from CEC2010 (18 with 10D and 18 with 30D), 10 real-application constrained problems from CEC2011 and 84 test problems from CEC2017 (28 with 10D, 28 with 30D and 28 with 50D). Several experiments were designed and carried out, to analyze the effects of different components on the proposed algorithm’s performance, and the results from the final variant of the proposed algorithm were compared with different variants of the same algorithm with different selection criteria. Subsequently, the best variant found after analyzing the algorithm’s components, was compared to several state-of-the-art algorithms, with the results showing the capability of the proposed algorithm to attain high-quality results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741741930750X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Differential evolution",
      "Evolutionary algorithm",
      "Gene",
      "Mathematical optimization",
      "Mathematics",
      "Operating system",
      "Operator (biology)",
      "Optimization problem",
      "Process (computing)",
      "Programming language",
      "Repressor",
      "Selection (genetic algorithm)",
      "Set (abstract data type)",
      "Transcription factor"
    ],
    "authors": [
      {
        "surname": "Sallam",
        "given_name": "Karam M."
      },
      {
        "surname": "Elsayed",
        "given_name": "Saber M."
      },
      {
        "surname": "Sarker",
        "given_name": "Ruhul A."
      },
      {
        "surname": "Essam",
        "given_name": "Daryl L."
      }
    ]
  },
  {
    "title": "Evaluating large, high-technology project portfolios using a novel interval-valued Pythagorean fuzzy set framework: An automated crane project case study",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113007",
    "abstract": "The contemporary organization relies increasingly on developing large, high technology projects in order to gain local and global competitive advantage. Uncertainty and the complexity of project evaluation requires improved and tailored decision making support systems. A new framework for high technology project portfolio evaluation is introduced. Novel development of an interval-valued Pythagorean fuzzy set (IVPFS) approach is shown to accommodate degrees of membership, non-membership and hesitancy in the evaluation process. Developed methods of linear assignment, IVPFS ranking, IVPFS knowledge index, and IVPFS comparison provide a new framework for group evaluation based on a weighting for each decision expert. The framework is developed as a last aggregation which avoids information loss and introduces a new aggregation process. A novel multi-objective model is then introduced to address project portfolio selection while optimizing the value of the portfolio in terms of resilience (the risk of disruption and delays) and skill utilization (assignment of human resources). The applicability of this framework is demonstrated through a case study in high technology portfolio evaluation. The case study shows that the presented framework can be applied as the core to a high technology evaluation decision support system.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419307249",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Decision support system",
      "Economics",
      "Engineering",
      "Financial economics",
      "Fuzzy logic",
      "Fuzzy set",
      "Geometry",
      "Machine learning",
      "Mathematics",
      "Medicine",
      "Operating system",
      "Operations research",
      "Portfolio",
      "Process (computing)",
      "Programming language",
      "Project management",
      "Project portfolio management",
      "Pythagorean theorem",
      "Radiology",
      "Ranking (information retrieval)",
      "Set (abstract data type)",
      "Systems engineering",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Mohagheghi",
        "given_name": "Vahid"
      },
      {
        "surname": "Mousavi",
        "given_name": "Seyed Meysam"
      },
      {
        "surname": "Mojtahedi",
        "given_name": "Mohammad"
      },
      {
        "surname": "Newton",
        "given_name": "Sidney"
      }
    ]
  },
  {
    "title": "A multi objective-BSC model for new product development project portfolio selection",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113757",
    "abstract": "The most significant factor for the survival of an enterprise under a high level of competition is new product development (NPD). As a result, selecting a potential NPD project portfolio to gain competitive advantage has become a major concern to enterprises. However, selection of an NPD project portfolio is intricate due to multiple selection criteria and factors. This study focuses on optimizing an NPD project portfolio selection problem. To this end, Balance Score Card (BSC) is employed as a comprehensive framework to define NPD project selection criteria. Afterward, a multi-objective mathematical model is formulated that attempts to maximize total outcome, to minimize total risk, and to maximize strategic advantages. Our proposed model also takes into account suppliers, consumer demands, and project interdependencies. Because of the NP-hardness of the proposed model, two multi-objective metaheuristic algorithms, multi-objective particle swarm optimization (MOPSO), and non dominated sorting genetic algorithm (NSGA-II) are applied to solve the proposed model. It should be noted that the performance of algorithms is evaluated using the ɛ-constraint method and enhanced using response surface methodology (RSM). Finally, several numerical examples of different sizes are generated to compare the performance of metaheuristic solution methods based on four comparing metrics. Computational results show that NSGA-II outperforms MOPSO in terms of all the evaluation metrics.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305819",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Business",
      "Computer science",
      "Economics",
      "Engineering",
      "Financial economics",
      "Genetic algorithm",
      "Machine learning",
      "Marketing",
      "Mathematical optimization",
      "Mathematics",
      "Metaheuristic",
      "Multi-objective optimization",
      "New product development",
      "Operations research",
      "Particle swarm optimization",
      "Portfolio",
      "Project management",
      "Project portfolio management",
      "Selection (genetic algorithm)",
      "Sorting",
      "Systems engineering"
    ],
    "authors": [
      {
        "surname": "Abbasi",
        "given_name": "Darya"
      },
      {
        "surname": "Ashrafi",
        "given_name": "Maryam"
      },
      {
        "surname": "Ghodsypour",
        "given_name": "Seyed Hassan"
      }
    ]
  },
  {
    "title": "Value of fuzzy logic for data mining and machine learning: A case study",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113781",
    "abstract": "In this paper, a case study on the role of fuzzy logic (FL) in data mining and machine learning is carried out. It is outlined that, in order to draw more attention of data-mining and machine-learning communities to FL, studies on FL could be more focused not on the activities that fuzzy methods can perform better but rather on the activities that fuzzy methods can perform and the non-fuzzy ones can’t. Such approach takes us away from discussing quantitative differences between fuzzy and non-fuzzy methods to discussing qualitative differences, which are possibly more favorable objects of scientific curiosity. Following the outlined suggestion, a novel speed-up technique is proposed in this paper to support association rule mining (ARM). The proposed technique is a clustering-based one and provides fusion of clustering and ARM. The catchy feature of this technique is that it works well if applied in fuzzy ARM and doesn’t work well if applied in non-fuzzy ARM. The proposed technique is put through experimental verification involving several real-world datasets, and the results substantiate its effectiveness.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306059",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Defuzzification",
      "Fuzzy classification",
      "Fuzzy clustering",
      "Fuzzy control system",
      "Fuzzy logic",
      "Fuzzy number",
      "Fuzzy set",
      "Fuzzy set operations",
      "Machine learning",
      "Neuro-fuzzy",
      "Value (mathematics)"
    ],
    "authors": [
      {
        "surname": "Mirzakhanov",
        "given_name": "Vugar E."
      }
    ]
  },
  {
    "title": "Biogeography-based optimization algorithm for large-scale multistage batch plant scheduling",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113776",
    "abstract": "The batch process is characterized by many varieties, small batches, redundant production equipment, flexible production process, and high-added-value products. This process is widely used in chemical, plastic, rubber, pharmaceutical, fine chemical, metallurgical, steel, food, and other industries. The optimized scheduling scheme of the batch process can effectively enhance enterprise competitiveness and improve economic benefits. Multistage Multiproduct Scheduling Problem (MMSP) is an important branch of batch scheduling problems. It is difficult to solve MMSP within a reasonable time by traditional mathematical programming, because once the scale of scheduling problems increases, the solution space expands exponentially. This study proposes a metaheuristic approach based on a time key biogeography-based optimization algorithm to solve MMSP. This new time key representation contains two vectors, which represent the processing sequence and equipment allocation of orders respectively. In accordance with the time information in the new representation, we add the preference of equipment processing time to migration and calculate the probability of every mutation value. In addition, the elite solution is combined with the active scheduling technique and modified Nawaz-Enscore-Ham (NEH) algorithm to improve the search accuracy of the proposed algorithm. To test the performance of Improved Time Key Biogeography-Based Optimization (Improved-TKBBO) algorithm, its results are compared with computational results of mathematical programming, Genetic Algorithm (GA), and Line-up Competition Algorithm (LCA). Simulation results show that the proposed Improved-TKBBO can solve the large-scale MMSP with non-identical parallel units effectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742030600X",
    "keywords": [
      "Algorithm",
      "Batch processing",
      "Batch production",
      "Computer science",
      "Engineering",
      "Job shop scheduling",
      "Mathematical optimization",
      "Mathematics",
      "Metaheuristic",
      "Operating system",
      "Operations management",
      "Programming language",
      "Schedule",
      "Scheduling (production processes)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Yaya"
      },
      {
        "surname": "Gu",
        "given_name": "Xingsheng"
      }
    ]
  },
  {
    "title": "Personalizing the Top-k Spatial Keyword Preference Query with textual classifiers",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113841",
    "abstract": "The volume of data available online has increased considerably in the last years. Among this extensive amount, there is a specific kind called spatial data – a representation of a physical object using its spatial coordinates (e.g. latitude and longitude). Spatial queries are widely employed to manipulate spatial data more efficiently. However, the user has a crucial role in the spatial information retrieval process when querying the needed information. During the last few decades, researchers have proposed several techniques to aid users to express the information needed, such as boolean models, pattern matching operators, and query expansion. Despite the existence of significant alternatives to express the information need, there is still a lack of solutions applied to keyword preference queries. In this context, this work proposes a personalization approach to improve the Top-k Spatial Keyword Preference Query (SKPQ) results. By exploiting reviews on points of interest, the system identifies points which satisfy the user to order the result set with respect to his/her preference. The SKPQ result is compared to the one generated by our proposal using the OpinRank dataset. The assessment indicates a relative NDCG (Normalized Discounted Cumulative Gain) improvement of the proposed approach over the SKPQ of 92% when using random query keywords, and 33% when using frequent keywords. This personalization approach can be applied to any spatial keyword query since it re-orders the results instead of modifying the query processing.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306503",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Data mining",
      "Information retrieval",
      "Mathematics",
      "Object (grammar)",
      "Paleontology",
      "Personalization",
      "Preference",
      "Programming language",
      "Query expansion",
      "Query optimization",
      "Search engine",
      "Set (abstract data type)",
      "Spatial analysis",
      "Spatial contextual awareness",
      "Spatial query",
      "Statistics",
      "Web query classification",
      "Web search query",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Dias de Almeida",
        "given_name": "João Paulo"
      },
      {
        "surname": "Durão",
        "given_name": "Frederico Araújo"
      }
    ]
  },
  {
    "title": "Clustering-based multiple instance learning with multi-view feature",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113027",
    "abstract": "Multi-instance learning (MIL) is a special kind of classification problem where samples (called “instances”) are grouped into bags and labels are given only on bag level instead of instance level. From the expert and intelligent system perspective, MIL does not require full ground-truth labels which helps to reduce the cost of data labeling in real tasks. Many inviting problems such as image classification, video annotation and object detection can be formulated in MIL frameworks. In this paper, we propose a similarity-based method with clustering in a multi-view feature manner to solve MIL problems efficiently. The clustering is introduced as a novel strategy for instance selection. Considering unlabeled data, clustering methods fit well for obtaining the hidden structure information in feature space, and thus we utilize a clustering-based strategy to exploit such information on discovering positive instances. To fully use the original input data, we further develop our method in multi-view feature space, in order to make our model capture information from different feature space and provide support for the whole system. Experiments on benchmark datasets for classification have been conducted, and we also perform image retrieval to examine the extended multi-view model on a large image dataset, MIL-NUS-WIDE. The promising results demonstrate the effectiveness of our clustering-based MIL (CMIL) model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419307444",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "CURE data clustering algorithm",
      "Cluster analysis",
      "Clustering high-dimensional data",
      "Computer science",
      "Computer security",
      "Consensus clustering",
      "Correlation clustering",
      "Data mining",
      "Exploit",
      "Feature (linguistics)",
      "Feature vector",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Linguistics",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "He",
        "given_name": "Chengkun"
      },
      {
        "surname": "Shao",
        "given_name": "Jie"
      },
      {
        "surname": "Zhang",
        "given_name": "Jiasheng"
      },
      {
        "surname": "Zhou",
        "given_name": "Xiangmin"
      }
    ]
  },
  {
    "title": "Generalized objects in the system with dispersed knowledge",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113773",
    "abstract": "The inference process based on a set of local decision tables is considered in the article. Determining global decisions in such a situation is a complex and time-consuming task. The aim of the presented approach is to simplify this process by significantly reducing the size of local decision tables and, at the same time, maintaining the quality of decisions made. The research objective is the application of generalized objects with respect to the indiscernibility relation. When we use the generalized objects only the relevant and consistent knowledge remains in tables. In addition, the use of generalized objects for local tables results in a significant reduction in the number of objects in the tables. Such a change has a huge impact on the system’s construction for dispersed data. The paper introduces a definition of the generalized objects that are suitable for quantitative data. In addition, a definition of the generalized objects generated with the accuracy expressed by the parameter, which are appropriate for qualitative data, is given. It was experimentally tested that the use of generalized objects significantly reduces the number of objects in local tables, up to 80 % or even 90 % percent of the original table. In addition, it was shown that the use of generalized objects for local tables with large number of attributes gives comparable quality of classification to the dispersed system using full objects. Moreover, it was depicted that the quality of classification obtained for such data and the dispersed system with the generalized objects is better than the quality that are obtained for full objects without using the dispersed system.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305972",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Decision table",
      "Economics",
      "Epistemology",
      "Geometry",
      "Inference",
      "Management",
      "Mathematics",
      "Operating system",
      "Philosophy",
      "Process (computing)",
      "Programming language",
      "Quality (philosophy)",
      "Reduction (mathematics)",
      "Relation (database)",
      "Rough set",
      "Set (abstract data type)",
      "Table (database)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Przybyła-Kasperek",
        "given_name": "Małgorzata"
      }
    ]
  },
  {
    "title": "A multi-objective optimization approach for the group formation problem",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113828",
    "abstract": "Group formation is one of the essential stages of collaborative learning. This paper proposes an intelligent computational approach to optimize the group formation process taking into account multiple criteria: inter-homogeneity, intra-heterogeneity, and empathy. More specifically, it uses a genetic algorithm to maximize the number of different student profiles in each group. The proposed method was evaluated regarding its computational performance comparing against three baselines; and in a real educational application, where it was compared with random and self-organized methods. The results showed the potential of the proposed method from both the computational and pedagogical points of view.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306400",
    "keywords": [
      "Chemistry",
      "Computer science",
      "Group (periodic table)",
      "Mathematical optimization",
      "Mathematics",
      "Organic chemistry"
    ],
    "authors": [
      {
        "surname": "Miranda",
        "given_name": "Péricles B.C."
      },
      {
        "surname": "Mello",
        "given_name": "Rafael Ferreira"
      },
      {
        "surname": "Nascimento",
        "given_name": "André C.A."
      }
    ]
  },
  {
    "title": "Modeling a shared hierarchical structure in data envelopment analysis: An application to bank branches",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113700",
    "abstract": "The paper addresses the question of ensuring comparability in data envelopment analysis (DEA) in situations when units are organized in an ordered hierarchy with functions shared at different levels. In such a case, although units may have identical input-output sets that they put to use in similar production, they are not an ideally homogeneous group and their comparability in a benchmarking context is limited. The paper proposes to control explicitly the degree of comparability by a fairly flexible comparability constraint in order to obtain more informative technical efficiency scores and economically feasible targets. Furthermore, the paper develops a methodology to identify closest targets under the comparability constraint that are more attainable for inefficient units than traditional targets. These ideas are demonstrated in a case study located in the area of bank branch performance assessment from which the motivation of the paper sprouted. The case study shows for three hierarchical branch categories of a Slovak commercial bank that the comparability constraint renders closest targets more apposite, but they depend on how slacks are handled, i.e. whether they are summarized by a normalized sum or by means of a slacks-based measure.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305248",
    "keywords": [
      "Benchmarking",
      "Biology",
      "Business",
      "Combinatorics",
      "Comparability",
      "Computer science",
      "Constraint (computer-aided design)",
      "Context (archaeology)",
      "Data envelopment analysis",
      "Data mining",
      "Economics",
      "Geometry",
      "Hierarchy",
      "Market economy",
      "Marketing",
      "Mathematical optimization",
      "Mathematics",
      "Measure (data warehouse)",
      "Operations research",
      "Paleontology"
    ],
    "authors": [
      {
        "surname": "Boďa",
        "given_name": "Martin"
      },
      {
        "surname": "Dlouhý",
        "given_name": "Martin"
      },
      {
        "surname": "Zimková",
        "given_name": "Emília"
      }
    ]
  },
  {
    "title": "Data representations for audio-to-score monophonic music transcription",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113769",
    "abstract": "This work presents an end-to-end method based on deep neural networks for audio-to-score music transcription of monophonic excerpts. Unlike existing music transcription methods, which normally perform pitch estimation, the proposed approach is formulated as an end-to-end task that outputs a notation-level music score. Using an audio file as input, modeled as a sequence of frames, a deep neural network is trained to provide a sequence of music symbols encoding a score, including key and time signatures, barlines, notes (with their pitch spelling and duration) and rests. Our framework is based on a Convolutional Recurrent Neural Network (CRNN) with Connectionist Temporal Classification (CTC) loss function trained in an end-to-end fashion, without requiring to align the input frames with the output symbols. A total of 246,870 incipits from the Répertoire International des Sources Musicales online catalog were synthesized using different timbres and tempos to build the training data. Alternative input representations (raw audio, Short-Time Fourier Transform (STFT), log-spaced STFT and Constant-Q transform) were evaluated for this task, as well as different output representations (Plaine & Easie Code, Kern, and a purpose-designed output). Results show that it is feasible to directly infer score representations from audio files and most errors come from music notation ambiguities and metering (time signatures and barlines).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305935",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Economics",
      "Fourier analysis",
      "Fourier transform",
      "Linguistics",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Musical",
      "Musical notation",
      "Philosophy",
      "Recurrent neural network",
      "Short-time Fourier transform",
      "Speech recognition",
      "Task (project management)",
      "Transcription (linguistics)",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Román",
        "given_name": "Miguel A."
      },
      {
        "surname": "Pertusa",
        "given_name": "Antonio"
      },
      {
        "surname": "Calvo-Zaragoza",
        "given_name": "Jorge"
      }
    ]
  },
  {
    "title": "Clustering and classification of time series using topological data analysis with applications to finance",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113868",
    "abstract": "In this paper, we propose new methods for time series classification and clustering. These methods are based on techniques of Topological Data Analysis (TDA) such as persistent homology and time delay embedding for analysing time-series data. We present a new clustering method SOM-TDA and a new classification method RF-TDA based on TDA. Using SOM-TDA we examine the topological similarities and dissimilarities of some well-known time-series models used in finance. We also use the RF-TDA to examine if the topological features can be used to distinguish between time series models using simulated data. The performance of RF-TDA on the classification task is compared against three other classification methods. We also consider an application of RF-TDA to financial time series classification using real-life price data of stocks belonging to different sectors. RF-TDA is seen to perform quite well in the two experiments based on real-life stock-price data. This implies that the topological features of the time series of stock prices in the different sectors are not identical and have distinctive features that can be discerned through the use of TDA. We also briefly consider multi-class classification using RF-TDA.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742030676X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Embedding",
      "Machine learning",
      "Mathematics",
      "Paleontology",
      "Persistent homology",
      "Series (stratigraphy)",
      "Time series",
      "Topological data analysis"
    ],
    "authors": [
      {
        "surname": "Majumdar",
        "given_name": "Sourav"
      },
      {
        "surname": "Laha",
        "given_name": "Arnab Kumar"
      }
    ]
  },
  {
    "title": "Research trends in text mining: Semantic network and main path analysis of selected journals",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113851",
    "abstract": "In this study, network and main path analyses were conducted on 1856 studies related to text mining, by extracting keywords and citation information from the text of each paper. Our findings indicate that research papers on text mining have been published in 45 academic disciplines in the 1980s and 1990s, 105 disciplines in the 2000s, and 171 disciplines in the 2010s. The results show that using text mining as a research topic and method has rapidly increased. We also demonstrate that the main theme of text mining research is discourse and content analysis in the 1980s and 1990s, biology and data mining in the 2000s, and medicine and advanced text mining in the 2010s. Moreover, we examined the main citation path for text mining studies and suggest that the main focus of text mining studies has evolved from information science to information systems and technology management. Additionally, influential papers have been recently published in fields such as architecture and social ecology revealing the wide scope of text mining. This article presents an understanding of previously unexplored research trends in text mining and how these trends shed light on the most influential academic papers in the field.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306631",
    "keywords": [
      "Biomedical text mining",
      "Citation",
      "Citation analysis",
      "Computer science",
      "Data mining",
      "Data science",
      "Field (mathematics)",
      "Information retrieval",
      "Mathematics",
      "Path (computing)",
      "Programming language",
      "Pure mathematics",
      "Scope (computer science)",
      "Social media",
      "Social network analysis",
      "Text mining",
      "Theme (computing)",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Jung",
        "given_name": "Hoon"
      },
      {
        "surname": "Lee",
        "given_name": "Bong Gyou"
      }
    ]
  },
  {
    "title": "Multi-view Convolutional Neural Network for lung nodule false positive reduction",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113017",
    "abstract": "Background and objective Computer-Aided Detection (CAD) systems save radiologists time and provide a second opinion in detecting lung cancer by performing automated analysis of the scans. False positive reduction is one of the most crucial components of these systems that play a great role in the early diagnosis and treatment process. The objective of this paper is to efficiently handle this problem by detecting nodules and separating them from a large number of false positive candidates. Methods The proposed algorithm segments lungs and nodules through a combination of 2D and 3D region growing, thresholding and morphological operations. Vessels and most of the internal lung structure have a tabular shape that differs from the compact rounded shape of nodules, therefore they are eliminated by building and thresholding a3D depth map, to produce the initial candidates. To reduce the number of false positives, a rule-based classifier is used to eliminate the obvious non-nodules, followed by a multi-view Convolutional Neural Network. The convolutional network is built specifically to handle the provided inputs and is customized to provide the best possible outputs without the extra computational complexity that is required when compared to a 3D network. 650 cases from the LIDC dataset are used to train and test the network. For each candidate, the axial, coronal and sagittal views are extracted and fed to the three network streams. Results The proposed algorithm achieved a high detection sensitivity of 85.256%, a specificity of 90.658% and an accuracy of 89.895%. Experimental results indicate that the proposed algorithm outperforms most of the other algorithms in terms of accuracy and sensitivity. The proposed solution achieves a good tradeoff between efficiency and effectivity and saves much computation time. Conclusion The work shows that the proposed multi-view 2D network is a simple, yet effective algorithm for the false positive reduction problem. It can detect nodules that are isolated, linked to a vessel or attached to the lung wall. The network can be improved to detect ground glass nodules in the future.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419307341",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Convolutional neural network",
      "False positive paradox",
      "Geometry",
      "Image (mathematics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Reduction (mathematics)",
      "Thresholding"
    ],
    "authors": [
      {
        "surname": "El-Regaily",
        "given_name": "Salsabil Amin"
      },
      {
        "surname": "Salem",
        "given_name": "Mohammed Abdel Megeed"
      },
      {
        "surname": "Abdel Aziz",
        "given_name": "Mohamed Hassan"
      },
      {
        "surname": "Roushdy",
        "given_name": "Mohamed Ismail"
      }
    ]
  },
  {
    "title": "Dynamic behavior based churn prediction in mobile telecom",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113779",
    "abstract": "Customer churn is one of the most challenging problems that affects revenue and customer base in mobile telecom operators. The success of retention campaigns depends not only on the accuracy of predicting potential churners, but with equal importance, it depends on the timing when the prediction is done. Previous works related to churn prediction presented models to predict churn monthly with a focus on the static behavior of customers, and even the studies that considered the dynamic behavior of the customer, looked mainly at the monthly level behavior. However, customer behavior is susceptible to changes over days of month, and during the time leading up to a customer decision to churn, he/she starts behaving differently. Therefore, considering monthly behavioral features negatively affects the predictive performance, because it ignores changes in behavior over days of month. Moreover, predicting churners on monthly basis will be late for customers who decided to leave at the beginning of the month because they will not be detected as churners until the next month. To address these issues, in this paper, we propose daily churn prediction instead of monthly based on the daily dynamic behavior of customer instead of his monthly one. More precisely, we represent customer’s daily behavior as multivariate time series and propose four models to predict churn daily based on this representation. Two models depend on features extracted from the multivariate time series, namely RFM-based model and statistics-based model. While the other models, exploit deep learning techniques for automatic feature extraction, namely LSTM-based model and CNN-based model. The predictive performance of the proposed models were investigated by evaluating them using a 150-day-long dataset collected from MTN operator in the country. The results showed that the daily models significantly outperform the monthly models in terms of predicting churners earlier and more accurately. Furthermore, the LSTM-based model significantly outperforms the CNN-based model. However, the prediction performances of the LSTM-based and the CNN-based models are equal to the prediction performance of the RFM-based model. Moreover, all of these three models significantly outperform the Statistics-based model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306035",
    "keywords": [
      "Artificial intelligence",
      "Business",
      "Computer science",
      "Computer security",
      "Customer base",
      "Customer satisfaction",
      "Data mining",
      "Economics",
      "Exploit",
      "Feature (linguistics)",
      "Finance",
      "Law",
      "Linguistics",
      "Machine learning",
      "Marketing",
      "Multivariate statistics",
      "Philosophy",
      "Political science",
      "Politics",
      "Predictive modelling",
      "Representation (politics)",
      "Revenue",
      "Time series"
    ],
    "authors": [
      {
        "surname": "Alboukaey",
        "given_name": "Nadia"
      },
      {
        "surname": "Joukhadar",
        "given_name": "Ammar"
      },
      {
        "surname": "Ghneim",
        "given_name": "Nada"
      }
    ]
  },
  {
    "title": "Weighted kNN and constrained elastic distances for time-series classification",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113829",
    "abstract": "Time-series classification has been addressed by a plethora of machine-learning techniques, including neural networks, support vector machines, Bayesian approaches, and others. It is an accepted fact, however, that the plain vanilla 1-nearest neighbor (1NN) classifier, combined with an elastic distance measure such as Dynamic Time Warping (DTW), is competitive and often superior to more complex classification methods, including the majority-voting k-nearest neighbor (kNN) classifier. With this paper we continue our investigation of the kNN classifier on time-series data and the impact of various classic distance-based vote weighting schemes by considering constrained versions of four common elastic distance measures: DTW, Longest Common Subsequence (LCS), Edit Distance with Real Penalty (ERP), and Edit Distance on Real sequence (EDR). By performing experiments on the entire UCR Time Series Classification Archive we show that weighted kNN is able to consistently outperform 1NN. Furthermore, we provide recommendations for the choices of the constraint width parameter r, neighborhood size k, and weighting scheme, for each mentioned elastic distance measure.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306412",
    "keywords": [
      "Artificial intelligence",
      "Bounded function",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Dynamic time warping",
      "Machine learning",
      "Mahalanobis distance",
      "Mathematical analysis",
      "Mathematics",
      "Medicine",
      "Naive Bayes classifier",
      "Nearest neighbor search",
      "Pattern recognition (psychology)",
      "Radiology",
      "Subsequence",
      "Support vector machine",
      "Weighting",
      "k-nearest neighbors algorithm"
    ],
    "authors": [
      {
        "surname": "Geler",
        "given_name": "Zoltan"
      },
      {
        "surname": "Kurbalija",
        "given_name": "Vladimir"
      },
      {
        "surname": "Ivanović",
        "given_name": "Mirjana"
      },
      {
        "surname": "Radovanović",
        "given_name": "Miloš"
      }
    ]
  },
  {
    "title": "ETH analysis and predictions utilizing deep learning",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113866",
    "abstract": "This paper attempts to provide a data analysis of cryptocurrency markets. Such markets have been developed rapidly and their volatility poses significant research challenges and justifies intensive behavior analysis. For this, we develop statistical and machine learning techniques and apply them to analyze their price variations and to generate inferences. In particular, we utilize deep learning algorithms to predict the closing price of the Ethereum cryptocurrency in a short period. The price data is accumulated from Poloniex exchange and analyzed through a Convolutional Neural Network and four types of Recurrent Neural Network including the Long Short Term Memory network, the Stacked Long Short Term Memory network, the Bidirectional Long Short Term Memory network, and the Gated Recurrent Unit network. These deep learning models are benchmarked and compared under various metrics. Our experimental data suggest that certain of the above models can be utilized to predict the Ethereum closing price in real time with promising accuracy and experimentally proven profitability.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306758",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Closing (real estate)",
      "Computer science",
      "Computer security",
      "Convolutional neural network",
      "Cryptocurrency",
      "Deep learning",
      "Econometrics",
      "Economics",
      "Finance",
      "Machine learning",
      "Profitability index",
      "Recurrent neural network",
      "Volatility (finance)"
    ],
    "authors": [
      {
        "surname": "Zoumpekas",
        "given_name": "Thanasis"
      },
      {
        "surname": "Houstis",
        "given_name": "Elias"
      },
      {
        "surname": "Vavalis",
        "given_name": "Manolis"
      }
    ]
  },
  {
    "title": "Modeling and analysis of a stock-based collaborative filtering algorithm for the Chinese stock market",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113006",
    "abstract": "Under the assumption of transmission effect presence between the movement of stocks in the Chinese stock market, we employ collaborative filtering, a new technique used by recommender systems, to construct a stock prediction algorithm. We find that, when put into a quantitative investment strategy, this algorithm leads to strong profitability with an average annualized return of 11.42%. Furthermore, our analysis of the transmission effect among industry indexes indicates that the information flows move along the industrial chain. Our results reveal the significant roles that the traditional industries such as banking, automobile, and real estate, play in the Chinese economy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419307237",
    "keywords": [
      "Algorithm",
      "Biology",
      "Business",
      "Collaborative filtering",
      "Computer science",
      "Engineering",
      "Finance",
      "Horse",
      "Machine learning",
      "Mechanical engineering",
      "Paleontology",
      "Profitability index",
      "Real estate",
      "Recommender system",
      "Stock (firearms)",
      "Stock market"
    ],
    "authors": [
      {
        "surname": "Zheng",
        "given_name": "Zeqi"
      },
      {
        "surname": "Gao",
        "given_name": "Yuandong"
      },
      {
        "surname": "Yin",
        "given_name": "Likang"
      },
      {
        "surname": "Rabarison",
        "given_name": "Monika K."
      }
    ]
  },
  {
    "title": "A drift aware adaptive method based on minimum uncertainty for anomaly detection in social networking",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113881",
    "abstract": "The social attack is an example of the anomaly that often changed their behavior, increased data volumes, and should be detected as early as possible to minimize damage. Data streaming mining is one of the solutions, which can handle the social attacks, and adapt to the change in the anomaly data stream. In this paper, we propose Online Fusion of Experts based on a minimum uncertainty to predict the concept drift in a data stream of social network-attack. Online learning algorithms such as Linear-order algorithms and Gaussian-order algorithms employ as an expert to identify the change in the anomaly data stream. First, online learning algorithms determine the error value for each data sample when data stream enter individually. Second, O F E utilizes a maximum-posterior estimation of the error rate of online learning algorithms to generate a new input data stream. Third, the Uncertainty Error Correlation Matrix (UECM) of input data applies to real-time behavior change detection of a data stream. Performance of O F E is evaluated by related data streaming algorithms using a benchmark, and real dataset from UCI repository (NSL-KDD, ISCX, and etc.), and malicious web pages, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306850",
    "keywords": [
      "Anomaly (physics)",
      "Anomaly detection",
      "Benchmark (surveying)",
      "Computer science",
      "Concept drift",
      "Condensed matter physics",
      "Data mining",
      "Data stream",
      "Data stream mining",
      "Gaussian",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Physics",
      "Quantum mechanics",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "mahmodi",
        "given_name": "Emad"
      },
      {
        "surname": "Yazdi",
        "given_name": "Hadi Sadoghi"
      },
      {
        "surname": "Bafghi",
        "given_name": "Abbas Ghaemi"
      }
    ]
  },
  {
    "title": "Efficiency analysis trees: A new methodology for estimating production frontiers through decision trees",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113783",
    "abstract": "In this paper, we introduce a new methodology based on regression trees for estimating production frontiers satisfying fundamental postulates of microeconomics, such as free disposability. This new approach, baptized as Efficiency Analysis Trees (EAT), shares some similarities with the Free Disposal Hull (FDH) technique. However, and in contrast to FDH, EAT overcomes the problem of overfitting by using cross-validation to prune back the deep tree obtained in the first stage. Finally, the performance of EAT is measured via Monte Carlo simulations, showing that the new approach reduces the mean squared error associated with the estimation of the true frontier by between 13% and 70% in comparison with the standard FDH.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306072",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Decision tree",
      "Economics",
      "Machine learning",
      "Macroeconomics",
      "Production (economics)"
    ],
    "authors": [
      {
        "surname": "Esteve",
        "given_name": "Miriam"
      },
      {
        "surname": "Aparicio",
        "given_name": "Juan"
      },
      {
        "surname": "Rabasa",
        "given_name": "Alejandro"
      },
      {
        "surname": "Rodriguez-Sala",
        "given_name": "Jesus J."
      }
    ]
  },
  {
    "title": "A multi-product job shop scenario utilising Model Predictive Control",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113734",
    "abstract": "Multi-product manufacturing scenarios today have to face many challenges considering external factors such as availability of resources or attending product demands and internal factors such as adjustment of buffer levels or utilisation of workstations. In this paper, a multi-product job shop consisting of workstations coupled by an unidirectional material flow with different production routings is considered. The existing model based on the bond graph technique is adapted to an optimal control problem, allowing the applicability of the Model Predictive Control scheme. Concerning performance criteria, two different objective functions are defined: the first aims for predefined processing frequencies of the workstations and the second one takes into account product demands. Both approaches were examined in simulations showing that a steady state is achieved in terms of stable buffer levels and processing frequencies.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305583",
    "keywords": [
      "Artificial intelligence",
      "Computer network",
      "Computer science",
      "Control (management)",
      "Economics",
      "Engineering",
      "Flow control (data)",
      "Geometry",
      "Industrial engineering",
      "Macroeconomics",
      "Mathematical analysis",
      "Mathematics",
      "Model predictive control",
      "Operating system",
      "Product (mathematics)",
      "Production (economics)",
      "Scheme (mathematics)",
      "Workstation"
    ],
    "authors": [
      {
        "surname": "Sprodowski",
        "given_name": "Tobias"
      },
      {
        "surname": "Sagawa",
        "given_name": "Juliana Keiko"
      },
      {
        "surname": "Maluf",
        "given_name": "Arthur Sarro"
      },
      {
        "surname": "Freitag",
        "given_name": "Michael"
      },
      {
        "surname": "Pannek",
        "given_name": "Jürgen"
      }
    ]
  },
  {
    "title": "A review of fuzzy AHP methods for decision-making with subjective judgements",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113738",
    "abstract": "Analytic Hierarchy Process (AHP) is a broadly applied multi-criteria decision-making method to determine the weights of criteria and priorities of alternatives in a structured manner based on pairwise comparison. As subjective judgments during comparison might be imprecise, fuzzy sets have been combined with AHP. This is referred to as fuzzy AHP or FAHP. An increasing amount of papers are published which describe different ways to derive the weights/priorities from a fuzzy comparison matrix, but seldomly set out the relative benefits of each approach so that the choice of the approach seems arbitrary. A review of various fuzzy AHP techniques is required to guide both academic and industrial experts to choose suitable techniques for a specific practical context. This paper reviews the literature published since 2008 where fuzzy AHP is applied to decision-making problems in industry, particularly the various selection problems. The techniques are categorised by the four aspects of developing a fuzzy AHP model: (i) representation of the relative importance for pairwise comparison, (ii) aggregation of fuzzy sets for group decisions and weights/priorities, (iii) defuzzification of a fuzzy set to a crisp value for final comparison, and (iv) consistency measurement of the judgements. These techniques are discussed in terms of their underlying principles, origins, strengths and weakness. Summary tables and specification charts are provided to guide the selection of suitable techniques. Tips for building a fuzzy AHP model are also included and six open questions are posed for future work.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305625",
    "keywords": [
      "Analytic hierarchy process",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Consistency (knowledge bases)",
      "Context (archaeology)",
      "Data mining",
      "Defuzzification",
      "Engineering",
      "Fuzzy logic",
      "Fuzzy number",
      "Fuzzy set",
      "Management science",
      "Mathematics",
      "Operations research",
      "Pairwise comparison",
      "Paleontology",
      "Programming language",
      "Selection (genetic algorithm)",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Yan"
      },
      {
        "surname": "Eckert",
        "given_name": "Claudia M."
      },
      {
        "surname": "Earl",
        "given_name": "Christopher"
      }
    ]
  },
  {
    "title": "Using metaheuristics for the location of bicycle stations",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113684",
    "abstract": "In this work, we solve the problem of finding the best locations to place stations for depositing/collecting shared bicycles. To do this, we model the problem as the p-median problem, that is a major existing localization problem in optimization. The p-median problem seeks to place a set of facilities (bicycle stations) in a way that minimizes the distance between a set of clients (citizens) and their closest facility (bike station). We have used a genetic algorithm, iterated local search, particle swarm optimization, simulated annealing, and variable neighbourhood search, to find the best locations for the bicycle stations and study their comparative advantages. We use irace to parameterize each algorithm automatically, to contribute with a methodology to fine-tune algorithms automatically. We have also studied different real data (distance and weights) from diverse open data sources from a real city, Malaga (Spain), hopefully leading to a final smart city application. We have compared our results with the implemented solution in Malaga. Finally, we have analyzed how we can use our proposal to improve the existing system in the city by adding more stations.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742030508X",
    "keywords": [],
    "authors": [
      {
        "surname": "Cintrano",
        "given_name": "C."
      },
      {
        "surname": "Chicano",
        "given_name": "F."
      },
      {
        "surname": "Alba",
        "given_name": "E."
      }
    ]
  },
  {
    "title": "Robust link prediction in criminal networks: A case study of the Sicilian Mafia",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113666",
    "abstract": "Link prediction exercises may prove particularly challenging with noisy and incomplete networks, such as criminal networks. Also, the link prediction effectiveness may vary across different relations within a social group. We address these issues by assessing the performance of different link prediction algorithms on a mafia organization. The analysis relies on an original dataset manually extracted from the judicial documents of operation “Montagna”, conducted by the Italian law enforcement agencies against individuals affiliated with the Sicilian Mafia. To run our analysis, we extracted two networks: one including meetings and one recording telephone calls among suspects, respectively. We conducted two experiments on these networks. First, we applied several link prediction algorithms and observed that link prediction algorithms leveraging the full graph topology (such as the Katz score) provide very accurate results even on very sparse networks. Second, we carried out extensive simulations to investigate how the noisy and incomplete nature of criminal networks may affect the accuracy of link prediction algorithms. The experimental findings suggest the soundness of link predictions is relatively high provided that only a limited amount of knowledge about connections is hidden or missing, and the unobserved edges follow some kind of generative law. The different results on the meeting and telephone call networks indicate that the specific features of a network should be taken into careful consideration.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420304905",
    "keywords": [
      "Artificial intelligence",
      "Computer network",
      "Computer science",
      "Data mining",
      "Law",
      "Law enforcement",
      "Linguistics",
      "Link (geometry)",
      "Link analysis",
      "Machine learning",
      "Philosophy",
      "Political science",
      "Programming language",
      "Sicilian",
      "Soundness"
    ],
    "authors": [
      {
        "surname": "Calderoni",
        "given_name": "Francesco"
      },
      {
        "surname": "Catanese",
        "given_name": "Salvatore"
      },
      {
        "surname": "De Meo",
        "given_name": "Pasquale"
      },
      {
        "surname": "Ficara",
        "given_name": "Annamaria"
      },
      {
        "surname": "Fiumara",
        "given_name": "Giacomo"
      }
    ]
  },
  {
    "title": "Gaining insight to B2B relationships through new segmentation approaches: Not all relationships are equal",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113767",
    "abstract": "B2B market segmentation has both structure complexity and computation complexity. The existing market segmentation methods can not directly address these two challenges simultaneously and provide a comprehensive view of the whole problem. Nor are they able to provide guidance on the selection of the most suitable solution among candidates. This study formulates the B2B segmentation as a multi-dimensional optimization problem that integrates both customer behavior and marketing effectiveness. It applies an integrated segmentation method that unifies two market segmentation approaches: the Embedded Exchange Approach (a descriptive model) and the Predictive Satisfaction Approach (a predictive model). It proposes the use of an evolutionary based, multi-objective segmentation method to solve the structural and computational challenges. The method generates a set of Pareto optimal solutions which not only gives a holistic view of possible solutions in the Pareto optimal space but also allows marketers to use solution selection algorithm based on the properties of Pareto optimal sets. The study develops a solution selection algorithm that represents a good tradeoff of two objectives based on the geometric shape of the Pareto optimal solution front.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305911",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "O'Brien",
        "given_name": "Matthew"
      },
      {
        "surname": "Liu",
        "given_name": "Ying"
      },
      {
        "surname": "Chen",
        "given_name": "Hongyu"
      },
      {
        "surname": "Lusch",
        "given_name": "Robert"
      }
    ]
  },
  {
    "title": "Genetic programming-based learning of texture classification descriptors from Local Edge Signature",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113667",
    "abstract": "Describing texture is a very challenging problem for many image-based expert and intelligent systems (e.g. defective product detection, people re-identification, abnormality investigation in medical imaging and remote sensing applications … ) since the process of texture classification relies on the quality of the extracted features. Indeed, detecting and extracting features is a hard and time-consuming task that requires the intervention of an expert, notably when dealing with challenging textures. Thus, machine learning-based descriptors have emerged as another alternative to deal with the difficulty of feature extracting. In this work, we propose a new operator, which we named Local Edge Signature (LES) descriptor, to locally represent texture. The proposed texture descriptor is based on statistical information on edge pixels’ arrangement and orientation in a specific local region, and it is insensitive to rotation and scale changes. A genetic programming-based approach is then fitted to automatically learn a global texture descriptor that we called Genetic Texture Signature (GTS). In fact, a tree representation of individuals is used to generate global texture features by applying elementary operations on LES elements at a set of keypoints, and a fitness function evaluates the descriptors considering intra-class homogeneity and inter-class discrimination properties of their generated features. The obtained results, on six challenging texture datasets (Brodatz, Outex_TC_00000, Outex_TC_00013, KTH-TIPS, KTH-TIPS2b and UIUCTex), show that the proposed classification method, which is fully automated, achieves state-of-the-art performance, especially when the number of available training samples is limited.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420304917",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Genetic programming",
      "Image (mathematics)",
      "Pattern recognition (psychology)",
      "Pixel",
      "Texture (cosmology)"
    ],
    "authors": [
      {
        "surname": "Ghazouani",
        "given_name": "Haythem"
      },
      {
        "surname": "Barhoumi",
        "given_name": "Walid"
      }
    ]
  },
  {
    "title": "Corrigendum to “Ranking multiple-input and multiple-output units: A comparative study of data envelopment analysis and rank aggregation” [Expert Syst. Appl. 160 (2020) 113687]",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113862",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306722",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Data envelopment analysis",
      "Data mining",
      "Mathematics",
      "Rank (graph theory)",
      "Ranking (information retrieval)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Derek D."
      }
    ]
  },
  {
    "title": "EMUCF: Enhanced multistage user-based collaborative filtering through non-linear similarity for recommendation systems",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113724",
    "abstract": "The data sparsity is an acute challenge in most of the collaborative filterings (CFs) as their performance is affected by the known ratings of target users. Recently, active learning has become a prevalent and straight forward approach to cope with the data sparsity. In this approach, the newly entered users are requested to rate certain items while they signup to the underlying recommendation system. This work proposes an Enhanced Multistage User-based CF (EMUCF) algorithm, which uses the concept of active learning and predicts the unknown ratings for target users in two stages. Here, the anonymous ratings of each intermediary stage are predicted with traditional User_CF algorithm. However, the similarity models commonly used in User_CF are not adequate to compute the similarity among users. Therefore, the most recently introduced Bhattacharyya Coefficient based nonlinear similarity model Bhat_sim is used for similarity computations; it utilizes all rating pairs of items in the final estimates of users similarities. Later, an extension of simple EMUCF, the (n > 2)-stage EMUCF is proposed to increase the prediction accuracy by progressively increasing the density of the original rating matrix. The performance of simple EMUCF and its extension is evaluated on two benchmark Movielens-100K and Movielens-1M datasets. They obtain far superior results for prediction accuracy and recommendation precision compared to several prominent competing algorithms. Finally, the potential improvement in the n-stage EMUCF algorithm is assessed by establishing the connection between rating prediction accuracy and matrix density.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305480",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Bhattacharyya distance",
      "Collaborative filtering",
      "Computer science",
      "Data mining",
      "Eigenvalues and eigenvectors",
      "Epistemology",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Machine learning",
      "Matrix decomposition",
      "MovieLens",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Recommender system",
      "Similarity (geometry)",
      "Simple (philosophy)"
    ],
    "authors": [
      {
        "surname": "Jain",
        "given_name": "Ankush"
      },
      {
        "surname": "Nagar",
        "given_name": "Surendra"
      },
      {
        "surname": "Singh",
        "given_name": "Pramod Kumar"
      },
      {
        "surname": "Dhar",
        "given_name": "Joydip"
      }
    ]
  },
  {
    "title": "Graph classification algorithm based on graph structure embedding",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113715",
    "abstract": "With the application of data mining in many fields such as information science, bioinformatics, and network intrusion detection, more and more data are showing new features such as strong structuration and complex relationships between data. As a complex data structure, a graph can be used to describe the relationship between things. Traditional graph classification methods based on graph feature vector construction need to select a feature vector construction criterion in advance, such as graph-based theoretical indicators or graph-based topology occurrences, and then extract features from each graph in the graph set according to the designated criterion. However, the construction method of the graph feature vector is easy to lose the graph structural information and requires strong professional knowledge. Inspired by the Word2Vec and Doc2Vec models in the Natural Language Processing (NLP), this paper first constructs a “word list” of graph data consisting of subgraphs. Then a neural network for training graph embedding is designed with the graph itself as its input, and the “word” in the graph and the attribute features of the graph are used as its output, so that the neural network automatically learns the graph embedding corresponding to each graph. The graph embedding not only reflects the features of the graph itself but also includes the relative relationship among graphs. Finally, on the basis of the well-trained graph embedding, the common classifier can be used to classify graphs. Based on real-world bioinformatics and social data sets, the experiments demonstrate that the proposed graph classification algorithm has advantages over the existing graph classification algorithms based on feature vector construction.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742030539X",
    "keywords": [
      "Computer science",
      "Graph",
      "Graph embedding",
      "Graph property",
      "Line graph",
      "Null graph",
      "Theoretical computer science",
      "Voltage graph"
    ],
    "authors": [
      {
        "surname": "Ma",
        "given_name": "Tinghuai"
      },
      {
        "surname": "Pan",
        "given_name": "Qian"
      },
      {
        "surname": "Wang",
        "given_name": "Hongmei"
      },
      {
        "surname": "Shao",
        "given_name": "Wenye"
      },
      {
        "surname": "Tian",
        "given_name": "Yuan"
      },
      {
        "surname": "Al-Nabhan",
        "given_name": "Najla"
      }
    ]
  },
  {
    "title": "Skin lesion segmentation using fully convolutional networks: A comparative experimental study",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113742",
    "abstract": "Because the most dangerous type of skin cancer, melanoma, is very difficult for dermatologists to detect because of the low contrast between the lesion and the adjacent skin, the automatic application of skin lesion segmentation is regarded as very challenging. This paper proposes the implementation of a medical image segmentation that will accelerate a melanoma diagnosis by dermatologists. In the implementation, Fully Convolutional Network (FCN) architectures generated by modifying Convolutional Neural Network (CNN) architectures are used. The proposed algorithm for an automatic semantic segmentation of skin lesions utilizes four different FCN architectures, FCN-AlexNet, FCN-8s, FCN-16s, and FCN-32s. The experimental studies in this paper are constructed on the ISIC 2017 dataset, and the evaluations of these architectures on the dataset are carried out for the first time with this study. In the experimental studies, once the images in the dataset are preprocessed, the FCNs are first trained separately. Secondly, the accuracies and Dice coefficients on the validation dataset are calculated by using these trained FCN architectures. Thirdly, the obtained results are compared. Finally, the inferences of lesion segmentation are visualized in order to exhibit how exactly the FCN architectures can segment the lesions. The experimental results show that the FCNs in the proposed algorithm are suitable for skin lesion segmentation. In addition, it is thought that the experimental results will contribute to the scientific literature and assist the researchers who are working on medical image segmentation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305662",
    "keywords": [
      "Artificial intelligence",
      "Cancer",
      "Computer science",
      "Convolutional neural network",
      "Image segmentation",
      "Internal medicine",
      "Medicine",
      "Pathology",
      "Pattern recognition (psychology)",
      "Segmentation",
      "Skin cancer",
      "Skin lesion"
    ],
    "authors": [
      {
        "surname": "Kaymak",
        "given_name": "Ruya"
      },
      {
        "surname": "Kaymak",
        "given_name": "Cagri"
      },
      {
        "surname": "Ucar",
        "given_name": "Aysegul"
      }
    ]
  },
  {
    "title": "An effective binary artificial bee colony algorithm for maximum set k-covering problem",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113717",
    "abstract": "Given a row set M and a column set C, where each column in C covers several rows of M, the maximum set k-covering problem (MKCP) is to select k columns from C, such hat the number of rows covered by the selected columns is maximized. It can be formulated as linear integer programming, and has several real world applications. Several heuristic approaches have been previously presented for solving the MKCP. However, the obtained solution quality is not stable, and the solution time increases very quickly as the size of the instance increases. This work proposes a hybrid binary artificial bee colony algorithm (HBABC) to solve the MKCP. First, based on the characteristic of MKCP, HBABC redesigns a food source updating method. The new updating method uses the previously found solutions to guide the search. Second, to improve the exploitation ability of the HBABC, a tabu based simulated annealing (TBSA) is proposed. Moveover, we employ a bucket sorting technique to speed up the TBSA. Finally, the computational results on 75 benchmark instances demonstrate that the HBABC competes favorably with other algorithms. Specifically, our algorithm improves the best known solutions on about 20 percent of the tested instances. In addition, the HBABC performs better than two binary artificial bee colony algorithms on the tested instances.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305418",
    "keywords": [
      "Algorithm",
      "Arithmetic",
      "Artificial bee colony algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Binary number",
      "Column (typography)",
      "Computer science",
      "Database",
      "Frame (networking)",
      "Geodesy",
      "Geography",
      "Heuristic",
      "Integer (computer science)",
      "Integer programming",
      "Mathematical optimization",
      "Mathematics",
      "Programming language",
      "Row",
      "Set (abstract data type)",
      "Set cover problem",
      "Simulated annealing",
      "Sorting",
      "Tabu search",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Lin",
        "given_name": "Geng"
      },
      {
        "surname": "Xu",
        "given_name": "Haiping"
      },
      {
        "surname": "Chen",
        "given_name": "Xiang"
      },
      {
        "surname": "Guan",
        "given_name": "Jian"
      }
    ]
  },
  {
    "title": "Corporate default forecasting with machine learning",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113567",
    "abstract": "We analyze the performance of a set of machine learning models in predicting default risk, using standard statistical models, such as the logistic regression, as a benchmark. When only a limited information set is available, for example in the case of an external assessment of credit risk, we find that machine learning models provide substantial gains in discriminatory power and precision, relative to statistical models. This advantage diminishes when confidential information, such as credit behavioral indicators, is also available, and it becomes negligible when the dataset is small. Moreover, we evaluate the consequences of using a credit allocation rule based on machine learning ratings on the overall supply of credit and the number of borrowers gaining access to credit. Machine learning models concentrate a greater extent of credit towards safer and larger borrowers, which would result in lower credit losses for their lenders.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420303912",
    "keywords": [
      "Actuarial science",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Business",
      "Computer science",
      "Computer security",
      "Credit risk",
      "Econometrics",
      "Economics",
      "Geodesy",
      "Geography",
      "Logistic regression",
      "Machine learning",
      "Programming language",
      "SAFER",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Moscatelli",
        "given_name": "Mirko"
      },
      {
        "surname": "Parlapiano",
        "given_name": "Fabio"
      },
      {
        "surname": "Narizzano",
        "given_name": "Simone"
      },
      {
        "surname": "Viggiano",
        "given_name": "Gianluca"
      }
    ]
  },
  {
    "title": "Stock market forecasting with super-high dimensional time-series data using ConvLSTM, trend sampling, and specialized data augmentation",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113704",
    "abstract": "Forecasting stock market indexes is an important issue for market participants, because even a small improvement in forecast accuracy may lead to better trading decisions than those of other participants. Rising interest in deep learning has led to its application in stock market forecasting. However, it is still challenging to use market-size time-series data to predict composite index prices. In this study, we propose a new stock market forecasting framework, NuNet, which can successfully learn high-level features from super-high dimensional time-series data. NuNet is an end-to-end integrated neural network framework consisting of two feature extractor modules, a super-high dimensional market information feature extractor and a target index feature extractor. In addition, we propose a mini-batch sampling technique, trend sampling, which probabilistically samples more recent data when training. Furthermore, we propose a novel regularization method, called column-wise random shuffling, which is a data augmentation technique that can be applied to convolutional neural networks. The experiments are comprehensively carried out in three aspects for three indexes, namely S&P500, KOSPI200, and FTSE100. The results demonstrate that the proposed model outperforms all baseline models. Specifically, for the S&P500, KOSPI200, and FTSE100, the overall mean squared error of our proposed model NuNet(DA, T) is 60.79%, 51.29%, and 43.36% lower than that of the baseline model SingleNet(R), respectively. Moreover, we employ trading simulations with realistic transaction costs. Our proposed model outperforms the buy-and-hold strategy being an average of 2.57 times more profitable in three indexes.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305285",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer science",
      "Data mining",
      "Database transaction",
      "Econometrics",
      "Economics",
      "Finance",
      "Horse",
      "Index (typography)",
      "Machine learning",
      "Market data",
      "Mathematics",
      "Paleontology",
      "Programming language",
      "Stock market",
      "Stock market index",
      "Time series",
      "Transaction data",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Lee",
        "given_name": "Si Woon"
      },
      {
        "surname": "Kim",
        "given_name": "Ha Young"
      }
    ]
  },
  {
    "title": "Improving classification accuracy using data augmentation on small data sets",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113696",
    "abstract": "Data augmentation (DA) is a key element in the success of Deep Learning (DL) models, as its use can lead to better prediction accuracy values when large size data sets are used. DA was not very much used with earlier neural network models before 2012, and the reason might be related to the type of models and the size of the data sets used. We investigate in this work, applying several state-of-the-art models based on Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs), the effect of DA when using small size data sets, analyzing the results in terms of the prediction accuracy obtained according to the different characteristics of the training samples (number of instances and features, and class unbalance degree). We further introduce modifications to the standard methods used to generate the synthetic samples to alter the class balance representation, and the overall results indicate that with some computational effort a significant increase in prediction accuracy can be obtained when small data sets are considered.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305200",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Class (philosophy)",
      "Computer science",
      "Computer security",
      "Data mining",
      "Key (lock)",
      "Law",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Moreno-Barea",
        "given_name": "Francisco J."
      },
      {
        "surname": "Jerez",
        "given_name": "José M."
      },
      {
        "surname": "Franco",
        "given_name": "Leonardo"
      }
    ]
  },
  {
    "title": "Facial expression distribution prediction based on surface electromyography",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113683",
    "abstract": "Facial expression recognition plays an important role in research on human–computer interaction. The common facial expressions are mixtures of six basic emotions: anger, disgust, fear, happiness, sadness, and surprise. The current study, however, focused on a single basic emotion on the basis of physiological signals. We proposed emotion distribution learning (EDL) based on surface electromyography (sEMG) for predicting the intensities of basic emotions. We recorded the sEMG signals from the depressor supercilii, zygomaticus major, frontalis medial, and depressor anguli oris muscles. Six features were extracted in the frequency, time, time–frequency, and entropy domains. Principal component analysis (PCA) was used to select the most representative features for prediction. The key idea of EDL is to learn a function that maps the PCA-selected features to the facial expression distributions such that the special description degrees of all basic emotions for an emotion can be learned by EDL. Simultaneously, Jeffrey's divergence considered the relationship between different basic emotions. The performance of EDL was compared with that of multilabel learning based on PCA-selected features. Predicted results were measured by six indices, which could reflect the distance or similarity degree between distributions. We conducted an experiment on six different emotion distributions. Experimental results show that the EDL can predict the facial expression distribution more accurately than the other methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305078",
    "keywords": [
      "Anger",
      "Artificial intelligence",
      "Communication",
      "Computer science",
      "Disgust",
      "Electromyography",
      "Emotion classification",
      "Facial electromyography",
      "Facial expression",
      "Facial muscles",
      "Pattern recognition (psychology)",
      "Principal component analysis",
      "Psychiatry",
      "Psychology",
      "Sadness",
      "Speech recognition",
      "Surprise"
    ],
    "authors": [
      {
        "surname": "Xi",
        "given_name": "Xugang"
      },
      {
        "surname": "Zhang",
        "given_name": "Yan"
      },
      {
        "surname": "Hua",
        "given_name": "Xian"
      },
      {
        "surname": "Miran",
        "given_name": "Seyed M."
      },
      {
        "surname": "Zhao",
        "given_name": "Yun-Bo"
      },
      {
        "surname": "Luo",
        "given_name": "Zhizeng"
      }
    ]
  },
  {
    "title": "Stacked auto-encoder based tagging with deep features for content-based medical image retrieval",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113693",
    "abstract": "Content-based medical image retrieval (CBMIR) is one of the most challenging and ambiguous tasks used to minimize the semantic gap between images and human queries in datasets with rich information content. Similar to the human visual saliency mechanism, CBMIR systems also use the visual features in the images for searching purposes. As a result of this search process, automatically accessing the images is very convenient in large and balanced datasets. Still, it is generally not possible to find such datasets in the medical domain. In this study, a four-step and effective hash code generation technique is presented to reduce the semantic gap between low-level features and high-level semantics for unbalanced medical image datasets. In the first stage, the convolutional neural network (CNN) architecture, the most effective feature representation method available today, is employed to extract discriminative features from images automatically. The features obtained in the last fully connected layer (FCL) at the output of the CNN architecture are used for hash code generation. In the second stage, using the Synthetic Minority Over-sampling Technique (SMOTE), the imbalance between the classes in the dataset is reduced. The solution to the unbalanced problem increases performance by almost 3%. In the third stage, balanced features are converted to a code of 13 symbols by using deep stacked auto-encoder. Finally, this code is translated to the standard 13-character labeling and retrieval code used by the 'Image retrieval in the medical application' (IRMA) dataset, since this is the database with which experiments have been done. IRMA error parameter, classification performance, and retrieval performance of the proposed method are more successful than other state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305170",
    "keywords": [
      "Artificial intelligence",
      "Code (set theory)",
      "Computer science",
      "Computer security",
      "Convolutional neural network",
      "Data mining",
      "Deep learning",
      "Discriminative model",
      "Encoder",
      "Feature (linguistics)",
      "Hash function",
      "Image (mathematics)",
      "Image retrieval",
      "Linguistics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Semantic gap",
      "Semantics (computer science)",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Öztürk",
        "given_name": "Şaban"
      }
    ]
  },
  {
    "title": "Two-stage DEA in banks: Terminological controversies and future directions",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113632",
    "abstract": "Given the importance that two-stage Data Envelopment Analysis (DEA) models have attained in recent years, this paper presents a systematic review of the literature on the topic focusing on the banking industry. We discuss the two-stage terminology itself, which is not yet not consolidated. We also discuss the current state-of-the-art and present opportunities, as well as challenges, for future studies. We analyse 59 papers, divided them into ten classes that cover various perspectives of two stage DEA studies, such as the economic context, geographic region of the banking units, methodological characteristics, and type of the models, either internal or external. Additionally, we investigate several controversial points regarding two-stage DEA models, such as the variable selection approach, the technique used in the second stage, and the possible impact of non-discretionary variables on efficiency. Results of the literature review indicate the lack of a uniform or universal terminology for two-stage DEA models in the baking industry. Moreover, the main objective of most papers involves extending or improving DEA models. Radial models, with variable returns of scale, and the intermediation approach are the most frequent configurations. Finally, we identify seven gaps in the literature for both internal and external two-stage DEA models and two specific gaps to external ones. Each gap is discussed in depth in the text and can be considered opportunities for future studies.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420304565",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Data envelopment analysis",
      "Econometrics",
      "Economics",
      "Engineering",
      "Industrial engineering",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Microeconomics",
      "Operations research",
      "Paleontology",
      "Philosophy",
      "Production (economics)",
      "Returns to scale",
      "Selection (genetic algorithm)",
      "Stage (stratigraphy)",
      "Statistics",
      "Terminology",
      "Variable (mathematics)"
    ],
    "authors": [
      {
        "surname": "Henriques",
        "given_name": "Iago Cotrim"
      },
      {
        "surname": "Sobreiro",
        "given_name": "Vinicius Amorim"
      },
      {
        "surname": "Kimura",
        "given_name": "Herbert"
      },
      {
        "surname": "Mariano",
        "given_name": "Enzo Barberio"
      }
    ]
  },
  {
    "title": "Foreground detection by ensembles of random polygonal tilings",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113518",
    "abstract": "In this work a novel region-based approach for the detection of foreground in video sequences is presented. The model consists of an ensemble of layers or tilings, where each tiling represents, by means of randomly chosen parallelogram regions, the background of the scene. Currently, the image size of video surveillance cameras far exceeds one megapixel (more than 1024 × 768), and pixel-based proposals are poorly suited for near real-time ratios. Therefore, the analysis by pixel is replaced by an analysis by region, improving the final resolution by overlapping regions or parallelograms with different shapes and sizes. Thus, for each frame, each region estimates the probability of belonging to the foreground or background, to finally compute the consensus foreground mask among all the tilings. With this proposal, it is possible to detect the foreground in high resolution sequences, a process that is not feasible using pixel-level techniques. Several experiments have been carried out by employing a wide range of videos. A qualitative and quantitative comparison with the state-of-the-art algorithms is performed by using a well-known video dataset benchmark. The results show the feasibility of our proposal compared with higher resolution methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420303420",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Background subtraction",
      "Benchmark (surveying)",
      "Composite material",
      "Computer science",
      "Computer vision",
      "Foreground detection",
      "Frame (networking)",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Materials science",
      "Operating system",
      "Parallelogram",
      "Pattern recognition (psychology)",
      "Pixel",
      "Process (computing)",
      "Range (aeronautics)",
      "Resolution (logic)",
      "Robot",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Molina-Cabello",
        "given_name": "Miguel A."
      },
      {
        "surname": "Elizondo",
        "given_name": "David A."
      },
      {
        "surname": "Luque-Baena",
        "given_name": "Rafael M."
      },
      {
        "surname": "López-Rubio",
        "given_name": "Ezequiel"
      }
    ]
  },
  {
    "title": "Enhancing web service clustering using Length Feature Weight Method for service description document vector space representation",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113682",
    "abstract": "Due to the rapid growth of web services in repositories, discovering the requisite web service is becoming increasingly cumbersome task. It has raised the demand for efficient web service clustering algorithms. In service repositories, when related web services are stored in a clustered way, it enhances the web service discovery process by reducing search space and time. Many eminent researchers have worked in this field and used the Term Frequency – Inverse Document Frequency (TF-IDF) method for representing web services in vector space. In general, there are various limitations of the TF-IDF approach i.e. (1) Not efficient for large documents (2) Position of term and its co-occurrences does not matter (3) Unable to analyze how terms are dispersed in different documents. In the web service scenario, services are represented in short text form. TF-IDF does not work well in web service representation because of the reason that it is unable to effectively find the importance of a term concerning its occurrence in other documents. If we compare two service documents i.e. ‘s1’ and ‘s2’ first having a large and second having small number of terms respectively then TF-IDF does not demonstrate the importance of terms in ‘s1’ as smaller to ‘s2’. Therefore, it is not possible to assign effective weights to the terms. In the lack of effective vector space representation, the performance of the clustering algorithm also degrades. In this paper, we propose a new approach i.e. LFW+K which is based on Length Feature Weight (LFW) for the vectorized representation of service followed by K-Means clustering. The proposed approach helps to find the informative term from web service and assigns the term weight accordingly by considering parameters like the dimension of the web service document, maximum frequency of a term in the document and occurrences of a term in other documents. LFW+K is applied on the datasets of real-world web services and the performance is measured using standard measurement criteria (i.e. precision, recall, F1-score, and accuracy). Results of the proposed approach are compared with K-Means clustering on TF-IDF representation method i.e. TF-IDF+K. Results show that the proposed method outperforms the clustering done by using TF-IDF method for vector space representation of web services.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305066",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Economics",
      "Economy",
      "Feature vector",
      "Information retrieval",
      "Law",
      "Physics",
      "Political science",
      "Politics",
      "Quantum mechanics",
      "Representation (politics)",
      "Service (business)",
      "Term (time)",
      "Vector space model",
      "Web service",
      "World Wide Web",
      "tf–idf"
    ],
    "authors": [
      {
        "surname": "Agarwal",
        "given_name": "Neha"
      },
      {
        "surname": "Sikka",
        "given_name": "Geeta"
      },
      {
        "surname": "Awasthi",
        "given_name": "Lalit Kumar"
      }
    ]
  },
  {
    "title": "A systematic survey on influential spreaders identification in complex networks with a focus on K-shell based techniques",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113681",
    "abstract": "Almost all the complex interactions between humans, animals, biological cells, neurons, or any other objects are now modeled as a graph with the nodes as the objects of interest and interactions as the edges. The identification of the most central or influential node in such a complex network has many practical applications in diverse domains such as viral marketing, infectious disease spreading, rumor spreading in a social network, virus/worm spreading in computer networks, etc. Many centrality measures using the position/location of a node and network structure have been proposed in the literature. The node degree, shortest paths(closeness), and betweenness are used since long with degree capturing local effect and others global effect. The k-shell considers the coreness of the nodes by dividing the network into layers or shells. Many variations of k-shell proposed in recent years, as well as many researchers, use k-shell as a building block in their heuristic technique to alleviate the problems of classical k-shell and to identify influential spreaders more elegantly. The main objective of this paper is to analyze and compare the major variations of the k-shell based methods along with representative network topology based hybrid techniques by considering a toy network with detailed computations. A discussion on different performance evaluation metrics and, simulation models such as the SIR epidemic model, has been undertaken with a comparative analysis between different state-of-the-art on a few standard real networks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305054",
    "keywords": [
      "Artificial intelligence",
      "Betweenness centrality",
      "Biology",
      "Block (permutation group theory)",
      "Botany",
      "Centrality",
      "Closeness",
      "Combinatorics",
      "Complex network",
      "Computer network",
      "Computer science",
      "Data mining",
      "Engineering",
      "Focus (optics)",
      "Geometry",
      "Heuristic",
      "Identification (biology)",
      "Mathematical analysis",
      "Mathematics",
      "Network topology",
      "Node (physics)",
      "Optics",
      "Physics",
      "Social media",
      "Structural engineering",
      "Theoretical computer science",
      "Topology (electrical circuits)",
      "Viral marketing",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Maji",
        "given_name": "Giridhar"
      },
      {
        "surname": "Mandal",
        "given_name": "Sharmistha"
      },
      {
        "surname": "Sen",
        "given_name": "Soumya"
      }
    ]
  },
  {
    "title": "Inference on historical factions based on multi-layered network of historical figures",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113703",
    "abstract": "With immense influx of historical data, quantitative inferences on history based on machine learning is becoming more prevalent, attracting many researchers. In particular, understanding the dynamics of historical factions is important as they shared academic beliefs, political views and interests, in which the interactions between the factions portray general political, social, and economic structure of a certain era. In recent years, studying such dynamics through network-based methods on human networks, constructed from genealogy data, have shown promising results. In this paper, we enhance the identification of historical factions by exploiting multi-layered network of historical figures. To understand the mechanisms of historical factions, it is pivotal to comprehend the change in relation between important historical events. The proposed method consists of constructing a multi-layered network of historical figures and applying semi-supervised learning framework to identify historical factions. The proposed method was applied to the classification of factions in the political turmoil occurred during the 15th to 16th century Korea.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305273",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Comparative historical research",
      "Computer science",
      "Data science",
      "Epistemology",
      "Identification (biology)",
      "Inference",
      "Law",
      "Philosophy",
      "Political science",
      "Politics",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Kim",
        "given_name": "Myungjun"
      },
      {
        "surname": "Lee",
        "given_name": "Dong-gi"
      },
      {
        "surname": "Lee",
        "given_name": "Sangkuk"
      },
      {
        "surname": "Lee",
        "given_name": "Geun-ho"
      },
      {
        "surname": "Shin",
        "given_name": "Hyunjung"
      }
    ]
  },
  {
    "title": "Combination of fuzzy and cognitive theories for adaptive e-assessment",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113614",
    "abstract": "A crucial factor for successful educational results in computer-based educational systems and e-learning systems is the learner’s assessment. Assessment is more effective when it is tailored to each individual student’s learning needs and abilities. Therefore, a significant research challenge is to create tests that include exercises/questions/activities etc., which conform to each learner’s knowledge level and learning needs and abilities. This goal constitutes the need for creating adaptive tests. However, the area of adaptive e-assessment has not yet been explored sufficiently and thus there is scope for a lot of improvement. To this end, in this paper we present a novel solution for adaptive e-assessment. The novelty and significance lie in the blending of fuzzy logic and cognitive theories for further enhancing the personalization and adaptivity in e-assessment. Particularly, fuzzy sets are used to describe the knowledge level of students in a more realistic way. Furthermore, the cognitive theory of Revised Bloom Taxonomy is used to express the learning objectives that are required to be assessed through the created test. In addition, a fuzzy rule-based reasoner, which decides about the number and the difficulty level of the test items that have to be included into the created personalized test for each level of the Revised Bloom Taxonomy, is used. The fuzzy rules are applied to the fuzzy sets that describe the learners’ knowledge level. For the formation of the fuzzy sets and rules, the opinion of several tutors, holding experience in the educational process and instruction, was taken into consideration. The created adaptive test comprises distinct test items based on the individual learning needs of each student. The presented method has been used in two tutoring systems and has been fully evaluated. The evaluation results show great accuracy in the selection of test items for each individual student.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420304383",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Bloom's taxonomy",
      "Botany",
      "Clinical psychology",
      "Cognition",
      "Computer science",
      "Computerized adaptive testing",
      "Fuzzy logic",
      "Machine learning",
      "Neuroscience",
      "Novelty",
      "Personalization",
      "Psychology",
      "Psychometrics",
      "Semantic reasoner",
      "Social psychology",
      "Taxonomy (biology)",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Chrysafiadi",
        "given_name": "Konstantina"
      },
      {
        "surname": "Troussas",
        "given_name": "Christos"
      },
      {
        "surname": "Virvou",
        "given_name": "Maria"
      }
    ]
  },
  {
    "title": "Remanufacturing in a competitive market: A closed-loop supply chain in a Stackelberg game framework",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113655",
    "abstract": "In this work, we study a closed-loop supply chain with remanufacturing in a competitive market, where the supply chain is a price taker. A Stackelberg game framework is considered, where a manufacturer (leader) has sufficient channel power over a retailer (follower). We develop analytical models to show that a closed-loop supply chain with remanufacturing in a competitive market can achieve the same return rate as that in the centrally coordinated channel by employing a contract between the manufacturer and the retailer. The contract consists of a wholesale price, and a progressive transfer price scheme with additional allowances (or charges) for the returned products. Our models also take into account the effect of green initiatives on consumers’ purchase intention by relating the green achievement from remanufacturing to consumer demand. In practice, our models can be directly applied to a closed-loop supply chain with remanufacturing in a competitive market.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420304796",
    "keywords": [
      "Business",
      "Channel (broadcasting)",
      "Closed loop",
      "Competitive advantage",
      "Computer science",
      "Control engineering",
      "Economics",
      "Engineering",
      "Game theory",
      "Industrial organization",
      "Manufacturing engineering",
      "Marketing",
      "Microeconomics",
      "Remanufacturing",
      "Stackelberg competition",
      "Supply chain",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Tang",
        "given_name": "Shaolong"
      },
      {
        "surname": "Wang",
        "given_name": "Wenjie"
      },
      {
        "surname": "Zhou",
        "given_name": "Gaoguang"
      }
    ]
  },
  {
    "title": "Heap-based optimizer inspired by corporate rank hierarchy for global optimization",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113702",
    "abstract": "In an organization, a group of people working for a common goal may not achieve their goal unless they organize themselves in a hierarchy called Corporate Rank Hierarchy (CRH). This principle motivates us to map the concept of CRH to propose a new algorithm for optimization that logically arranges the search agents in a hierarchy based on their fitness. The proposed algorithm is named as heap-based optimizer (HBO) because it utilizes the heap data structure to map the concept of CRH. The mathematical model of HBO is built on three pillars: the interaction between the subordinates and their immediate boss, the interaction between the colleagues, and self-contribution of the employees. The proposed algorithm is benchmarked with 97 diverse test functions including 29 CEC-BC-2017 functions with very challenging landscapes against 7 highly-cited optimization algorithms including the winner of CEC-BC-2017 (EBO-CMAR). In the first two experiments, the exploitative and explorative behavior of HBO is evaluated by using 24 unimodal and 44 multimodal functions, respectively. It is shown through experiments and Friedman mean rank test that HBO outperforms and secures 1st rank. In the third experiment, we use 29 CEC-BC-2017 benchmark functions. According to Friedman mean rank test HBO attains 2nd position after EBO-CMAR; however, the difference in ranks of HBO and EBO-CMAR is shown to be statistically insignificant by using Bonferroni method based multiple comparison test. Moreover, it is shown through the Friedman test that the overall rank of HBO is 1st for all 97 benchmarks. In the fourth and the last experiment, the applicability on real-world problems is demonstrated by solving 3 constrained mechanical engineering optimization problems. The performance is shown to be superior or equivalent to the other algorithms, which have been used in the literature. The source code of HBO is publicly available at https://github.com/qamar-askari/HBO.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305261",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Combinatorics",
      "Computer science",
      "Friedman test",
      "Geodesy",
      "Geography",
      "Heap (data structure)",
      "Hierarchy",
      "Law",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Paleontology",
      "Political science",
      "Rank (graph theory)",
      "Statistical hypothesis testing",
      "Statistics",
      "Test (biology)"
    ],
    "authors": [
      {
        "surname": "Askari",
        "given_name": "Qamar"
      },
      {
        "surname": "Saeed",
        "given_name": "Mehreen"
      },
      {
        "surname": "Younas",
        "given_name": "Irfan"
      }
    ]
  },
  {
    "title": "Latent state recognition by an enhanced hidden Markov model",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113722",
    "abstract": "In this paper, we start from relaxing assumptions of traditional hidden Markov model then develop a novel framework for decoding the latent states, from which the dynamics of multi-variable financial data is generated. To construct the framework, we model the observed variables as a p-order vector autoregressive process, allow the latent state to evolve through a semi-Markov chain, and shrink the auto-regression and covariance matrices via a penalized maximization likelihood method. Using the 50-dimensional simulated data, the 12-dimensional 5-min order book data of the Chinese CSI 300 index component stocks, the 49-dimensional daily data of U.S. industry portfolio, and 1-dimensional hourly data of four primary foreign exchange rates, our empirical analyses show that the proposed model outperforms the alternative model in accurately recognizing anomalous events and achieves better sharp ratio in a pseudo trading strategy via the latent states. The superior performance is across the data frequency of minute, hour and daily, the dimension of one, twelve, and fifty, the data type of stock, foreign exchange rate, and industry portfolio.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305467",
    "keywords": [
      "Artificial intelligence",
      "Autoregressive model",
      "Computer science",
      "Dimension (graph theory)",
      "Econometrics",
      "Economics",
      "Finance",
      "Hidden Markov model",
      "Hidden semi-Markov model",
      "Latent variable",
      "Machine learning",
      "Markov chain",
      "Markov model",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Portfolio",
      "Pure mathematics",
      "Variable-order Markov model"
    ],
    "authors": [
      {
        "surname": "Yao",
        "given_name": "Yuan"
      },
      {
        "surname": "Cao",
        "given_name": "Yi"
      },
      {
        "surname": "Zhai",
        "given_name": "Jia"
      },
      {
        "surname": "Liu",
        "given_name": "Junxiu"
      },
      {
        "surname": "Xiang",
        "given_name": "Mengyuan"
      },
      {
        "surname": "Wang",
        "given_name": "Lu"
      }
    ]
  },
  {
    "title": "Detecting and visualizing hate speech in social media: A cyber Watchdog for surveillance",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113725",
    "abstract": "The multi-fold growth of the social media user-base fuelled a substantial increase in the amount of hate speech posts on social media platforms. The enormous data volume makes it hard to capture such cases and either moderate or delete them. This paper presents an approach to detect and visualize online aggression, a special case of hate speech, over social media. Aggression is categorized into overtly aggressive (OAG), covertly aggressive (CAG), and non-aggressive labels (NAG). We have designed a user interface based on a web browser plugin over Facebook and Twitter to visualize the aggressive comments posted on the Social media user’s timelines. This plugin interface might help to the security agency to keep a tab on the social media stream. It also provides citizens with a tool that is typically only available for large enterprises. The availability of such a tool alleviates the technological imbalance between industry and citizens. Besides, the system might be helpful to the research community to create further tools and prepare weakly labeled training data in a few minutes using comments posted by users on celebrity’s Facebook, Twitter timeline. We have reported the results on a newly created dataset of user comments posted on Facebook and Twitter using our proposed plugins and the standard Trolling Aggression Cyberbullying 2018 (TRAC) dataset in English and code-mixed Hindi. Various classifiers like Support Vector Machine (SVM), Logistic regression, deep learning model based on Convolution Neural Network (CNN), Attention-based model, and the recently proposed BERT pre-trained language model by Google AI, have been used for aggression classification. The weighted F1-score of around 0.64 and 0.62 is achieved on TRAC Facebook English and Hindi datasets while on Twitter English and Hindi datasets, the weighted F1-score is 0.58 and 0.50, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305492",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Convolutional neural network",
      "History",
      "Internet privacy",
      "Machine learning",
      "Plug-in",
      "Programming language",
      "Social media",
      "Support vector machine",
      "Timeline",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Modha",
        "given_name": "Sandip"
      },
      {
        "surname": "Majumder",
        "given_name": "Prasenjit"
      },
      {
        "surname": "Mandl",
        "given_name": "Thomas"
      },
      {
        "surname": "Mandalia",
        "given_name": "Chintak"
      }
    ]
  },
  {
    "title": "Robust matching cost function based on evolutionary approach",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113712",
    "abstract": "This paper proposes a novel stereo matching method with a matching cost function learned from training data. Because the cost function includes a considerably large number of parameters required to select their values, it is nearly impossible to manually select the values. We employ an evolutionary algorithm to automatically optimize the parameter values for each dataset. Using Middlebury, KITTI 2012, and KITTI 2015 dataset, we compare the proposed stereo matching method with state-of-the-art stereo matching methods that can achieve real-time computation. Experimental results show that the proposed method outperforms the real-time state-of-the-art stereo matching methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305364",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Computation",
      "Computer science",
      "Computer vision",
      "Evolutionary biology",
      "Function (biology)",
      "Machine learning",
      "Matching (statistics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "State (computer science)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Hong",
        "given_name": "Phuc Nguyen"
      },
      {
        "surname": "Ahn",
        "given_name": "Chang Wook"
      }
    ]
  },
  {
    "title": "Succinct contrast sets via false positive controlling with an application in clinical process redesign",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113670",
    "abstract": "Many applications of intelligent systems involve understanding a group of contrastively different outcome (e.g., all survivors of a deadly cancer, a top performing team in a large corporation). The intelligent system needs to identify attributes (features) which best describe or explain the group versus its alternatives. In data mining, this problem is studied under the framework of contrast set mining (CSM). Although CSM is not new, the era of big data has produced new computational and statistical challenges. In particular, existing algorithms fail (1) to perform efficiently in terms of runtime on large-scale datasets and (2) to accommodate simultaneous inference on an overwhelming array of features which are often repetitive and collinear. In this paper, we develop a CSM algorithm which addresses both challenges. The computational challenge is addressed with a tree structure and two theorems while the statistical challenge is addressed with the application of false discovery rate for multiple testing. The computational and statistical advantages of the proposed algorithm over three state-of-the-art algorithms are demonstrated with comprehensive experiments. In addition, we also show the effectiveness of our proposed method in an intelligence-system application involving hospital process redesign. The proposed method not only improves the performance of machine learning systems, but also generates succinct and insightful patterns directly relevant to clinical decision-making.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420304942",
    "keywords": [
      "Artificial intelligence",
      "Big data",
      "Computer science",
      "Contrast (vision)",
      "Data mining",
      "Decision tree",
      "Inference",
      "Machine learning",
      "Mathematical economics",
      "Mathematics",
      "Operating system",
      "Outcome (game theory)",
      "Process (computing)",
      "Programming language",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Nguyen",
        "given_name": "Dang"
      },
      {
        "surname": "Luo",
        "given_name": "Wei"
      },
      {
        "surname": "Vo",
        "given_name": "Bay"
      },
      {
        "surname": "Pedrycz",
        "given_name": "Witold"
      }
    ]
  },
  {
    "title": "Detecting and visualizing hate speech in social media: A cyber Watchdog for surveillance",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113725",
    "abstract": "The multi-fold growth of the social media user-base fuelled a substantial increase in the amount of hate speech posts on social media platforms. The enormous data volume makes it hard to capture such cases and either moderate or delete them. This paper presents an approach to detect and visualize online aggression, a special case of hate speech, over social media. Aggression is categorized into overtly aggressive (OAG), covertly aggressive (CAG), and non-aggressive labels (NAG). We have designed a user interface based on a web browser plugin over Facebook and Twitter to visualize the aggressive comments posted on the Social media user’s timelines. This plugin interface might help to the security agency to keep a tab on the social media stream. It also provides citizens with a tool that is typically only available for large enterprises. The availability of such a tool alleviates the technological imbalance between industry and citizens. Besides, the system might be helpful to the research community to create further tools and prepare weakly labeled training data in a few minutes using comments posted by users on celebrity’s Facebook, Twitter timeline. We have reported the results on a newly created dataset of user comments posted on Facebook and Twitter using our proposed plugins and the standard Trolling Aggression Cyberbullying 2018 (TRAC) dataset in English and code-mixed Hindi. Various classifiers like Support Vector Machine (SVM), Logistic regression, deep learning model based on Convolution Neural Network (CNN), Attention-based model, and the recently proposed BERT pre-trained language model by Google AI, have been used for aggression classification. The weighted F1-score of around 0.64 and 0.62 is achieved on TRAC Facebook English and Hindi datasets while on Twitter English and Hindi datasets, the weighted F1-score is 0.58 and 0.50, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305492",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Convolutional neural network",
      "History",
      "Internet privacy",
      "Machine learning",
      "Plug-in",
      "Programming language",
      "Social media",
      "Support vector machine",
      "Timeline",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Modha",
        "given_name": "Sandip"
      },
      {
        "surname": "Majumder",
        "given_name": "Prasenjit"
      },
      {
        "surname": "Mandl",
        "given_name": "Thomas"
      },
      {
        "surname": "Mandalia",
        "given_name": "Chintak"
      }
    ]
  },
  {
    "title": "ReEx: An integrated architecture for preference model representation and explanation",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113706",
    "abstract": "Recommender systems based on collaborative filtering suggest items to users according to the similarity of items or similarity of preferences of other users. Latent factor models produce rather accurate predictions of user preferences, but the latency of the features extracted make it difficult to substantiate a recommendation to a user. The realisation that many aspects tend to exist in rating and social connections data, such as social influence or bias, led to the development of a component-based matrix factorisation approach in earlier work. The ability to quantify the contributions of a component to a recommendation opens up possibilities to identify reasons for recommendations which can be presented to the consumer receiving them. Reviews that accompany rating data can be analysed and correlated with latent factors to provide more detailed reasons for recommendations. This paper introduces a general comprehensive framework which supplements earlier work that models a users’ preferences and makes recommendations by extracting reasons from the recommendation framework as well as the accompanying reviews. Both rating-based and review-based reasons for recommending an item are presented in a visual manner. These visualisations help the user understand the origins of the recommendations but also assist businesses in identifying properties in users and communities that open up opportunities for targeted marketing and customer management. The usefulness of the explanation tool is demonstrated with an example recommendation of four items for a user in the Yelp restaurants dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305303",
    "keywords": [
      "Artificial intelligence",
      "Collaborative filtering",
      "Component (thermodynamics)",
      "Computer science",
      "Data science",
      "Economics",
      "Image (mathematics)",
      "Information retrieval",
      "Law",
      "Microeconomics",
      "Physics",
      "Political science",
      "Politics",
      "Preference",
      "Quantum mechanics",
      "Realisation",
      "Recommender system",
      "Representation (politics)",
      "Similarity (geometry)",
      "Thermodynamics"
    ],
    "authors": [
      {
        "surname": "Zafari",
        "given_name": "Farhad"
      },
      {
        "surname": "Moser",
        "given_name": "Irene"
      },
      {
        "surname": "Sellis",
        "given_name": "Timos"
      }
    ]
  },
  {
    "title": "Dominant point detection based on suboptimal feature selection methods",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113741",
    "abstract": "This paper presents a viable alternative solution for dominant point detection predicated on the comparison of suboptimal feature selection methods. Suboptimal feature selection methods are utilized as standard criteria to identify dominant points. Considering that all of the combinations of points comprise many sets, an algorithm that eliminates some of them is affirmed and illustrated. The sequential backward selection, sequential forward selection, generalized sequential forward selection, generalized sequential backward selection and plus l-take away r selection methods are performed on the remaining points to extract the dominant points. The simulation results exhibit that this method is significantly more effective and efficient in comparison to other proposed methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305650",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Feature (linguistics)",
      "Feature selection",
      "Geometry",
      "Linguistics",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Point (geometry)",
      "Selection (genetic algorithm)"
    ],
    "authors": [
      {
        "surname": "Işık",
        "given_name": "Şahin"
      }
    ]
  },
  {
    "title": "Adaptive boost LS-SVM classification approach for time-series signal classification in epileptic seizure diagnosis applications",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113676",
    "abstract": "Epileptic seizures are characterised by abnormal neuronal discharge, causing notable disturbances in electrical activities of the human brain. Traditional methods based on manual approaches applied in seizure detection in electroencephalograms (EEG) have drawbacks (e.g., time constraint, lack of effective feature identification relative to disease symptoms and susceptibility to human errors) that can lead to inadequate treatment options. Designing an automated expert system to detect epileptic seizures can proactively support a neurologist’s effort to improve authenticity, speed and accuracy of detecting signs of a seizure. We propose a novel two-phase EEG classification technique to detect seizures from EEG by employing covariance matrix coupled with Adaptive Boosting Least Square-Support Vector Machine (i.e., AdaBoost LS-SVM) framework. In first phase, the covariance matrix is employed as a dimensionality reduction tool with feature extraction applied to analyse epileptic patients’ EEG records. Initially, each single EEG channel is partitioned into respective k segment with m clusters. Subsequently, covariance matrix is adopted with eigenvalues of each cluster extracted and tested through statistical metrics to identify the most representative, optimally classified features. In the second phase, a robust classifier (i.e., AB-LS-SVM) is proposed to resolve issues of unbalanced data, to detect epileptic events, yielding a high classification accuracy compared to its competing counterparts. The results demonstrates that AB-LS-SVM (optimised by a covariance matrix) is able to achieve satisfactory results (>99% accuracy) for eleven prominent features in EEG signals. The results are compared with state-of-art algorithms (i.e., k-means, SVM, k-nearest neighbour, Random Forest) on identical databases, demonstrating the capability of AB-LS-SVM method as a promising diagnostic tool and its practicality for implementation in seizure detection. The study avers that the proposed approach can aid clinicians in diagnosis or interventions to treat epileptic disease, including a potential use in expert systems where EEG needs to be classified through pattern recognition.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305005",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Covariance matrix",
      "Electroencephalography",
      "Epileptic seizure",
      "Feature extraction",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Psychology",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Al-Hadeethi",
        "given_name": "Hanan"
      },
      {
        "surname": "Abdulla",
        "given_name": "Shahab"
      },
      {
        "surname": "Diykh",
        "given_name": "Mohammed"
      },
      {
        "surname": "Deo",
        "given_name": "Ravinesh C."
      },
      {
        "surname": "Green",
        "given_name": "Jonathan H"
      }
    ]
  },
  {
    "title": "A binary social spider algorithm for uncapacitated facility location problem",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113618",
    "abstract": "In order to find efficient solutions to real complex world problems, computer sciences and especially heuristic algorithms are often used. Heuristic algorithms can give optimal solutions for large scale optimization problems in an acceptable period. Social Spider Algorithm (SSA), which is a heuristic algorithm created on spider behaviors are studied. The original study of this algorithm was proposed to solve continuous problems. In this paper, the binary version of the Social Spider Algorithm called Binary Social Spider Algorithm (BinSSA) is proposed for binary optimization problems. BinSSA is obtained from SSA, by transforming constant search space to binary search space with four transfer functions. Thus, BinSSA variations are created as BinSSA1, BinSSA2, BinSSA3, and BinSSA4. The study steps of the original SSA are re-updated for BinSSA. A random walking schema in SSA is replaced by a candidate solution schema in BinSSA. Two new methods (similarity measure and logic gate) are used in candidate solution production schema for increasing the exploration and exploitation capacity of BinSSA. The performance of both techniques on BinSSA is examined. BinSSA is named as BinSSA(Sim&Logic). Local search and global search performance of BinSSA is increased by these two methods. Three different studies are performed with BinSSA. In the first study, the performance of BinSSA is tested on the classic eighteen unimodal and multimodal benchmark functions. Thus, the best variation of BinSSA and BinSSA(Sim&Logic) is determined as BinSSA4(Sim&Logic). BinSSA4(Sim&Logic) has been compared with other heuristic algorithms on CEC2005 and CEC2015 functions. In the second study, the uncapacitated facility location problems (UFLPs) are solved with BinSSA(Sim&Logic). UFL problems are one of the pure binary optimization problems. BinSSA is tested on low-scaled, middle-scaled, and large-scaled fifteen UFLP samples and obtained results are compared with eighteen state-of-art algorithms. In the third study, we solved UFL problems on a different dataset named M* with BinSSA(Sim&Logic). The results of BinSSA(Sim&Logic) are compared with the Local Search (LS), Tabu Search (TS), and Improved Scatter Search (ISS) algorithms. Obtained results have shown that BinSSA offers quality and stable solutions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420304425",
    "keywords": [
      "Algorithm",
      "Arithmetic",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Binary number",
      "Binary search algorithm",
      "Computer science",
      "Geodesy",
      "Geography",
      "Heuristic",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Schema (genetic algorithms)",
      "Search algorithm",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Baş",
        "given_name": "Emine"
      },
      {
        "surname": "Ülker",
        "given_name": "Erkan"
      }
    ]
  },
  {
    "title": "Bayesian networks for supply chain risk, resilience and ripple effect analysis: A literature review",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113649",
    "abstract": "In the broad sense, the Bayesian networks (BN) are probabilistic graphical models that possess unique methodical features to model dependencies in complex networks, such as forward and backward propagation (inference) of disruptions. BNs have transitioned from an emerging topic to a growing research area in supply chain (SC) resilience and risk analysis. As a result, there is an acute need to review existing literature to ascertain recent developments and uncover future areas of research. Despite the increasing number of publications on BNs in the domain of SC uncertainty, an extensive review on their application to SC risk and resilience is lacking. To address this gap, we analyzed research articles published in peer-reviewed academic journals from 2007 to 2019 using network analysis, visualization-based scientometric analysis, and clustering analysis. Through this study, we contribute to literature by discussing the challenges of current research, and, more importantly, identifying and proposing future research directions. The results of our survey show that further debate on the theory and application of BNs to SC resilience and risk management is a significant area of interest for both academics and practitioners. The applications of BNs, and their conjunction with machine learning algorithms to solve big data SC problems relating to uncertainty and risk, are also discussed.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420304735",
    "keywords": [
      "Artificial intelligence",
      "Bayesian inference",
      "Bayesian network",
      "Bayesian probability",
      "Cluster analysis",
      "Computer science",
      "Data science",
      "Graphical model",
      "Inference",
      "Machine learning",
      "Medicine",
      "Physics",
      "Probabilistic logic",
      "Resilience (materials science)",
      "Risk analysis (engineering)",
      "Thermodynamics"
    ],
    "authors": [
      {
        "surname": "Hosseini",
        "given_name": "Seyedmohsen"
      },
      {
        "surname": "Ivanov",
        "given_name": "Dmitry"
      }
    ]
  },
  {
    "title": "Genetic state-grouping algorithm for deep reinforcement learning",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113695",
    "abstract": "Although Reinforcement learning has already been considered one of the most important and well-known techniques of machine learning, its applicability remains limited in the real-world problems due to its long initial learning time and unstable learning. Especially, the problem of an overwhelming number of the branching factors under real-time constraint still stays unconquered, demanding a new method for the next generation of reinforcement learning. In this paper, we propose Genetic State-Grouping Algorithm based on deep reinforcement learning. The core idea is to divide the entire set of states into a few state groups. Each group consists of states that are mutually similar, thus representing their common features. The state groups are then processed with the Genetic Optimizer, which finds outstanding actions. These steps help the Deep Q Network avoid excessive exploration, thereby contributing to the significant reduction of initial learning time. The experiment on the real-time fighting video game (FightingICE) shows the effectiveness of our proposed approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305194",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Constraint (computer-aided design)",
      "Core (optical fiber)",
      "Deep learning",
      "Genetic algorithm",
      "Geometry",
      "Learning classifier system",
      "Machine learning",
      "Mathematics",
      "Programming language",
      "Q-learning",
      "Reinforcement learning",
      "Set (abstract data type)",
      "State (computer science)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Kim",
        "given_name": "Man-Je"
      },
      {
        "surname": "Kim",
        "given_name": "Jun Suk"
      },
      {
        "surname": "Kim",
        "given_name": "Sungjin James"
      },
      {
        "surname": "Kim",
        "given_name": "Min-jung"
      },
      {
        "surname": "Ahn",
        "given_name": "Chang Wook"
      }
    ]
  },
  {
    "title": "Performance improvement strategies on Cuckoo Search algorithms for solving the university course timetabling problem",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113732",
    "abstract": "The university course timetabling problem (UCTP) arises every academic year and must be solved by academic staff with/without a course timetabling tool. A Hybrid Self-adaptive Cuckoo Search-based Timetabling (HSCST) tool has been developed for minimising the total university operating costs. The HSCST tool was applied to solve eleven problem instances obtained from the Faculty of Engineering, Naresuan University. The performance improvements of the Cuckoo Search (CS) algorithm embedded within the proposed tool were demonstrated using three strategies: parameter setting approaches (static and adaptive), movement strategies (Lévy flights and Gaussian random walks), and local search hybridisation techniques. Sequential computational experiments were designed and conducted to investigate the efficiency of the three proposed strategies. The statistical analysis on the computational results suggested that the proposed algorithms significantly outperformed the conventional CS, Particle Swarm Optimisation (PSO), and hybrid PSO for all problem instances.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742030556X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Astronomy",
      "Computer science",
      "Constraint satisfaction problem",
      "Course (navigation)",
      "Cuckoo search",
      "Gaussian",
      "Hybrid algorithm (constraint satisfaction)",
      "Local consistency",
      "Local search (optimization)",
      "Lévy flight",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Particle swarm optimization",
      "Physics",
      "Probabilistic logic",
      "Quantum mechanics",
      "Random walk",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Thepphakorn",
        "given_name": "Thatchai"
      },
      {
        "surname": "Pongcharoen",
        "given_name": "Pupong"
      }
    ]
  },
  {
    "title": "Knowledge-based framework for estimating the relevance of scientific articles",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113692",
    "abstract": "The volume of published papers provided by the scientific community has increased over the last years in a drastic way. This fact has led to having a considerable growth of the topics covered by different publications. Despite topics under discussion on these publications were usually regarded as cutting edge subjects when released in conferences and journals, the restless evolution of science may have faded their relative importance away over the years. This issue undoubtedly poses big challenges to those researchers interested in gathering information to enrich their own background. Consequently, the development of a system able to automatically organize and provide relevance to scientific papers should play a crucial role to address the aforementioned problem. In this paper, the Webelance framework is presented. It makes use of a lexicon and Machine Learning techniques to accomplish these tasks. It has been built by using specific metrics for the scientific domain to measure the relative importance of papers. Several experiments using more than 50 , 000 articles focused on the medicine domain have been addressed to illustrate the viability of the proposal. The obtained results both confirm the usability of the system and its good performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305169",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data science",
      "Domain (mathematical analysis)",
      "Human–computer interaction",
      "Information retrieval",
      "Law",
      "Lexicon",
      "Mathematical analysis",
      "Mathematics",
      "Political science",
      "Relevance (law)",
      "Usability"
    ],
    "authors": [
      {
        "surname": "Fernández-Isabel",
        "given_name": "Alberto"
      },
      {
        "surname": "A. Barriuso",
        "given_name": "Adrián"
      },
      {
        "surname": "Cabezas",
        "given_name": "Javier"
      },
      {
        "surname": "Martín de Diego",
        "given_name": "Isaac"
      },
      {
        "surname": "J. Viseu Pinheiro",
        "given_name": "J.F."
      }
    ]
  },
  {
    "title": "Inverse data envelopment analysis for operational planning: The impact of oil price shocks on the production frontier",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113726",
    "abstract": "Inverse data envelopment analysis (DEA) is a useful planning tool, especially when it is combined with frontier changes that accurately reflect reality. This paper proposes an inverse optimization model for operational planning by taking into account frontier changes in conjunction with environmental factors. The aim is not only to present a computational procedure of a new measure to properly capture the effective frontier changes, but more importantly to demonstrate how frontier changes observed in the past can be utilized to provide insights into estimation of the future production frontier. In essence, the proposed model is intended to help establish realistic goals for operational planning practices. The model is applied to the Korean natural gas industry as an empirical demonstration.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305509",
    "keywords": [
      "Archaeology",
      "Computer science",
      "Data envelopment analysis",
      "Econometrics",
      "Economics",
      "Frontier",
      "Geometry",
      "History",
      "Inverse",
      "Mathematical optimization",
      "Mathematics",
      "Microeconomics",
      "Monetary economics",
      "Oil price",
      "Operations research",
      "Production (economics)",
      "Production planning",
      "Production–possibility frontier"
    ],
    "authors": [
      {
        "surname": "Lim",
        "given_name": "Dong-Joon"
      }
    ]
  },
  {
    "title": "An effective discrete artificial bee colony algorithm for multi-AGVs dispatching problem in a matrix manufacturing workshop",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113675",
    "abstract": "This paper addresses a new multiple automatic guided vehicle dispatching problem (AGVDP) from material handling process in a matrix manufacturing workshop. The problem aims to determine a solution with the objective of minimizing the transportation cost including travel cost, penalty cost for violating time and AGV cost. For this purpose, a mixed integer linear programming model is first formulated based on a comprehensive investigation. Then, a discrete artificial bee colony algorithm (DABC) is presented together with some novel and advanced techniques for solving the problem. In the proposed DABC algorithm, a nearest-neighbor-based heuristic based on the problem-specific characteristics is presented to generate an initial solution with a high level of quality. Five effective neighborhood operators are presented to generated neighboring solutions with a high level of diversity. Four theorems are proposed to avoid the unfeasible solutions generated by the neighborhood operators. Two new control parameters are introduced. One is to balance the global exploration and local exploitation in employed bee and onlooker bee phases. The other is to enhance the local exploitation capability of the neighborhood operators. Besides, an insertion-based local search method is provided for the scout bee phase to lead the algorithm to a promising region of the solution space. A comprehensive and thorough evaluation with 110 instances collected from a real-world factory shows that the presented algorithm produces superior results which are also demonstrated to be statistically significant than the existing algorithms in the close related literature.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420304991",
    "keywords": [
      "Algorithm",
      "Artificial bee colony algorithm",
      "Artificial intelligence",
      "Composite material",
      "Computer science",
      "Engineering",
      "Epistemology",
      "Factory (object-oriented programming)",
      "Heuristic",
      "Industrial engineering",
      "Local search (optimization)",
      "Material handling",
      "Materials science",
      "Mathematical optimization",
      "Mathematics",
      "Matrix (chemical analysis)",
      "Operating system",
      "Philosophy",
      "Process (computing)",
      "Programming language",
      "Quality (philosophy)"
    ],
    "authors": [
      {
        "surname": "Zou",
        "given_name": "Wen-Qiang"
      },
      {
        "surname": "Pan",
        "given_name": "Quan-Ke"
      },
      {
        "surname": "Meng",
        "given_name": "Tao"
      },
      {
        "surname": "Gao",
        "given_name": "Liang"
      },
      {
        "surname": "Wang",
        "given_name": "Yu-Long"
      }
    ]
  },
  {
    "title": "Ensemble topic modeling using weighted term co-associations",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113709",
    "abstract": "Topic modeling is a popular unsupervised technique that is used to discover the latent thematic structure in text corpora. The evaluation of topic models typically involves measuring the semantic coherence of the terms describing each topic, where a single value is used to summarize the quality of an overall model. However, this can create difficulties when one seeks to interpret the strengths and weaknesses of a given topic model. With this in mind, we propose a new ensemble topic modeling approach that incorporates both stability information, in the form of term co-associations, and semantic similarity information, as derived from a word embedding constructed on a background corpus. Our evaluations show that this approach can simultaneously yield higher quality models when considering the produced topic descriptors and document-topic assignments, while also facilitating the comparison and evaluation of solutions through the visualization of the discovered topical structure, the ordering of the topic descriptors, and the ranking of term pairs which appear in topic descriptors.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305339",
    "keywords": [
      "Artificial intelligence",
      "Coherence (philosophical gambling strategy)",
      "Computer science",
      "Epistemology",
      "Geometry",
      "Image (mathematics)",
      "Information retrieval",
      "Mathematics",
      "Natural language processing",
      "Philosophy",
      "Physics",
      "Programming language",
      "Quantum mechanics",
      "Ranking (information retrieval)",
      "Similarity (geometry)",
      "Statistics",
      "Strengths and weaknesses",
      "Term (time)",
      "Thematic structure",
      "Topic model",
      "Visualization",
      "Word (group theory)"
    ],
    "authors": [
      {
        "surname": "Belford",
        "given_name": "Mark"
      },
      {
        "surname": "Greene",
        "given_name": "Derek"
      }
    ]
  },
  {
    "title": "The Time-dependent Electric Vehicle Routing Problem: Model and solution",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113593",
    "abstract": "We study a new problem named the Time-dependent Electric Vehicle Routing Problem (TDEVRP) which involves routing a fleet of electric vehicles to serve a set of customers and determining the vehicle’s speed and departure time at each arc of the routes with the purpose of minimizing a cost function. We propose an integer linear programming (ILP) model to formulate the TDEVRP and show that the state-of-the-art commercial optimizer (CPLEX) can only solve instances of very limited sizes (with no more than 15 customers). We thus propose an iterated variable neighbourhood search (IVNS) algorithm to find near-optimal solutions for larger instances. The key ingredients of IVNS include a fast evaluation method that allows local search moves to be evaluated in constant time O ( 1 ) , a variable neighbourhood descent (VND) procedure to optimize the node sequences, and a departure time and speed optimization procedure(DSOP) to optimize the speed and departure time on each arc of the routes. The proposed algorithm demonstrates excellent performances on a set of newly created instances. In particular, it can achieve optimal or near-optimal solutions for all small-size instances (with no more than 15 customers) and is robust for large-size instances where the gap between the average and the best solution value is consistently lower than 2.38%. Additional experimental results on 40 benchmark instances of the closely related Time-Dependent Pollution Routing Problem indicate that the proposed IVNS algorithm also performs very well and even discovers 39 new best-known solutions (improved upper bounds).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420304176",
    "keywords": [
      "Computer network",
      "Computer science",
      "Electric vehicle",
      "Mathematical optimization",
      "Mathematics",
      "Physics",
      "Power (physics)",
      "Quantum mechanics",
      "Routing (electronic design automation)",
      "Vehicle routing problem"
    ],
    "authors": [
      {
        "surname": "Lu",
        "given_name": "Ji"
      },
      {
        "surname": "Chen",
        "given_name": "Yuning"
      },
      {
        "surname": "Hao",
        "given_name": "Jin-Kao"
      },
      {
        "surname": "He",
        "given_name": "Renjie"
      }
    ]
  },
  {
    "title": "Self-supervised multimodal reconstruction of retinal images over paired datasets",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113674",
    "abstract": "Data scarcity represents an important constraint for the training of deep neural networks in medical imaging. Medical image labeling, especially if pixel-level annotations are required, is an expensive task that needs expert intervention and usually results in a reduced number of annotated samples. In contrast, extensive amounts of unlabeled data are produced in the daily clinical practice, including paired multimodal images from patients that were subjected to multiple imaging tests. This work proposes a novel self-supervised multimodal reconstruction task that takes advantage of this unlabeled multimodal data for learning about the domain without human supervision. Paired multimodal data is a rich source of clinical information that can be naturally exploited by trying to estimate one image modality from others. This multimodal reconstruction requires the recognition of domain-specific patterns that can be used to complement the training of image analysis tasks in the same domain for which annotated data is scarce. In this work, a set of experiments is performed using a multimodal setting of retinography and fluorescein angiography pairs that offer complementary information about the eye fundus. The evaluations performed on different public datasets, which include pathological and healthy data samples, demonstrate that a network trained for self-supervised multimodal reconstruction of angiography from retinography achieves unsupervised recognition of important retinal structures. These results indicate that the proposed self-supervised task provides relevant cues for image analysis tasks in the same domain.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742030498X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Data set",
      "Deep learning",
      "Domain (mathematical analysis)",
      "Economics",
      "Machine learning",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Medical imaging",
      "Modality (human–computer interaction)",
      "Pattern recognition (psychology)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Hervella",
        "given_name": "Álvaro S."
      },
      {
        "surname": "Rouco",
        "given_name": "José"
      },
      {
        "surname": "Novo",
        "given_name": "Jorge"
      },
      {
        "surname": "Ortega",
        "given_name": "Marcos"
      }
    ]
  },
  {
    "title": "Comparison of state-of-the-art deep learning APIs for image multi-label classification using semantic metrics",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113656",
    "abstract": "Image understanding heavily relies on accurate multi-label classification. In recent years, deep learning algorithms have become very successful for such tasks, and various commercial and open-source APIs have been released for public use. However, these APIs are often trained on different datasets, which, besides affecting their performance, might pose a challenge to their performance evaluation. This challenge concerns the different object-class dictionaries of the APIs’ training dataset and the benchmark dataset, in which the predicted labels are semantically similar to the benchmark labels but considered different simply because they have different wording in the dictionaries. To face this challenge, we propose semantic similarity metrics to obtain richer understating of the APIs predicted labels and thus their performance. In this study, we evaluate and compare the performance of 13 of the most prominent commercial and open-source APIs in a best-of-breed challenge on the Visual Genome and Open Images benchmark datasets. Our findings demonstrate that, while using traditional metrics, the Microsoft Computer Vision, Imagga, and IBM APIs performed better than others. However, applying semantic metrics also unveil the InceptionResNet-v2, Inception-v3, and ResNet50 APIs, which are trained only with the simple ImageNet dataset, as challengers for top semantic performers.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420304802",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Semantic similarity",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Kubany",
        "given_name": "Adam"
      },
      {
        "surname": "Ben Ishay",
        "given_name": "Shimon"
      },
      {
        "surname": "Ohayon",
        "given_name": "Ruben Sacha"
      },
      {
        "surname": "Shmilovici",
        "given_name": "Armin"
      },
      {
        "surname": "Rokach",
        "given_name": "Lior"
      },
      {
        "surname": "Doitshman",
        "given_name": "Tomer"
      }
    ]
  },
  {
    "title": "A density-based approach for querying informative constraints for clustering",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113690",
    "abstract": "During the last years, constrained clustering has emerged as an interesting direction in machine learning research. With constrained clustering, the quality of results can be improved by using constraints if a high-quality set of constraints is selected. Querying beneficial constraints is a challenging task because there is no metric for measuring the quality of constraints before clustering. A new method is proposed in this study that estimates density and impurity of data points on different adjacency distances and calculates centrality for each data point by applying a density tracking approach on the obtained densities. The obtained information is then used to select a set of high-quality constraints. Multi-resolution density analysis to more accurately estimate the point-point relationship of data, data density tracking in order to estimate the impurity and centrality of data, and selection of constraints from skeleton of clusters in order to discover the intrinsic structure of data can be mentioned as the most important contributions of this study. To verify the effectiveness of the proposed method, we conducted a series of experiments on real data sets. The obtained results show that the proposed algorithm can improve the clustering process compare with some recent reference algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305145",
    "keywords": [
      "Adjacency list",
      "Algorithm",
      "Artificial intelligence",
      "Centrality",
      "Cluster analysis",
      "Computer science",
      "Correlation clustering",
      "Data mining",
      "Data set",
      "Economics",
      "Mathematics",
      "Metric (unit)",
      "Operating system",
      "Operations management",
      "Process (computing)",
      "Programming language",
      "Set (abstract data type)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Abin",
        "given_name": "Ahmad Ali"
      },
      {
        "surname": "Vu",
        "given_name": "Viet-Vu"
      }
    ]
  },
  {
    "title": "A novel approach to predictive analysis using attribute-oriented rough fuzzy sets",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113644",
    "abstract": "In this study, a forecasting decision-making method is put forward to deal with multi-attribute decision-making problems. On the basis of rough set theory, (γ, δ)-rough fuzzy sets are presented using δ-clusters in data mining. Furthermore, several characteristics of the upper and lower (γ, δ)-approximations are obtained. Lastly, the difference between the fuzzy set A of the object and the upper and lower rough (γ, δ)-approximation operators on A is analyzed. We also design a novel algorithm to forecast decision making and provide a related example illustrating the new method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420304681",
    "keywords": [
      "Artificial intelligence",
      "Basis (linear algebra)",
      "Computer science",
      "Data mining",
      "Dominance-based rough set approach",
      "Fuzzy logic",
      "Fuzzy set",
      "Geometry",
      "Mathematics",
      "Object (grammar)",
      "Rough set"
    ],
    "authors": [
      {
        "surname": "Yu",
        "given_name": "Bin"
      },
      {
        "surname": "Cai",
        "given_name": "Mingjie"
      },
      {
        "surname": "Dai",
        "given_name": "Jianhua"
      },
      {
        "surname": "Li",
        "given_name": "Qingguo"
      }
    ]
  },
  {
    "title": "Saliency detection via multiple-morphological and superpixel based fast fuzzy C-mean clustering network",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113654",
    "abstract": "To model a human perception-based saliency detection algorithm in cluttered and noisy background images is a challenging problem in computer vision. Recently, many saliency detection algorithms have been proposed, which exploit the background information, or boundary priors of an image, to detect the salient object similar to the human attention system. These algorithms may not provide satisfying detection results for color images due to the assimilation of local spatial information. In this paper, we propose an unsupervised saliency detection technique, which uses multi-color space-based morphological gradient images. These gradient images contain different edge features, which are useful to obtain an accurate counter-based superpixel image containing both foreground and background clusters. A robust background measuring technique is implemented to remove background clusters, which describes the spatial information of an image cluster to image boundaries. This geometric clarification method effectively removes multiple low-level clues to produce a precise and uniform saliency map. These initially obtained saliency maps are fused using a multi-map fusion technique, and a compact saliency map prevails. The proposed algorithm is evaluated by executing different experiments on nine data sets. The results show that the proposed algorithm performs well for the detection of both single and multiple objects. The proposed algorithm is computationally efficient and provides better saliency detection results than state-of-the-art algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420304784",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Computer vision",
      "Image (mathematics)",
      "Kadir–Brady saliency detector",
      "Object detection",
      "Pattern recognition (psychology)",
      "Saliency map"
    ],
    "authors": [
      {
        "surname": "Nawaz",
        "given_name": "Mehmood"
      },
      {
        "surname": "Yan",
        "given_name": "Hong"
      }
    ]
  },
  {
    "title": "Search and rescue optimization algorithm: A new optimization method for solving constrained engineering optimization problems",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113698",
    "abstract": "A new optimization method namely the Search and Rescue optimization algorithm (SAR) is presented here to solve constrained engineering optimization problems. This metaheuristic algorithm imitates the explorations behavior of humans during search and rescue operations. The ε-constrained method is utilized as a constraint-handling technique. Besides, a restart strategy is proposed to avoid local infeasible minima in some complex constrained optimization problems. SAR is applied to solve 18 benchmark constraint functions presented in CEC 2010, 13 benchmark constraint functions, and 7 constrained engineering design problems reported in the specialized literature. The performance of SAR is compared with some state-of-the-art optimization algorithms. According to the statistical comparison results, the performance of SAR is better or highly competitive against the compared algorithms on most of the studied problems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305224",
    "keywords": [
      "Algorithm",
      "Benchmark (surveying)",
      "Computer science",
      "Constrained optimization",
      "Constrained optimization problem",
      "Constraint (computer-aided design)",
      "Continuous optimization",
      "Engineering optimization",
      "Geodesy",
      "Geography",
      "Geometry",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Maxima and minima",
      "Metaheuristic",
      "Multi-swarm optimization",
      "Optimization problem"
    ],
    "authors": [
      {
        "surname": "Shabani",
        "given_name": "Amir"
      },
      {
        "surname": "Asgarian",
        "given_name": "Behrouz"
      },
      {
        "surname": "Salido",
        "given_name": "Miguel"
      },
      {
        "surname": "Asil Gharebaghi",
        "given_name": "Saeed"
      }
    ]
  },
  {
    "title": "A knowledge-based approach to hierarchical classification: A voting metaphor",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113737",
    "abstract": "The paper proposes a new approach to hierarchical classification based on condition-action rules that represent expert knowledge in a given domain. The approach adopts a voting metaphor: each rule is regarded as a voter that expresses a preference for a given category to be assigned to an item to be classified; the category that receives more votes wins. Novel performance measures of hierarchical classifiers are also introduced that aim at overcoming the limitations of the current concepts of precision and recall. The proposed approach can be applied to any hierarchical classification task, for which expert knowledge is available. The viability of the approach and its performance are shown through a real-size application concerning the e-mail dispatching task inside a large public administration. The results obtained demonstrate that the proposed knowledge-based approach to hierarchical classification can reach a performance level comparable to that of human experts, if not even better.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305613",
    "keywords": [
      "Action (physics)",
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Domain (mathematical analysis)",
      "Domain knowledge",
      "Economics",
      "Law",
      "Machine learning",
      "Majority rule",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Physics",
      "Political science",
      "Politics",
      "Preference",
      "Quantum mechanics",
      "Statistics",
      "Task (project management)",
      "Voting"
    ],
    "authors": [
      {
        "surname": "Fogli",
        "given_name": "Daniela"
      },
      {
        "surname": "Guida",
        "given_name": "Giovanni"
      },
      {
        "surname": "Redolfi",
        "given_name": "Massimiliano"
      },
      {
        "surname": "Tonoli",
        "given_name": "Rossana"
      }
    ]
  },
  {
    "title": "Research on an advanced intelligence implementation system for engineering process in industrial field under big data",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113751",
    "abstract": "To develop an advanced CBR system to well adapt to the intelligence implementation of new engineering process in the big data environment, Bayesian network (BN) model is introduced to CBR system for knowledge reasoning. However, as engineering application is becoming more and more complicated, the number of parameters used to define engineering application grows larger and larger, leading to the seriously reduced efficiency as well as the accuracy of the integrated model. For the problem of reduced efficiency, this paper proposes In-External (IE) algorithm to perform the assignment of big data distribution for parallel data processing, which can fully utilize the capacity of Hadoop system and attain the best efficiency of knowledge reasoning. For the problem of reduced accuracy, in view of the fact that traditional probability learning methods are unfit for the proposed CBR system, this paper proposes Discount Exponential Coefficients of Multivariate Beta Distribution (DECMBD) algorithm to conduct the probability learning of proposed system. In DECMBD algorithm, a discount ratio is given to each exponential coefficient of multivariate Beta distribution to improve the occurrence times counting of all problem features and then gain better effect of probability learning. Finally, lots of experiments are performed to validate the effectiveness of the proposed advanced CBR system.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305753",
    "keywords": [
      "Artificial intelligence",
      "Bayesian network",
      "Big data",
      "Computer science",
      "Data mining",
      "Field (mathematics)",
      "Machine learning",
      "Mathematics",
      "Operating system",
      "Process (computing)",
      "Pure mathematics"
    ],
    "authors": [
      {
        "surname": "Guo",
        "given_name": "Yuan"
      },
      {
        "surname": "Zhang",
        "given_name": "Bing"
      },
      {
        "surname": "Yu",
        "given_name": "S."
      },
      {
        "surname": "Kai",
        "given_name": "W."
      }
    ]
  },
  {
    "title": "Joint production planning, pricing and retailer selection with emission control based on Stackelberg game and nested genetic algorithm",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113733",
    "abstract": "In practice, it is of paramount importance that firms make joint decisions in production planning, pricing and retailer selection while considering emission regulation. This is because the joint decisions can ensure firms to obtain higher profits while contributing to sustainable environments. However, due to the problem complexity, no models facilitating such decision making are available. This study aims to develop a model to help firms make optimal joint decisions. To model the situations where a manufacturer is the leader and the retailers are followers, we adopt the Stackelberg game theory and develop a 0–1 mixed nonlinear bilevel program to maximize the profits of both the manufacturer and his retailers. We further develop a nested genetic algorithm to solve the game model. Numerical examples demonstrate (i) the applicability of the game model and the algorithm and (ii) the robustness of the algorithm. Managerial insights are obtained, suggesting that (i) manufacturers need to identify the capacity ranges (called capacity traps) where capacity increases result in reduced profits when making decisions to optimize profits; (ii) retailers should make suitable, e.g., pricing decisions so that the manufacturers can include them in the supply chains; (iii) both manufacturers and retailers may not need to consider the carbon emission buying (or selling) price when making decisions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305571",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Business",
      "Chemistry",
      "Computer science",
      "Decision model",
      "Economics",
      "Gene",
      "Genetic algorithm",
      "Machine learning",
      "Marketing",
      "Mathematical optimization",
      "Mathematics",
      "Microeconomics",
      "Operations research",
      "Production (economics)",
      "Production planning",
      "Robustness (evolution)",
      "Selection (genetic algorithm)",
      "Stackelberg competition",
      "Supply chain"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Linda L."
      },
      {
        "surname": "D.U.",
        "given_name": "Gang"
      },
      {
        "surname": "W.U.",
        "given_name": "Jun"
      },
      {
        "surname": "M.A.",
        "given_name": "Yujie"
      }
    ]
  },
  {
    "title": "Cost-sensitive learning classification strategy for predicting product failures",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113653",
    "abstract": "In the current era of Industry 4.0, sensor data used in connection with machine learning algorithms can help manufacturing industries to reduce costs and to predict failures in advance. This paper addresses a binary classification problem found in manufacturing engineering, which focuses on how to ensure product quality delivery and at the same time to reduce production costs. The aim behind this problem is to predict the number of faulty products, which in this case is extremely low. As a result of this characteristic, the problem is reduced to an imbalanced binary classification problem. The authors contribute to imbalanced classification research in three important ways. First, the industrial application coming from the electronic manufacturing industry is presented in detail, along with its data and modelling challenges. Second, a modified cost-sensitive classification strategy based on a combination of Voronoi diagrams and genetic algorithm is applied to tackle this problem and is compared to several base classifiers. The results obtained are promising for this specific application. Third, in order to evaluate the flexibility of the strategy, and to demonstrate its wide range of applicability, 25 real-world data sets are selected from the KEEL repository with different imbalance ratios and number of features. The strategy, in this case implemented without a predefined cost, is compared with the same base classifiers as those used for the industrial problem.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420304772",
    "keywords": [
      "Artificial intelligence",
      "Binary classification",
      "Computer science",
      "Data mining",
      "Flexibility (engineering)",
      "Genetic algorithm",
      "Geometry",
      "Machine learning",
      "Mathematics",
      "Product (mathematics)",
      "Statistics",
      "Support vector machine",
      "Voronoi diagram"
    ],
    "authors": [
      {
        "surname": "Frumosu",
        "given_name": "Flavia Dalia"
      },
      {
        "surname": "Khan",
        "given_name": "Abdul Rauf"
      },
      {
        "surname": "Schiøler",
        "given_name": "Henrik"
      },
      {
        "surname": "Kulahci",
        "given_name": "Murat"
      },
      {
        "surname": "Zaki",
        "given_name": "Mohamed"
      },
      {
        "surname": "Westermann-Rasmussen",
        "given_name": "Peter"
      }
    ]
  },
  {
    "title": "Dynamic prioritization of surveillance video data in real-time automated detection systems",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113672",
    "abstract": "Automated object detection systems are a key component of modern surveillance applications. These systems rely on computationally expensive computer vision algorithms that perform object detection on visual data recorded by surveillance cameras. Due to the security and safety implications of these systems, this visual data st be processed accurately and in real-time. However, many of the frames that are created by the surveillance cameras may be of low importance, providing little or no useful information to the object detection system. Sub-sampling surveillance data by prioritizing important camera frames can greatly reduce unnecessary computation. Consequently, several works have explored dynamic visual data sub-sampling using various modalities of information (ie. spatial or temporal information) for prioritization. Few works, however, have combined and evaluated different modalities of information together for real-time prioritization of visual surveillance data. This work evaluates several individual and combined prioritization metrics derived from different modalities of information for use with a modern deep learning-based object detection algorithm. Both processing time and object detection rate are measured and used to rank the prioritization metrics. A novel approach that uses the historical detection confidences created by the object detection algorithm was demonstrated to be the best standalone prioritization metric. Additionally, a novel ensemble method that uses a KNN regressor to combine the best of the previously evaluated metrics to create a dynamic prioritization method is presented. This ensemble approach is shown to increase the object detection rate by up to 60% as compared to a static sub-sampling baseline as demonstrated using three publicly available datasets. The increased object detection rate was achieved while meeting the real-time constraints of the automated object detection system.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420304966",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Data mining",
      "Economics",
      "Filter (signal processing)",
      "Key (lock)",
      "Machine learning",
      "Management science",
      "Mathematics",
      "Metric (unit)",
      "Object (grammar)",
      "Object detection",
      "Operations management",
      "Pattern recognition (psychology)",
      "Prioritization",
      "Rank (graph theory)",
      "Sampling (signal processing)"
    ],
    "authors": [
      {
        "surname": "Cameron",
        "given_name": "James"
      },
      {
        "surname": "Kaye",
        "given_name": "Mary E."
      },
      {
        "surname": "Scheme",
        "given_name": "Erik"
      }
    ]
  },
  {
    "title": "Cepstral-based clustering of financial time series",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113705",
    "abstract": "In this paper, following the Partitioning Around Medoids (PAM) approach and the fuzzy theory, we propose a clustering model for financial time series based on the estimated cepstrum which represents the spectrum of the logarithm of the spectral density function. Selecting the optimal set of financial securities to build a portfolio that aims to maximize the risk-return tradeoff is a largely investigated topic in finance. The proposed model inherits all the advantages connected to PAM approach and fuzzy theory and it is able to compute objectively the cepstral weight associated to each cepstral coefficient by means of a suitable weighting system incorporated in the clustering model. In this way, the clustering model is able to tune objectively the different influence of each cepstral coefficient in the clustering process. The proposed clustering model performs better with respect to other clustering models. The proposed clustering model applied to each security sharpe ratio provides an efficient tool of clustering of stocks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305297",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Correlation clustering",
      "Data mining",
      "Fuzzy clustering",
      "Medicine",
      "Radiology",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "D’Urso",
        "given_name": "Pierpaolo"
      },
      {
        "surname": "Giovanni",
        "given_name": "Livia De"
      },
      {
        "surname": "Massari",
        "given_name": "Riccardo"
      },
      {
        "surname": "D’Ecclesia",
        "given_name": "Rita L."
      },
      {
        "surname": "Maharaj",
        "given_name": "Elizabeth Ann"
      }
    ]
  },
  {
    "title": "A two-step hybrid unsupervised model with attention mechanism for aspect extraction",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113673",
    "abstract": "Social networking sites have a wealth of user-generated unstructured text for fine-grained sentiment analysis regarding the changing dynamics in the marketplace. In aspect-level sentiment analysis, aspect term extraction (ATE) task identifies the targets of user opinions in the sentence. In the last few years, deep learning approaches significantly improved the performance of aspect extraction. However, the performance of recent models relies on the accuracy of dependency parser and part-of-speech (POS) tagger, which degrades the performance of the system if the sentence doesn't follow the language constraints and the text contains a variety of multi-word aspect-terms. Furthermore, lack of domain and contextual information is again an issue to extract domain-specific, most relevant aspect terms. The existing approaches are not capable of capturing long term dependencies for noun phrases, which in turn fails to extract some valid aspect terms. Therefore, this paper proposes a two-step mixed unsupervised model by combining linguistic patterns with deep learning techniques to improve the ATE task. The first step uses rules-based methods to extract the single word and multi-word aspects, which further prune domain-specific relevant aspects using fine-tuned word embedding. In the second step, the extracted aspects in the first step are used as label data to train the attention-based deep learning model for aspect-term extraction. The experimental evaluation on the SemEval-16 dataset validates our approach as compared to the most recent and baseline techniques.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420304978",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Deep learning",
      "Dependency (UML)",
      "Dependency grammar",
      "Domain (mathematical analysis)",
      "Economics",
      "Embedding",
      "Linguistics",
      "Machine learning",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Natural language processing",
      "Parsing",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "SemEval",
      "Sentence",
      "Sentiment analysis",
      "Task (project management)",
      "Term (time)",
      "Variety (cybernetics)",
      "Word (group theory)",
      "Word embedding"
    ],
    "authors": [
      {
        "surname": "Singh Chauhan",
        "given_name": "Ganpat"
      },
      {
        "surname": "Kumar Meena",
        "given_name": "Yogesh"
      },
      {
        "surname": "Gopalani",
        "given_name": "Dinesh"
      },
      {
        "surname": "Nahta",
        "given_name": "Ravi"
      }
    ]
  },
  {
    "title": "Deep multi-hybrid forecasting system with random EWT extraction and variational learning rate algorithm for crude oil futures",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113686",
    "abstract": "Machine learning algorithms provide feasibility for crude oil price prediction. In this paper, a novel multi-hybrid predictive neural network model is proposed based on complex deep learning algorithm, which integrates empirical wavelet transform, random inheritance formula error correction algorithm, deep bidirectional LSTM neural network and Elman recurrent neural network with variational learning rate. The prediction model is selected according to the sequence frequency after EWT feature extraction, and the prediction results are obtained by separately predicting and reintegrating. On the basis of individual model, the structure of deep bidirectional training, random inheritance formula and variational learning rate are proposed, which further ameliorate the performance of the model and achieve more effective data information capture. Simultaneously, the examination of variational learning rate provides us with a feasible parameter selection. The proposed model achieves high-precision prediction of crude oil futures price, and stands out in the multi-model comparison analysis and q-DSCID synchronous evaluation, with superior prediction accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305108",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Deep learning",
      "Economics",
      "Financial economics",
      "Futures contract",
      "Machine learning"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Bin"
      },
      {
        "surname": "Wang",
        "given_name": "Jun"
      }
    ]
  },
  {
    "title": "Non-intrusive load disaggregation based on composite deep long short-term memory network",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113669",
    "abstract": "Non-invasive load monitoring (NILM) is a vital step to realize the smart grid. Although the existing various NILM algorithms have made significant progress in energy consumption feedback, there are still some problems need to further addressed, such as the exponential growth of state space with the increase of the number of multi-state devices, which leads to the dimension disaster; and it is difficult to capture the power fluctuation information effectively because of the neglect of time-dependency problem load disaggregation; traditional disaggregation involves a process of one sequence to one sequence optimization, which is inefficient. In our study, a composite deep LSTM is proposed for load disaggregation. The proposed algorithm considers the process of load disaggregation as a signal separation process and establishes regression learning from a single sequence to multiple sequences to avoid dimension disaster. In addition, an encoder-separation-decoder structure is introduced for load disaggregation. Encoder completes the effective encoding of the mains power and differential power information, the time-dependency of the encoding process implemented by a deep LSTM, separation realizes the disaggregation process by separating the encoded information, and decoder decode the separated signal into the sequences of corresponding electrical appliances. Compared with the one sequence to one sequence disaggregation method, the proposed method simplified disaggregation complexity and improves the efficiency of disaggregation. The experimental results on WikiEnergy and REDD datasets show that the proposed method can reduce the disaggregation error and improve the comprehensive performance of event detection. Besides, our study can provide conditions for the realization of the bidirectional interaction of the smart grid and the improvement of the smart grid scheduling.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420304930",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Decoding methods",
      "Dependency (UML)",
      "Dimension (graph theory)",
      "Ecology",
      "Encoder",
      "Encoding (memory)",
      "Genetics",
      "Mathematics",
      "Operating system",
      "Process (computing)",
      "Pure mathematics",
      "Real-time computing",
      "Sequence (biology)",
      "Smart grid"
    ],
    "authors": [
      {
        "surname": "Xia",
        "given_name": "Min"
      },
      {
        "surname": "Liu",
        "given_name": "Wan’an"
      },
      {
        "surname": "Wang",
        "given_name": "Ke"
      },
      {
        "surname": "Song",
        "given_name": "Wenzhu"
      },
      {
        "surname": "Chen",
        "given_name": "Chunling"
      },
      {
        "surname": "Li",
        "given_name": "Yaping"
      }
    ]
  },
  {
    "title": "An ensemble discrete differential evolution for the distributed blocking flowshop scheduling with minimizing makespan criterion",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113678",
    "abstract": "The distributed blocking flowshop scheduling problem (DBFSP) plays an essential role in the manufacturing industry and has been proven to be as a strongly NP-hard problem. In this paper, an ensemble discrete differential evolution (EDE) algorithm is proposed to solve the blocking flowshop scheduling problem with the minimization of the makespan in the distributed manufacturing environment. In the EDE algorithm, the candidates are represented as discrete job permutations. Two heuristics method and one random strategy are integrated to provide a set of desirable initial solution for the distributed environment. The front delay, blocking time and idle time are considered in these heuristics methods. The mutation, crossover and selection operators are redesigned to assist the EDE algorithm to execute in the discrete domain. Meanwhile, an elitist retain strategy is introduced into the framework of EDE algorithm to balance the exploitation and exploration ability of the EDE algorithm. The parameters of the EDE algorithm are calibrated by the design of experiments (DOE) method. The computational results and comparisons demonstrated the efficiency and effectiveness of the EDE algorithm for the distributed blocking flowshop scheduling problem.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305029",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Blocking (statistics)",
      "Computer network",
      "Computer science",
      "Crossover",
      "Differential evolution",
      "Flow shop scheduling",
      "Heuristics",
      "Job scheduler",
      "Job shop scheduling",
      "Mathematical optimization",
      "Mathematics",
      "Operating system",
      "Programming language",
      "Queue",
      "Schedule",
      "Scheduling (production processes)"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Fuqing"
      },
      {
        "surname": "Zhao",
        "given_name": "Lexi"
      },
      {
        "surname": "Wang",
        "given_name": "Ling"
      },
      {
        "surname": "Song",
        "given_name": "Houbin"
      }
    ]
  },
  {
    "title": "Accuracy weighted diversity-based online boosting",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113723",
    "abstract": "Target distributional change occurring in a data stream known as concept drift, causes a challenging task for an online learning method, as the accuracy of an online learning method may decrease due to these changes. In this paper, the Accuracy Weighted Diversity-based Online Boosting (AWDOB) method has been proposed, which is based on Adaptable Diversity-based Online Boosting (ADOB) and, other modifications. More precisely, AWDOB uses the proposed accuracy weighting scheme which is based on previous expert’s results of the sums of correctly classified and incorrectly classified instances to calculate the weight of current expert, which improved the overall accuracy of the AWDOB. Experiments were conducted to compare the accuracy results of AWDOB against other methods using ten real-world datasets and thirty-two artificial datasets. Artificial datasets were generated by the four artificial data generators which included gradual and abrupt concept drifts within them. Experimental results suggest that AWDOB beats the accuracy results of other tested methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305479",
    "keywords": [
      "Artificial intelligence",
      "Boosting (machine learning)",
      "Computer science",
      "Data mining",
      "Machine learning",
      "Medicine",
      "Pattern recognition (psychology)",
      "Radiology",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Baidari",
        "given_name": "Ishwar"
      },
      {
        "surname": "Honnikoll",
        "given_name": "Nagaraj"
      }
    ]
  },
  {
    "title": "Identifying influential nodes in heterogeneous networks",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113580",
    "abstract": "Identifying influential users and measure the influence of nodes in social networks have become an interesting and important topic of research. It is crucial to find out to what extent individuals influence each other because it can be used to control rumors, diseases, and diffusion. There are numerous relevant models most of which are based on a homogeneous network. However, in the real world, we face heterogeneous networks where the nodes and edges are different types. A network is homogeneous if and only if the edges and nodes are of the same type, and it is considered heterogeneous if the nodes and edges are different. In heterogeneous networks, there is a concept known as meta-path, which indicates the type of communication between two nodes. In this paper, we aim to locate influential nodes by calculating the entropy of different meta-paths. To evaluate information diffusion in a heterogeneous network, we used the known susceptible-infectious model. The results of our experiments on three real-world networks’ dataset show that the proposed method outperforms state-of-the-art influence maximization algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420304048",
    "keywords": [
      "Combinatorics",
      "Computer network",
      "Computer science",
      "Data mining",
      "Entropy (arrow of time)",
      "Heterogeneous network",
      "Homogeneous",
      "Mathematical optimization",
      "Mathematics",
      "Maximization",
      "Path (computing)",
      "Physics",
      "Quantum mechanics",
      "Telecommunications",
      "Theoretical computer science",
      "Wireless",
      "Wireless network"
    ],
    "authors": [
      {
        "surname": "Molaei",
        "given_name": "Soheila"
      },
      {
        "surname": "Farahbakhsh",
        "given_name": "Reza"
      },
      {
        "surname": "Salehi",
        "given_name": "Mostafa"
      },
      {
        "surname": "Crespi",
        "given_name": "Noel"
      }
    ]
  },
  {
    "title": "Simultaneous incremental matrix factorization for streaming recommender systems",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113685",
    "abstract": "Recommender systems are large-scale machine learning and knowledge discovery tools aimed at providing personalized recommendations to customers based on their preferences and needs. They need to handle large quantities of diverse and very sparse data in a matter of seconds. Matrix factorization techniques have proven to be useful and reliable for implementing recommender systems, while data sparsity problem can be indirectly alleviated by considering multiple heterogeneous data sources. Furthermore, utilization of data fusion can resolve in a higher predictive accuracy. For real-world applications, e.g., such with continuous user feedback, incrementally handling recommender systems upon multiple data streams remains a crucial and only partially solved problem. This paper presents one way of fusing multiple data streams through matrix factorization. Our proposed method (SIMF) models heterogeneous and asynchronous data streams and provides predictions in real time. As a result of incremental updating, the proposed method successfully adapts to changes in data concepts, while application of data fusion improves prediction accuracy and reduces effects of the cold-start problem. Using the proposed methodology, we have develop a streaming algorithm and show how prediction accuracy can be substantially increased by considering multiple data sources, while at the same time the negative effects of the cold-start can be greatly diminished. Evaluations on a large-scale real-life problem (Yelp recommendations) confirm these claims as we present a highly scalable streaming recommender system that adapts to new concepts in data and provides accurate predictions (compared to the other matrix factorization techniques) in a very sparse problem domain. Apart from a recommender system proposed in this work, the versatility of matrix factorization could further allow the presented methodology for adaptation to solve several other machine learning problems, such as dimensionality reduction, clustering and classification.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305091",
    "keywords": [
      "Artificial intelligence",
      "Asynchronous communication",
      "Big data",
      "Collaborative filtering",
      "Computer network",
      "Computer science",
      "Data mining",
      "Data stream",
      "Data stream mining",
      "Database",
      "Eigenvalues and eigenvectors",
      "Gaussian",
      "Machine learning",
      "Matrix decomposition",
      "Physics",
      "Quantum mechanics",
      "Recommender system",
      "Scalability",
      "Sensor fusion",
      "Sparse matrix",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Jakomin",
        "given_name": "Martin"
      },
      {
        "surname": "Bosnić",
        "given_name": "Zoran"
      },
      {
        "surname": "Curk",
        "given_name": "Tomaž"
      }
    ]
  },
  {
    "title": "SVR-FFS: A novel forward feature selection approach for high-frequency time series forecasting using support vector regression",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113729",
    "abstract": "In this paper, we propose a novel support vector regression (SVR) approach for time series analysis. An efficient forward feature selection strategy has been designed for dealing with high-frequency time series with multiple seasonal periods. Inspired by the literature on feature selection for support vector classification, we designed a technique for assessing the contribution of additional covariates to the SVR solution, including them in a forward fashion. Our strategy extends the reasoning behind Auto-ARIMA, a well-known approach for automatic model specification for traditional time series analysis, to kernel machines. Experiments on well-known high-frequency datasets demonstrate the virtues of the proposed method in terms of predictive performance, confirming the virtues of an automatic model specification strategy and the use of nonlinear predictors in time series forecasting. Our empirical analysis focus on the energy load forecasting task, which is arguably the most popular application for high-frequency, multi-seasonal time series forecasting.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305534",
    "keywords": [
      "Artificial intelligence",
      "Autoregressive integrated moving average",
      "Biology",
      "Combinatorics",
      "Computer science",
      "Data mining",
      "Feature (linguistics)",
      "Feature selection",
      "Feature vector",
      "Kernel (algebra)",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Paleontology",
      "Philosophy",
      "Selection (genetic algorithm)",
      "Series (stratigraphy)",
      "Support vector machine",
      "Time series"
    ],
    "authors": [
      {
        "surname": "Valente",
        "given_name": "José Manuel"
      },
      {
        "surname": "Maldonado",
        "given_name": "Sebastián"
      }
    ]
  },
  {
    "title": "A machine learning model to identify early stage symptoms of SARS-Cov-2 infected patients",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113661",
    "abstract": "The recent outbreak of the respiratory ailment COVID-19 caused by novel coronavirus SARS-Cov2 is a severe and urgent global concern. In the absence of effective treatments, the main containment strategy is to reduce the contagion by the isolation of infected individuals; however, isolation of unaffected individuals is highly undesirable. To help make rapid decisions on treatment and isolation needs, it would be useful to determine which features presented by suspected infection cases are the best predictors of a positive diagnosis. This can be done by analyzing patient characteristics, case trajectory, comorbidities, symptoms, diagnosis, and outcomes. We developed a model that employed supervised machine learning algorithms to identify the presentation features predicting COVID-19 disease diagnoses with high accuracy. Features examined included details of the individuals concerned, e.g., age, gender, observation of fever, history of travel, and clinical details such as the severity of cough and incidence of lung infection. We implemented and applied several machine learning algorithms to our collected data and found that the XGBoost algorithm performed with the highest accuracy (>85%) to predict and select features that correctly indicate COVID-19 status for all age groups. Statistical analyses revealed that the most frequent and significant predictive symptoms are fever (41.1%), cough (30.3%), lung infection (13.1%) and runny nose (8.43%). While 54.4% of people examined did not develop any symptoms that could be used for diagnosis, our work indicates that for the remainder, our predictive model could significantly improve the prediction of COVID-19 status, including at early stages of infection.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420304851",
    "keywords": [
      "Artificial intelligence",
      "Bioinformatics",
      "Biology",
      "Computer science",
      "Coronavirus disease 2019 (COVID-19)",
      "Disease",
      "Infectious disease (medical specialty)",
      "Intensive care medicine",
      "Internal medicine",
      "Isolation (microbiology)",
      "Machine learning",
      "Medical diagnosis",
      "Medicine",
      "Nose",
      "Outbreak",
      "Pandemic",
      "Pathology",
      "Surgery"
    ],
    "authors": [
      {
        "surname": "Ahamad",
        "given_name": "Md. Martuza"
      },
      {
        "surname": "Aktar",
        "given_name": "Sakifa"
      },
      {
        "surname": "Rashed-Al-Mahfuz",
        "given_name": "Md."
      },
      {
        "surname": "Uddin",
        "given_name": "Shahadat"
      },
      {
        "surname": "Liò",
        "given_name": "Pietro"
      },
      {
        "surname": "Xu",
        "given_name": "Haoming"
      },
      {
        "surname": "Summers",
        "given_name": "Matthew A."
      },
      {
        "surname": "Quinn",
        "given_name": "Julian M.W."
      },
      {
        "surname": "Moni",
        "given_name": "Mohammad Ali"
      }
    ]
  },
  {
    "title": "Emerging technologies and industrial leadership. A Wikipedia-based strategic analysis of Industry 4.0",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113645",
    "abstract": "Among emerging technologies large attention has been devoted to the so called Fourth Industrial Revolution, or Industry 4.0, which is also a case study of a major industrial policy initiative, led by Germany. In the field of methodologies to profile and monitor emerging technologies the role of Wikipedia has been recently explored. In this paper we extend the use of Wikipedia by comparing the German edition with the world largest edition (in English) in order to examine whether there are significant structural differences. We first validate the use of Wikipedia for emerging technologies and for cross-language comparisons as a tool for (almost) real time strategic analysis. We then extract all Wikipedia pages related to Industry 4.0, build up a knowledge network and study its topological properties in the two editions. We find striking differences, which can be explained with respect to the persistence of the industrial pattern of specialization of Germany with respect to all other countries. Emerging technologies introduce novelty but also preserve path-dependency in the pattern of specialization. The implications for companies and policy makers are discussed.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420304693",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Business",
      "Computer science",
      "Data mining",
      "Data science",
      "Dependency (UML)",
      "Economic geography",
      "Economics",
      "Emerging markets",
      "Emerging technologies",
      "Field (mathematics)",
      "Finance",
      "German",
      "History",
      "Industrial Revolution",
      "Industry 4.0",
      "Knowledge management",
      "Law",
      "Mathematics",
      "Novelty",
      "Order (exchange)",
      "Path dependency",
      "Philosophy",
      "Political science",
      "Pure mathematics",
      "Theology"
    ],
    "authors": [
      {
        "surname": "Bonaccorsi",
        "given_name": "Andrea"
      },
      {
        "surname": "Chiarello",
        "given_name": "Filippo"
      },
      {
        "surname": "Fantoni",
        "given_name": "Gualtiero"
      },
      {
        "surname": "Kammering",
        "given_name": "Hanna"
      }
    ]
  },
  {
    "title": "Stock index futures trading impact on spot price volatility. The CSI 300 studied with a TGARCH model",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113688",
    "abstract": "A TGARCH modeling is argued to be the optimal basis for investigating the impact of index futures trading on spot price variability. We discuss the CSI-300 index (China-Shanghai-Shenzhen-300-Stock Index) as a test case. The results prove that the introduction of CSI-300 index futures (CSI-300-IF) trading significantly reduces the volatility in the corresponding spot market. It is also found that there is a stationary equilibrium relationship between the CSI-300 spot and CSI-300-IF markets. A bidirectional Granger causality is also detected. “Finally”, it is deduced that spot prices are predicted with greater accuracy over a 3 or 4 lag day time span.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305121",
    "keywords": [
      "Biology",
      "Computer science",
      "Econometrics",
      "Economics",
      "Financial economics",
      "Futures contract",
      "Horse",
      "Index (typography)",
      "Paleontology",
      "Series (stratigraphy)",
      "Spot contract",
      "Stock index futures",
      "Stock market",
      "Stock market index",
      "Stock price",
      "Volatility (finance)",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Ausloos",
        "given_name": "Marcel"
      },
      {
        "surname": "Zhang",
        "given_name": "Yining"
      },
      {
        "surname": "Dhesi",
        "given_name": "Gurjeet"
      }
    ]
  },
  {
    "title": "Sound quality prediction and improving of vehicle interior noise based on deep convolutional neural networks",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113657",
    "abstract": "Interior sound quality plays a vital role in vehicle quality assessment because it forms users' general impressions of vehicles and influences consumers' purchase intentions. Thus, evaluating vehicle interior sound quality is important. Many researchers have developed intelligent prediction models to precisely evaluate vehicle interior sound quality. Deep convolutional neural networks (CNNs) can automatically learn features and many studies have applied deep CNNs to address noise and vibration issues. However, those studies suffer from two problems: i) the time and frequency characteristics of noise that influence interior sound quality have not been considered simultaneously; ii) the noise features that deep CNNs have learned need to be explored. Therefore, in this paper, to overcome the first problem, we develop a regularized deep CNN model that takes a noise time–frequency image as input. In addition, we introduce a neuron visualization algorithm for deep CNNs to solve the second problem. To verify the proposed methods, we establish an interior noise dataset through vehicular road tests and subjective evaluations. The sound quality of this recorded interior noise is evaluated through the developed deep CNN model, which reveals that deep CNNs that use a noise time–frequency image as input perform better than do those using time vector and frequency vector data as input. By analyzing feature maps extracted from the convolutional layers and the fully connected layer of the CNNs, we found that the deep CNN feature learning process can be regarded as color filter and Gabor filter processes applied to the noise time–frequency image. These results provide a new approach for evaluating vehicle interior sound quality and help in understanding which noise features deep CNNs learn.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420304814",
    "keywords": [
      "Acoustics",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Deep neural networks",
      "Epistemology",
      "Image (mathematics)",
      "Machine learning",
      "Noise (video)",
      "Philosophy",
      "Physics",
      "Quality (philosophy)",
      "Sound (geography)",
      "Sound quality",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Xiaorong"
      },
      {
        "surname": "Huang",
        "given_name": "Haibo"
      },
      {
        "surname": "Wu",
        "given_name": "Jiuhui"
      },
      {
        "surname": "Yang",
        "given_name": "Mingliang"
      },
      {
        "surname": "Ding",
        "given_name": "Weiping"
      }
    ]
  },
  {
    "title": "Deep reinforcement learning based preventive maintenance policy for serial production lines",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113701",
    "abstract": "In the manufacturing industry, the preventive maintenance (PM) is a common practice to reduce random machine failures by replacing/repairing the aged machines or parts. The decision on when and where the preventive maintenance needs to be carried out is nontrivial due to the complex and stochastic nature of a serial production line with intermediate buffers. In order to improve the cost efficiency of the serial production lines, a deep reinforcement learning based approach is proposed to obtain PM policy. A novel modeling method for the serial production line is adopted during the learning process. A reward function is proposed based on the system production loss evaluation. The algorithm based on the Double Deep Q-Network is applied to learn the PM policy. Using the simulation study, the learning algorithm is proved effective in delivering PM policy that leads to an increased throughput and reduced cost. Interestingly, the learned policy is found to frequently conduct “group maintenance” and “opportunistic maintenance”, although their concepts and rules are not provided during the learning process. This finding further demonstrates that the problem formulation, the proposed algorithm and the reward function setting in this paper are effective.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742030525X",
    "keywords": [
      "Artificial intelligence",
      "Business",
      "Computer science",
      "Economics",
      "Engineering",
      "Marketing",
      "Microeconomics",
      "Preventive maintenance",
      "Proactive maintenance",
      "Production (economics)",
      "Production line",
      "Psychology",
      "Reinforcement",
      "Reinforcement learning",
      "Reliability engineering",
      "Social psychology"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Jing"
      },
      {
        "surname": "Chang",
        "given_name": "Qing"
      },
      {
        "surname": "Arinez",
        "given_name": "Jorge"
      }
    ]
  },
  {
    "title": "Backtracking search algorithm with competitive learning for identification of unknown parameters of photovoltaic systems",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113750",
    "abstract": "Metaheuristic algorithms have been successfully used to parameter identification of photovoltaic systems. However, this still faces the following two challenges. Firstly, most of the applied algorithms are complex and need some extra control parameters except the essential population size and stopping criterion, which is against their applications in photovoltaic systems with different characteristics. Secondly, how to obtain model parameters with higher accuracy and reliability has been a very valuable topic. To address the two challenges, this paper presents a new optimization method called backtracking search algorithm with competitive learning (CBSA) for parameter identification of photovoltaic systems. The remarkable features of CBSA are that it has a very simple structure and only needs the essential parameters. The core idea of CBSA is to increase the chance of backtracking search algorithm (BSA) to jump out of the local optimum by the designed competitive learning mechanism. In CBSA, the population is first divided into two subpopulations by built competitive mechanism. Then each subpopulation is optimized by the different search operators with multiple learning strategies. In order to test the performance of CBSA, CBSA is first employed to solve five challenging engineering design optimization problems and then is used to estimate the unknown parameters of three photovoltaic models. Experimental results show the solutions offered by CBSA outperform those of the compared algorithms including BSA, two recently proposed variants of BSA and other some state-of-the-art algorithms on nearly all test problems, which proves the effectiveness of the improved strategies.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305741",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Backtracking",
      "Biology",
      "Botany",
      "Computer science",
      "Demography",
      "Electrical engineering",
      "Engineering",
      "Identification (biology)",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Metaheuristic",
      "Photovoltaic system",
      "Population",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Yiying"
      },
      {
        "surname": "Ma",
        "given_name": "Maode"
      },
      {
        "surname": "Jin",
        "given_name": "Zhigang"
      }
    ]
  },
  {
    "title": "Automatic robust estimation for exponential smoothing: Perspectives from statistics and machine learning",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113637",
    "abstract": "A major challenge in automating the production of a large number of forecasts, as often required in many business applications, is the need for robust and reliable predictions. Increased noise, outliers and structural changes in the series, all too common in practice, can severely affect the quality of forecasting. We investigate ways to increase the reliability of exponential smoothing forecasts, the most widely used family of forecasting models in business forecasting. We consider two alternative sets of approaches, one stemming from statistics and one from machine learning. To this end, we adapt M-estimators, boosting and inverse boosting to parameter estimation for exponential smoothing. We propose appropriate modifications that are necessary for time series forecasting while aiming to obtain scalable algorithms. We evaluate the various estimation methods using multiple real datasets and find that several approaches outperform the widely used maximum likelihood estimation. The novelty of this work lies in (1) demonstrating the usefulness of M-estimators, (2) and of inverse boosting, which outperforms standard boosting approaches, and (3) a comparative look at statistics versus machine learning inspired approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420304619",
    "keywords": [
      "Artificial intelligence",
      "Boosting (machine learning)",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Estimator",
      "Exponential smoothing",
      "Interpretability",
      "Machine learning",
      "Mathematics",
      "Novelty",
      "Outlier",
      "Philosophy",
      "Smoothing",
      "Statistics",
      "Theology"
    ],
    "authors": [
      {
        "surname": "Barrow",
        "given_name": "Devon"
      },
      {
        "surname": "Kourentzes",
        "given_name": "Nikolaos"
      },
      {
        "surname": "Sandberg",
        "given_name": "Rickard"
      },
      {
        "surname": "Niklewski",
        "given_name": "Jacek"
      }
    ]
  },
  {
    "title": "Supporting better practice benchmarking: A DEA-ANN approach to bank branch performance assessment",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113599",
    "abstract": "The quest for best practices may lead to an increased risk of poor decision-making, especially when aiming to attain best practice levels reveals that efforts are beyond the organization’s present capabilities. This situation is commonly known as the “best practice trap”. Motivated by such observation, the purpose of the present paper is to develop a practical methodology to support better practice benchmarking, with an application to the banking sector. In this sense, we develop a two-stage hybrid model that employs Artificial Neural Network (ANN) via integration with Data Envelopment Analysis (DEA), which is used as a preprocessor, to investigate the ability of the DEA-ANN approach to classify the sampled branches of a Greek bank into predefined efficiency classes. ANN is integrated with a family of radial and non-radial DEA models. This combined approach effectively captures the information contained in the characteristics of the sampled branches, and subsequently demonstrates a satisfactory classification ability especially for the efficient branches. Our prediction results are presented using four performance measures (hit rates): percent success rate of classifying a bank branch’s performance exactly or within one class of its actual performance, as well as just one class above the actual class and just one class below the actual class. The proposed modeling approach integrates the DEA context with ANN and advances benchmarking practices to enhance the decision-making process for efficiency improvement.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420304231",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmarking",
      "Biology",
      "Business",
      "Class (philosophy)",
      "Computer science",
      "Context (archaeology)",
      "Data envelopment analysis",
      "Data mining",
      "Engineering",
      "Machine learning",
      "Marketing",
      "Mathematical optimization",
      "Mathematics",
      "Operating system",
      "Operations research",
      "Paleontology",
      "Preprocessor",
      "Process (computing)"
    ],
    "authors": [
      {
        "surname": "Tsolas",
        "given_name": "Ioannis E."
      },
      {
        "surname": "Charles",
        "given_name": "Vincent"
      },
      {
        "surname": "Gherman",
        "given_name": "Tatiana"
      }
    ]
  },
  {
    "title": "Detection of unexpected findings in radiology reports: A comparative study of machine learning approaches",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113647",
    "abstract": "This study explores machine learning methods for the detection of unexpected findings in Spanish radiology reports. Regarding radiological reports, unexpected findings are the set of radiological signs identified at a certain imaging modality exam which meet two characteristics: they are not apparently related with the a priori expected results of the radiological exam and involve a clinical emergency or urgency situation that must be reported shortly to the prescribing physician or another medical specialist as well as to the patient in order to preserve life and/or prevent dangerous occurrences. Several traditional machine learning and deep learning classification algorithms are evaluated and compared. To carry out the task we use 5947 anonymous radiology reports from HT médica. Experimental results suggest that the performance of the Convolutional Neural Networks models are better than traditional machine learning. The best F1 score for the identification of an unexpected finding was 90%. Finally, we also perform an error analysis which will guide us to achieve better results in the future.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420304711",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Computer science",
      "Convolutional neural network",
      "Economics",
      "Identification (biology)",
      "Machine learning",
      "Management",
      "Medicine",
      "Programming language",
      "Radiological weapon",
      "Radiology",
      "Set (abstract data type)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "López-Úbeda",
        "given_name": "Pilar"
      },
      {
        "surname": "Díaz-Galiano",
        "given_name": "Manuel Carlos"
      },
      {
        "surname": "Martín-Noguerol",
        "given_name": "Teodoro"
      },
      {
        "surname": "Ureña-López",
        "given_name": "Alfonso"
      },
      {
        "surname": "Martín-Valdivia",
        "given_name": "María-Teresa"
      },
      {
        "surname": "Luna",
        "given_name": "Antonio"
      }
    ]
  },
  {
    "title": "Entity disambiguation with context awareness in user-generated short texts",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113652",
    "abstract": "Conceptualization is to obtain the most appropriate concepts for noun terms (entities) under different contexts, which plays an important role in human knowledge understanding. However, in natural language, entities are often ambiguous, which creates difficulties in conceptualization. To accurately conceptualize, we must eliminate the ambiguity of entities. Existing methods mainly rely on similar or related entities in context for disambiguation. However, due to the sparsity of user-generated short texts, the number of entities that can be extracted from them is limited. In this paper, we propose an entity disambiguation method, which consists of three steps. (1) Measuring the correlation between terms, which uses both corpus and knowledge information to capture the specific semantic relationship. (2) Selecting informative terms, which considers various types of contextual terms, not just entities, thereby mitigating the effects of text sparsity. (3) Prioritizing informative terms to highlight their discriminative power, which reduces noise interference. Finally, the target entity is disambiguated based on informative terms. Experimental results on ground-truth datasets demonstrate that the proposed method outperforms baseline methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420304760",
    "keywords": [
      "Ambiguity",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Conceptualization",
      "Context (archaeology)",
      "Discriminative model",
      "Entity linking",
      "Information retrieval",
      "Knowledge base",
      "Natural language",
      "Natural language processing",
      "Natural language understanding",
      "Noun",
      "Paleontology",
      "Programming language"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Jiaqi"
      },
      {
        "surname": "Li",
        "given_name": "Yongjun"
      },
      {
        "surname": "Gao",
        "given_name": "Congjie"
      },
      {
        "surname": "Dong",
        "given_name": "Wei"
      }
    ]
  },
  {
    "title": "An efficient approach for outlier detection from uncertain data streams based on maximal frequent patterns",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113646",
    "abstract": "Outlier identification is an important technology to improve the credibility of data and aims at detecting patterns that rarely appear and exhibit a significant difference from other data. However, the detection accuracy achieved by the simple deviation factors of existing pattern-based outlier detection methods is not competitive. In addition, given the large scale of uncertain data streams, the efficiency of many pattern-based outlier detection methods is not high because they use a vast number of frequent patterns to conduct the outlier detection. In this paper, to contend with the uncertain data streams, we propose a maximal-frequent-pattern-based outlier detection method, namely, MFP-OD, for identifying the outliers with a lower time cost. For further improving the detection accuracy of existing outlier detection methods, we design three deviation factors to measure the deviation degree of each transaction. The experimental results indicate that the proposed MFP-OD method can quickly and accurately identify the outliers from uncertain data streams.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742030470X",
    "keywords": [
      "Anomaly detection",
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Computer science",
      "Credibility",
      "Data mining",
      "Data stream mining",
      "Database",
      "Database transaction",
      "Identification (biology)",
      "Law",
      "Outlier",
      "Pattern recognition (psychology)",
      "Physics",
      "Political science",
      "Quantum mechanics",
      "Scale (ratio)",
      "Transaction data"
    ],
    "authors": [
      {
        "surname": "Cai",
        "given_name": "Saihua"
      },
      {
        "surname": "Li",
        "given_name": "Li"
      },
      {
        "surname": "Li",
        "given_name": "Sicong"
      },
      {
        "surname": "Sun",
        "given_name": "Ruizhi"
      },
      {
        "surname": "Yuan",
        "given_name": "Gang"
      }
    ]
  },
  {
    "title": "An efficient memetic algorithm for distributed flexible job shop scheduling problem with transfers",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113721",
    "abstract": "The traditional distributed flexible job shop scheduling problem (DFJSP) assumes that operations of a job cannot be transferred between different factories. However, in real-world production settings, the operations of a job may need to be processed in different factories owing to requirements of economic globalization or complexity of the job. Hence, in this paper, we propose a distributed flexible job shop scheduling problem with transfers (DFJSPT), in which operations of a job can be processed in different factories. An efficient memetic algorithm (EMA) is proposed to solve the DFJSPT with the objectives of minimizing the makespan, maximum workload, and total energy consumption of factories. In the proposed EMA, a well-designed chromosome presentation and initialization methods are presented to obtain a high-quality initial population. Several crossover and mutation operators and three effective neighborhood structures are designed to expand the search space and accelerate the convergence speed of the solution. Forty benchmark instances of the DFJSPT are constructed to evaluate the EMA and facilitate further studies. The Taguchi method of design of experiments is used to obtain the best combination of key EMA parameters. Extensive computational experiments are carried out to compare the EMA with three well-known algorithms from the literature. The computational results show that the EMA can obtain better solutions for approximately 90% of the tested benchmark instances compared to the three well-known algorithms, thereby demonstrating the DFJSPT’s competitive performance and efficiency.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305455",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Cloud computing",
      "Computer science",
      "Crossover",
      "Flow shop scheduling",
      "Geodesy",
      "Geography",
      "Job scheduler",
      "Job shop",
      "Job shop scheduling",
      "Local search (optimization)",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Memetic algorithm",
      "Operating system",
      "Schedule",
      "Taguchi methods"
    ],
    "authors": [
      {
        "surname": "Luo",
        "given_name": "Qiang"
      },
      {
        "surname": "Deng",
        "given_name": "Qianwang"
      },
      {
        "surname": "Gong",
        "given_name": "Guiliang"
      },
      {
        "surname": "Zhang",
        "given_name": "Like"
      },
      {
        "surname": "Han",
        "given_name": "Wenwu"
      },
      {
        "surname": "Li",
        "given_name": "Kexin"
      }
    ]
  },
  {
    "title": "Representation learning via serial robust autoencoder for domain adaptation",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113635",
    "abstract": "Domain adaptation aims to apply knowledge obtained from a labeled source domain to an unseen target domain from a different distribution. Recently, domain adaptation approaches based on autoencoder have achieved promising performances. However, almost of these approaches ignore the potential relationships of intra-domain features, which can be used to further reduce the distribution discrepancy between source and target domains. Furthermore, almost of them depend on the single autoencoder model, which brings the challenge of extracting multiple characteristics of data. To address these issues, in this paper, we propose a new representation learning method based on serial robust autoencoder for domain adaptation, named SERA. SERA first enriches intra-domain knowledge by mining the potential relationships of features in the source domain and target domain, respectively. Then, SERA learns domain invariant representations by serially connecting two new proposed autoencoder models, including marginalized denoising autoencoder via adaptation regularization (AMDA) and robust autoencoder via graph regularization (GRA). Extensive experiments on four public datasets demonstrate the effectiveness of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420304590",
    "keywords": [
      "Artificial intelligence",
      "Autoencoder",
      "Classifier (UML)",
      "Computer science",
      "Deep learning",
      "Domain (mathematical analysis)",
      "Domain adaptation",
      "Feature learning",
      "Law",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Regularization (linguistics)",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Shuai"
      },
      {
        "surname": "Zhang",
        "given_name": "Yuhong"
      },
      {
        "surname": "Wang",
        "given_name": "Hao"
      },
      {
        "surname": "Li",
        "given_name": "Peipei"
      },
      {
        "surname": "Hu",
        "given_name": "Xuegang"
      }
    ]
  },
  {
    "title": "Improving interpretability of word embeddings by generating definition and usage",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113633",
    "abstract": "Word embeddings are substantially successful in capturing semantic relations among words. However, these lexical semantics are difficult to be interpreted. Definition modeling provides a more intuitive way to evaluate embeddings by utilizing them to generate natural language definitions of corresponding words. This task is of great significance for practical application and in-depth understanding of word representations. We propose a novel framework for definition modeling, which can generate reasonable and understandable context-dependent definitions. Moreover, we introduce usage modeling and study whether it is possible to utilize embeddings to generate example sentences of words. These ways are a more direct and explicit expression of embedding’s semantics for better interpretability. We extend the single task model to multi-task setting and investigate several joint multi-task models to combine usage modeling and definition modeling together. Experimental results on existing Oxford dataset and a new collected Oxford-2019 dataset show that our single-task model achieves the state-of-the-art result in definition modeling and the multi-task learning methods are helpful for two tasks to improve the performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420304577",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Distributional semantics",
      "Economics",
      "Embedding",
      "Interpretability",
      "Linguistics",
      "Management",
      "Natural language",
      "Natural language processing",
      "Natural language understanding",
      "Paleontology",
      "Philosophy",
      "Programming language",
      "Semantic similarity",
      "Semantics (computer science)",
      "Task (project management)",
      "Word (group theory)",
      "Word embedding"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Haitong"
      },
      {
        "surname": "Du",
        "given_name": "Yongping"
      },
      {
        "surname": "Sun",
        "given_name": "Jiaxin"
      },
      {
        "surname": "Li",
        "given_name": "Qingxiao"
      }
    ]
  },
  {
    "title": "An ensemble imbalanced classification method based on model dynamic selection driven by data partition hybrid sampling",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113660",
    "abstract": "In many real-world applications classification problems suffer from class-imbalance. The classification methods for imbalanced data with only data processing or algorithm improvement cannot get satisfied classification performance of the minority class. This paper proposes an ensemble classification method based on model dynamic selection driven by data partition hybrid sampling for imbalanced data. The method includes two core components: the generation of balanced datasets and the dynamic selection of classification models. At the data level a data partition hybrid sampling (DPHS) method is proposed to balance datasets. In particular the data space is divided into four regions according to the majority class proportion in minority class neighborhoods. Then we present a boundary minority class weighted over-sampling (BMW-SMOTE) method where the weight of each minority class instance is calculated by the ratio between the majority class proportion in the neighborhood of the current instance and the sum of all these proportions. The number of synthetic instances is determined by the weight. At the algorithm level we present a model dynamic selection (MDS) strategy. Three ensemble learning models are built. Among them the local regions reinforce and weaken model adopts the balanced dataset obtained by proposed DPHS method for training to strengthen the identification of test instances on the boundary and appropriately weakens the dense distribution of majority class. The model for each test instance is selected adaptively according to the imbalance degree of its neighbors. The experimental results show that the proposed method outperforms typical imbalanced classification methods for F-measure and G-mean.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742030484X",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Data classification",
      "Data mining",
      "Ensemble learning",
      "Feature selection",
      "Filter (signal processing)",
      "Machine learning",
      "Mathematics",
      "Partition (number theory)",
      "Pattern recognition (psychology)",
      "Sampling (signal processing)",
      "Selection (genetic algorithm)"
    ],
    "authors": [
      {
        "surname": "Gao",
        "given_name": "Xin"
      },
      {
        "surname": "Ren",
        "given_name": "Bing"
      },
      {
        "surname": "Zhang",
        "given_name": "Hao"
      },
      {
        "surname": "Sun",
        "given_name": "Bohao"
      },
      {
        "surname": "Li",
        "given_name": "Junliang"
      },
      {
        "surname": "Xu",
        "given_name": "Jianhang"
      },
      {
        "surname": "He",
        "given_name": "Yang"
      },
      {
        "surname": "Li",
        "given_name": "Kangsheng"
      }
    ]
  },
  {
    "title": "Integrating systems thinking skills with multi-criteria decision-making technology to recruit employee candidates",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113585",
    "abstract": "The emergence of modern complex systems is often exacerbated by a proliferation of information and complication of technologies. Because current complex systems challenges can limit an organization's ability to efficiently handle socio-technical systems, it is essential to provide methods and techniques that count on individuals' systems skills. When selecting future employees, companies must constantly refresh their recruitment methods in order to find capable candidates with the required level of systemic skills who are better fit for their organization's requirements and objectives. The purpose of this study is to use systems thinking skills as a supplemental selection tool when recruiting prospective employees. To the best of our knowledge, there is no prior research that studied the use of systems thinking skills for recruiting purposes. The proposed framework offers an established tool to HRM professionals for assessing and screening of prospective employees of an organization based on their level of systems thinking skills while controlling uncertainties of complex decision-making environment with the fuzzy linguistic approach. This framework works as an expert system to find the most appropriate candidate for the organization to enhance the human capital for the organization. Several large industries, among others, Boeing, the government such as the Army, Military Academy, and National Science Foundation, highlighted the significance of having qualified (systemic) individuals who can successfully deal with complex systems problems. The correct recruiting decision will reduce the rate of job turnover and also help organizations to eliminate unnecessary budget allocated for costly recruitment processes. The proposed framework is intended to first evaluate the pool of applicants according to their level of systems thinking skills and then rank them based on the recruitment strategy and workforce needs of the organization. To achieve the purpose of the study, two recruiting strategies are adopted from the human resource management literature 1) Job-Fit Recruiting strategy—finding candidates who are most aligned with a specific position requirement and 2) Flexible Recruiting strategy—finding candidates with the highest potentials. The proposed framework is validated using a real case study in a US large-scale organization.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420304097",
    "keywords": [
      "Artificial intelligence",
      "Business",
      "Computer science",
      "Finance",
      "Government (linguistics)",
      "Knowledge management",
      "Linguistics",
      "Order (exchange)",
      "Philosophy",
      "Risk analysis (engineering)",
      "Systems thinking"
    ],
    "authors": [
      {
        "surname": "Karam",
        "given_name": "Sofia"
      },
      {
        "surname": "Nagahi",
        "given_name": "Morteza"
      },
      {
        "surname": "Dayarathna (Nick)",
        "given_name": "Vidanelage L."
      },
      {
        "surname": "Ma",
        "given_name": "Junfeng"
      },
      {
        "surname": "Jaradat",
        "given_name": "Raed"
      },
      {
        "surname": "Hamilton",
        "given_name": "Michael"
      }
    ]
  },
  {
    "title": "An ensemble imbalanced classification method based on model dynamic selection driven by data partition hybrid sampling",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113660",
    "abstract": "In many real-world applications classification problems suffer from class-imbalance. The classification methods for imbalanced data with only data processing or algorithm improvement cannot get satisfied classification performance of the minority class. This paper proposes an ensemble classification method based on model dynamic selection driven by data partition hybrid sampling for imbalanced data. The method includes two core components: the generation of balanced datasets and the dynamic selection of classification models. At the data level a data partition hybrid sampling (DPHS) method is proposed to balance datasets. In particular the data space is divided into four regions according to the majority class proportion in minority class neighborhoods. Then we present a boundary minority class weighted over-sampling (BMW-SMOTE) method where the weight of each minority class instance is calculated by the ratio between the majority class proportion in the neighborhood of the current instance and the sum of all these proportions. The number of synthetic instances is determined by the weight. At the algorithm level we present a model dynamic selection (MDS) strategy. Three ensemble learning models are built. Among them the local regions reinforce and weaken model adopts the balanced dataset obtained by proposed DPHS method for training to strengthen the identification of test instances on the boundary and appropriately weakens the dense distribution of majority class. The model for each test instance is selected adaptively according to the imbalance degree of its neighbors. The experimental results show that the proposed method outperforms typical imbalanced classification methods for F-measure and G-mean.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742030484X",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Data classification",
      "Data mining",
      "Ensemble learning",
      "Feature selection",
      "Filter (signal processing)",
      "Machine learning",
      "Mathematics",
      "Partition (number theory)",
      "Pattern recognition (psychology)",
      "Sampling (signal processing)",
      "Selection (genetic algorithm)"
    ],
    "authors": [
      {
        "surname": "Gao",
        "given_name": "Xin"
      },
      {
        "surname": "Ren",
        "given_name": "Bing"
      },
      {
        "surname": "Zhang",
        "given_name": "Hao"
      },
      {
        "surname": "Sun",
        "given_name": "Bohao"
      },
      {
        "surname": "Li",
        "given_name": "Junliang"
      },
      {
        "surname": "Xu",
        "given_name": "Jianhang"
      },
      {
        "surname": "He",
        "given_name": "Yang"
      },
      {
        "surname": "Li",
        "given_name": "Kangsheng"
      }
    ]
  },
  {
    "title": "Integrating systems thinking skills with multi-criteria decision-making technology to recruit employee candidates",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113585",
    "abstract": "The emergence of modern complex systems is often exacerbated by a proliferation of information and complication of technologies. Because current complex systems challenges can limit an organization's ability to efficiently handle socio-technical systems, it is essential to provide methods and techniques that count on individuals' systems skills. When selecting future employees, companies must constantly refresh their recruitment methods in order to find capable candidates with the required level of systemic skills who are better fit for their organization's requirements and objectives. The purpose of this study is to use systems thinking skills as a supplemental selection tool when recruiting prospective employees. To the best of our knowledge, there is no prior research that studied the use of systems thinking skills for recruiting purposes. The proposed framework offers an established tool to HRM professionals for assessing and screening of prospective employees of an organization based on their level of systems thinking skills while controlling uncertainties of complex decision-making environment with the fuzzy linguistic approach. This framework works as an expert system to find the most appropriate candidate for the organization to enhance the human capital for the organization. Several large industries, among others, Boeing, the government such as the Army, Military Academy, and National Science Foundation, highlighted the significance of having qualified (systemic) individuals who can successfully deal with complex systems problems. The correct recruiting decision will reduce the rate of job turnover and also help organizations to eliminate unnecessary budget allocated for costly recruitment processes. The proposed framework is intended to first evaluate the pool of applicants according to their level of systems thinking skills and then rank them based on the recruitment strategy and workforce needs of the organization. To achieve the purpose of the study, two recruiting strategies are adopted from the human resource management literature 1) Job-Fit Recruiting strategy—finding candidates who are most aligned with a specific position requirement and 2) Flexible Recruiting strategy—finding candidates with the highest potentials. The proposed framework is validated using a real case study in a US large-scale organization.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420304097",
    "keywords": [
      "Artificial intelligence",
      "Business",
      "Computer science",
      "Finance",
      "Government (linguistics)",
      "Knowledge management",
      "Linguistics",
      "Order (exchange)",
      "Philosophy",
      "Risk analysis (engineering)",
      "Systems thinking"
    ],
    "authors": [
      {
        "surname": "Karam",
        "given_name": "Sofia"
      },
      {
        "surname": "Nagahi",
        "given_name": "Morteza"
      },
      {
        "surname": "Dayarathna (Nick)",
        "given_name": "Vidanelage L."
      },
      {
        "surname": "Ma",
        "given_name": "Junfeng"
      },
      {
        "surname": "Jaradat",
        "given_name": "Raed"
      },
      {
        "surname": "Hamilton",
        "given_name": "Michael"
      }
    ]
  },
  {
    "title": "Attention-based deep neural network for Internet platform group users’ dynamic identification and recommendation",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113728",
    "abstract": "Under the Internet background, group recommendation has become a major interest in the study of recommendation systems. In the method of group recommendation, the existing researches are mostly conducted using cluster analysis and similarity analysis. The group characteristics studied are relatively generalized, and the group objects studied are mostly fixed, so the group cannot change in real time according to the different attributes and characteristics of the different products. At the same time, for the research object of group recommendation, the existing research mainly consider the recommended project group or user group, but seldom consider recommending the appropriate project group to the appropriate user group to improve the recommendation efficiency and user satisfaction. In view of these problems, this paper proposes a deep neural network that integrates the attention mechanism for group users’ dynamic identification and recommendation on the Internet platform. This paper uses an attention mechanism and deep neural networks to generate the attention preference weights for the group users according to the product attributes. Doing so achieves the purpose of recommending many types of projects to different groups to adapt to their preferences. We compare this method with other baseline methods on two public datasets to validate the effectiveness of the proposed method, which achieves better performance than the most advanced methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305522",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Baseline (sea)",
      "Biology",
      "Botany",
      "Chemistry",
      "Computer science",
      "Data mining",
      "Economics",
      "Geology",
      "Geometry",
      "Group (periodic table)",
      "Identification (biology)",
      "Image (mathematics)",
      "Information retrieval",
      "Machine learning",
      "Mathematics",
      "Microeconomics",
      "Object (grammar)",
      "Oceanography",
      "Organic chemistry",
      "Preference",
      "Product (mathematics)",
      "Recommender system",
      "Similarity (geometry)",
      "The Internet",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Xuna"
      },
      {
        "surname": "Tan",
        "given_name": "Qingmei"
      },
      {
        "surname": "Goh",
        "given_name": "Mark"
      }
    ]
  },
  {
    "title": "Investigating the potential for using gamification to empower knowledge workers",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113694",
    "abstract": "The increasingly popular trend of gamification has proved powerful in many areas, such as education and marketing, and has started making its way to the corporate environment. This exploratory study is focused on a particular part of corporate applications – using gamification to empower knowledge workers and to help them to interact with each other. Based on a review of the extant literature and an exploratory case study, we conceptualise different ways in which gamification supports knowledge workers and influences the dynamics of their interactions. The case study we present is that of online retailer Zappos who have been pioneers in this field. This paper is intended as the beginning of a journey towards utilising gamification in various aspects of knowledge work. Through studying the Zappos case, we draw out key learning points that can be used by other organisations in their journey to use gamification to empower knowledge workers. The paper also identifies areas for further research relevant to expert and intelligent systems, including the potential for synergies between gamification and intelligent systems, and the use of gamification in intelligent systems implementation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305182",
    "keywords": [
      "Anthropology",
      "Biology",
      "Computer science",
      "Computer security",
      "Engineering",
      "Evolutionary biology",
      "Exploratory research",
      "Extant taxon",
      "Field (mathematics)",
      "Key (lock)",
      "Knowledge management",
      "Mathematics",
      "Mechanical engineering",
      "Pure mathematics",
      "Sociology",
      "Work (physics)"
    ],
    "authors": [
      {
        "surname": "Spanellis",
        "given_name": "Agnessa"
      },
      {
        "surname": "Dӧrfler",
        "given_name": "Viktor"
      },
      {
        "surname": "MacBryde",
        "given_name": "Jillian"
      }
    ]
  },
  {
    "title": "Neural-based time series forecasting of loss of coolant accidents in nuclear power plants",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113699",
    "abstract": "In the last few years, deep learning in neural networks demonstrated impressive successes in the areas of computer vision, speech and image recognition, text generation, and many others. However, sensitive engineering areas such as nuclear engineering benefited less from these efficient techniques. In this work, deep learning expert systems are utilized to model and predict time series progression of a design-basis nuclear accident, featuring a loss of coolant accident. Two major findings are accomplished in this work. First, the ability to train expert systems with high accuracy, which could help nuclear power plant operators to figure out plant responses during the accident. Second, building fast, efficient, and accurate deep models to simulate nuclear phenomena, which could be valuable to nuclear computational science. In this work, large amount of time series data is obtained from simulation tools by simulating different conditions of the base-case/nominal accident scenario. Four critical outputs/responses are monitored during the accident (e.g. temperature, pressure, break flow rate, water level). Two approaches are adopted in this work. The first approach is to use feedforward deep neural networks (DNN) to fit all time steps and outputs in a single model. The second approach is to use long short-term memory (LSTM) to fit all time steps together for each reactor response separately. Both DNN and LSTM demonstrate very good performance in predicting the test and base-case scenarios, with accuracy as low as 92% and as high as 99%, where these test scenarios are unknown to the expert systems and are not included in the model training. In addition, both approaches demonstrate a significant reduction in computational costs, as the deep expert system is able to accurately predict the accident 100,000 times faster than the original simulation tool. Given sufficient data, the methodology adopted in this study demonstrates that DNN/LSTM expert systems can be used as a decision support system to model advanced time series phenomena within nuclear power plants with high accuracy and negligible computational costs.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305236",
    "keywords": [
      "Accident (philosophy)",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer science",
      "Control engineering",
      "Coolant",
      "Deep learning",
      "Ecology",
      "Engineering",
      "Epistemology",
      "Feed forward",
      "Feedforward neural network",
      "Loss-of-coolant accident",
      "Machine learning",
      "Mechanical engineering",
      "Nuclear physics",
      "Nuclear power",
      "Nuclear power plant",
      "Philosophy",
      "Physics",
      "Work (physics)"
    ],
    "authors": [
      {
        "surname": "Radaideh",
        "given_name": "Majdi I."
      },
      {
        "surname": "Pigg",
        "given_name": "Connor"
      },
      {
        "surname": "Kozlowski",
        "given_name": "Tomasz"
      },
      {
        "surname": "Deng",
        "given_name": "Yujia"
      },
      {
        "surname": "Qu",
        "given_name": "Annie"
      }
    ]
  },
  {
    "title": "An improved general variable neighborhood search for a static bike-sharing rebalancing problem considering the depot inventory",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113752",
    "abstract": "Smart shared mobility is an emerging transportation strategy that promotes sustainable and intelligent transportation. As one mode of smart shared mobility bike sharing is gaining popularity in recent years. A daily rebalancing operation is commonly carried out to keep high level service of bike-sharing systems (BSSs). The static bike-sharing rebalancing problems (SBRPs) studied in existing papers focus on determining the vehicle routes with minimal traveling cost. However, the depot inventory is rarely considered during the relocation. Thus, this paper researches the integration of the depot inventory and vehicle routing problems, with the aim of minimizing the daily operational cost including the depot inventory cost (DIC) and the traveling cost. First, two mixed integer programming (MIP) formulations are proposed to find the daily optimal decision on the vehicle routes and the numbers of bikes and vehicles employed from the depot. Based on the models, an improved general variable neighborhood search (IGVNS) algorithm is developed with a variety of neighborhood structures and a hybrid strategy. Finally, we apply a set of benchmark instances to test our proposed model and approach, and the computational results demonstrate that IGVNS can efficiently compute the SBRP and achieve lower operational cost than the existing solutions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305765",
    "keywords": [
      "Algorithm",
      "Archaeology",
      "Benchmark (surveying)",
      "Computer network",
      "Computer science",
      "Depot",
      "Economics",
      "Economy",
      "Engineering",
      "Geodesy",
      "Geography",
      "History",
      "Integer programming",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Operations research",
      "Programming language",
      "Relocation",
      "Routing (electronic design automation)",
      "Service (business)",
      "Set (abstract data type)",
      "Variable (mathematics)",
      "Vehicle routing problem"
    ],
    "authors": [
      {
        "surname": "Ren",
        "given_name": "Yaping"
      },
      {
        "surname": "Meng",
        "given_name": "Leilei"
      },
      {
        "surname": "Zhao",
        "given_name": "Fu"
      },
      {
        "surname": "Zhang",
        "given_name": "Chaoyong"
      },
      {
        "surname": "Guo",
        "given_name": "Hongfei"
      },
      {
        "surname": "Tian",
        "given_name": "Ying"
      },
      {
        "surname": "Tong",
        "given_name": "Wen"
      },
      {
        "surname": "Sutherland",
        "given_name": "John W."
      }
    ]
  },
  {
    "title": "A time-series clustering methodology for knowledge extraction in energy consumption data",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113731",
    "abstract": "In the Energy Efficiency field, the incorporation of intelligent systems in cities and buildings is motivated by the energy savings and pollution reduction that can be attained. To achieve this goal, energy modelling and a better understanding of how energy is consumed are fundamental factors. As a result, this study proposes a methodology for knowledge acquisition in energy-related data through Time-Series Clustering (TSC) techniques. In our experimentation, we utilize data from the buildings at the University of Granada (Spain) and compare several clustering methods to get the optimum model, in particular, we tested k-Means, k-Medoids, Hierarchical clustering and Gaussian Mixtures; as well as several algorithms to obtain the best grouping, such as PAM, CLARA, and two variants of Lloyd’s method, Small and Large. Thus, our methodology can provide non-trivial knowledge from raw energy data. In contrast to previous studies in this field, not only do we propose a clustering methodology to group time series straightforwardly, but we also present an automatic strategy to search and analyse energy periodicity in these series recursively so that we can deepen granularity and extract information at different levels of detail. The results show that k-Medoids with PAM is the best approach in virtually all cases, and the Squared Euclidean distance outperforms the rest of the metrics.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305558",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Electrical engineering",
      "Energy (signal processing)",
      "Energy consumption",
      "Engineering",
      "Euclidean distance",
      "Field (mathematics)",
      "Granularity",
      "Hierarchical clustering",
      "Machine learning",
      "Mathematics",
      "Medoid",
      "Operating system",
      "Paleontology",
      "Pure mathematics",
      "Series (stratigraphy)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Ruiz",
        "given_name": "L.G.B."
      },
      {
        "surname": "Pegalajar",
        "given_name": "M.C."
      },
      {
        "surname": "Arcucci",
        "given_name": "R."
      },
      {
        "surname": "Molina-Solana",
        "given_name": "M."
      }
    ]
  },
  {
    "title": "Ranking multiple-input and multiple-output units: A comparative study of data envelopment analysis and rank aggregation",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113687",
    "abstract": "Ranking multiple-input and multiple-output units is a critical problem that arises in a broad range of disciplines. While various methods have been proposed and applied, their comparative strengths and weaknesses are not well understood. In this paper, we assess and compare two popular methods, data envelopment analysis (DEA) and heuristic rank aggregation approach (i.e., the Borda method), in the context of ranking multiple-input and multiple-output units. Both methods exploit the output-input ratios, but in different ways. The Borda method sorts the units by taking the arithmetic average of the ranks in terms of individual output-input ratios, whereas DEA ranks the units based on composite output-input ratios. We use simulations to compare Borda rank aggregation and six DEA models, including CCR (Charnes, Cooper, Rhodes), super-efficiency CCR, BCC (Banker, Charnes, Cooper), super-efficiency BCC, SBM (slacks-based measure), and super-efficiency SBM. The simulations are based on Cobb-Douglas and translog production functions with both single output and multiple outputs. We show that the heuristic Borda rank aggregation, though simple to implement, performs better than DEA models for Cobb-Douglas production function under three situations: small sample size, relatively balanced weights for production factors, and presence of multiple outputs. For translog production function, the Borda method generally performs better than the CCR model, but cannot match up to other DEA models. We also demonstrate the performance of different methods via application to the well-known problem of ranking countries by human development. Our research sheds light on the potential of rank aggregation to complement or even supplant DEA under certain conditions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742030511X",
    "keywords": [
      "Aggregate (composite)",
      "Artificial intelligence",
      "Biology",
      "Chemistry",
      "Chromatography",
      "Combinatorics",
      "Composite material",
      "Computer science",
      "Context (archaeology)",
      "Data envelopment analysis",
      "Data mining",
      "Econometrics",
      "Economics",
      "Evolutionary biology",
      "Function (biology)",
      "Heuristic",
      "Macroeconomics",
      "Materials science",
      "Mathematical optimization",
      "Mathematics",
      "Measure (data warehouse)",
      "Paleontology",
      "Production (economics)",
      "Rank (graph theory)",
      "Ranking (information retrieval)",
      "Sample (material)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Derek D."
      }
    ]
  },
  {
    "title": "Classifying Papanicolaou cervical smears through a cell merger approach by deep learning technique",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113707",
    "abstract": "Early detection of cancer is important to improve survival and reduce associated morbility. Nowadays, there is no automatic classification process with enough accuracy to be recommended to its use in population cervical cancer screening. In most automatic medical image classifications, these images are clean in background and without overlap between elements, which means that these images do not reflect reality and the model cannot be applied to directly obtained images from medical samples. The objectives of this study are to design and implement a Cell Merger Approach to improve the efficiency and realism of the PAP-smears classification model, by allowing overlapping and folding of different cells, to design and implement a Convolutional Neural Network for PAP-smears image classification, and to optimize and integrate the cell fusion approach with the neural network building a feasible, reliable and highly accurate system for cervical smears classification. The carried out experiments have validated both the CNN and the proposed Cell Merger Approach with very interesting results. The most outstanding results show that the Convolutional Neural Network models together with the Cell Merger Approach have a classification accuracy of 88.8% with a standard deviation of 1%, obtaining a sensitivity and specificity of 0.92 and 0.83 respectively. This classification level depicts a robust and accurate model that is comparable to an expert pathologist competencies.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305315",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Cancer",
      "Cervical cancer",
      "Computer science",
      "Convolutional neural network",
      "Electronic engineering",
      "Engineering",
      "Environmental health",
      "Internal medicine",
      "Machine learning",
      "Medicine",
      "Operating system",
      "Papanicolaou stain",
      "Pattern recognition (psychology)",
      "Population",
      "Process (computing)",
      "Sensitivity (control systems)"
    ],
    "authors": [
      {
        "surname": "Martínez-Más",
        "given_name": "José"
      },
      {
        "surname": "Bueno-Crespo",
        "given_name": "Andrés"
      },
      {
        "surname": "Martínez-España",
        "given_name": "Raquel"
      },
      {
        "surname": "Remezal-Solano",
        "given_name": "Manuel"
      },
      {
        "surname": "Ortiz-González",
        "given_name": "Ana"
      },
      {
        "surname": "Ortiz-Reina",
        "given_name": "Sebastián"
      },
      {
        "surname": "Martínez-Cendán",
        "given_name": "Juan-Pedro"
      }
    ]
  },
  {
    "title": "A cooperative coevolutionary optimization design of urban transit network and operating frequencies",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113736",
    "abstract": "The transit network design and frequency setting problem (TNDFSP) is a complex combinatorial optimization problem. Generally, the nature of multiobjective in TNDFSP has not attracted enough attention, and the frequency setting is directly embedded as a subproblem to generate a unique set of frequencies for a given transit network, ignoring trade-off solutions among multiple objectives with different sets of frequencies. In this study, the problem is formulated as a multiobjective model with two conflicting objectives of minimizing passengers’ and operators’ costs. Moreover, we establish two populations to simultaneously optimize networks and frequencies. Also a cooperative coevolutionary multiobjective evolutionary algorithm (CCMOEA) is developed to collaboratively coevolve these two populations along multiple objectives. Unsatisfied demand is embedded into the individual prioritization process, and infeasible individuals can be retained instead of being replaced arbitrarily, driving the evolution to gradually generate more feasible solutions. The proposed CCMOEA is tested on the well-known Mandl’s benchmark. The results show that our algorithm can efficiently produce a comprehensive set of high-quality trade-off solutions. These solutions perform well with lower waiting time, competitive in-vehicle travel time and number of transfers, resulting in lower user costs than previously published results in the same fleet size.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305601",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Economics",
      "Engineering",
      "Epistemology",
      "Evolutionary algorithm",
      "Geodesy",
      "Geography",
      "Management science",
      "Mathematical optimization",
      "Mathematics",
      "Multi-objective optimization",
      "Operating system",
      "Operations research",
      "Philosophy",
      "Prioritization",
      "Process (computing)",
      "Programming language",
      "Public transport",
      "Quality (philosophy)",
      "Set (abstract data type)",
      "Transit (satellite)",
      "Transport engineering"
    ],
    "authors": [
      {
        "surname": "Liang",
        "given_name": "Mingzhang"
      },
      {
        "surname": "Wang",
        "given_name": "Wei"
      },
      {
        "surname": "Dong",
        "given_name": "Changyin"
      },
      {
        "surname": "Zhao",
        "given_name": "De"
      }
    ]
  },
  {
    "title": "Repair equipment allocation problem for a support-and-repair ship on a deep sea: A hybrid multi-criteria decision making and optimization approach",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113658",
    "abstract": "To address the repair equipment allocation problem for a support-and-repair ship on a deep sea, a hybrid multi-criteria decision making and optimization approach is designed. Evidence reasoning approach is used to aggregate the evaluation information of quantitative criteria (i.e., weight and economics) and qualitative criteria (i.e., repair ability, reliability and convenience). Then, a mathematical repair equipment allocation model of a support-and-repair ship is formulated, which is a mixed-integer nonlinear model. A removal strategy based on a greedy algorithm that modifies infeasible solutions is designed to facilitate the use of a genetic algorithm with an elite strategy to address the model above. The proposed solution method using the removal strategy based on the greedy algorithm obtains better solution accuracy and global search performance than three widely used penalty-based methods by several test instances generated randomly. The results of a case study prove that the mathematical model and solution method can effectively obtain the optimal repair equipment allocation solution. The hybrid multi-criteria decision making and optimization approach has certain guiding significance for decision makers to determine their repair equipment allocation strategies for support-and-repair ships on a deep sea in practice.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420304826",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Decision support system",
      "Genetic algorithm",
      "Integer (computer science)",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Operations research",
      "Physics",
      "Power (physics)",
      "Programming language",
      "Quantum mechanics",
      "Reliability (semiconductor)"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Ruijia"
      },
      {
        "surname": "Xie",
        "given_name": "Xinlian"
      },
      {
        "surname": "Yu",
        "given_name": "Wenzhang"
      }
    ]
  },
  {
    "title": "A study on adaptation lightweight architecture based deep learning models for bearing fault diagnosis under varying working conditions",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113710",
    "abstract": "Deep learning models have been widely studied in fault diagnosis recently. A mainstream application is to recognize patterns in spectrograms of faults. However, some common drawbacks still remain as following: a) Preprocess to improve the quality of spectrograms is rarely explored; b) Computing cost of a conventional CNN far exceeds the requirements of fast analysis in industry; c) Adequate labeled data cannot be acquired to train a comprehensive diagnosis model for varying working conditions. In this paper, an Adaptive Logarithm Normalization (ALN) is proposed to realize preprocess considering data distribution, it attempts to improve the quality of spectrograms via eliminating truncation phenomenon and enriching details simultaneously; Meanwhile, simplified lightweight models are built on the basis of present lightweight building blocks to reduce parameters, while maintaining high performances; Furthermore, an adaptation architecture is proposed by integrating Deep Adaptation Network (DAN) idea with simplified lightweight models, aiming at enhancing the generalization capability of models. Experiments have been carried out to implement the proposed methods with two different datasets. The overall success not only proves the methods feasible, but also indicates a possible diagnosis prospect for real industrial scenarios.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305340",
    "keywords": [
      "Adaptation (eye)",
      "Anthropology",
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Deep learning",
      "Fault (geology)",
      "Generalization",
      "Geology",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Normalization (sociology)",
      "Operating system",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Profiling (computer programming)",
      "Seismology",
      "Sociology",
      "Spectrogram"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Jie"
      },
      {
        "surname": "Tang",
        "given_name": "Tang"
      },
      {
        "surname": "Chen",
        "given_name": "Ming"
      },
      {
        "surname": "Wang",
        "given_name": "Yi"
      },
      {
        "surname": "Wang",
        "given_name": "Kesheng"
      }
    ]
  },
  {
    "title": "A novel method based on symbolic regression for interpretable semantic similarity measurement",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113663",
    "abstract": "The problem of automatically measuring the degree of semantic similarity between textual expressions is a challenge that consists of calculating the degree of likeness between two text fragments that have none or few features in common according to human judgment. In recent times, several machine learning methods have been able to establish a new state-of-the-art regarding the accuracy, but none or little attention has been paid to their interpretability, i.e. the extent to which an end-user could be able to understand the cause of the output from these approaches. Although such solutions based on symbolic regression already exist in the field of clustering, we propose here a new approach which is being able to reach high levels of interpretability without sacrificing accuracy in the context of semantic textual similarity. After a complete empirical evaluation using several benchmark datasets, it is shown that our approach yields promising results in a wide range of scenarios.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420304875",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Genetic programming",
      "Image (mathematics)",
      "Machine learning",
      "Mathematics",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Regression",
      "Semantic similarity",
      "Similarity (geometry)",
      "Statistics",
      "Symbolic regression"
    ],
    "authors": [
      {
        "surname": "Martinez-Gil",
        "given_name": "Jorge"
      },
      {
        "surname": "Chaves-Gonzalez",
        "given_name": "Jose M."
      }
    ]
  },
  {
    "title": "Fair comparison of skin detection approaches on publicly available datasets",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113677",
    "abstract": "Skin detection is the process of discriminating skin and non-skin regions in a digital image and it is widely used in several applications ranging from hand gesture analysis to track body parts and face detection. Skin detection is a challenging problem which has drawn extensive attention from the research community in the context of expert and intelligent systems, nevertheless a fair comparison among approaches is very difficult due to the lack of a common benchmark and a unified testing protocol. In the recent era, the success of deep convolutional neural network (CNN) has strongly influenced the field of image segmentation and gave us various successful models to date. Anyway, due to the lack of large ground truth for skin detection only few works have addressed the skin detection problem using CNN models. In this work, we investigate the most recent researches in this field, and we propose a fair comparison among approaches using several different datasets. The major contributions of this work are (i) an exhaustive literature review of skin color detection approaches and a comparison of approaches that can be useful to researchers and practitioners to select the most suitable method for their application, (ii) the collection and examination of many datasets with ground truth for skin detection that can be useful to produce a training set for CNN models, (iii) a framework to evaluate and combine different skin detector approaches, whose source code is made freely available for future research, and (iv) an extensive experimental comparison among several recent methods which have also been used to define an ensemble that works well in many different problems. Experiments are carried out in 10 different datasets including more than 10,000 labelled images: experimental results confirm that the best method here proposed obtains a very good performance with respect to other stand-alone approaches, without requiring ad hoc parameter tuning. A MATLAB version of the framework for testing and of the methods proposed in this paper will be freely available from https://github.com/LorisNanni.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305017",
    "keywords": [
      "Alternative medicine",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Convolutional neural network",
      "Face (sociological concept)",
      "Face detection",
      "Facial recognition system",
      "Field (mathematics)",
      "Geodesy",
      "Geography",
      "Ground truth",
      "Machine learning",
      "Mathematics",
      "Medicine",
      "Operating system",
      "Paleontology",
      "Pathology",
      "Pattern recognition (psychology)",
      "Process (computing)",
      "Protocol (science)",
      "Pure mathematics",
      "Segmentation",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Lumini",
        "given_name": "Alessandra"
      },
      {
        "surname": "Nanni",
        "given_name": "Loris"
      }
    ]
  },
  {
    "title": "Forecasting with time series imaging",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113680",
    "abstract": "Feature-based time series representations have attracted substantial attention in a wide range of time series analysis methods. Recently, the use of time series features for forecast model averaging has been an emerging research focus in the forecasting community. Nonetheless, most of the existing approaches depend on the manual choice of an appropriate set of features. Exploiting machine learning methods to extract features from time series automatically becomes crucial in state-of-the-art time series analysis. In this paper, we introduce an automated approach to extract time series features based on time series imaging. We first transform time series into recurrence plots, from which local features can be extracted using computer vision algorithms. The extracted features are used for forecast model averaging. Our experiments show that forecasting based on automatically extracted features, with less human intervention and a more comprehensive view of the raw time series data, yields highly comparable performances with the best methods in the largest forecasting competition dataset (M4) and outperforms the top methods in the Tourism forecasting competition dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305042",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Data mining",
      "Feature (linguistics)",
      "Focus (optics)",
      "Linguistics",
      "Machine learning",
      "Optics",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Series (stratigraphy)",
      "Time series"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Xixi"
      },
      {
        "surname": "Kang",
        "given_name": "Yanfei"
      },
      {
        "surname": "Li",
        "given_name": "Feng"
      }
    ]
  },
  {
    "title": "Cross-language text alignment: A proposed two-level matching scheme for plagiarism detection",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113718",
    "abstract": "The exponential growth of documents in various languages throughout the web, along with the availability of several editing and translation tools have made the cross-language plagiarism detection a challenging issue. Regarding its high importance, the present study focuses on the task of cross-language text alignment also known as detailed analysis which works on the outputs of the source retrieval step of cross-language plagiarism detection systems. The paper proposes a two-level matching approach with the aim of considering both syntactic and semantic information to align plagiarism fragments from the source and suspicious documents, accurately. At the first level, a vector space model which employs a multilingual word embeddings based dictionary and a local weighting technique is used in order to extract a minimal set of highly potential candidate fragment pairs rather than considering all possible pairs of fragments. This step also contains a dynamic expansion technique to cover more candidate pairs aiming at improving the system’s recall. It is followed by a more precise algorithm that examines the candidate pairs at the sentence level using a graph-of-words representation of text. As a result, by modelling both the words and their relationships, an acceptable increase in the system’s precision which is the goal of the second level is also observed. To identify evidence of plagiarism, i.e. potential cases of unauthorized text reuse, the algorithm tries to find maximum cliques from the match graph of source and suspicious texts. With this two-level investigation, the approach is capable to discriminate true plagiarism cases from the original text. The experimental results on different datasets such as PAN-PC-11, PAN-PC-12, and SemEval-2017 show that the proposed cross-language text alignment approach significantly outperforms the state-of-the-art models and can be fed into an expert system for further improvement of cross-language plagiarism detection. The source codes are publicly available on GitHub 1 1 https://github.com/MeysamRoostaee/CLPD-DetailAnalysis/archive/master.zip , for the purposes of reproducible research.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742030542X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Graph",
      "Information retrieval",
      "Linguistics",
      "Matching (statistics)",
      "Mathematics",
      "Medicine",
      "Natural language processing",
      "Philosophy",
      "Plagiarism detection",
      "Precision and recall",
      "Programming language",
      "Radiology",
      "Sentence",
      "Set (abstract data type)",
      "Source text",
      "Statistics",
      "Theoretical computer science",
      "Weighting",
      "Word (group theory)"
    ],
    "authors": [
      {
        "surname": "Roostaee",
        "given_name": "Meysam"
      },
      {
        "surname": "Fakhrahmad",
        "given_name": "Seyed Mostafa"
      },
      {
        "surname": "Sadreddini",
        "given_name": "Mohammad Hadi"
      }
    ]
  },
  {
    "title": "Probabilistic optimization algorithms for real-coded problems and its application in Latin hypercube problem",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113589",
    "abstract": "This paper proposes a novel optimization algorithm for read-coded problems called the Probabilistic Optimization Algorithm (POA). In the proposed algorithm, rather than a binary or integer, a probabilistic representation is used for the individuals. Each individual in the proposed algorithm is a probability density function and is capable of representing the entire search space simultaneously. In the search process, each solution performs as a local search and climbs the local optima, and at the same time, the interaction among the probabilistic individuals in the population offers a global search. The parameters of the proposed algorithm are studied in this paper and their effect on the search process is presented. A structured population is proposed for the algorithm and the effect of different structures is analyzed. The algorithm is used to solve Latin Hyper-cube problem and experimental studies suggest promising results. Different benchmark functions are also used to test the algorithm and results are presented. The analyses suggest that the improvement is more significant for large scale problems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420304139",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Binary search algorithm",
      "Computer science",
      "Demography",
      "Geodesy",
      "Geography",
      "Latin hypercube sampling",
      "Local search (optimization)",
      "Mathematical optimization",
      "Mathematics",
      "Monte Carlo method",
      "Population",
      "Probabilistic logic",
      "Search algorithm",
      "Sociology",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Najaran",
        "given_name": "Mohammad Hassan Tayarani"
      },
      {
        "surname": "Tootounchi",
        "given_name": "Mohammad Reza Akbarzadeh"
      }
    ]
  },
  {
    "title": "Optimization-based automated unsupervised classification method: A novel approach",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113735",
    "abstract": "Unsupervised classification algorithms are methods for the analysis of remotely sensed images. Since these methods do not include a training phase, they require less time to apply and are more practical to use. Traditional unsupervised classification methods work with parameters given by the user, such as the number of classes, the stop criterion or the number of iterations of the algorithm. Determining the optimum values of these parameters to obtain successful classification result is a major problem. In this study, we propose two new methods, the weighted density based optimized classification method (DBOC-Weighted) and the automatic density based optimized classification method (DBOC-Automatic). Both work automatically without the need for parameters from the user, but the DBOC-Weighted only requires layer weights. These methods consist of data range expansion, useful data selection, segmentation and optimization stages, and perform the classification automatically. Both create new layers of data using remotely sensed images. After creating the initial classes based on density from all the data layers, the results are created by optimizing all classes in terms of quality indices. Four Sentinel 2 images are used to test the performance of the proposed methods. These images are selected from regions that have different geographical, climatic and vegetation properties. The results obtained are compared with the unsupervised classification methods frequently used in the literature. The accuracy analysis results show that the proposed classification algorithms produce satisfactory accuracy compared to the results of other algorithms. The results show that the proposed methods can be used successfully in the creation of expert and intelligent analysis systems, by eliminating user-induced error in the analysis of remotely sensed images. Thus, smart analysis tools can be created so that users from various professional disciplines can easily use them without being image processing specialists.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305595",
    "keywords": [
      "Artificial intelligence",
      "Composite material",
      "Computer science",
      "Data mining",
      "Machine learning",
      "Materials science",
      "Pattern recognition (psychology)",
      "Range (aeronautics)",
      "Segmentation",
      "Selection (genetic algorithm)"
    ],
    "authors": [
      {
        "surname": "Kucuk Matci",
        "given_name": "Dilek"
      },
      {
        "surname": "Avdan",
        "given_name": "Uğur"
      }
    ]
  },
  {
    "title": "Improving neighbor-based collaborative filtering by using a hybrid similarity measurement",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113651",
    "abstract": "Memory-based collaborative filtering is one of the recommendation system methods used to predict a user’s rating or preference by exploring historic ratings, but without incorporating any content information about users or items. It can be either item-based or user-based. Taking item-based Collaborative Filtering (CF) as an example, the way it makes predictions is accomplished in 2 steps: first, it selects based on pair-wise similarities a number of most similar items to the predicting item from those that the user has already rated on. Second, it aggregates the user’s opinions on those most similar items to predict a rating on the predicting item. Thus, similarity measurement determines which items are similar, and plays an important role on how accurate the predictions are. Many studies have been conducted on memory-based CFs to improve prediction accuracy, but none of them have achieved better prediction accuracy than state-of-the-art model-based CFs. In this paper, we proposed a new approach that combines both structural and rating-based similarity measurement. We found that memory-based CF using combined similarity measurement can achieve better prediction accuracy than model-based CFs in terms of lower MAE and reduce memory and time by using less neighbors than traditional memory-based CFs on MovieLens and Netflix datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420304759",
    "keywords": [
      "Artificial intelligence",
      "Collaborative filtering",
      "Computer science",
      "Data mining",
      "Image (mathematics)",
      "Information retrieval",
      "Machine learning",
      "Mathematics",
      "MovieLens",
      "Preference",
      "Recommender system",
      "Similarity (geometry)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Dawei"
      },
      {
        "surname": "Yih",
        "given_name": "Yuehwern"
      },
      {
        "surname": "Ventresca",
        "given_name": "Mario"
      }
    ]
  },
  {
    "title": "Electric vehicle routing problem with non-linear charging and load-dependent discharging",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113714",
    "abstract": "We propose a three-index formulation for E-VRP with Non-Linear charging and Load-Dependent discharging (E-VRP-NL-LD), and an Adaptive Large Neighborhood Search (ALNS) algorithm to solve the E-VRP-NL- LD and E-VRP-NL-LD with Capacitated Charging Stations (E-VRP-NL-LD-CCS). Existing implementations of EVRP duplicate charging station nodes which enables the modelling of EVRP using extended VRP formulations. Two limitations of such an approach are: (i) the number of such duplications is not known a priori, and (ii) the size of the problem increases. In our formulation, we allow multiple visits to a charging station without duplicating nodes. We propose five new operators for ALNS which are tested on 120 instances each of E-VRP-NL and E-VRP-NL-LD, and 80 instances of E-VRP-NL-LD-CCS. Results show that our ALNS outperforms the existing algorithms improving the solution in 63% of the instances and matching the best known solution in 31% of the instances. Results also show that considering load-dependent discharge is critical to optimally solve E-VRP.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305388",
    "keywords": [
      "A priori and a posteriori",
      "Algorithm",
      "Computer network",
      "Computer science",
      "Electric vehicle",
      "Epistemology",
      "Mathematical optimization",
      "Mathematics",
      "Philosophy",
      "Physics",
      "Power (physics)",
      "Quantum mechanics",
      "Routing (electronic design automation)",
      "Vehicle routing problem"
    ],
    "authors": [
      {
        "surname": "Kancharla",
        "given_name": "Surendra Reddy"
      },
      {
        "surname": "Ramadurai",
        "given_name": "Gitakrishnan"
      }
    ]
  },
  {
    "title": "A revision on multi-criteria decision making methods for multi-UAV mission planning support",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113708",
    "abstract": "Over the last decade, Unmanned Aerial Vehicles (UAVs) have been extensively used in many commercial applications due to their manageability and risk avoidance. One of the main problems considered is the mission planning for multiple UAVs, where a solution plan must be found satisfying the different constraints of the problem. This problem has multiple variables that must be optimized simultaneously, such as the makespan, the cost of the mission or the risk. Therefore, the problem has a lot of possible optimal solutions, and the operator must select the final solution to be executed among them. In order to reduce the workload of the operator in this decision process, a Decision Support System (DSS) becomes necessary. In this work, a DSS consisting of ranking and filtering systems, which order and reduce the optimal solutions, has been designed. With regard to the ranking system, a wide range of Multi-Criteria Decision Making (MCDM) methods, including some fuzzy MCDM, are compared on a multi-UAV mission planning scenario, in order to study which method could fit better in a multi-UAV decision support system. Expert operators have evaluated the solutions returned, and the results show, on the one hand, that fuzzy methods generally achieve better average scores, and on the other, that all of the tested methods perform better when the preferences of the operators are biased towards a specific variable, and worse when their preferences are balanced. For the filtering system, a similarity function based on the proximity of the solutions has been designed, and on top of that, a threshold is tuned empirically to decide how to filter solutions without losing much of the hypervolume of the space of solutions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305327",
    "keywords": [
      "Artificial intelligence",
      "Business",
      "Computer science",
      "Decision support system",
      "Engineering",
      "Operations research",
      "Process management"
    ],
    "authors": [
      {
        "surname": "Ramirez-Atencia",
        "given_name": "Cristian"
      },
      {
        "surname": "Rodriguez-Fernandez",
        "given_name": "Victor"
      },
      {
        "surname": "Camacho",
        "given_name": "David"
      }
    ]
  },
  {
    "title": "A progressive hybrid set covering based algorithm for the traffic counting location problem",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113641",
    "abstract": "This work tackles the Traffic Counting Location Problem (TCLP), where we aim finding the best number and location of counting stations to cover a road network in order to obtain its traffic flows. It is important to reduce deployment, maintenance and operation costs of traffic stations. We propose a progressive hybrid algorithm based on exact, heuristic and hybrid approaches embedded on a set covering framework to solve the TCLP. This algorithm employs a simple and innovative concept which has not yet been explored in the literature. Twenty-six real-world instances obtained from the Brazilian states are used in the computational experiments and the results show that the TCLP can be solved more efficiently than previous approaches, with 84 % of the instances solved optimally, and three new best known solutions found.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420304656",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Constraint logic programming",
      "Constraint satisfaction",
      "Cover (algebra)",
      "Engineering",
      "Epistemology",
      "Heuristic",
      "Hybrid algorithm (constraint satisfaction)",
      "Mathematical optimization",
      "Mathematics",
      "Mechanical engineering",
      "Operating system",
      "Philosophy",
      "Probabilistic logic",
      "Programming language",
      "Set (abstract data type)",
      "Set cover problem",
      "Simple (philosophy)",
      "Software deployment"
    ],
    "authors": [
      {
        "surname": "Vieira",
        "given_name": "Bruno S."
      },
      {
        "surname": "Ferrari",
        "given_name": "Thayse"
      },
      {
        "surname": "Ribeiro",
        "given_name": "Glaydston M."
      },
      {
        "surname": "Bahiense",
        "given_name": "Laura"
      },
      {
        "surname": "Orrico Filho",
        "given_name": "Romulo D."
      },
      {
        "surname": "Abramides",
        "given_name": "Carlos Alberto"
      },
      {
        "surname": "Rosa Campos Júnior",
        "given_name": "Nilo Flavio"
      }
    ]
  },
  {
    "title": "Evaluation of customer behavior with temporal centrality metrics for churn prediction of prepaid contracts",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113553",
    "abstract": "The telecommunication industry is a saturated market where a proper implementation of a retention campaign is critical to be competitive, since retaining a customer is cheaper than attracting a new one. Hence, it is crucial to detect customer behavioral patterns and define accurate approaches to predict potential churners. Multiple researchers have used binary classification methods to predict churn of customers. Some of them verify that customers’ social relationships influence the decision of changing the operator. We propose a novel method to extract the dynamic relevance of each customer using social network analysis techniques with a binary classification method called similarity forests. The dynamic importance of each customer is determined by applying various centrality metrics over temporal graphs, to represent the relationships between customers and to extract behavioral patterns of churners and non-churners. These relationships are established in a temporal graph using the call detail records (CDR) of telco’s customers. In this paper, we compare the performance of different centrality metrics applied over two types of temporal graphs: Time-Order Graph and Aggregated Static Graph.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420303778",
    "keywords": [
      "Artificial intelligence",
      "Centrality",
      "Combinatorics",
      "Computer science",
      "Data mining",
      "Graph",
      "Image (mathematics)",
      "Law",
      "Machine learning",
      "Mathematics",
      "Political science",
      "Relevance (law)",
      "Similarity (geometry)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Calzada-Infante",
        "given_name": "Laura"
      },
      {
        "surname": "Óskarsdóttir",
        "given_name": "María"
      },
      {
        "surname": "Baesens",
        "given_name": "Bart"
      }
    ]
  },
  {
    "title": "Stock returns prediction using kernel adaptive filtering within a stock market interdependence approach",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113668",
    "abstract": "Stock returns are continuously generated by different data sources and depend on various factors such as financial policies and national economic growths. Stock returns prediction, unlike traditional regression, requires consideration of both the sequential and interdependent nature of financial time-series. This work uses a two-stage approach, using kernel adaptive filtering (KAF) within a stock market interdependence approach to sequentially predict stock returns. Thus, unlike traditional KAF formulations, prediction uses not only their local models but also the individual local models learned from other stocks, enhancing prediction accuracy. The enhanced KAF plus market interdependence framework has been tested on 24 different stocks from major economies. The enhanced approach obtains higher sharpe ratio when compared with KAF-based methods, long short-term memory, and autoregressive-based models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420304929",
    "keywords": [
      "Autoregressive model",
      "Biology",
      "Computer science",
      "Econometrics",
      "Economics",
      "Engineering",
      "Financial economics",
      "Horse",
      "Interdependence",
      "Law",
      "Mechanical engineering",
      "Paleontology",
      "Political science",
      "Portfolio",
      "Sharpe ratio",
      "Stock (firearms)",
      "Stock market"
    ],
    "authors": [
      {
        "surname": "Garcia-Vega",
        "given_name": "Sergio"
      },
      {
        "surname": "Zeng",
        "given_name": "Xiao-Jun"
      },
      {
        "surname": "Keane",
        "given_name": "John"
      }
    ]
  },
  {
    "title": "Incorporating part-whole hierarchies into fully convolutional network for scene parsing",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113662",
    "abstract": "In this paper, a new approach to scene parsing is proposed which integrates part-whole hierarchies relationship in the last feature map to assign a semantic class label to each pixel. Recently, deep learning-based approaches have had a great impact on scene parsing. However, these methods could not preserve the spatial information about the high-level (or mid-level) features. Hence, Hinton, one of the fathers of deep learning, introduced the capsule concept to encode pose information such as orientation. All of the capsules which have a similar pose matrix value are grouped to form a parent capsule. However, their work has two challenges: 1) the extensive time required to perform dynamic routing agreement to obtain the routing coefficient and 2) the variation of the appearance and the spatial hierarchies between part capsules and their corresponding parent are not encoded. In this study, to consider these challenges, the general Hough transform (GHT) and tensor normal distribution are utilized to propose a novel capsule concept. In this case, each capsule has k offset vectors for each semantic class. The offset vectors are oriented from the capsule to the k other capsules which have an effective role in assigning that capsule to a specific semantic class. The problem formulation is proposed such that evaluating the approach on large datasets is feasable. Also, a new score function is designed to accumulate the vote’s strengths for capsule class estimation. To do so, we use tensor normal distribution in which the covariance matrix is defined as the Kronecker product of the capsule feature covariance and the between-capsule covariance. The proposed approach, for the first time, encodes the relations between part capsules to vote to a whole capsule through the between-capsule covariance matrix. To evaluate our proposed approach, it is applied to SiftFlow, NYUD-v2 and PASCAL VOC 2012 datasets. The results show that our approach achieves superior performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420304863",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Hyperparameter",
      "Parsing",
      "Pattern recognition (psychology)",
      "Pixel"
    ],
    "authors": [
      {
        "surname": "Abbasi",
        "given_name": "Karim"
      },
      {
        "surname": "Razzaghi",
        "given_name": "Parvin"
      }
    ]
  },
  {
    "title": "Learning competitive channel-wise attention in residual network with masked regularization and signal boosting",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113591",
    "abstract": "Image classification is an essential component of expert and intelligent systems. The accuracy and efficiency of image classification algorithms significantly affect the performance of related expert systems. Residual network (ResNet) shows strong superiority in image modeling. However, it has also been proved to be low-efficient. In this study, we proposed a novel channel-wise attention mechanism to alleviate the redundancy of ResNet. We introduce the identity mappings into the scope of channel relationship modeling. In this way, the identity mapping can join the optimized process of self-supplementary modeling. Besides, we present the masked regularization for squeezed signals and enhance the robustness of channel-relation encoding. Finally, we verify the performance of the proposed method. The experiments are carried out on the datasets CIFAR-10, CIFAR-100, SVHN, and ImageNet. The proposed method effectively improves the performance of image classification-related expert systems. Moreover, our approach is hot-swappable, has broad applicability, so it has great practical significance for experts and intelligent systems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420304152",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Boosting (machine learning)",
      "Chemistry",
      "Computer science",
      "Data mining",
      "Gene",
      "Machine learning",
      "Operating system",
      "Pattern recognition (psychology)",
      "Redundancy (engineering)",
      "Regularization (linguistics)",
      "Residual",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Luo",
        "given_name": "Mingnan"
      },
      {
        "surname": "Wen",
        "given_name": "Guihua"
      },
      {
        "surname": "Hu",
        "given_name": "Yang"
      },
      {
        "surname": "Dai",
        "given_name": "Dan"
      },
      {
        "surname": "Ma",
        "given_name": "Jiajiong"
      }
    ]
  },
  {
    "title": "Extracting highlights of scientific articles: A supervised summarization approach",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113659",
    "abstract": "Scientific articles can be annotated with short sentences, called highlights, providing readers with an at-a-glance overview of the main findings. Highlights are usually manually specified by the authors. This paper presents a supervised approach, based on regression techniques, with the twofold aim at automatically extracting highlights of past articles with missing annotations and simplifying the process of manually annotating new articles. To this end, regression models are trained on a variety of features extracted from previously annotated articles. The proposed approach extends existing extractive approaches by predicting a similarity score, based on n-gram co-occurrences, between article sentences and highlights. The experimental results, achieved on a benchmark collection of articles ranging over heterogeneous topics, show that the proposed regression models perform better than existing methods, both supervised and not.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420304838",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Automatic summarization",
      "Benchmark (surveying)",
      "Computer science",
      "Data mining",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Information retrieval",
      "Machine learning",
      "Mathematics",
      "Natural language processing",
      "Operating system",
      "Process (computing)",
      "Regression",
      "Similarity (geometry)",
      "Statistics",
      "Supervised learning",
      "Variety (cybernetics)"
    ],
    "authors": [
      {
        "surname": "Cagliero",
        "given_name": "Luca"
      },
      {
        "surname": "La Quatra",
        "given_name": "Moreno"
      }
    ]
  },
  {
    "title": "Genetic algorithm based local and global spectral features extraction for ear recognition",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113639",
    "abstract": "Identification systems based on biometric features are becoming increasingly important. One of the most common biometric features is the ear. The accuracy of these systems is heavily dependent on the characteristics extracted from them. In this paper, an appropriate combination of local and global features in the frequency domain is extracted as unique features of the ear region. In the proposed approach, at first the image quality is improved by Contrast-limited Adaptive Histogram Equalization. Then, the global features of the ear region are extracted by applying the Gabor-Zernike operator to the whole image and its non-overlapping blocks. In addition, to extract of local features, the local phase quantization operator is used on the original image of the ear region. Then, the optimum combination of global and local features is selected using Genetic Algorithm. Finally, the nearest neighbor classifier with Canberra distance is used to identify users. The proposed approach is evaluated using three databases, i.e. USTB-1, IIT125 and IIT221. The recognition rate of 100%, 99.2% and 97.13%, is reported on these databases, respectively. The obtained results show that the proposed approach performs better than existing ear recognition methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420304632",
    "keywords": [
      "Artificial intelligence",
      "Biometrics",
      "Classifier (UML)",
      "Computer science",
      "Histogram",
      "Image (mathematics)",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Vector quantization",
      "Wavefront",
      "Zernike polynomials"
    ],
    "authors": [
      {
        "surname": "Sajadi",
        "given_name": "Shabbou"
      },
      {
        "surname": "Fathi",
        "given_name": "Abdolhossein"
      }
    ]
  },
  {
    "title": "New look at the inconsistency analysis in the pairwise-comparisons-based prioritization problems",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113549",
    "abstract": "The main goal of the multiple-criteria decision analysis is to create a ranking of available decision-alternatives, which is conventionally achieved by estimation of their priority weights. Among the most widely used prioritization methodologies is the Analytic Hierarchy Process (AHP). The prioritization methods that are used under the AHP scheme are based upon pairwise comparison matrix (PCM) that contains the decision-makers' evaluations of the priority-weights-ratios. In praxis, these evaluations are usually erroneous and as a result, the PCM is inconsistent. Because serious inconsistency makes the data contained in the PCM useless it is important to distinguish between useful and useless PCMs. In this paper, a novel principle for the assessment of the inconsistency-indices-usefulness is introduced. It is proposed to take into account their relationship with the correctness of the final ranking of the alternatives – a crucial and natural criterion for the performance of the indices. To find the best one in this regard, various inconsistency indices (both well-known and new) are compared via simulation experiments. Next, a new PCM-acceptance method is proposed. In this method, regression models that relate the probability of incorrect-ranking occurrence to the indices values, are used to assist the decision-making process. Those models enable decision-makers to accept or reject the PCMs, based on the assessment of incorrect-ranking risk. Thus, in contrast to the procedure still supported by the classical AHP, that new approach is based on sound statistical concepts and complies with the modern risk-analysis-based uncertain-decision-making.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420303730",
    "keywords": [
      "Algorithm",
      "Analytic hierarchy process",
      "Artificial intelligence",
      "Computer science",
      "Correctness",
      "Data mining",
      "Engineering",
      "Machine learning",
      "Management science",
      "Mathematics",
      "Operations research",
      "Pairwise comparison",
      "Prioritization",
      "Ranking (information retrieval)"
    ],
    "authors": [
      {
        "surname": "Grzybowski",
        "given_name": "Andrzej Z."
      },
      {
        "surname": "Starczewski",
        "given_name": "Tomasz"
      }
    ]
  },
  {
    "title": "Classification of EEG signals produced by musical notes as stimuli",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113507",
    "abstract": "In this paper, we present the classification of electroencephalograph (EEG) signals produced by the first-octave musical notes of the piano as stimuli. The EEG classification of musical notes is attempted for the first time, to the best of our knowledge. This type of classification could be applied towards the development of Brain-Computer Interfaces (BCIs) for the composition of music via thought as well as the definition of mappings between different stimuli for Sensory Substitution Devices (SSDs) that are based on their actual impact on brain signals and thus serve better the purpose of SSDs, which is to translate between senses at the perceptual level. Event-Related Spectral Perturbations (ERSP) are extracted as features and are fed into a Support Vector Machine (SVM) classifier. Our aim was to classify musical notes as C, C#, D, D#, E, F, F#, G, G#, A, A#, B and we have achieved it with an average accuracy of 70%.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420303316",
    "keywords": [
      "Acoustics",
      "Art",
      "Artificial intelligence",
      "Brain–computer interface",
      "Classifier (UML)",
      "Computer science",
      "Electroencephalography",
      "Event-related potential",
      "Musical",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Perception",
      "Physics",
      "Piano",
      "Psychology",
      "Speech recognition",
      "Support vector machine",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Tsekoura",
        "given_name": "Konstantina"
      },
      {
        "surname": "Foka",
        "given_name": "Amalia"
      }
    ]
  },
  {
    "title": "A HITS-based model for facility location decision",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113616",
    "abstract": "This paper presents a model to the facility location problem which incorporates both location characteristics and rival effects within a business cluster. Location characteristics in the model are predefined selection criteria such as rent, distance, safety and size. Rival effects represent the competition and collaboration relationship among close facility locations in the business cluster where candidate locations are located. The model presented uses data envelopment analysis (DEA) and weighted hyperlink-induced topic search (HITS) algorithm. DEA determines the efficient and inefficient locations as well as benchmarking relationship. The weighted HITS algorithm with the distance parameter considers the rival effects among locations to identify hubs and authorities. The applicability of our proposed model is demonstrated with a case study that rank apartments in the University area.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420304401",
    "keywords": [
      "Artificial intelligence",
      "Benchmarking",
      "Biology",
      "Business",
      "Combinatorics",
      "Competition (biology)",
      "Computer science",
      "Data envelopment analysis",
      "Data mining",
      "Ecology",
      "Facility location problem",
      "Location model",
      "Marketing",
      "Mathematical optimization",
      "Mathematics",
      "Operations research",
      "Rank (graph theory)",
      "Selection (genetic algorithm)"
    ],
    "authors": [
      {
        "surname": "Fang",
        "given_name": "Jin"
      },
      {
        "surname": "Partovi",
        "given_name": "Fariborz Y."
      }
    ]
  },
  {
    "title": "Information hiding in motion data of virtual characters",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113516",
    "abstract": "As an import branch of information security, image/video-based information hiding system has been investigated extensively in the past few decades. Meanwhile, 3D applications are increasingly popular in various areas such as animation, virtual, and augmented reality. This work explores the feasibility of data hiding in 3D applications and proposes a novel method to encrypt a message in the motion data of 3D virtual character. For the human visual system, it is challenging to notice the subtle details in dynamic scenarios at a fast frame rate. Therefore, the message encryption in 3D character movements offers an intuitive solution to information hiding. Our work can be divided into two stages. The first stage optimally selects a group of joints which introduces minimal modification on the global presentation of the character motion to maintain the maximum visual consistency between the embedding motion data and the original motion data. The second stage is to encrypt the secret data based on the Sudoku magic matrix and hide the data in the optimal selection of joints. The experimental results show that our system has a high hiding capacity and excellent visual quality.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420303407",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Human–computer interaction",
      "Image (mathematics)",
      "Information hiding",
      "Motion (physics)"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Xu"
      },
      {
        "surname": "Guo",
        "given_name": "Shihui"
      },
      {
        "surname": "Xing",
        "given_name": "Gao"
      },
      {
        "surname": "Liao",
        "given_name": "Minghong"
      },
      {
        "surname": "Chang",
        "given_name": "Chin-Chen"
      },
      {
        "surname": "Yau",
        "given_name": "Wei-Chuen"
      }
    ]
  },
  {
    "title": "Enhanced Crow Search Algorithm for Feature Selection",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113572",
    "abstract": "The crow search algorithm (CSA) is a recent metaheuristic inspired by the intelligent group behavior of crows. It has attracted the attention of many researchers because of its simplicity and easy implementation. However, it suffers from premature convergence because of its ability to balance between exploration and exploitation is weak. Therefore, we investigate in this paper, an enhanced version of CSA called by us ECSA as a wrapper feature selection method to extract the best feature subsets. This enhancement achieved by introducing three modifications to the original CSA to improve its performance. Firstly, we propose an adaptive awareness probability to enhance the balance between exploration and exploitation. Secondly, we replace the random choice of the crow to follow by the dynamic local neighborhood to guide the local search. Thirdly, we introduce a novel global search strategy to increase the global exploration capability of the crow. The performance of ECSA is measured using three performance metrics and statistical significance over 16 datasets from the UCI repository. The obtained results are compared with those of the original CSA and some state-of-the-art techniques in the literature. Experimental results showed that ECSA presents a better convergence speed and a better-quality solution.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420303961",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Convergence (economics)",
      "Data mining",
      "Economic growth",
      "Economics",
      "Feature (linguistics)",
      "Feature selection",
      "Linguistics",
      "Machine learning",
      "Metaheuristic",
      "Philosophy",
      "Selection (genetic algorithm)"
    ],
    "authors": [
      {
        "surname": "Ouadfel",
        "given_name": "Salima"
      },
      {
        "surname": "Abd Elaziz",
        "given_name": "Mohamed"
      }
    ]
  },
  {
    "title": "A multi-kernel method of measuring adaptive similarity for spectral clustering",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113570",
    "abstract": "Accurate representation of similarities between data points is an important determinant in the success of many clustering approaches. Previous studies have shown that kernel methods are effective in this representation. However, using multi-kernels to merge probabilistic neighborhoods in data still needs further research. In this paper, a multi-kernel method of measuring adaptive similarity for spectral clustering is proposed. Kernels with more accurate adaptive similarity measure on the data are assigned bigger weights and an optimum combined kernel that truly reflects the internal structure of the data points is obtained. The proposed method ascribes adaptive and optimal neighbors to each data point based on the local structure using the combined kernels. The combined similarity measure and data clustering are learnt simultaneously to obtain optimal clustering results. We rank constraint the Laplacian matrix of the data similarity matrix to ensure that the connected components in the similarity matrix are exactly equal to the cluster number. The presented technique is significant in the sense that it is able to search the underlying similarity relationships amongst data points and is robust to complex data. Compared with other state-of-the-art spectral clustering methods, our proposed method achieves better performance in terms of NMI and accuracy in experiments performed on both synthetic and real datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420303948",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Combinatorics",
      "Computer science",
      "Data mining",
      "Data point",
      "Image (mathematics)",
      "Kernel (algebra)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Similarity (geometry)",
      "Similarity measure",
      "Spectral clustering"
    ],
    "authors": [
      {
        "surname": "Monney",
        "given_name": "Augustine"
      },
      {
        "surname": "Zhan",
        "given_name": "Yongzhao"
      },
      {
        "surname": "Jiang",
        "given_name": "Zhen"
      },
      {
        "surname": "Benuwa",
        "given_name": "Ben-Bright"
      }
    ]
  },
  {
    "title": "Stability analysis of the human behavior-based particle swarm optimization without stagnation assumption",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113638",
    "abstract": "Human behaviour-based particle swarm optimization (HPSO) is an improved particle swarm optimization (PSO) algorithm, which can effectively increase the diversity of population, convergence speed and precision by changing particles’ flight schema. In this paper, the first- and second-order stabilities of HPSO on the assumption that the personal optimum, the global optimum and the global worst particle are mutual independence during the whole evolutionary process, while the expectations and variances of particles’ positions vary with time so that our calculations do not require the stagnation assumption. The first- and second-order stabilities of HPSO are expressed by stochastic recurrence relation. The convergence boundaries of HPSO are obtained, and which show that the stabilities of HPSO are independent of the expectations and variances of the personal best and the global best. In addition, the reason for choosing the coefficient of the impelled/penalized term in HPSO is analysed, and the results of simulation experiments further confirm the theoretical analysis. Finally, HPSO is used to solve four real-world engineering design problems. Experimental results further confirm the theoretic results and illustrate that the performance of HPSO is promising and competitive.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420304620",
    "keywords": [
      "Computer science",
      "Convergence (economics)",
      "Demography",
      "Economic growth",
      "Economics",
      "Global optimization",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Multi-swarm optimization",
      "Particle swarm optimization",
      "Population",
      "Sociology",
      "Stability (learning theory)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Hao"
      },
      {
        "surname": "Zhang",
        "given_name": "XuWei"
      },
      {
        "surname": "Liang",
        "given_name": "Hong"
      },
      {
        "surname": "Tu",
        "given_name": "LiangPing"
      }
    ]
  },
  {
    "title": "Remote tracking of Parkinson's Disease progression using ensembles of Deep Belief Network and Self-Organizing Map",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113562",
    "abstract": "Parkinson’s Disease (PD) is one of the most prevalent neurological disorders characterized by impairment of motor function. Early diagnosis of PD is important for initial treatment. This paper presents a newly developed method for application in remote tracking of PD progression. The method is based on deep learning and clustering approaches. Specifically, we use the Deep Belief Network (DBN) and Support Vector Regression (SVR) to predict Unified Parkinson's Disease Rating Scale (UPDRS). The DBN prediction models were developed by different epoch numbers. We use a clustering approach, namely, Self-Organizing Map (SOM), to improve the accuracy and scalability of prediction. We evaluate our method on a real-world PD dataset. In all, nine clusters were detected from the data with the best SOM map quality for clustering, and for each cluster, a DBN was developed with a specific number of epochs. The results of the DBN prediction models were integrated by the SVR technique. Further, we compare our work with other supervised learning techniques, SVR and Neuro-Fuzzy techniques. The results revealed that the hybrid of clustering and DBN with the aid of SVR for an ensemble of the DBN outputs can make relatively better predictions of Total-UPDRS and Motor-UPDRS than other learning techniques.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420303869",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Deep belief network",
      "Deep learning",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Nilashi",
        "given_name": "Mehrbakhsh"
      },
      {
        "surname": "Ahmadi",
        "given_name": "Hossein"
      },
      {
        "surname": "Sheikhtaheri",
        "given_name": "Abbas"
      },
      {
        "surname": "Naemi",
        "given_name": "Roya"
      },
      {
        "surname": "Alotaibi",
        "given_name": "Reem"
      },
      {
        "surname": "Abdulsalam Alarood",
        "given_name": "Ala"
      },
      {
        "surname": "Munshi",
        "given_name": "Asmaa"
      },
      {
        "surname": "Rashid",
        "given_name": "Tarik A."
      },
      {
        "surname": "Zhao",
        "given_name": "Jing"
      }
    ]
  },
  {
    "title": "A modified bond energy algorithm with fuzzy merging and its application to Arabic text document clustering",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113598",
    "abstract": "Conventional textual documents clustering algorithms suffer from several shortcomings, such as the slow convergence of the immense high-dimensional data, the sensitivity to the initial value, and the understandability of the description of the resulted clusters. Although many clustering algorithms have been developed for English and other languages, very few have tackled the problem of clustering the under-resourced Arabic language. In this work, we propose a modified version of the Bond Energy Algorithm (BEA) combined with a fuzzy merging technique to solve the problem of Arabic text document clustering. The proposed algorithm, Clustering Arabic Documents based on Bond Energy, hereafter named CADBE, attempts to identify and display natural variable clusters within huge sized data. CADBE has three steps to cluster Arabic documents: the first step instantiates a cluster affinity matrix using the BEA, the second step uses a new and novel method to partition the cluster matrix automatically into small coherent clusters, and the last step uses a fuzzy merging technique to merge similar clusters based on the associations and interrelations between the resulted clusters. Experimental results showed that the proposed algorithm effectively outperformed the conventional clustering algorithms such as Expectation–Maximization (EM), Single Linkage, and UPGMA in terms of clustering purity and entropy. It also outperformed k-means, k-means++, spherical k-means, and CoclusMod in most test cases. However, there are several merits of CADBE. First, unlike the traditional clustering algorithms, it does not require to specify the number of clusters. In addition, it produces clusters with distinct boundaries, which makes its results more objective, and finally it is deterministic, such that it is insensitive to the order in which documents are presented to the algorithm.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742030422X",
    "keywords": [
      "Algorithm",
      "Arabic",
      "Artificial intelligence",
      "CURE data clustering algorithm",
      "Canopy clustering algorithm",
      "Cluster analysis",
      "Computer science",
      "Correlation clustering",
      "Data mining",
      "Document clustering",
      "Fuzzy clustering",
      "Linguistics",
      "Philosophy",
      "Single-linkage clustering"
    ],
    "authors": [
      {
        "surname": "AlMahmoud",
        "given_name": "Rana Husni"
      },
      {
        "surname": "Hammo",
        "given_name": "Bassam"
      },
      {
        "surname": "Faris",
        "given_name": "Hossam"
      }
    ]
  },
  {
    "title": "Decision making with dynamic uncertain continuous information",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113586",
    "abstract": "Decision making is the ability to select the best alternative from a set of candidates based on their respective values. When the value depends on uncertain future events, this task becomes more complicated. The question is then whether to wait for more information before making a decision or to stop and make a decision based on uncertain information. This has been addressed in previous work, when the information (events) could be represented as discrete random variables. However, there are real world domains where this assumption is incorrect. Thus, in this paper, we propose a novel framework and algorithms designed to cope with the challenge posed when future events are represented as continuous random variables. More specifically, we define a mathematical representation to model the utility functions of the candidates and introduce optimal and approximate algorithms to compute the best time to stop, and make a decision in order to optimize the utility. We evaluate our model and algorithms theoretically and empirically, and measure their performance in terms of the gain they achieve and their runtime. Our experiment demonstrates that there is no significant difference between the quality of the decision reached by the two algorithms, while the runtime of the optimal algorithm is much higher than that of the approximate algorithm.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420304103",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Decision tree",
      "Economics",
      "Epistemology",
      "Law",
      "Machine learning",
      "Management",
      "Mathematical optimization",
      "Mathematics",
      "Measure (data warehouse)",
      "Optimal decision",
      "Philosophy",
      "Political science",
      "Politics",
      "Programming language",
      "Quality (philosophy)",
      "Representation (politics)",
      "Set (abstract data type)",
      "Task (project management)",
      "Value of information"
    ],
    "authors": [
      {
        "surname": "Reches",
        "given_name": "Shulamit"
      },
      {
        "surname": "Kalech",
        "given_name": "Meir"
      }
    ]
  },
  {
    "title": "A deep neural network approach towards real-time on-branch fruit recognition for precision horticulture",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113594",
    "abstract": "Real-time and accurate on-branch fruit recognition in an uncontrolled/unstructured environment of orchards could facilitate Precision Horticulture (PH) practices. These practices which are based on the site-specific or variety-specific treatment of an orchard include applications like remote recognition of tree species, variety-specific orchard agrochemical applications, orchard yield mapping, robotic fruit picking, fruit tree disease treatment, etc. For this purpose, in the current work a Convolutional Neural Network (CNN) was developed and optimized for fruit recognition based on RGB images. The images from six classes of on-branch fruits i.e. green apples, nectarine, apricot, peach, sour cherry, and amber-colored plums were captured from local orchards at Semnan province, Iran. To avoid over-fitting, the dataset was then augmented to create more training data from existing samples. The model consisted of multiple convolutional, max-pooling, Global Average Pooling (GAP), and fully connected layers. Using GAP instead of the Flatten layer improved the performance of the model by increasing the accuracy as well as significantly reducing the trainable parameters (about 65 times reduction). The optimization phase of the model development was performed by testing four optimizers (RMSprop, SGD, Adam, and Nadam) with three batch sizes (16, 32, and 64) each with 50 epochs. Accordingly, Nadam optimizer with batch size = 32 demonstrated the best results. The best configuration achieved the accuracy of 99.8% and the cross-entropy loss of 0.019 for the test dataset. This result shows that the model is well developed and has good generalization. This reflects the potential of the method for the remote recognition and classification of different varieties of fruits in an orchard regardless of the environmental effects like complex background, variable light, overlaps, and occlusions with other plant parts, etc. The proposed network was also compared with popular structures like VGG11, ResNet50, ResNet152, and YOLOv3. The processing time of this model was about 8 ms per image while it was 351 ms for ResNet152, proving that the proposed network is much better for real-time applications. Consequently, this study presents a robust method for fulfilling the requirements of a PH practice in a high-tech horticulture system.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420304188",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Horticulture",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Orchard",
      "Pattern recognition (psychology)",
      "Pooling",
      "RGB color model",
      "Tree (set theory)"
    ],
    "authors": [
      {
        "surname": "Saedi",
        "given_name": "Seyed Iman"
      },
      {
        "surname": "Khosravi",
        "given_name": "Hossein"
      }
    ]
  },
  {
    "title": "A TOPSIS-based approach for the best match between manufacturing technologies and product specifications",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113610",
    "abstract": "When manufacturing a product, companies must consider the specifications of its design and choose the manufacturing technology that matches them the best in terms of product quality, production time and costs. Since all these parameters can be represented by several different and conflicting indicators, the problem of technology selection can be defined as a multi-criteria decision-making (MCDM) problem. Although several mathematical models have been developed to solve similar problems, recent literature still presents a lack of specific applications of renowned decision-making techniques to the technology matching problem in the manufacturing sector. This study attempts to fill this gap by proposing a manufacturing-oriented model of the Technique for Order Preference by Similarity to Ideal Solutions (TOPSIS), one of the most solid and robust MCDM methods. The solution we present, which is designed for general manufacturing processes, has been applied to the specific case of a producer of food and beverage plants and equipment that is interested in reengineering one of its products. Due to the complexity of the food and beverage industry, the case study is useful for supporting the definition of the general model and validating its applicability. Further, the results of the specific application prove the effectiveness of our model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420304346",
    "keywords": [
      "Business",
      "Business process reengineering",
      "Computer science",
      "Engineering",
      "Epistemology",
      "Finance",
      "Geometry",
      "Ideal solution",
      "Industrial engineering",
      "Lean manufacturing",
      "Manufacturing engineering",
      "Matching (statistics)",
      "Mathematics",
      "Multiple-criteria decision analysis",
      "Operations research",
      "Order (exchange)",
      "Philosophy",
      "Physics",
      "Product (mathematics)",
      "Quality (philosophy)",
      "Risk analysis (engineering)",
      "Statistics",
      "TOPSIS",
      "Thermodynamics"
    ],
    "authors": [
      {
        "surname": "Bertolini",
        "given_name": "Massimo"
      },
      {
        "surname": "Esposito",
        "given_name": "Giovanni"
      },
      {
        "surname": "Romagnoli",
        "given_name": "Giovanni"
      }
    ]
  },
  {
    "title": "A novel deep learning framework: Prediction and analysis of financial time series using CEEMD and LSTM",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113609",
    "abstract": "Deep learning is well-known for extracting high-level abstract features from a large amount of raw data without relying on prior knowledge, which is potentially attractive in forecasting financial time series. Long short-term memory (LSTM) networks are deemed as state-of-the-art techniques in sequence learning, which are less commonly applied to financial time series predictions, yet inherently suitable for this domain. We propose a novel methodology of deep learning prediction, and based on this, construct a deep learning hybrid prediction model for stock markets—CEEMD-PCA-LSTM. In this model, complementary ensemble empirical mode decomposition (CEEMD), as a sequence smoothing and decomposition module, can decompose the fluctuations or trends of different scales of time series step by step, generating a series of intrinsic mode functions (IMFs) with different characteristic scales. Then, with retaining the most of information on raw data, PCA reduces dimension of the decomposed IMFs component, eliminating the redundant information and improving prediction response speed. After that, high-level abstract features are separately fed into LSTM networks to predict closing price of the next trading day for each component. Finally, synthesizing the predicted values of individual components is utilized to obtain a final predicted value. The empirical results of six representative stock indices from three types of markets indicate that our proposed model outperforms benchmark models in terms of predictive accuracy, i.e., lower test error and higher directional symmetry. Leveraging key research findings, we perform trading simulations to validate that the proposed model outperforms benchmark models in both absolute profitability performance and risk-adjusted profitability performance. Furthermore, model robustness test unveils the more stable robustness compared to benchmark models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420304334",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Computer science",
      "Computer vision",
      "Curse of dimensionality",
      "Data mining",
      "Deep learning",
      "Filter (signal processing)",
      "Geodesy",
      "Geography",
      "Hilbert–Huang transform",
      "Horse",
      "Machine learning",
      "Paleontology",
      "Series (stratigraphy)",
      "Smoothing",
      "Stock market",
      "Time series"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Yong'an"
      },
      {
        "surname": "Yan",
        "given_name": "Binbin"
      },
      {
        "surname": "Aasma",
        "given_name": "Memon"
      }
    ]
  },
  {
    "title": "Maximal association analysis using logical formulas over soft sets",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113557",
    "abstract": "Discovering interesting and useful association rules from the collected data is an issue of great importance in pattern mining. Although a myriad of association rules can be extracted with traditional rule mining techniques, some of the obtained rules might be redundant or even meaningless in many cases. To overcome this difficulty, logical formulas over soft sets are applied to maximal association mining in this study. With the help of logical formulas over soft sets, all critical concepts for mining both regular and maximal association rules are incorporated into a common framework, and uniform mathematical characterizations of these concepts are provided accordingly. Three algorithms are also designed to develop a new approach to maximal association rule mining using logical formulas over soft sets. Moreover, we present two examples to show theoretical value of the obtained results and practical applicability of the proposed approach. The first example relies on a clinical diagnosis data set and illustrates the advantages of applying logical formula over soft sets to rule extraction. In the second example, we conduct a case study based on a data set regarding Nobel Laureates to show that the proposed approach is helpful for discovering interesting facts in real-world scenarios.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742030381X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Association (psychology)",
      "Association rule learning",
      "Computer science",
      "Data mining",
      "Epistemology",
      "Fuzzy logic",
      "Logical conjunction",
      "Machine learning",
      "Philosophy",
      "Programming language",
      "Set (abstract data type)",
      "Soft set",
      "Value (mathematics)"
    ],
    "authors": [
      {
        "surname": "Feng",
        "given_name": "Feng"
      },
      {
        "surname": "Wang",
        "given_name": "Qian"
      },
      {
        "surname": "Yager",
        "given_name": "Ronald R."
      },
      {
        "surname": "R. Alcantud",
        "given_name": "José Carlos"
      },
      {
        "surname": "Zhang",
        "given_name": "Longyao"
      }
    ]
  },
  {
    "title": "Simultaneous localization and mapping using swarm intelligence based methods",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113547",
    "abstract": "The problem known as simultaneous localization and mapping is of fundamental importance both in its own right and because of its potential applications in the development of autonomous robots. While many solutions exist that are based on classical Newton-like optimization techniques regarding scan matching, relatively no work has been done with respect to the application of derivative free bio-inspired techniques to this particular area of robotics. That being said, we propose a novel approach to the scan-matching step within the Simultaneous Localization And Mapping (SLAM) problem based on the exploitation of swarm intelligence. For this purpose, we have chosen three swarm intelligence optimization methods to be the subjects of our research investigation, namely particle swarm optimization, artificial bee colony and the firefly algorithm. Aiming at reducing further the translational and rotational scan alignment errors, the proposed scan matching proceeds in two main steps, namely scan-to-scan and scan-to-map matching. Furthermore, we have made use of a pose graph based approach as a means of maintaining the consistency of our estimates across the mapping process. Systems designed to perform simultaneous localization and mapping using pose graphs are currently the state of the art and it is our belief that a robust scan-matching system is currently of the utmost importance to further the field. We tested the proposed solution in many scenarios. We conclude that the artificial bee colony strategy is efficient in a large range of circumstances. This affirmation is backed by the fact that in the best case scenarios, we have obtained good and some times better accuracy gains, regarding the translational and rotational estimates of the robot’s trajectory, by using the this meta-heuristic, when compared to state of the art SLAM systems. The firefly algorithm, while not as accurate as the artificial bee colony technique, was faster on 7 out of the 8 public domain datasets. The firefly algorithm consumed, on average less time spent per scan than by the artificial bee colony optimization technique. Particle swarm optimization has shown an inferior accuracy when compared to the artificial bee colony optimization technique and an intermediate processing time when compared to the other two optimization methods. So, firefly based meta-heuristic can be considered as the technique that provides a good trade-off between the high accuracy offered by the artificial bee colony and the high execution speed of the particle swarm optimization.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420303717",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Graph",
      "Machine learning",
      "Matching (statistics)",
      "Mathematics",
      "Mobile robot",
      "Operating system",
      "Particle swarm optimization",
      "Process (computing)",
      "Robot",
      "Robotics",
      "Simultaneous localization and mapping",
      "Statistics",
      "Swarm intelligence",
      "Swarm robotics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Nedjah",
        "given_name": "Nadia"
      },
      {
        "surname": "Macedo Mourelle",
        "given_name": "Luiza"
      },
      {
        "surname": "Albuquerque de Oliveira",
        "given_name": "Pedro Jorge"
      }
    ]
  },
  {
    "title": "A hybrid artificial neural network, genetic algorithm and column generation heuristic for minimizing makespan in manual order picking operations",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113566",
    "abstract": "At an operational level, order picking is the main activity in fulfillment centers. Motivated by and through collaboration with a third party logistic company, this study presents a novel hybrid column generation (CG), genetic algorithm (GA), and artificial neural network (ANN) heuristic for minimizing makespan in manual order picking operations. The results of column generation heuristic is compared against a mixed integer programming model solved by Gurobi, and a parallel simulated annealing and ant colony optimization (PSA-ACO) previously proposed in the literature. Through numerical experiments, the superiority of CG heuristic compared to other methods is shown, and some managerial insights regarding the relationship between makespan optimization, workload balance, picking capacity, and number of pickers in order picking operations is presented.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420303900",
    "keywords": [
      "Algorithm",
      "Ant colony optimization algorithms",
      "Artificial intelligence",
      "Artificial neural network",
      "Column generation",
      "Computer science",
      "Genetic algorithm",
      "Heuristic",
      "Integer programming",
      "Job shop scheduling",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Metaheuristic",
      "Operating system",
      "Schedule",
      "Simulated annealing",
      "Workload"
    ],
    "authors": [
      {
        "surname": "Ardjmand",
        "given_name": "Ehsan"
      },
      {
        "surname": "Ghalehkhondabi",
        "given_name": "Iman"
      },
      {
        "surname": "Young II",
        "given_name": "William A."
      },
      {
        "surname": "Sadeghi",
        "given_name": "Azadeh"
      },
      {
        "surname": "Weckman",
        "given_name": "Gary R."
      },
      {
        "surname": "Shakeri",
        "given_name": "Heman"
      }
    ]
  },
  {
    "title": "Grape detection with convolutional neural networks",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113588",
    "abstract": "Convolutional neural networks, as a type of deep learning approach, have revolutionized the field of computer vision and pattern recognition through state of the art performance in a large number of classification tasks. Machine learning has been recently incorporated into intelligent systems related to agricultural and food production to decrease manual processing when dealing with large number of operations. Feedforward artificial neural networks such as convolutional neural networks can be used in agriculture for the segmentation and classification of images containing objects of interests such as fruits, or leaves. It is however unknown what is the best architecture to use, if it is necessary to propose new architectures, and what is the impact of the input feature space on the classification performance. In this paper, we propose to detect two types of grapes (Albariño white grapes and Barbera red grapes) in images. We investigate 1) the impact of the input feature space: color images, grayscale images, and color histograms using convolutional neural networks; 2) the impact of the parameters such as the size of the blocks, and the impact of data augmentation; 3) the performance of 11 pre-trained deep learning architectures, i.e. using a transfer learning approach for the classification. The results support the conclusion that images of grapes can be efficiently segmented using different feature spaces where color images provide the best performance. With convolutional neural networks using transfer learning, the best performance is achieved with Resnet networks reaching an accuracy of 99% for both red and white grapes. Finally, data augmentation, image normalization, and the input feature space have a key impact on the overall performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420304127",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Contextual image classification",
      "Convolutional neural network",
      "Deep learning",
      "Feature (linguistics)",
      "Field (mathematics)",
      "Histogram",
      "Image (mathematics)",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pure mathematics",
      "Segmentation",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Cecotti",
        "given_name": "Hubert"
      },
      {
        "surname": "Rivera",
        "given_name": "Agustin"
      },
      {
        "surname": "Farhadloo",
        "given_name": "Majid"
      },
      {
        "surname": "Pedroza",
        "given_name": "Miguel A."
      }
    ]
  },
  {
    "title": "An online portfolio selection algorithm using clustering approaches and considering transaction costs",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113546",
    "abstract": "This paper presents an online portfolio selection algorithm based on pattern matching principle where it makes a decision on the optimal portfolio in each period and updates the optimal portfolio at the beginning of each period. The proposed method consists of two steps: i) sample selection, ii) portfolio optimization. First, in the sample selection, clustering algorithms including k-means, k-medoids, spectral and hierarchical clustering are applied to discover time windows (TW) similar to the recent time window. Then, after finding the similar time windows and predicting the market behavior of the next day, the optimum function along with the transaction cost is used in the portfolio optimization step in which, four algorithms including KMNLOG, KMDLOG, SPCLOG and HRCLOG are proposed for this purpose. The presented algorithms are applied on 5 different datasets with different characteristics including different markets, stocks, and time periods, and their performance has been evaluated. The results show that the provided algorithms in this paper, have better performance in terms of efficiency compared to the algorithms provided in the literature.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420303705",
    "keywords": [
      "Algorithm",
      "Chemistry",
      "Chromatography",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Database",
      "Database transaction",
      "Economics",
      "Finance",
      "Machine learning",
      "Matching (statistics)",
      "Mathematical optimization",
      "Mathematics",
      "Medoid",
      "Portfolio",
      "Portfolio optimization",
      "Sample (material)",
      "Selection (genetic algorithm)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Khedmati",
        "given_name": "Majid"
      },
      {
        "surname": "Azin",
        "given_name": "Pejman"
      }
    ]
  },
  {
    "title": "A new fuzzy unit selection cost function optimized by relaxed gradient descent algorithm",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113552",
    "abstract": "In data-driven corpus-based text-to-speech synthesis systems, the main issue is to select the most natural-sounding sequence of acoustic units without unnatural acoustic transitions, and to minimize all acoustic mismatches at the concatenation points. Unit selection algorithms incorporating unit selection cost functions have been known to synthesize speech close to natural quality. However, these algorithms operate over large acoustic inventories with huge number of acoustic units in a broad spectrum of linguistic, prosodic and acoustic contexts, and with a huge number of concatenation possibilities. Moreover, the shape of the unit selection cost function, which evaluates the cost of concatenating two subsequent acoustic units, is modelled manually in a time-consuming and laborious iterative process, which is based on subjective evaluation. Since this process must be repeated for any new acoustic inventory, or even after changes in a given acoustic inventory, we propose instead a new fuzzy unit selection cost function. We further propose to optimize fully automatically the shape of the fuzzy unit selection cost function to the given acoustic inventory’s context by using a relaxed gradient descent algorithm, where the subjective tests are replaced by a novel objective measure needed to evaluate unit selection cost function performance. Furthermore, the proposed approach is fully interpretable and also highlights insights into which parts of the fuzzy unit selection cost function’s shape could be further improved. The experiments show that the optimized fuzzy unit selection cost function significantly outperforms the baseline fuzzy unit selection cost function. Moreover, the results prove that the unit selection optimization algorithm is capable of finding the optimal shape of the fuzzy unit selection cost function, even when optimized over a small subset of sentences.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420303766",
    "keywords": [
      "Algorithm",
      "Arithmetic",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer science",
      "Concatenation (mathematics)",
      "Context (archaeology)",
      "Evolutionary biology",
      "Function (biology)",
      "Fuzzy logic",
      "Gradient descent",
      "Mathematical optimization",
      "Mathematics",
      "Operating system",
      "Paleontology",
      "Process (computing)",
      "Selection (genetic algorithm)",
      "Stochastic gradient descent"
    ],
    "authors": [
      {
        "surname": "Rojc",
        "given_name": "Matej"
      },
      {
        "surname": "Mlakar",
        "given_name": "Izidor"
      }
    ]
  },
  {
    "title": "Optimized micro-hydro power plants layout design using messy genetic algorithms",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113539",
    "abstract": "Micro Hydro-Power Plants (MHPP) represent a powerful and effective solution to address the problem of energy poverty in rural remote areas, with the advantage of preserving the natural resources and minimizing the impact on the environment. Nevertheless, the lack of resources and qualified manpower usually constitutes a big obstacle to its adequate application, generally translating into sub-optimal generation systems with poor levels of efficiency. Therefore, the study and development of expert, simple and efficient strategies to assist the design of these installations is of especial relevance. This work proposes a design methodology based on a tailored messy evolutionary computational approach, with the objective of finding the most suitable layout of MHPP, considering several constraints derived from a minimal power supply requirement, the maximum flow usage, and the physical feasibility of the plant in accordance with the real terrain profile. This profile is built on the basis of a discrete topographic survey, by means of a shape-preserving interpolation, which permits the application of a continuous variable-length Messy Genetic Algorithm (MGA). The optimization problem is then formulated in both single-objective (cost minimization) and multi-objective (cost minimization and power supply maximization) modes, including the study of the Pareto dominance. The algorithm is applied to a real scenario in a remote community in Honduras, obtaining a 56.96% of cost reduction with respect to previous works.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420303638",
    "keywords": [
      "Computer science",
      "Electrical engineering",
      "Electricity",
      "Engineering",
      "Evolutionary algorithm",
      "Genetic algorithm",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Maximization",
      "Micro hydro",
      "Minification",
      "Multi-objective optimization",
      "Pareto principle"
    ],
    "authors": [
      {
        "surname": "Tapia",
        "given_name": "A."
      },
      {
        "surname": "Reina",
        "given_name": "D.G."
      },
      {
        "surname": "Millán",
        "given_name": "P."
      }
    ]
  },
  {
    "title": "A hybrid recommendation system with many-objective evolutionary algorithm",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113648",
    "abstract": "Recommendation system (RS) is a technology that provides accurate recommendations to users. However, it is not comprehensive to only consider the accuracy of the recommendation because users have different requirements. To improve the comprehensive performance, this paper presents a hybrid recommendation model based on many-objective optimization, which can simultaneously optimize the accuracy, diversity, novelty and coverage of recommendation. This model enhances the robustness of recommendations by mixing three different basic recommendation technologies. Additionally, we solve it with many-objective evolutionary algorithm (MaOEA) and test it extensively. Experimental results demonstrate the effectiveness of the presented model, which can provide the recommendations with more and novel items on the basis of accurate and diverse.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420304723",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Data mining",
      "Evolutionary algorithm",
      "Gene",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Novelty",
      "Optimization algorithm",
      "Philosophy",
      "Recommender system",
      "Robustness (evolution)",
      "Theology"
    ],
    "authors": [
      {
        "surname": "Cai",
        "given_name": "Xingjuan"
      },
      {
        "surname": "Hu",
        "given_name": "Zhaoming"
      },
      {
        "surname": "Zhao",
        "given_name": "Peng"
      },
      {
        "surname": "Zhang",
        "given_name": "WenSheng"
      },
      {
        "surname": "Chen",
        "given_name": "Jinjun"
      }
    ]
  },
  {
    "title": "A comparative evaluation of unsupervised deep architectures for intrusion detection in sequential data streams",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113577",
    "abstract": "Cybersecurity data remains a challenge for the machine learning community as the high volume of traffic makes it difficult to properly disambiguate anomalous from normal behaviour. That decision is the core of an intelligent Intrusion Detection System (IDS), a component responsible for raising alerts whenever a potential threat is detected. However, with high volume data in contemporary systems, these IDSs generate numerous alerts, too large for human operators to exhaustively investigate. Moreover, simply reporting a single possible threat is often not sufficient, since the security analyst has to investigate the alert without any further clues of the underlying cause. In order to combat these issues, we empirically compare popular deep neural learning architectures for the problem of intrusion detection in sequential data streams. Contrary to a majority of research studies, we do not take a classification-based approach that requires labeled examples of hostile attacks. Instead, we adopt an unsupervised anomaly detection approach that aims to model a benign sequential data distribution against which new test instances are compared to. We also examine one additional deep network in the form of an attention model capable of providing explanations in addition to its predictions; such information is of crucial importance to network operators since it provides additional guidance to resolve potential threats. For our experiments, we evaluate the models against a variety of data sets of different complexities, ranging from simple unidimensional (synthetic and Yahoo!) to more complex multi-source (CICIDS2017 and small real-world enterprise network) data streams. In order to facilitate end-user needs, we focus on ranking-based metrics for comparing different deep neural architectures. This evaluation is especially important for security analysts to prioritize their anomaly investigations. Overall, our experiments demonstrate that a variant of a recurrent neural network generally outperforms a popular non-sequential deep autoencoder commonly used for unsupervised anomaly detection. The attentional model did not provide sufficiently good performance and explanations that we discuss in our analysis. Nonetheless, given that the global financial outlays for cybersecurity are calculated in trillions of dollars, our evaluation and identification of the top-performing RNN architectures for anomaly detection in sequential data streams can lead to improved intelligent IDS design, while our contributions of attentional explanation will hopefully lay the foundations for future improvements to the explanatory capability of these intelligent learning-based IDSs.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420304012",
    "keywords": [
      "Anomaly detection",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Data mining",
      "Data stream mining",
      "Deep learning",
      "Intrusion detection system",
      "Machine learning",
      "Physics",
      "Quantum mechanics",
      "Variety (cybernetics)",
      "Volume (thermodynamics)"
    ],
    "authors": [
      {
        "surname": "Sovilj",
        "given_name": "Dušan"
      },
      {
        "surname": "Budnarain",
        "given_name": "Paul"
      },
      {
        "surname": "Sanner",
        "given_name": "Scott"
      },
      {
        "surname": "Salmon",
        "given_name": "Geoff"
      },
      {
        "surname": "Rao",
        "given_name": "Mohan"
      }
    ]
  },
  {
    "title": "Boosted hunting-based fruit fly optimization and advances in real-world problems",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113502",
    "abstract": "Fruit fly optimization algorithm (FOA) is a well-established meta-heuristic method with a clear core concept, a straightforward computation, and a code framework that is easy to build. In the case of large scale and multifaceted practical problems, the optimization effect of FOA may be unsatisfactory, and it is prone to stagnation. In this paper, to enrich the exploration and exploitation capability of the classic FOA, an effective whale-inspired hunting strategy is introduced to replace the random search plan of the original FOA, which we named it as WFOA. The proposed WFOA is compared with 9 state-of-the-art FOA’s variants on a comprehensive set of 23 benchmark set and 30 IEEE CEC 2014 functions and advanced algorithms on a set of 21 test functions to validate its effectiveness. In addition, the effectiveness of WFOA is also verified on 20 IEEE CEC 2011 benchmark problems for tackling real-world problems. The statistical data shows that developed components effectively expand the exploration and exploitation capacity of the original FOA.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420303262",
    "keywords": [
      "Algorithm",
      "Archaeology",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computation",
      "Computer science",
      "Core (optical fiber)",
      "Geodesy",
      "Geography",
      "Heuristic",
      "History",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Physics",
      "Plan (archaeology)",
      "Programming language",
      "Quantum mechanics",
      "Scale (ratio)",
      "Set (abstract data type)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Fan",
        "given_name": "Yi"
      },
      {
        "surname": "Wang",
        "given_name": "Pengjun"
      },
      {
        "surname": "Heidari",
        "given_name": "Ali Asghar"
      },
      {
        "surname": "Wang",
        "given_name": "Mingjing"
      },
      {
        "surname": "Zhao",
        "given_name": "Xuehua"
      },
      {
        "surname": "Chen",
        "given_name": "Huiling"
      },
      {
        "surname": "Li",
        "given_name": "Chengye"
      }
    ]
  },
  {
    "title": "Influential nodes detection in dynamic social networks: A survey",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113642",
    "abstract": "The influence maximization problem has gained increasing attention in recent years. Previous research focuses on the development of algorithms to analyze static social networks. However, real social networks are not static but they are represented as dynamic networks that evolve across time. Motivated by this drawback, the purpose of this survey is to highlight the characteristics and challenges of the influential nodes detection problem. A classification of published approaches should be proposed. This work is organizing state-of-the-art methods into a technical comparison that are based on network models. Due to the definition of network models and the influential nodes detection problem, this survey will help researchers to find the set of methods best suited for their needs. The proposed classification could also help researchers to select the right direction in which their future research should be oriented.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420304668",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Data science",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Maximization",
      "Programming language",
      "Set (abstract data type)",
      "Social media",
      "Social network (sociolinguistics)",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Hafiene",
        "given_name": "Nesrine"
      },
      {
        "surname": "Karoui",
        "given_name": "Wafa"
      },
      {
        "surname": "Ben Romdhane",
        "given_name": "Lotfi"
      }
    ]
  },
  {
    "title": "Learning path personalization and recommendation methods: A survey of the state-of-the-art",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113596",
    "abstract": "A learning path is the implementation of a curriculum design. It consists of a set of learning activities that help users achieve particular learning goals. Personalizing these paths became a significant task due to differences in users’ limitations, backgrounds, goals, etc. Since the last decade, researchers have proposed a variety of learning path personalization methods using different techniques and approaches. In this paper, we present an overview of the methods that are applied to personalize learning paths as well as their advantages and disadvantages. The main parameters for personalizing learning paths are also described. In addition, we present approaches that are used to evaluate path personalization methods. Finally, we highlight the most significant challenges of these methods, which need to be tackled in order to enhance the quality of the personalization.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420304206",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Cooperative learning",
      "Curriculum",
      "Economics",
      "Epistemology",
      "Human–computer interaction",
      "Machine learning",
      "Management",
      "Mathematics",
      "Mathematics education",
      "Multimedia",
      "Open learning",
      "Path (computing)",
      "Pedagogy",
      "Personalization",
      "Personalized learning",
      "Philosophy",
      "Programming language",
      "Psychology",
      "Quality (philosophy)",
      "Set (abstract data type)",
      "Task (project management)",
      "Teaching method",
      "Variety (cybernetics)",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Nabizadeh",
        "given_name": "Amir Hossein"
      },
      {
        "surname": "Leal",
        "given_name": "José Paulo"
      },
      {
        "surname": "Rafsanjani",
        "given_name": "Hamed N."
      },
      {
        "surname": "Shah",
        "given_name": "Rajiv Ratn"
      }
    ]
  },
  {
    "title": "Targeted evidence collection for uncertain supplier selection",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113583",
    "abstract": "The problem of selecting which suppliers, and how much of different items to order from each, involves multiple, often conflicting, criteria such as costs and delivery times. Within real world multi-criteria supplier selection problems there is inherent uncertainty involved, and consideration of its impacts and mitigation is a current and important research direction going forward within the field of supplier selection. Uncertainty within multi-criteria supplier selection may be in relation to (i) a decision maker’s ambiguous preferences, such as the importance between criteria, (ii) the suppliers’ supply capacities of, and demand for, different items, and (iii) known information about suppliers with respect to the set of criteria, such as each supplier’s delivery times or their average defect ratios. Whilst previous work has explored the first two of these, less work has explored uncertainty pertaining to information about suppliers in terms of the criteria and, specifically, how it could be efficiently reduced. Such uncertainty is an important problem to address, as it may have a large impact upon an order regarding its perceived quality compared to its realised quality, so reducing such uncertainty can have a significant impact. This paper presents a Targeted Evidence Collection (TEC) approach for efficient reduction of uncertainty, pertaining to suppliers, by looking to efficiently collect additional evidence. The approach looks to utilise and gather evidence intelligently and dynamically – by considering both the likelihood that each supplier will be part of a solution, along with a decision maker’s preferences between criteria – to reduce the uncertainty and efficaciously move towards the most appropriate solution given no uncertainty. The approach is able to handle scenarios for which there are both certain and uncertain criteria present, and can take into account any number of criteria. The TEC strategy is evaluated against alternative approaches, including an active learning based approach, for varying numbers of uncertain criteria, numbers of suppliers, and variations in a decision maker’s preferences. The experimentation highlights how TEC efficiently reduces uncertainty, relating to information about suppliers with respect to the set of criteria, requiring up to three times less evidence than its competitors. In this way, TEC helps to effectively mitigate the uncertainty’s adverse effects, and reduce the risks inherent within a supplier selection problem.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420304073",
    "keywords": [
      "Artificial intelligence",
      "Business",
      "Computer science",
      "Data mining",
      "Decision maker",
      "Engineering",
      "Epistemology",
      "Finance",
      "Mathematics",
      "Mechanical engineering",
      "Operations research",
      "Order (exchange)",
      "Philosophy",
      "Programming language",
      "Quality (philosophy)",
      "Relation (database)",
      "Risk analysis (engineering)",
      "Selection (genetic algorithm)",
      "Set (abstract data type)",
      "Work (physics)"
    ],
    "authors": [
      {
        "surname": "Abel",
        "given_name": "Edward"
      },
      {
        "surname": "Cortés Ríos",
        "given_name": "Julio César"
      },
      {
        "surname": "Paton",
        "given_name": "Norman W."
      },
      {
        "surname": "Keane",
        "given_name": "John A."
      },
      {
        "surname": "Fernandes",
        "given_name": "Alvaro A.A."
      }
    ]
  },
  {
    "title": "Granular fuzzy pay-off method for real option valuation",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113597",
    "abstract": "In the last decade, the fuzzy pay-off method has emerged as a widely used alternative approach for real option valuation thanks to its simplicity that makes it easily approachable by practitioners from various application domains. In this study, a new direction for real option valuation is pursued by proposing a granular fuzzy pay-off method. We motivate the proposal by discussing how the extension of the original approach with granular representation can further improve the fuzzy pay-off method. The design of the granular fuzzy pay-off method is founded on the principle of justifiable granularity, which in turn relies on numeric data to build information granules that are semantically sound and experimentally justified. To illustrate the method, a case study in R&D investment in manufacturing is worked out. The extended granular fuzzy pay-off method improves performance and usability in cases with uncertainty.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420304218",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Economics",
      "Finance",
      "Fuzzy logic",
      "Fuzzy rule",
      "Fuzzy set",
      "Granular computing",
      "Granularity",
      "Human–computer interaction",
      "Operating system",
      "Rough set",
      "Usability",
      "Valuation (finance)"
    ],
    "authors": [
      {
        "surname": "Cabrerizo",
        "given_name": "Francisco Javier"
      },
      {
        "surname": "Heikkilä",
        "given_name": "Markku"
      },
      {
        "surname": "Mezei",
        "given_name": "József"
      },
      {
        "surname": "Morente-Molinera",
        "given_name": "Juan Antonio"
      },
      {
        "surname": "Herrera-Viedma",
        "given_name": "Enrique"
      },
      {
        "surname": "Carlsson",
        "given_name": "Christer"
      }
    ]
  },
  {
    "title": "A knowledge-based reasoning model for crime reconstruction and investigation",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113611",
    "abstract": "Artificial intelligence has been successfully applied in many areas including forensic sciences. Perhaps all forensic works can be regarded as helping reconstruct crimes, i.e. clarify and sequence the events that took place in the commission of a crime through evidence. However, there are few researches on the crime reconstruction using artificial intelligence methods. In this paper, we present a model based on Bayesian networks to help solve crimes. The model, which is termed ‘case-type based model’, is based on the knowledge of a type of crimes. We use Bayesian networks to represent the knowledge and conduct the uncertainty reasoning. We propose a growth algorithm of Bayesian networks to adapt the model to different cases. The model was tested through a real case, and the results indicate that the model can provide effective investigation suggestions and achieve the crime reconstruction.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420304358",
    "keywords": [
      "Artificial intelligence",
      "Bayesian network",
      "Bayesian probability",
      "Computer science",
      "Machine learning"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Litao"
      },
      {
        "surname": "Jia",
        "given_name": "Meisheng"
      },
      {
        "surname": "Shi",
        "given_name": "Yi"
      },
      {
        "surname": "Chen",
        "given_name": "Feiyu"
      },
      {
        "surname": "Ni",
        "given_name": "Shunjiang"
      },
      {
        "surname": "Shen",
        "given_name": "Shifei"
      }
    ]
  },
  {
    "title": "Multi-kernel fuzzy clustering based on auto-encoder for fMRI functional network",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113513",
    "abstract": "The existing clustering algorithms based on auto-encoder only use one layer of information. In this paper, we propose a new subspace clustering algorithm that uses multiple hidden layers of information from the stacked auto-encoder to construct different kernels. The proposed fuzzy multi-kernel clustering method based on auto-encoder is realized by updating the membership matrix and coefficients of these kernels until the value of objective function is iterated to the minimum error. Also, the proposed method combining auto-encoder achieves high dimension reduction of input data. In order to test the effectiveness of the algorithm, we first performed experiments on the published brain network dataset. Compared with MKFC, RMKKM and other algorithms, the proposed method can significantly improve accuracy. Specifically, the brain network has large dimensions when the functional magnetic resonance imaging (fMRI) data is preprocessed. Therefore, the method in this paper is applied to the Autism Spectrum Disorder (ASD) of the Autism Brain Imaging Data Exchange (ABIDE) database. The experiment results on the network dataset we constructed with high dimension are better than the present several clustering algorithms. The results show that the subspace information after dimensionality reduction is more conducive to clustering.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420303377",
    "keywords": [
      "Artificial intelligence",
      "CURE data clustering algorithm",
      "Cluster analysis",
      "Clustering high-dimensional data",
      "Combinatorics",
      "Computer science",
      "Data mining",
      "Dimensionality reduction",
      "Fuzzy clustering",
      "Fuzzy logic",
      "Kernel (algebra)",
      "Mathematics",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Lu",
        "given_name": "Hu"
      },
      {
        "surname": "Liu",
        "given_name": "Saixiong"
      },
      {
        "surname": "Wei",
        "given_name": "Hui"
      },
      {
        "surname": "Tu",
        "given_name": "Juanjuan"
      }
    ]
  },
  {
    "title": "Advanced orthogonal moth flame optimization with Broyden–Fletcher–Goldfarb–Shanno algorithm: Framework and real-world problems",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113617",
    "abstract": "As a typical emergent swarm intelligence algorithm, Moth-Flame Optimization (MFO) has been created to deal with global optimization problems. Since the introduction, it has been applied to various optimization problems. However, MFO may have the trouble of getting into the local best, and the convergence rate cannot be satisfying when handling the high-dimensional and some multimodal problems. In this work, an enhanced MFO integrated with orthogonal learning (OL) and Broyden-Fletcher-Goldfarb-Shanno (BFGS), which we called BFGSOLMFO, is proposed to alleviate the stagnation shortcomings and accelerate the performance of well-regarded MFO. In the BFGSOLMFO, OL is used to construct a better candidate solution for each moth and then guide the whole population to a reasonable potential area. Meanwhile, in each iteration, after the evolution of population finished and the global optima are obtainable, BFGS is employed to further excavate the potential of the global best moth in the current population. With the aim of evaluating the efficacy of the BFGSOLMFO, first of all, the IEEE CEC2014 benchmark set is utilized to measure the performance in solving function optimizations with high-dimensional and multimodal characteristics. Both sets of the IEEE CEC2011 real-world benchmark problems and the three constrained engineering optimization problems are adopted to estimate the performance of BFGSOLMFO in tackling practical scenarios. In all the experiments, the developed BFGSOLMFO is compared with state-of-the-art advanced algorithms. Experimental results and statistical tests demonstrate that the proposed method outperforms the basic MFO and a comprehensive set of advanced algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420304413",
    "keywords": [
      "Algorithm",
      "Benchmark (surveying)",
      "Broyden–Fletcher–Goldfarb–Shanno algorithm",
      "Computer science",
      "Computer security",
      "Convergence (economics)",
      "Data mining",
      "Demography",
      "Economic growth",
      "Economics",
      "Exploit",
      "Geodesy",
      "Geography",
      "Mathematical optimization",
      "Mathematics",
      "Measure (data warehouse)",
      "Optimization problem",
      "Population",
      "Programming language",
      "Set (abstract data type)",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Hongliang"
      },
      {
        "surname": "Li",
        "given_name": "Rong"
      },
      {
        "surname": "Cai",
        "given_name": "Zhennao"
      },
      {
        "surname": "Gu",
        "given_name": "Zhiyang"
      },
      {
        "surname": "Heidari",
        "given_name": "Ali Asghar"
      },
      {
        "surname": "Wang",
        "given_name": "Mingjing"
      },
      {
        "surname": "Chen",
        "given_name": "Huiling"
      },
      {
        "surname": "Chen",
        "given_name": "Mayun"
      }
    ]
  },
  {
    "title": "An interval type-2 fuzzy reasoning model for digital transformation project risk assessment",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113579",
    "abstract": "This study presents a new risk assessment model by combining interval type-2 fuzzy best-worst method (IT2F-BWM) and perceptual reasoning for risk evaluation of digital transformation projects. Digital transformations are technology-driven change processes that transform many businesses. Digital transformations are risky and difficult undertakings so that the decision-makers should be supported with appropriate risk assessment tools. The proposed model is a step towards this direction by providing decision-makers with a practical and applicable risk assessment tool that computes with words through a transparent and interpretable reasoning mechanism. In the proposed model, the relative importance of risk factors (RFs) is derived by using IT2F-BWM. Then, IT2F rule-based model is developed by considering risk factors along with the likelihood and severity of risks. The risk magnitudes of digital transformation projects are evaluated via perceptual reasoning. Both the input and output of the reasoning model are IT2F numbers, and the resulting risk magnitudes are decoded into words such as critical, major, and minor so as to ease interpretation. A real-life digital transformation risk assessment case study is conducted in order to demonstrate the applicability of the proposed model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420304036",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Biology",
      "Chemistry",
      "Combinatorics",
      "Computer science",
      "Computer security",
      "Data mining",
      "Digital transformation",
      "Fuzzy logic",
      "Gene",
      "Interpretation (philosophy)",
      "Interval (graph theory)",
      "Machine learning",
      "Mathematics",
      "Medicine",
      "Neuroscience",
      "Perception",
      "Programming language",
      "Risk analysis (engineering)",
      "Risk assessment",
      "Transformation (genetics)",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Gölcük",
        "given_name": "İlker"
      }
    ]
  },
  {
    "title": "Flower classification with modified multimodal convolutional neural networks",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113455",
    "abstract": "The new multi-view learning algorithm is proposed by modifying an existing method, the multimodal convolutional neural networks originally developed for image-text matching (modified m-CNN), to use not only images but also texts for classification. Firstly, pre-trained CNN and word embedding models are applied to extract visual features and represent each word in a text as a vector, respectively. Secondly, textual features are extracted by employing a CNN model for text data. Finally, pairs of features extracted through the text and image CNNs are concatenated and input to convolutional layer which can obtain a better learn of the important feature information in the integrated representation of image and text. Features extracted from the convolutional layer are input to a fully connected layer to perform classification. Experimental results demonstrate that the proposed algorithm can obtain superior performance compared with other data fusion methods for flower classification using data of images of flowers and their Korean descriptions. More specifically, the accuracy of the proposed algorithm is 10.1% and 14.5% higher than m-CNN and multimodal recurrent neural networks algorithms, respectively. The proposed method can significantly improve the performance of flower classification. The code and related data are publicly available via our GitHub repository.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420302797",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Contextual image classification",
      "Convolutional neural network",
      "Embedding",
      "Feature (linguistics)",
      "Geometry",
      "Image (mathematics)",
      "Law",
      "Layer (electronics)",
      "Linguistics",
      "Matching (statistics)",
      "Mathematics",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Statistics",
      "Word (group theory)",
      "Word embedding"
    ],
    "authors": [
      {
        "surname": "Bae",
        "given_name": "Kang Il"
      },
      {
        "surname": "Park",
        "given_name": "Junghoon"
      },
      {
        "surname": "Lee",
        "given_name": "Jongga"
      },
      {
        "surname": "Lee",
        "given_name": "Yungseop"
      },
      {
        "surname": "Lim",
        "given_name": "Changwon"
      }
    ]
  },
  {
    "title": "Fake news detection using an ensemble learning model based on Self-Adaptive Harmony Search algorithms",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113584",
    "abstract": "In general, the features of fake news are almost the same as those of real news, so it is not easy to identify them. In this paper, we propose a fake news detection system using a deep learning model. First, news articles are preprocessed and analyzed based on different training models. Then, an ensemble learning model combining four different models called embedding LSTM, depth LSTM, LIWC CNN, and N-gram CNN is proposed for fake news detection. Besides, to achieve higher accuracy in fake news detection, the optimized weights of the ensemble learning model are determined using the Self-Adaptive Harmony Search (SAHS) algorithm. In the experiments, we verify that the proposed model is superior to the state-of-the-art methods, with the highest accuracy of 99.4%. Furthermore, we also investigate the cross-domain intractability issue and achieve the highest accuracy of 72.3%. Finally, we believe there is still room for improving the ensemble learning model in addressing the cross-domain intractability issue.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420304085",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Deep learning",
      "Domain (mathematical analysis)",
      "Embedding",
      "Ensemble learning",
      "Harmony search",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Yin-Fu"
      },
      {
        "surname": "Chen",
        "given_name": "Po-Hong"
      }
    ]
  },
  {
    "title": "Convolutional neural network based emotion classification using electrodermal activity signals and time-frequency features",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113571",
    "abstract": "In this work, an attempt has been made to classify emotional states using Electrodermal Activity (EDA) signals and Convolutional Neural Network (CNN) learned features. The EDA signals are obtained from the publicly available DEAP database and are decomposed into tonic and phasic components. The phasic component is subjected to the short-time Fourier transform. Thirty-eight features of time, frequency, and time–frequency domain are extracted from the phasic signal. These extracted features are applied to CNN to learn robust and prominent features. Five machine learning algorithms, namely linear discriminant analysis, multilayer perceptron, support vector machine, decision tree, and extreme learning machine are used for the classification. The results show that the proposed approach is able to classify the emotional states using arousal-valence dimensions. Classification using CNN learned features are found to be better than the conventional features. The trained end-to-end CNN model is found to be accurate (F-measure = 79.30% and 71.41% for arousal and valence dimensions) in classifying various emotional states. The proposed method is found to be robust in handling the dynamic variation of EDA signals for different emotional states. The results show that the proposed approach outperformed most of the state-of-the-art methods. Thus, it appears that the proposed method could be beneficial in analyzing various emotional states in both normal and clinical conditions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742030395X",
    "keywords": [
      "Arousal",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Decision tree",
      "Emotion classification",
      "Feature extraction",
      "Frequency domain",
      "Linear discriminant analysis",
      "Machine learning",
      "Multilayer perceptron",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Psychology",
      "Speech recognition",
      "Support vector machine",
      "Time domain"
    ],
    "authors": [
      {
        "surname": "Ganapathy",
        "given_name": "Nagarajan"
      },
      {
        "surname": "Veeranki",
        "given_name": "Yedukondala Rao"
      },
      {
        "surname": "Swaminathan",
        "given_name": "Ramakrishnan"
      }
    ]
  },
  {
    "title": "Vague set theory based segmented image fusion technique for analysis of anatomical and functional images",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113592",
    "abstract": "The present study focuses on the design of feature based efficient fusion scheme using multiscale shift invariant shearlet transform. For enhancing the obscured but spectacular detailing of brain MRI which is necessary for analyzing the affected tissues, a flexible approach of vague set (VS) theory based segmentation method has been proposed. The existence of non null hesitation in VS theory makes it appropriate for modeling the high degrees of uncertainties/impreciseness presents in MRI. Hence the proposed segmentation method efficiently captures the salient features and fine structures of MRI and makes the irrelevant artifacts smooth. As the relevant local information related to energy activity level, dominant textural variation, spatial structures belonging to edges/contours, likelihood of neighboring contrast distribution, signal complexity are important to produce the fused image, various indices such as root mean square value of local energy (RMSLE), local information entropy (LIE), local contrast index (LCI) and local standard deviation (LSTD) of subband coefficients are captured by placing a 3 × 3 kernel. In this view, principle component analysis (PCA) approach is addressed to produce a composite subband (CSb) image carrying all those local information. Finally, the fused image is constructed based on the strength of CSb for all source images. Experimental results show that the proposed fusion approach is able to integrate utmost information content of the source images and preserve color saliency very efficiently.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420304164",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Entropy (arrow of time)",
      "Estimator",
      "Invariant (physics)",
      "Kernel (algebra)",
      "Kernel density estimation",
      "Mathematical physics",
      "Mathematics",
      "Mutual information",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Salient",
      "Segmentation",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Mukherjee",
        "given_name": "Suranjana"
      },
      {
        "surname": "Das",
        "given_name": "Arpita"
      }
    ]
  },
  {
    "title": "DHLP 1&2: Giraph based distributed label propagation algorithms on heterogeneous drug-related networks",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113640",
    "abstract": "Background and objective Heterogeneous complex networks are large graphs consisting of different types of nodes and edges. The knowledge extraction from these networks is complicated. Moreover, the scale of these networks is steadily increasing. Thus, scalable methods are required. Methods In this paper, two distributed label propagation algorithms for heterogeneous networks, namely DHLP-1 and DHLP-2 have been introduced. Biological networks are one type of the heterogeneous complex networks. As a case study, we have measured the efficiency of our proposed DHLP-1 and DHLP-2 algorithms on a biological network consisting of drugs, diseases, and targets. The subject we have studied in this network is drug repositioning but our algorithms can be used as general methods for heterogeneous networks other than the biological network. Results We compared the proposed algorithms with similar non-distributed versions of them namely MINProp and Heter-LP. The experiments revealed the good performance of the algorithms in terms of running time and accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420304644",
    "keywords": [
      "Algorithm",
      "Bioinformatics",
      "Biological network",
      "Biology",
      "Complex network",
      "Computer science",
      "Database",
      "Heterogeneous network",
      "Scalability",
      "Telecommunications",
      "Wireless",
      "Wireless network",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Farhangi Maleki",
        "given_name": "Erfan"
      },
      {
        "surname": "Ghadiri",
        "given_name": "Nasser"
      },
      {
        "surname": "Lotfi Shahreza",
        "given_name": "Maryam"
      },
      {
        "surname": "Maleki",
        "given_name": "Zeinab"
      }
    ]
  },
  {
    "title": "A novel data repairing approach based on constraints and ensemble learning",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113511",
    "abstract": "Data repairing is an important task in data mining. This paper proposes a novel data repairing approach based on a combination of constraints and ensemble learning. At first, functional dependencies (FDs) are used as constraints to identify inconsistent records. For each FD, all repeated values in the correct records are discovered. After that, noisy attributes in erroneous records are detected using correct records and the repeated values. To correct the detected noises, a supervised ensemble learning model is constructed for each attribute. The ensemble model consists of a Bayes classifier, a decision tree, and a MultiLayer Perceptron (MLP). A majority of votes is used as the combination strategy in the ensemble learning model. The proposed approach automatically repairs data without any user interaction. Moreover, the proposed method can detect more than one noise in a record. Experimental results show that our approach outperforms similar repairing algorithms (HoloClean and KATARA) in both terms of precision and recall.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420303353",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Decision tree",
      "Economics",
      "Ensemble learning",
      "Machine learning",
      "Management",
      "Multilayer perceptron",
      "Naive Bayes classifier",
      "Pattern recognition (psychology)",
      "Support vector machine",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Ataeyan",
        "given_name": "Mahdieh"
      },
      {
        "surname": "Daneshpour",
        "given_name": "Negin"
      }
    ]
  },
  {
    "title": "AGCN: Attention-based graph convolutional networks for drug-drug interaction extraction",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113538",
    "abstract": "Extracting drug-drug interaction (DDI) relations is one of the most typical tasks in the field of biomedical relation extraction. Automatic DDI extraction from the biomedical corpus is central to the mining of knowledge hidden in the biomedical literature. Existing approaches for DDI extraction primarily focus on either the contextual or the structural information of the sentence, despite their complementary role. Also, previous studies do not even exploit the entire knowledge of the input sentence, which could lead to a loss of crucial clues. In this paper, we propose an Attention-based Graph Convolutional Networks (AGCN) to address these issues. In contrast to the existing DDI extraction methods, the AGCN is designed to leverage contextual and structural knowledge together, where GCN is employed in combination with encoders based on recurrent networks. Additionally, we apply a novel attention-based pruning strategy to optimallyuse syntactic information while ignoring irrelevant information, in contrast to previous rule-based pruning methods. Therefore, AGCN can take advantage of the context and structure of the input sentence as efficiently as possible. We evaluate our model using a dominant DDI extraction corpus. The experimental results demonstrate the effectiveness of our model, which outperforms existing approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420303626",
    "keywords": [
      "Agronomy",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Encoder",
      "Graph",
      "Information extraction",
      "Knowledge graph",
      "Leverage (statistics)",
      "Machine learning",
      "Natural language processing",
      "Operating system",
      "Paleontology",
      "Pruning",
      "Relationship extraction",
      "Sentence",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Park",
        "given_name": "Chanhee"
      },
      {
        "surname": "Park",
        "given_name": "Jinuk"
      },
      {
        "surname": "Park",
        "given_name": "Sanghyun"
      }
    ]
  },
  {
    "title": "A novel path planning approach for smart cargo ships based on anisotropic fast marching",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113558",
    "abstract": "Path planning is an essential tool for smart cargo ships that navigate in coastal waters, inland waters or other crowded waters. These ships require expert and intelligent systems to plan safe paths in order to avoid collision with both static and dynamic obstacles. This research proposes a novel path planning approach based on the anisotropic fast marching (FM) method to specifically assist with safe operations in complex marine navigation environments. A repulsive force field is specially produced to describe the safe area distribution surrounding obstacles based on the knowledge of human. In addition, a joint potential field is created to evaluate the travel cost and a gradient descent method is used to search for appropriate paths from the start point to the end point. Meanwhile, the approach can be used to constantly optimize the paths with the help of the expert knowledge in collision avoidance. Particularly, the approach is validated and evaluated through simulations. The obtained results show that it is capable of providing a reasonable and smooth path in a crowded waters. Moreover, the ability of this approach exhibits a significant contribution to the development of expert and intelligent systems in autonomous collision avoidance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420303821",
    "keywords": [
      "Artificial intelligence",
      "Collision",
      "Collision avoidance",
      "Computer network",
      "Computer science",
      "Computer security",
      "Engineering",
      "Fast marching method",
      "Field (mathematics)",
      "Geology",
      "Geometry",
      "Mathematics",
      "Motion planning",
      "Operations research",
      "Paleontology",
      "Path (computing)",
      "Plan (archaeology)",
      "Point (geometry)",
      "Pure mathematics",
      "Real-time computing",
      "Robot",
      "Simulation"
    ],
    "authors": [
      {
        "surname": "Yan",
        "given_name": "Xin-ping"
      },
      {
        "surname": "Wang",
        "given_name": "Shu-wu"
      },
      {
        "surname": "Ma",
        "given_name": "Feng"
      },
      {
        "surname": "Liu",
        "given_name": "Yuan-chang"
      },
      {
        "surname": "Wang",
        "given_name": "Jin"
      }
    ]
  },
  {
    "title": "Combination of spatially enhanced bag-of-visual-words model and genuine difference subspace for fake coin detection",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113551",
    "abstract": "Fake coins are harmful for society, the detection of which is of paramount importance. Due to the large quantities of fake coins in the real world, it is impossible to examine them manually. To address this issue, we present an intelligent system to automatically detect fake coins based on their images. The intelligent system consists of two components: coin image representation and classifier learning. To represent the coin image, a new spatially enhanced bag-of-visual-words model, called SEBOVW model, is proposed. Afterwards, we improve the representation by building a genuine difference subspace. The coin is finally represented based on its projection onto this subspace. In order to discriminate between genuine and fake coins, we train a classifier using the subspace representations. A thorough evaluation of the proposed intelligent system has been conducted on four coin datasets, consisting of thousands of coins of different denominations and from two countries. Promising experimental results in excess of 98 % accuracy demonstrate its effectiveness and validity.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420303754",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Bag-of-words model",
      "Bag-of-words model in computer vision",
      "Classifier (UML)",
      "Computer science",
      "Computer vision",
      "Image (mathematics)",
      "Image retrieval",
      "Law",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Projection (relational algebra)",
      "Random subspace method",
      "Representation (politics)",
      "Subspace topology",
      "Visual Word"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Li"
      },
      {
        "surname": "Qiu",
        "given_name": "Taorong"
      },
      {
        "surname": "Lu",
        "given_name": "Yue"
      },
      {
        "surname": "Chen",
        "given_name": "Qiu"
      },
      {
        "surname": "Suen",
        "given_name": "Ching Y."
      }
    ]
  },
  {
    "title": "An efficient convolutional neural network for coronary heart disease prediction",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113408",
    "abstract": "This study proposes an efficient neural network with convolutional layers to classify significantly class-imbalanced clinical data. The data is curated from the National Health and Nutritional Examination Survey (NHANES) with the goal of predicting the occurrence of Coronary Heart Disease (CHD). While the majority of the existing machine learning models that have been used on this class of data are vulnerable to class imbalance even after the adjustment of class-specific weights, our simple two-layer CNN exhibits resilience to the imbalance with fair harmony in class-specific performance. Given a highly imbalanced dataset, it is often challenging to simultaneously achieve a high class 1 (true CHD prediction rate) accuracy along with a high class 0 accuracy, as the test data size increases. We adopt a two-step approach: first, we employ least absolute shrinkage and selection operator (LASSO) based feature weight assessment followed by majority-voting based identification of important features. Next, the important features are homogenized by using a fully connected layer, a crucial step before passing the output of the layer to successive convolutional stages. We also propose a training routine per epoch, akin to a simulated annealing process, to boost the classification accuracy. Despite a high class imbalance in the NHANES dataset, the investigation confirms that our proposed CNN architecture has the classification power of 77% to correctly classify the presence of CHD and 81.8% to accurately classify the absence of CHD cases on a testing data, which is 85.70% of the total dataset. This result signifies that the proposed architecture can be generalized to other studies in healthcare with a similar order of features and imbalances. While the recall values obtained from other machine learning methods, such as SVM and random forest, are comparable to that of our proposed CNN model, our model predicts the negative (Non-CHD) cases with higher accuracy. Our model architecture exhibits a way forward to develop better investigative tools, improved medical treatment and lower diagnostic costs by incorporating a smart diagnostic system in the healthcare system. The balanced accuracy of our model (79.5%) is also better than individual accuracies of SVM or random forest classifiers. The CNN classifier results in high specificity and test accuracy along with high values of recall and area under the curve (AUC).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420302323",
    "keywords": [],
    "authors": [
      {
        "surname": "Dutta",
        "given_name": "Aniruddha"
      },
      {
        "surname": "Batabyal",
        "given_name": "Tamal"
      },
      {
        "surname": "Basu",
        "given_name": "Meheli"
      },
      {
        "surname": "Acton",
        "given_name": "Scott T."
      }
    ]
  },
  {
    "title": "Best practices in Analytic Network Process studies",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113536",
    "abstract": "It has been more than 20 years since the appearance of the Analytic Network Process (ANP) in the scientific literature. Since that time, this method has been used to address complex decision-making situations and capture the dependency and feedback among the different elements in the decision model. Yet, a review of ANP studies published in 2015 shows that the reports of these studies are either deficient or incomplete in the analysis or reporting to the point that it casts a shadow on the validity of their conclusions. We propose, to our knowledge for the first time, a set of best practices to conduct, analyze and report ANP studies.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420303602",
    "keywords": [
      "Analytic hierarchy process",
      "Analytic network process",
      "Artificial intelligence",
      "Best practice",
      "Computer science",
      "Data mining",
      "Data science",
      "Dependency (UML)",
      "Economics",
      "Geometry",
      "Management",
      "Mathematics",
      "Operating system",
      "Operations research",
      "Point (geometry)",
      "Process (computing)",
      "Programming language",
      "Psychology",
      "Psychotherapist",
      "Set (abstract data type)",
      "Shadow (psychology)"
    ],
    "authors": [
      {
        "surname": "Mu",
        "given_name": "Enrique"
      },
      {
        "surname": "Cooper",
        "given_name": "Orrin"
      },
      {
        "surname": "Peasley",
        "given_name": "Michael"
      }
    ]
  },
  {
    "title": "An analytical system for evaluating academia units based on metrics provided by academic social network",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113608",
    "abstract": "Social networks are becoming more and more popular, not only among young people looking for entertainment, but also among specialists, experts and researchers who wish to establish professional networks, develop business or research projects. They may be useful also for the comparison and evaluation of scientists and research organizations. This study aims to show how to build a framework of an analytical system for evaluation of researchers and research units using the data retrieved from an academic social network. Acquired data are used to find out the main differences between ResarchGate (RG) usage and values of metrics owned by scientists of different gender, scientific title and field of study to find out if various groups of employees can be directly compared. The authors apply web scraping technique for collecting data from university web page (2847 employees) and use R scripts to acquire the metrics form RG portal. Also, data of 1497 researchers and teaching workers from 11 faculties at Polish university were explored. The descriptive statistics, Chi square test, ANOVA and logistic regression were used to analyse the main RG metrics: RG Score, number of publications, reads and citations. Analysis shows the significant differences both in terms of popularity of ResearchGate and values of its main metrics. The research confirmed that 1) the rvest package allows for fast data acquisition from RG, 2) RG metrics can be used by university managers to compare achievements and progress of single researchers, research labs, departments or faculties, 3) Researchers employed at the faculties of formal and natural sciences use RG portal more frequently, possess higher values of RG metrics, therefore different types of workers and various branches of science shouldn’t be compared directly.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420304322",
    "keywords": [
      "Computer science",
      "Data science",
      "Field (mathematics)",
      "Mathematics",
      "Operating system",
      "Popularity",
      "Psychology",
      "Pure mathematics",
      "Scripting language",
      "Social media",
      "Social network analysis",
      "Social psychology",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Wiechetek",
        "given_name": "Lukasz"
      },
      {
        "surname": "Phusavat",
        "given_name": "Kongkiti"
      },
      {
        "surname": "Pastuszak",
        "given_name": "Zbigniew"
      }
    ]
  },
  {
    "title": "Machining sensor data management for operation-level predictive model",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113600",
    "abstract": "Effective transition from raw industrial data to knowledge-based executive actions without human action requires developing new analytical tools, what also means new challenges for expert and intelligent systems. Studies must be conducted especially on developing effective analytical solutions for intelligent modules of Computerized Maintenance Management Systems, that take advantage of data analysis and decision support tools to predict and prevent the potential failure of machines or its elements. This is why the idea of a new classifier for condition assessment and Remaining Useful Life (RUL) prediction as an expert system tool for real-time monitoring of the manufacturing process was presented. Based on monitoring and current system check data, a new method enabling both early prediction of the machine tool’s remaining useful life and its current condition classification was devised. Its failure and normal properties were distinguished as well. To this end, it was proposed that the remaining useful life prediction should be made via the combined use of the Support Vector Machine (SVM) as a classification tool and AutoRegressive and Integrated Moving Average (ARIMA) based identification. This would provide process engineers and machine operators with an expert system that is easy to implement and use at the operational level, thus allowing them confidently perform technological processes, according to the acceptable failure probability.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420304243",
    "keywords": [
      "Artificial intelligence",
      "Autoregressive integrated moving average",
      "Computer science",
      "Data mining",
      "Decision support system",
      "Engineering",
      "Expert system",
      "Machine learning",
      "Machine tool",
      "Machining",
      "Mechanical engineering",
      "Operating system",
      "Process (computing)",
      "Prognostics",
      "Reliability engineering",
      "Support vector machine",
      "Time series"
    ],
    "authors": [
      {
        "surname": "Kozłowski",
        "given_name": "Edward"
      },
      {
        "surname": "Mazurkiewicz",
        "given_name": "Dariusz"
      },
      {
        "surname": "Żabiński",
        "given_name": "Tomasz"
      },
      {
        "surname": "Prucnal",
        "given_name": "Sławomir"
      },
      {
        "surname": "Sęp",
        "given_name": "Jarosław"
      }
    ]
  },
  {
    "title": "A novel tree-based dynamic heterogeneous ensemble method for credit scoring",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113615",
    "abstract": "Ensemble models have been extensively applied to credit scoring. However, advanced tree-based classifiers have been seldom utilized as components of ensemble models. Moreover, few studies have considered dynamic ensemble selection. To fill the research gap, this paper aims to develop a novel tree-based overfitting-cautious heterogeneous ensemble model (i.e., OCHE) for credit scoring which departs from existing literature on base models and ensemble selection strategy. Regarding base models, tree-based techniques are employed to acquire a balance between predictive accuracy and computational cost. In terms of ensemble selection, the proposed method can assign weights to base models dynamically according to the overfitting measure. Validated on five public datasets, the proposed approach is compared with several popular benchmark models and selection strategies on predictive accuracy and computational cost measures. For predictive accuracy, the proposed approach outperforms the benchmark models significantly in most cases based on the non-parametric significance test. It also performs marginally better than several state-of-the-art studies. Our proposal remains robust in several scenarios. In terms of computational cost, the proposed method provides acceptable performance and benefits from GPU acceleration considerably.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420304395",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Computer science",
      "Data mining",
      "Decision tree",
      "Ensemble forecasting",
      "Ensemble learning",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Model selection",
      "Overfitting",
      "Parametric statistics",
      "Selection (genetic algorithm)",
      "Statistics",
      "Tree (set theory)"
    ],
    "authors": [
      {
        "surname": "Xia",
        "given_name": "Yufei"
      },
      {
        "surname": "Zhao",
        "given_name": "Junhao"
      },
      {
        "surname": "He",
        "given_name": "Lingyun"
      },
      {
        "surname": "Li",
        "given_name": "Yinguo"
      },
      {
        "surname": "Niu",
        "given_name": "Mengyi"
      }
    ]
  },
  {
    "title": "GSDroid: Graph Signal Based Compact Feature Representation for Android Malware Detection",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113581",
    "abstract": "Android malwares have evolved in sophistications and intelligence to become more and more evasive to existing detection systems especially those that are signature-based. Machine learning techniques have risen to become a better choice for detecting emerging sophisticated and intelligent Android malwares because of their high prediction accuracies in unseen data. The performance of a machine learning classifier highly depends on the optimal feature representation of the data points. The main behavior of an Android application usually gets reflected in its generated system call sequence. Therefore, system call analysis mechanisms are considered to be more effective for malware detection. The existing feature representations for system call based malware detection mechanisms have high dimensionality and lack the system call dependency relations. Curse of dimensionality (high dimensionality of feature vectors) is a problem in which the higher number of feature components in a feature vector will result in high sparsity in the data. This high sparsity in data can unnecessarily increase the storage space and processing time of the classifier. In order to overcome these limitations, in this paper, we propose a novel graph signal based low dimensional feature representation and extraction mechanism to detect Android malware applications. These low dimensional features are then incorporated as part of a machine learning based intelligent expert system that can automate the malware detection task. In our experimental evaluations, we found that a feature vector of dimension 16 was enough for malware classification when used with random forest and decision tree classifiers. We obtained a maximum accuracy of 0.99 with a feature vector of dimension 16 corresponding to the signal values of 16 prominent system calls when random forest classifier was applied.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742030405X",
    "keywords": [
      "Android (operating system)",
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Curse of dimensionality",
      "Data mining",
      "Decision tree",
      "Dimensionality reduction",
      "Feature extraction",
      "Feature vector",
      "Machine learning",
      "Malware",
      "Operating system",
      "Pattern recognition (psychology)",
      "Random forest",
      "Support vector machine",
      "System call"
    ],
    "authors": [
      {
        "surname": "Surendran",
        "given_name": "Roopak"
      },
      {
        "surname": "Thomas",
        "given_name": "Tony"
      },
      {
        "surname": "Emmanuel",
        "given_name": "Sabu"
      }
    ]
  },
  {
    "title": "The impact of stock market price Fourier transform analysis on the Gated Recurrent Unit classifier model",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113565",
    "abstract": "In this paper, we suggest new feature extraction models based on the stock market price signal analysis. In particular, we study the behavior observed in signals originating from different sources, such as prices of different Limit Order Book levels and open, close, low, high prices of the preselected time intervals. We apply Fourier transformation to extract new features. Moreover, we evaluate if the performance of the model based on the Gated Recurrent Unit (GRU) architecture is improved when we select features utilizing the proposed methods. Furthermore, we benchmark the performance of new indicators on the GRU model and provide quantified results proving the significant performance improvement obtained by incorporating the suggested features.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420303894",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Econometrics",
      "Engineering",
      "Feature extraction",
      "Fourier transform",
      "Geodesy",
      "Geography",
      "Horse",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Mechanical engineering",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Series (stratigraphy)",
      "Stock (firearms)",
      "Stock market",
      "Stock price"
    ],
    "authors": [
      {
        "surname": "Radojičić",
        "given_name": "Dragana"
      },
      {
        "surname": "Kredatus",
        "given_name": "Simeon"
      }
    ]
  },
  {
    "title": "Exploratory differential ant lion-based optimization",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113548",
    "abstract": "In this work, an improved alternative method of the ant lion optimizer (ALO), integrating opposition-based training with two practical operators on the basis of differential evolution, named MALO, is proposed to cope with the implied weaknesses of classical ALO. Firstly, opposition-based practice is adopted into the ALO to prevent it from the searching deflation and obtain a faster convergence rate. Besides, two more operators, mutation and crossover strategies are implemented to further improve the local searching efficiency of the agents. Additionally, to verify the effectiveness of the enhanced process, comparison with existing optimizers was conducted for different benchmark functions with different qualities likewise unimodal, multimodal, and fixed-dimensional multimodaltasks were also carried out. Moreover, the extensibility test is, undertaken to assess the dimensional influence on problem consistency and optimization quality. Furthermore, the enhanced method is exploited to crack three practical, well-known constrained optimization problems, including spring plan, the concern of the welded beam case and the subject of a pressure vessel. The findings show that the introduced strategies will significantly enhance ALO's capability in optimizing different tasks. Promisingly, the proposed approach can be viewed as an efficient and effective strategy for more optimization scenarios.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420303729",
    "keywords": [
      "Ant colony optimization algorithms",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Crossover",
      "Differential evolution",
      "Geodesy",
      "Geography",
      "Mathematical optimization",
      "Mathematics"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Mingjing"
      },
      {
        "surname": "Heidari",
        "given_name": "Ali Asghar"
      },
      {
        "surname": "Chen",
        "given_name": "Mengxiang"
      },
      {
        "surname": "Chen",
        "given_name": "Huiling"
      },
      {
        "surname": "Zhao",
        "given_name": "Xuehua"
      },
      {
        "surname": "Cai",
        "given_name": "Xueding"
      }
    ]
  },
  {
    "title": "Classification of non-small cell lung cancer using one-dimensional convolutional neural network",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113564",
    "abstract": "Non-Small Cell Lung Cancer (NSCLC) is a major lung cancer type. Proper diagnosis depends mainly on tumor staging and grading. Pathological prognosis often faces problems because of the limited availability of tissue samples. Machine learning methods may play a vital role in such cases. 2D or 3D Deep Neural Networks (DNNs) has been the predominant technology in this domain. Contemporary studies tried to classify NSCLC tumors as benign or malignant. The application of 1D CNN in automated staging and grading of NSCLC is not very frequent. The aim of the present study is to develop a 1D CNN model for automated staging and grading of NSCLC. The updated NSCLC Radiogenomics Collection from The Cancer Imaging Archive (TCIA) was used in the study. The segmented tumor images were fed into a hybrid feature detection and extraction model (MSER-SURF). The extracted features were clubbed with the clinical TNM stage and histopathological grade information and fed into the 1D CNN model. The performance of the proposed CNN model was satisfactory. The accuracy and ROC-AUC score were higher than the other leading machine learning methods. The study also did well compared to state-of-the-art studies. The proposed model shows that 1D CNN is equally useful in NSCLC prediction like a conventional 2D/3D CNN model. The model may further be refined by carrying out experiments with varied hyper-parameters. Further studies may be conducted by considering semi-supervised or unsupervised learning techniques.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420303882",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Civil engineering",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Engineering",
      "Feature extraction",
      "Grading (engineering)",
      "Lung cancer",
      "Machine learning",
      "Medicine",
      "Pathology",
      "Pattern recognition (psychology)",
      "Radiogenomics",
      "Radiomics"
    ],
    "authors": [
      {
        "surname": "Moitra",
        "given_name": "Dipanjan"
      },
      {
        "surname": "Kr. Mandal",
        "given_name": "Rakesh"
      }
    ]
  },
  {
    "title": "Pixel sampling by clustering",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113576",
    "abstract": "In this paper, we describe Pixel Sampling Clustering Technique (PSCT), a data-driven sampling procedure used to reduce pixel sets. We view the pixels in an image as a high redundancy 3D space. We also refer to this space as our color model. Our method aims to retain a relevant sample of the data so it can act as a new smaller, hence more efficient, color model. PSCT uses a pair of fast density-based clustering algorithms in tandem. First, it applies Birch and then DBSCAN to keep the most densely represented colors. We cluster the resulting color model and use the labels to segment images. We also complement the sampling method with a refinement algorithm intended to improve color representation. In our paper, we show how to reconstruct images using our reduced color model. We also show that reconstructed images have enough information to perform image related learning tasks with almost the same accuracy than the original images but with only a small fraction of the data. We test our sampling method in three image related supervised and unsupervised tasks and compare them with state-of-the-art methods. For our experiments, we use two image datasets: MIT’s Vision Texture Dataset and Berkeley’s BSD500.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420304000",
    "keywords": [
      "Artificial intelligence",
      "Canopy clustering algorithm",
      "Cluster analysis",
      "Computer science",
      "Computer vision",
      "Correlation clustering",
      "DBSCAN",
      "Filter (signal processing)",
      "Pattern recognition (psychology)",
      "Pixel",
      "Sampling (signal processing)"
    ],
    "authors": [
      {
        "surname": "Bayá",
        "given_name": "Ariel E."
      },
      {
        "surname": "Larese",
        "given_name": "Mónica G."
      }
    ]
  },
  {
    "title": "An interactive preference-guided firefly algorithm for personalized tourist itineraries",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113563",
    "abstract": "The present research proposes an interactive optimization framework to aid tourists to organize their trip by generating personalized walking itineraries among several Points of Interest (POIs). The solution of the multi-objective Prize-Collecting Vehicle Routing Problem (MO-PCVRP) is used to simulate this tourist trip design problem. The objectives of the proposed formulation are the minimization of the total distance walked among selected POIs, the minimization of a fixed cost related to the number of the created itineraries, and the maximization of the total satisfaction gained by visiting the selected POIs. The optimization of the MO-PCVRP is conducted by the proposed Preference-Guided Firefly Algorithm (PGFA), which allows for preferences articulated by a decision-maker (DM) to guide the search. The PGFA is incorporated into an interactive framework, where a DM provides his/her preferential information, progressively during the optimization process, by ranking a small representative set of Pareto optimal solutions. The DM’s articulated preferences are elicited utilizing a preference disaggregation method, the UTASTAR, which results in a preference model, which is ultimately used to guide the search towards the DM’s Region of Interest (ROI) in the Pareto front. The effectiveness and robustness of the proposed interactive PGFA framework are demonstrated over experimental scenarios.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420303870",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Biology",
      "Chemistry",
      "Computer science",
      "Decision maker",
      "Firefly algorithm",
      "Firefly protocol",
      "Gene",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Maximization",
      "Minification",
      "Multi-objective optimization",
      "Operations research",
      "Pareto principle",
      "Particle swarm optimization",
      "Point of interest",
      "Preference",
      "Programming language",
      "Ranking (information retrieval)",
      "Robustness (evolution)",
      "Set (abstract data type)",
      "Statistics",
      "World Wide Web",
      "Zoology"
    ],
    "authors": [
      {
        "surname": "Trachanatzi",
        "given_name": "Dimitra"
      },
      {
        "surname": "Rigakis",
        "given_name": "Manousos"
      },
      {
        "surname": "Marinaki",
        "given_name": "Magdalene"
      },
      {
        "surname": "Marinakis",
        "given_name": "Yannis"
      }
    ]
  },
  {
    "title": "The application of PROMETHEE multi-criteria decision aid in financial decision making: Case of distress prediction models evaluation",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113438",
    "abstract": "Conflicting rankings corresponding to alternative performance criteria and measures are mostly reported in the mono-criterion evaluation of competing distress prediction models (DPMs). To overcome this issue, this study extends the application of the expert system to corporate credit risk and distress prediction through proposing a Multi-criteria Decision Aid (MCDA), namely PROMETHEE II, which provides a multi-criteria evaluation of competing DPMs. In addition, using data on Chinese firms listed on Shanghai and Shenzhen stock exchanges, we perform an exhaustive comparative analysis of the most popular DPMs; namely, statistical, artificial intelligence and machine learning models under both mono-criterion and multi-criteria frameworks. Further, we address two prevailing research questions; namely, \"which DPM performs better in predicting distress?\" and \"will training models with corporate governance indicators (CGIs) enhance the performance of models?”; and discuss our findings. Our multi-criteria ranking suggests that non-parametric DPMs outperform parametric ones, where random forest and bagging CART are among the best machine learning DPMs. Further, models fed with CGIs as features outperform those fed without CGIs.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420302621",
    "keywords": [
      "Artificial intelligence",
      "Bankruptcy",
      "Bankruptcy prediction",
      "Business",
      "Computer science",
      "Data mining",
      "Finance",
      "Financial distress",
      "Financial system",
      "Machine learning",
      "Mathematics",
      "Multiple-criteria decision analysis",
      "Operations research",
      "Parametric statistics",
      "Random forest",
      "Ranking (information retrieval)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Mousavi",
        "given_name": "Mohammad Mahdi"
      },
      {
        "surname": "Lin",
        "given_name": "Jilei"
      }
    ]
  },
  {
    "title": "A new prediction method for recommendation system based on sampling reconstruction of signal on graph",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113587",
    "abstract": "Recommendation technology is widely used in various e-commerce platforms. Accurately predicting user’s preference is the most important goal of recommendation technology. One of the core difficulties of recommendation technology that the rating matrices are seriously sparse. However, the unknown entries in the rating matrix actually contain a lot of useful information for prediction, which are usually discarded in traditional methods. Based on the idea of semi-supervised learning, this paper models the recommendation problem as a signal reconstruction problem on a graph. The new model utilizes both the information of the unlabeled samples and the location information, and thus achieves an excellent predictive performance. Meanwhile, to reduce the computational complexity a strategy is designed skillfully to approximately solve the model. Experimental results shows that the proposed method significantly outperforms the reference methods in predictive accuracy and is robust to the diversity of data sets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420304115",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Filter (signal processing)",
      "Graph",
      "Machine learning",
      "Recommender system",
      "Sampling (signal processing)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Zhihua"
      },
      {
        "surname": "Zhou",
        "given_name": "Feng"
      },
      {
        "surname": "Yang",
        "given_name": "Lihua"
      },
      {
        "surname": "Zhang",
        "given_name": "Qian"
      }
    ]
  },
  {
    "title": "Post Pareto-optimal ranking algorithm for multi-objective optimization using extended angle dominance",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113446",
    "abstract": "This paper presents a solution ranking algorithm to find the outstanding solutions in given set of non-dominated solutions of multi-objective optimization problems, which are the results from either Multi-Objective Evolutionary Algorithms (MOEAs) or exact methods. The algorithm enables the decision makers to identify outstanding solutions without a deep understanding of the problem. The algorithm provides a ranking for all solutions so that they can obtain any top K ranked solutions to implement. This novel parameter-free solution ranking approach is based on two concepts: an extended angle-based dominance technique from the algorithm called ADaptive angle-based pruning Algorithm (ADA) for discovering the knee solutions and the inverse-square law of light for enhancing the diversity of solutions. We evaluate the performance of the approach on several well-known test problems against well-known knee finding algorithms as well as on a practical system design and optimization problem to demonstrate the usefulness of the algorithm.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420302700",
    "keywords": [
      "Agronomy",
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Evolutionary algorithm",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Multi-objective optimization",
      "Pareto optimal",
      "Programming language",
      "Pruning",
      "Ranking (information retrieval)",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Choachaicharoenkul",
        "given_name": "Supoj"
      },
      {
        "surname": "Wattanapongsakorn",
        "given_name": "Naruemon"
      }
    ]
  },
  {
    "title": "An EEG based familiar and unfamiliar person identification and classification system using feature extraction and directed functional brain network",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113448",
    "abstract": "People are extremely proficient at recognizing familiar person, but are much worse at matching unfamiliar one. However, the neural correlation of this proposed difference in neural representations of familiar and unfamiliar identities remains unclear. New methods of EEG data analysis, functional networks and time frequency analyses, are highly recommended to advance the knowledge of those brain mechanisms. Developing an EEG based pattern recognition system could potentially be used to improve the current person recognition strategies. In this study, we designed a multi-channel EEG based pattern recognition system for person recognition. To do this, a new feature extraction method combining directed functional network analysis and signal complexity of EEGs from different brain regions was proposed, which is the main contribution of this paper. The proposed method was tested in an experiment of 20 subjects underlying visual and auditory stimuli simultaneously. The features were calculated in delta, theta, alpha and beta band respectively, then SVM and KNN classifiers were applied to these feature sets and the results showed the recognition accuracies of these four bands are relatively stable with the best accuracy of 90.58% in delta band for SVM. In addition, theta and alpha band also showed good performance for the two classifiers. It indicated delta wave is the best sub band for person perception and SVM is better than KNN in this system. This work is the first time to construct the directed functional network in person recognition study, and it demonstrated the combination of non-linear complexities and network features are efficient for EEG based expert and intelligent system for person recognition.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420302724",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Electroencephalography",
      "Feature (linguistics)",
      "Feature extraction",
      "Linguistics",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Psychology",
      "Speech recognition",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Chang",
        "given_name": "Wenwen"
      },
      {
        "surname": "Wang",
        "given_name": "Hong"
      },
      {
        "surname": "Yan",
        "given_name": "Guanghui"
      },
      {
        "surname": "Liu",
        "given_name": "Chong"
      }
    ]
  },
  {
    "title": "Maximum-relevance and maximum-diversity of positive ranks: A novel feature selection method",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113499",
    "abstract": "With the existing abundance of intelligent and expert systems, there is a need for selecting a subset of highly relevant features with low redundancy. In filter approaches, the feature subsets are iteratively computed by evaluating the candidate features in terms of their relevance with the target class and pairwise redundancies. The use mutual information-based metrics has been extensively studied as an approach to quantifying the relevance and redundancy of candidate features. In this study, a novel filter approach based on ranks of positive instances is proposed. In this approach, redundancy is replaced by diversity to quantify the complementarity of a candidate feature with respect to the already selected subset. Both relevance and diversity are computed in terms of the ranks of positive instances, which is analogous to the computation of the area under the receiver operating characteristic curve (AUC). Experiments conducted on 15 UCI and microarray gene expression data sets have confirmed that the proposed multivariate filter feature selection approach provides better performance scores when compared to other competing multivariate methods as well as benchmark univariate filters.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420303237",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Feature selection",
      "Filter (signal processing)",
      "Geodesy",
      "Geography",
      "Law",
      "Machine learning",
      "Minimum redundancy feature selection",
      "Multivariate statistics",
      "Operating system",
      "Pairwise comparison",
      "Pattern recognition (psychology)",
      "Political science",
      "Redundancy (engineering)",
      "Relevance (law)",
      "Univariate"
    ],
    "authors": [
      {
        "surname": "Sheikhi",
        "given_name": "Ghazaal"
      },
      {
        "surname": "Altınçay",
        "given_name": "Hakan"
      }
    ]
  },
  {
    "title": "Stochastic data-driven optimization for multi-class dynamic pricing and capacity allocation in the passenger railroad transportation",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113568",
    "abstract": "As for any passenger transportation service provider, pricing and capacity management are two critical tools for the profitability of a passenger railroad service provider: pricing affects the demand for the services and the capacity management sets the availability of the services in advance. In this study, an expert system is developed as a decision support tool for a passenger railroad service provider’s integrated pricing and capacity management problem, which has great significance for the success of the service provider. Considering the demand uncertainty, we first formulate the integrated pricing and capacity management problem as a stochastic nonlinear integer programming (SNLIP) model. This model includes dynamic pricing and dynamic capacity allocation decisions for multiple service classes over a planning horizon in order to maximize profit. Also, several key characteristics of the passenger railroad service operations are captured in the model. Due to inherent demand uncertainty as well as the dynamic nature of the problem, a fast and efficient solution approach is needed. Therefore, a simulation-based procedure embedded in a simulated annealing method is proposed to solve the model. Several real-life cases from Fadak Five-Star Trains (an Iranian luxurious passenger railroad service provider) are presented to demonstrate the model and the solution approach. The results of the case studies show the operational and profitability impacts of using the proposed decision support tool as well as its potential capabilities for practical use by other service providers.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420303924",
    "keywords": [
      "Business",
      "Capacity management",
      "Computer network",
      "Computer science",
      "Dynamic pricing",
      "Economics",
      "Engineering",
      "Finance",
      "Marketing",
      "Mathematical optimization",
      "Mathematics",
      "Microeconomics",
      "Operations research",
      "Profit (economics)",
      "Profitability index",
      "Service (business)",
      "Service level",
      "Service provider",
      "Time horizon"
    ],
    "authors": [
      {
        "surname": "Kamandanipour",
        "given_name": "Keyvan"
      },
      {
        "surname": "Mahdi Nasiri",
        "given_name": "Mohammad"
      },
      {
        "surname": "Konur",
        "given_name": "Dinçer"
      },
      {
        "surname": "Haji Yakhchali",
        "given_name": "Siamak"
      }
    ]
  },
  {
    "title": "Improved reinforcement learning with curriculum",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113515",
    "abstract": "Humans tend to learn complex abstract concepts faster if examples are presented in a structured manner. For instance, when learning how to play a board game, usually one of the first concepts learned is how the game ends, i.e. the actions that lead to a terminal state (win, lose or draw). The advantage of learning end-games first is that once the actions leading to a terminal state are understood, it becomes possible to incrementally learn the consequences of actions that are further away from a terminal state – we call this an end-game-first curriculum. The state-of-the-art machine learning player for general board games, AlphaZero by Google DeepMind, does not employ a structured training curriculum. Whilst Deepmind’s approach is effective, their method for generating experiences by self-play is resource intensive, costing literally millions of dollars in computational resources. We have developed a new method called the end-game-first training curriculum, which, when applied to the self-play/experience-generation loop, reduces the required computational resources to achieve the same level of learning. Our approach improves performance by not generating experiences which are expected to be of low training value. The end-game-first curriculum enables significant savings in processing resources and is potentially applicable to other problems that can be framed in terms of a game.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420303390",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Curriculum",
      "Game based learning",
      "Game design",
      "Game mechanics",
      "Human–computer interaction",
      "Mathematics education",
      "Meteorology",
      "Multimedia",
      "Pedagogy",
      "Physics",
      "Psychology",
      "Reinforcement learning",
      "State (computer science)",
      "Telecommunications",
      "Terminal (telecommunication)",
      "Training (meteorology)"
    ],
    "authors": [
      {
        "surname": "West",
        "given_name": "Joseph"
      },
      {
        "surname": "Maire",
        "given_name": "Frederic"
      },
      {
        "surname": "Browne",
        "given_name": "Cameron"
      },
      {
        "surname": "Denman",
        "given_name": "Simon"
      }
    ]
  },
  {
    "title": "Opposition-based learning Harris hawks optimization with advanced transition rules: principles and analysis",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113510",
    "abstract": "Harris hawks optimizer (HHO) is a recently developed, efficient meta-heuristic optimization approach, which is inspired by the chasing style and collaborative behavior of Harris hawks in nature. However, for some optimization cases, the algorithm suffers from an immature balance between exploitation and exploration. Therefore, in the present study, four effective strategies are introduced into conventional HHO, such as proposing a non-linear energy parameter for the nergy of prey, differor rapid dives, a greedy selection mechanism, and opposition-based learning. These strategies enhance the search-efficiency of HHO and help to alleviate the issues of stagnation at the sub-optimal solution and premature convergence. A well-known collection of 33 benchmark problems is taken to examine the effectiveness of the proposed m-HHO, and the comparison is performed with conventional HHO and other state-of-the-art algorithms. Accordingly, the proposed m-HHO can serve as an effective and efficient optimization tool for global optimization problems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420303341",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Convergence (economics)",
      "Economic growth",
      "Economics",
      "Geodesy",
      "Geography",
      "Heuristic",
      "Law",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Opposition (politics)",
      "Political science",
      "Politics"
    ],
    "authors": [
      {
        "surname": "Gupta",
        "given_name": "Shubham"
      },
      {
        "surname": "Deep",
        "given_name": "Kusum"
      },
      {
        "surname": "Heidari",
        "given_name": "Ali Asghar"
      },
      {
        "surname": "Moayedi",
        "given_name": "Hossein"
      },
      {
        "surname": "Wang",
        "given_name": "Mingjing"
      }
    ]
  },
  {
    "title": "An estimating combination method for interval forecasting of electrical load time series",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113498",
    "abstract": "Due to the failure of deterministic point forecasting to capture the uncertainty associated with the original time series, and because it can reflect the range of electrical load fluctuation, the importance of probabilistic interval forecasting has gradually increased. However, the existing theoretical system of interval forecasting is still incomplete, is a complicated process, and has relatively low accuracy. The objective of this study is to propose an interval forecasting approach based on feature selection, the optimized machine learning method, and correction of the Gaussian distribution. By applying this approach, the distribution of predictions can be established to include all the information of prediction intervals at each confidence level, making the best use of the known information. The electrical load time series of Australia are used to examine the effectiveness of the proposed approach, and compared with other models, it is proven to not only simplify the forecasting process and shorten the processing time, but also significantly improve the forecasting efficiency, flexibility, and accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420303225",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Combinatorics",
      "Composite material",
      "Computer science",
      "Data mining",
      "Flexibility (engineering)",
      "Gaussian",
      "Gaussian process",
      "Interval (graph theory)",
      "Machine learning",
      "Materials science",
      "Mathematics",
      "Operating system",
      "Paleontology",
      "Physics",
      "Prediction interval",
      "Probabilistic forecasting",
      "Probabilistic logic",
      "Process (computing)",
      "Quantum mechanics",
      "Range (aeronautics)",
      "Series (stratigraphy)",
      "Statistics",
      "Time series"
    ],
    "authors": [
      {
        "surname": "Ma",
        "given_name": "Xuejiao"
      },
      {
        "surname": "Dong",
        "given_name": "Yunxuan"
      }
    ]
  },
  {
    "title": "Enhancing a Pairs Trading strategy with the application of Machine Learning",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113490",
    "abstract": "Pairs Trading is one of the most valuable market-neutral strategies used by hedge funds. It is particularly interesting because it overcomes the arduous process of valuing securities by focusing on relative pricing. By buying a relatively undervalued security and selling a relatively overvalued one, a profit can be made upon the pair’s price convergence. However, with the growing availability of data, it became increasingly harder to find rewarding pairs. In this work, we address two problems: (i) how to find profitable pairs while constraining the search space and (ii) how to avoid long decline periods due to prolonged divergent pairs. To manage these difficulties, the application of promising Machine Learning techniques is investigated in detail. We propose the integration of an Unsupervised Learning algorithm, OPTICS, to handle problem (i). The results obtained demonstrate the suggested technique can outperform the common pairs’ search methods, achieving an average portfolio Sharpe ratio of 3.79, in comparison to 3.58 and 2.59 obtained by standard approaches. For problem (ii), we introduce a forecasting-based trading model, capable of reducing the periods of portfolio decline by 75%. Yet, this comes at the expense of decreasing overall profitability. The proposed strategy is tested using an ARMA model, an LSTM and an LSTM Encoder-Decoder. This work’s results are simulated during varying periods between January 2009 and December 2018, using 5-min price data from a group of 208 commodity-linked ETFs, and accounting for transaction costs.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420303146",
    "keywords": [
      "Algorithmic trading",
      "Alternative trading system",
      "Artificial intelligence",
      "Computer science",
      "Econometrics",
      "Economics",
      "Finance",
      "Financial economics",
      "Machine learning",
      "Microeconomics",
      "Pairs trade",
      "Portfolio",
      "Profit (economics)",
      "Profitability index",
      "Sharpe ratio",
      "Trading strategy"
    ],
    "authors": [
      {
        "surname": "Sarmento",
        "given_name": "Simão Moraes"
      },
      {
        "surname": "Horta",
        "given_name": "Nuno"
      }
    ]
  },
  {
    "title": "Bee Colony Optimization metaheuristic for fuzzy membership functions tuning",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113601",
    "abstract": "The successfulness of the application of fuzzy logic to real-life problems depends on a number of parameters, such as the number and shapes of fuzzy membership functions, which are usually defined upon intuition or the subjective knowledge of relevant experts. One way to improve the performance of the fuzzy reasoning model is to use heuristic or metaheuristic techniques in order to optimize the shapes of membership functions. In this paper, the Bee Colony Optimization (BCO) algorithm is applied as a tool suitable for the specified optimization problem. The BCO belongs to the group of nature-inspired metaheuristics. The purpose of this paper is to present and discuss a strategy for the adjustment of fuzzy logic membership functions using the variant of the BCO algorithm based on the improvement of complete solutions and show its real-life application to the problem of the estimation of freight train energy consumption. According to the obtained results, it can be concluded that the precision of the developed fuzzy reasoning model is significantly increased after tuning membership functions by the BCO.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420304255",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Defuzzification",
      "Epistemology",
      "Fuzzy logic",
      "Fuzzy number",
      "Fuzzy set",
      "Heuristic",
      "Intuition",
      "Mathematical optimization",
      "Mathematics",
      "Membership function",
      "Metaheuristic",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Nikolić",
        "given_name": "Miloš"
      },
      {
        "surname": "Šelmić",
        "given_name": "Milica"
      },
      {
        "surname": "Macura",
        "given_name": "Dragana"
      },
      {
        "surname": "Ćalić",
        "given_name": "Jovana"
      }
    ]
  },
  {
    "title": "Toward security monitoring of industrial Cyber-Physical systems via hierarchically distributed intrusion detection",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113578",
    "abstract": "Industrial Cyber-physical systems (ICPSs), integrating communication, computation and control of industrial processes are referred to as a core technology to approach the Industry 4.0. Ensuring the ICPS security is of paramount importance in smart manufacturing. Considering the characteristics of large-scale, geographically-dispersed and multi-dimensional heterogeneous, federated and life-critical natures of ICPSs, this paper investigates a hierarchically distributed intrusion detection scheme that seeks to achieve the all-round safety protection of ICPSs according to the system structure and attacking types of each ICPS layer. For physical system-relevant perceptual executive layer, potential and covert attacks are detected by the clustered sensory system state residual anomaly monitoring based on a process noise and measurement noise-adaptive Kalman filter (PNMN-AKF). PNMN-AKF can perform a joint recursive estimation of dynamic system states, time-varying process and measurement noise covariance matrices by the variational Bayes approximation framework. In cyberspace, potential cyber-attacks are detected by the anomaly monitoring of the statistical distribution of the network transmission characteristics of data transmission layer by introducing a forgetting factor-induced recursive Gaussian mixture model (FF-RGMM). In the application control layer, a regularized sparse deep belief network model is introduced to characterize the misuse behavior for detecting potential attacks. Extensive validation and comparative experiments have been conducted on a numerical simulation system and a comprehensive ICPS simulation platform by using OPNET and a commonly-used benchmark simplified Tennessee Eastman process (STEP) based on Matlab/Simulink. Experimental results demonstrate that the proposed hierarchically distributed intrusion detection method can efficiently recognize potential and covert cyber-attacks in each ICPSs link with low false alarm rate and missing detection rate, which lays a foundation for the overall security monitoring of ICPSs.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420304024",
    "keywords": [
      "Anomaly detection",
      "Artificial intelligence",
      "Computer science",
      "Cyber-physical system",
      "Data mining",
      "Distributed computing",
      "Image (mathematics)",
      "Intrusion detection system",
      "Kalman filter",
      "Noise (video)",
      "Operating system",
      "Real-time computing"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Jinping"
      },
      {
        "surname": "Zhang",
        "given_name": "Wuxia"
      },
      {
        "surname": "Ma",
        "given_name": "Tianyu"
      },
      {
        "surname": "Tang",
        "given_name": "Zhaohui"
      },
      {
        "surname": "Xie",
        "given_name": "Yongfang"
      },
      {
        "surname": "Gui",
        "given_name": "Weihua"
      },
      {
        "surname": "Niyoyita",
        "given_name": "Jean Paul"
      }
    ]
  },
  {
    "title": "An effective real time GRASP-based metaheuristic: Application to order consolidation and dynamic selection of transshipment points for time-critical freight logistics",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113574",
    "abstract": "Time-critical freight logistics is an area within logistics research where the shipper’s orders need to be received relatively urgently using a third party logistics (3PL) that provides a quote (bid) to the shipper within a very short period. We solved this 3PL problem by developing an effective meta-heuristic based on the Greedy Randomized Adaptive Search Procedure (GRASP). This is achieved by introducing novel attributes in the construction of the restricted candidate list while incorporating flexible and intelligent rules, some of which are inspired by expert knowledge. The approach performs order consolidation, locates transshipment points and performs an optimal assignment of shipments to the selected consolidation points dynamically and in real time. This intelligent system embeds expert knowledge within the design of neighbourhood reduction schemes and data structures to speed up the search. This is achieved by recording computed data that does not need to be recomputed again while avoiding unnecessary computations of the non-promising alternatives. The performance of this real time optimisation and scheduling tool is tested with a European 3PL company over a 13 weeks period in late 2017 resulting in a significant cost saving and a considerable reduction in CO2 emissions. This powerful decision support system assists the 3PL company in gaining competitive leadership advantage through producing promising quotes that turn customer requests into real customer orders.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420303985",
    "keywords": [
      "Accounting",
      "Artificial intelligence",
      "Business",
      "Computer science",
      "Computer security",
      "Consolidation (business)",
      "Finance",
      "GRASP",
      "Mathematics",
      "Metaheuristic",
      "Operations research",
      "Order (exchange)",
      "Programming language",
      "Selection (genetic algorithm)",
      "Transshipment (information security)"
    ],
    "authors": [
      {
        "surname": "Salhi",
        "given_name": "Said"
      },
      {
        "surname": "Gutierrez",
        "given_name": "Brian"
      },
      {
        "surname": "Wassan",
        "given_name": "Niaz"
      },
      {
        "surname": "Wu",
        "given_name": "Shaomin"
      },
      {
        "surname": "Kaya",
        "given_name": "Rukiye"
      }
    ]
  },
  {
    "title": "Automatic alarm prioritization by data mining for fault management in cellular networks",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113526",
    "abstract": "Network management systems play an important role to deal with the large size and complexity of current cellular networks. Thus, operators and vendors focus much of their efforts on developing new techniques and tools for network management. One of the most critical processes in network management is fault management, since a failure in a network element might have a strong impact on user satisfaction due to service degradation. Unfortunately, cellular networks generate thousands of alarms daily, which have to be checked manually by operator personnel. With the latest advances in big data analytics, different methods for reducing the number of alarms to be monitored have been proposed in the literature. In this work, an automatic method for prioritizing alarms based on the need for specialized personnel is presented. The core of the method is an ensemble model built with supervised learning that estimates the probability that an alarm generates a trouble ticket. The model is trained with trouble ticket data from the network operation center. A performance comparison of four classical base classifiers (naïve Bayes, random forest, artificial neural network and support vector machine) for the ensemble is presented. The model is implemented in IBM SPSS Modeler and tested with a real alarm and trouble ticket dataset taken from a live cellular network. Results show that the proposed model correctly flags those alarms that need further analysis by the operator and filter out those alarms that do not have impact on network performance. The main contribution of this work is unveiling a new application (the automatic prioritization of alarms in a cellular network based on the need for specialized personnel) and presenting for the first time a performance comparison of base classifiers used for this purpose (since the required dataset is extremely difficult to find for privacy reasons).",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742030350X",
    "keywords": [
      "ALARM",
      "Artificial intelligence",
      "Artificial neural network",
      "Cellular network",
      "Composite material",
      "Computer network",
      "Computer science",
      "Data mining",
      "Fault management",
      "Machine learning",
      "Materials science",
      "Network element",
      "Network management",
      "Random forest",
      "Ticket"
    ],
    "authors": [
      {
        "surname": "García",
        "given_name": "Antonio J."
      },
      {
        "surname": "Toril",
        "given_name": "Matías"
      },
      {
        "surname": "Oliver",
        "given_name": "Pablo"
      },
      {
        "surname": "Luna-Ramírez",
        "given_name": "Salvador"
      },
      {
        "surname": "Ortiz",
        "given_name": "Manuel"
      }
    ]
  },
  {
    "title": "Semi-supervised learning for ECG classification without patient-specific labeled data",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113411",
    "abstract": "In this paper, we propose a semi-supervised learning-based ECG classification system for detection of supraventricular ectopic beats (SVEB or S beats) and ventricular ectopic beats (VEB or V beats) which does not require manual labeling of the patient-specific ECG data. Owing to inter-subject variability in ECG signal, patient-specific data is usually required to achieve good performance in ECG classification system. However, manual labeling of patient-specific data requires expert intervention, which is costly and time consuming. Our proposed system is based on a 2D convolutional neural network (CNN) with inputs generated from heartbeat triplets. The system also consists of two auxiliary modules: a normal beat estimation module and an iterative beat label update algorithm. The normal beat estimation selects a small amount of patient-specific normal beats accurately from the testing ECG record in an unsupervised manner. These estimated normal beats are used, together with a common pool dataset, to train a preliminary patient-specific CNN classifier which provides initial labels for the testing data. These labels then undergo a semi-supervised iterative update process for improved performance. Our proposed system was evaluated on the MIT-BIH arrhythmia database. The training of our proposed system is fully automatic, and its performance is comparable with several state-of-art supervised methods which require extra manual labeling of patient-specific ECG data. Our proposed system can be a useful tool for batch processing a large amount of ECG data in clinical applications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420302359",
    "keywords": [
      "Acoustics",
      "Artificial intelligence",
      "Beat (acoustics)",
      "Classifier (UML)",
      "Computer science",
      "Computer security",
      "Convolutional neural network",
      "Heartbeat",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Physics",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Zhai",
        "given_name": "Xiaolong"
      },
      {
        "surname": "Zhou",
        "given_name": "Zhanhong"
      },
      {
        "surname": "Tin",
        "given_name": "Chung"
      }
    ]
  },
  {
    "title": "Automatic scale estimation for music score images",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113590",
    "abstract": "Optical Music Recognition (OMR) is the research field focused on the automatic reading of music from scanned images. Its main goal is to encode the content into a digital and structured format with the advantages that this entails. This discipline is traditionally aligned to a workflow whose first step is the document analysis. This step is responsible of recognizing and detecting different sources of information—e.g. music notes, staff lines and text—to extract them and then processing automatically the content in the following steps of the workflow. One of the most difficult challenges it faces is to provide a generic solution to analyze documents with diverse resolutions. The endless number of existing music sources does not meet a standard that normalizes the data collections, giving complete freedom for a wide variety of image sizes and scales, thereby making this operation unsustainable. In the literature, this question is commonly overlooked and a uniform scale is assumed. In this paper, a machine learning-based approach to estimate the scale of music documents with respect to a reference scale is presented. Our goal is to propose a robust and generalizable method to adapt the input image to the requirements of an OMR system. For this, two goal-directed case studies are included to evaluate the proposed approach over common task within the OMR workflow, comparing the behavior with other state-of-the-art methods. Results suggest that it is necessary to perform this additional step in the first stage of the workflow to correct the scale of the input images. In addition, it is empirically demonstrated that our specialized approach is more promising than image augmentation strategies for the multi-scale challenge.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420304140",
    "keywords": [
      "Artificial intelligence",
      "Cartography",
      "Computer science",
      "Computer vision",
      "Economics",
      "Estimation",
      "Geography",
      "Management",
      "Pattern recognition (psychology)",
      "Scale (ratio)"
    ],
    "authors": [
      {
        "surname": "Castellanos",
        "given_name": "Francisco J."
      },
      {
        "surname": "Gallego",
        "given_name": "Antonio-Javier"
      },
      {
        "surname": "Calvo-Zaragoza",
        "given_name": "Jorge"
      }
    ]
  },
  {
    "title": "Multi-attribute decision making applied to financial portfolio optimization problem",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113527",
    "abstract": "This paper proposes an integer multiobjective mean-CVaR portfolio optimization model with variable cardinality constraint and rebalancing and two different methods of decision-maker used to guide and select, according to the decision maker preferences, a solution comes from the non-dominated portfolios generated by a proposed evolutionary algorithm. The decision-making methods were used to approximate investor behavior according to three functions, chosen to represent different investor profiles (conservative, moderate and aggressive). The proposed methods are compared with those found in the literature. Additionally, computational simulations are performed using assets from the Brazilian stock exchange for the period between January 2011 and December 2015. The strategy is that each beginning of the month: the previous portfolio is sold, the optimization is performed, and the decision-making method selects the new portfolio to be purchased. Results of the simulations consider monthly maximum drawdown and cumulative return during the entire study period and show that the optimization model is robust, considering the three simulated profiles. The methods always present cumulative returns above safe investments for the analyzed period, and the aggressive profile obtained bigger gains with greater risk.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420303511",
    "keywords": [
      "CVAR",
      "Cardinality (data modeling)",
      "Computer science",
      "Constraint (computer-aided design)",
      "Data mining",
      "Decision maker",
      "Econometrics",
      "Economics",
      "Expected shortfall",
      "Finance",
      "Geometry",
      "Management",
      "Mathematical optimization",
      "Mathematics",
      "Operations research",
      "Portfolio",
      "Portfolio optimization",
      "Project management",
      "Project portfolio management",
      "Rate of return on a portfolio"
    ],
    "authors": [
      {
        "surname": "Mendonça",
        "given_name": "Gustavo H.M."
      },
      {
        "surname": "Ferreira",
        "given_name": "Fernando G.D.C."
      },
      {
        "surname": "Cardoso",
        "given_name": "Rodrigo T.N."
      },
      {
        "surname": "Martins",
        "given_name": "Flávio V.C."
      }
    ]
  },
  {
    "title": "A hybrid recommender system for recommending relevant movies using an expert system",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113452",
    "abstract": "Currently, the Internet contains a large amount of information, which must then be filtered to determine suitability for certain users. Recommender systems are a very suitable tool for this purpose. In this paper, we propose a monolithic hybrid recommender system called Predictory, which combines a recommender module composed of a collaborative filtering system (using the SVD algorithm), a content-based system, and a fuzzy expert system. The proposed system serves to recommend suitable movies. The system works with favorite and unpopular genres of the user, while the final list of recommended movies is determined using a fuzzy expert system, which evaluates the importance of the movies. The expert system works with several parameters – average movie rating, number of ratings, and the level of similarity between already rated movies. Therefore, our system achieves better results than traditional approaches, such as collaborative filtering systems, content-based systems, and weighted hybrid systems. The system verification based on standard metrics (precision, recall, F1-measure) achieves results over 80%. The main contribution is the creation of a complex hybrid system in the area of movie recommendation, which has been verified on a group of users using the MovieLens dataset and compared with other traditional recommender systems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420302761",
    "keywords": [
      "Artificial intelligence",
      "Collaborative filtering",
      "Computer science",
      "Data mining",
      "Fuzzy logic",
      "Image (mathematics)",
      "Information retrieval",
      "MovieLens",
      "Precision and recall",
      "Recommender system",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Walek",
        "given_name": "Bogdan"
      },
      {
        "surname": "Fojtik",
        "given_name": "Vladimir"
      }
    ]
  },
  {
    "title": "Robust scheduling based on extreme learning machine for bi-objective flexible job-shop problems with machine breakdowns",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113545",
    "abstract": "In modern manufacturing systems, a flexible job-shop schedule problem (FJSP) with random machine breakdown has been widely studied. Two objectives, namely makespan and robustness, were simultaneously considered in this study. Maximizing the workload and float time of each operation and the machine breakdowns, one surrogate measure named RMc was developed via an extreme learning machine (ELM) to evaluate robustness. Specifically, this measure determines the impact of float time on the robustness by the probability of machine breakdown and the location of float time. Simultaneously, the impact was automatically adjusted by the ELM. Then, a method combining an improved version of nondominated sorting genetic algorithm II and RMc was proposed to address the bi-objective FJSP. Computational results on the benchmarks show that RMc accurately evaluates the robustness of the schedules with a small amount of computation cost.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420303699",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Computation",
      "Computer science",
      "Extreme learning machine",
      "Gene",
      "Job shop scheduling",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Operating system",
      "Robustness (evolution)",
      "Schedule",
      "Scheduling (production processes)",
      "Tardiness",
      "Workload"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Yu"
      },
      {
        "surname": "Huang",
        "given_name": "Min"
      },
      {
        "surname": "Yu Wang",
        "given_name": "Zhen"
      },
      {
        "surname": "Bing Zhu",
        "given_name": "Qi"
      }
    ]
  },
  {
    "title": "An expert system to discover key congestion points for urban traffic",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113544",
    "abstract": "Discovering key congestion points periodically in traffic jams is a critical issue. It supports road managers to make sense of the situations, and rule out the congestion economically and efficiently. However, city-scale and synchronal traffic data bring hardships for such kind of analyses. With recent developments in data science, the availability of traffic conditions data generated by the rising digital map applications makes this issue feasible. Therefore, we firstly propose a digital map data-driven expert system to discover and measure the city-scale key congestion points. It is based on a state-of-the-art feature selection method, BSSReduce (Bijective soft set based feature selection). Data from Baidu Map for Chongqing and Beijing are collected as a case to conduct this study. The results indicate that our proposed method helps the road managers recognize 75 and 300 key congestion points from over 10,000 and 50,000 points of the urban roads each month. The visualized results, as well as the significance measurements, provide road managers an expert system to quickly rule out congestion and work out new solutions to future traffic management.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420303687",
    "keywords": [
      "Archaeology",
      "Beijing",
      "Cartography",
      "China",
      "Computer science",
      "Computer security",
      "Data mining",
      "Data science",
      "Engineering",
      "Floating car data",
      "Geography",
      "Key (lock)",
      "Programming language",
      "Scale (ratio)",
      "Set (abstract data type)",
      "Traffic congestion",
      "Transport engineering"
    ],
    "authors": [
      {
        "surname": "Gong",
        "given_name": "Ke"
      },
      {
        "surname": "Zhang",
        "given_name": "Li"
      },
      {
        "surname": "Ni",
        "given_name": "Du"
      },
      {
        "surname": "Li",
        "given_name": "Huamin"
      },
      {
        "surname": "Xu",
        "given_name": "Maozeng"
      },
      {
        "surname": "Wang",
        "given_name": "Yong"
      },
      {
        "surname": "Dong",
        "given_name": "Yuanxiang"
      }
    ]
  },
  {
    "title": "Uncertainty mode selection in categorical clustering using the rough set theory",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113555",
    "abstract": "Clustering is an unsupervised Machine Learning technique widely used to arrange a set of observations into distinct groups called clusters. The problem of categorical clustering has attracted much attention since many real world applications tend to produce such data types. The k-mode was among the first algorithms developed in this context. This algorithms uses the notion of modes to represent the centroids within the clusters. However, its major drawback lies in the random selection of the modes in each iteration during the clustering process. In this paper, we tackled this random selection issue and proposed a new method based on identifying the most adequate modes among a list of candidate ones. The proposed algorithm called Density Rough k-modes (DRk-M) is based on computing the density of each candidate mode to characterize the distribution of the observations around it. Then, we use the Rough Set Theory to deal with the uncertainty involved in this process. The DRk-M was experimented using real world datasets extracted from the UCI (University of California Irvine) Machine Learning Repository, the Global Terrorism Database (GTD) and a set of scrapped Tweets. The DRk-M was compared to many state of the art methods including the k-modes (1998), the Ng’s method (2007), Cao’s method (2012) and Bai’s technique (2014) and it has shown great efficiency.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420303791",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Categorical variable",
      "Centroid",
      "Cluster analysis",
      "Computer science",
      "Context (archaeology)",
      "Data mining",
      "Machine learning",
      "Mode (computer interface)",
      "Operating system",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Programming language",
      "Rough set",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Naouali",
        "given_name": "Sami"
      },
      {
        "surname": "Salem",
        "given_name": "Semeh Ben"
      },
      {
        "surname": "Chtourou",
        "given_name": "Zied"
      }
    ]
  },
  {
    "title": "Improved subspace clustering algorithm using multi-objective framework and subspace optimization",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113487",
    "abstract": "Subspace clustering technique divides the data set into different groups or clusters where each cluster comprises of objects that share some similar properties. Again, the feature sets or the subspace features that are used to represent clusters are different for different clusters. Moreover, in subspace clustering, the grouping of similar objects and the subspace feature set representing that group are identified simultaneously. In evolutionary-based machine learning problems, two critical measures to determine the quality of the generated clusters are compactness within and separation between the clusters. However, the distance-based separation between two clusters may not be useful in the context of subspace clustering, as the clusters may belong to two different subspaces. Again, in the case of subspace clustering, the selection of relevant subspace features plays a primary role in generating good quality subspace clusters. Therefore, the proposed approach optimizes the subspace features by considering two new objective functions, feature non-redundancy (FNR) and feature per cluster (FPC) represented in the form of PSM-index. Another objective function, intra-cluster compactness (ICC-index), is modified and used to optimize the compactness among objects within the cluster. Finally, an evolutionary-based multi-objective subspace clustering technique is developed in this paper optimizing these validity indices. A new mutation operator, namely duplication and deletion along with the modified version of the exogenous genetic material uptake, are developed to explore the search space effectively. The developed algorithm is tested on sixteen synthetic data sets and seven standard real-life data sets for identifying different subspace clusters. Again, to show the effectiveness of using multiple objectives, the algorithm is also tested on three big data sets and a MNIST data set. Also, an application of the proposed method is shown in bi-clustering the gene expression data. The results obtained by the proposed algorithm are compared against some state-of-the-art methods. Experimentation reveals that the proposed algorithm can take advantage of its evolvable genomic structure and the newly defined objective functions on the multi-objective based framework.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420303110",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Compact space",
      "Computer science",
      "Data mining",
      "Geometry",
      "Linear subspace",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pure mathematics",
      "Random subspace method",
      "Subspace topology"
    ],
    "authors": [
      {
        "surname": "Paul",
        "given_name": "Dipanjyoti"
      },
      {
        "surname": "Saha",
        "given_name": "Sriparna"
      },
      {
        "surname": "Mathew",
        "given_name": "Jimson"
      }
    ]
  },
  {
    "title": "A comprehensive review of Branch-and-Bound algorithms: Guidelines and directions for further research on the flowshop scheduling problem",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113556",
    "abstract": "This article is a comprehensive review of Branch-and-Bound algorithms for solving flowshop scheduling problems, from the early works of Ignall and Schrage (1965) and Brown and Lomnicki (1966) to the recent approaches of Labidi et al. (2018) and Li et al. (2018). The first part of the article contains an overview of the Branch-and-Bound algorithm, how it is applied for scheduling problems and its different components. The literature review is focused on permutation flowshop problems, and shows the contribution of each article to the method itself and its application. The articles are divided according to the characteristics of the problem and summarized in tables for an easier viewing. The objectives of this review are to provide guidelines for future research in the application of the Branch-and-Bound algorithm for scheduling problems and also to be used as an index for authors to locate the articles for particular problems within the state-of-the-art literature.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420303808",
    "keywords": [
      "Algorithm",
      "Branch and bound",
      "Computer science",
      "Job shop scheduling",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Operating system",
      "Operations research",
      "Schedule",
      "Scheduling (production processes)",
      "Upper and lower bounds"
    ],
    "authors": [
      {
        "surname": "Tomazella",
        "given_name": "Caio Paziani"
      },
      {
        "surname": "Nagano",
        "given_name": "Marcelo Seido"
      }
    ]
  },
  {
    "title": "Convolutional neural network-based safety evaluation method for structures with dynamic responses",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113634",
    "abstract": "The strain sensors that are used to evaluate structural members have a limited lifespan and thus have shown limitations to perform long-term structural health monitoring (SHM). This study presents a convolutional neural network (CNN)-based strain prediction technique that allows for structural safety evaluations in case of absence or defect of strain sensors. In the proposed method, CNNs were used to establish a relationship between the dynamic structural response and the strain response measured in the structure. A number of dynamic structural responses and the structural member’s strain response that are measured before the strain sensor malfunctions are used as input data and output data, respectively, to train the CNNs. The trained CNNs can estimate the strain and evaluate the structural safety even when the later strain measurement response cannot be used. Dynamic acceleration and displacement responses are used as input data in the two CNNs presented in this study, called CNN_A and CNN_D respectively. A numerical study of a beam-like structure and an experimental study which includes shaking table tests on a reinforced concrete frame specimen were conducted to confirm the validity of the strain predictions by the proposed method with CNN_A and CNN_D. The strain prediction performance of the proposed CNNs is compared in these applications. This study also examines the proposed technique’s strain prediction performance according to the amount of data used to train the CNNs. In addition, this study discusses influences of variations in the number of locations for measuring the dynamic structural responses that are used as the CNNs’ input data on the strain prediction performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420304589",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Convolutional neural network",
      "Data mining",
      "Machine learning",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Seon Park",
        "given_name": "Hyo"
      },
      {
        "surname": "Hwan An",
        "given_name": "Jung"
      },
      {
        "surname": "Jun Park",
        "given_name": "Young"
      },
      {
        "surname": "Kwan Oh",
        "given_name": "Byung"
      }
    ]
  },
  {
    "title": "RecRisk: An enhanced recommendation model with multi-facet risk control",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113561",
    "abstract": "Recommender systems (RSs) play a crucial role in helping users quickly find their desired services and promoting sales of service providers in e-commerce. However, service providers intentionally upload cherry-picked service information to mislead RSs for greater profit. Such misleading recommendations pose the risk of degrading user experience and gradually undermine users’ confidence in the whole service market over the long term. Worse still, most current expert recommendation methods are more susceptible to such risk because they are heavily dependent on this incomplete service information and assume it is trusted. Therefore, how to satisfy users’ service requirements with risks minimized at the same time motivates our work. In this paper, we first discern two risky facets that pose significant challenges to expert recommendation: Sense Drop and Blue Joy. Then we propose a unified framework called RecRisk, which integrates trust, heat equation, and modern portfolio theory to address the above challenges. The main contributions of RecRisk are twofold: (1) To select the services which satisfy the users’ preferences, we design a trust-aware heat equation model (TAHE) that combines heat flow theory with trust elements. (2) We develop a flexible model based on modern portfolio theory to weigh users’ satisfaction and services’ risk facets, and finally recommend a ranked service list to users. Our experimental results demonstrate that RecRisk simultaneously achieves higher recommendation precision and decreases risks when compared to state-of-the-art approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420303857",
    "keywords": [
      "Computer science",
      "Correctness",
      "Economics",
      "Economy",
      "Finance",
      "Portfolio",
      "Programming language",
      "RSS",
      "Recommender system",
      "Service (business)",
      "Service provider",
      "Upload",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Xiao",
        "given_name": "Yang"
      },
      {
        "surname": "Pei",
        "given_name": "Qingqi"
      },
      {
        "surname": "Yao",
        "given_name": "Lina"
      },
      {
        "surname": "Wang",
        "given_name": "Xianzhi"
      }
    ]
  },
  {
    "title": "Semi-supervised learning with generative model for sentiment classification of stock messages",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113540",
    "abstract": "Classification of investors’ sentiments in stock message boards has attracted a great deal of attention. Since the messages are usually short, we propose a semi-supervised learning method to make full use of the features in both train and test messages. The generative emotion model takes message, emotion and words into consideration simultaneously. Based on the facts that words are of different ability in discriminating sentiments, they are categorized into three classes in the model with different emotion strength. Training the generative model can transform the messages into emotion vectors which finally feeds to a sentiment classifier. The experiment results show that the proposed model and learning method are efficient for modeling sentiment in short text, and by properly selecting the amount of train data and the percent of test samples, we can achieve higher classification accuracy than traditional ones. The results indicate that the generative model is effective for short message sentiment classification, and provides a significant approach for the implementation of semi-supervised learning which is a typical expert and intelligent information processing method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742030364X",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Generative grammar",
      "Generative model",
      "Machine learning",
      "Natural language processing",
      "Programming language",
      "Sentiment analysis",
      "Test data"
    ],
    "authors": [
      {
        "surname": "Duan",
        "given_name": "Jiangjiao"
      },
      {
        "surname": "Luo",
        "given_name": "Banghui"
      },
      {
        "surname": "Zeng",
        "given_name": "Jianping"
      }
    ]
  },
  {
    "title": "Random-forest-based real-time contrasts control chart using adaptive breakpoints with symbolic aggregate approximation",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113407",
    "abstract": "For high yield management, process monitoring has become an increasingly important task. The real-time contrasts (RTC) control chart uses the real-time classification method for process monitoring and outperforms the existing real-time control chart. The original RTC control chart identifies the cause of faults using a random forest classifier. However, the random forest provides discrete monitoring statistics that could make the overall performance less efficient. To improve the performance of the RTC control chart, we propose a random-forest-based RTC control chart that uses adaptive breakpoints with symbolic aggregate approximation (ABP-SAX). The monitoring statistics of the RTC control chart indicate the process condition, and the quality of the monitoring statistics is determined by the classification performance of the classifier. Therefore, to improve the classification performance of individual decision trees, we proposed ABP-SAX. The original SAX causes time-information loss and distortions in the data slope and pattern. We prevent these problems using the mean squared error to minimize the difference between the represented and original data. After the applied ABP-SAX representation, the raw data are represented by categorical values that preserve information from the original data, and the represented data improve the performance of the RTC control chart. Therefore, the proposed RTC control chart could detect shifts more quickly and identify the cause of the faults. Our improvements can contribute to high yield management and quick response to abnormalities.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420302311",
    "keywords": [
      "Artificial intelligence",
      "Categorical variable",
      "Chart",
      "Computer science",
      "Control chart",
      "Control limits",
      "Data mining",
      "EWMA chart",
      "Machine learning",
      "Mathematics",
      "Operating system",
      "Process (computing)",
      "Random forest",
      "Statistical process control",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Lee",
        "given_name": "In-seok"
      },
      {
        "surname": "Park",
        "given_name": "Seung Hwan"
      },
      {
        "surname": "Baek",
        "given_name": "Jun-Geol"
      }
    ]
  },
  {
    "title": "Rumor detection based on propagation graph neural network with attention mechanism",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113595",
    "abstract": "Rumors on social media have always been an important issue that seriously endangers social security. Researches on timely and effective detection of rumors have aroused lots of interest in both academia and industry. At present, most existing methods identify rumors based solely on the linguistic information without considering the temporal dynamics and propagation patterns. In this work, we aim to solve rumor detection task under the framework of representation learning. We first propose a novel way to construct the propagation graph by following the propagation structure (who replies to whom) of posts on Twitter. Then we propose a gated graph neural network based algorithm called PGNN, which can generate powerful representations for each node in the propagation graph. The proposed PGNN algorithm repeatedly updates node representations by exchanging information between the neighbor nodes via relation paths within a limited time steps. On this basis, we propose two models, namely GLO-PGNN (rumor detection model based on the global embedding with propagation graph neural network) and ENS-PGNN (rumor detection model based on the ensemble learning with propagation graph neural network). They respectively adopt different classification strategies for rumor detection task, and further improve the performance by including attention mechanism to dynamically adjust the weight of each node in the propagation graph. Experiments on a real-world Twitter dataset demonstrate that our proposed models achieve much better performance than state-of-the-art methods both on the rumor detection task and early detection task.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742030419X",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Economics",
      "Engineering",
      "Graph",
      "Machine learning",
      "Management",
      "Node (physics)",
      "Political science",
      "Public relations",
      "Rumor",
      "Structural engineering",
      "Task (project management)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Zhiyuan"
      },
      {
        "surname": "Pi",
        "given_name": "Dechang"
      },
      {
        "surname": "Chen",
        "given_name": "Junfu"
      },
      {
        "surname": "Xie",
        "given_name": "Meng"
      },
      {
        "surname": "Cao",
        "given_name": "Jianjun"
      }
    ]
  },
  {
    "title": "Experimentation and performance in advertising: An observational survey of firm practices on Facebook",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113554",
    "abstract": "It is widely assumed that firms experiment with their online advertising to identify more profitable approaches to then increase their investment in more profitable advertising, increasing their overall performance. Generalizable evidence on the actual use of such experiment-based learning by firms is sparse. The study herein addresses this shortcoming – detailing the extent to which large advertisers are utilizing experimentation along with evidence on the benefits of doing so. The findings are gleaned from firms’ marketing and experimentation practices on a large online advertising platform and indicate that, while experimentation is utilized by some, adoption is far from perfect. Among the few firms making use of experiments, even fewer invest a significant share of their advertising spend in experimentation. This finding is surprising in light of broadly assumed regular experimentation by firms. Experimenting firms further experience higher concurrent and subsequent performance, suggesting that leading firms indeed successfully use experiment-based learning to improve their advertising policies – and that many firms may fall short of their potential by not (yet) using experiments in advertising.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742030378X",
    "keywords": [
      "Advertising",
      "Advertising research",
      "Business",
      "Computer science",
      "Investment (military)",
      "Law",
      "Marketing",
      "Online advertising",
      "Political science",
      "Politics",
      "The Internet",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Runge",
        "given_name": "Julian"
      },
      {
        "surname": "Geinitz",
        "given_name": "Steven"
      },
      {
        "surname": "Ejdemyr",
        "given_name": "Simon"
      }
    ]
  },
  {
    "title": "Monaural speech enhancement through deep wave-U-net",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113582",
    "abstract": "In this paper, we present Speech Enhancement through Wave-U-Net (SEWUNet), an end-to-end approach to reduce noise from speech signals. This background context is detrimental to several downstream systems, including automatic speech recognition (ASR) and word spotting, which in turn can negatively impact end-user applications. We show that our proposal does improve signal-to-noise ratio (SNR) and word error rate (WER) compared with existing mechanisms in the literature. In the experiments, network input is a 16 kHz sample rate audio waveform corrupted by an additive noise. Our method is based on the Wave-U-Net architecture with some adaptations to our problem. Four simple enhancements are proposed and tested with ablation studies to prove their validity. In particular, we highlight the weight initialization through an autoencoder before training for the main denoising task, which leads to a more efficient use of training time and a higher performance. Through quantitative metrics, we show that our method is prefered over the classical Wiener filtering and shows a better performance than other state-of-the-art proposals.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420304061",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Geometry",
      "Image (mathematics)",
      "Initialization",
      "Mathematics",
      "Noise (video)",
      "Noise reduction",
      "Paleontology",
      "Programming language",
      "Speech enhancement",
      "Speech recognition",
      "Word (group theory)",
      "Word error rate"
    ],
    "authors": [
      {
        "surname": "Guimarães",
        "given_name": "Heitor R."
      },
      {
        "surname": "Nagano",
        "given_name": "Hitoshi"
      },
      {
        "surname": "Silva",
        "given_name": "Diego W."
      }
    ]
  },
  {
    "title": "T2-FDL: A robust sparse representation method using adaptive type-2 fuzzy dictionary learning for medical image classification",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113500",
    "abstract": "In this paper, a robust sparse representation for medical image classification is proposed based on the adaptive type-2 fuzzy learning (T2-FDL) system. In the proposed method, sparse coding and dictionary learning processes are executed iteratively until a near-optimal dictionary is obtained. The sparse coding step aiming at finding a combination of dictionary atoms to represent the input data efficiently, and the dictionary learning step rigorously adjusts a minimum set of dictionary items. The two-step operation helps create an adaptive sparse representation algorithm by involving the type-2 fuzzy sets in the design process of image classification. Since the existing image measurements are not made under the same conditions and with the same accuracy, the performance of medical diagnosis is always affected by noise and uncertainty. By introducing an adaptive type-2 fuzzy learning method, a better approximation in an environment with higher degrees of uncertainty and noise is achieved. The experiments are executed over two open-access brain tumor magnetic resonance image databases, REMBRANDT and TCGA-LGG, from The Cancer Imaging Archive (TCIA). The experimental results of a brain tumor classification task show that the proposed T2-FDL method can adequately minimize the negative effects of uncertainty in the input images. The results demonstrate the outperformance of T2-FDL compared to other important classification methods in the literature, in terms of accuracy, specificity, and sensitivity.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420303249",
    "keywords": [
      "Artificial intelligence",
      "Coding (social sciences)",
      "Computer science",
      "Fuzzy logic",
      "Image (mathematics)",
      "K-SVD",
      "Machine learning",
      "Mathematics",
      "Neural coding",
      "Noise (video)",
      "Pattern recognition (psychology)",
      "Sparse approximation",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Ghasemi",
        "given_name": "Majid"
      },
      {
        "surname": "Kelarestaghi",
        "given_name": "Manoochehr"
      },
      {
        "surname": "Eshghi",
        "given_name": "Farshad"
      },
      {
        "surname": "Sharifi",
        "given_name": "Arash"
      }
    ]
  },
  {
    "title": "A whale optimization algorithm with chaos mechanism based on quasi-opposition for global optimization problems",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113612",
    "abstract": "Whale Optimization Algorithm (WOA), as a newly developed meta-heuristic algorithm, performs well in solving optimization problems. A WOA with chaos mechanism based on quasi-opposition (OBCWOA) is proposed in this paper to overcome the slow convergence speed of the original WOA and to avoid being trapped in local optimal solutions when dealing with high-dimensional problems. We applied two strategies to the original WOA: using chaos mechanism to generate initial value to improve convergence speed of the algorithm and using the opposition-based learning method to balance exploration and development ability of the algorithm to help the algorithm jump out of local optimal solutions. The proposed algorithm is compared with other algorithms on unimodal functions, multimodal functions and fixed dimensional multimodal functions, and is applied to a famous engineering design problem. Results show that combination of the two strategies can improve convergence speed and enhance global search ability of the original WOA. OBCWOA proposed in this paper performs better than the other existing algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742030436X",
    "keywords": [
      "Algorithm",
      "Computer science",
      "Convergence (economics)",
      "Economic growth",
      "Economics",
      "Jump",
      "Law",
      "Mathematical optimization",
      "Mathematics",
      "Opposition (politics)",
      "Optimization algorithm",
      "Physics",
      "Political science",
      "Politics",
      "Quantum mechanics"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Hui"
      },
      {
        "surname": "Li",
        "given_name": "Weide"
      },
      {
        "surname": "Yang",
        "given_name": "Xuan"
      }
    ]
  },
  {
    "title": "Fuzzy logic aggregation of crisp data partitions as learning analytics in triage decisions",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113512",
    "abstract": "This paper provides an analytical learning system based on Fuzzy Logic AGgregation (FLAG) of crisp data partitions to improve the Triage process in a hospitality emergency department. The method compares patient rankings made by nurses with those made by an Expert to detect points for improvement. Specifically, a normalized concordance index per nurse and the average of them allow for the evaluation of the ability of the nurses of well aggregating the cases in Triage process. The proposed FLAG system is tested through an empirical case study by simulating the patients arriving at two Emergency Departments Triage. The main contribution is the definition of the global performance index combining both the nurse’s partitioning concordance with respect to the Expert’s one and the accuracy in class assignment. The empirical distribution function of the global concordance index is derived through permutation method. In this way, Kolmogorov-Smirnov testing provides the comparison of the performances of the two healthcare units. The pay-off table concordance-accuracy allows to address improvement actions. Another tool of the system is the correspondence analysis to visualize the accuracy of decisions on the class priority as well as the sharing behaviours that influence the nurse’s judgements. All this becomes part of the FLAG learning analytics system which is able to outline critical points by red flags. to improve further assignments and the overall management organization. FLAG system can be adapted in all other situations of risk where cognitive heuristics face an accuracy-effort trade-off such that their simplified decision process leads to reduced accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420303365",
    "keywords": [
      "Analytics",
      "Artificial intelligence",
      "Computer science",
      "Data analysis",
      "Data mining",
      "Data science",
      "Emergency medicine",
      "Fuzzy logic",
      "Machine learning",
      "Medicine",
      "Triage"
    ],
    "authors": [
      {
        "surname": "Pandolfo",
        "given_name": "Giuseppe"
      },
      {
        "surname": "D’Ambrosio",
        "given_name": "Antonio"
      },
      {
        "surname": "Cannavacciuolo",
        "given_name": "Lorella"
      },
      {
        "surname": "Siciliano",
        "given_name": "Roberta"
      }
    ]
  },
  {
    "title": "Pattern recognition and automatic identification of early-stage atrial fibrillation",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113560",
    "abstract": "Atrial fibrillation (AF) is a common cardiac arrhythmia and is responsible for a number of complications. While early-stage AF typically lasts only a few episodes and may not be immediately life-threatening, the cardiac arrhythmia favors electrical and structural alteration of the atria that tends to intensify and perpetuate AF even at incipient stage. Therefore, recognition and identification of patterns associated with early-stage AF episodes is demanded for effective treatment and disease management. Nonetheless, the brevity of early-stage AF negates a myriad of conventional models for effective detection, particularly when only a few seconds of recording is available. In this paper, we investigate constructive patterns based upon intrinsic time-scale decomposition (ITD) to parse single-lead ECG signals with short duration collected from wearable devices. ITD provides accurate instantaneous time–frequency-energy characteristics for the nonlinear and nonstationary data, particularly the short-term time series signals. Our model registers average accuracy of 95%, specificity of 96% and sensitivity of 93% for the diagnosis of AF events, handily outperforming wavelet-based algorithm.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420303845",
    "keywords": [
      "Artificial intelligence",
      "Atrial fibrillation",
      "Biology",
      "Cardiac arrhythmia",
      "Cardiology",
      "Computer science",
      "Medicine",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Stage (stratigraphy)"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Xiaodan"
      },
      {
        "surname": "Zheng",
        "given_name": "Yumeng"
      },
      {
        "surname": "Che",
        "given_name": "Yiming"
      },
      {
        "surname": "Cheng",
        "given_name": "Changqing"
      }
    ]
  },
  {
    "title": "Understanding the apparent superiority of over-sampling through an analysis of local information for class-imbalanced data",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113026",
    "abstract": "Data plays a key role in the design of expert and intelligent systems and therefore, data preprocessing appears to be a critical step to produce high-quality data and build accurate machine learning models. Over the past decades, increasing attention has been paid towards the issue of class imbalance and this is now a research hotspot in a variety of fields. Although the resampling methods, either by under-sampling the majority class or by over-sampling the minority class, stand among the most powerful techniques to face this problem, their strengths and weaknesses have typically been discussed based only on the class imbalance ratio. However, several questions remain open and need further exploration. For instance, the subtle differences in performance between the over- and under-sampling algorithms are still under-comprehended, and we hypothesize that they could be better explained by analyzing the inner structure of the data sets. Consequently, this paper attempts to investigate and illustrate the effects of the resampling methods on the inner structure of a data set by exploiting local neighborhood information, identifying the sample types in both classes and analyzing their distribution in each resampled set. Experimental results indicate that the resampling methods that produce the highest proportion of safe samples and the lowest proportion of unsafe samples correspond to those with the highest overall performance. The significance of this paper lies in the fact that our findings may contribute to gain a better understanding of how these techniques perform on class-imbalanced data and why over-sampling has been reported to be usually more efficient than under-sampling. The outcomes in this study may have impact on both research and practice in the design of expert and intelligent systems since a priori knowledge about the internal structure of the imbalanced data sets could be incorporated to the learning algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419307432",
    "keywords": [
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Chemistry",
      "Chromatography",
      "Class (philosophy)",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Data pre-processing",
      "Data quality",
      "Data science",
      "Data set",
      "Economics",
      "Filter (signal processing)",
      "Machine learning",
      "Metric (unit)",
      "Operations management",
      "Oversampling",
      "Preprocessor",
      "Programming language",
      "Resampling",
      "Sample (material)",
      "Sampling (signal processing)",
      "Set (abstract data type)",
      "Variety (cybernetics)"
    ],
    "authors": [
      {
        "surname": "García",
        "given_name": "V."
      },
      {
        "surname": "Sánchez",
        "given_name": "J.S."
      },
      {
        "surname": "Marqués",
        "given_name": "A.I."
      },
      {
        "surname": "Florencia",
        "given_name": "R."
      },
      {
        "surname": "Rivera",
        "given_name": "G."
      }
    ]
  },
  {
    "title": "An intelligent financial portfolio trading strategy using deep Q-learning",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113573",
    "abstract": "Portfolio traders strive to identify dynamic portfolio allocation schemes that can allocate their total budgets efficiently through the investment horizon. This study proposes a novel portfolio trading strategy in which an intelligent agent is trained to identify an optimal trading action using deep Q-learning. We formulate a Markov decision process model for the portfolio trading process that adopts a discrete combinatorial action space and determines the trading direction at a prespecified trading size for each asset, thus ensuring practical applicability. Our novel portfolio trading strategy takes advantage of three features to outperform other strategies in real-world trading. First, a mapping function is devised to handle and transform any action that is initially proposed but found to be infeasible into a similar and valuable feasible action. Second, by overcoming the dimensionality problem, this study establishes agent and Q-network models to derive a multi-asset trading strategy in the predefined action space. Last, this study introduces a technique that can derive a well-fitted multi-asset trading strategy by designing an agent to simulate all feasible actions in each state. To validate our approach, we conduct backtesting for two representative portfolios and demonstrate superior results over the benchmark strategies.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420303973",
    "keywords": [],
    "authors": [
      {
        "surname": "Park",
        "given_name": "Hyungjun"
      },
      {
        "surname": "Sim",
        "given_name": "Min Kyu"
      },
      {
        "surname": "Choi",
        "given_name": "Dong Gu"
      }
    ]
  },
  {
    "title": "Bark texture classification using improved local ternary patterns and multilayer neural network",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113509",
    "abstract": "Tree identification is one of the areas that are regarded by researchers. It is done by human expert with high cost. Experts believe that tree bark has a high relation with species in comparison with other phenotype properties. Repeated textures in the bark is usually various with slight differences. So, lbp-like descriptors used in most recent works. But, most of them do not provide discriminative features. Also some texture descriptors are sensitive to noise and rotation. Local ternary pattern is one of the operators that are resistant to the noise with high discrimination. In most of descriptors, histogram of patterns is used to extract features. But, it is rotation sensitive with high computational complexity. In this paper, the main contribution is to propose a method for bark texture classification with high accuracy based on the improved local ternary patterns (ILTP). In the proposed ILTP, the ternary patterns are coded into two binary patterns, and then each one is classified into two uniform/non-uniform groups. The extracted patterns are labeled according to the degree of uniformity. Finally the occurrence probability of the labels is extracted as features. Also, a multilayer perceptron is designed with four theories in the number of hidden nodes. Experimental results on two benchmark datasets showed that our proposed approach provides higher classification accuracy than most well known methods. Noise-resistant and rotation invariant are other advantages of the presented method. The proposed bark texture classification, because of its high classification accuracy, can be applied in real applications and reduce the financial costs and human risks in the diagnosis of plant species.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742030333X",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Discriminative model",
      "Histogram",
      "Image (mathematics)",
      "Invariant (physics)",
      "Local binary patterns",
      "Mathematical physics",
      "Mathematics",
      "Multilayer perceptron",
      "Noise (video)",
      "Pattern recognition (psychology)",
      "Programming language",
      "Ternary operation"
    ],
    "authors": [
      {
        "surname": "Fekri-Ershad",
        "given_name": "Shervan"
      }
    ]
  },
  {
    "title": "Fake news detection in multiple platforms and languages",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113503",
    "abstract": "The debate around fake news has grown recently because of the potential harm they can have on different fields, being politics one of the most affected. Due to the amount of news being published every day, several studies in computer science have proposed models using machine learning to detect fake news. However, most of these studies focus on news from one language (mostly English) or rely on characteristics of social media-specific platforms (like Twitter or Sina Weibo). Our work proposes to detect fake news using only text features that can be generated regardless of the source platform and are the most independent of the language as possible. We carried out experiments from five datasets, comprising both texts and social media posts, in three language groups: Germanic, Latin, and Slavic, and got competitive results when compared to benchmarks. We compared the results obtained through a custom set of features and with other popular techniques when dealing with natural language processing, such as bag-of-words and Word2Vec.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420303274",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Embedding",
      "Fake news",
      "Focus (optics)",
      "Harm",
      "Internet privacy",
      "Law",
      "Natural language processing",
      "Optics",
      "Physics",
      "Political science",
      "Programming language",
      "Set (abstract data type)",
      "Social media",
      "Word2vec",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Faustini",
        "given_name": "Pedro Henrique Arruda"
      },
      {
        "surname": "Covões",
        "given_name": "Thiago Ferreira"
      }
    ]
  },
  {
    "title": "Graph based feature selection investigating boundary region of rough set for language identification",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113575",
    "abstract": "Language can be chosen to be a species where maximum information can be extracted. In the world, there are many countries, some of which are of numerous types and flavours of regions based on their languages. The challenge is to make the spoken language recognition to be automated through machine learning. The proposed language identification system extracts various features from speech of different languages and constructs a complete weighted graph with extracted features as nodes and similarity among the features as weights of the edges. Similarity values are computed using the concepts of positive region and boundary region of rough set theory and a graph based feature selection algorithm is devised to select only the minimal subset of features relevant to language identification. It is observed that, investigating the boundary region together with the positive region, more valuable information is extracted which helps in selection of more relevant features for language identification. The constructed complete weighted graph is made sparse using Gini index based sparsity measure. As a result, the graph contains only the edges whose terminal nodes are highly similar. Next, a maximal spanning tree of the graph is generated using Prim’s algorithm. This tree is a basic structure that provides the maximal similarity among the nodes in the graph. Finally, score of each node is computed based on weights of the edges in the tree and a node with the highest score is selected and removed from the spanning tree. This process of selection and removal of nodes is continued until the graph becomes null. The resultant set of selected nodes is considered as the important feature subset of the audio speeches used for language identification. Experimental results show the effectiveness of the proposed rough set theory based feature selection method. The results also demonstrate the usefulness of investigation of boundary region of rough sets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420303997",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Data mining",
      "Feature selection",
      "Graph",
      "Image (mathematics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Rough set",
      "Similarity (geometry)",
      "Spanning tree",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Yasmin",
        "given_name": "Ghazaala"
      },
      {
        "surname": "Das",
        "given_name": "Asit Kumar"
      },
      {
        "surname": "Nayak",
        "given_name": "Janmenjoy"
      },
      {
        "surname": "Pelusi",
        "given_name": "Danilo"
      },
      {
        "surname": "Ding",
        "given_name": "Weiping"
      }
    ]
  },
  {
    "title": "A decision support system for urban infrastructure inter-asset management employing domain ontologies and qualitative uncertainty-based reasoning",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113461",
    "abstract": "Urban infrastructure assets (e.g. roads, water pipes) perform critical functions to the health and well-being of society. Although it has been widely recognised that different infrastructure assets are highly interconnected, infrastructure management in practice such as planning, installation and maintenance are often undertaken by different stakeholders without considering these dependencies due to the lack of relevant data and cross-domain knowledge, which may cause unexpected cascading social, economic and environmental effects. In this paper, we present a knowledge based decision support system for urban infrastructure inter-asset management. By considering various infrastructure assets (e.g. road, ground, cable), triggers (e.g. pipe leaking) and potential consequences (e.g. traffic disruption) as a holistic system, we model each sub-domain using a modular ontology and encapsulate the interdependence between them using a set of rules. Moreover, qualitative likelihood is assigned to each rule by domain experts (e.g. civil engineers) to encode the uncertainty of knowledge, and an inference engine is applied to predict the potential consequences of a given trigger with location specific data and the encoded rules. A web-based prototype system has been developed based on the above concept and demonstrated to a wide range of stakeholders. The system can assist in the process of decision making by aiding data collation and integration, as well as presenting potential consequences of possible triggers, advising on whether additional information is needed or suggesting ways of obtaining such information. The work shows an intelligent approach to integrate and process multi-source data to pioneer a novel way to aid a complex decision process with a high social impact.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420302852",
    "keywords": [
      "Artificial intelligence",
      "Asset (computer security)",
      "Asset management",
      "Business",
      "Computer science",
      "Computer security",
      "Critical infrastructure",
      "Data mining",
      "Data science",
      "Decision support system",
      "Domain (mathematical analysis)",
      "Electrical engineering",
      "Engineering",
      "Epistemology",
      "Finance",
      "Inference",
      "Inference engine",
      "Information infrastructure",
      "Information system",
      "Knowledge management",
      "Mathematical analysis",
      "Mathematics",
      "Ontology",
      "Operating system",
      "Philosophy",
      "Process (computing)",
      "Risk analysis (engineering)",
      "Semantic Web",
      "Semantic reasoner"
    ],
    "authors": [
      {
        "surname": "Wei",
        "given_name": "Lijun"
      },
      {
        "surname": "Du",
        "given_name": "Heshan"
      },
      {
        "surname": "Mahesar",
        "given_name": "Quratul-ain"
      },
      {
        "surname": "Al Ammari",
        "given_name": "Kareem"
      },
      {
        "surname": "Magee",
        "given_name": "Derek R."
      },
      {
        "surname": "Clarke",
        "given_name": "Barry"
      },
      {
        "surname": "Dimitrova",
        "given_name": "Vania"
      },
      {
        "surname": "Gunn",
        "given_name": "David"
      },
      {
        "surname": "Entwisle",
        "given_name": "David"
      },
      {
        "surname": "Reeves",
        "given_name": "Helen"
      },
      {
        "surname": "Cohn",
        "given_name": "Anthony G."
      }
    ]
  },
  {
    "title": "Real-time classification for autonomous drowsiness detection using eye aspect ratio",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113505",
    "abstract": "Various automated systems require human supervision in complex environments: this can be a monotonous task but still requiring a significant degree of attention. If those tasks are decisive to the process and work safety, then it is imperative that operators maintain adequate levels of alertness to execute necessary actions. Here, we developed a methodology for drowsiness detection based on eye patterns of people monitored by video streams. In contrast to physically intrusive methods based on a biological approach (e.g. electrooculogram), computer vision and machine leaning (ML) were used to create a low-cost real-time system to detect whether a user (operator) is drowsy using a simple web camera. The proposed methodology employs drowsiness rules for blink patterns from neuroscience literature, which allows for automatic alertness supervision of users reducing risks of potential human errorand then preventing accidents. Specifically, a temporal element is introduced by concatenating information from several consecutive video frames coupled with the ability of ML models in identifying different eye behavior. Here, multilayer perceptron, random forest, and support vector machines were analyzed: the latter had the overall best performance in terms of average test accuracy (94.9%) and required execution time. The proposed methodology also contains a personal feedback proposal to adapt models for each specific user providing even better results. We validated our model in DROZY – a public database for human drowsiness. Inter- and intra-subject investigations were conducted considering the Karolinska Sleepiness Scale (KSS) evaluation and the reaction time as performance indicators. In inter-subject analysis, our model did not provide any warning when a subject was awake, but an average of 16.1 warnings were emitted for drowsy subjects with 94.44% accuracy. For intra-subject analysis, our model could detect when subjects were prone to drowsiness. These are interesting and promising results regarding drowsiness detection.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420303298",
    "keywords": [
      "Alertness",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Economics",
      "Machine learning",
      "Management",
      "Medicine",
      "Multilayer perceptron",
      "Operating system",
      "Pharmacology",
      "Process (computing)",
      "Support vector machine",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Maior",
        "given_name": "Caio Bezerra Souto"
      },
      {
        "surname": "Moura",
        "given_name": "Márcio José das Chagas"
      },
      {
        "surname": "Santana",
        "given_name": "João Mateus Marques"
      },
      {
        "surname": "Lins",
        "given_name": "Isis Didier"
      }
    ]
  },
  {
    "title": "Evaluating the feasibility of blockchain in logistics operations: A decision framework",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113543",
    "abstract": "The main purpose of this study is to investigate the feasibility of blockchain technology in logistics industry using a quantitative approach. To this end, a decision framework is proposed based on a multi-criteria decision structure that incorporates AHP into VIKOR under Intuitionistic Fuzzy Theory. This integration presents different solutions and rankings based on different decision-making strategies and also captures uncertainty in the evaluation process. While Intuitionistic Fuzzy AHP calculates the importance weights of the proposed criteria indicated as scalability, privacy, interoperability, audit, latency, visibility, trust, and security, Fuzzy VIKOR ranks the logistics operations demonstrated as materials handling, warehousing, order processing, transportation, packaging, fleet management, labeling, vehicle routing and product returns management. The proposed decision framework was applied in a large-scale logistics company located in Turkey. The findings of this study suggest that while the most important criteria are security, visibility and audit, the most feasible logistics operations proved to be transportation, materials handling, warehousing, order processing and fleet management in a possible blockchain implementation. The decision framework in this study may enable decision makers to evaluate the feasibility of blockchain in logistics operations, which is one of the main research gaps in the current blockchain research. Furthermore, this is the first study that integrates AHP and VIKOR methods under Intuitionistic Fuzzy Theory in the context of blockchain.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420303675",
    "keywords": [
      "Analytic hierarchy process",
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Database",
      "Decision support system",
      "Engineering",
      "Fuzzy logic",
      "Interoperability",
      "Multiple-criteria decision analysis",
      "Operating system",
      "Operations research",
      "Scalability"
    ],
    "authors": [
      {
        "surname": "Ar",
        "given_name": "Ilker Murat"
      },
      {
        "surname": "Erol",
        "given_name": "Ismail"
      },
      {
        "surname": "Peker",
        "given_name": "Iskender"
      },
      {
        "surname": "Ozdemir",
        "given_name": "Ali Ihsan"
      },
      {
        "surname": "Medeni",
        "given_name": "Tunc Durmus"
      },
      {
        "surname": "Medeni",
        "given_name": "Ihsan Tolga"
      }
    ]
  },
  {
    "title": "Deep learning radiomics in breast cancer with different modalities: Overview and future",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113501",
    "abstract": "Recent improvements in deep learning radiomics (DLR) extracting high-level features form medical imaging could promote the performance of computer aided diagnosis (CAD) for cancer. Breast cancer is the most frequent cancer among women and prospective achievements have been reported by CAD systems based on deep learning methods for breast imaging. In this paper, we aim to provide a comprehensive overview of the recent research efforts on DLR in breast cancer with different modalities and propose the future directions in this field. First, we respectively summarize and analyze the dataset, architecture, application and evaluation on DLR for breast cancer with three main imaging modalities, i.e., ultrasound, mammography, magnetic resonance imaging. Especially, we provide a survey on deep learning architectures exploited in breast cancer, including discriminative architectures and generative architectures. Then, we propose some potential challenges along with future research directions as references to the clinical treatment management and decision making utilizing such breast cancer CAD systems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420303250",
    "keywords": [
      "Artificial intelligence",
      "Breast cancer",
      "Breast imaging",
      "Cancer",
      "Computer science",
      "Computer-aided diagnosis",
      "Deep learning",
      "Internal medicine",
      "Machine learning",
      "Magnetic resonance imaging",
      "Mammography",
      "Medical imaging",
      "Medical physics",
      "Medicine",
      "Modalities",
      "Radiology",
      "Radiomics",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Pang",
        "given_name": "Ting"
      },
      {
        "surname": "Wong",
        "given_name": "Jeannie Hsiu Ding"
      },
      {
        "surname": "Ng",
        "given_name": "Wei Lin"
      },
      {
        "surname": "Chan",
        "given_name": "Chee Seng"
      }
    ]
  },
  {
    "title": "A co-optimal coverage path planning method for aerial scanning of complex structures",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113535",
    "abstract": "The utilization of unmanned aerial vehicles (UAVs) in survey and inspection of civil infrastructure has been growing rapidly. However, computationally efficient solvers that find optimal flight paths while ensuring high-quality data acquisition of the complete 3D structure remains a difficult problem. Existing solvers typically prioritize efficient flight paths, or coverage, or reducing computational complexity of the algorithm – but these objectives are not co-optimized holistically. In this work we introduce a co-optimal coverage path planning (CCPP) method that simultaneously co-optimizes the UAV path, the quality of the captured images, and reducing computational complexity of the solver all while adhering to safety and inspection requirements. The result is a highly parallelizable algorithm that produces more efficient paths where quality of the useful image data is improved. The path optimization algorithm utilizes a particle swarm optimization (PSO) framework which iteratively optimizes the coverage paths without needing to discretize the motion space or simplify the sensing models as is done in similar methods. The core of the method consists of a cost function that measures both the quality and efficiency of a coverage inspection path, and a greedy heuristic for the optimization enhancement by aggressively exploring the viewpoints search spaces. To assess the proposed method, a coverage path quality evaluation method is also presented in this research, which can be utilized as the benchmark for assessing other CPP methods for structural inspection purpose. The effectiveness of the proposed method is demonstrated by comparing the quality and efficiency of the proposed approach with the state-of-art through both synthetic and real-world scenes. The experiments show that our method enables significant performance improvement in coverage inspection quality while preserving the path efficiency on different test geometries.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420303596",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computational complexity theory",
      "Computer science",
      "Data mining",
      "Geodesy",
      "Geography",
      "Heuristic",
      "Mathematical optimization",
      "Mathematics",
      "Motion planning",
      "Particle swarm optimization",
      "Path (computing)",
      "Programming language",
      "Robot",
      "Solver"
    ],
    "authors": [
      {
        "surname": "Shang",
        "given_name": "Zhexiong"
      },
      {
        "surname": "Bradley",
        "given_name": "Justin"
      },
      {
        "surname": "Shen",
        "given_name": "Zhigang"
      }
    ]
  },
  {
    "title": "Markov blanket-based universal feature selection for classification and regression of mixed-type data",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113398",
    "abstract": "Feature selection has been successfully applied to improve the quality of data analysis in various expert and intelligent systems. However, because most real-world data nowadays come with mixed features, traditional feature selection approaches that are mainly designed to handle single-type data are not suitable for this situation. In addition, most of existing methods are only applicable to a specific problem, either classification or regression. Therefore, it is an urgent need to develop a universal feature selection method that can be applied to classification and regression with mixed-type data. In response to this, our paper presents a new feature selection method based on a Markov blanket (MB) called Mixed-MB. The key idea behind this is to embed a likelihood ratio-based generalized conditional independence test into an efficient MB search algorithm to find the minimal set of features to fully explain the target variable on mixed-type data. This new MB feature selection method eliminates the weakness of existing MB feature selection method that it only can handle single-type data, while maintaining its strengths such as theoretical soundness, simplicity, speed, and versatility. Experimental results on real-world data sets with mixed features demonstrate that the proposed method is effective for improving the accuracy of prediction models in both classification and regression. It is also shown to be able to yield more accurate results with fewer features than other methods. We believe that Mixed-MB will be widely used in expert and intelligent systems that utilize various data to create value since it can be applied to any type of data and problem.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420302220",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Feature (linguistics)",
      "Feature selection",
      "Linguistics",
      "Machine learning",
      "Markov blanket",
      "Markov chain",
      "Markov model",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Variable-order Markov model"
    ],
    "authors": [
      {
        "surname": "Lee",
        "given_name": "Junghye"
      },
      {
        "surname": "Jeong",
        "given_name": "Jun-Yong"
      },
      {
        "surname": "Jun",
        "given_name": "Chi-Hyuck"
      }
    ]
  },
  {
    "title": "Advancement of the search process for digital heritage by utilizing artificial intelligence algorithms",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113559",
    "abstract": "The increasing amount of pressure to digitalize what we are used to consider a conventional data has created a need to analyze, search and process unique data structures in a timely manner. The progressive world has created a justified need not only for a fast query searches but also the most related and meaningful searches with a minimal guidance. This article explores the potential benefits of using unorthodox solutions, including Artificial Intelligence algorithms in processing big data that are unconventional data structures. One of such big data sources was created as part of digitalization of historic heritage materials, documents and artifacts. The article calls out the benefits and disadvantages of some Artificial Intelligence algorithms and explores ways to use a few of those algorithms for the purposes of the digital heritage. It also offers the solution to maximize the potential of the search engine that could be built for digital heritage or any other unstructured data.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420303833",
    "keywords": [
      "Algorithm",
      "Archaeology",
      "Artificial intelligence",
      "Big data",
      "Computer science",
      "Cultural heritage",
      "Data mining",
      "Data science",
      "History",
      "Operating system",
      "Process (computing)"
    ],
    "authors": [
      {
        "surname": "Gasimova",
        "given_name": "Rena T."
      },
      {
        "surname": "Abbasli",
        "given_name": "Rahim N."
      }
    ]
  },
  {
    "title": "Advancement of the search process for digital heritage by utilizing artificial intelligence algorithms",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113559",
    "abstract": "The increasing amount of pressure to digitalize what we are used to consider a conventional data has created a need to analyze, search and process unique data structures in a timely manner. The progressive world has created a justified need not only for a fast query searches but also the most related and meaningful searches with a minimal guidance. This article explores the potential benefits of using unorthodox solutions, including Artificial Intelligence algorithms in processing big data that are unconventional data structures. One of such big data sources was created as part of digitalization of historic heritage materials, documents and artifacts. The article calls out the benefits and disadvantages of some Artificial Intelligence algorithms and explores ways to use a few of those algorithms for the purposes of the digital heritage. It also offers the solution to maximize the potential of the search engine that could be built for digital heritage or any other unstructured data.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420303833",
    "keywords": [
      "Algorithm",
      "Archaeology",
      "Artificial intelligence",
      "Big data",
      "Computer science",
      "Cultural heritage",
      "Data mining",
      "Data science",
      "History",
      "Operating system",
      "Process (computing)"
    ],
    "authors": [
      {
        "surname": "Gasimova",
        "given_name": "Rena T."
      },
      {
        "surname": "Abbasli",
        "given_name": "Rahim N."
      }
    ]
  },
  {
    "title": "A multi-criteria ratio-based approach for two-stage data envelopment analysis",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113508",
    "abstract": "Data Envelopment Analysis (DEA) is a well-known technique for assessing efficiency levels of decision-making units (DMUs). Very often, available data may be expressed as ratios and, in such cases, traditional DEA models cannot be applied as long as biased efficiency results are produced, yielding the issues of efficiency underestimation and pseudo-inefficiency. In this paper, a novel two-stage MCDEA-R model to handle ratio data is developed observing three distinct assumptions – black-box, free-link, and fixed-link – offering a multi-criteria decision making (MCDM) perspective to the efficiency assessment problem in productive networks. While the proposed models are tested by evaluating the efficiency levels of a set of 30 bank branches in Iran, their distinctive features are highlighted in terms of previous literature to model ratio data under network structures. Precisely, there were not only gains in terms of mitigating pseudo-inefficiency and lack of discrimination power of weights issues, but there were also actual gains in terms of efficiency reliability as measured by information entropy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420303328",
    "keywords": [
      "Computer science",
      "Data envelopment analysis",
      "Data mining",
      "Econometrics",
      "Economics",
      "Efficiency",
      "Entropy (arrow of time)",
      "Envelopment",
      "Estimator",
      "Inefficiency",
      "Mathematical optimization",
      "Mathematics",
      "Microeconomics",
      "Multiple-criteria decision analysis",
      "Physics",
      "Power (physics)",
      "Quantum mechanics",
      "Reliability (semiconductor)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Gerami",
        "given_name": "Javad"
      },
      {
        "surname": "Reza Mozaffari",
        "given_name": "Mohammad"
      },
      {
        "surname": "Wanke",
        "given_name": "P.F."
      }
    ]
  },
  {
    "title": "Anytime automatic algorithm selection for knapsack",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113613",
    "abstract": "In this paper, we present a new approach for Automatic Algorithm Selection. In this new procedure, we feed the predictor of the best algorithm choice with a runtime limit for the solvers. Hence, the machine learning model should consider and learn from the Anytime Behavior of the solvers, together with features characterizing each instance. For this purpose, we propose a general Framework and apply it to the Knapsack problem. Thus, we created a large and diverse dataset of 15 , 000 instances, recorded the anytime behavior of 8 solvers on them and trained and tested three machine learning strategies, collecting the results for different machine learning algorithms. Our results show that, for the majority of the tuples < instance, time > , the solver that computes the best objective value can be predicted. We also make this data publicly available, as a challenge for the community to work in this problem and propose new and better machine learning models and solvers.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420304371",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Discrete mathematics",
      "Knapsack problem",
      "Limit (mathematics)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Programming language",
      "Selection (genetic algorithm)",
      "Solver",
      "Tuple"
    ],
    "authors": [
      {
        "surname": "Huerta",
        "given_name": "Isaías I."
      },
      {
        "surname": "Neira",
        "given_name": "Daniel A."
      },
      {
        "surname": "Ortega",
        "given_name": "Daniel A."
      },
      {
        "surname": "Varas",
        "given_name": "Vicente"
      },
      {
        "surname": "Godoy",
        "given_name": "Julio"
      },
      {
        "surname": "Asín-Achá",
        "given_name": "Roberto"
      }
    ]
  },
  {
    "title": "Automatic detection of tuberculosis related abnormalities in Chest X-ray images using hierarchical feature extraction scheme",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113514",
    "abstract": "Machine learning techniques have been widely used for abnormality detection in medical images. Chest X-ray images (CXR) are among the non-invasive diagnostic tools used to detect various disease pathologies. The ambiguous anatomical structure of soft tissues is one of the major challenges for segregating normal and abnormal images. The main objective of this study is to mimic the expert radiologist’s interpretation procedure in computer-aided diagnosis (CAD) systems. We propose an automatic technique for detection of abnormal CXR images containing one or more pathologies like pleural effusion, infiltration, fibrosis, hila enlargement, dense consolidation, etc. due to tuberculosis (TB). The proposed abnormality detection technique is based on the hierarchical feature extraction scheme in which the features are used in two-level of hierarchy to categorize healthy and unhealthy groups. In level one the handcrafted geometrical features like shape, size, eccentricity, perimeter, etc. and in level 2 traditional first order statistical feature along with texture features like energy, entropy, contrast, correlation, etc. are extracted from segmented lung-fields. Further, a supervised classification approach is employed on the extracted features to detect normal and abnormal CXR images. The performance of the algorithm is validated on a total of 800 CXR images from two public datasets, namely the Montgomery set and Shenzhen set. The obtained results (accuracy = 95.60 ± 5.07% and area under curve (AUC) = 0.95 ± 0.06 for Montgomery collection, and accuracy = 99.40 ± 1.05% and AUC = 0.99 ± 0.01 for Shenzhen collection) shows the promising performance of the proposed technique for TB detection compared to the existing state of the art approaches. Further, the obtained results are statistically validated using Friedman post-hoc multiple comparison methods, which confirms the significance of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420303389",
    "keywords": [
      "Abnormality",
      "Artificial intelligence",
      "Computer science",
      "Computer-aided diagnosis",
      "Feature extraction",
      "Medicine",
      "Pattern recognition (psychology)",
      "Psychiatry"
    ],
    "authors": [
      {
        "surname": "Chandra",
        "given_name": "Tej Bahadur"
      },
      {
        "surname": "Verma",
        "given_name": "Kesari"
      },
      {
        "surname": "Singh",
        "given_name": "Bikesh Kumar"
      },
      {
        "surname": "Jain",
        "given_name": "Deepak"
      },
      {
        "surname": "Netam",
        "given_name": "Satyabhuwan Singh"
      }
    ]
  },
  {
    "title": "Leveraging tacit knowledge for shipyard facility layout selection using fuzzy set theory",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113423",
    "abstract": "A shipyard's layout contributes significantly to manufacturing performance. The practitioners based on experience can provide feasible alternative layouts by considering qualitative factors. However, no study in the shipyard layout context has leveraged their valuable subjective knowledge. This study addresses the above research gap. It proposes a two-stage approach using the Fuzzy similarity index (FSI) and the Fuzzy goal programming model (FGPM). The first stage elicits alternative layouts and relationship (REL) charts from practitioners. REL's fuzzy proximity ratings are assigned by subjective assessment of shipbuilding process flow, inter-shop material flow, distance, cost per unit distance, sharing of material handling equipment. Thereafter, the FSI of each alternative layout with respect to the ideal layout is evaluated. However, implementing the alternative layout with the highest FSI may not be feasible due to practical constraints. Therefore, in the second stage, FGPM is formulated incorporating practical constraints related to site factors, harmful gases emission, environmental, and safety to yield an optimal selection of layout.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420302475",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Ecology",
      "Engineering",
      "Fuzzy logic",
      "Fuzzy set",
      "History",
      "Industrial engineering",
      "Knowledge management",
      "Manufacturing engineering",
      "Material flow",
      "Mathematics",
      "Operating system",
      "Operations research",
      "Paleontology",
      "Process (computing)",
      "Programming language",
      "Selection (genetic algorithm)",
      "Set (abstract data type)",
      "Shipbuilding",
      "Shipyard",
      "Tacit knowledge"
    ],
    "authors": [
      {
        "surname": "Dixit",
        "given_name": "Vijaya"
      },
      {
        "surname": "Verma",
        "given_name": "Priyanka"
      },
      {
        "surname": "Raj",
        "given_name": "Piyush"
      }
    ]
  },
  {
    "title": "NI-MWMOTE: An improving noise-immunity majority weighted minority oversampling technique for imbalanced classification problems",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113504",
    "abstract": "Oversampling techniques have been favored by researchers because of their simplicity and versatility in dealing with imbalanced classification problems. For oversampling techniques appeared in recent years (e.g. Majority Weighted Minority Oversampling Technique (MWMOTE)), noise processing plays an important role. This is because the processing of noise directly affects the distribution of new synthetic instances. MWMOTE and many other oversampling techniques use knn based noise processing method. While the knn method can effectively handle partial noise when the neighborhood parameter k value is reasonable, it may lead to under-recognition or over-recognition without prior experience. Therefore, we propose an improving noise-immunity majority weighted minority oversampling technique abbreviated NI-MWMOTE. NI-MWMOTE uses an adaptive noise processing scheme, which combines Euclidean distance and neighbor density to rank the probability that suspected noise (knn method) is true noise, and then adaptively selects the best noise processing scheme through iteration and misclassification error. Then, aggregative hierarchical clustering (AHC) method is used to cluster minority instances. And, in each sub-cluster, the sampling size of new samples is adaptively determined by classification complexity and cross-validation. NI-MWMOTE not only avoids the generation of new noise, but also effectively overcomes both between-class imbalances and within-class imbalances. Results demonstrate that NI-MWMOTE achieves significantly better results in most imbalanced datasets than eight popular oversampling algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420303286",
    "keywords": [
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Computer science",
      "Image (mathematics)",
      "Machine learning",
      "Noise (video)",
      "Noise immunity",
      "Oversampling",
      "Pattern recognition (psychology)",
      "Telecommunications",
      "Transmission (telecommunications)"
    ],
    "authors": [
      {
        "surname": "Wei",
        "given_name": "Jianan"
      },
      {
        "surname": "Huang",
        "given_name": "Haisong"
      },
      {
        "surname": "Yao",
        "given_name": "Liguo"
      },
      {
        "surname": "Hu",
        "given_name": "Yao"
      },
      {
        "surname": "Fan",
        "given_name": "Qingsong"
      },
      {
        "surname": "Huang",
        "given_name": "Dong"
      }
    ]
  },
  {
    "title": "Structural order measure of manufacturing systems based on an information-theoretic approach",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113636",
    "abstract": "A manufacturing structure is the material basis of production operation and management, and its rationality is a necessary prerequisite for ensuring highly efficient operation of manufacturing systems. Therefore, how to determine the rationality of manufacturing structures and accurately screen the optimal scheme in multiple alternatives has become an important production decision-making problem with expert and intelligent systems in recent years and needs to be solved urgently. To solve this problem, we put forward a structure entropy model and a structural order parameter in this paper, respectively. First, combined with the structural characteristics of manufacturing systems, the structure entropy model of manufacturing structures with dynamic characteristics is established and specially treated to make it operable in practical applications. Second, the structural order parameter and the definition of the order of manufacturing structures are developed, which are combined with the structure entropy model to realize the quantitative evaluation of the rationality of manufacturing structures and accurate selection of the optimal structure in the alternatives. Finally, in an empirical study, based on the two parameters of connective paths and connective spans, the structure diagrams of different manufacturing systems are made, which can directly reflect the complexity of manufacturing structures and the importance of equipment utilization, and provide another effective way for the assessment and selection of manufacturing structures. The results of the empirical study demonstrate the effectiveness of the developed approaches, which can solve the production choke point that can only rely on qualitative evaluation or trial operation of all schemes before determining the final one. Therefore, our algorithm not only provides theoretical support and decision-making basis for screening manufacturing structures, but also enriches the evaluation and decision-making methods with expert and intelligent systems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420304607",
    "keywords": [
      "Computer science",
      "Engineering",
      "Entropy (arrow of time)",
      "Flexible manufacturing system",
      "Industrial engineering",
      "Law",
      "Mathematical optimization",
      "Mathematics",
      "Operations research",
      "Physics",
      "Political science",
      "Quantum mechanics",
      "Rationality",
      "Scheduling (production processes)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Zhifeng"
      },
      {
        "surname": "David",
        "given_name": "Janet"
      }
    ]
  },
  {
    "title": "HIN_DRL: A random walk based dynamic network representation learning method for heterogeneous information networks",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113427",
    "abstract": "Learning the low-dimensional vector representation of networks can effectively reduce the complexity of various network analysis tasks, such as link prediction, clustering and classification. However, most of the existing network representation learning (NRL) methods are aimed at homogeneous or static networks, while the real-world networks are usually heterogeneous and tend to change dynamically over time, therefore providing an intelligent insight into the evolution of heterogeneous networks is more practical and significant. Based on this consideration, we focus on the dynamic representation learning problem for heterogeneous information networks, and propose a random walk based Dynamic Representation Learning method for Heterogeneous Information Networks (HIN_DRL), which can learn the representation of network nodes at different timestamps. Specifically, we improve the first step of the existing random walk based NRL methods, which generally include two steps: constructing node sequences through random walk process, and then learning node representations by throwing the node sequences into a homogeneous or heterogeneous Skip-Gram model. In order to construct optimized node sequences for evolving heterogeneous networks, we propose a method for automatically extracting and extending meta-paths, and propose a new method for generating node sequences via dynamic random walk based on meta-path and timestamp information of networks. We also propose two strategies for adjusting the quantity and length of node sequences during each random walk process, which makes it more effective to construct the node sequences for heterogeneous information networks at a specific timestamp, thus improving the effect of dynamic representation learning. Extensive experimental results show that compared with the state-of-art algorithms, HIN_DRL achieves better results in Macro-F1, Micro-F1 and NMI for multi-label node classification, multi-class node classification and node clustering on several real-world network datasets. Furthermore, case studies of visualization and dynamic on Microsoft Academic dataset demonstrate that HIN_DRL can learn network representation dynamically and more effectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420302517",
    "keywords": [
      "Artificial intelligence",
      "Computer network",
      "Computer science",
      "Construct (python library)",
      "Engineering",
      "Feature learning",
      "Law",
      "Machine learning",
      "Mathematics",
      "Node (physics)",
      "Political science",
      "Politics",
      "Random walk",
      "Representation (politics)",
      "Statistics",
      "Structural engineering",
      "Theoretical computer science",
      "Timestamp"
    ],
    "authors": [
      {
        "surname": "Meilian",
        "given_name": "LU"
      },
      {
        "surname": "Danna",
        "given_name": "YE"
      }
    ]
  },
  {
    "title": "An exact algorithm for the flexible multilevel project scheduling problem",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113485",
    "abstract": "Single project scheduling has received far more attention than have schedules of project portfolios or multi-projects. This lack of scheduling techniques is especially true for flexible portfolios, such as agile, hybrid and extreme project portfolios, which require flexible project structures. While multilevel project scheduling algorithms already exist for deterministic multilevel project structures, they are not able to handle flexible structures. This short paper wants to fill this gap. A matrix-based multilevel multimode project scheduling (M4PSP) algorithm is proposed to schedule flexible multilevel projects.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420303092",
    "keywords": [
      "Agile software development",
      "Computer science",
      "Distributed computing",
      "Dynamic priority scheduling",
      "Engineering",
      "Industrial engineering",
      "Mathematical optimization",
      "Mathematics",
      "Operating system",
      "Schedule",
      "Scheduling (production processes)",
      "Software engineering"
    ],
    "authors": [
      {
        "surname": "Kosztyán",
        "given_name": "Zsolt T."
      }
    ]
  },
  {
    "title": "A new metaheuristic based on vapor-liquid equilibrium for solving a new patient bed assignment problem",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113506",
    "abstract": "Bio-inspired computing is an emerging paradigm which is based on the basics and inspiration of natural phenomena to design new and robust competing techniques. Various nature science areas have motivated the inspiration for the design of new intelligent systems. Chemical engineering is one of them. In Chemistry, the vapor-liquid equilibrium process describes the distribution of chemical species combining two essential phases: vapor phase and liquid phase. Using a binary system of compounds is possible to simulate a search process based on the equilibrium between both phases. In this paper, we propose a new algorithm inspired by this chemical phenomenon for solving a new patient bed assignment problem. This problem consists of assigning patients to beds by considering relevant medical requirements trying to maximize the most covered soft constraints. For that, we take a traditional model, and we transform it by using the constraint optimization paradigm. We test our algorithm on 30 benchmarks taken of Chilean health services. To verify results, we perform statistical comparatives with artificial bee algorithm, ant colony optimization, the bat method, cuckoo search, genetic algorithm, particle swarm optimization, and a random strategy. Computational experiments illustrate that the VLE algorithm properly solved 30 instances, finding all global optimal. In nineteen instances, VLE converged towards the best solution in its median value. In ten instances, the median, the average and the best value, all of them achieved the global optimal. Now, when comparing VLE against other techniques, we can note that VLE is surpassed by the artificial bee colony in two instances only. The rest of the results show VLE as a robust algorithm able to suppress classical, such as genetic algorithm and particle swarm optimization.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420303304",
    "keywords": [
      "Algorithm",
      "Ant colony",
      "Ant colony optimization algorithms",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Constraint (computer-aided design)",
      "Cuckoo search",
      "Genetic algorithm",
      "Geometry",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Metaheuristic",
      "Operating system",
      "Particle swarm optimization",
      "Process (computing)",
      "Soft computing"
    ],
    "authors": [
      {
        "surname": "Taramasco",
        "given_name": "Carla"
      },
      {
        "surname": "Crawford",
        "given_name": "Broderick"
      },
      {
        "surname": "Soto",
        "given_name": "Ricardo"
      },
      {
        "surname": "Cortés-Toro",
        "given_name": "Enrique M."
      },
      {
        "surname": "Olivares",
        "given_name": "Rodrigo"
      }
    ]
  },
  {
    "title": "Semi-supervised regression trees with application to QSAR modelling",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113569",
    "abstract": "Despite the ease of collecting abundance of data about various phenomena, obtaining labeled data needed for learning models with high predictive performance remains a difficult and expensive task in many domains. This issue is particularly present in the case of the analysis of scientific data where obtaining labeled data typically requires expensive experiments. Moreover, in the analysis of scientific data, another issue is of fundamental importance: the interpretability of the models and the explainability of their decisions. By taking into account these considerations, we propose a novel semi-supervised method to learn regression trees. Thanks to the semi-supervised machine learning approach, the method is able to exploit information coming not only from labeled data, but also from unlabeled data, thus alleviating the issue of lack of labeled data. The method is based on the predictive clustering trees paradigm that extends regression trees towards structured output prediction. This allows us to obtain interpretable regression trees. The method we propose is particularly suited for the chemoinformatics task of quantitative structure-activity relationship (QSAR) modeling, which is the main application context considered in this paper. Specifically, we evaluate the proposed method on 4 QSAR modelling datasets and illustrate its use on a case study of predicting farnesyltransferase inhibitors. Additionally, we also evaluate our approach on 10 benchmark datasets not related to the QSAR modeling problem. The evaluation reveals the following: semi-supervised trees and ensembles thereof have better predictive performance than their supervised counterparts (especially when the number of labeled examples is very small); different datasets and different amounts of labeled data require different amounts of unlabeled data to be included in the learning process; and the learned semi-supervised regression trees can be used to better understand the problem at hand and the way predictions are being made.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420303936",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Biology",
      "Cheminformatics",
      "Chemistry",
      "Cluster analysis",
      "Computational chemistry",
      "Computer science",
      "Context (archaeology)",
      "Data mining",
      "Decision tree",
      "Geodesy",
      "Geography",
      "Interpretability",
      "Machine learning",
      "Mathematics",
      "Paleontology",
      "Quantitative structure–activity relationship",
      "Regression",
      "Statistics",
      "Supervised learning"
    ],
    "authors": [
      {
        "surname": "Levatić",
        "given_name": "Jurica"
      },
      {
        "surname": "Ceci",
        "given_name": "Michelangelo"
      },
      {
        "surname": "Stepišnik",
        "given_name": "Tomaž"
      },
      {
        "surname": "Džeroski",
        "given_name": "Sašo"
      },
      {
        "surname": "Kocev",
        "given_name": "Dragi"
      }
    ]
  },
  {
    "title": "Evolution features and behavior characters of friendship networks on campus life",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113519",
    "abstract": "Analyzing and mining students’ behaviors and interactions from big data is an essential part of education data mining. Based on the data of campus smart cards, which include not only static demographic information but also dynamic behavioral data from more than 30000 anonymous students, in this paper, the evolution features of friendship and the relations between behavior characters and student interactions are investigated. On the one hand, four different evolving friendship networks are constructed by means of the friend ties proposed in this paper, which are extracted from monthly consumption records. In addition, the features of the giant connected components (GCCs) of friendship networks are analyzed via social network analysis (SNA) and percolation theory. On the other hand, two high-level behavior characters, orderliness and diligence, are adopted to analyze their associations with student interactions. Our experiment/empirical results indicate that the sizes of friendship networks have declined with time growth and both the small-world effect and power-law degree distribution are found in friendship networks. Second, the results of the assortativity coefficient of both orderliness and diligence verify that there are strong peer effects among students. Finally, the percolation analysis of orderliness on friendship networks shows that a phase transition exists, which is enlightening in that swarm intelligence can be realized by intervening the key students near the transition point.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420303432",
    "keywords": [
      "Assortativity",
      "Complex network",
      "Computer science",
      "Diligence",
      "Friendship",
      "Neuroscience",
      "Orderliness",
      "Percolation (cognitive psychology)",
      "Psychology",
      "Social psychology",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Zongkai"
      },
      {
        "surname": "Su",
        "given_name": "Zhu"
      },
      {
        "surname": "Liu",
        "given_name": "Sannyuya"
      },
      {
        "surname": "Liu",
        "given_name": "Zhi"
      },
      {
        "surname": "Ke",
        "given_name": "Wenxiang"
      },
      {
        "surname": "Zhao",
        "given_name": "Liang"
      }
    ]
  },
  {
    "title": "Rationalized fruit fly optimization with sine cosine algorithm: A comprehensive analysis",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113486",
    "abstract": "The fruit fly optimization algorithm (FOA) is a well-regarded algorithm for searching the global optimal solution by simulating the foraging behavior of fruit flies. However, when solving high dimensional mathematical and practical application problems, FOA is not competitive in convergence speed, and it may quickly fall into the local optimum. Therefore, in this paper, an enhanced fruit fly optimizer, termed SCA_FOA, is developed by introducing the logic of the sine cosine algorithm (SCA). Specifically, in the process of searching for food utilizing the osphresis organ, the individual fruit fly adopts the way inspired by the SCA to fly outward or inward to find the global optimum. A comprehensive set of 28 benchmark functions were used to measure the exploitation and exploration abilities of the proposed SCA_FOA. The results demonstrate that SCA_FOA is superior to other competitive algorithms. Moreover, 10 practical problems from IEEE CEC 2011, three engineering problems, three shifted and asymmetrical functions, and optimization problems of kernel extreme learning machines (KELM) were also solved, effectively. The results and observations indicate that not only the proposed SCA_FOA can be used for simulated problems as a very efficient method, but also it can be employed for real-world applications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420303109",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Combinatorics",
      "Computer science",
      "Convergence (economics)",
      "Discrete cosine transform",
      "Economic growth",
      "Economics",
      "Geodesy",
      "Geography",
      "Geometry",
      "Image (mathematics)",
      "Kernel (algebra)",
      "Mathematical optimization",
      "Mathematics",
      "Operating system",
      "Optimization algorithm",
      "Process (computing)",
      "Sine",
      "Trigonometric functions"
    ],
    "authors": [
      {
        "surname": "Fan",
        "given_name": "Yi"
      },
      {
        "surname": "Wang",
        "given_name": "Pengjun"
      },
      {
        "surname": "Heidari",
        "given_name": "Ali Asghar"
      },
      {
        "surname": "Wang",
        "given_name": "Mingjing"
      },
      {
        "surname": "Zhao",
        "given_name": "Xuehua"
      },
      {
        "surname": "Chen",
        "given_name": "Huiling"
      },
      {
        "surname": "Li",
        "given_name": "Chengye"
      }
    ]
  },
  {
    "title": "Negation and entropy: Effectual knowledge management equipment for learning organizations",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113497",
    "abstract": "The present paper aims to put forward negation and entropy as novel perspectives for knowledge management in learning organizations. The present work considers the surrounding factors of information to enumerate the effectiveness of information in the knowledge management field. Moreover, by utilizing the effectiveness of surrounding factors, a framework inspired by Dempster-Shafer theory has been presented to generate negation. Finally, the present paper defines and calculates entropy for information. We have applied negation and entropy concepts to two well-known cases of organizational systems and management domains to study their impact on the knowledge management domain. The cases have supported our decisions taken after the calculation of proposed negation and entropy concepts. The proposed method has a quality interpretation of the information operators and has the merit of simplifying knowledge management and decision making problems in the context of a learning organization. The present study combines multiple aspects of knowledge management, organizational systems, decision making, and organizational learning.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420303213",
    "keywords": [
      "Artificial intelligence",
      "Autoepistemic logic",
      "Computer science",
      "Description logic",
      "Domain knowledge",
      "Economics",
      "Entropy (arrow of time)",
      "Knowledge management",
      "Management science",
      "Multimodal logic",
      "Negation",
      "Negation as failure",
      "Organizational learning",
      "Physics",
      "Programming language",
      "Quantum mechanics"
    ],
    "authors": [
      {
        "surname": "Anjaria",
        "given_name": "Kushal"
      }
    ]
  },
  {
    "title": "An online isotonic separation with cascade architecture for binary classification",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113466",
    "abstract": "Isotonic separation (IS) is a non-parametric classification technique which constructs an isotonic function from ordered data. The rationale is to convert partially isotonic data into isotonic using a linear programming problem (LPP) and partition the input space into isotonic and non-isotonic regions to make predictions easier. Despite the widespread applications of IS in diverse domains where monotonicity exists, it has certain limitations: Firstly, computing time and the constraints of the LPP in isotonic separation increase polynomially as size of the data increases and it is highly complex to solve the LPP and obtain the model on large data sets. In order to support dynamic stream data and address the computational overhead and size of the LPP issues, this paper proposes an online isotonic separation algorithm called Cascade-IS (CIS) for binary classification. The rationale behind CIS is that it splits the data set into a sequence of partitions and models are obtained and combined in cascade. Statistical and experimental analysis are done on datasets with isotonic properties and the results prove that CIS is superior to its variants in terms of training time, performance measures and number of constraints in the LPP.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420302906",
    "keywords": [
      "Algorithm",
      "Arithmetic",
      "Artificial intelligence",
      "Binary classification",
      "Binary number",
      "Cascade",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Data mining",
      "Estimator",
      "Internal medicine",
      "Isotonic",
      "Isotonic regression",
      "Mathematics",
      "Medicine",
      "Statistics",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Malar",
        "given_name": "B."
      },
      {
        "surname": "Nadarajan",
        "given_name": "R."
      }
    ]
  },
  {
    "title": "Boundary constrained voxel segmentation for 3D point clouds using local geometric differences",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113439",
    "abstract": "In 3D point cloud processing, the spatial continuity of points is convenient for segmenting point clouds obtained by 3D laser scanners, RGB-D cameras and LiDAR (light detection and ranging) systems in general. In real life, the surface features of both objects and structures give meaningful information enabling them to be identified and distinguished. Segmenting the points by using their local plane directions (normals), which are estimated by point neighborhoods, is a method that has been widely used in the literature. The angle difference between two nearby local normals allows for measurement of the continuity between the two planes. In real life, the surfaces of objects and structures are not simply planes. Surfaces can also be found in other forms, such as cylinders, smooth transitions and spheres. The proposed voxel-based method developed in this paper solves this problem by inspecting only the local curvatures with a new merging criteria and using a non-sequential region growing approach. The general prominent feature of the proposed method is that it mutually one-to-one pairs all of the adjoining boundary voxels between two adjacent segments to examine the curvatures of all of the pairwise connections. The proposed method uses only one parameter, except for the parameter of unit point group (voxel size), and it does not use a mid-level over-segmentation process, such as supervoxelization. The method checks the local surface curvatures using unit normals, which are close to the boundary between two growing adjacent segments. Another contribution of this paper is that some effective solutions are introduced for the noise units that do not have surface features. The method has been applied to one indoor and four outdoor datasets, and the visual and quantitative segmentation results have been presented. As quantitative measurements, the accuracy (based on the number of true segmented points over all points) and F1 score (based on the means of precision and recall values of the reference segments) are used. The results from testing over five datasets show that, according to both measurement techniques, the proposed method is the fastest and achieves the best mean scores among the methods tested.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420302633",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Boundary (topology)",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Geometry",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Normal",
      "Philosophy",
      "Plane (geometry)",
      "Point (geometry)",
      "Point cloud",
      "Segmentation",
      "Surface (topology)",
      "Voxel"
    ],
    "authors": [
      {
        "surname": "Saglam",
        "given_name": "Ali"
      },
      {
        "surname": "Makineci",
        "given_name": "Hasan Bilgehan"
      },
      {
        "surname": "Baykan",
        "given_name": "Nurdan Akhan"
      },
      {
        "surname": "Baykan",
        "given_name": "Ömer Kaan"
      }
    ]
  },
  {
    "title": "Modeling adaptive E-Learning environment using facial expressions and fuzzy logic",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113460",
    "abstract": "The integration of two or more intelligent systems, such as deep neural networks and fuzzy techniques, results in so-called hybrid intelligent systems. These have recently attracted considerable attention owing to their broad success in several complex real-world applications. An interesting example is the development of adaptive computer-based learning environments, in which learner responses to certain questions are considered so that the next level of the learning process may be determined. However, these methods fail to capture the behavior and emotional expressions of the learner during the learning process. Capturing these could make the learning flow more adaptive, and could allow the learning environment to redirect learners to different learning paths based on their capabilities and interaction levels. In this regard, the contribution of the present study is threefold. First, it proposes an approach for modeling an intelligent adaptive e-learning environment by considering the integration of the learner responses to questions and their emotional states. In the proposed approach, a loosely coupled integration between a convolutional neural network (CNN) and a fuzzy system is adopted. The CNN is used to detect a learner’s facial expressions, and outperforms other CNN models on the same training benchmark. The fuzzy system is used to determine the next learning level based on the extracted facial expression states from the CNN and several response factors by the learner. Second, the study introduces methods whereby a group of facial expressions is aggregated into a single representative. Third, it introduces corpora for evaluating the performance of the proposed approach. The corpora of 12 learners contain 72 learning activities and 1735 data points of distinct emotional states. The experimental results using these corpora demonstrate that the proposed approach provides adaptive learning flows that match the learning capabilities of all learners in a group. Moreover, the approach allows decision makers to monitor the learning performance for each learner.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420302840",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Facial expression",
      "Fuzzy logic",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Operating system",
      "Process (computing)"
    ],
    "authors": [
      {
        "surname": "Megahed",
        "given_name": "Mohammed"
      },
      {
        "surname": "Mohammed",
        "given_name": "Ammar"
      }
    ]
  },
  {
    "title": "An ACS-based memetic algorithm for the heterogeneous vehicle routing problem with time windows",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113379",
    "abstract": "This paper presents a solution methodology to solve the heterogeneous vehicle routing problem with time windows (HVRPTW). This problem appears when a limited fleet of vehicles, characterized by different capacities, fixed costs and variable costs, is available for serving a set of customers which have to be visited within a predefined time window. The objective is to perform the route design minimizing the total fixed vehicle costs and distribution costs and satisfying all problem constraints. The problem is solved using an Ant Colony System (ACS) algorithm which has been successfully applied to combinatorial optimization problems. Moreover, to improve the performance of the ACS on the HVRPTW, a hybridized ACS with local search, called memetic ACS algorithm is proposed where the local search is performed by a variable neighborhood Tabu Search algorithm. Experiments are conducted on sets of benchmark instances from the scientific literature to evaluate the performance of the proposed algorithm. The results show that the algorithm has a good performance on the HVRPTW. In particular, out of the 80 instances, it obtained 65 new best solutions and matched 6 within reasonable computational times.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420302037",
    "keywords": [
      "Algorithm",
      "Ant colony optimization algorithms",
      "Benchmark (surveying)",
      "Computer network",
      "Computer science",
      "Geodesy",
      "Geography",
      "Guided Local Search",
      "Local search (optimization)",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Memetic algorithm",
      "Metaheuristic",
      "Programming language",
      "Routing (electronic design automation)",
      "Set (abstract data type)",
      "Tabu search",
      "Variable (mathematics)",
      "Variable neighborhood search",
      "Vehicle routing problem"
    ],
    "authors": [
      {
        "surname": "Molina",
        "given_name": "Jose C."
      },
      {
        "surname": "Salmeron",
        "given_name": "Jose L."
      },
      {
        "surname": "Eguia",
        "given_name": "Ignacio"
      }
    ]
  },
  {
    "title": "Entrotaxis-Jump as a hybrid search algorithm for seeking an unknown emission source in a large-scale area with road network constraint",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113484",
    "abstract": "In a sudden hazardous material leakage accident, the rapid and accurate localization of the leakage source can effectively reduce casualties and property losses. Utilizing the sensible robot to seek an unknown emission source has become a promising field, while most researches in this field fail to consider some intractable but practical factors, such as the large spatial scale of some search domains and the road network (that can obstruct robot’s maneuver). This paper proposes an efficient search algorithm, named as Entrotaxis-Jump, to seek an unknown emission source and obtain other source terms (e.g., source strength) in a large-scale (>0.1 km2) practical scene with road network constraints, such as a chemical cluster. The hybrid algorithm incorporates the Entrotaxis algorithm (a kind of cognitive search algorithm) with the intermittent search strategy, so it can utilize the triggering jump motion to alleviate the negative factors for search (road network constraints, expansive search domain, and turbulence effect). We select a chemical cluster in Shanghai, China as the typical research area and conduct a series of simulations in it to compare the performance of the Entrotaxis with the Entrotaxis-Jump under various release strength Q and wind speed V. The performance is reflected by the success rate (SR) and mean search time (MST), and we propose a skill score S to consider the two indexes synthetically. The results denote that the Entrotaxis-Jump outperforms Entrotaxis in all of our simulated scenarios, especially when the wind speed V > 2, in which case the SR of Entrotaxis drops sharply while the SR of the Entrotaxis-Jump witnesses a little decline but remains over 90%. The Entrotaxis-Jump algorithm proposed in this paper considers more practical factors, compared to previous researches, and is suitable and robust to utilize in real scenarios.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420303080",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Field (mathematics)",
      "Jump",
      "Mathematical optimization",
      "Mathematics",
      "Physics",
      "Pure mathematics",
      "Quantum mechanics",
      "Robot",
      "Search algorithm",
      "Search and rescue",
      "Simulation"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Yong"
      },
      {
        "surname": "Chen",
        "given_name": "Bin"
      },
      {
        "surname": "Zhu",
        "given_name": "Zhengqiu"
      },
      {
        "surname": "Chen",
        "given_name": "Feiran"
      },
      {
        "surname": "Wang",
        "given_name": "Yiduo"
      },
      {
        "surname": "Ma",
        "given_name": "Denglong"
      }
    ]
  },
  {
    "title": "Oslcfit (organic simultaneous LSTM and CNN Fit): A novel deep learning based solution for sentiment polarity classification of reviews",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113488",
    "abstract": "Review sentiment influences purchase decisions and indicates user satisfaction. Inferring the sentiment from reviews is an essential task in Natural Language Processing and has managerial implications for improving customer satisfaction and item quality. Traditional approaches to polarity classification use bag-of-words techniques and lexicons combined with machine learning. These approaches suffer from an inability to capture semantics and context. We propose a Deep Learning solution called OSLCFit (Organic Simultaneous LSTM and CNN Fit). In our architecture, we include all the components of a CNN until but not including the final fully connected layer and do the same in case of a bi-directional LSTM. The final fully connected layer in our architecture consists of fixed length features from the CNN, and features for both variable length and temporal dependencies from the bi-directional LSTM. The solution fine-tunes Language Model embeddings for the specific task of polarity classification using transfer learning, enabling the capture of semantics and context. The key contribution of this paper is the combination of features from both a CNN and a bi-directional LSTM into a single architecture with a single optimizer. This combination forms an organic combination and uses embeddings fine-tuned to the reviews for the specific purpose of sentiment polarity classification. The solution is benchmarked on six different datasets such as SMS Spam, YouTube Spam, Large Movie Review Corpus, Stanford Sentiment Treebank, Amazon Cellphone & Accessories and Yelp, where it beats existing benchmarks and scales to large datasets. The source code is available for the purposes of reproducible research on GitHub. 1 1 https://github.com/efpm04013/finalexp34",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420303122",
    "keywords": [
      "Annotation",
      "Architecture",
      "Art",
      "Artificial intelligence",
      "Biology",
      "Cell",
      "Chemistry",
      "Computer science",
      "Context (archaeology)",
      "Deep learning",
      "Economics",
      "Genetics",
      "Layer (electronics)",
      "Machine learning",
      "Management",
      "Natural language processing",
      "Organic chemistry",
      "Paleontology",
      "Polarity (international relations)",
      "Programming language",
      "Semantics (computer science)",
      "Sentiment analysis",
      "Task (project management)",
      "Transfer of learning",
      "Treebank",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Kiran",
        "given_name": "R."
      },
      {
        "surname": "Kumar",
        "given_name": "Pradeep"
      },
      {
        "surname": "Bhasker",
        "given_name": "Bharat"
      }
    ]
  },
  {
    "title": "Two-stage additive network DEA: Duality, frontier projection and divisional efficiency",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113478",
    "abstract": "In the previous literature, it is demonstrated that the dual equivalence of multiplier and envelopment models that exists in standard data envelopment analysis (DEA) is not necessarily true for network DEA to derive frontier projection and divisional efficiency. Multiplier network model is often used for computing the divisional efficiency while envelopment network model is often used for identifying the frontier projection for inefficient decision making units (DMUs). In this paper, we show that the duality of standard DEA can be extended to two-stage additive network DEA. We propose an improved golden section method to solve parametric linear multiplier network model. Based on the primal-dual correspondence of parametric linear programming, we subsequently develop envelopment network model in parametric linear form to determine frontier projection and to find divisional efficiency as well.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742030302X",
    "keywords": [
      "Algorithm",
      "Archaeology",
      "Art",
      "Computer science",
      "Data envelopment analysis",
      "Discrete mathematics",
      "Dual (grammatical number)",
      "Duality (order theory)",
      "Economics",
      "Efficient frontier",
      "Envelopment",
      "Equivalence (formal languages)",
      "Financial economics",
      "Frontier",
      "History",
      "Linear programming",
      "Literature",
      "Macroeconomics",
      "Mathematical optimization",
      "Mathematics",
      "Multiplier (economics)",
      "Parametric programming",
      "Parametric statistics",
      "Portfolio",
      "Projection (relational algebra)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Guo",
        "given_name": "Chuanyin"
      },
      {
        "surname": "Zhang",
        "given_name": "Jian"
      },
      {
        "surname": "Zhang",
        "given_name": "Linyan"
      }
    ]
  },
  {
    "title": "Rhetorical structure theory: A comprehensive review of theory, parsing methods and applications",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113421",
    "abstract": "Rhetorical structure theory (RST) is a significant theory about discourse organization. With an increasing number of research interests focus on RST, many novel parsing approaches have been proposed and motivated many brand new applications, such as chatbots and other expert and intelligent systems. However, the work on RST dates back many years and there remains a lack of comprehensive literature reviews. The aim of this study is therefore to provide a comprehensive overview of RST, parsing methods and applications. In this paper, we first give a detailed introduction to RST. Then the commonly used discourse treebank: RST-DT is elaborated. We propose a new taxonomy to divide the RST parsing methods into different categories. With a focus on the classical and latest methods that have recently been developed, we review the pros and cons of these approaches, along with the performance analysis of them. We then summarize the applications of RST across various domains. Moreover, we present a comparative study of RST with other discourse structure theories. Finally, we discuss some implications of our findings and outline future research directions in this challenging and fast-growing field.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420302451",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Computer science",
      "Data science",
      "Field (mathematics)",
      "Focus (optics)",
      "Linguistics",
      "Mathematics",
      "Natural language processing",
      "Optics",
      "Parsing",
      "Philosophy",
      "Physics",
      "Pure mathematics",
      "Rhetorical question",
      "Taxonomy (biology)",
      "Treebank"
    ],
    "authors": [
      {
        "surname": "Hou",
        "given_name": "Shengluan"
      },
      {
        "surname": "Zhang",
        "given_name": "Shuhan"
      },
      {
        "surname": "Fei",
        "given_name": "Chaoqun"
      }
    ]
  },
  {
    "title": "Gold volatility prediction using a CNN-LSTM approach",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113481",
    "abstract": "Prediction of volatility for different types of financial assets is one of the tasks of greater mathematical complexity in time series prediction, mainly due to its noisy, non-stationary and heteroscedastic structure. On the other hand, gold is an asset of particular importance for hedging and diversification of investment portfolios, and therefore it is important to predict future volatility of this asset. This paper seeks to significantly improve the forecast of gold volatility by combining two deep learning methodologies: short-term memory networks (LSTM) added to convolutional neural networks (specifically a pre-trained VGG16 network). It is important to mention that these types of hybrid architectures have not been used in time series prediction, so it is a completely new approach to solving these types of problems. The CNN-LSTM hybrid model is capable of including images as input which provides a wide variety of information associated with both static and dynamic characteristics of the series. In parallel, different lags of profitability of the series are entered as input, which allows it to learn from the temporal structure. The results show a substantial improvement when this hybrid model is compared to the GARCH and LSTM models. A 37% reduction in MSE is observed compared to the classic GARCH model, and 18% compared to the LSTM model. Finally, the Model Confidence Model (MCS) determines a significant improvement in the prediction of the hybrid model. The fundamental importance of this research lies in the application of a new type of architecture capable of processing various sources of information for any time series prediction task.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420303055",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Autoregressive conditional heteroskedasticity",
      "Business",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Diversification (marketing strategy)",
      "Econometrics",
      "Economics",
      "Finance",
      "Heteroscedasticity",
      "Machine learning",
      "Marketing",
      "Mathematics",
      "Profitability index",
      "Time series",
      "Volatility (finance)"
    ],
    "authors": [
      {
        "surname": "Vidal",
        "given_name": "Andrés"
      },
      {
        "surname": "Kristjanpoller",
        "given_name": "Werner"
      }
    ]
  },
  {
    "title": "On the design of hardware architectures for parallel frequent itemsets mining",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113440",
    "abstract": "Algorithms for Frequent Itemsets Mining have proved their effectiveness for extracting frequent sets of patterns in datasets. However, in some specific cases, they do not obtain the expected results in an acceptable time. For this reason, Field Programmable Gates Array-based architectures for Frequent Itemsets Mining have been proposed to accelerate this task. The current paper proposes a search strategy for Frequent Itemsets Mining based on equivalence classes partitioning. The partitioning on equivalence classes allows dividing the search space into disjoint sets that can be processed in parallel. Consequently, this paper presents the design and implementation of two hardware architectures that exploit the nested parallelism in the proposed search strategy. These hardware architectures are capable of obtaining frequent itemsets regardless of the number of distinct items and the number of transactions in the dataset, which are the main issues reported in the reviewed literature. Furthermore, the proposed architectures explore the trade-off between acceleration and hardware resource utilization. The experimental results obtained demonstrate that the proposed search strategy can be scaled to achieve a speedup in the processing time of 40 times faster than software-based implementations.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420302645",
    "keywords": [
      "Computer architecture",
      "Computer science",
      "Data mining",
      "Embedded system",
      "Parallel computing"
    ],
    "authors": [
      {
        "surname": "Letras",
        "given_name": "Martín"
      },
      {
        "surname": "Bustio-Martínez",
        "given_name": "Lázaro"
      },
      {
        "surname": "Cumplido",
        "given_name": "René"
      },
      {
        "surname": "Hernández-León",
        "given_name": "Raudel"
      },
      {
        "surname": "Feregrino-Uribe",
        "given_name": "Claudia"
      }
    ]
  },
  {
    "title": "PV-DAE: A hybrid model for deceptive opinion spam based on neural network architectures",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113517",
    "abstract": "Opinion review is of great importance for both customers and organizations. Indeed, it helps customers in buying decisions and represents a valuable feedback for the companies, allowing them to improve their productions. However, numerous greedy companies resort to fake reviews in order to influence the customer and brighten the brand image, or to defame the one of their competitors. Various models are proposed in order to detect deceptive opinion reviews. Most of these models adopt traditional methods focusing on feature extraction and traditional classifiers. Unfortunately, these models do not capture the semantic aspect while ignoring the opinion’s context. In order to tackle this issue, we propose a new approach based on Paragraph Vector Distributed Bag of Words (PV-DBOW) and the Denoising Autoencoder (DAE). The proposed customized model provides a strong representation which is based on a global representation of the opinions while preserving their semantics. Indeed, the embedding vectors capture the semantic meaning of all words in the context of each opinion. The generated review representations are fed into a fully connected neural network in order to detect deceptive opinion spam. The obtained results concerning the deception dataset show that our model is effective and outperforms the existing state-of-the-art methodologies.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420303419",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Autoencoder",
      "Biology",
      "Competitor analysis",
      "Computer science",
      "Context (archaeology)",
      "Deception",
      "Economics",
      "Finance",
      "Law",
      "Machine learning",
      "Management",
      "Order (exchange)",
      "Paleontology",
      "Paragraph",
      "Political science",
      "Politics",
      "Programming language",
      "Psychology",
      "Representation (politics)",
      "Semantics (computer science)",
      "Sentiment analysis",
      "Social psychology",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Fahfouh",
        "given_name": "Anass"
      },
      {
        "surname": "Riffi",
        "given_name": "Jamal"
      },
      {
        "surname": "Adnane Mahraz",
        "given_name": "Mohamed"
      },
      {
        "surname": "Yahyaouy",
        "given_name": "Ali"
      },
      {
        "surname": "Tairi",
        "given_name": "Hamid"
      }
    ]
  },
  {
    "title": "HBoost: A heterogeneous ensemble classifier based on the Boosting method and entropy measurement",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113482",
    "abstract": "In recent years, ensemble classifiers have attracted a lot of attention in the field of machine learning. The main challenges with these classifiers are 1) to select the base classifiers and 2) to combine the outputs. The key point for an ensemble to be successful is the diversity and accuracy of the base classifiers. This paper proposes a heterogeneous Boosting-based ensemble classifier (HBoost) which is inspired by Boosting algorithm and aims at increasing the diversity by recruiting distinct learning algorithms. In this approach, for each learning algorithm, several classifiers are generated by using the Boosting method and a matrix of heterogeneous classifiers is formed. Since all generated classifiers may not be appropriate for generating the final output, the subset of those that make the most diverse and accurate classifiers is selected and the rest of the classifiers are eliminated. Finally, the classifiers in the subset are combined based on the weight assigned to each classifier. HBoost is a self-configured algorithm i.e., unlike existing methods, it does not require a human expert to specify the type of base classifiers manually. It can also automatically determine the optimum number of base classifiers which should be combined. For evaluating the HBoost, two evaluation metrics have been used: Accuracy and Geometric Mean which are the most popular metrics to measure the performance of a classifier. We compare the proposed approach in three different scenarios 1) all the base classifiers are in the ensemble, 2) four traditional ensemble methods including Bagging, Boosting, Stacking, and StackingC, 3) two state-of-the-art approaches in the literature including Random Forest and Classifier Subset Selection (CSS). HBoost is performed on 20 datasets from the UCI repository and experimental results reveal how adequate the HBoost is in comparison to other approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420303067",
    "keywords": [
      "Artificial intelligence",
      "Boosting (machine learning)",
      "Cascading classifiers",
      "Classifier (UML)",
      "Computer science",
      "Ensemble learning",
      "Entropy (arrow of time)",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Random subspace method"
    ],
    "authors": [
      {
        "surname": "Kadkhodaei",
        "given_name": "Hamid Reza"
      },
      {
        "surname": "Moghadam",
        "given_name": "Amir Masoud Eftekhari"
      },
      {
        "surname": "Dehghan",
        "given_name": "Mehdi"
      }
    ]
  },
  {
    "title": "Cost-sensitive multiple-instance learning method with dynamic transactional data for personal credit scoring",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113489",
    "abstract": "We study how to assess an applicant’s credit risk with dynamic transactional data. The problem arises when an applicant applies for loans from financial institutions. A traditional credit-risk assessment model utilizes individual demographic and loan information from an application form. Nevertheless, dynamic transactional data is good indicators of an applicant’s credit risk. However, the lack of available data and the preexisting limitations of conventional approaches limit the use of the dynamic transactional data. In this study, we propose a cost-sensitive multiple-instance learning (MIL) approach to evaluate applicants’ credit scores that incorporate their dynamic transactional data and static individual information. Traditionally, MIL approaches can handle the variable number of input instances. However, to facilitate the implementation of MIL into credit scoring, we extend the MIL to consider the dynamic transactional data and cost-sensitive problem simultaneously. We compare our model with several benchmark MIL models by testing them on real-world data sets. Experimental results show that our model outperforms most benchmarks in many widely used criteria.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420303134",
    "keywords": [
      "Benchmark (surveying)",
      "Computer science",
      "Credit score",
      "Data mining",
      "Database",
      "Database transaction",
      "Dynamic data",
      "Economics",
      "Finance",
      "Geodesy",
      "Geography",
      "Loan",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Psychology",
      "Social psychology",
      "Transaction data",
      "Transactional leadership",
      "Variable (mathematics)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Wei"
      },
      {
        "surname": "Xu",
        "given_name": "Wei"
      },
      {
        "surname": "Hao",
        "given_name": "Haijing"
      },
      {
        "surname": "Zhu",
        "given_name": "Dan"
      }
    ]
  },
  {
    "title": "Wearable payment: A deep learning-based dual-stage SEM-ANN analysis",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113477",
    "abstract": "The research paper purports to assess the antecedents that affect users’ behavioral intention to use wearable payment. Specifically, this empirical research examines the roles of perceived aesthetics, technology readiness, mobile usefulness, and mobile ease of use on behavioral intention. Differing from past mobile payment studies, a newly proposed methodology that involves a dual-stage analysis and an emerging Artificial Intelligence analysis named deep learning was performed on 307 usable responses. Findings revealed that all relationships were supported except for the linkage between mobile ease of use and behavioral intention. The results of this study provide valuable insights to payment companies and smart wearable device manufacturers to come up with plans and marketing strategies to convince the potential adopters to adopt wearable payment, guiding marketers to design a more successful wearable payment solution. Theoretically, the newly integrated theoretical model that incorporates Mobile Technology Acceptance Model, Fashion Theory, and Technology Readiness Theory could help ascertain the relative significance of certain determinants, providing a clearer insight on the acceptance of wearable payment among consumers.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420303018",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Computer science",
      "Deep learning",
      "Dual (grammatical number)",
      "Embedded system",
      "Geology",
      "Literature",
      "Machine learning",
      "Paleontology",
      "Payment",
      "Stage (stratigraphy)",
      "Wearable computer",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Lee",
        "given_name": "Voon-Hsien"
      },
      {
        "surname": "Hew",
        "given_name": "Jun-Jie"
      },
      {
        "surname": "Leong",
        "given_name": "Lai-Ying"
      },
      {
        "surname": "Tan",
        "given_name": "Garry Wei-Han"
      },
      {
        "surname": "Ooi",
        "given_name": "Keng-Boon"
      }
    ]
  },
  {
    "title": "GIMO: A multi-objective anytime rule mining system to ease iterative feedback from domain experts",
    "journal": "Expert Systems with Applications: X",
    "year": "2020",
    "doi": "10.1016/j.eswax.2020.100040",
    "abstract": "Data extracted from software repositories is used intensively in Software Engineering research, for example, to predict defects in source code. In our research in this area, with data from open source projects as well as an industrial partner, we noticed several shortcomings of conventional data mining approaches for classification problems: (1) Domain experts’ acceptance is of critical importance, and domain experts can provide valuable input, but it is hard to use this feedback. (2) Evaluating the quality of the model is not a matter of calculating AUC or accuracy. Instead, there are multiple objectives of varying importance with hard to quantify trade-offs. Furthermore, the performance of the model cannot be evaluated on a per-instance level in our case, because it shares aspects with the set cover problem. To overcome these problems, we take a holistic approach and develop a rule mining system that simplifies iterative feedback from domain experts and can incorporate the domain-specific evaluation needs. A central part of the system is a novel multi-objective anytime rule mining algorithm. The algorithm is based on the GRASP-PR meta-heuristic but extends it with ideas from several other approaches. We successfully applied the system in the industrial context. In the current article, we focus on the description of the algorithm and the concepts of the system. We make an implementation of the system available.",
    "link": "https://www.sciencedirect.com/science/article/pii/S2590188520300196",
    "keywords": [],
    "authors": [
      {
        "surname": "Baum",
        "given_name": "Tobias"
      },
      {
        "surname": "Herbold",
        "given_name": "Steffen"
      },
      {
        "surname": "Schneider",
        "given_name": "Kurt"
      }
    ]
  },
  {
    "title": "A multi-population, multi-objective memetic algorithm for energy-efficient job-shop scheduling with deteriorating machines",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113348",
    "abstract": "This paper focuses on an energy-efficient job-shop scheduling problem within a machine speed scaling framework, where productivity is affected by deterioration. To alleviate the deterioration effect, necessary maintenance activities must be put in place during the scheduling process. In addition to sequencing operations on machines, the problem at hand aims to determine the appropriate speeds of machines and positions of maintenance activities for the schedule, in order to minimise the total weighted tardiness and total energy consumption simultaneously. To deal with this problem, a multi-population, multi-objective memetic algorithm is proposed, in which the solutions are distributed into sub-populations. Besides a general local search, an advanced objective-oriented local search is also executed periodically on a portion of the population. These local search methods are designed based on a new disjunctive graph introduced to cover the solution space. Furthermore, an efficient non-dominated sorting method for bi-objective optimisation is developed. The performance of the memetic algorithm is evaluated via a series of comprehensive computational experiments, comparing it with state-of-the-art algorithms presented for job-shop scheduling problems with/without considering energy efficiency. Experimental results confirm that the proposed algorithm can outperform other algorithms being compared across a range of performance metrics.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420301731",
    "keywords": [
      "Algorithm",
      "Computer science",
      "Demography",
      "Flow shop scheduling",
      "Job shop",
      "Job shop scheduling",
      "Local search (optimization)",
      "Mathematical optimization",
      "Mathematics",
      "Memetic algorithm",
      "Operating system",
      "Population",
      "Schedule",
      "Scheduling (production processes)",
      "Sociology",
      "Sorting",
      "Tardiness"
    ],
    "authors": [
      {
        "surname": "Abedi",
        "given_name": "Mehdi"
      },
      {
        "surname": "Chiong",
        "given_name": "Raymond"
      },
      {
        "surname": "Noman",
        "given_name": "Nasimul"
      },
      {
        "surname": "Zhang",
        "given_name": "Rui"
      }
    ]
  },
  {
    "title": "Parsimonious fuzzy time series modelling",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113447",
    "abstract": "This paper proposes a novel modelling structure to ensure the parsimony of fuzzy time series (FTS) models while retaining certain level of out-of-sample accuracy. A parsimonious FTS model requires multiple optimizations of hyper-parameters such as time lags and partitioning which consists of the number of fuzzy sets, the partitioning type and the membership functions. In the vast literature of fuzzy time series, hyper-parameter optimization is usually ignored. In addition to that, optimization process for the hyper-parameters is also not presented properly. In this study, a parsimonious FTS modelling approach is introduced by using genetic algorithm (GA). Three major innovations are proposed: (1) Hyper-parameters of FTS structure are optimized to eliminate subjective preferences with the help of GA. Some of those parameters are never optimized or simply ignored in the past research. (2) The set of hyper-parameters is optimized subject to highest accuracy in validation set data and model’s complexity. (3) For achieving sparsification and accuracy simultaneously at reasonable computation time, a two-stage GA optimization is run to search for higher accuracy and lower complexity consecutively. Empirical studies are conducted on two types of datasets. Prices of liquid bulk cargo carriers (i.e. tanker) and secondhand ship have been predicted using the proposed approach. Potential benchmarks as well as a simple Nave forecast have been compared to the proposed model for validation based on mean absolute scaled error and root mean squared error.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420302712",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Composite material",
      "Computer science",
      "Data mining",
      "Fuzzy logic",
      "Genetic algorithm",
      "Machine learning",
      "Materials science",
      "Mathematical optimization",
      "Mathematics",
      "Mean squared error",
      "Operating system",
      "Paleontology",
      "Process (computing)",
      "Programming language",
      "Range (aeronautics)",
      "Series (stratigraphy)",
      "Set (abstract data type)",
      "Statistics",
      "Time series"
    ],
    "authors": [
      {
        "surname": "Gao",
        "given_name": "Ruobin"
      },
      {
        "surname": "Duru",
        "given_name": "Okan"
      }
    ]
  },
  {
    "title": "Effective algorithms for single-machine learning-effect scheduling to minimize completion-time-based criteria with release dates",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113445",
    "abstract": "Multi-variety and small-batch productions are usually undertaken by skilled workers instead of an automatic assembly line because of economic cost consideration. In the production process, a worker's familiarity to an operation influences the length of task execution time. An interesting phenomenon called learning effect has become a trending research topic. This study investigates a learning effect scheduling model on a single machine system, in which the learning effect is position-dependent and each task is released at different dates. Two optimal criteria are individually discussed: one is total k-power completion time, and the other is maximum lateness. Both problems are NP-hard, therefore, effective algorithms are provided to handle different scale problems within an appropriate CPU time. The heuristic algorithms, namely, shortest processing time available and earliest due date available, are introduced to achieve feasible schedules for large-scale instances, and their asymptotic optimality is proven given that the problem scale tends to infinity. The two heuristics can thus serve as optimal algorithms in mass production. For small-scale instances, a branch and bound algorithm is presented to achieve the optimal solution, where a release-date-based branching rule and preemption-based lower bounds eliminate as many invalid nodes as possible. For medium-scale instances, an evolutionary-based metaheuristic algorithm, namely, discrete differential evolution, is utilized to seek high-quality solutions, in which the initial population and crossover operator are well-designed to enhance its performance. A number of random experiments demonstrate the superiority of the proposed algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420302694",
    "keywords": [
      "Algorithm",
      "Computer science",
      "Crossover",
      "Demography",
      "Heuristics",
      "Job shop scheduling",
      "Machine learning",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Operating system",
      "Population",
      "Preemption",
      "Schedule",
      "Scheduling (production processes)",
      "Sociology",
      "Upper and lower bounds"
    ],
    "authors": [
      {
        "surname": "Bai",
        "given_name": "Danyu"
      },
      {
        "surname": "Xue",
        "given_name": "Hanyu"
      },
      {
        "surname": "Wang",
        "given_name": "Ling"
      },
      {
        "surname": "Wu",
        "given_name": "Chin-Chia"
      },
      {
        "surname": "Lin",
        "given_name": "Win-Chin"
      },
      {
        "surname": "Abdulkadir",
        "given_name": "Danladi H."
      }
    ]
  },
  {
    "title": "DECAF: Deep Case-based Policy Inference for knowledge transfer in Reinforcement Learning",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113420",
    "abstract": "Having the ability to solve increasingly complex problems using Reinforcement Learning (RL) has prompted researchers to start developing a greater interest in systematic approaches to retain and reuse knowledge over a variety of tasks. With Case-based Reasoning (CBR) there exists a general methodology that provides a framework for knowledge transfer which has been underrepresented in the RL literature so far. We formulate a terminology for the CBR framework targeted towards RL researchers with the goal of facilitating communication between the respective research communities. Based on this framework, we propose the Deep Case-based Policy Inference (DECAF) algorithm to accelerate learning by building a library of cases and reusing them if they are similar to a new task when training a new policy. DECAF guides the training by dynamically selecting and blending policies according to their usefulness for the current target task, reusing previously learned policies for a more effective exploration but still enabling the adaptation to particularities of the new task. We show an empirical evaluation in the Atari game playing domain depicting the benefits of our algorithm with regards to sample efficiency, robustness against negative transfer, and performance increase when compared to state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742030244X",
    "keywords": [
      "Adaptation (eye)",
      "Artificial intelligence",
      "Biochemistry",
      "Biology",
      "Chemistry",
      "Classifier (UML)",
      "Computer science",
      "Domain adaptation",
      "Ecology",
      "Economics",
      "First language",
      "Gene",
      "Inference",
      "Knowledge management",
      "Knowledge transfer",
      "Linguistics",
      "Machine learning",
      "Management",
      "Negative transfer",
      "Optics",
      "Philosophy",
      "Physics",
      "Reinforcement learning",
      "Reuse",
      "Robustness (evolution)",
      "Task (project management)",
      "Terminology",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Glatt",
        "given_name": "Ruben"
      },
      {
        "surname": "Da Silva",
        "given_name": "Felipe Leno"
      },
      {
        "surname": "da Costa Bianchi",
        "given_name": "Reinaldo Augusto"
      },
      {
        "surname": "Costa",
        "given_name": "Anna Helena Reali"
      }
    ]
  },
  {
    "title": "A novel data clustering algorithm based on gravity center methodology",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113435",
    "abstract": "The concept of clustering is to separate clusters based on the similarity which is greater within cluster than among clusters. The similarity consists of two principles, namely, connectivity and cohesion. However, in partitional clustering, while some algorithms such as K-means and K-medians divides the dataset points according to the first principle (connectivity) based on centroid clusters without any regard to the second principle (cohesion), some others like K-medoids partially consider cohesion in addition to connectivity. This prevents to discover clusters with convex shape and results are affected negatively by outliers. In this paper a new Gravity Center Clustering (GCC) algorithm is proposed which depends on critical distance (λ) to define threshold among clusters. The algorithm falls under partition clustering and is based on gravity center which is a point within cluster that verifies both the connectivity and cohesion in determining the similarity of each point in the dataset. Therefore, the proposed algorithm deals with any shape of data better than K-means, K-medians and K-medoids. Furthermore, GCC algorithm does not need any parameters beforehand to perform clustering but can help user improving the control over clustering results and deal with overlapping and outliers providing two coefficients and an indicator. In this study, 22 experiments are conducted using different types of synthetic, and real healthcare datasets. The results show that the proposed algorithm satisfies the concept of clustering and provides great flexibility to get the optimal solution especially since clustering is considered as an optimization problem.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420302591",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "CURE data clustering algorithm",
      "Centroid",
      "Cluster analysis",
      "Complete-linkage clustering",
      "Computer science",
      "Correlation clustering",
      "Data mining",
      "Fuzzy clustering",
      "Image (mathematics)",
      "Mathematics",
      "Medoid",
      "Outlier",
      "Similarity (geometry)",
      "Single-linkage clustering",
      "k-medians clustering"
    ],
    "authors": [
      {
        "surname": "Kuwil",
        "given_name": "Farag Hamed"
      },
      {
        "surname": "Atila",
        "given_name": "Ümit"
      },
      {
        "surname": "Abu-Issa",
        "given_name": "Radwan"
      },
      {
        "surname": "Murtagh",
        "given_name": "Fionn"
      }
    ]
  },
  {
    "title": "Fusing Transformed Deep and Shallow features (FTDS) for image-based facial expression recognition",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113459",
    "abstract": "In this paper, we propose combining between the transformed hand-crafted and deep features using PCA to recognize the six-basic facial expressions from static images. To evaluate our approach, we use three popular databases (CK+, CASIA and MMI). We introduce the use of the Pyramid Multi Level (PML) face representation for facial expression recognition. The hand-crafted features are obtained with such representations. Initially, we determine the optimal level of the PML features of three hand-crafted descriptors (HOG, LPQ and BSIF) using CK+, CASIA and MMI databases. After the optimal level of the PML is found for each descriptor, we combine them together with the transformed final VGG-face layers (FC6 and FC7) in order to get a compact image descriptor. In within-database experiments, our approach achieved higher accuracy than the state-of-art methods on both the CK+ and CASIA databases, and competitive result on the MMI database. Likewise, our approach outperformed the static methods in all six experiments of cross-databases.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420302839",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Expression (computer science)",
      "Face (sociological concept)",
      "Facial expression",
      "Facial recognition system",
      "Geometry",
      "Image (mathematics)",
      "Law",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Programming language",
      "Pyramid (geometry)",
      "Representation (politics)",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Bougourzi",
        "given_name": "F."
      },
      {
        "surname": "Dornaika",
        "given_name": "F."
      },
      {
        "surname": "Mokrani",
        "given_name": "K."
      },
      {
        "surname": "Taleb-Ahmed",
        "given_name": "A."
      },
      {
        "surname": "Ruichek",
        "given_name": "Y."
      }
    ]
  },
  {
    "title": "A deceptive review detection framework: Combination of coarse and fine-grained features",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113465",
    "abstract": "Electronic commerce has become a popular shopping mode. To enhance their reputations, attract more customers, and finally obtain more benefits, dishonest sellers often recruit buyers or robots to post a large number of deceptive reviews to mislead users. According to the interpretability of learning results, existing methods for detecting deceptive reviews can be mainly divided into explicit feature-based mining ones and neural network-based implicit feature mining ones. The nature of these works is accurate text classification based on coarse-grained features (e.g., topic, sentence, and document) or fine-grained features (e.g., word). To take full merits of existing approaches, this paper proposes a new framework that explores a method to combine the coarse-grained features and the fine-grained features. In this framework, the coarse-grained implicit semantic features of the topic distribution are learned by the concatenation of a Latent Dirichlet Allocation (LDA) topic model and a 2-layered neural network. The fine-grained implicit semantic features from the word vectors representation of the reviews are parallelly learned by a deep learning framework. Finally, these two granular features are combined and adopted to train a Support Vector Machine (SVM) classifier for detecting whether a review is deceptive or not. To verify the effectiveness and performance of this framework, we derive three models by specifying three popular deep learning models, such as TextCNN, long short-term memory (LSTM), and Bi-directional LSTM (BiLSTM) to learn the fine-grained features. Experimental results on a mixed-domain dataset and balanced/unbalanced in-domain datasets show that all the combination models are superior to the corresponding baseline models considering single features.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742030289X",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Deep learning",
      "Feature (linguistics)",
      "Feature learning",
      "Interpretability",
      "Latent Dirichlet allocation",
      "Linguistics",
      "Machine learning",
      "Natural language processing",
      "Philosophy",
      "Sentence",
      "Support vector machine",
      "Topic model"
    ],
    "authors": [
      {
        "surname": "Cao",
        "given_name": "Ning"
      },
      {
        "surname": "Ji",
        "given_name": "Shujuan"
      },
      {
        "surname": "Chiu",
        "given_name": "Dickson K.W."
      },
      {
        "surname": "He",
        "given_name": "Mingxiang"
      },
      {
        "surname": "Sun",
        "given_name": "Xiaohong"
      }
    ]
  },
  {
    "title": "Model checking intelligent avionics systems for test cases generation using multi-agent systems",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113458",
    "abstract": "The paper contributes by introducing a novel, formal and operational approach that addresses the open challenging issues of modeling, verifying, and testing intelligent critical avionics systems. We advance the state-of-the-art by unifying the three challenges and considering the intelligence, autonomy, and accountability of the components as first citizen concepts. The proposed methodology is effectively applied to a real, practical and complex case study of intelligent avionics systems, namely the landing gear system and uses multi-agent systems to model each main component in the system as an intelligent agent. We also introduce the formalism of extended interpreted systems that supports intelligence, autonomy, communication, input and output actions, predicate conditions and post-conditions. The paper adopts the computation tree logic of conditional commitments to model communication among autonomous agents and trace its progress. The symbolic model checker of this logic is used to run the verification of the system model, encoded in an extended input language, against coverage criteria and properties. Furthermore, we introduce a new testing methodology that: 1) Follows a test-driven development approach; 2) performs unit testing, component testing, and system testing in each increment; and 3) uses model checking to generate automatically counterexamples and witness traces interpreted into concrete test suites that achieve new coverage criteria. The experimental results showed the efficiency and scalability of the developed approach against a transformation-based technique. Finally, the computational complexity of the developed approach is analysed.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420302827",
    "keywords": [
      "Avionics",
      "Component (thermodynamics)",
      "Composite material",
      "Computer science",
      "Database",
      "Distributed computing",
      "Machine learning",
      "Materials science",
      "Model checking",
      "Model-based testing",
      "Physics",
      "Predicate abstraction",
      "Regression analysis",
      "Scalability",
      "Test case",
      "Theoretical computer science",
      "Thermodynamics"
    ],
    "authors": [
      {
        "surname": "Elkholy",
        "given_name": "Warda"
      },
      {
        "surname": "El-Menshawy",
        "given_name": "Mohamed"
      },
      {
        "surname": "Bentahar",
        "given_name": "Jamal"
      },
      {
        "surname": "Elqortobi",
        "given_name": "Mounia"
      },
      {
        "surname": "Laarej",
        "given_name": "Amine"
      },
      {
        "surname": "Dssouli",
        "given_name": "Rachida"
      }
    ]
  },
  {
    "title": "Evaluating the environmental protection strategy of a printed circuit board manufacturer using a Tw fuzzy importance performance analysis with Google Trends",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113483",
    "abstract": "Printed circuit boards (PCBs) are very important materials in consumer electronic products. The process of making PCBs usually uses chemicals, and large quantities of water are needed. PCB manufacturers are devoted to improving the process in order to conform to environmental protection rules and regulations. However, evaluating the performance of environmental protection strategies by quantitative methods is very difficult. Therefore, this study attempts to evaluate the environmental protection strategy of a PCB manufacturer using the novel weakest t-norm (Tw ) fuzzy importance-performance analysis with Google Trends (TFIPA-Google). The TFIPA-Google methodology obtain advantages of Tw operations, IPA, and Google Trends, which can handle uncertainty based on a fuzzy matrix of IPA, reduce fuzzy accumulation using the Tw operators, and analyze social media viewpoints using Google Trends. This empirical example based on the TFIPA-Google method shows that the recovered waste material management system should be a priority for PCB manufacturers. Moreover, the TFIPA-Google method can provide more creditable information, based on Tw operations and volume of Google Trends for decision-makers, than a conventional importance-performance analysis (IPA) model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420303079",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Business",
      "Computer science",
      "Engineering",
      "Finance",
      "Fuzzy logic",
      "Operating system",
      "Operations research",
      "Order (exchange)",
      "Printed circuit board",
      "Process (computing)",
      "Risk analysis (engineering)",
      "Viewpoints",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Kuen-Suan"
      },
      {
        "surname": "Lin",
        "given_name": "Kuo-Ping"
      },
      {
        "surname": "Lin",
        "given_name": "Li-Ju"
      }
    ]
  },
  {
    "title": "Aggregation of preference relations to enhance the ranking quality of collaborative filtering based group recommender system",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113476",
    "abstract": "The recommendation of suitable products/items for a group of users has always been a difficult task. Most of the recommender systems are designed for individual use only. However, there are many scenarios where the recommendations are intended to serve a group of users. Each member of the group has their own set of preferences, and it is challenging to satisfy each member of the group with the recommended list. It has also been observed in recent studies that mere aggregation of preferences (e.g., ratings) does not provide good group recommendations. The quality of the group recommendation depends on two essential things: the ranking quality and the aggregation strategy. The first one confirms that the higher preferred items always appear first in the list, and the second one confirms the agreement among users of the group towards the recommendation list. Hence, this study proposes a method that uses the preference relation based matrix factorization technique to obtain the predicted preference (e.g., ratings) and then uses graph aggregation strategy to aggregate the preferences of the group members. We applied collective rationality during graph aggregation to maintain consistency in preferences among group members. Three benchmark datasets were used to evaluate and compare the proposed model with other baselines in terms of ranking quality of the group recommendation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420303006",
    "keywords": [
      "Aggregate (composite)",
      "Aggregation problem",
      "Artificial intelligence",
      "Chemistry",
      "Collaborative filtering",
      "Combinatorics",
      "Composite material",
      "Computer science",
      "Consistency (knowledge bases)",
      "Epistemology",
      "Graph",
      "Group (periodic table)",
      "Information retrieval",
      "Materials science",
      "Mathematical economics",
      "Mathematics",
      "Organic chemistry",
      "Philosophy",
      "Preference",
      "Preference relation",
      "Quality (philosophy)",
      "Ranking (information retrieval)",
      "Recommender system",
      "Statistics",
      "Theoretical computer science",
      "Transitive relation"
    ],
    "authors": [
      {
        "surname": "Pujahari",
        "given_name": "Abinash"
      },
      {
        "surname": "Sisodia",
        "given_name": "Dilip Singh"
      }
    ]
  },
  {
    "title": "Stock market movement forecast: A Systematic review",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113464",
    "abstract": "Achieving accurate stock market models can provide investors with tools for making better data-based decisions. These models can help traders to reduce investment risk and select the most profitable stocks. Furthermore, creating advanced models enable the usage of non-traditional data like historical stock prices and news. There are several review articles about financial problems, including stock market analysis and forecast, currency exchange forecast, optimal portfolio selection, among others. However, the recent advances in machine learning techniques, like Deep Learning, Text Mining Techniques, and Ensemble Techniques, raises the need to perform an updated review. This study aims to fill this gap by providing an updated systematic review of the forecasting techniques used in the stock market, including their classification, characterization and comparison. The review is focused on studies on stock market movement prediction from 2014 to 2018, obtained from the scientific databases Scopus and Web of Science. Besides, it analyzes surveys and other reviews of recent studies published in the same time frame and the same databases.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420302888",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Econometrics",
      "Economics",
      "Engineering",
      "Finance",
      "Financial economics",
      "Horse",
      "Law",
      "MEDLINE",
      "Machine learning",
      "Mechanical engineering",
      "Paleontology",
      "Political science",
      "Portfolio",
      "Scopus",
      "Stock (firearms)",
      "Stock exchange",
      "Stock market",
      "Stock market prediction",
      "Systematic review",
      "Trading strategy"
    ],
    "authors": [
      {
        "surname": "Bustos",
        "given_name": "O"
      },
      {
        "surname": "Pomares-Quimbaya",
        "given_name": "A."
      }
    ]
  },
  {
    "title": "Refining understanding of corporate failure through a topological data analysis mapping of Altman’s Z-score model",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113475",
    "abstract": "Corporate failure resonates widely, leaving practitioners searching for understanding of default risk. Managers seek to steer away from trouble, credit providers to avoid risky loans and investors to mitigate losses. Applying Topological Data Analysis tools, this paper explores whether failing firms from the United States organise neatly along the five predictors of default proposed by the Z-score models. Each firm is represented as a point in a five-dimensional point cloud, each dimension being one of the five predictors. Visualising that cloud using Ball Mapper reveals failing firms are not always located in similar regions of the point cloud, that is they are not concentrated in an easily split out area of the space. As new modelling approaches vie to better predict firm failure, often using black boxes to deliver potentially over-fitting models, a timely reminder is sounded on the importance of evidencing the identification process. Value is added to the understanding of where in the parameter space failure occurs, and how firms might act to move away from financial distress. Further, lenders may find opportunity amongst subsets of firms that are traditionally considered to be in danger of bankruptcy, but which the Ball Mapper plots developed herein clarify actually sit in characteristic spaces where failure has not occurred.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420302992",
    "keywords": [
      "Actuarial science",
      "Algorithm",
      "Artificial intelligence",
      "Bankruptcy",
      "Biology",
      "Botany",
      "Business",
      "Cloud computing",
      "Computer science",
      "Finance",
      "Financial distress",
      "Financial system",
      "Geometry",
      "Identification (biology)",
      "Mathematics",
      "Operating system",
      "Point (geometry)",
      "Point cloud",
      "Space (punctuation)",
      "Topological data analysis"
    ],
    "authors": [
      {
        "surname": "Qiu",
        "given_name": "Wanling"
      },
      {
        "surname": "Rudkin",
        "given_name": "Simon"
      },
      {
        "surname": "Dłotko",
        "given_name": "Paweł"
      }
    ]
  },
  {
    "title": "Financial portfolio optimization with online deep reinforcement learning and restricted stacked autoencoder—DeepBreath",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113456",
    "abstract": "The process of continuously reallocating funds into financial assets, aiming to increase the expected return of investment and minimizing the risk, is known as portfolio management. In this paper, a portfolio management framework is developed based on a deep reinforcement learning framework called DeepBreath. The DeepBreath methodology combines a restricted stacked autoencoder and a convolutional neural network (CNN) into an integrated framework. The restricted stacked autoencoder is employed in order to conduct dimensionality reduction and features selection, thus ensuring that only the most informative abstract features are retained. The CNN is used to learn and enforce the investment policy which consists of reallocating the various assets in order to increase the expected return on investment. The framework consists of both offline and online learning strategies: the former is required to train the CNN while the latter handles concept drifts i.e. a change in the data distribution resulting from unforeseen circumstances. These are based on passive concept drift detection and online stochastic batching. Settlement risk may occur as a result of a delay in between the acquisition of an asset and its payment failing to deliver the terms of a contract. In order to tackle this challenging issue, a blockchain is employed. Finally, the performance of the DeepBreath framework is tested with four test sets over three distinct investment periods. The results show that the return of investment achieved by our approach outperforms current expert investment strategies while minimizing the market risk.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420302803",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Autoencoder",
      "Business",
      "Computer science",
      "Convolutional neural network",
      "Economics",
      "Finance",
      "Investment (military)",
      "Investment performance",
      "Law",
      "Machine learning",
      "Microeconomics",
      "Political science",
      "Politics",
      "Portfolio",
      "Production (economics)",
      "Reinforcement learning",
      "Return on investment"
    ],
    "authors": [
      {
        "surname": "Soleymani",
        "given_name": "Farzan"
      },
      {
        "surname": "Paquet",
        "given_name": "Eric"
      }
    ]
  },
  {
    "title": "An event-driven behavior trees extension to facilitate non-player multi-agent coordination in video games",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113457",
    "abstract": "In this paper, we extend behavior trees (BTs), a behavior creation method that is popular in the video game industry, with three new types of nodes that facilitate the design and implementation of non-player characters (NPCs) that need to coordinate with each other. We provide an implementation and a methodology to use the coordination nodes of our extension appropriately, and we show how to use them to develop an application scenario. In the last years, coordination in multi-agent systems has been a very active research field, both from theoretical and practical points of view. Something similar has happened with the development of new tools for the video game industry. Our approach contributes to both areas by providing a novel extension that facilitates the design and implementation of agents that need to coordinate with each other. In video games, agents or NPCs are—as their name implies—characters that are not controlled by the player but by the game through an algorithmic, predetermined, or responsive behavior, or a more sophisticated AI technique. Some video games require NPCs with dynamic, credible, and intelligently unpredictable behaviors to keep players engaged and immersed. Instead of endowing NPCs with very complex individual behaviors, a feasible way to improve their unpredictability in an intelligent and credible manner is allowing them to coordinate with each other. Since BTs focus on the creation of individual behaviors, coordinated behaviors nowadays tend to be achieved by hard-coding the coordination itself. However, that ad hoc solution partially drives away some of the benefits that popularized BTs: Being visually intuitive, scalable, and reusable. For this reason, we propose an extension to BTs that developers can use to coordinate NPCs without going against the development paradigm: creating complex behaviors by designing an intuitive tree structure.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420302815",
    "keywords": [
      "Coding (social sciences)",
      "Computer science",
      "Extension (predicate logic)",
      "Field (mathematics)",
      "Focus (optics)",
      "Human–computer interaction",
      "Mathematics",
      "Multimedia",
      "Optics",
      "Physics",
      "Programming language",
      "Pure mathematics",
      "Statistics",
      "Video game"
    ],
    "authors": [
      {
        "surname": "Agis",
        "given_name": "Ramiro A."
      },
      {
        "surname": "Gottifredi",
        "given_name": "Sebastian"
      },
      {
        "surname": "García",
        "given_name": "Alejandro J."
      }
    ]
  },
  {
    "title": "Importance-weighted conditional adversarial network for unsupervised domain adaptation",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113404",
    "abstract": "In the construction of expert and intelligent systems, annotating and curating large datasets is very expensive; hence, there is a need to transfer the knowledge from existing annotated datasets to unlabeled data. However, data that are relevant for a specific application usually differ from publicly available datasets because they are sampled from a different domain. Domain adaptation (DA) has emerged as an efficient technique to compensate for such a domain shift. Recent studies have suggested that deep adversarial networks can achieve promising results for DA problems. However, existing adversarial DA methods assign equal importance to different examples and ignore the effect of difference in source domain samples or noise on adversarial performance. Moreover, most DA methods only focus on reducing the distribution difference, but not to learn a good target domain model. To address these issues, we propose an importance-weighted conditional adversarial (IWCA) network for unsupervised DA. In this study, an importance criterion based on domain similarity and prediction certainty is proposed to assign weights to different samples, which can reduce the harmful effects of difficult-to-transfer samples when reducing their cross-domain class conditional distribution differences. Furthermore, a sample selection criterion derived from the perspective of transfer cross validation is used to progressively select appropriate pseudo-labeled target samples to fine-tune the target model. These two criteria work in an EM-like manner that alternating align class conditional distribution for weighted samples and progressively select certain pseudo-labeled target samples to fine-tune the joint model. In this manner, the network will gradually generate features that approximate the actual conditional distribution of the target domain. The results of extensive experiments conducted on four datasets show that IWCA outperforms several state-of-the-art deep DA methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420302281",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Conditional probability distribution",
      "Data mining",
      "Domain (mathematical analysis)",
      "Focus (optics)",
      "Image (mathematics)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Similarity (geometry)",
      "Statistics",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Peng"
      },
      {
        "surname": "Xiao",
        "given_name": "Ting"
      },
      {
        "surname": "Fan",
        "given_name": "Cangning"
      },
      {
        "surname": "Zhao",
        "given_name": "Wei"
      },
      {
        "surname": "Tang",
        "given_name": "Xianglong"
      },
      {
        "surname": "Liu",
        "given_name": "Hongwei"
      }
    ]
  },
  {
    "title": "A physics-informed Run-to-Run control framework for semiconductor manufacturing",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113424",
    "abstract": "For decades, Run-to-Run (R2R) controllers have been widely implemented in semiconductor manufacturing. They operate over key process parameters on the basis of the metrological measurements acquired from the process and their deviations from the target setpoints. Conventionally, R2R controllers have been implemented independently of the actual equipment condition, which is obviously affecting the process stability and performance. Therefore, both equipment signals and process states shall be considered to make the R2R controllers more robust to the equipment condition drifts. In this paper, we propose a novel physics-informed framework to integrate the real-time equipment condition, based on the Fault Detection and Classification (FDC) data, into the R2R controllers. By utilizing Dynamic Bayesian Networks (DBN), the implicit relationship structure between metrology measurements, FDC indicators, and R2R regulators can be learned and reviewed explicitly. The structure shall be further reviewed to valid with the existing relationships and expert knowledge. Infeasible causalities on the structure will be constrained via setting up the blacklist at the structure learning stage. The proposed framework consists of the offline modeling stage, which incorporates the process, equipment variables, and the expert knowledge in the structure learning, and the online control stage, which constructs the Structured R2R controller (SRC) based on the relationship structure. As a result, the model is consistent by design with empirically known relationships and fundamental physical laws. The proposed SRC not only optimizes the operation with respect to the target control values but also considers the equipment and process states simultaneously. The effectiveness of SRC and the derivative control strategy are validated through a real dataset of a Chemical-Mechanical Polishing (CMP) process, and two simulated studies.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420302487",
    "keywords": [
      "Agronomy",
      "Artificial intelligence",
      "Bayesian probability",
      "Biology",
      "Computer science",
      "Computer security",
      "Control engineering",
      "Controller (irrigation)",
      "Dynamic Bayesian network",
      "Electrical engineering",
      "Engineering",
      "Industrial engineering",
      "Key (lock)",
      "Machine learning",
      "Operating system",
      "Process (computing)",
      "Reliability engineering",
      "Semiconductor device fabrication",
      "Wafer"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Wei-Ting"
      },
      {
        "surname": "Blue",
        "given_name": "Jakey"
      },
      {
        "surname": "Roussy",
        "given_name": "Agnès"
      },
      {
        "surname": "Pinaton",
        "given_name": "Jacques"
      },
      {
        "surname": "Reis",
        "given_name": "Marco S."
      }
    ]
  },
  {
    "title": "A data analytic framework for physical fatigue management using wearable sensors",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113405",
    "abstract": "The use of expert systems in optimizing and transforming human performance has been limited in practice due to the lack of understanding of how an individual’s performance deteriorates with fatigue accumulation, which can vary based on both the worker and the workplace conditions. As a first step toward realizing the human-centered approach to artificial intelligence and expert systems, this paper lays the foundation for a data analytic approach to managing fatigue in physically-demanding workplaces. The proposed framework capitalizes on continuously collected human performance data from wearable sensor technologies, and is centered around four distinct phases of fatigue: (a) detection, where machine learning methodologies are deployed to detect the occurrence of fatigue; (b) identification, where key features relating to the fatigue occurrence is to be identified; (c) diagnosis, where the fatigue mode is identified based on the knowledge generated in the previous two phases; and (d) recovery, where a suitable intervention is applied to return the worker to mitigate the detrimental effects of fatigue on the worker. Moreover, the framework establishes criteria for feature and machine learning algorithm selection for fatigue management. Two specific application cases of the framework, for two types of manufacturing-related tasks, are presented. Based on the proposed framework and a large number of test sets used in the two case studies, we have shown that: (i) only one wearable sensor is needed for fatigue detection with an average accuracy of ≥ 0.850 and a random forest model comprised of < 7 features; and (ii) the selected features are task-dependent, and thus capturing different modes of fatigue. Therefore, this research presents an important foundation for future expert systems that attempt to quantify/predict changes in workers’ performance as an input to prescriptive rest-break scheduling, job-rotation, and task assignment models. To encourage future work in this important area, we provide links to our data and code as Supplementary materials.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420302293",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Computer science",
      "Embedded system",
      "Engineering",
      "Identification (biology)",
      "Machine learning",
      "Random forest",
      "Systems engineering",
      "Task (project management)",
      "Wearable computer",
      "Wearable technology"
    ],
    "authors": [
      {
        "surname": "Sedighi Maman",
        "given_name": "Zahra"
      },
      {
        "surname": "Chen",
        "given_name": "Ying-Ju"
      },
      {
        "surname": "Baghdadi",
        "given_name": "Amir"
      },
      {
        "surname": "Lombardo",
        "given_name": "Seamus"
      },
      {
        "surname": "Cavuoto",
        "given_name": "Lora A."
      },
      {
        "surname": "Megahed",
        "given_name": "Fadel M."
      }
    ]
  },
  {
    "title": "The estimation of low and high-pass active filter parameters with opposite charged system search algorithm",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113474",
    "abstract": "Algorithms are frequently used to solve problems that have a large search space and take a long time to be mathematically solved. They can later be improved with different improvement methods based on the structure and the type of the problem. In this study, the charged system search algorithm (CSS), which has been successfully implemented in the solutions of numerous engineering problems studied within the literature, was improved by introducing opposition-based learning method (OBL) to it in two different methods. With these improved algorithms, solutions were developed for 30-dimensional multimodal test functions in the first place and the results were discussed. In the second place, the parameters of active filters given below were determined from E24 standard series with developed approaches. Filters are electronic circuits that enhance the wanted frequency components of electric signals applied on their inputs and remove harmonics and interferences from these signals. They are divided into two types; active and passive filters. Active filters are produced with transistors or op-amps. They are financially more advantageous compared to passive filters. These filters are preferred especially in low frequencies due to their low costs. Adjustable for a large frequency domain, active filters are very convenient in terms of size and weight and their designs are highly simple. They can easily be connected successively without affecting one another. In this study, the parameter values of Sallen-Key topology Butterworth low and high-pass active filters, which have an extensive area of use, were determined through improved algorithms. My suggestions for the future studies are these: the effects of the opposite position learning approach on other heuristic algorithms can be analysed solving test functions and different engineering problems; different approaches other than the two approaches proposed in this study, can be developed for the opposite position learning concept; LPF and HPF designs solved in the study can be solved for different degrees and stages and finally new designs can be done for different filter types and different resistor and capacitor series that haven’t been handled in this study.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420302980",
    "keywords": [
      "Active filter",
      "Algorithm",
      "Computer science",
      "Computer vision",
      "Electrical engineering",
      "Electronic circuit",
      "Electronic filter",
      "Engineering",
      "Filter (signal processing)",
      "Harmonics",
      "Low-pass filter",
      "Prototype filter",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Temurtaş",
        "given_name": "Hasan"
      }
    ]
  },
  {
    "title": "No free lunch but a cheaper supper: A general framework for streaming anomaly detection",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113453",
    "abstract": "In recent years, research interest in detecting anomalies in temporal streaming data has increased significantly. A variety of algorithms are being developed in the data mining community. They can be broadly divided into two categories, namely general-purpose and ad hoc ones. In most cases, general approaches assume a one-size-fits-all solution model, and strive to design a single “optimal” anomaly detector which can detect all anomalies in any domain. To date, there exists no universal method that has been shown to outperform the others across different anomaly types, use cases and datasets. In this paper, we propose SAFARI, a framework created by abstracting and unifying the fundamental tasks within the streaming anomaly detection. SAFARI provides a flexible and extensible anomaly detection procedure to overcome the limitations of one-size-fits-all solutions. Such abstraction helps to facilitate more elaborate algorithm comparisons by allowing us to isolate the effects of shared and unique characteristics of diverse algorithms on the performance. Using the framework, we have identified a research gap that motivated us to propose a novel learning strategy. We implemented twenty different anomaly detectors and conducted an extensive evaluation study, comparing their performances using real-world benchmark datasets with different properties. The results indicate that there is no single superior detector which works perfectly for every case, proving our hypothesis that “there is no free lunch” in the streaming anomaly detection world. Finally, we discuss the benefits and drawbacks of each method in-depth, drawing a set of conclusions and guidelines to guide future users of SAFARI.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420302773",
    "keywords": [
      "Anomaly (physics)",
      "Anomaly detection",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Condensed matter physics",
      "Data mining",
      "Detector",
      "Domain (mathematical analysis)",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Physics",
      "Programming language",
      "Set (abstract data type)",
      "Streaming data",
      "Telecommunications",
      "Variety (cybernetics)"
    ],
    "authors": [
      {
        "surname": "Calikus",
        "given_name": "Ece"
      },
      {
        "surname": "Nowaczyk",
        "given_name": "Sławomir"
      },
      {
        "surname": "Sant’Anna",
        "given_name": "Anita"
      },
      {
        "surname": "Dikmen",
        "given_name": "Onur"
      }
    ]
  },
  {
    "title": "An efficient Harris hawks-inspired image segmentation method",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113428",
    "abstract": "Segmentation is a crucial phase in image processing because it simplifies the representation of an image and facilitates its analysis. The multilevel thresholding method is more efficient for segmenting digital mammograms compared to the classic bi-level thresholding since it uses a higher number of intensities to represent different regions in the image. In the literature, there are different techniques for multilevel segmentation; however, most of these approaches do not obtain good segmented images. In addition, they are computationally expensive. Recently, statistical criteria such as Otsu, Kapur, and cross-entropy have been utilized in combination with evolutionary and swarm-based strategies to investigate the optimal threshold values for multilevel segmentation. In this paper, an efficient methodology for multilevel segmentation is proposed using the Harris Hawks Optimization (HHO) algorithm and the minimum cross-entropy as a fitness function. To substantiate the results and effectiveness of the HHO-based method, it has been tested over a benchmark set of reference images, with the Berkeley segmentation database, and with medical images of digital mammography. The proposed HHO-based solver is verified based on other comparable optimizers and two machine learning algorithms K-means and the Fuzzy IterAg. The comparisons were performed based on three groups. This first one is to provide evidence of the optimization capabilities of the HHO using the Wilcoxon test, and the second is to verify segmented image quality using the PSNR, SSIM, and FSIM metrics. Then, the third way is to verify the segmented image comparing it with the ground-truth through the metrics PRI, GCE, and VoI. The experimental results, which are validated by statistical analysis, show that the introduced method produces efficient and reliable results in terms of quality, consistency, and accuracy in comparison with the other methods. This HHO-based method presents an improvement over other segmentation approaches that are currently used in the literature.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420302529",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Entropy (arrow of time)",
      "Image (mathematics)",
      "Image segmentation",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Scale-space segmentation",
      "Segmentation",
      "Thresholding"
    ],
    "authors": [
      {
        "surname": "Rodríguez-Esparza",
        "given_name": "Erick"
      },
      {
        "surname": "Zanella-Calzada",
        "given_name": "Laura A."
      },
      {
        "surname": "Oliva",
        "given_name": "Diego"
      },
      {
        "surname": "Heidari",
        "given_name": "Ali Asghar"
      },
      {
        "surname": "Zaldivar",
        "given_name": "Daniel"
      },
      {
        "surname": "Pérez-Cisneros",
        "given_name": "Marco"
      },
      {
        "surname": "Foong",
        "given_name": "Loke Kok"
      }
    ]
  },
  {
    "title": "A survey of blockchain consensus algorithms performance evaluation criteria",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113385",
    "abstract": "How to reach an agreement in a blockchain network is a complex and important task that is defined as a consensus problem and has wide applications in reality including distributed computing, load balancing, and transaction validation in blockchains. Over recent years, many studies have been done to cope with this problem. In this paper, a comparative and analytical review on the state-of-the-art blockchain consensus algorithms is presented to enlighten the strengths and constraints of each algorithm. Based on their inherent specifications, each algorithm has a different domain of applicability that yields to propose several performance criteria for the evaluation of these algorithms. To overview and provide a basis of comparison for further work in the field, a set of incommensurable and conflicting performance evaluation criteria is identified and weighted by the pairwise comparison method. These criteria are classified into four categories including algorithms’ throughput, the profitability of mining, degree of decentralization and consensus algorithms vulnerabilities and security issues. Based on the proposed framework, the pros and cons of consensus algorithms are systematically analyzed and compared in order to provide a deep understanding of the existing research challenges and clarify the future study directions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420302098",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Blockchain",
      "Computer science",
      "Computer security",
      "Consensus algorithm",
      "Data mining",
      "Database",
      "Database transaction",
      "Domain (mathematical analysis)",
      "Field (mathematics)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pairwise comparison",
      "Programming language",
      "Pure mathematics",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Bamakan",
        "given_name": "Seyed Mojtaba Hosseini"
      },
      {
        "surname": "Motavali",
        "given_name": "Amirhossein"
      },
      {
        "surname": "Babaei Bondarti",
        "given_name": "Alireza"
      }
    ]
  },
  {
    "title": "Swarm optimized cluster based framework for information retrieval",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113441",
    "abstract": "This work explores the integrated power of swarm intelligence and advances in data mining techniques to solve the information retrieval (IR) problem of rapidly growing digital content on the World Wide Web. We propose a swarm optimized cluster based framework with frequent pattern mining techniques to retrieve user-specific knowledge from extensive document collections. In the pre-processing phase, we split the task into two sub-tasks. The first is to decompose the document collection into groups using a bio-inspired K-Flock clustering algorithm, while the second extracts frequent patterns from each cluster using a memory-efficient Recursive Elimination (RElim) algorithm. In the next phase, we implement a cosine similarity based probabilistic model to retrieve query-specific documents from clusters based on the matching scores between the closed frequent patterns of queries and clusters. The performance of a system is evaluated by conducting several experiments which are carried out on five well-known, diverse and variable size datasets viz- TREC 2014-15 CDS (Clinical Decision Support) datasets containing 733,138 records, OHSUMED dataset with 348,566 records from Medline database, NPL dataset with 11,429 records, LISA document collection of 6004 records, CACM (Collection of ACM) dataset of 3204 records. The results show that the proposed IR framework significantly outperforms the traditional sequential IR approach and other state-of-the-art IR approaches, both in terms of the quality of the returned documents and the time of execution.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420302657",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Cosine similarity",
      "Data mining",
      "Database",
      "Economics",
      "Information retrieval",
      "Management",
      "Swarm behaviour",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Bhopale",
        "given_name": "Amol P."
      },
      {
        "surname": "Tiwari",
        "given_name": "Ashish"
      }
    ]
  },
  {
    "title": "An analysis of boosted ensembles of binary fuzzy decision trees",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113436",
    "abstract": "Classification is a functionality that plays a central role in the development of modern expert systems, across a wide variety of application fields: using accurate, efficient, and compact classification models is often a prime requirement. Boosting (and AdaBoost in particular) is a well-known technique to obtain robust classifiers from properly-learned weak classifiers, thus it is particularly attracting in many practical settings. Although the use of traditional classifiers as base learners in AdaBoost has already been widely studied, the adoption of fuzzy weak learners still requires further investigations. In this paper we describe FDT-Boost, a boosting approach shaped according to the SAMME-AdaBoost scheme, which leverages fuzzy binary decision trees as multi-class base classifiers. Such trees are kept compact by constraining their depth, without lowering the classification accuracy. The experimental evaluation of FDT-Boost has been carried out using a benchmark containing eighteen classification datasets. Comparing our approach with FURIA, one of the most popular fuzzy classifiers, with a fuzzy binary decision tree, and with a fuzzy multi-way decision tree, we show that FDT-Boost is accurate, getting to results that are statistically better than those achieved by the other approaches. Moreover, compared to a crisp SAMME-AdaBoost implementation, FDT-Boost shows similar performances, but the relative produced models are significantly less complex, thus opening up further exploitation chances also in memory-constrained systems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420302608",
    "keywords": [
      "Arithmetic",
      "Artificial intelligence",
      "Binary number",
      "Computer science",
      "Data mining",
      "Decision tree",
      "Fuzzy logic",
      "Machine learning",
      "Mathematics"
    ],
    "authors": [
      {
        "surname": "Barsacchi",
        "given_name": "Marco"
      },
      {
        "surname": "Bechini",
        "given_name": "Alessio"
      },
      {
        "surname": "Marcelloni",
        "given_name": "Francesco"
      }
    ]
  },
  {
    "title": "Interactive example-based finding of text items",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113403",
    "abstract": "We consider the problem of identifying within a given document all text items which follow a certain pattern to be specified by a user. In particular, we focus on scenarios in which the task is to be completed very quickly and the user is not able to specify the exact pattern of interest. The key use case corresponds to the interactive exploration of documents in search of snippets that do not fit Boolean, word-based search expressions. We propose an interactive framework in which the user provides examples of the items he is interested in, the system identifies items similar to those provided by the user and progressively refines the similarity criterion by submitting selected queries to the user, in an active learning fashion. The fact that the search is to be executed very quickly places severe requirements on the algorithms that can be used by the system, both for identifying the items and for constructing the queries. We propose and assess experimentally in detail a number of different design options for the components of the learning machinery. The results demonstrate the ability of our approach to achieve effectiveness close to state-of-the-art approaches based on regular expressions, while requiring an execution time which is orders of magnitude shorter.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742030227X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Economics",
      "Focus (optics)",
      "Image (mathematics)",
      "Information retrieval",
      "Key (lock)",
      "Linguistics",
      "Management",
      "Optics",
      "Philosophy",
      "Physics",
      "Similarity (geometry)",
      "Task (project management)",
      "Word (group theory)"
    ],
    "authors": [
      {
        "surname": "Medvet",
        "given_name": "Eric"
      },
      {
        "surname": "Bartoli",
        "given_name": "Alberto"
      },
      {
        "surname": "De Lorenzo",
        "given_name": "Andrea"
      },
      {
        "surname": "Tarlao",
        "given_name": "Fabiano"
      }
    ]
  },
  {
    "title": "A white-box analysis on the writer-independent dichotomy transformation applied to offline handwritten signature verification",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113397",
    "abstract": "High number of writers, small number of training samples per writer with high intra-class variability and heavily imbalanced class distributions are among the challenges and difficulties of the offline Handwritten Signature Verification (HSV) problem. A good alternative to tackle these issues is to use a writer-independent (WI) framework. In WI systems, a single model is trained to perform signature verification for all writers from a dissimilarity space generated by the dichotomy transformation. Among the advantages of this framework is its scalability to deal with some of these challenges and its ease in managing new writers, and hence of being used in a transfer learning context. In this work, we present a white-box analysis of this approach highlighting how it handles the challenges, the dynamic selection of references through fusion function, and its application for transfer learning. All the analyses are carried out at the instance level using the instance hardness (IH) measure. The experimental results show that, using the IH analysis, we were able to characterize “good” and “bad” quality skilled forgeries as well as the frontier region between positive and negative samples. This enables futures investigations on methods for improving discrimination between genuine signatures and skilled forgeries by considering these characterizations.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420302219",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Biology",
      "Chemistry",
      "Class (philosophy)",
      "Computer science",
      "Context (archaeology)",
      "Database",
      "Gene",
      "Geometry",
      "Machine learning",
      "Mathematics",
      "Natural language processing",
      "Paleontology",
      "Scalability",
      "Signature (topology)",
      "Transformation (genetics)"
    ],
    "authors": [
      {
        "surname": "Souza",
        "given_name": "Victor L.F."
      },
      {
        "surname": "Oliveira",
        "given_name": "Adriano L.I."
      },
      {
        "surname": "Cruz",
        "given_name": "Rafael M.O."
      },
      {
        "surname": "Sabourin",
        "given_name": "Robert"
      }
    ]
  },
  {
    "title": "Integration and comparison of multi-criteria decision making methods in safe route planner",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113399",
    "abstract": "Motor vehicle crashes are a leading cause of death in the U.S. In order to reduce death and serious injury, road and traffic engineers manually evaluate road segments and visualize the safety level of roads. These existing risk maps can be confusing and must be manually interpreted by drivers to find the safest path from a source to a given destination; this can result in ignoring the safety of the routes by drivers. In addition, common navigation systems such as Google Maps and Waze present two or three alternative paths from a source to a given destination based on the travel time and distance. A navigation system is required to take the safety level of the road segments into consideration while suggesting a path. This navigation system needs to acquire knowledge from various sources, a user interface to obtain user preferences, and an inference engine to find the best paths. Such a system can still suggest multiple conflicting paths, such as shortest, fastest and safest paths. This paper presents the addition of a multi-criteria decision-making (MCDM) method, Analytical Hierarchy Process, to a previously designed Safe Route Planner to aid users in choosing the most suitable path among M alternative paths. Different MCDM methods can generate different results while applied to the same problem. There are a few comparative studies to compare the results of different Multi-Criteria Decision-Making (MCDM) methods. Therefore, a particular attention is devoted to comparing the results of five decision-making techniques, namely AHP, Fuzzy AHP, TOPSIS, Fuzzy TOPSIS and PROMETHEE through two real-world case studies. In addition, the comparative studies fail to adequately quantify the results of the MCDM methods; consequently, another aim of this research is to investigate the applicability of Spearman's rank correlation coefficient, Average Overlap and Discounted Cumulative Gain techniques to quantify the results of the MCDM methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420302232",
    "keywords": [
      "Analytic hierarchy process",
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Engineering",
      "Fuzzy logic",
      "Graph",
      "Medicine",
      "Multiple-criteria decision analysis",
      "Operating system",
      "Operations research",
      "Path (computing)",
      "Planner",
      "Process (computing)",
      "Programming language",
      "Risk analysis (engineering)",
      "Shortest path problem",
      "TOPSIS",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Sarraf",
        "given_name": "Reza"
      },
      {
        "surname": "McGuire",
        "given_name": "Michael P."
      }
    ]
  },
  {
    "title": "An efficient double adaptive random spare reinforced whale optimization algorithm",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113018",
    "abstract": "Whale optimization algorithm (WOA) is a newly developed meta-heuristic algorithm, which is mainly based on the predation behavior of humpback whales in the ocean. In this paper, a reinforced variant called RDWOA is proposed to alleviate the central shortcomings of the original method that converges slowly, and it is easy to fall into local optimum when dealing with multi-dimensional problems. Two strategies are introduced into the original WOA. One is the strategy of random spare or random replacement to enhance the convergence speed of this algorithm. The other method is the strategy of double adaptive weight, which is introduced to improve the exploratory searching trends during the early stages and exploitative behaviors in the later stages. The combination of the two strategies significantly improves the convergence speed and the overall search ability of the algorithm. The advantages of the proposed RDWOA are deeply analyzed and studied by using typical benchmark examples such as unimodal, multi-modal, and fixed multi-modal functions, and three famous engineering design problems. The experimental results show that the exploratory and exploitative tendencies of WOA and its convergence mode have been significantly improved. The RDWOA developed in this paper is a promising improved WOA variant, and it has better efficacy compared to other state-of-the-art algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419307353",
    "keywords": [
      "Adaptive strategies",
      "Algorithm",
      "Archaeology",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Chemistry",
      "Computer science",
      "Convergence (economics)",
      "Economic growth",
      "Economics",
      "Engineering",
      "Fishery",
      "Geodesy",
      "Geography",
      "Heuristic",
      "History",
      "Mathematical optimization",
      "Mathematics",
      "Modal",
      "Mode (computer interface)",
      "Operating system",
      "Operations management",
      "Polymer chemistry",
      "Spare part",
      "Whale"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Huiling"
      },
      {
        "surname": "Yang",
        "given_name": "Chenjun"
      },
      {
        "surname": "Heidari",
        "given_name": "Ali Asghar"
      },
      {
        "surname": "Zhao",
        "given_name": "Xuehua"
      }
    ]
  },
  {
    "title": "Retraction notice to “New Hybrid Methodology for Stock Volatility Prediction” [Expert Systems with Applications 36/2P1 (2008) 1833--1839]",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113480",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420303043",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Econometrics",
      "Law",
      "Machine learning",
      "Mathematics",
      "Notice",
      "Political science",
      "Volatility (finance)"
    ],
    "authors": [
      {
        "surname": "Tseng",
        "given_name": "Chih-Hsiung"
      },
      {
        "surname": "Cheng",
        "given_name": "Sheng-Tzong"
      },
      {
        "surname": "Wang",
        "given_name": "Yi-Hsien"
      }
    ]
  },
  {
    "title": "A modified Sine Cosine Algorithm with novel transition parameter and mutation operator for global optimization",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113395",
    "abstract": "Inspired by the mathematical characteristics of sine and cosine trigonometric functions, the Sine Cosine Algorithm (SCA) has shown competitive performance among other meta-heuristic algorithms. However, despite its sufficient global search ability, its low exploitation ability and immature balance between exploitation and exploration remain weaknesses. In order to improve Sine Cosine Algorithm (SCA), this paper presents a modified version of the SCA called MSCA. Firstly, a non-linear transition rule is introduced instead of a linear transition to provide comparatively better transition from the exploration to exploitation. Secondly, the classical search equation of the SCA is modified by introducing the leading guidance based on the elite candidate solution. When the above proposed modified search mechanism fails to provide a better solution, in addition, a mutation operator is used to generate a new position to avoid the situation of getting trapped in locally optimal solutions during the search. Thus, the MSCA effectively maximizes the advantages of proposed strategies in maintaining a comparatively better balance of exploration and exploitation as compared to the classical SCA. The validity of the MSCA is tested on a set of 33 benchmark optimization problems and employed for training multilayer perceptrons. The numerical results and comparisons among several algorithms show the enhanced search efficiency of the MSCA.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420302190",
    "keywords": [
      "Algorithm",
      "Benchmark (surveying)",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Gene",
      "Geodesy",
      "Geography",
      "Geometry",
      "Heuristic",
      "Local search (optimization)",
      "Mathematical optimization",
      "Mathematics",
      "Operator (biology)",
      "Programming language",
      "Repressor",
      "Set (abstract data type)",
      "Sine",
      "Transcription factor",
      "Trigonometric functions"
    ],
    "authors": [
      {
        "surname": "Gupta",
        "given_name": "Shubham"
      },
      {
        "surname": "Deep",
        "given_name": "Kusum"
      },
      {
        "surname": "Mirjalili",
        "given_name": "Seyedali"
      },
      {
        "surname": "Kim",
        "given_name": "Joong Hoon"
      }
    ]
  },
  {
    "title": "A model for sector restructuring through genetic algorithm and inverse DEA",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113422",
    "abstract": "The aim of this study is to devise a sector restructuring model in which all the decision making units (DMUs) satisfy a predefined global efficiency level. The proposal makes several realistic assumptions regarding the merging of DMUs under specific circumstances. The model computes the global efficiency target by giving preference to merging DMUs over saving inputs, hence considering that the affected stakeholders may be resistant to restructuring, and this resistance may have overall negative effects on the image and reputation of the companies and organizations. In addition, the number of constituents in the new entities can be limited by the decision maker after the restructuring process, so that the model also considers a constraint on cardinality. The proposal combines the inverse data envelopment analysis (InvDEA), which computes the merger’s input savings, and the genetic algorithm (GA), which solves the combinatorial problem of identifying the merging units. The proposal is illustrated by two examples from banking and higher education.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420302463",
    "keywords": [
      "Algorithm",
      "Cardinality (data modeling)",
      "Computer science",
      "Constraint (computer-aided design)",
      "Data envelopment analysis",
      "Data mining",
      "Economics",
      "Finance",
      "Genetic algorithm",
      "Geometry",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Microeconomics",
      "Operating system",
      "Operations research",
      "Preference",
      "Process (computing)",
      "Reputation",
      "Restructuring",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Guijarro",
        "given_name": "Francisco"
      },
      {
        "surname": "Martínez-Gómez",
        "given_name": "Mónica"
      },
      {
        "surname": "Visbal-Cadavid",
        "given_name": "Delimiro"
      }
    ]
  },
  {
    "title": "Measuring employee-tourist encounter experience value: A big data analytics approach",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113450",
    "abstract": "This paper takes a text analytics approach to measuring dimensions of employee-visitor encounters that impact on visitor outcomes. A conceptual model measuring dimensions of employee-tourist encounters is implemented using a big data analytics approach more suited to large-scale online review data than the traditional, limited survey approach. Using a dictionary-based measurement approach and a large sample of reviews for hotels (n = 265,016), we test the model and the importance of the factors for leveraging perceptions of satisfaction, service and value. The results demonstrate the importance of the different dimensions of experiential value in employee-tourist encounters in creating positive tourist perceptions. This knowledge is crucial for tourism companies aiming to create experiential value for visitors, rather than simply delivering service quality.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420302748",
    "keywords": [
      "Analytics",
      "Archaeology",
      "Big data",
      "Business",
      "Cartography",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Data mining",
      "Data science",
      "Experiential learning",
      "Geography",
      "Knowledge management",
      "Machine learning",
      "Marketing",
      "Mathematics education",
      "Neuroscience",
      "Perception",
      "Programming language",
      "Psychology",
      "Sample (material)",
      "Scale (ratio)",
      "Service (business)",
      "Service quality",
      "Tourism",
      "Value (mathematics)",
      "Visitor pattern"
    ],
    "authors": [
      {
        "surname": "Barnes",
        "given_name": "Stuart J."
      },
      {
        "surname": "Mattsson",
        "given_name": "Jan"
      },
      {
        "surname": "Sørensen",
        "given_name": "Flemming"
      },
      {
        "surname": "Jensen",
        "given_name": "Jens Friis"
      }
    ]
  },
  {
    "title": "A conversational recommender system for diagnosis using fuzzy rules",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113449",
    "abstract": "Graded implications in the framework of Fuzzy Formal Concept Analysis are used as the knowledge guiding the recommendations. An automated engine based on fuzzy Simplification Logic is proposed to make the suggestions to the users. Conversational recommender systems have proven to be a good approach in telemedicine, building a dialogue between the user and the recommender based on user preferences provided at each step of the conversation. Here, we propose a conversational recommender system for medical diagnosis using fuzzy logic. Specifically, fuzzy implications in the framework of Formal Concept Analysis are used to store the knowledge about symptoms and diseases and Fuzzy Simplification Logic is selected as an appropriate engine to guide the conversation to a final diagnosis. The recommender system has been used to provide differential diagnosis between schizophrenia and schizoaffective and bipolar disorders. In addition, we have enriched the conversational strategy with two strategies (namely critiquing and elicitation mechanism) for a better understanding of the knowledge-driven conversation, allowing user’s feedback in each step of the conversation and improving the performance of the method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420302736",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Conversation",
      "Fuzzy logic",
      "Human–computer interaction",
      "Information retrieval",
      "Linguistics",
      "Machine learning",
      "Philosophy",
      "Recommender system"
    ],
    "authors": [
      {
        "surname": "Cordero",
        "given_name": "P."
      },
      {
        "surname": "Enciso",
        "given_name": "M."
      },
      {
        "surname": "López",
        "given_name": "D."
      },
      {
        "surname": "Mora",
        "given_name": "A."
      }
    ]
  },
  {
    "title": "Euclidean distance based feature ranking and subset selection for bearing fault diagnosis",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113400",
    "abstract": "Bearing failure can cause hazardous effects on rotating machinery. The diagnosis of the fault is very critical for reliable operation. The main steps for the machine learning process involve feature extraction, selection, and classification. Feature selection contains an identification of noble features that performs for better classification accuracy with fewer features and with less computational time. For a large feature dimension; a critical study is required to catch the best feature subset for proper diagnosis. So, this paper presents a unique feature ordering and selection technique called Feature Ranking and Subset Selection based on Euclidean distance (FRSSED). Two bearing databases have considered for verification of the robustness of the proposed technique. One database was obtained from the experiment, and the other publicly available database was collected from Case Western Reserve University (CWRU). Initially, the vibration signals have captured from bearings having an individual as well as combined defects in various components along with healthy bearing. EEMD was applied to these signals, and then, the sensitive IMF was selected by the envelope spectrum. In the later stage, the feature extraction was carried out from the selected IMF using fifteen statistical features. Afterward, the extracted features were introduced into FRSSED algorithm for feature ordering. These ordered features were fed into various classifiers. The comparison was made for classification accuracy and time consumption among generalized method (without feature ordering), principal component analysis (PCA), and FRSSED. The diagnostic outcomes describe that the suggested feature reduction technique improves the classification accuracy with fewer feature subset along with considerable time-saving.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420302244",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Dimensionality reduction",
      "Euclidean distance",
      "Feature (linguistics)",
      "Feature extraction",
      "Feature selection",
      "Gene",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Principal component analysis",
      "Ranking (information retrieval)",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Patel",
        "given_name": "Sachin P."
      },
      {
        "surname": "Upadhyay",
        "given_name": "S.H."
      }
    ]
  },
  {
    "title": "H2-SLAN: A hyper-heuristic based on stochastic learning automata network for obtaining, storing, and retrieving heuristic knowledge",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113426",
    "abstract": "Over the years, the meta-heuristics have been adapted and based on metaphors, whose proposals show effective solutions. Nevertheless, these abstractions are proving to be simple camouflage processes. This study presents a hyper-heuristic based on stochastic automata networks with learning, controlling a set of meta-heuristics. In this sense, this work investigates the moving through the search space performed by heuristic mechanisms, free of abstractions used by meta-heuristic. Or, in the conceptual terms proposed, we built up a hyper-heuristic model based on stochastic automata networks with learning, for selection and parameterization of low-level heuristics. The approach works on eight known meta-heuristic, exploiting features, and strengths of each algorithm. This identification allied to the theory of stochastic automata networks with learning guided the construction of their representations. These representations are consolidated in a meta-space, a part of the architecture of the hyper-heuristic proposed in work, named H2-SLAN Model. The results illustrate the effectiveness of our hyper-heuristic approach, regardless of heuristic composition, when compared to each meta-heuristic. Besides, hyper-heuristic performs better than individual meta-heuristics, thus significantly increasing optimization opportunities. As a result of this work, we got a system capable of selecting and parameterize low-level heuristics, with the ability to learn the heuristic movements employed by the model in the search space.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420302505",
    "keywords": [
      "Artificial intelligence",
      "Automaton",
      "Computer science",
      "Heuristic",
      "Heuristics",
      "Hyper-heuristic",
      "Learning automata",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Mobile robot",
      "Operating system",
      "Robot",
      "Robot learning",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Nesi",
        "given_name": "Luan Carlos"
      },
      {
        "surname": "Righi",
        "given_name": "Rodrigo da Rosa"
      }
    ]
  },
  {
    "title": "Hybrid flow shop with multiple servers: A computational evaluation and efficient divide-and-conquer heuristics",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113462",
    "abstract": "This paper focuses on the minimisation of the makespan in a hybrid flow shop layout with multiple servers and identical machines. Servers are renewable secondary resources responsible of executing the setup times of the jobs. Although the use of human supervision is very extensive in real manufacturing scenarios, its study in academia is still very scarce. In fact, to the best of our knowledge, the hybrid flow shop with servers has not been addressed in the literature so far. Hence, we first analyse the problem and identify a number of problem properties. By using these properties, we design two constructive heuristics based on a divide-and-conquer mechanism and four composite heuristics based on memory-based procedures and local search that use an efficient representation of the solutions. In addition, in order to picture the state-of-the-art of the most efficient heuristics for this problem, we re-implement and adapt the most promising heuristics from related scheduling problems. All these heuristics, a total of 31, are compared in an extensive computational evaluation with 1620 instances. The results show the excellent performance of the heuristics proposed.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420302864",
    "keywords": [
      "Algorithm",
      "Computer network",
      "Computer science",
      "Distributed computing",
      "Divide and conquer algorithms",
      "Flow shop scheduling",
      "Heuristics",
      "Job shop scheduling",
      "Mathematical optimization",
      "Mathematics",
      "Operating system",
      "Routing (electronic design automation)",
      "Scheduling (production processes)",
      "Server"
    ],
    "authors": [
      {
        "surname": "Fernandez-Viagas",
        "given_name": "Victor"
      },
      {
        "surname": "Costa",
        "given_name": "Antonio"
      },
      {
        "surname": "Framinan",
        "given_name": "Jose M."
      }
    ]
  },
  {
    "title": "A hybrid heuristic algorithm for cyclic inventory-routing problem with perishable products in VMI supply chain",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113322",
    "abstract": "The VMI supply chain can bring benefit to concerned parties. The paper studies cyclic inventory-routing problem (IRP) under VMI policy. Cyclic IRP as a variant of the IRP belong to long-term decision. It means once the replenishment policy and vehicle routing are determined, they will stay the same in the following periods. This paper considers the loss cost caused by perishability of perishable products and assumes the demand is dependent on price and stock. Based on the above consideration, three different cyclic IRP models for perishable products with stock and price dependent demand in VMI supply chain are put forward. They are IRP model ending with shortage, IRP model starting with shortage, and IRP model with no shortage. These models are composed of a single manufacturer and multiple retailers. The objective is to minimize the average total cost. The total cost includes not only fixed and transportation cost of vehicles, inventory (order and holding) and shortage cost of retailers, but also startup and holding cost of manufacturer. The proposed models are nonlinear mixed integer programming models and NP-hard problem. In order to solve these models, a hybrid heuristic algorithm that is developed by combining cuckoo algorithm with improved Clarke-Wright savings algorithm is developed. In computational experiments, the proposed algorithm is compared with the optimization solver. The results demonstrate the proposed algorithm outperforms the optimization solver. In addition, the results show the average total cost may be reduced by using the strategy of stockout in some cases. Sensitivity analysis is also implemented to study the influence of parameters on optimal solution. The results of computational experiments validate the applicability of the proposed models and the effectiveness of hybrid heuristic algorithm. The study also provides policymakers with management implications. Finally, the conclusions and future research directions are given.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420301470",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Business",
      "Computer network",
      "Computer science",
      "Heuristic",
      "Marketing",
      "Mathematical optimization",
      "Mathematics",
      "Operations research",
      "Routing (electronic design automation)",
      "Supply chain",
      "Vehicle routing problem"
    ],
    "authors": [
      {
        "surname": "Dai",
        "given_name": "Zhuo"
      },
      {
        "surname": "Gao",
        "given_name": "Kuo"
      },
      {
        "surname": "Giri",
        "given_name": "B.C."
      }
    ]
  },
  {
    "title": "An integrated early warning system for stock market turbulence",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113463",
    "abstract": "This study constructs an integrated early warning system (EWS) that identifies and predicts stock market turbulence. Based on switching ARCH (SWARCH) filtering probabilities of the high volatility regime, the proposed EWS first classifies stock market crises according to an indicator function with thresholds dynamically selected by the two-peak method. An hybrid algorithm is then developed in the framework of a long short-term memory (LSTM) network to make daily predictions that alert turmoils. In the empirical evaluation based on ten-year Chinese stock data, the proposed EWS yields satisfying results with test-set accuracy of 96.4% and an average of 2.8 days forewarned period. The model’s stability and practical value in the real-time decision-making are also proven by the cross-validation, back-testing and reality check.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420302876",
    "keywords": [
      "Biology",
      "Computer science",
      "Econometrics",
      "Economics",
      "Engineering",
      "Horse",
      "Mechanical engineering",
      "Paleontology",
      "Stock (firearms)",
      "Stock market",
      "Telecommunications",
      "Volatility (finance)",
      "Warning system"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Peiwan"
      },
      {
        "surname": "Zong",
        "given_name": "Lu"
      },
      {
        "surname": "Ma",
        "given_name": "Ye"
      }
    ]
  },
  {
    "title": "A review on deep learning methods for ECG arrhythmia classification",
    "journal": "Expert Systems with Applications: X",
    "year": "2020",
    "doi": "10.1016/j.eswax.2020.100033",
    "abstract": "Deep Learning (DL) has recently become a topic of study in different applications including healthcare, in which timely detection of anomalies on Electrocardiogram (ECG) can play a vital role in patient monitoring. This paper presents a comprehensive review study on the recent DL methods applied to the ECG signal for the classification purposes. This study considers various types of the DL methods such as Convolutional Neural Network (CNN), Deep Belief Network (DBN), Recurrent Neural Network (RNN), Long Short-Term Memory (LSTM), and Gated Recurrent Unit (GRU). From the 75 studies reported within 2017 and 2018, CNN is dominantly observed as the suitable technique for feature extraction, seen in 52% of the studies. DL methods showed high accuracy in correct classification of Atrial Fibrillation (AF) (100%), Supraventricular Ectopic Beats (SVEB) (99.8%), and Ventricular Ectopic Beats (VEB) (99.7%) using the GRU/LSTM, CNN, and LSTM, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S2590188520300123",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Atrial fibrillation",
      "Cardiac arrhythmia",
      "Cardiology",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Feature (linguistics)",
      "Feature extraction",
      "Linguistics",
      "Machine learning",
      "Medicine",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Recurrent neural network",
      "Supraventricular arrhythmia"
    ],
    "authors": [
      {
        "surname": "Ebrahimi",
        "given_name": "Zahra"
      },
      {
        "surname": "Loni",
        "given_name": "Mohammad"
      },
      {
        "surname": "Daneshtalab",
        "given_name": "Masoud"
      },
      {
        "surname": "Gharehbaghi",
        "given_name": "Arash"
      }
    ]
  },
  {
    "title": "An ensemble-based approach to the security-oriented classification of low-level log traces",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113386",
    "abstract": "Traditionally, Expert Systems have found a natural application in the behavioral analysis of processes. In fact, they have proved effective in the tasks of interpreting the data collected during the process executions and of analyzing these data with the aim of diagnosing/detecting anomalies. In this context, we focus on log data generated by executions of business processes, and consider the issue of detecting “insecure” process instances, involving some kind of security breach (e.g. attacks, frauds). We propose a hybrid framework for accomplishing a security-oriented classification of activity-unawaretraces, i.e., traces consisting of “low-level” events with no explicit reference to the “high-level” activities the analysts are typically familiar with. The framework integrates two classification approaches traditionally used as alternative ways to decide on the “secureness” of process traces: (i) a model-driven approach, using knowledge of behavioral models expressed at the abstraction level of the activities, and (ii) an example driven approach, exploiting the availability of event sequences labeled by experts as symptomatic of “secure” or “insecure” behavior. The core of our solution is a meta-classifier combining (i) and (ii) thanks to a probabilistic Montecarlo mechanism that allows the traces to be simultaneously viewed as sequences of low-level events and of high-level activities. The framework has been empirically proved effective in jointly exploiting the two aforementioned forms of knowledge/expertise, typically coming from different experts, and in acting as a sort of “super-expert” classification tool. Its accuracy and efficiency make it a solid basis for implementing a novel kind of expert system for the security-oriented monitoring/analysis of business processes.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420302104",
    "keywords": [
      "Abstraction",
      "Artificial intelligence",
      "Biology",
      "Classifier (UML)",
      "Computer science",
      "Context (archaeology)",
      "Data mining",
      "Epistemology",
      "Event (particle physics)",
      "Information retrieval",
      "Machine learning",
      "Operating system",
      "Paleontology",
      "Philosophy",
      "Physics",
      "Probabilistic logic",
      "Process (computing)",
      "Quantum mechanics",
      "sort"
    ],
    "authors": [
      {
        "surname": "Fazzinga",
        "given_name": "Bettina"
      },
      {
        "surname": "Folino",
        "given_name": "Francesco"
      },
      {
        "surname": "Furfaro",
        "given_name": "Filippo"
      },
      {
        "surname": "Pontieri",
        "given_name": "Luigi"
      }
    ]
  },
  {
    "title": "Fake news, rumor, information pollution in social media and web: A contemporary survey of state-of-the-arts, challenges and opportunities",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112986",
    "abstract": "Internet and social media have become a widespread, large scale and easy to use platform for real-time information dissemination. It has become an open stage for discussion, ideology expression, knowledge dissemination, emotions and sentiment sharing. This platform is gaining tremendous attraction and a huge user base from all sections and age groups of society. The matter of concern is that up to what extent the contents that are circulating among all these platforms every second changing the mindset, perceptions and lives of billions of people are verified, authenticated and up to the standards. This paper puts forward a holistic view of how the information is being weaponized to fulfil the malicious motives and forcefully making a biased user perception about a person, event or firm. Further, a taxonomy is provided for the classification of malicious information content at different stages and prevalent technologies to cope up with this issue form origin, propagation, detection and containment stages. We also put forward a research gap and possible future research directions so that the web information content could be more reliable and safer to use for decision making as well as for knowledge sharing.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419307043",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Data science",
      "Information sharing",
      "Internet privacy",
      "Mindset",
      "Neuroscience",
      "Perception",
      "Political science",
      "Public relations",
      "Rumor",
      "Social media",
      "The Internet",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Meel",
        "given_name": "Priyanka"
      },
      {
        "surname": "Vishwakarma",
        "given_name": "Dinesh Kumar"
      }
    ]
  },
  {
    "title": "Optimisation of phonetic aware speech recognition through multi-objective evolutionary algorithms",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113402",
    "abstract": "Recent advances in the availability of computational resources allow for more sophisticated approaches to speech recognition than ever before. This study considers Artificial Neural Network and Hidden Markov Model methods of classification for Human Speech Recognition through Diphthong Vowel sounds in the English Phonetic Alphabet rather than the classical approach of the classification of whole words and phrases, with a specific focus on both single and multi-objective evolutionary optimisation of bioinspired classification methods. A set of audio clips are recorded by subjects from the United Kingdom and Mexico and the recordings are transformed into a static dataset of statistics by way of their Mel-Frequency Cepstral Coefficients (MFCC) at sliding window length of 200ms as well as a reshaped MFCC timeseries format for forecast-based models. An deep neural network with evolutionary optimised topology achieves 90.77% phoneme classification accuracy in comparison to the best HMM that achieves 86.23% accuracy with 150 hidden units, when only accuracy is considered in a single-objective optimisation approach. The obtained solutions are far more complex than the HMM taking around 248 seconds to train on powerful hardware versus 160 for the HMM. A multi-objective approach is explored due to this. In the multi-objective approaches of scalarisation presented, within which real-time resource usage is also considered towards solution fitness, far more optimal solutions are produced which train far quicker than the forecast approach (69 seconds) with classification ability retained (86.73%). Weightings towards either maximising accuracy or reducing resource usage from 0.1 to 0.9 are suggested depending on the resources available, since many future IoT devices and autonomous robots may have limited access to cloud resources at a premium in comparison to the GPU used in this experiment.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420302268",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Feature extraction",
      "Hidden Markov model",
      "Machine learning",
      "Mel-frequency cepstrum",
      "Operating system",
      "Pattern recognition (psychology)",
      "Programming language",
      "Set (abstract data type)",
      "Sliding window protocol",
      "Speech recognition",
      "Window (computing)"
    ],
    "authors": [
      {
        "surname": "Bird",
        "given_name": "Jordan J."
      },
      {
        "surname": "Wanner",
        "given_name": "Elizabeth"
      },
      {
        "surname": "Ekárt",
        "given_name": "Anikó"
      },
      {
        "surname": "Faria",
        "given_name": "Diego R."
      }
    ]
  },
  {
    "title": "GRAM: An efficient (k, l) graph anonymization method",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113454",
    "abstract": "There are plenty of applications that use graphs for representing the association between different entities. Many research communities are interested in publishing such graphs to study the knowledge contained in them while the privacy of involved individuals is a challenging issue. A successful computational privacy model to address the problem of data privacy in microdata publishing is k-anonymity. A relaxed version of it for graphs is called (k, l)-anonymity: identifying at most l neighbors of a vertex v in the graph, an attacker cannot limit v in a group of less than k vertices. This paper introduces the GRAM: an efficient (k, l) GRaph Anonymization Method based on edge addition. At first, the GRAM adds enough edges to the graph to meet the privacy requirement. Then, the algorithm removes some redundant added edges to minimize changes in the original graph. The necessary and sufficient conditions for removing an added edge while maintaining the privacy requirement are proposed in the paper. This makes it possible to realize the model efficiently. An extensive set of experiments shows that the GRAM usually achieves a good trade-off between data utility and privacy, while it is more efficient than similar techniques.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420302785",
    "keywords": [
      "Anonymity",
      "Census",
      "Computer science",
      "Computer security",
      "Data anonymization",
      "Data mining",
      "Data publishing",
      "Demography",
      "Graph",
      "Information privacy",
      "Law",
      "Microdata (statistics)",
      "Political science",
      "Population",
      "Publishing",
      "Sociology",
      "Theoretical computer science",
      "Vertex (graph theory)",
      "k-anonymity"
    ],
    "authors": [
      {
        "surname": "Mortazavi",
        "given_name": "R."
      },
      {
        "surname": "Erfani",
        "given_name": "S.H."
      }
    ]
  },
  {
    "title": "An improved differential evolution algorithm with dual mutation strategies collaboration",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113451",
    "abstract": "To reduce the effect of the selections of mutation strategies and control parameters on the performance of differential evolution (DE), this paper proposes an improved differential evolution algorithm with dual mutation strategies collaboration (DMCDE), in which two main improvements are presented. First, DMCDE introduces an elite guidance mechanism to propose two new variants of the classical DE/rand/2 and DE/best/2 mutation strategies, which we call DE/e-rand/2 and DE/e-best/2 respectively. They use the individuals randomly chosen from superior elite population as the base vector and the first vector of difference vectors, thereby providing clearer guidance for individual mutation without losing randomness. Second, a mechanism of dual mutation strategies collaboration is utilized to obtain a trade-off between global exploration and local exploitation of the algorithm. The performance of DMCDE is evaluated by using the commonly used test functions as well as a real-world optimization problem. The results show that DMCDE can significantly improve the optimization performance of DE, and is superior to the comparative competitors.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742030275X",
    "keywords": [
      "Algorithm",
      "Art",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Demography",
      "Differential evolution",
      "Dual (grammatical number)",
      "Gene",
      "Literature",
      "Mathematical optimization",
      "Mathematics",
      "Mutation",
      "Population",
      "Randomness",
      "Sociology",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Yuzhen"
      },
      {
        "surname": "Wang",
        "given_name": "Shihao"
      },
      {
        "surname": "Yang",
        "given_name": "Bo"
      }
    ]
  },
  {
    "title": "An NLP-Powered Human Rights Monitoring Platform",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113365",
    "abstract": "Effective information management has long been a problem in organisations that are not of a scale that they can afford their own department dedicated to this task. Growing information overload has made this problem even more pronounced. On the other hand we have recently witnessed the emergence of intelligent tools, packages and resources that made it possible to rapidly transfer knowledge from the academic community to industry, government and other potential beneficiaries. Here we demonstrate how adopting state-of-the-art natural language processing (NLP) and crowdsourcing methods has resulted in measurable benefits for a human rights organisation by transforming their information and knowledge management using a novel approach that supports human rights monitoring in conflict zones. More specifically, we report on mining and classifying Arabic Twitter in order to identify potential human rights abuse incidents in a continuous stream of social media data within a specified geographical region. Results show deep learning approaches such as LSTM allow us to push the precision close to 85% for this task with an F1-score of 75%. Apart from the scientific insights we also demonstrate the viability of the framework which has been deployed as the Ceasefire Iraq portal for more than three years which has already collected thousands of witness reports from within Iraq. This work is a case study of how progress in artificial intelligence has disrupted even the operation of relatively small-scale organisations.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420301901",
    "keywords": [
      "Artificial intelligence",
      "Autoencoder",
      "Computer science",
      "Crowdsourcing",
      "Data science",
      "Deep learning",
      "Economics",
      "Engineering",
      "Government (linguistics)",
      "Human rights",
      "Information overload",
      "Knowledge management",
      "Law",
      "Linguistics",
      "Management",
      "Mechanical engineering",
      "Philosophy",
      "Physics",
      "Political science",
      "Programming language",
      "Quantum mechanics",
      "Scale (ratio)",
      "Social media",
      "Task (project management)",
      "Witness",
      "Work (physics)",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Alhelbawy",
        "given_name": "Ayman"
      },
      {
        "surname": "Lattimer",
        "given_name": "Mark"
      },
      {
        "surname": "Kruschwitz",
        "given_name": "Udo"
      },
      {
        "surname": "Fox",
        "given_name": "Chris"
      },
      {
        "surname": "Poesio",
        "given_name": "Massimo"
      }
    ]
  },
  {
    "title": "Adaptive variable neighborhood search solution methods for the fleet size and mix pollution location-inventory-routing problem",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113444",
    "abstract": "This work introduces the Fleet-size and Mix Pollution Location-Inventory-Routing Problem with Just-in-Time replenishment policy and Capacity Planning. This problem extends the strategic-level decisions of classic LIRP by considering capacity selection decisions and heterogeneous fleet composition. An MIP formulation of this new complex combinatorial optimization problem is proposed and small-sized problem instances are solved using the CPLEX solver. For the solution of more realistic-sized problem instances, a General Variable Neighborhood Search (GVNS)-based framework is adopted. Novel adaptive shaking methods are proposed as intelligent components of the developed GVNS algorithms to further improve their performance. To evaluate the proposed GVNS schemes, several problem instances are randomly generated by following specific instructions from the literature and adopting real vehicles’ parameters. Comparisons between these solutions and the corresponding ones achieved by CPLEX are made. The computational results indicate the efficiency of the proposed GVNS-based algorithms, with the best GVNS scheme to produce 7% better solutions than CPLEX for small problems. Finally, the economic and environmental impacts of using either homogeneous or heterogeneous fleet of vehicles are examined.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420302682",
    "keywords": [
      "Combinatorics",
      "Computer network",
      "Computer science",
      "Homogeneous",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Metaheuristic",
      "Operations research",
      "Routing (electronic design automation)",
      "Scheme (mathematics)",
      "Solver",
      "Variable (mathematics)",
      "Variable neighborhood search",
      "Vehicle routing problem"
    ],
    "authors": [
      {
        "surname": "Karakostas",
        "given_name": "Panagiotis"
      },
      {
        "surname": "Sifaleras",
        "given_name": "Angelo"
      },
      {
        "surname": "Georgiadis",
        "given_name": "Michael C."
      }
    ]
  },
  {
    "title": "Automatic segmentation model combining U-Net and level set method for medical images",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113419",
    "abstract": "We introduce a new method that combines a constrained term and level set method for the automated segmentation of medical image. There are two types of constrained terms, fully automatic and semi-automatic. It is fully automatic to use the U-Net’s segmentation result as a constrained term, and the manual segmentation result as a constrained term is semi-automatic. The level set method does not require a large training set and is theoretically very explanatory, but is usually sensitive to the initial contour. The U-Net can segment more complex medical images, but requires a large number of manually labeled images and usually needs to be normalized to produce a good generalization. Therefore, the combination of these methods combines the advantages of both methods, resulting in a method that requires a small training set and produces accurate segmentation results. We test our method on the melanoma and left ventricle images. Among them, when segmenting melanoma images, our semi-automatic segmentation and full-automatic segmentation results are better than the U-Net and RSF segmentation results alone. When segmenting the left ventricle images, our semi-automatic segmentation result is better than the RSF segmentation result.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420302438",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Image segmentation",
      "Pattern recognition (psychology)",
      "Programming language",
      "Segmentation",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Yunyun"
      },
      {
        "surname": "Feng",
        "given_name": "Chong"
      },
      {
        "surname": "Wang",
        "given_name": "Ruofan"
      }
    ]
  },
  {
    "title": "Bioacoustic signal classification in continuous recordings: Syllable-segmentation vs sliding-window",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113390",
    "abstract": "Frog population has been experiencing rapid decreases worldwide, which is regarded as one of the most critical threats to the global biodiversity. Therefore, large volumes of frog recordings have been collected for assessing this decline. Building an automatic frog species classification system is becoming ever more important. The traditional system for classifying frog species consists of four steps: (1) bioacoustic signal preprocessing, (2) segmentation, (3) feature extraction, (4) classification. Each prior step has a direct impact on the subsequent step. Consequently, the final classification performance is highly affected by the initial three steps. However, the performance of bioacoustic signal segmentation is highly dependent on the background noise of those environmental recordings. In this study, we propose an end-to-end approach for acoustic classification of frog species in continuous recordings. First, a sliding window is used to segment the audio signal into frames. Then, 1D-Convolution Neural Network and long short-term memory (CNN-LSTM) network is used to learn a representation from the raw audio signal, where three Convolutional layers and one LSTM layer are used to capture the signal’s pattern. Experimental results in classifying 23 Australian frog species demonstrate the effectiveness of our proposed CNN-LSTM based method. Compared to the syllable-segmentation based frog species classification system, our proposed CNN-LSTM based approach is more robust in frog species classification under various noisy conditions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420302141",
    "keywords": [
      "Artificial intelligence",
      "Audio signal",
      "Bioacoustics",
      "Computer science",
      "Convolutional neural network",
      "Demography",
      "Feature (linguistics)",
      "Feature extraction",
      "Image (mathematics)",
      "Linguistics",
      "Noise (video)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Population",
      "Preprocessor",
      "Programming language",
      "SIGNAL (programming language)",
      "Segmentation",
      "Sliding window protocol",
      "Sociology",
      "Speech coding",
      "Speech recognition",
      "Telecommunications",
      "Window (computing)"
    ],
    "authors": [
      {
        "surname": "Xie",
        "given_name": "Jie"
      },
      {
        "surname": "Hu",
        "given_name": "Kai"
      },
      {
        "surname": "Zhu",
        "given_name": "Mingying"
      },
      {
        "surname": "Guo",
        "given_name": "Ya"
      }
    ]
  },
  {
    "title": "A new preference disaggregation TOPSIS approach applied to sort corporate bonds based on financial statements and expert's assessment",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113369",
    "abstract": "This paper presents a new version of the TOPSIS method for sorting problems. The proposed method, called Preference Disaggregation on Technique for Order of Preference by Similarity to Ideal Solution - Sort (PDTOPSIS-Sort), is based on nonlinear programming for inferring parameters and uses expert's holistic evaluations. The existing TOPSIS-Sort method demands a significant number of parameters from the expert, including the definition of boundary profiles and weights. The proposed method contributes to the literature by relieving the demand for cognitive effort observed in prior methods. Instead of providing boundary profiles for the limit between every two consecutive classes, the expert provides decision examples. In addition, the specification of weights is not required. A numerical validation of PDTOPSIS-Sort was undertaken based on results previously obtained from the literature for TOPSIS-Sort, which has been presented in detail as supplementary material. In addition, the first analysis of Brazilian corporate bonds supported by an MCDM/A model is presented. To do so, data were collected from the financial statements published by the issuers of these bonds. In total, the method proposed classified 50 debentures and the results were consistent with the preferences of the decision-maker, an investment-banking expert.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420301949",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Ideal solution",
      "Image (mathematics)",
      "Information retrieval",
      "Mathematics",
      "Multiple-criteria decision analysis",
      "Operations research",
      "Physics",
      "Preference",
      "Similarity (geometry)",
      "Sorting",
      "Sorting algorithm",
      "Statistics",
      "TOPSIS",
      "Thermodynamics",
      "sort"
    ],
    "authors": [
      {
        "surname": "de Lima Silva",
        "given_name": "Diogo Ferreira"
      },
      {
        "surname": "Ferreira",
        "given_name": "Luciano"
      },
      {
        "surname": "de Almeida-Filho",
        "given_name": "Adiel Teixeira"
      }
    ]
  },
  {
    "title": "Extracting actionable knowledge from social networks with node attributes",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113382",
    "abstract": "Actionable Knowledge Discovery has attracted much interest lately. It is almost a new paradigm shift toward mining more usable and more applicable knowledge in each specific domain. An action is a new tool in this research area that suggests some changes to the user to gain a profit in his/her domain. Currently, most of action mining methods rely on simple data which describes each object independently. Since social data has more complex structure due to the relationships between individuals, a major problem is that such structural information is not taken into account in the action mining process. This leads to miss some useful knowledge and profitable actions. Consequently, more effective methods are needed for mining actions. The main focus of this work is to extract cost-effective actions from social networks in which nodes have attributes. The actions suggest optimal changes in nodes’ attributes that are likely to result in changing labels of users to more desired one when they are applied. We develop an action mining method based on Random Walks that naturally combines the information from the network structure with nodes attributes. We formulate action mining as an optimization problem where the goal is to learn a function that varies the values of nodes’ attributes which in turn affect edges’ weights in the network so that the labels of intended individuals are likely to take the desired label while minimizing the cost of incurring the changes. Experiments confirm that the proposed approach outperforms the current state-of-the-art in action mining.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420302062",
    "keywords": [
      "Action (physics)",
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Data science",
      "Domain (mathematical analysis)",
      "Domain knowledge",
      "Engineering",
      "Focus (optics)",
      "Knowledge extraction",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Node (physics)",
      "Operating system",
      "Optics",
      "Physics",
      "Process (computing)",
      "Quantum mechanics",
      "Structural engineering",
      "USable",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Kalanat",
        "given_name": "Nasrin"
      },
      {
        "surname": "Khanjari",
        "given_name": "Eynollah"
      }
    ]
  },
  {
    "title": "A modified particle swarm optimization using adaptive strategy",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113353",
    "abstract": "In expert systems, complex optimization problems are usually nonlinear, nonconvex, multimodal and discontinuous. As an efficient and simple optimization algorithm, particle swarm optimization(PSO) has been widely applied to solve various real optimization problems in expert systems. However, avoiding premature convergence and balancing the global exploration and local exploitation capabilities of the PSO remains an open issue. To overcome these drawbacks and strengthen the ability of PSO in solving complex optimization problems, a modified PSO using adaptive strategy called MPSO is proposed. In MPSO, in order to well balance the global exploration and local exploitation capabilities of the PSO, a chaos-based non-linear inertia weight is proposed. Meanwhile, to avoid the premature convergence, stochastic and mainstream learning strategies are adopted. Finally, an adaptive position updating strategy and terminal replacement mechanism are employed to enhance PSO’s ability to solve complex optimization problems in expert systems. 30 complex CEC2017 benchmark functions are utilized to verify the promising performance of MPSO, experimental results and statistical analysis indicate that MPSO has competitive performance compared with 16 state-of-the-art algorithms. The source code of MPSO is provided at https://github.com/lhustl/MPSO .",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420301780",
    "keywords": [
      "Adaptive strategies",
      "Archaeology",
      "Artificial intelligence",
      "Computer science",
      "History",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Metaheuristic",
      "Multi-swarm optimization",
      "Particle swarm optimization",
      "Swarm behaviour"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Hao"
      },
      {
        "surname": "Zhang",
        "given_name": "Xu-Wei"
      },
      {
        "surname": "Tu",
        "given_name": "Liang-Ping"
      }
    ]
  },
  {
    "title": "An improved quantum particle swarm optimization algorithm for environmental economic dispatch",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113370",
    "abstract": "Consumption of traditional fossil energy has promoted rapid economic development and caused effects such as climate warming and environmental degradation. In order to solve the problem of environmental economic dispatch (EED), this paper proposes a DE-CQPSO (Differential Evolution-Crossover Quantum Particle Swarm Optimization) algorithm based on the fast convergence of differential evolution algorithms and the particle diversity of crossover operators of genetic algorithms. In order to obtain better optimization results, a parameter adaptive control method is used to update the crossover probability. And the problem of multi-objective optimization is solved by introducing a penalty factor. The experimental results show that: the evaluation index and convergence speed of the DE-CQPSO algorithm are better than QPSO (Quantum Particle Swarm Optimization) and other algorithms, whether it is single-objective optimization of fuel cost and emissions or multi-objective optimization considering both optimization objectives. A good compromise value is verified, which verifies the effectiveness and robustness of the DE-CQPSO algorithm in solving environmental economic dispatch problems. The study provides a new research direction for solving environmental economic dispatch problems. At the same time, it provides a reference for the reasonable output of the unit to a certain extent.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420301950",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Convergence (economics)",
      "Crossover",
      "Differential evolution",
      "Economic dispatch",
      "Economic growth",
      "Economics",
      "Electric power system",
      "Gene",
      "Imperialist competitive algorithm",
      "Mathematical optimization",
      "Mathematics",
      "Meta-optimization",
      "Metaheuristic",
      "Multi-swarm optimization",
      "Optimization problem",
      "Particle swarm optimization",
      "Physics",
      "Power (physics)",
      "Quantum mechanics",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Xin-gang",
        "given_name": "Zhao"
      },
      {
        "surname": "Ji",
        "given_name": "Liang"
      },
      {
        "surname": "Jin",
        "given_name": "Meng"
      },
      {
        "surname": "Ying",
        "given_name": "Zhou"
      }
    ]
  },
  {
    "title": "A weighted information-gain measure for ordinal classification trees",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113375",
    "abstract": "This paper proposes an ordinal decision-tree model, which applies a new weighted information-gain ratio (WIGR) measure for selecting the classifying attributes in the tree. The proposed measure utilizes a weighted entropy function that is defined proportionally to the value deviation of different classes and thus reflects the consequences of the magnitude of potential classification errors. The WIGR can be used to select the classifying attributes in decision trees in a manner that reduces risks. The proposed ordinal decision tree is found effective for classification problems in which the class variable exhibits some form of ordinal ordering, and where dependencies between the attributes and the class value can be non-monotonic. In a series of experiments based on publicly-known datasets, it is shown that the proposed ordinal decision tree outperforms its non-ordinal counterparts that utilize traditional entropy measures. The proposed model can be used as a part of an expert system for ordinal classification applications, such as health-state monitoring, portfolio investments classification and performance evaluation of service systems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420302001",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Decision tree",
      "Decision tree learning",
      "Entropy (arrow of time)",
      "Information gain ratio",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Measure (data warehouse)",
      "Monotonic function",
      "Mutual information",
      "Ordinal data",
      "Ordinal optimization",
      "Ordinal regression",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics"
    ],
    "authors": [
      {
        "surname": "Singer",
        "given_name": "Gonen"
      },
      {
        "surname": "Anuar",
        "given_name": "Roee"
      },
      {
        "surname": "Ben-Gal",
        "given_name": "Irad"
      }
    ]
  },
  {
    "title": "FluidsNet: End-to-end learning for Lagrangian fluid simulation",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113410",
    "abstract": "Over the past few decades, fluid simulation has emerged as an important tool in computer animation. However, traditional physical-based fluid simulation systems are time consuming and requires large computational resources to generate large-scale fluid flows. Intelligent systems provide us a new method of data-driven to accelerate simulations. Previous intelligent system manually crafted feature vectors by using a context-based integral method and resulted in high computational and memory requirements. Unlike the existing techniques, we do not use any manually crafted feature, instead directly operate on Lagrangian fluid simulation data and use machine learned features. This paper presents a novel end-to-end deep learning neural network that can automatically generate a model for fluid animation based-on Lagrangian fluid simulation data. This approach synthesizes velocity fields with irregular Lagrangian data structure using neural network. Every fluid particle is treated independently and identically. We use symmetric functions to capture space structure and interactions among particles and design different network structures to learn various hierarchical features of fluid. We test this method using several data sets and applications in various scenes with different sizes. Our experiments show that the model is able to infer velocity field with realistic details such as splashes. In addition, compared with exiting simulation system, this method shows significant speed-ups, especially on large scene simulations.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420302347",
    "keywords": [
      "Animation",
      "Applied mathematics",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computational science",
      "Computer animation",
      "Computer graphics (images)",
      "Computer science",
      "Context (archaeology)",
      "Feature (linguistics)",
      "Field (mathematics)",
      "Lagrangian",
      "Linguistics",
      "Mathematics",
      "Paleontology",
      "Philosophy",
      "Pure mathematics"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Yalan"
      },
      {
        "surname": "Ban",
        "given_name": "Xiaojuan"
      },
      {
        "surname": "Du",
        "given_name": "Feilong"
      },
      {
        "surname": "Di",
        "given_name": "Wu"
      }
    ]
  },
  {
    "title": "An aggregative learning gravitational search algorithm with self-adaptive gravitational constants",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113396",
    "abstract": "The gravitational search algorithm (GSA) is a meta-heuristic algorithm based on the theory of Newtonian gravity. This algorithm uses the gravitational forces among individuals to move their positions in order to find a solution to optimization problems. Many studies indicate that the GSA is an effective algorithm, but in some cases, it still suffers from low search performance and premature convergence. To alleviate these issues of the GSA, an aggregative learning GSA called the ALGSA is proposed with a self-adaptive gravitational constant in which each individual possesses its own gravitational constant to improve the search performance. The proposed algorithm is compared with some existing variants of the GSA on the IEEE CEC2017 benchmark test functions to validate its search performance. Moreover, the ALGSA is also tested on neural network optimization to further verify its effectiveness. Finally, the time complexity of the ALGSA is analyzed to clarify its search performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420302207",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Classical mechanics",
      "Computer science",
      "Constant (computer programming)",
      "Convergence (economics)",
      "Economic growth",
      "Economics",
      "Geodesy",
      "Geography",
      "Gravitation",
      "Gravitational constant",
      "Gravitational search algorithm",
      "Heuristic",
      "Mathematical optimization",
      "Mathematics",
      "Particle swarm optimization",
      "Physics",
      "Premature convergence",
      "Programming language"
    ],
    "authors": [
      {
        "surname": "Lei",
        "given_name": "Zhenyu"
      },
      {
        "surname": "Gao",
        "given_name": "Shangce"
      },
      {
        "surname": "Gupta",
        "given_name": "Shubham"
      },
      {
        "surname": "Cheng",
        "given_name": "Jiujun"
      },
      {
        "surname": "Yang",
        "given_name": "Gang"
      }
    ]
  },
  {
    "title": "Privacy-preserving human action recognition as a remote cloud service using RGB-D sensors and deep CNN",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113349",
    "abstract": "Cloud-based expert systems are highly emerging nowadays. However, the data owners and cloud service providers are not in the same trusted domain in practice. For the sake of data privacy, sensitive data usually has to be encrypted before outsourcing which makes effective cloud utilization a challenging task. Taking this concern into account, we propose a novel cloud-based approach to securely recognize human activities. A few schemes exist in the literature for secure recognition. However, they suffer from the problem of constrained data and are vulnerable to re-identification attack, where advanced deep learning models are used to predict an object’s identity. We address these problems by considering color and depth data, and securing them using position based superpixel transformation. The proposed transformation is designed by actively involving additional noise while resizing the underlying image. Due to this, a higher degree of obfuscation is achieved. Further, in spite of securing the complete video, we secure only four images, that is, one motion history image and three depth motion maps which are highly saving the data overhead. The recognition is performed using a four stream deep Convolutional Neural Network (CNN), where each stream is based on pre-trained MobileNet architecture. Experimental results show that the proposed approach is the best suitable candidate in “security-recognition accuracy (%)” trade-off relation among other image obfuscation as well as state-of-the-art schemes. Moreover, a number of security tests and analyses demonstrate robustness of the proposed approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420301743",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Cloud computing",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Convolutional neural network",
      "Data mining",
      "Deep learning",
      "Encryption",
      "Gene",
      "Machine learning",
      "Obfuscation",
      "Operating system",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Rajput",
        "given_name": "Amitesh Singh"
      },
      {
        "surname": "Raman",
        "given_name": "Balasubramanian"
      },
      {
        "surname": "Imran",
        "given_name": "Javed"
      }
    ]
  },
  {
    "title": "Interpreting RFID tracking data for simultaneously moving objects: An offline sampling-based approach",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113368",
    "abstract": "We consider the scenario of multiple RFID-tagged objects that simultaneously move across an indoor space where several RFID antennas are placed. We assume that a logical partition of the indoor space into a set of locations is given, along with a set of hard and weak integrity constraints describing both the valid movements of the objects and the capacity of the locations. In this setting, we address the problem of matching the collected readings to the trajectories(namely, the sequences of locations) followed by the target objects. We model this problem as estimating a probability distribution function over the possible matchings of the readings to the locations. The core of our approach is a novel Metropolis Hastings sampler that is guided by the integrity constraints to distinguish between likely and unlikely ways of interpreting the readings. The challenges of integrating the constraints into the sampler are discussed, and a thorough experimental analysis, where the proposed approach is compared with the state of the art, is provided.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420301937",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Data mining",
      "Matching (statistics)",
      "Mathematics",
      "Object (grammar)",
      "Operating system",
      "Partition (number theory)",
      "Pedagogy",
      "Programming language",
      "Psychology",
      "Set (abstract data type)",
      "Space (punctuation)",
      "State space",
      "Statistics",
      "Tracking (education)"
    ],
    "authors": [
      {
        "surname": "Fazzinga",
        "given_name": "Bettina"
      },
      {
        "surname": "Flesca",
        "given_name": "Sergio"
      },
      {
        "surname": "Furfaro",
        "given_name": "Filippo"
      },
      {
        "surname": "Parisi",
        "given_name": "Francesco"
      }
    ]
  },
  {
    "title": "PQ-RRT*: An improved path planning algorithm for mobile robots",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113425",
    "abstract": "During the last decade, sampling-based algorithms for path planning have gained considerable attention. The RRT*, a variant of RRT (rapidly-exploring random trees), is of particular concern to researchers due to its asymptotic optimality. However, the limits of the slow convergence rate of RRT* makes it inefficient for applications. For the purposes of overcoming these limitations, this paper proposes a novel algorithm, PQ-RRT*, which combines the strengths of P-RRT* (potential functions based RRT*) and Quick-RRT*. PQ-RRT* guarantees a fast convergence to an optimal solution and generates a better initial solution. The asymptotic optimality and fast convergence of the proposed algorithm are proved in this paper. Comparisons of PQ-RRT* with P-RRT* and Quick-RRT* in four benchmarks verify the effectiveness of the proposed algorithm.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420302499",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Convergence (economics)",
      "Economic growth",
      "Economics",
      "Mathematical optimization",
      "Mathematics",
      "Motion planning",
      "Path (computing)",
      "Programming language",
      "Random tree",
      "Robot"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Yanjie"
      },
      {
        "surname": "Wei",
        "given_name": "Wu"
      },
      {
        "surname": "Gao",
        "given_name": "Yong"
      },
      {
        "surname": "Wang",
        "given_name": "Dongliang"
      },
      {
        "surname": "Fan",
        "given_name": "Zhun"
      }
    ]
  },
  {
    "title": "Word2vec-based latent semantic analysis (W2V-LSA) for topic modeling: A study on blockchain technology trend analysis",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113401",
    "abstract": "Blockchain has become one of the core technologies in Industry 4.0. To help decision-makers establish action plans based on blockchain, it is an urgent task to analyze trends in blockchain technology. However, most of existing studies on blockchain trend analysis are based on effort demanding full-text investigation or traditional bibliometric methods whose study scope is limited to a frequency-based statistical analysis. Therefore, in this paper, we propose a new topic modeling method called Word2vec-based Latent Semantic Analysis (W2V-LSA), which is based on Word2vec and Spherical k-means clustering to better capture and represent the context of a corpus. We then used W2V-LSA to perform an annual trend analysis of blockchain research by country and time for 231 abstracts of blockchain-related papers published over the past five years. The performance of the proposed algorithm was compared to Probabilistic LSA, one of the common topic modeling techniques. The experimental results confirmed the usefulness of W2V-LSA in terms of the accuracy and diversity of topics by quantitative and qualitative evaluation. The proposed method can be a competitive alternative for better topic modeling to provide direction for future research in technology trend analysis and it is applicable to various expert systems related to text mining.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420302256",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Blockchain",
      "Computer science",
      "Computer security",
      "Context (archaeology)",
      "Data mining",
      "Data science",
      "Embedding",
      "Latent semantic analysis",
      "Paleontology",
      "Probabilistic latent semantic analysis",
      "Probabilistic logic",
      "Programming language",
      "Scope (computer science)",
      "Topic model",
      "Word2vec"
    ],
    "authors": [
      {
        "surname": "Kim",
        "given_name": "Suhyeon"
      },
      {
        "surname": "Park",
        "given_name": "Haecheong"
      },
      {
        "surname": "Lee",
        "given_name": "Junghye"
      }
    ]
  },
  {
    "title": "A novel approach to define the local region of dynamic selection techniques in imbalanced credit scoring problems",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113351",
    "abstract": "Lenders, such as banks and credit card companies, use credit scoring models to evaluate the potential risk posed by lending money to customers, and therefore to mitigate losses due to bad credit. The profitability of the banks thus highly depends on the models used to decide on the customer’s loans. State-of-the-art credit scoring models are based on machine learning and statistical methods. One of the major problems of this field is that lenders often deal with imbalanced datasets that usually contain many paid loans but very few not paid ones (called defaults). Recently, dynamic selection methods combined with ensemble methods and preprocessing techniques have been evaluated to improve classification models in imbalanced datasets presenting advantages over the static machine learning methods. In a dynamic selection technique, samples in the neighborhood of each query sample are used to compute the local competence of each base classifier. Then, the technique selects only competent classifiers to predict the query sample. In this paper, we evaluate the suitability of dynamic selection techniques for credit scoring problem, and we present Reduced Minority k-Nearest Neighbors (RMkNN), an approach that enhances state of the art in defining the local region of dynamic selection techniques for imbalanced credit scoring datasets. This proposed technique has a superior prediction performance in imbalanced credit scoring datasets compared to state of the art. Furthermore, RMkNN does not need any preprocessing or sampling method to generate the dynamic selection dataset (called DSEL). Additionally, we observe an equivalence between dynamic selection and static selection classification. We conduct a comprehensive evaluation of the proposed technique against state-of-the-art competitors on six real-world public datasets and one private one. Experiments show that RMkNN improves the classification performance of the evaluated datasets regarding AUC, balanced accuracy, H-measure, G-mean, F-measure, and Recall.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420301767",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Credit card",
      "Credit card fraud",
      "Data mining",
      "Machine learning",
      "Payment",
      "Preprocessor",
      "Selection (genetic algorithm)",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Melo Junior",
        "given_name": "Leopoldo"
      },
      {
        "surname": "Nardini",
        "given_name": "Franco Maria"
      },
      {
        "surname": "Renso",
        "given_name": "Chiara"
      },
      {
        "surname": "Trani",
        "given_name": "Roberto"
      },
      {
        "surname": "Macedo",
        "given_name": "Jose Antonio"
      }
    ]
  },
  {
    "title": "Marine Predators Algorithm: A nature-inspired metaheuristic",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113377",
    "abstract": "This paper presents a nature-inspired metaheuristic called Marine Predators Algorithm (MPA) and its application in engineering. The main inspiration of MPA is the widespread foraging strategy namely Lévy and Brownian movements in ocean predators along with optimal encounter rate policy in biological interaction between predator and prey. MPA follows the rules that naturally govern in optimal foraging strategy and encounters rate policy between predator and prey in marine ecosystems. This paper evaluates the MPA's performance on twenty-nine test functions, test suite of CEC-BC-2017, randomly generated landscape, three engineering benchmarks, and two real-world engineering design problems in the areas of ventilation and building energy performance. MPA is compared with three classes of existing optimization methods, including (1) GA and PSO as the most well-studied metaheuristics, (2) GSA, CS and SSA as almost recently developed algorithms and (3) CMA-ES, SHADE and LSHADE-cnEpSin as high performance optimizers and winners of IEEE CEC competition. Among all methods, MPA gained the second rank and demonstrated very competitive results compared to LSHADE-cnEpSin as the best performing method and one of the winners of CEC 2017 competition. The statistical post hoc analysis revealed that MPA can be nominated as a high-performance optimizer and is a significantly superior algorithm than GA, PSO, GSA, CS, SSA and CMA-ES while its performance is statistically similar to SHADE and LSHADE-cnEpSin. The source code is publicly available at: https://github.com/afshinfaramarzi/Marine-Predators-Algorithm, http://built-envi.com/portfolio/marine-predators-algorithm/, https://www.mathworks.com/matlabcentral/fileexchange/74578-marine-predators-algorithm-mpa, and http://www.alimirjalili.com/MPA.html.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420302025",
    "keywords": [
      "Algorithm",
      "Biology",
      "Competition (biology)",
      "Computer science",
      "Ecology",
      "Foraging",
      "Mathematical optimization",
      "Mathematics",
      "Metaheuristic",
      "Predation"
    ],
    "authors": [
      {
        "surname": "Faramarzi",
        "given_name": "Afshin"
      },
      {
        "surname": "Heidarinejad",
        "given_name": "Mohammad"
      },
      {
        "surname": "Mirjalili",
        "given_name": "Seyedali"
      },
      {
        "surname": "Gandomi",
        "given_name": "Amir H."
      }
    ]
  },
  {
    "title": "Assessing a swarm-GAP based solution for the task allocation problem in dynamic scenarios",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113437",
    "abstract": "Swarm-GAP is a heuristic that combines a swarm intelligence strategy with the generalized assignment problem (GAP) method. This approach is especially appropriate when there are agents engaged in a collaborative task, but in general, heuristics have drawbacks to optimize resource allocation. A previous work proposed the usage of three swarm-GAP variants to solve the task allocation problem among agents representing a group of Unmanned Aerial Vehicles (UAVs) aiming at the optimization of their resources usage applied in the context of static environments. However, there is a lack of empirical assessment of these algorithms in dynamic scenarios, i.e., with some attributes changing along the system execution. Such changes represent important features of real-world application scenarios, such as in military operations in which a number of non-expected events may happen, e.g., loss of members of the UAV-team or onboard sensor failure. Therefore, the contributions of this work are the performance evaluation of the original algorithms in dynamic context, and the extension of these algorithms to properly address more realistic dynamic scenarios. Considering changes in some attributes of the environment, a trade-off in terms of the quality in the mission performance and the overhead in the communication among the UAVs is explored. The empirical assessment of the original algorithms and the proposed extensions were performed by conducting independent replications in a scenario where the number of agents (UAVs) changes at runtime and adaptations occur autonomously to maintain the mission execution. The acquired results provide evidence that the proposed solution is capable of dealing with dynamic scenarios, covering the gap left by other works in the literature, and enriching the realism of applications in autonomous intelligent systems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742030261X",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer network",
      "Computer science",
      "Context (archaeology)",
      "Distributed computing",
      "Engineering",
      "Heuristic",
      "Heuristics",
      "Machine learning",
      "Operating system",
      "Overhead (engineering)",
      "Paleontology",
      "Particle swarm optimization",
      "Resource allocation",
      "Swarm behaviour",
      "Swarm intelligence",
      "Systems engineering",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Amorim",
        "given_name": "Junier Caminha"
      },
      {
        "surname": "Alves",
        "given_name": "Vander"
      },
      {
        "surname": "de Freitas",
        "given_name": "Edison Pignaton"
      }
    ]
  },
  {
    "title": "Healthcare informatics and analytics in big data",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113388",
    "abstract": "Healthcare informatics and analytics (HCI&A), also known as healthcare information technology (HIT), healthcare IS (HIS), and so on, has rapidly evolved with the emerge of advanced data analytics technologies applied to the medical domain. Currently, HCI&A has emerged as an important area of study for both practitioners and academic researchers. Accordingly, this emerging field has prompted for an inquiry of the opportunities and challenges related to management of healthcare data, and the application of advanced data analytics to the contemporary healthcare industry. In order to contribute to the literature of healthcare informatics and analytics, this study proposes an HCI&A framework under the context of big data, which covers four important segments such as the underlying technologies, system applications, system evaluations, and emerging research areas. Based on the key features and capabilities of underpinning technologies, the evolution of HCI&A are conceptualized by three stages, namely HCI&A 1.0, HCI&A 2.0, and HCI&A 3.0. By analyzing the technological growth and current research trends, this study outlines the trend map of HCI&A for education and knowledge transfer. We also contributed to conduct a bibliographic study on healthcare informatics and healthcare information systems. To the best of our knowledge, our study is among the very few comprehensive bibliographic studies about HCI&A. We hope that our study can contribute to supplement contemporary thoughts on HCI&A research, and facilitate the related knowledge transfer to the healthcare industry.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420302128",
    "keywords": [
      "Analytics",
      "Big data",
      "Biology",
      "Business informatics",
      "Civil engineering",
      "Computer science",
      "Context (archaeology)",
      "Data mining",
      "Data science",
      "Electrical engineering",
      "Engineering",
      "Engineering informatics",
      "Health Administration Informatics",
      "Health care",
      "Health informatics",
      "Informatics",
      "Knowledge management",
      "Law",
      "Paleontology",
      "Political science",
      "Underpinning"
    ],
    "authors": [
      {
        "surname": "Pramanik",
        "given_name": "Md. Ileas"
      },
      {
        "surname": "Lau",
        "given_name": "Raymond Y.K."
      },
      {
        "surname": "Azad",
        "given_name": "Md. Abul Kalam"
      },
      {
        "surname": "Hossain",
        "given_name": "Md. Sakir"
      },
      {
        "surname": "Chowdhury",
        "given_name": "Md. Kamal Hossain"
      },
      {
        "surname": "Karmaker",
        "given_name": "B.K."
      }
    ]
  },
  {
    "title": "A co-evolutionary genetic algorithm for the two-machine flow shop group scheduling problem with job-related blocking and transportation times",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113360",
    "abstract": "This study investigates a new two-machine flow shop group scheduling problem with job-related blocking and transportation times, which is derived from the realistic pipe-making process of steel pipe products in the modern steel manufacturing industry. In contrast to the traditional blocking constraint, the attributes of jobs, not the quantity of jobs in the buffer area, are used to determine the need for a blocking feature. The objective is to minimize the makespan. We present a mixed integer linear programming model and prove that the problem is strongly NP-hard. As the problem is a joint decision of two sub-problems, namely group scheduling and job scheduling within each group, a co-evolutionary genetic algorithm (CGA) is proposed to solve it. In the proposed CGA, the two sub-problems are synergistically evolved via a co-evolutionary framework. A block-mining-based artificial chromosome construction strategy is designed to speed up the convergence process. Computational experiments based on actual production data are carried out. The results indicate that the proposed CGA is effective for the considered problem.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420301858",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Blocking (statistics)",
      "Computer network",
      "Computer science",
      "Evolutionary algorithm",
      "Flow shop scheduling",
      "Genetic algorithm",
      "Integer programming",
      "Job shop scheduling",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Operating system",
      "Schedule",
      "Scheduling (production processes)"
    ],
    "authors": [
      {
        "surname": "Yuan",
        "given_name": "Shuaipeng"
      },
      {
        "surname": "Li",
        "given_name": "Tieke"
      },
      {
        "surname": "Wang",
        "given_name": "Bailin"
      }
    ]
  },
  {
    "title": "Deep submodular network: An application to multi-document summarization",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113392",
    "abstract": "Employing deep learning makes it possible to learn high-level features from raw data, resulting in more precise models. On the other hand, submodularity makes the solution scalable and provides the means to guarantee a lower bound for its performance. In this paper, a deep submodular network (DSN) is introduced, which is a deep network meeting submodularity characteristics. DSN lets modular and submodular features to participate in constructing a tailored model that fits the best with a problem. Various properties of DSN are examined and its learning method is presented. By proving that cost function used for learning process is a convex function, it is concluded that minimization can be done in polynomial time and also, by choosing a suitable learning rate and performing enough iterations, a lower empirical error can be ensured. Finally, in order to demonstrate the applicability of DSN for real-world problems, automatic multi-document summarization is considered and a summarizer called DSNSum is introduced. Then, the performance of DSNSum is compared with the state-of-the-art summarizers based on DUC 2004 and CNN/DailyMail corpora. The experimental results show that the performance of the proposed summarizer is comparable with the state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420302165",
    "keywords": [
      "Artificial intelligence",
      "Automatic summarization",
      "Biology",
      "Computer science",
      "Database",
      "Deep learning",
      "Evolutionary biology",
      "Function (biology)",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Modular design",
      "Operating system",
      "Scalability",
      "Submodular set function",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Ghadimi",
        "given_name": "Alireza"
      },
      {
        "surname": "Beigy",
        "given_name": "Hamid"
      }
    ]
  },
  {
    "title": "A comparative evaluation of aggregation methods for machine learning over vertically partitioned data",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113406",
    "abstract": "It is increasingly common applications where data are naturally generated in a distributed fashion, especially after the emergence of technologies like the Internet of Things (IoT). In sensor networks, in collaborative health or genomic projects, in credit risk analysis, among other domains, distinct features are collected from multiple sources, including the use of social media and mobile applications, and due to privacy concerns or communication costs, may not be shared among sites. This scenario of vertical data partitioning poses challenges to traditional machine learning (ML) approaches, as classical algorithms are designed to learn from the complete set of features. A common strategy is to combine predictions from local models trained at each site into a global model, and for this purpose, several aggregation methods have been proposed. In this work we tackle a gap within the related literature, performing a comparative evaluation of elementary and meta-learning-based aggregation methods to reveal their strengths and weakness for 46 datasets with varied characteristics. We show that no method outperforms its counterparts in all domains, emphasizing the need for experimental comparison to ensure a good choice in the domain of interest. Moreover, our experiments provide the first insights into the relations between datasets’ properties and aggregators’ performance. We show that for low class imbalance and a good instance-to-feature ratio, almost all aggregation methods tend to perform well. The silhouette coefficient (reflecting class separability) and class imbalance coefficient are the most influential properties on aggregators’ performance, thus we recommend their analysis in the first step of the methodological design. We found that arithmetic-based methods are not suitable for datasets with poor class separability and a large number of classes, whereas meta-learning approaches are less sensitive for datasets with silhouette coefficient close to 0. Our analyses were summarized as classification and regression trees, which have the impact to serve as practical tools for future research. Taken together, our findings give rise to interesting applications in the domain of intelligent systems, especially regarding their potential to reduce the burden of vast experimental comparisons when training ML models with feature-partitioned data.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742030230X",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer network",
      "Computer science",
      "Data aggregator",
      "Data mining",
      "Data science",
      "Domain (mathematical analysis)",
      "Feature (linguistics)",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Philosophy",
      "Programming language",
      "Set (abstract data type)",
      "Wireless sensor network"
    ],
    "authors": [
      {
        "surname": "Trevizan",
        "given_name": "Bernardo"
      },
      {
        "surname": "Chamby-Diaz",
        "given_name": "Jorge"
      },
      {
        "surname": "Bazzan",
        "given_name": "Ana L.C."
      },
      {
        "surname": "Recamonde-Mendoza",
        "given_name": "Mariana"
      }
    ]
  },
  {
    "title": "An efficient henry gas solubility optimization for feature selection",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113364",
    "abstract": "In classification, regression, and other data mining applications, feature selection (FS) is an important pre-process step which helps avoid advert effect of noisy, misleading, and inconsistent features on the model performance. Formulating it into a global combinatorial optimization problem, researchers have employed metaheuristic algorithms for selecting the prominent features to simplify and enhance the quality of the high-dimensional datasets, in order to devise efficient knowledge extraction systems. However, when employed on datasets with extensively large feature-size, these methods often suffer from local optimality problem due to considerably large solution space. In this study, we propose a novel approach to dimensionality reduction by using Henry gas solubility optimization (HGSO) algorithm for selecting significant features, to enhance the classification accuracy. By employing several datasets with wide range of feature size, from small to massive, the proposed method is evaluated against well-known metaheuristic algorithms including grasshopper optimization algorithm (GOA), whale optimization algorithm (WOA), dragonfly algorithm (DA), grey wolf optimizer (GWO), salp swarm algorithm (SSA), and others from recent relevant literature. We used k-nearest neighbor (k-NN) and support vector machine (SVM) as expert systems to evaluate the selected feature-set. Wilcoxon’s ranksum non-parametric statistical test was carried out at 5% significance level to judge whether the results of the proposed algorithms differ from those of the other compared algorithms in a statistically significant way. Overall, the empirical analysis suggests that the proposed approach is significantly effective on low, as well as, considerably high dimensional datasets, by producing 100% accuracy on classification problems with more than 11,000 features.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420301895",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Curse of dimensionality",
      "Data mining",
      "Feature (linguistics)",
      "Feature selection",
      "Linguistics",
      "Machine learning",
      "Metaheuristic",
      "Particle swarm optimization",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Neggaz",
        "given_name": "Nabil"
      },
      {
        "surname": "Houssein",
        "given_name": "Essam H."
      },
      {
        "surname": "Hussain",
        "given_name": "Kashif"
      }
    ]
  },
  {
    "title": "Structural optimization of fuzzy rule-based models: Towards efficient complexity management",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113362",
    "abstract": "The primary aim of this study is concerned with the structural optimization of data-driven fuzzy rule-based systems (FRBS), with the intent of their complexity management. This is accomplished in two ways: the first one involves a structuralization of the antecedents and the second one deals with a structuralization of the consequents of the fuzzy rules. More specifically, this study contributes to the complexity management of fuzzy models by focusing on (i) the efficient arrangement (reduction) of the input spaces over which the antecedents of rules are formed and (ii) allocating the orders of local polynomial functions across the consequents of the rules. The originality of the study comes with the flexibility of FRBS that is endowed by admitting variability of input spaces standing in the antecedents of different rules as well as the variability of orders of polynomials (local functions) forming the consequents of the rules. Particle swarm optimization (PSO) is guided by the root mean squared error (RMSE) accuracy criterion to realize the efficient arrangement of input spaces and an allocation of the orders of the individual polynomials. In this optimization process, the Fuzzy C-Means (FCM) algorithm is employed to create fuzzy sets in the antecedents of the rules, while the standard Least Square Error (LSE) criterion is minimized to determine the coefficients of the polynomials in the consequents. The performance of the proposed model is quantified using some numeric data, including both synthetic and machine learning datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420301871",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Flexibility (engineering)",
      "Fuzzy logic",
      "Fuzzy number",
      "Fuzzy rule",
      "Fuzzy set",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Mean squared error",
      "Operating system",
      "Particle swarm optimization",
      "Polynomial",
      "Process (computing)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Mamaghani",
        "given_name": "Ali Safari"
      },
      {
        "surname": "Pedrycz",
        "given_name": "Witold"
      }
    ]
  },
  {
    "title": "Enhancing random projection with independent and cumulative additive noise for privacy-preserving data stream mining",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113380",
    "abstract": "The sensitive nature of many data streams necessitates data mining techniques that are privacy-preserving. This paper proposes two data perturbation methods for privacy-preserving stream mining based on a combination of random projection, random translation, and two alternative forms of additive noise: noise generated independently for each record and noise that accumulates over the lifetime of a stream. Variations of the known input-output Maximum A Posteriori (MAP) attack that can account for the combinations of perturbation techniques are proposed as a means of evaluating the privacy guarantees of the proposed perturbation methods. The capabilities of the proposed methods to resist privacy-breaching recovery attacks and retain accuracy in models trained on perturbed data are experimentally evaluated. Experimentation revealed that the cumulative noise injection scheme outperformed other schemes by achieving a superior trade-off between privacy and classification.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420302049",
    "keywords": [
      "A priori and a posteriori",
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Data mining",
      "Data stream",
      "Data stream mining",
      "Epistemology",
      "Image (mathematics)",
      "Information privacy",
      "Noise (video)",
      "Perturbation (astronomy)",
      "Philosophy",
      "Physics",
      "Projection (relational algebra)",
      "Quantum mechanics",
      "Random noise",
      "Random projection",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Denham",
        "given_name": "Benjamin"
      },
      {
        "surname": "Pears",
        "given_name": "Russel"
      },
      {
        "surname": "Naeem",
        "given_name": "M. Asif"
      }
    ]
  },
  {
    "title": "Adaptive sampling using self-paced learning for imbalanced cancer data pre-diagnosis",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113334",
    "abstract": "The early diagnosis of cancer diseases is an indispensable part in the cancer research. It urges people to develop many new machine learning approaches to assist the diseases identification based on the gene expression data. However, the race occurrence of malignant tumors creates a challenge due to the potential over-fitting risk in the current model training. Typically, people use various sampling methods (e.g., random oversampling and undersampling) to address this challenge to provide a balanced data distribution. However, these methods might discard potentially useful samples. In this paper, we proposed an imbalanced sampling approach via self-paced learning (ISPL) to effectively select high-quality samples to improve the robustness. The experimental results showed that our proposed ISPL method increased the classification accuracy by approximately 16% compared with the average performance obtained by other sampling methods. In addition, the new method successfully selected some important genes for further investigation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420301597",
    "keywords": [
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Biochemistry",
      "Chemistry",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Data sampling",
      "Filter (signal processing)",
      "Gene",
      "Machine learning",
      "Oversampling",
      "Random forest",
      "Robustness (evolution)",
      "Sampling (signal processing)",
      "Support vector machine",
      "Undersampling"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Qingyong"
      },
      {
        "surname": "Zhou",
        "given_name": "Yun"
      },
      {
        "surname": "Zhang",
        "given_name": "Weiming"
      },
      {
        "surname": "Tang",
        "given_name": "Zhangui"
      },
      {
        "surname": "Chen",
        "given_name": "Xiaojing"
      }
    ]
  },
  {
    "title": "A novel weighted TPR-TNR measure to assess performance of the classifiers",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113391",
    "abstract": "Assessing performance of different classifiers and selecting the best one is one of the most important tasks in classification problem. The assessment of classifiers becomes more complex when dataset is imbalanced because most of the frequently used performance metrics can be misleading. Many real world classification problems such as fraud detection, churn prediction, medical diagnosis, and cyber-security suffer from the problem of imbalanced datasets. Therefore, in all such classification tasks it is very important to select the best classifier very carefully. In this study we propose new weighted TPR-TNR measure to assess performance of the classifiers. The proposed measure takes into consideration imbalance ratio of the dataset and assign different weights to the TPR and TNR to assess classifiers performance. We have used five different datasets to assess performance of twelve different classifiers using weighted TPR-TNR measure and compared it with the existing measures. The experimental results show that the weighted TPR-TNR measure is more suitable to assess performance of the classifiers when dataset is imbalanced.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420302153",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Machine learning",
      "Measure (data warehouse)",
      "Pattern recognition (psychology)",
      "Random subspace method"
    ],
    "authors": [
      {
        "surname": "Jadhav",
        "given_name": "Anil S."
      }
    ]
  },
  {
    "title": "Semantic-k-NN algorithm: An enhanced version of traditional k-NN algorithm",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113374",
    "abstract": "The k-NN algorithm is one of the most renowned ML algorithms widely used in the area of data classification research. With the emergence of big data, the performance and the efficiency of the traditional k-NN algorithm is fast becoming a critical issue. The traditional k-NN algorithm is inefficient to solve the high volume multi-categorical training datasets Traditional k-NN algorithm has a constraint in filtering the training dataset to yield training data that are most relevant to the intended or the targeted test dataset/file. It has to scan through all the training datasets categories to classify the intended/targeted data. As such, traditional k-NN is considered not intelligent and consequently is suffering poor accuracy performance with high computational complexity. A Semantic-kNN (Sk-NN) algorithm for ML is thus proposed in this paper to address the limitations in the traditional k-NN. The proposed Sk-NN deploys a process by leveraging on the semantic itemization and bigram model to filter the training dataset in accordance with the relevant information engaged in the test dataset. It is aimed for general security applications such as finding (the confidentiality level of the data when the algorithm is trained with multiple training categories during the data classification phase. Ultimately, Sk-NN is to elevate the ML performance in pattern extraction and labeling in the big data context.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420301998",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Big data",
      "Bigram",
      "Biology",
      "Categorical variable",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Data mining",
      "Filter (signal processing)",
      "Machine learning",
      "Paleontology",
      "Programming language",
      "Test data",
      "Trigram"
    ],
    "authors": [
      {
        "surname": "Ali",
        "given_name": "Munwar"
      },
      {
        "surname": "Jung",
        "given_name": "Low Tang"
      },
      {
        "surname": "Abdel-Aty",
        "given_name": "Abdel-Haleem"
      },
      {
        "surname": "Abubakar",
        "given_name": "Mustapha Y."
      },
      {
        "surname": "Elhoseny",
        "given_name": "Mohamed"
      },
      {
        "surname": "Ali",
        "given_name": "Irfan"
      }
    ]
  },
  {
    "title": "Discovering patterns of online popularity from time series",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113337",
    "abstract": "How is popularity gained online? Is being successful strictly related to rapidly becoming viral in an online platform, or is it possible to acquire popularity in a steady and disciplined fashion? What are other temporal characteristics that can unveil the popularity of online content? To answer these questions, we leverage a multifaceted temporal analysis of the evolution of popular online content. We present dipm-SC: a multidimensional shape-based time-series clustering algorithm with a heuristic to find the optimal number of clusters. First, we validate the accuracy of our algorithm on synthetic datasets generated from benchmark time series models. Second, we show that dipm-SC can uncover meaningful clusters of popularity behaviors in real-world GitHub and Twitter datasets. By clustering the multidimensional time-series of the popularity of contents coupled with other domain-specific dimensions, we discover two main patterns of popularity: bursty and steady temporal behaviors. Furthermore, we find that the way popularity is gained over time has no significant impact on the final cumulative popularity.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420301627",
    "keywords": [],
    "authors": [
      {
        "surname": "Ozer",
        "given_name": "Mert"
      },
      {
        "surname": "Sapienza",
        "given_name": "Anna"
      },
      {
        "surname": "Abeliuk",
        "given_name": "Andrés"
      },
      {
        "surname": "Muric",
        "given_name": "Goran"
      },
      {
        "surname": "Ferrara",
        "given_name": "Emilio"
      }
    ]
  },
  {
    "title": "A comparison of clustering algorithms for automatic modulation classification",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113317",
    "abstract": "In this paper, the k-means, k-medoids, fuzzy c-means, Density-Based Spatial Clustering of Applications with Noise (DBSCAN), Ordering Points To Identify the Clustering Structure (OPTICS), and hierarchical clustering algorithms (with the addition of the elbow method) are examined for the purpose of Automatic Modulation Classification (AMC). This study compares these algorithms in terms of classification accuracy and execution time for either estimating the modulation order, determining centroid locations, or both. The best performing algorithms are combined to provide a simple AMC method which is then evaluated in an Additive White Gaussian Noise (AWGN) channel with M-Quadrature Amplitude Modulation (QAM) and M-Phase Shift Keying (PSK). Such an AMC method does not rely on any thresholds to be set by a human or machine learning algorithm, resulting in a highly flexible system. The proposed method can be configured to not give false positives, making it suitable for applications such as spectrum monitoring and regulatory enforcement.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420301421",
    "keywords": [
      "Additive white Gaussian noise",
      "Algorithm",
      "Artificial intelligence",
      "Bit error rate",
      "Canopy clustering algorithm",
      "Centroid",
      "Channel (broadcasting)",
      "Cluster analysis",
      "Computer science",
      "DBSCAN",
      "Data mining",
      "Decoding methods",
      "Fuzzy clustering",
      "Fuzzy logic",
      "Pattern recognition (psychology)",
      "Phase-shift keying",
      "Quadrature amplitude modulation",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Mouton",
        "given_name": "Jacques P."
      },
      {
        "surname": "Ferreira",
        "given_name": "Melvin"
      },
      {
        "surname": "Helberg",
        "given_name": "Albertus S.J."
      }
    ]
  },
  {
    "title": "Item recommendation by predicting bipartite network embedding of user preference",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113339",
    "abstract": "With the development of e-commerce, various methodologies have studied to improve recommendation performance. Recently, many deep learning based network embedding approaches are applied to the recommendation domain. However, these approaches still have several limitations, such as the problem of data sparseness and the changing in user preference over time, which cannot be considered. In this paper, we propose a novel method for item recommendation based on network embedding. First, we apply a bipartite network embedding to address the data sparsity problem. Bipartite network embedding is a vector representation method that reflects explicit (i.e., observed data) and implicit relations (i.e., unobserved data). Bipartite network embedding methodology can address the data sparsity problem by using implicit relationship information from applying the random walk approach. Second, we predict future bipartite network embedding of user preference by adopting a Kalman filter to consider the changes in user preferences. We have conducted experiments to evaluate the effectiveness and performance of the proposed recommendation method. Through experimentation, the proposed recommendation method is validated as outperforming than the existing approaches including existing network embedding methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420301640",
    "keywords": [
      "Artificial intelligence",
      "Bipartite graph",
      "Collaborative filtering",
      "Computer science",
      "Data mining",
      "Embedding",
      "Graph",
      "Information retrieval",
      "Law",
      "Machine learning",
      "Mathematics",
      "Political science",
      "Politics",
      "Preference",
      "Recommender system",
      "Representation (politics)",
      "Statistics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Yoon",
        "given_name": "Yiyeon"
      },
      {
        "surname": "Hong",
        "given_name": "Juneseok"
      },
      {
        "surname": "Kim",
        "given_name": "Wooju"
      }
    ]
  },
  {
    "title": "Detection of malicious social bots: A survey and a refined taxonomy",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113383",
    "abstract": "Social bots represent a new generation of bots that make use of online social networks (OSNs) as command and control (C&C) channels. Malicious social bots have been used as tools for launching large-scale spam campaigns, promoting low-cap stocks, manipulating users’ digital influence, and conducting political astroturfing. Recent studies in this area either focus only on general security issues related to social networks or on coarse-grained categorization to support detection approaches. This survey aims to provide a comprehensive analysis from a social network perspective. To this end, we first categorize social bot attacks at different stages, then provide an overview of different types of social bots. Next, we propose a refined taxonomy that shows how different techniques within a category are related or differ from each other, followed by a detailed discussion of the strengths and limitations of each method. Following this, we review the existing datasets and summarize the results of empirical investigations. Finally, we highlight the limitations of existing detection approaches and suggest future directions for further improvement. Our study should help OSN administrators and researchers understand the destructive potential of malicious social bots and improve upon the current defensive strategies.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420302074",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Categorization",
      "Computer science",
      "Computer security",
      "Data science",
      "Focus (optics)",
      "Honeypot",
      "Internet privacy",
      "Optics",
      "Physics",
      "Taxonomy (biology)"
    ],
    "authors": [
      {
        "surname": "Latah",
        "given_name": "Majd"
      }
    ]
  },
  {
    "title": "Selective Opposition based Grey Wolf Optimization",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113389",
    "abstract": "The use of metaheuristics is widespread for optimization in both scientific and industrial problems due to several reasons, including flexibility, simplicity, and robustness. Grey Wolf Optimizer (GWO) is one of the most recent and popular algorithms in this area. In this work, opposition-based learning (OBL) is combined with GWO to enhance its exploratory behavior while maintaining a fast convergence rate. Spearman's correlation coefficient is used to determine the omega (ω) wolves (wolves with the lowest social status in the pack) on which to perform opposition learning. Instead of opposing all the dimensions in the wolf, a few dimensions of the wolf are selected on which opposition is applied. This assists with avoiding unnecessary exploration and achieving a fast convergence without deteriorating the probability of finding optimum solutions. The proposed algorithm is tested on 23 optimization functions. An extensive comparative study demonstrates the superiority of the proposed method. The source code for this algorithm is available at \"https://github.com/dhargupta-souvik/sogwo\"",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742030213X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Law",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Metaheuristic",
      "Opposition (politics)",
      "Optimization algorithm",
      "Optimization problem",
      "Political science",
      "Politics"
    ],
    "authors": [
      {
        "surname": "Dhargupta",
        "given_name": "Souvik"
      },
      {
        "surname": "Ghosh",
        "given_name": "Manosij"
      },
      {
        "surname": "Mirjalili",
        "given_name": "Seyedali"
      },
      {
        "surname": "Sarkar",
        "given_name": "Ram"
      }
    ]
  },
  {
    "title": "Detecting clickbaits using two-phase hybrid CNN-LSTM biterm model",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113350",
    "abstract": "Clickbait indicates the type of content with an intending goal to attract the attention of readers. It has grown to become a nuisance to social media users. The purpose of clickbait is to bring an appealing link in front of users. Clickbaits seen in the form of headlines influence people to get attracted and curious to read the inside content. The content seen in the form of text on clickbait posts is very short to identify its features as clickbait. In this paper, a novel approach (two-phase hybrid CNN-LSTM Biterm model) has been proposed for modeling short topic content. The hybrid CNN-LSTM model when implemented with pre-trained GloVe embedding yields the best results based on accuracy, recall, precision, and F1-score performance metrics. The proposed model achieves 91.24%, 95.64%, 95.87% precision values for Dataset 1, Dataset 2 and Dataset 3, respectively. Eight types of clickbait such as Reasoning, Number, Reaction, Revealing, Shocking/Unbelievable, Hypothesis/Guess, Questionable, Forward referencing are classified in this work using the Biterm Topic Model (BTM). It has been shown that the clickbaits such as Shocking/Unbelievable, Hypothesis/Guess and Reaction are the highest in numbers among rest of the clickbait headlines published online. Also, a ground dataset of non-textual (image-based) data using multiple social media platforms has been created in this paper. The textual information has been retrieved from the images with the help of OCR tool. A comparative study is performed to show the effectiveness of our proposed model which helps to identify the various categories of clickbait headlines that are spread on social media platforms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420301755",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Embedding",
      "Information retrieval",
      "Linguistics",
      "Machine learning",
      "Natural language processing",
      "Philosophy",
      "Recall",
      "Social media",
      "Topic model",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Kaur",
        "given_name": "Sawinder"
      },
      {
        "surname": "Kumar",
        "given_name": "Parteek"
      },
      {
        "surname": "Kumaraguru",
        "given_name": "Ponnurangam"
      }
    ]
  },
  {
    "title": "Locality adaptive preserving projections for linear dimensionality reduction",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113352",
    "abstract": "Dimensionality reduction techniques aim to transform the high-dimensional data into a meaningful reduced representation and have been consistently playing a fundamental role in the study of intrinsic dimensionality estimation and the design of an intelligent expert system towards real-world applications. From the perspective of manifold learning, locality preserving projections is a classical and commonly used dimensionality reduction method and it essentially learns the low-dimensional embedding under the constraint of preserving the local geometry of data. However, since it determines the neighborhood relationships in the original feature space that probably contains noisy and irrelevant features, the derived similarity between the neighbors are unreliable and the corresponding local data manifold tends to be error-prone, which inevitably leads to degraded performance for subsequent data analyses. Hence, how to accurately identify the true neighbor relationships for each sample remains crucial to the robustness improvement. In this work, we propose a novel approach, termed locality adaptive preserving projections (LAPP), to adaptively determine the neighbors and their relationships in the optimal subspace rather than in the original space. Specifically, due to the absence of prior knowledge of local properties of the underlying manifold, LAPP adopts a coarse-to-fine strategy to iteratively update the projected low-dimensional subspace and optimize the identification of the local structure of the data. Moreover, an iterative algorithm with fast convergence is utilized to solve the transformation matrix for explicit out-of-sample extension. Besides, LAPP is easy to implement and its key idea can be potentially extended to other methods for neighbor-finding and similarity measurement. To evaluate the performance of LAPP, we conduct comparative experiments on numerous synthetic and real-world datasets. Experimental results show that seeking the local structure in the original feature space misleads the selection of neighbors and the calculation of similarity and that the proposed method helps alleviate the negative effect of noisy and irrelevant features, which demonstrates its effectiveness. Besides, this study has the potential to enlighten relevant studies to consider the problem of optimizing the neighborhood relationships.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420301779",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Curse of dimensionality",
      "Dimensionality reduction",
      "Embedding",
      "Engineering",
      "Gene",
      "Linguistics",
      "Locality",
      "Manifold (fluid mechanics)",
      "Mechanical engineering",
      "Nonlinear dimensionality reduction",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Robustness (evolution)",
      "Subspace topology"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Aiguo"
      },
      {
        "surname": "Zhao",
        "given_name": "Shenghui"
      },
      {
        "surname": "Liu",
        "given_name": "Jinjun"
      },
      {
        "surname": "Yang",
        "given_name": "Jing"
      },
      {
        "surname": "Liu",
        "given_name": "Li"
      },
      {
        "surname": "Chen",
        "given_name": "Guilin"
      }
    ]
  },
  {
    "title": "Subject-specific identification of three dimensional foot shape deviations using statistical shape analysis",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113372",
    "abstract": "The high prevalence of foot pain, and its relation to foot shape, indicates the need for an expert system to identify foot shape abnormalities. Yet, to date, no such expert system exists that examines the full 3D foot shape and produces an intuitive explanation of why a foot is abnormal. In this work, we present the first such expert system that satisfies those goals. The system is based on the concept of model-based outlier detection: a statistical shape model (SSM) is generated from 186 3D optical foot scans of healthy feet. This model acts as a knowledge base which is then parameterized by one’s demographic characteristics (e.g., age, weight, height, shoe size) through a multivariate regression. This regression introduces model flexibility as it allows the model to be fine tuned to a specific individual. This fine tuned model is then used as a baseline to which the individual’s 3D foot scan can be compared using standard statistical tests (e.g. t-tests). These statistical tests are performed at each vertex along the foot surface to identify the degree and location of shape outliers. Our expert system was validated on foot scans from patients with hallux valgus and abnormal foot arches. As expected, our results varied per patient, confirming that feet with the same clinical classification still show high shape variability. Additionally, the foot shape abnormalities identified by our system not only agreed with the expected location and severity of the tested foot deformities, but our analysis of the full 3D foot shape was able to completely characterize the extent of those abnormalities for the first time. These results show that the combination of statistical shape modelling, multivariate regression, and statistical testing is powerful enough to perform outlier detection for 3D foot shapes. The resulting insights provided by this system could prove useful in both shoe design and clinical diagnosis.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420301974",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Foot (prosody)",
      "Linguistics",
      "Medicine",
      "Orthodontics",
      "Outlier",
      "Parameterized complexity",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Segmentation",
      "Valgus"
    ],
    "authors": [
      {
        "surname": "Stanković",
        "given_name": "Kristina"
      },
      {
        "surname": "Huysmans",
        "given_name": "Toon"
      },
      {
        "surname": "Danckaers",
        "given_name": "Femke"
      },
      {
        "surname": "Sijbers",
        "given_name": "Jan"
      },
      {
        "surname": "Booth",
        "given_name": "Brian G."
      }
    ]
  },
  {
    "title": "An experimental evaluation of mixup regression forests",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113376",
    "abstract": "Over the past few decades, the remarkable prediction capabilities of ensemble methods have been used within a wide range of applications. Maximization of base-model ensemble accuracy and diversity are the keys to the heightened performance of these methods. One way to achieve diversity for training the base models is to generate artificial/synthetic instances for their incorporation with the original instances. Recently, the mixup method was proposed for improving the classification power of deep neural networks (Zhang, Cissé, Dauphin, and Lopez-Paz, 2017). Mixup method generates artificial instances by combining pairs of instances and their labels, these new instances are used for training the neural networks promoting its regularization. In this paper, new regression tree ensembles trained with mixup, which we will refer to as Mixup Regression Forest, are presented and tested. The experimental study with 61 datasets showed that the mixup approach improved the results of both Random Forest and Rotation Forest.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420302013",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Composite material",
      "Computer science",
      "Data mining",
      "Decision tree",
      "Ensemble learning",
      "Machine learning",
      "Materials science",
      "Mathematical optimization",
      "Mathematics",
      "Maximization",
      "Random forest",
      "Range (aeronautics)",
      "Regression",
      "Regularization (linguistics)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Rodríguez",
        "given_name": "Juan J."
      },
      {
        "surname": "Juez-Gil",
        "given_name": "Mario"
      },
      {
        "surname": "Arnaiz-González",
        "given_name": "Álvar"
      },
      {
        "surname": "Kuncheva",
        "given_name": "Ludmila I."
      }
    ]
  },
  {
    "title": "Geographic-aware collaborative filtering for web service recommendation",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113347",
    "abstract": "The explosion of reusable Web services (e.g., open APIs, open data sources, and cloud/IoT services), has become a new opportunity for modern service-composition based applications development. However, this enormous growth of Web services increases the difficulty of selecting the best suitable Web services for a particular application. Hence, the design of an effective and efficient Web service recommendation, primarily based on user feedback, has become a challenge. In the mashup-API recommendation scenario, the most available feedback is the implicit invocation data, i.e., the binary data indicating whether or not a mashup has invoked an API. Various efforts are exploiting potential impact factors, such as the invocation context, to augment the implicit invocation data with the aim to improve service recommendation performance. One significant factor affecting the context of Web service invocations is geographical location, but it has been given less attention in the implicit-based service recommendation. In this paper, we propose a probabilistic matrix factorization based recommendation approach, which considers geographic location information in the derivation of the preference degree underlying a mashup-API interaction. The geographic information, which is integrated with functional descriptions, complements the mashup-API invocation data input for our matrix factorization model. We demonstrate the effectiveness of our approach by conducting extensive experiments on a real dataset crawled from ProgrammableWeb. The evaluation results show that augmenting the implicit data with geographical location information increases the precision of API recommendation for mashup services.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742030172X",
    "keywords": [
      "Biology",
      "Cloud computing",
      "Collaborative filtering",
      "Computer science",
      "Context (archaeology)",
      "Database",
      "Economics",
      "Economy",
      "Information retrieval",
      "Mashup",
      "Operating system",
      "Paleontology",
      "Recommender system",
      "Service (business)",
      "Web modeling",
      "Web service",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Botangen",
        "given_name": "Khavee Agustus"
      },
      {
        "surname": "Yu",
        "given_name": "Jian"
      },
      {
        "surname": "Sheng",
        "given_name": "Quan Z."
      },
      {
        "surname": "Han",
        "given_name": "Yanbo"
      },
      {
        "surname": "Yongchareon",
        "given_name": "Sira"
      }
    ]
  },
  {
    "title": "Forecasting model selection using intermediate classification: Application to MonarchFx corporation",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113371",
    "abstract": "Organizations rely on accurate demand forecasts to make production and ordering decisions in a variety of supply chain positions. Significant research in time series forecasting techniques and a variety of forecasting methods are available in the market. However, selecting the most accurate forecasting model for a given time series has become a complicated decision. Prior studies of forecasting methods have used either in-sample or out-of-sample performance as the basis for model selection procedures, but typically fail to incorporate both in their decision-making framework. In this research, we develop an expert system for time series forecasting model selection, using both relative in-sample performance and out-of-sample performance simultaneously to train classifiers. These classifiers are employed to automatically select the best performing forecasting model without the need for decision-maker intervention. The new model selection scheme bridges the gap between using in-sample and out-of-sample performance separately. The best performing model on the validation set is not necessarily selected by the expert system, since both in-sample and out-of-sample information are essential in the selection process. The performance of the proposed expert system is tested using the monthly dataset from the M3-Competition, and the results demonstrate an overall minimum of 20% improvement in the optimality gap comparing to the train/validation method. The new forecasting expert system is also applied to a real case study dataset obtained from MonarchFx (a distributed logistics solutions provider). This result demonstrates a robust predictive capability with lower mean squared errors, which allows organizations to achieve a higher level of accuracy in demand forecasts.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420301962",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Data mining",
      "Demand forecasting",
      "Engineering",
      "Machine learning",
      "Model selection",
      "Operations research",
      "Programming language",
      "Sample (material)",
      "Selection (genetic algorithm)",
      "Set (abstract data type)",
      "Variety (cybernetics)"
    ],
    "authors": [
      {
        "surname": "Taghiyeh",
        "given_name": "Sajjad"
      },
      {
        "surname": "Lengacher",
        "given_name": "David C."
      },
      {
        "surname": "Handfield",
        "given_name": "Robert B."
      }
    ]
  },
  {
    "title": "Light field reconstruction using hierarchical features fusion",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113394",
    "abstract": "Light field imagery has attracted increasing attention for its capacity of simultaneously capturing intensity values of light rays from multiple directions. Such imagery technique has become widely accessible with the emergence of consumer-grade devices, e.g. Lytro, and the Virtual Reality (VR) / Augmented Reality (AR) areas. Light field reconstruction is a critical topic to mitigate the trade-off problem between the spatial and angular resolutions. Learning-based methods have attained outstanding performance among the recently proposed methods, however, the state-of-the-art methods still suffer from heavy artifacts in the case of occlusion. This is likely to be a consequence of failure in capturing the semantic information from the limited spatial receptive field during training. It is crucial for light field reconstruction to learn semantic features and understand a wider context in both the angular and spatial dimensions. To address this issue, we introduce a novel end-to-end U-Net with SAS network (U-SAS-Net) to extract and fuse hierarchical features, both local and semantic, from a relatively large receptive field while establishing the relation of the correlated sub-aperture images. Experimental results on extensive light field datasets demonstrate that our method produces a state-of-the-art performance that exceeds the previous works by more than 0.6 dB PSNR with the fused hierarchical features, especially the semantic features for handling scenes with occlusion and the local features for recovering the rich details. Meanwhile, our method is at a substantially lower cost which takes 48% parameters and less than 10% computation of the previous state-of-the-art method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420302189",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Field (mathematics)",
      "Fusion",
      "Light field",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pure mathematics"
    ],
    "authors": [
      {
        "surname": "Hu",
        "given_name": "Zexi"
      },
      {
        "surname": "Chung",
        "given_name": "Yuk Ying"
      },
      {
        "surname": "Ouyang",
        "given_name": "Wanli"
      },
      {
        "surname": "Chen",
        "given_name": "Xiaoming"
      },
      {
        "surname": "Chen",
        "given_name": "Zhibo"
      }
    ]
  },
  {
    "title": "Hesitant fuzzy numbers with (α, k)-cuts in compact intervals and applications",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113363",
    "abstract": "In this paper, we propose a new definition of hesitant fuzzy numbers (HFNs) and study some essential properties of these numbers. We show (α, k)-cuts that were discussed in the recent literature for hesitant fuzzy sets (HFSs), on HFNs have resulted in compact intervals. In the following, we propose a new binary operation on these numbers. It has shown that the outcome of the proposed operation is a HFN. In addition, a new hesitant fuzzy relationship for comparing two HFNs is given. Finally, some applications of these numbers are presented in two examples. For this purpose, we propose a new approach to solve linear programming with hesitant fuzzy parameters.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420301883",
    "keywords": [
      "Algorithm",
      "Arithmetic",
      "Artificial intelligence",
      "Binary number",
      "Computer science",
      "Discrete mathematics",
      "Fuzzy logic",
      "Fuzzy number",
      "Fuzzy set",
      "Mathematical optimization",
      "Mathematics"
    ],
    "authors": [
      {
        "surname": "Ranjbar",
        "given_name": "Mahdi"
      },
      {
        "surname": "Miri",
        "given_name": "Seyed Mohsen"
      },
      {
        "surname": "Effati",
        "given_name": "Sohrab"
      }
    ]
  },
  {
    "title": "Modeling methodology for early warning of chronic heart failure based on real medical big data",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113361",
    "abstract": "Heart failure (HF) is among the most costly diseases to our society, and the prevalence keeps on increasing these days. Early detection of HF plays a vital role in saving lives through adjusting lifestyles and drug interventions that can slow down disease progression or prevent HF. There are many cardiovascular risk factors associated with HF, and they often coexist. In this paper, we assess the predictive value of pathological factors for early HF detection through a social network based approach. We use electronic health records (collected from the project HeartCarer) and compute the similarity of risk factors. The similarity values are used to construct an unweighted and a weighted medical social network. The constructed medical social network is further divided into a HF high-risk group and HF low-risk group using a group division algorithm. Patients in the high-risk group will be suggested for early screening. To evaluate the prediction value of our method, we perform four experiments based on real world data. The results demonstrate the high effectiveness of our method on heart failure risk assessment, with the best accuracy close to 90%.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742030186X",
    "keywords": [
      "Artificial intelligence",
      "Big data",
      "Computer science",
      "Construct (python library)",
      "Data mining",
      "Disease",
      "Heart failure",
      "Image (mathematics)",
      "Internal medicine",
      "Medicine",
      "Programming language",
      "Psychiatry",
      "Psychological intervention",
      "Similarity (geometry)",
      "Telecommunications",
      "Warning system"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Chunjie"
      },
      {
        "surname": "Li",
        "given_name": "Ali"
      },
      {
        "surname": "Hou",
        "given_name": "Aihua"
      },
      {
        "surname": "Zhang",
        "given_name": "Zhiwang"
      },
      {
        "surname": "Zhang",
        "given_name": "Zhenxing"
      },
      {
        "surname": "Dai",
        "given_name": "Pengfei"
      },
      {
        "surname": "Wang",
        "given_name": "Fusheng"
      }
    ]
  },
  {
    "title": "Multitaper-based method for automatic k-complex detection in human sleep EEG",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113331",
    "abstract": "In this paper, we propose a novel method for automatic k-complex (KC) detection in human sleep EEG, named MT-KCD. KCs are slow oscillations in the EEG signal characterized by a well-delineated, negative, sharp waves immediately followed by a positive component standing out from the background, with high-amplitude and total duration ≥ 0.5 s. Among the important aspects of the KC are its homeostatic and reactive functions in the brain, functioning as a sleep protection mechanism, and its practical use as a marker of N2 sleep stage during sleep studies. Given the importance of the KC, and the effort required from human experts to analyze EEG recordings visually, some recent research works have proposed automatic methods for KC detection. In comparison with existing methods, a key feature and novelty of MT-KCD is the use of multitaper spectral analysis to pre-process the EEG signal and automatically extract candidate KCs from it (characterized as 0-4 Hz power concentrations standing out from the background). After extraction, candidates are accepted/rejected depending on time domain characteristics (peak-to-peak amplitude ≥ 75 µV, duration ≤ 2 s). The method overall time complexity is O ( N · log N ) . Regarding effectiveness, we have evaluated MT-KCD by using a public KC database (DREAMS) consisting of ten polysomnographic recordings of healthy patients (6 female and 4 male subjects with age range 20–47 years) partially annotated by two experts. Results have shown that MT-KCD improves detection metrics, especially F1 and F2 scores (harmonic averages of recall and precision), when compared to existing methods. Besides, improving F1 and F2 scores, MT-KCD also contributes to the automatic analysis of sleep EEG multitaper spectrograms, a technique recently proposed by researchers in the area of sleep studies as a complement to the traditional hypnogram (sleep stages diagram).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420301561",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Electroencephalography",
      "Feature extraction",
      "Multitaper",
      "Neuroscience",
      "Novelty",
      "Operating system",
      "Pattern recognition (psychology)",
      "Polysomnography",
      "Programming language",
      "Psychology",
      "SIGNAL (programming language)",
      "Sleep (system call)",
      "Sleep Stages",
      "Social psychology",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Oliveira",
        "given_name": "Gustavo H.B.S."
      },
      {
        "surname": "Coutinho",
        "given_name": "Luciano R."
      },
      {
        "surname": "Silva",
        "given_name": "Josenildo C. da"
      },
      {
        "surname": "Pinto",
        "given_name": "Ivan J.P."
      },
      {
        "surname": "Ferreira",
        "given_name": "Júlia M.S."
      },
      {
        "surname": "Silva",
        "given_name": "Francisco J.S."
      },
      {
        "surname": "Santos",
        "given_name": "Davi V."
      },
      {
        "surname": "Teles",
        "given_name": "Ariel S."
      }
    ]
  },
  {
    "title": "Automatic segmentation of whole-slide H&E stained breast histopathology images using a deep convolutional neural network architecture",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113387",
    "abstract": "The segmentation of malignant breast tissue from histological images represents a crucial task for the diagnosis of breast cancer (BC). This is a time-consuming process that could be alleviated with the help of computerized segmentation methods, leading to elevated precision and reproducibility results. However, this automated segmentation poses a challenge due to the large size of histological whole-slide images and the significant variability, heterogeneity and complexity of features in them. In this research, we propose a processing pipeline for the automatic segmentation of stained BC images presenting different types of histopathological patterns. To deal with the gigantic size of whole-slide images, the digital preparations were processed in a tile-wise manner: a large part of the image is split into patches. Then, the segmentation of each tile was accomplished by applying a deep convolutional neural network (DCNN) along with an encoder-decoder with separable atrous convolution architecture, which, once successfully validated, has revealed to be a promising method to segment pathological image patches. Next, in order to combine the local segmentation results (segmented tiles), while avoiding discontinuities and inconsistencies, an improved merging strategy based on an efficient fully connected Conditional Random Field (CRF) was applied. Experimental results on a collection of patches of breast cancer images demonstrate how the designed processing pipeline performs properly regardless the size, texture or any other colour-shape features typical of the malignant carcinomas considered in this study. The estimated segmentation accuracy and frequency weighted intersection over union (FWIoU) were 95.62%, 92.52%, respectively. Additionally, in order to facilitate the collaboration between pathologists and researchers to extract the specialist knowledge in form of training datasets that allows the training of new algorithms, a web-based platform which includes a slide-viewer and an annotation tool was developed. The automatic segmentation method proposed in this work was integrated into this platform and currently, it is being used as a decision support tool by pathologists.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420302116",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Deep learning",
      "Image segmentation",
      "Pattern recognition (psychology)",
      "Pipeline (software)",
      "Programming language",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Priego-Torres",
        "given_name": "Blanca Maria"
      },
      {
        "surname": "Sanchez-Morillo",
        "given_name": "Daniel"
      },
      {
        "surname": "Fernandez-Granero",
        "given_name": "Miguel Angel"
      },
      {
        "surname": "Garcia-Rojo",
        "given_name": "Marcial"
      }
    ]
  },
  {
    "title": "entity2rec: Property-specific knowledge graph embeddings for item recommendation",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113235",
    "abstract": "Knowledge graphs have shown to be highly beneficial to recommender systems, providing an ideal data structure to generate hybrid recommendations using both content-based and collaborative filtering. Most knowledge-aware recommender systems are based on manually engineered features, typically relying on path counting and/or on random walks. Recently, knowledge graph embeddings have proven to be extremely effective at learning features for prediction tasks, reducing the complexity and time required to manually design effective features. In this work, we present entity2rec, which learns user-item relatedness for item recommendation through property-specific knowledge graph embeddings. A key element of entity2rec is the construction of property-specific subgraphs. Through an extensive evaluation on three datasets, we show that: (1) hybrid property-specific subgraphs consistently enhance the quality of recommendations with respect to collaborative and content-based subgraphs; (2) entity2rec generates accurate and non-obvious recommendations, compared to a set of state-of-the-art recommender systems, achieving high accuracy, serendipity and novelty. More in detail, entity2rec is particularly effective when the dataset is sparse and has a low popularity bias; (3) entity2rec is easily interpretable and can thus be configured for a particular recommendation problem.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300610",
    "keywords": [
      "Artificial intelligence",
      "Collaborative filtering",
      "Computer science",
      "Data mining",
      "Epistemology",
      "Graph",
      "Information retrieval",
      "Machine learning",
      "Novelty",
      "Philosophy",
      "Popularity",
      "Property (philosophy)",
      "Psychology",
      "Recommender system",
      "Serendipity",
      "Social psychology",
      "Theology",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Palumbo",
        "given_name": "Enrico"
      },
      {
        "surname": "Monti",
        "given_name": "Diego"
      },
      {
        "surname": "Rizzo",
        "given_name": "Giuseppe"
      },
      {
        "surname": "Troncy",
        "given_name": "Raphaël"
      },
      {
        "surname": "Baralis",
        "given_name": "Elena"
      }
    ]
  },
  {
    "title": "GUI information-based interaction logging and visualization for asynchronous usability testing",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113289",
    "abstract": "Asynchronous usability testing is efficient and promising usability testing methodology. However, evaluators are required to dedicate a considerable amount of effort for identifying usability problems in asynchronous testing. Evaluators cannot observe user interactions directly; only the collected data is available to them for identifying the usability problems. To reduce the amount of effort required, this paper presents a framework and tool for interaction visualization. This framework tracing user interactions based on meaningful GUI changes. The collected interaction log is visualized in a way that is similar to the visualization of traversed path used in web domains. Using our approach, the evaluator shall be able to understand user behavior more easily. In addition, we designed a black-box-based interaction logging for minimizing the implementation costs. Our tool especially helpful when testing has a large number of interaction data to analyze. To evaluate the proposed framework and tool, user interaction data were collected, and the results were evaluated by mobile application experts. The evaluation results demonstrate that the user interaction data traced by the proposed framework accurately reflect the user interactions and that the visualization results are valid.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420301147",
    "keywords": [
      "Asynchronous communication",
      "Computer network",
      "Computer science",
      "Data mining",
      "Heuristic evaluation",
      "Human–computer interaction",
      "Programming language",
      "Usability",
      "User interface",
      "Visualization"
    ],
    "authors": [
      {
        "surname": "Jeong",
        "given_name": "JongWook"
      },
      {
        "surname": "Kim",
        "given_name": "NeungHoe"
      },
      {
        "surname": "In",
        "given_name": "Hoh Peter"
      }
    ]
  },
  {
    "title": "A crossover operator for improving the efficiency of permutation-based genetic algorithms",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113381",
    "abstract": "Crossover is one of the most important operators in a genetic algorithm by which offspring production for the next generation is performed. There are a number of crossover operators for each type of chromosome representation of solutions that are closely related to different types of optimisation problems. Crossover operation in genetic algorithms, aimed at solving permutation-based combinatorial optimisation problems, is more computationally expensive compared to other cases. This is mainly caused by the fact that no duplicate numbers are allowed in a chromosome and therefore offspring legalisation is needed after each substring exchange. Under these conditions, the time required for performing crossover operation increases significantly with increasing chromosome size, which may deeply affect the efficiency of these genetic algorithms. In this paper, a genetic algorithm that uses path representation for chromosomes and benefits from an alternative form of the well-known partially mapped crossover is proposed. The results of numerical experiments performed on a set of benchmark problems clearly show that the use of this crossover operator can significantly increase the efficiency of permutation-based genetic algorithms and also help in producing good quality solutions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420302050",
    "keywords": [
      "Acoustics",
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Chromosome",
      "Computer science",
      "Crossover",
      "Gene",
      "Genetic algorithm",
      "Genetic operator",
      "Genetics",
      "Geodesy",
      "Geography",
      "Law",
      "Mathematical optimization",
      "Mathematics",
      "Operator (biology)",
      "Permutation (music)",
      "Physics",
      "Political science",
      "Politics",
      "Population-based incremental learning",
      "Representation (politics)",
      "Repressor",
      "Transcription factor"
    ],
    "authors": [
      {
        "surname": "Koohestani",
        "given_name": "Behrooz"
      }
    ]
  },
  {
    "title": "Scalable auto-encoders for gravitational waves detection from time series data",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113378",
    "abstract": "Gravitational waves represent a new opportunity to study and interpret phenomena from the universe. In order to efficiently detect and analyze them, advanced and automatic signal processing and machine learning techniques could help to support standard tools and techniques. Another challenge relates to the large volume of data collected by the detectors on a daily basis, which creates a gap between the amount of data generated and effectively analyzed. In this paper, we propose two approaches involving deep auto-encoder models to analyze time series collected from Gravitational Waves detectors and provide a classification label (noise or real signal). The purpose is to discard noisy time series accurately and identify time series that potentially contain a real phenomenon. Experiments carried out on three datasets show that the proposed approaches implemented using the Apache Spark framework, represent a valuable machine learning tool for astrophysical analysis, offering competitive accuracy and scalability performances with respect to state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420301986",
    "keywords": [
      "Artificial intelligence",
      "Astrophysics",
      "Biology",
      "Computer science",
      "Data mining",
      "Database",
      "Detector",
      "Gravitational wave",
      "Image (mathematics)",
      "Machine learning",
      "Noise (video)",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Physics",
      "Programming language",
      "SIGNAL (programming language)",
      "SPARK (programming language)",
      "Scalability",
      "Series (stratigraphy)",
      "Telecommunications",
      "Time series"
    ],
    "authors": [
      {
        "surname": "Corizzo",
        "given_name": "Roberto"
      },
      {
        "surname": "Ceci",
        "given_name": "Michelangelo"
      },
      {
        "surname": "Zdravevski",
        "given_name": "Eftim"
      },
      {
        "surname": "Japkowicz",
        "given_name": "Nathalie"
      }
    ]
  },
  {
    "title": "Enhanced attentive convolutional neural networks for sentence pair modeling",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113384",
    "abstract": "Sentence Pair Modeling is a critical and challenging problem in natural language processing (NLP). It aims to seek the underlying semantic relationship between two sentences. Inspired by human learning, attention mechanisms are widely used in NLP. Previous attentive CNNs mainly focus on attentive pooling which only compute the matching scores between sentence pairs with the same filter size and lack substantive interactive context. In this paper, we propose Enhanced Attentive Convolutional Neural Networks (EACNNs) for modeling sentence pairs to make full use of the characteristics of convolution. Enhanced attention mechanisms help strengthen the interaction between sentences via adding alignment context into local context in convolution operation and combining multi-grained similarity features in different filter sizes. We exploit two attention schemes: (i) attention before representation to capture the interaction information of sentence pairs by attentive convolution (AC) and multi-window advanced attention (MWA), and (ii) attention after representation to emphasize different importance of each word by multi-view similarity measurement layer (MVS). All the enhanced attention mechanisms can make our EACNNs outperform existing attentive CNN models. Furthermore, the combinations of these attention have strong competition in complicated LSTMs and show great advantage in training efficiency.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420302086",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Filter (signal processing)",
      "Focus (optics)",
      "Image (mathematics)",
      "Law",
      "Linguistics",
      "Machine learning",
      "Natural language processing",
      "Optics",
      "Paleontology",
      "Philosophy",
      "Physics",
      "Political science",
      "Politics",
      "Pooling",
      "Representation (politics)",
      "Sentence",
      "Similarity (geometry)",
      "Word (group theory)"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Shiyao"
      },
      {
        "surname": "E",
        "given_name": "Shijia"
      },
      {
        "surname": "Xiang",
        "given_name": "Yang"
      }
    ]
  },
  {
    "title": "Classification methods for planar shapes",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113320",
    "abstract": "The goal of this paper is to propose supervised learning methods considering two-dimensional landmarks (planar shapes). We introduce a novel method based on a support vector machine (SVM) algorithm, which had to be adapted to complex vectors. We also propose other novel methods based on density estimation, kernel k-means, and hill-climbing. Combinations between the classifiers using the ensemble method are considered as well. Furthermore, we compared the proposed methods to the existing Bayes discriminant approach. We conducted simulation experiments to evaluate the performance of the proposed methods. The numerical results prove that for low concentrated data sets, the SVM algorithm outperforms the other methods. Moreover, four real-world data sets are considered: gorilla skulls, orangutan skulls, mouse vertebrae, and schizophrenia. These data sets present different configurations, such as several landmarks and variability. The proposed SVM method achieved the best performance in three data sets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420301457",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Discriminant",
      "Kernel (algebra)",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Carvalho",
        "given_name": "Jhonnata Bezerra de"
      },
      {
        "surname": "Amaral",
        "given_name": "Getúlio José Amorim do"
      }
    ]
  },
  {
    "title": "DNet: A lightweight and efficient model for aspect based sentiment analysis",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113393",
    "abstract": "Aspect based sentiment analysis (ABSA) is the task of identifying fine-grained opinion polarity towards a specific target in a sentence, which is empowering experts and intelligent systems with enriched interaction capabilities. Most of approaches to date usually capture semantic relations between target and context words based on RNNs (Recurrent Neural Networks) or pre-trained models (e.g. BERT). However, due to computational complexity and size constraints, these models are often hosted in the cloud. Enabling ABSA models to run on resource-constrained end-devices with quick response time is still challenging and not yet well studied. This paper presents distillation network (DNet), a lightweight and efficient sentiment analysis model based on gated convolutional neural networks for on-device inference. Through combining stacked gated convolution with attention mechanism, DNet can distill aspect-aware context information from unstructured text progressively, achieving high performance with less inference latency and reduced model size. Experiments on SemEval 2014 Task 4 and ACL14 Twitter datasets demonstrate that our approach achieves the state-of-the-art performance. Furthermore, compared with the BERT-based model, DNet reduces the model size by more than 50 times and improves the responsiveness by 24 times.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420302177",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Convolutional neural network",
      "Economics",
      "Inference",
      "Machine learning",
      "Management",
      "Paleontology",
      "Recurrent neural network",
      "SemEval",
      "Sentence",
      "Sentiment analysis",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Ren",
        "given_name": "Feiyang"
      },
      {
        "surname": "Feng",
        "given_name": "Liangming"
      },
      {
        "surname": "Xiao",
        "given_name": "Ding"
      },
      {
        "surname": "Cai",
        "given_name": "Ming"
      },
      {
        "surname": "Cheng",
        "given_name": "Sheng"
      }
    ]
  },
  {
    "title": "Efficient synthetical clustering validity indexes for hierarchical clustering",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113367",
    "abstract": "Clustering validation and identifying the optimal number of clusters are of great importance in expert and intelligent systems. However, the commonly used similarity measures for validating are not versatile to measure the complex data structure, in reality, some of which are not as effective as that of the used clustering algorithm which gives the clustering results. This paper studies the validity indexes for the hierarchical clustering algorithm and proposes a unified validity index framework. For the single-linkage agglomerative hierarchical clustering we propose two efficient synthetical clustering validity (SCV) indexes using the minimum spanning tree to calculate the intra-cluster compactness to overcome the deficiencies of the measurements in the existing validity indexes. For the general hierarchical clustering, a self-adaptive similarity measure strategy and two generalized synthetical clustering validity (GSCV) indexes, which are the extension of the proposed SCV indexes, are developed. The proposed SCV and GSCV indexes constitute a unified validity index framework, where SCV index is a special case of GSCV index, can avoid the incompatibility of the similarity measure between the clustering and validation. The experimental comparisons with the state-of-the-art validity indexes on artificial and real-world data sets demonstrate the efficiency of the proposed validity indexes in discovering the true number of clusters and dealing with various sorts of data sets, including imbalanced data sets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420301925",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "CURE data clustering algorithm",
      "Chemistry",
      "Cluster analysis",
      "Complete linkage",
      "Computer science",
      "Consensus clustering",
      "Correlation clustering",
      "Data mining",
      "Fuzzy clustering",
      "Gene",
      "Genotype",
      "Hierarchical clustering",
      "Image (mathematics)",
      "Mathematics",
      "Measure (data warehouse)",
      "Pattern recognition (psychology)",
      "Similarity (geometry)",
      "Similarity measure",
      "Single-linkage clustering",
      "Single-nucleotide polymorphism"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Qin"
      },
      {
        "surname": "Zhang",
        "given_name": "Qiang"
      },
      {
        "surname": "Liu",
        "given_name": "Jinpei"
      },
      {
        "surname": "Luo",
        "given_name": "Bin"
      }
    ]
  },
  {
    "title": "Seq2Seq models for recommending short text conversations",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113270",
    "abstract": "The massive amounts of data on social media networks can be overwhelming for users; for this reason, recommending relevant content becomes an essential task to avoid information overload. In this paper, we propose a new task for recommending users that might be interested in join conversations on specific domains. To that end, we introduce a new corpus that contains conversations threads from popular users on Twitter on domains such as politics, sports, or humanitarian activism. Modeling short-text conversations on microblogs can be difficult because user-generated data is unstructured and noisy. Previous works focused on recommending content to users based on latent factors models and collaborative filtering methods. We propose a state-of-the-art recommendation model based on a sequence-to-sequence neural architecture that encodes the text of users’ profiles and the conversations’ context using several variants of recurrent neural networks. The experimental results show that our method provides as much as 20% higher recall compared to baseline methods. Moreover, we use an end-to-end learning framework that allows downstream applications to use recommender systems (RSs) that generalize better to new content by using pre-trained embeddings, thus being useful across domains or events.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300956",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Baseline (sea)",
      "Biology",
      "CRFS",
      "Collaborative filtering",
      "Computer science",
      "Conditional random field",
      "Context (archaeology)",
      "Economics",
      "Geology",
      "Information overload",
      "Information retrieval",
      "Linguistics",
      "Management",
      "Microblogging",
      "Oceanography",
      "Paleontology",
      "Philosophy",
      "RSS",
      "Recall",
      "Recommender system",
      "Recurrent neural network",
      "Social media",
      "Task (project management)",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Torres",
        "given_name": "Johnny"
      },
      {
        "surname": "Vaca",
        "given_name": "Carmen"
      },
      {
        "surname": "Terán",
        "given_name": "Luis"
      },
      {
        "surname": "Abad",
        "given_name": "Cristina L."
      }
    ]
  },
  {
    "title": "SSDTW: Shape segment dynamic time warping",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113291",
    "abstract": "In order to increase the yield of a process, it is essential to establish a process control based on manufacturing data. Process management systems mainly consist of statistical process control (SPC), fault detection and classification (FDC), and advanced process control (APC), and are modeled using time series data. However, large amounts of time series data and various distributions are collected in the process; hence, preprocessing measures, such as length adjustment, are essential for modeling. Dynamic time warping (DTW) has been widely used as an algorithm that can measure the similarity between two different time series data and adjust their length. However, owing to the complex structure and time lag of processing time series data, there are limitations in applying the traditional DTW. Therefore, to solve this problem, we propose the shape segment dynamic time warping (SSDTW) algorithm that improves DTW in consideration of the structure information of time series data. By using the maximum overlap discrete wavelet transform (MODWT), the proposed method reflects the peripheral information of the time series data and divides the time series data interval to achieve a reasonable local alignment path. SSDTW attains more accurate alignment paths than DTW, derivative dynamic time warping (DDTW), and shapeDTW. Experiments conducted using semiconductor signal data and UCR time series data sets show that the proposed method is more effective than DTW, DDTW, and shapeDTW.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420301160",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Data mining",
      "Data pre-processing",
      "Dynamic time warping",
      "Image (mathematics)",
      "Machine learning",
      "Operating system",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Process (computing)",
      "Series (stratigraphy)",
      "Similarity (geometry)",
      "Time series"
    ],
    "authors": [
      {
        "surname": "Hong",
        "given_name": "Jae Yeol"
      },
      {
        "surname": "Park",
        "given_name": "Seung Hwan"
      },
      {
        "surname": "Baek",
        "given_name": "Jun-Geol"
      }
    ]
  },
  {
    "title": "A cost-sensitive convolution neural network learning for control chart pattern recognition",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113275",
    "abstract": "Abnormal control chart patterns are naturally infrequent in an industrial setting. However, such patterns may indicate manufacturing faults that, if not treated in a timely manner, can lead to significant internal and external failure costs, ultimately threatening the product reputation. Therefore, the detection of abnormalities, which is sought in the well-known control chart pattern recognition (CCPR) problem, is of utmost importance. Standard machine learning algorithms have been extensively applied to this problem. However, they often produce biased classifiers unless the inherent data imbalancedness, which originates from the scarcity of abnormal patterns, are carefully addressed. In this paper, we develop a cost-sensitive classification scheme within a deep convolutional neural network (CSCNN) for the imbalanced CCPR problem. We further investigate the performance of our algorithm on both simulated and real-world datasets to determine separable and non-separable common fault patterns in a manufacturing setting. As the contribution of this work, we particularly demonstrate that the cost weighting strategy is both robust and efficient for moderately- and severely-imbalanced cases. We further show that our method can either be fine-tuned to specific faults or trained to detect multiple faults while remaining efficient for large datasets. To the best of our knowledge, this is the first deep CSCNN designed for imbalanced CCPR problems, which presents great promise for other manufacturing applications in the presence of imbalanced datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420301007",
    "keywords": [
      "Actuator",
      "Artificial intelligence",
      "Artificial neural network",
      "Chart",
      "Computer science",
      "Control (management)",
      "Control chart",
      "Convolutional neural network",
      "Data mining",
      "Fault detection and isolation",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Medicine",
      "Operating system",
      "Pattern recognition (psychology)",
      "Process (computing)",
      "Radiology",
      "Scheme (mathematics)",
      "Statistics",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Fuqua",
        "given_name": "Donovan"
      },
      {
        "surname": "Razzaghi",
        "given_name": "Talayeh"
      }
    ]
  },
  {
    "title": "Ranking-based instance selection for pattern classification",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113269",
    "abstract": "In instance-based learning algorithms, the need to store a large number of examples as the training set results in several drawbacks related to large memory requirements, oversensitivity to noise, and slow execution speed. Instance selection techniques can improve the performance of these algorithms by selecting the best instances from the original data set, removing, for example, redundant information and noisy points. The relationship between an instance and the other patterns in the training set plays an important role and can impact its misclassification by learning algorithms. Such a relationship can be represented as a value that measures how difficult such instance is regarding classification purposes. Based on that, we introduce a novel instance selection algorithm called Ranking-based Instance Selection (RIS) that attributes a score per instance that depends on its relationship with all other instances in the training set. In this sense, instances with higher scores form safe regions (neighborhood of samples with relatively homogeneous class labels) in the feature space, and instances with lower scores form an indecision region (borderline samples of different classes). This information is further used in a selection process to remove instances from both safe and indecision regions that are considered irrelevant to represent their clusters in the feature space. In contrast to previous algorithms, the proposal combines a raking procedure with a selection process aiming to find a promising tradeoff between accuracy and reduction rate. Experiments are conducted on twenty-four real-world classification problems and show the effectiveness of the RIS algorithm when compared against other instance selection algorithms in the literature.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300944",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Contrast (vision)",
      "Data mining",
      "Feature (linguistics)",
      "Feature selection",
      "Image (mathematics)",
      "Linguistics",
      "Machine learning",
      "Noise (video)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Process (computing)",
      "Programming language",
      "Ranking (information retrieval)",
      "Selection (genetic algorithm)",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Cavalcanti",
        "given_name": "George D.C."
      },
      {
        "surname": "Soares",
        "given_name": "Rodolfo J.O."
      }
    ]
  },
  {
    "title": "Yin-Yang firefly algorithm based on dimensionally Cauchy mutation",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113216",
    "abstract": "Firefly algorithm (FA) is a classical and efficient swarm intelligence optimization method and has a natural capability to address multimodal optimization. However, it suffers from premature convergence and low stability in the solution quality. In this paper, a Yin-Yang firefly algorithm (YYFA) based on dimensionally Cauchy mutation is proposed for performance improvement of FA. An initial position of fireflies is specified by the good nodes set (GNS) strategy to ensure the spatial representativeness of the firefly population. A designed random attraction model is then used in the proposed work to reduce the time complexity of the algorithm. Besides, a key self-learning procedure on the brightest firefly is undertaken to strike a balance between exploration and exploitation. The performance of the proposed algorithm is verified by a set of CEC 2013 benchmark functions used for the single objective real parameter algorithm competition. Experimental results are compared with those of other the state-of-the-art variants of FA. Nonparametric statistical tests on the results demonstrate that YYFA provides highly competitive performance in terms of the tested algorithms. In addition, the application in constrained engineering optimization problems shows the practicability of YYFA algorithm.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300427",
    "keywords": [
      "Algorithm",
      "Benchmark (surveying)",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Convergence (economics)",
      "Economic growth",
      "Economics",
      "Firefly algorithm",
      "Gene",
      "Geodesy",
      "Geography",
      "Mathematical optimization",
      "Mathematics",
      "Mutation",
      "Optimization problem",
      "Particle swarm optimization",
      "Premature convergence",
      "Swarm intelligence"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Wen-chuan"
      },
      {
        "surname": "Xu",
        "given_name": "Lei"
      },
      {
        "surname": "Chau",
        "given_name": "Kwok-wing"
      },
      {
        "surname": "Xu",
        "given_name": "Dong-mei"
      }
    ]
  },
  {
    "title": "Shop floor simulation optimization using machine learning to improve parallel metaheuristics",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113272",
    "abstract": "Simulation optimization is a tool commonly used as a decision-making support system on industrial problems in order to find the best resource allocation, which has a direct influence on costs and revenues. The present study proposed an open-source framework developed on Python, integrating different strategies for a novel optimization algorithm. The framework includes multicore parallelism (tested on two different types of computer sets), (two) population-based metaheuristics, and 33 machine learning methods. Moreover, the study tested the framework to optimize resource allocation on a theoretical shop floor case study, evaluating 12 optimization scenarios. The use of metaheuristic with parallelism reduced 88.3% the processing time compared with the serial metaheuristic, while the integration of metaheuristic with the selected machine learning generated an additional reduction of 59.0% on the necessary processing time. The combination of the optimization methods created a solution of 95.3% near the global optimum and time reduction of 95.2%.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742030097X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Meta-optimization",
      "Metaheuristic",
      "Operating system",
      "Parallel metaheuristic",
      "Python (programming language)"
    ],
    "authors": [
      {
        "surname": "Sousa Junior",
        "given_name": "Wilson Trigueiro de"
      },
      {
        "surname": "Montevechi",
        "given_name": "José Arnaldo Barra"
      },
      {
        "surname": "Miranda",
        "given_name": "Rafael de Carvalho"
      },
      {
        "surname": "Oliveira",
        "given_name": "Mona Liza Moura de"
      },
      {
        "surname": "Campos",
        "given_name": "Afonso Teberga"
      }
    ]
  },
  {
    "title": "Corrigendum to “Classification of brain MRI using hyper column technique with convolutional neural network and feature selection method” Expert Systems with Applications 149 (2020) 1--8 /113274",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113366",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420301913",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Column (typography)",
      "Computer science",
      "Convolutional neural network",
      "Feature (linguistics)",
      "Feature selection",
      "Frame (networking)",
      "Linguistics",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Selection (genetic algorithm)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Toğaçar",
        "given_name": "Mesut"
      },
      {
        "surname": "Cömert",
        "given_name": "Zafer"
      },
      {
        "surname": "Ergen",
        "given_name": "Burhan"
      },
      {
        "surname": "Özyurt",
        "given_name": "Fatih"
      }
    ]
  },
  {
    "title": "Estimation of linear motion in dense crowd videos using Langevin model",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113333",
    "abstract": "Expert and intelligent systems are highly popular for designing cross-domain autonomous systems. Computer vision aided by expert decision-making systems are widely used to automate tasks that are normally carried out manually. For example, in the traditional way of traffic monitoring, signals are either controlled through predefined set-ups or with the help of visual observations. Even though some of the modern cities employ sensor-based surveillance at large, full automation is still far from the desirable accuracy. This is more challenging for monitoring high-density crowds that often cause unwanted situations due to sudden changes in dynamics. It has been shown in this paper that the movement of a dense crowd can be approximated using well-known physics-based models. Such a modeling can help to understand the overall crowd behavior. In accomplishing this, we have introduced a computer vision guided expert system with the help of a Langevin equation-based force model to represent the linear flow of the crowd, particularly in situations when the density is high. One of the primary contributions of our proposed model is its computational efficiency, particularly when a timely decision can help to avoid unwanted situations. Our proposed three-term force model is capable of predicting the positions of the group of key-points in a video frame leading to a significant computational gain. We have carried out several experiments on publicly available videos as well as our own videos to validate the claims in terms of accuracy as well as computational gain. It has been observed that the proposed physics-based model outperforms the existing systems with a 4 − 6 % improvement in the segmentation accuracy. Moreover, we have achieved multi-fold computational gain. We believe the proposed work, when supported by appropriate post-processing, can be used to develop crowd monitoring applications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420301585",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Computer science",
      "Computer vision",
      "Crowd psychology",
      "Demography",
      "Economics",
      "Financial economics",
      "Identification (biology)",
      "Image (mathematics)",
      "LEAPS",
      "Linear model",
      "Machine learning",
      "Mathematics",
      "Motion (physics)",
      "Optical flow",
      "Population",
      "Randomness",
      "Segmentation",
      "Sociology",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Behera",
        "given_name": "Shreetam"
      },
      {
        "surname": "Dogra",
        "given_name": "Debi P"
      },
      {
        "surname": "Bandyopadhyay",
        "given_name": "Malay K"
      },
      {
        "surname": "Roy",
        "given_name": "Partha P"
      }
    ]
  },
  {
    "title": "Generative adversarial network-based semi-supervised learning for real-time risk warning of process industries",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113244",
    "abstract": "Due to the non-cognition of real-time data, rare loss-based risk warning methods can effectively respond to unexpected emergencies. Machine learning has powerful data processing capabilities and real-time computing functions and thus is suitable for offsetting the shortcomings of traditional risk methods. Risk analysis can be easily employed to perform risk-based data classification for a set of process data. However, the risk analysis process is too complicated to label risk levels for all processes, which is hard to satisfy the requirements of the amount of data for supervised learning. Therefore, the present paper focuses on developing semi-supervised learning methods for the construction of real-time risk-based early warning systems. By using fuzzy HAZOP, we estimate the risk of systems quantitatively based on the process data. With the consideration of scarce labeled data and numerous unlabeled information, we develop the generative adversarial network (GAN)-based semi-supervised learning method to identify the process risk timely. Besides, deep network architecture integrated with the convolutional neural network (CNN) is used for the codification of multi-dimensional process data to enhance the generalization of warning models. Finally, the effectiveness of the proposed method is evaluated through a comparative study with different algorithms on a case of multizone circulating reactor (MZCR).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300701",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Data mining",
      "Hazard and operability study",
      "Machine learning",
      "Operability",
      "Operating system",
      "Process (computing)",
      "Semi-supervised learning",
      "Software engineering",
      "Supervised learning",
      "Telecommunications",
      "Warning system"
    ],
    "authors": [
      {
        "surname": "He",
        "given_name": "Rui"
      },
      {
        "surname": "Li",
        "given_name": "Xinhong"
      },
      {
        "surname": "Chen",
        "given_name": "Guoming"
      },
      {
        "surname": "Chen",
        "given_name": "Guoxing"
      },
      {
        "surname": "Liu",
        "given_name": "Yiwei"
      }
    ]
  },
  {
    "title": "Hand sign language recognition using multi-view hand skeleton",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113336",
    "abstract": "Hand sign language recognition from video is a challenging research area in computer vision, which performance is affected by hand occlusion, fast hand movement, illumination changes, or background complexity, just to mention a few. In recent years, deep learning approaches have achieved state-of-the-art results in the field, though previous challenges are not completely solved. In this work, we propose a novel deep learning-based pipeline architecture for efficient automatic hand sign language recognition using Single Shot Detector (SSD), 2D Convolutional Neural Network (2DCNN), 3D Convolutional Neural Network (3DCNN), and Long Short-Term Memory (LSTM) from RGB input videos. We use a CNN-based model which estimates the 3D hand keypoints from 2D input frames. After that, we connect these estimated keypoints to build the hand skeleton by using midpoint algorithm. In order to obtain a more discriminative representation of hands, we project 3D hand skeleton into three views surface images. We further employ the heatmap image of detected keypoints as input for refinement in a stacked fashion. We apply 3DCNNs on the stacked features of hand, including pixel level, multi-view hand skeleton, and heatmap features, to extract discriminant local spatio-temporal features from these stacked inputs. The outputs of the 3DCNNs are fused and fed to a LSTM to model long-term dynamics of hand sign gestures. Analyzing 2DCNN vs. 3DCNN using different number of stacked inputs into the network, we demonstrate that 3DCNN better capture spatio-temporal dynamics of hands. To the best of our knowledge, this is the first time that this multi-modal and multi-view set of hand skeleton features are applied for hand sign language recognition. Furthermore, we present a new large-scale hand sign language dataset, namely RKS-PERSIANSIGN, including 10′000 RGB videos of 100 Persian sign words. Evaluation results of the proposed model on three datasets, NYU, First-Person, and RKS-PERSIANSIGN, indicate that our model outperforms state-of-the-art models in hand sign language recognition, hand pose estimation, and hand action recognition.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420301615",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Deep learning",
      "Discriminative model",
      "Geodesy",
      "Geography",
      "Gesture",
      "Gesture recognition",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pipeline (software)",
      "Programming language",
      "RGB color model",
      "Sign language"
    ],
    "authors": [
      {
        "surname": "Rastgoo",
        "given_name": "Razieh"
      },
      {
        "surname": "Kiani",
        "given_name": "Kourosh"
      },
      {
        "surname": "Escalera",
        "given_name": "Sergio"
      }
    ]
  },
  {
    "title": "An AHP-based multi-criteria model for sustainable supply chain development in the renewable energy sector",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113321",
    "abstract": "The aim of this paper is to provide a multi-criteria decision making framework based on the Triple Bottom Line principles and Analytic Hierarchy Process methodology for sustainable supply chain development in the renewable energy sector. The proposed framework encompasses the whole energy production supply chain, from raw materials’ suppliers to disposal. In particular, the photovoltaic energy sector has been used as case study and represents the focus of this work. The framework is based on the three Triple Bottom Line dimensions such as social, economic and environmental. Furthermore, literature review and expert opinions are used to identify and assess the sub-criteria for each dimension, followed by pair-wise comparison. Finally, the proposed framework is used to evaluate the seven European countries that conjointly represent the 86.8% of the total photovoltaic installed capacity in Europe, using both logical and quantitative information. Results are in agreement with the photovoltaic development in the period 2000-2017 in these countries. The proposed framework provides the decision makers with a powerful tool for making sustainable investment decisions in the photovoltaic energy sector.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420301469",
    "keywords": [
      "Analytic hierarchy process",
      "Business",
      "Computer science",
      "Dimension (graph theory)",
      "Economics",
      "Electrical engineering",
      "Engineering",
      "Environmental economics",
      "Investment (military)",
      "Law",
      "Marketing",
      "Mathematics",
      "Mechanical engineering",
      "Operations research",
      "Photovoltaic system",
      "Political science",
      "Politics",
      "Pure mathematics",
      "Renewable energy",
      "Supply chain",
      "Sustainable development",
      "Triple bottom line",
      "Work (physics)"
    ],
    "authors": [
      {
        "surname": "Mastrocinque",
        "given_name": "Ernesto"
      },
      {
        "surname": "Ramírez",
        "given_name": "F. Javier"
      },
      {
        "surname": "Honrubia-Escribano",
        "given_name": "Andrés"
      },
      {
        "surname": "Pham",
        "given_name": "Duc T."
      }
    ]
  },
  {
    "title": "Improving spherical k-means for document clustering: Fast initialization, sparse centroid projection, and efficient cluster labeling",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113288",
    "abstract": "Due to its simplicity and intuitive interpretability, spherical k-means is often used for clustering a large number of documents. However, there exist a number of drawbacks that need to be addressed for much effective document clustering. Without well-dispersed initial points, spherical k-means fails to converge quickly, which is critical for clustering a large number of documents. Furthermore, its dense centroid vectors needlessly incorporate the impact of infrequent and less-informative words, thereby distorting the distance calculation between the document vectors. In this paper, we propose practical improvements on spherical k-means to overcome these issues during document clustering. Our proposed initialization method not only guarantees dispersed initial points, but is also up to 1000 times faster than previously well-known initialization method such as k-means++. Furthermore, we enforce sparsity on the centroid vectors by using a data-driven threshold that is capable of dynamically adjusting its value depending on the clusters. Additionally, we propose an unsupervised cluster labeling method that effectively extracts meaningful keywords to describe each cluster. We have tested our improvements on seven different text datasets that include both new and publicly available datasets. Based on our experiments on these datasets, we have found that our proposed improvements successfully overcome the drawbacks of spherical k-means in significantly reduced computation time. Furthermore, we have qualitatively verified the performance of the proposed cluster labeling method by extracting descriptive keywords of the clusters from these datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420301135",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Centroid",
      "Cluster (spacecraft)",
      "Cluster analysis",
      "Clustering high-dimensional data",
      "Computer science",
      "Data mining",
      "Document clustering",
      "Initialization",
      "Interpretability",
      "Pattern recognition (psychology)",
      "Programming language",
      "k-means clustering"
    ],
    "authors": [
      {
        "surname": "Kim",
        "given_name": "Hyunjoong"
      },
      {
        "surname": "Kim",
        "given_name": "Han Kyul"
      },
      {
        "surname": "Cho",
        "given_name": "Sungzoon"
      }
    ]
  },
  {
    "title": "Improving patient-care services at an oncology clinic using a flexible and adaptive scheduling procedure",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113267",
    "abstract": "This paper studies an online scheduling problem dealing with patients’ multiple requests for chemotherapy treatments at the cancer centre of a major metropolitan hospital in Canada. The proposed solution to the problem is an adaptive and flexible procedure that systematically combines two optimization models. The first model is intended to dynamically schedule incoming appointment requests, which arrive in the form of waiting lists, and the second model is used to reschedule already booked appointments with the goal of better allocating resources as new information becomes available. The performance and potential impact of the proposed procedure is assessed using historical data provided by the cancer centre. Moreover, a sensitivity analysis is carried out to draw insights that may help hospital managers to deal more efficiently with both incoming requests and unexpected events.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300920",
    "keywords": [
      "Computer science",
      "Economics",
      "Engineering",
      "Medicine",
      "Metropolitan area",
      "Operating system",
      "Operations management",
      "Operations research",
      "Pathology",
      "Schedule",
      "Scheduling (production processes)"
    ],
    "authors": [
      {
        "surname": "Hooshangi-Tabrizi",
        "given_name": "Pedram"
      },
      {
        "surname": "Contreras",
        "given_name": "Ivan"
      },
      {
        "surname": "Bhuiyan",
        "given_name": "Nadia"
      },
      {
        "surname": "Batist",
        "given_name": "Gerald"
      }
    ]
  },
  {
    "title": "SPOCC: Scalable POssibilistic Classifier Combination - toward robust aggregation of classifiers",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113332",
    "abstract": "We investigate a problem in which each member of a group of learners is trained separately to solve the same classification task. Each learner has access to a training dataset (possibly with overlap across learners) but each trained classifier can be evaluated on a validation dataset. We propose a new approach to aggregate the learner predictions in the possibility theory framework. For each classifier prediction, we build a possibility distribution assessing how likely the classifier prediction is correct using frequentist probabilities estimated on the validation set. The possibility distributions are aggregated using an adaptive t-norm that can accommodate dependency and poor accuracy of the classifier predictions. We prove that the proposed approach possesses a number of desirable classifier combination robustness properties. Moreover, the method is agnostic on the base learners, scales well in the number of aggregated classifiers and is incremental as a new classifier can be appended to the ensemble by building upon previously computed parameters and structures. A python implementation can be downloaded at this link https://github.com/john-klein/SPOCC.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420301573",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Database",
      "Machine learning",
      "Operating system",
      "Pattern recognition (psychology)",
      "Python (programming language)",
      "Scalability"
    ],
    "authors": [
      {
        "surname": "Albardan",
        "given_name": "Mahmoud"
      },
      {
        "surname": "Klein",
        "given_name": "John"
      },
      {
        "surname": "Colot",
        "given_name": "Olivier"
      }
    ]
  },
  {
    "title": "Learning local representations for scalable RGB-D face recognition",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113319",
    "abstract": "In this article we present a novel RGB-D learned local representations for face recognition based on facial patch description and matching. The major contribution of the proposed approach is an efficient learning and combination of data-driven descriptors to characterize local patches extracted around image reference points. We explored the complementarity between both of deep learning and statistical image features as data-driven descriptors. In addition, we proposed an efficient high-level fusion scheme based on a sparse representation algorithm to leverage the complementarity between image and depth modalities and also the used data-driven features. Our approach was extensively evaluated on four well-known benchmarks to prove its robustness against known challenges in the case of face recognition. The obtained experimental results are competitive with the state-of-the-art methods while providing a scalable and adaptive RGB-D face recognition method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420301445",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Biology",
      "Chemistry",
      "Complementarity (molecular biology)",
      "Computer science",
      "Computer vision",
      "Database",
      "Facial recognition system",
      "Gene",
      "Genetics",
      "Leverage (statistics)",
      "Machine learning",
      "Pattern recognition (psychology)",
      "RGB color model",
      "Robustness (evolution)",
      "Scalability"
    ],
    "authors": [
      {
        "surname": "Grati",
        "given_name": "Nesrine"
      },
      {
        "surname": "Ben-Hamadou",
        "given_name": "Achraf"
      },
      {
        "surname": "Hammami",
        "given_name": "Mohamed"
      }
    ]
  },
  {
    "title": "Effects of the validation set on stock returns forecasting",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113271",
    "abstract": "Deep neural networks are potentially suitable tools for time series forecasting due to their ability to extract complex patterns of nonlinear data and their versatility in terms of models and applications. Even though they are powerful instruments and well-behaved approaches for certain tasks, they are sometimes surpassed by data complexity, and thus struggle to find an error that generalizes well enough on unseen data, especially in cases like times series forecasting for stock trading strategies. In this paper, the complex characteristics of time series are addressed by separating data by means of simpler yet more relevant distinctions in order to create a single model for every existing or created category, with a focus on model validation. This creates models which are trained on the same data, but validated for a particular class so that the models’ hyperparameters are specifically tuned to that class. Experiments on convolutional networks applied to the DJIA, Nasdaq and S&P 500 indices using volatility as a class or category indicator, have shown that it is possible to improve predictions after validating the model, obtaining the best model per the Model Confidence Set among different regression models on all time series datasets. Even the best and only model necessary for the DJIA and S&P 500 indices can be obtained at a significance value of 5% given that the level of volatility is known. The results highlight the importance of knowing the data and how to potentially separate them into simpler yet relevant classes. The results also reveal how model validation on different data is capable of creating models that better explain information just by tuning the model’s architectural hyperparameters, even though the models where trained on the very same data. This finding could be applied to any task requiring validation without modifying the training set, which is usually bigger and more expensive to obtain.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300968",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Econometrics",
      "Hyperparameter",
      "Machine learning",
      "Mathematics",
      "Time series",
      "Volatility (finance)"
    ],
    "authors": [
      {
        "surname": "Jerez",
        "given_name": "Tomas"
      },
      {
        "surname": "Kristjanpoller",
        "given_name": "Werner"
      }
    ]
  },
  {
    "title": "Fast hybrid dimensionality reduction method for classification based on feature selection and grouped feature extraction",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113277",
    "abstract": "Dimensionality reduction is one basic and critical technology for data mining, especially in current “big data” era. As two different types of methods, feature selection and feature extraction each have their pros and cons. In this paper, we combine multi-strategy feature selection and grouped feature extraction and propose a novel fast hybrid dimension reduction method, incorporating their advantages of removing irrelevant and redundant information. Firstly, the intrinsic dimensionality of the data set is estimated by the maximum likelihood estimation method. Fisher Score and Information Gain based feature selection are used as multi-strategy methods to remove irrelevant features. With the redundancy among the selected features as clustering criterion, they are grouped into a certain amount of clusters. In every cluster, Principal Component Analysis (PCA) based feature extraction is carried out to remove redundant information. Four classical classifiers and representation entropy are used to evaluate the classification performance and information loss of the reduced set. The runtime results of different methods show that the proposed hybrid method is consistently much faster than the other three in almost all of the sets used. Meanwhile, the proposed method shows competitive classification performance, which has no significant difference basically compared with the other methods. The proposed method reduces the dimensionality of the raw data fast and it has excellent efficiency and competitive classification performance compared with the contrastive methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420301020",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Curse of dimensionality",
      "Data mining",
      "Dimensionality reduction",
      "Feature (linguistics)",
      "Feature extraction",
      "Feature selection",
      "Geometry",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Reduction (mathematics)",
      "Selection (genetic algorithm)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Mengmeng"
      },
      {
        "surname": "Wang",
        "given_name": "Haofeng"
      },
      {
        "surname": "Yang",
        "given_name": "Lifang"
      },
      {
        "surname": "Liang",
        "given_name": "You"
      },
      {
        "surname": "Shang",
        "given_name": "Zhigang"
      },
      {
        "surname": "Wan",
        "given_name": "Hong"
      }
    ]
  },
  {
    "title": "Relation chaining in binary positive-only recommender systems",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113296",
    "abstract": "Recommender systems typically work on user-item preference relation data. Recommendations can be improved by including side relations that are indirectly linked to the target relation. Data fusion by matrix co-factorization is one such method that can integrate heterogeneous representations of objects of different types. A shared latent matrix factor model is inferred, which is then used to approximate and thus predict multiple data matrices at the same time. The factor model can also be used to infer indirect relations among objects, by multiplying the corresponding factor matrices on a chain of relations that link those objects (i.e., relation chaining). We show that recommendation in binary positive-only data can be improved by relation chaining. We can filter out less reliable relations by using the number of supporting intermediate paths as an additional parameter in a multi-objective Pareto optimization. Our method outperforms other state-of-the-art chaining methods. To speed-up the computation of relation chaining, we propose a chain matrix multiplication-based approach for chaining. To evaluate our method, we have created synthetic data on transitive relations among objects, for a varying degree of noise. Results on synthetic data show that chaining indeed works on chains containing transitive relations. Results on three real datasets show that the inclusion of the number of intermediate paths improves relation chaining predictions. Compared to the full data matrix multiplication approach, the proposed relation chaining method achieved two-fold speed-up.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420301214",
    "keywords": [
      "Algorithm",
      "Binary relation",
      "Chaining",
      "Collaborative filtering",
      "Combinatorics",
      "Computer science",
      "Data mining",
      "Discrete mathematics",
      "Eigenvalues and eigenvectors",
      "Machine learning",
      "Mathematics",
      "Matrix decomposition",
      "Multiplication (music)",
      "Physics",
      "Psychology",
      "Psychotherapist",
      "Quantum mechanics",
      "Recommender system",
      "Relation (database)",
      "Theoretical computer science",
      "Transitive relation"
    ],
    "authors": [
      {
        "surname": "Gomišček",
        "given_name": "Rok"
      },
      {
        "surname": "Curk",
        "given_name": "Tomaž"
      }
    ]
  },
  {
    "title": "Weighted consensus clustering and its application to Big data",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113294",
    "abstract": "The aim of this study is the development of a weighted consensus clustering that assigns weights to single clustering methods using the purity utility function. In the case of Big data that does not contain labels, the utility function based on the Davies-Bouldin index is proposed in this paper. The Banknote authentication, Phishing, Diabetic, Magic04, Credit card clients, Covertype, Phone accelerometer, and NSL-KDD datasets are used to assess the efficiency of the proposed consensus approach. The proposed approach is evaluated using the Euclidean, Minkowski, squared Euclidean, cosine, and Chebychev distance metrics. It is compared with single clustering algorithms (DBSCAN, OPTICS, CLARANS, k-means, and shared nearby neighbor clustering). The experimental results show the effectiveness of the proposed approach to the Big data clustering in comparison to single clustering methods. The proposed weighted consensus clustering using the squared Euclidean distance metric achieves the highest accuracy, which is a very promising result for Big data clustering. It can be applied to expert systems to help experts make group decisions based on several alternatives. The paper also provides directions for future research on consensus clustering in this area.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420301196",
    "keywords": [
      "Artificial intelligence",
      "Big data",
      "CURE data clustering algorithm",
      "Cluster analysis",
      "Clustering high-dimensional data",
      "Computer science",
      "Consensus clustering",
      "Correlation clustering",
      "DBSCAN",
      "Data mining",
      "Economics",
      "Euclidean distance",
      "Metric (unit)",
      "Minkowski distance",
      "Operations management",
      "Pattern recognition (psychology)",
      "Rand index"
    ],
    "authors": [
      {
        "surname": "Alguliyev",
        "given_name": "Rasim M."
      },
      {
        "surname": "Aliguliyev",
        "given_name": "Ramiz M."
      },
      {
        "surname": "Sukhostat",
        "given_name": "Lyudmila V."
      }
    ]
  },
  {
    "title": "A case-based reasoning system for supervised classification problems in the medical field",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113335",
    "abstract": "Case-Based Reasoning (CBR) system relies on reuse for solving new problems. The system uses the experiences it previously acquired and stored into its case base to address the newly faced problems. A static and non-evolutive case base hinders the system and limits the accuracy of the CBR in problem-solving. While a massive case base can affect the resolution time. Randomization represents a way to generate data without deteriorating the spatial image of the case base and by extension the search time as well. However, the cases generated by randomization are not necessarily valid and require a thorough validation process to access their validity. This paper presents a new amplification technique based on randomization for a CBR system incorporating a structured case-base that speeds up case retrieval while supporting case retention. The generated data by randomization is validated through a three-layer validation process: coherence verification, stochastic validation, and absolute validation. Furthermore, we propose a new way to segment the case base along with new similarity functions based on features’ weights to speed CBR retrieval. We carried out experiments on mammography mass and thyroid disease datasets to validate our approach, where the proposed approach is compared to several popular supervised machine-learning methods and other related works that utilize the same datasets. Experiments have shown that our approach can generate relevant data, which significantly improves the resolution accuracy and makes CBR a good competitor to classification methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420301603",
    "keywords": [
      "Artificial intelligence",
      "Case-based reasoning",
      "Computer science",
      "Data mining",
      "Image (mathematics)",
      "Knowledge base",
      "Machine learning",
      "Operating system",
      "Process (computing)",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Bentaiba-Lagrid",
        "given_name": "Miled Basma"
      },
      {
        "surname": "Bouzar-Benlabiod",
        "given_name": "Lydia"
      },
      {
        "surname": "Rubin",
        "given_name": "Stuart H."
      },
      {
        "surname": "Bouabana-Tebibel",
        "given_name": "Thouraya"
      },
      {
        "surname": "Hanini",
        "given_name": "Maria R."
      }
    ]
  },
  {
    "title": "Opinion mining and emotion recognition applied to learning environments",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113265",
    "abstract": "This paper presents a comparison among several sentiment analysis classifiers using three different techniques – machine learning, deep learning, and an evolutionary approach called EvoMSA – for the classification of educational opinions in an Intelligent Learning Environment called ILE-Java. To make this comparison, we develop two corpora of expressions into the programming languages domain, which reflect the emotional state of students regarding teachers, exams, homework, and academic projects, among others. A corpus called sentiTEXT has polarity (positive and negative) labels, while a corpus called eduSERE has positive and negative learning-centered emotions (engaged, excited, bored, and frustrated) labels. From the experiments carried out with the three techniques, we conclude that the evolutionary algorithm (EvoMSA) generated the best results with an accuracy of 93% for the corpus sentiTEXT, and 84% for the corpus eduSERE.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300907",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Emotion recognition",
      "Machine learning",
      "Sentiment analysis"
    ],
    "authors": [
      {
        "surname": "Barrón Estrada",
        "given_name": "María Lucía"
      },
      {
        "surname": "Zatarain Cabada",
        "given_name": "Ramón"
      },
      {
        "surname": "Oramas Bustillos",
        "given_name": "Raúl"
      },
      {
        "surname": "Graff",
        "given_name": "Mario"
      }
    ]
  },
  {
    "title": "A multi-objective instance-based decision support system for investment recommendation in peer-to-peer lending",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113278",
    "abstract": "Peer-to-peer (P2P) lending has attracted many investors and borrowers since 2005. This financial market helps investors and borrowers to invest in or get loans without a traditional financial intermediary. Investors in the P2P lending market are allowed to invest in multiple loans instead of financing one loan entirely, so investment decision-making in P2P lending can be challenging for lenders because they are not usually expert in loan investing. The goal of this paper is to propose a data-driven investment decision-making framework for this competitive market. We use the artificial neural network and logistic regression to estimate the return and the probability of default (PD) of each individual loan. The return variable is the internal rate of return (IRR). Moreover, we formulate the investment decision-making in P2P lending as a multi-objective portfolio optimization problem based on the mean-variance theory by the use of the non-dominated sorting genetic algorithm (NSGA2). To validate the proposed model, we use a real-world dataset from one of the most popular P2P lending marketplaces. In addition, our model is compared with a single-objective model and a profit-based approach. Throughout the experiment, the empirical results reveal that our multi-objective model in comparison with the single-objective model can improve a lender's investment decision based on both objectives of investments. It means that while the return increases, the risk decreases, simultaneously. On the other hand, it is concluded that the profit scoring model leads to a more profitable investment but with a high level of risk. Finally, a sensitivity analysis is done to check the sensitivity of our model to the total investment amount.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420301032",
    "keywords": [
      "Actuarial science",
      "Behavioral economics",
      "Business",
      "Computer science",
      "Distributed computing",
      "Economics",
      "Finance",
      "Investment (military)",
      "Investment decisions",
      "Law",
      "Loan",
      "Microeconomics",
      "Peer-to-peer",
      "Political science",
      "Politics",
      "Portfolio",
      "Profit (economics)",
      "Rate of return",
      "Return on investment"
    ],
    "authors": [
      {
        "surname": "Babaei",
        "given_name": "Golnoosh"
      },
      {
        "surname": "Bamdad",
        "given_name": "Shahrooz"
      }
    ]
  },
  {
    "title": "Orthogonally-designed adapted grasshopper optimization: A comprehensive analysis",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113282",
    "abstract": "Grasshopper optimization algorithm (GOA) is a newly proposed meta-heuristic algorithm that simulates the biological habits of grasshopper seeking for food sources. Nonetheless, some shortcomings exist in the basic version of GOA. It may quickly drop into local optima and show slow convergence rates when facing some complex basins. In this work, an improved GOA is proposed to alleviate the core shortcomings of GOA and handle continuous optimization problems more efficiently. For this purpose, two strategies, including orthogonal learning and chaotic exploitation, are introduced into the conventional GOA to find a more stable trade-off between the exploration and exploitation cores. Adding orthogonal learning to GOA can enhance the diversity of agents, whereas a chaotic exploitation strategy can update the position of grasshoppers within a limited local region. To confirm the efficacy of GOA, we compared it with a variety of famous classical meta-heuristic algorithms performed on 30 IEEE CEC2017 benchmark functions. Also, it is applied to feature selection cases, and three structural design problems are employed to validate its efficacy in terms of different metrics. The experimental results illustrate that the above tactics can mitigate the deficiencies of GOA, and the improved variant can reach high-quality solutions for different problems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742030107X",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Chaotic",
      "Computer science",
      "Convergence (economics)",
      "Ecology",
      "Economic growth",
      "Economics",
      "Geodesy",
      "Geography",
      "Grasshopper",
      "Heuristic",
      "Local optimum",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Selection (genetic algorithm)"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Zhangze"
      },
      {
        "surname": "Hu",
        "given_name": "Zhongyi"
      },
      {
        "surname": "Heidari",
        "given_name": "Ali Asghar"
      },
      {
        "surname": "Wang",
        "given_name": "Mingjing"
      },
      {
        "surname": "Zhao",
        "given_name": "Xuehua"
      },
      {
        "surname": "Chen",
        "given_name": "Huiling"
      },
      {
        "surname": "Cai",
        "given_name": "Xueding"
      }
    ]
  },
  {
    "title": "Object traceability graph: Applying temporal graph traversals for efficient object traceability",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113287",
    "abstract": "EPC Information Services (EPCIS) is a de-facto standard for information systems for object traceability. Expert and intelligent systems complying with the standard reap benefits from its global interoperability, and thus every application can easily retrieve track and trace information of everyday-objects in an identical manner. However, existing systems are bound to go through scalability issues due to inevitable recursive queries because users are required to handle multiple transformation and aggregation of the objects instead of the systems. In the article, we propose an enhanced EPCIS system called Object Traceability Graph (OTG). The system applies a technique, temporal graph traversal, to resolve the issues. With the system, applications do not need to request recursive queries on their side. Instead, applications are able to represent their ad-hoc traceability queries in a single statement provided by the system. Then, the statement is interpreted and efficiently processed on the system side. In our evaluation, it is shown that our approach enhances the scalability of EPCIS systems by reducing the number of queries and the amount of data transmission. The proposed approach can be applied to existing expert and intelligent systems. Furthermore, we believe that an additional interface, abstracting our approach, is general enough to be included in the standard.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420301123",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Database",
      "Distributed computing",
      "Graph",
      "Graph database",
      "Graph rewriting",
      "Graph traversal",
      "Interoperability",
      "Object (grammar)",
      "Scalability",
      "Software engineering",
      "Theoretical computer science",
      "Traceability",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Byun",
        "given_name": "Jaewook"
      },
      {
        "surname": "Kim",
        "given_name": "Daeyoung"
      }
    ]
  },
  {
    "title": "Learning hidden Markov models with persistent states by penalizing jumps",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113307",
    "abstract": "Hidden Markov models are applied in many expert and intelligent systems to detect an underlying sequence of persistent states. When the model is misspecified or misestimated, however, it often leads to unrealistically rapid switching dynamics. To address this issue, we propose a novel estimation approach based on clustering temporal features while penalizing jumps. We compare the approach to spectral clustering and the standard approach of maximizing the likelihood function in an extensive simulation study and an application to financial data. The advantages of the proposed jump estimator include that it learns the hidden state sequence and model parameters simultaneously and faster while providing control over the transition rate, it is less sensitive to initialization, it performs better when the number of states increases, and it is robust to misspecified conditional distributions. The value of estimating the true persistence of the state process is illustrated through a simple trading strategy where improved estimates result in much lower transaction costs. Robustness is particularly critical when the model is part of a system used in production. Therefore, our proposed estimator significantly improves the potential for using hidden Markov models in practical applications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420301329",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Estimator",
      "Forward algorithm",
      "Gene",
      "Hidden Markov model",
      "Hidden semi-Markov model",
      "Initialization",
      "Machine learning",
      "Markov chain",
      "Markov model",
      "Mathematics",
      "Programming language",
      "Robustness (evolution)",
      "Statistics",
      "Variable-order Markov model"
    ],
    "authors": [
      {
        "surname": "Nystrup",
        "given_name": "Peter"
      },
      {
        "surname": "Lindström",
        "given_name": "Erik"
      },
      {
        "surname": "Madsen",
        "given_name": "Henrik"
      }
    ]
  },
  {
    "title": "Non-numerical nearest neighbor classifiers with value-object hierarchical embedding",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113206",
    "abstract": "Non-numerical classification plays an essential role in many real-world applications such as DNA analysis, recommendation systems and expert systems. The nearest neighbor classifier is one of the most popular and flexible models for performing classification tasks in these applications. However, due to the complexity of non-numerical data, existing nearest neighbor classifiers that use the overlap measure and its variants cannot capture the inherent ordered relationship and statistic information of non-numerical data. This phenomenon leads to the classification limitation of nearest neighbor classifiers in non-numerical data environments. To overcome this challenge, we propose a novel object distance metric, i.e., value-object hierarchical metric (VOHM), which is able to capture inherent ordered relationships within non-numerical data. Then, we construct two nearest neighbor classifiers, i.e., the value-object hierarchical embedded nearest neighbor classifier (VO-kNN) and the two-stage value-object hierarchical embedded nearest neighbor classifier (TSVO-kNN), which take advantages of both VOHM and non-numerical feature selection. Experiments show that both VO-kNN and TSVO-kNN could mine more knowledge from data and achieve better performance than state-of-the-art classifiers in non-numerical data environments.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300324",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Embedding",
      "Machine learning",
      "Nearest neighbor search",
      "Pattern recognition (psychology)",
      "k-nearest neighbors algorithm"
    ],
    "authors": [
      {
        "surname": "Luo",
        "given_name": "Sheng"
      },
      {
        "surname": "Miao",
        "given_name": "Duoqian"
      },
      {
        "surname": "Zhang",
        "given_name": "Zhifei"
      },
      {
        "surname": "Wei",
        "given_name": "Zhihua"
      }
    ]
  },
  {
    "title": "A hybrid energy–Aware virtual machine placement algorithm for cloud environments",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113306",
    "abstract": "The high energy consumption of cloud data centers presents a significant challenge from both economic and environmental perspectives. Server consolidation using virtualization technology is widely used to reduce the energy consumption rates of data centers. Efficient Virtual Machine Placement (VMP) plays an important role in server consolidation technology. VMP is an NP-hard problem for which optimal solutions are not possible, even for small-scale data centers. In this paper, a hybrid VMP algorithm is proposed based on another proposed improved permutation-based genetic algorithm and multidimensional resource-aware best fit allocation strategy. The proposed VMP algorithm aims to improve the energy consumption rate of cloud data centers through minimizing the number of active servers that host Virtual Machines (VMs). Additionally, the proposed VMP algorithm attempts to achieve balanced usage of the multidimensional resources (CPU, RAM, and Bandwidth) of active servers, which in turn, reduces resource wastage. The performance of both proposed algorithms are validated through intensive experiments. The obtained results show that the proposed improved permutation-based genetic algorithm outperforms several other permutation-based algorithms on two classical problems (the Traveling Salesman Problem and the Flow Shop Scheduling Problem) using various standard datasets. Additionally, this study shows that the proposed hybrid VMP algorithm has promising energy saving and resource wastage performance compared to other heuristics and metaheuristics. Moreover, this study reveals that the proposed VMP algorithm achieves a balanced usage of the multidimensional resources of active servers while others cannot.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420301317",
    "keywords": [
      "Algorithm",
      "Biology",
      "Cloud computing",
      "Computer network",
      "Computer science",
      "Data center",
      "Distributed computing",
      "Ecology",
      "Energy consumption",
      "Heuristics",
      "Operating system",
      "Server",
      "Virtual machine",
      "Virtualization"
    ],
    "authors": [
      {
        "surname": "Abohamama",
        "given_name": "A.S."
      },
      {
        "surname": "Hamouda",
        "given_name": "Eslam"
      }
    ]
  },
  {
    "title": "Nonlinear process monitoring based on decentralized generalized regression neural networks",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113273",
    "abstract": "Given that the main task of process monitoring (i.e., fault detection) is actually a classical one-class classification problem, the generalized regression neural network (GRNN) is directly inapplicable for handling process modeling and monitoring issues. Through the selection of only one variable to be the output while the others serve as the corresponding input, a GRNN model can then be constructed to approximate the nonlinear input to output relationship. The residuals, signifying the inconsistency between the actual measurement and the predicted output from the GRNN model, could be a good indicator for online fault detection. The proposed nonlinear process monitoring approach is termed decentralized GRNN (DGRNN), which applies the GRNN in an extremely decentralized manner and utilizes the squared Mahalanobis distance for the online monitoring of the abnormalities captured by the generated residuals. The effectiveness and superiority of the DGRNN-based nonlinear process monitoring approach over other state-of-the-art nonlinear process monitoring methods are investigated by comparisons in two nonlinear processes.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300981",
    "keywords": [
      "Actuator",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Data mining",
      "Fault (geology)",
      "Fault detection and isolation",
      "Geology",
      "Machine learning",
      "Mahalanobis distance",
      "Mathematics",
      "Nonlinear system",
      "Operating system",
      "Pattern recognition (psychology)",
      "Physics",
      "Process (computing)",
      "Quantum mechanics",
      "Regression",
      "Seismology",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Lan",
        "given_name": "Ting"
      },
      {
        "surname": "Tong",
        "given_name": "Chudong"
      },
      {
        "surname": "Yu",
        "given_name": "Haizhen"
      },
      {
        "surname": "Shi",
        "given_name": "Xuhua"
      },
      {
        "surname": "Luo",
        "given_name": "Lijia"
      }
    ]
  },
  {
    "title": "An energy-efficient permutation flowshop scheduling problem",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113279",
    "abstract": "The permutation flowshop scheduling problem (PFSP) has been extensively explored in scheduling literature because it has many real-world industrial implementations. In some studies, multiple objectives related to production efficiency have been considered simultaneously. However, studies that consider energy consumption and environmental impacts are very rare in a multi-objective setting. In this work, we studied two contradictory objectives, namely, total flowtime and total energy consumption (TEC) in a green permutation flowshop environment, in which the machines can be operated at varying speed levels corresponding to different energy consumption values. A bi-objective mixed-integer programming model formulation was developed for the problem using a speed-scaling framework. To address the conflicting objectives of minimizing TEC and total flowtime, the augmented epsilon-constraint approach was employed to obtain Pareto-optimal solutions. We obtained near approximations for the Pareto-optimal frontiers of small-scale problems using a very small epsilon level. Furthermore, the mathematical model was run with a time limit to find sets of non-dominated solutions for large instances. As the problem was NP-hard, two effective multi-objective iterated greedy algorithms and a multi-objective variable block insertion heuristic were also proposed for the problem as well as a novel construction heuristic for initial solution generation. The performance of the developed heuristic algorithms was assessed on well-known benchmark problems in terms of various quality measures. Initially, the performance of the algorithms was evaluated on small-scale instances using Pareto-optimal solutions. Then, it was shown that the developed algorithms are tremendously effective for solving large instances in comparison to time-limited model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420301044",
    "keywords": [
      "Acoustics",
      "Algorithm",
      "Benchmark (surveying)",
      "Biology",
      "Computer network",
      "Computer science",
      "Ecology",
      "Energy consumption",
      "Geodesy",
      "Geography",
      "Heuristic",
      "Job shop scheduling",
      "Mathematical optimization",
      "Mathematics",
      "Metaheuristic",
      "Pareto principle",
      "Permutation (music)",
      "Physics",
      "Routing (electronic design automation)",
      "Scheduling (production processes)",
      "Variable neighborhood search"
    ],
    "authors": [
      {
        "surname": "Öztop",
        "given_name": "Hande"
      },
      {
        "surname": "Tasgetiren",
        "given_name": "M. Fatih"
      },
      {
        "surname": "Eliiyi",
        "given_name": "Deniz Türsel"
      },
      {
        "surname": "Pan",
        "given_name": "Quan-Ke"
      },
      {
        "surname": "Kandiller",
        "given_name": "Levent"
      }
    ]
  },
  {
    "title": "An Ontology-based approach to Knowledge-assisted Integration and Visualization of Urban Mobility Data",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113260",
    "abstract": "This paper proposes an ontology-based framework to support integration and visualization of data from Intelligent Transportation Systems. These activities may be technically demanding for transportation stakeholders, due to technical and human factors, and may hinder the use of visualization tools in practice. The existing ontologies do not provide the necessary semantics for integration of spatio-temporal data from such systems. Moreover, a formal representation of the components of visualization techniques and expert knowledge can leverage the development of visualization tools that facilitate data analysis. The proposed Visualization-oriented Urban Mobility Ontology (VUMO) provides a semantic foundation to knowledge-assisted visualization tools (KVTs). VUMO contains three facets that interrelate the characteristics of spatio-temporal mobility data, visualization techniques and expert knowledge. A built-in rule set leverages semantic technologies standards to infer which visualization techniques are compatible with analytical tasks, and to discover implicit relationships within integrated data. The annotation of expert knowledge encodes qualitative and quantitative feedback from domain experts that can be exploited by recommendation methods to automate part of the visualization workflow. Data from the city of Porto, Portugal were used to demonstrate practical applications of the ontology for each facet. As a foundational domain ontology, VUMO can be extended to meet the distinctiveness of a KVT.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300853",
    "keywords": [
      "Computer science",
      "Data mining",
      "Data science",
      "Database",
      "Epistemology",
      "Information retrieval",
      "Information visualization",
      "Knowledge base",
      "Ontology",
      "Ontology-based data integration",
      "Philosophy",
      "Semantic Web",
      "Visual analytics",
      "Visualization",
      "Workflow",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Sobral",
        "given_name": "Thiago"
      },
      {
        "surname": "Galvão",
        "given_name": "Teresa"
      },
      {
        "surname": "Borges",
        "given_name": "José"
      }
    ]
  },
  {
    "title": "Detection of illicit accounts over the Ethereum blockchain",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113318",
    "abstract": "The recent technological advent of cryptocurrencies and their respective benefits have been shrouded with a number of illegal activities operating over the network such as money laundering, bribery, phishing, fraud, among others. In this work we focus on the Ethereum network, which has seen over 400 million transactions since its inception. Using 2179 accounts flagged by the Ethereum community for their illegal activity coupled with 2502 normal accounts, we seek to detect illicit accounts based on their transaction history using the XGBoost classifier. Using 10 fold cross-validation, XGBoost achieved an average accuracy of 0.963 ( ± 0.006) with an average AUC of 0.994 ( ± 0.0007). The top three features with the largest impact on the final model output were established to be ‘Time diff between first and last (Mins)’, ‘Total Ether balance’ and ‘Min value received’. Based on the results we conclude that the proposed approach is highly effective in detecting illicit accounts over the Ethereum network. Our contribution is multi-faceted; firstly, we propose an effective method to detect illicit accounts over the Ethereum network; secondly, we provide insights about the most important features; and thirdly, we publish the compiled data set as a benchmark for future related works.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420301433",
    "keywords": [
      "Advertising",
      "Business",
      "Computer science",
      "Computer security",
      "Cryptocurrency",
      "Data mining",
      "Database",
      "Database transaction",
      "Finance",
      "Money laundering",
      "Operating system",
      "Phishing",
      "Publication",
      "The Internet"
    ],
    "authors": [
      {
        "surname": "Farrugia",
        "given_name": "Steven"
      },
      {
        "surname": "Ellul",
        "given_name": "Joshua"
      },
      {
        "surname": "Azzopardi",
        "given_name": "George"
      }
    ]
  },
  {
    "title": "Solving the generalized cubic cell formation problem using discrete flower pollination algorithm",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113345",
    "abstract": "The manufacturing cell formation represents one of the important stages of the construction of cellular manufacturing systems. It focuses on grouping machines, parts and workers and assigning them to the corresponding cells. This assignment is guided by multiple objectives, and is subject to many constraints. In this paper, the focus is made on a variant of the cell formation problem, which is the Generalized Cubic Cell Formation Problem (GCCFP). In this study, a mathematical model is developed for this variant of the problem. Besides the multiple objectives considered in most research works, the quality index of the produced parts is also considered in this study. To solve the problem, a Discrete Flower Pollination Algorithm (DFPA) is developed. To validate the model and the DFPA, a set of randomly generated instances were solved using B&B under LINGO software, DFPA and Simulated Annealing (SA) algorithm. The performance of DFPA, from the standpoint of the considered objectives and the time of calculation, has been tested. The experiment results show the efficiency of the developed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420301706",
    "keywords": [
      "Algorithm",
      "Cell formation",
      "Computer science",
      "Epistemology",
      "Mathematical optimization",
      "Mathematics",
      "Philosophy",
      "Programming language",
      "Quality (philosophy)",
      "Set (abstract data type)",
      "Simulated annealing"
    ],
    "authors": [
      {
        "surname": "Bouaziz",
        "given_name": "Hamida"
      },
      {
        "surname": "Berghida",
        "given_name": "Meryem"
      },
      {
        "surname": "Lemouari",
        "given_name": "Ali"
      }
    ]
  },
  {
    "title": "Alzheimer's disease and automatic speech analysis: A review",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113213",
    "abstract": "The objective of this paper is to present the state of-the-art relating to automatic speech and voice analysis techniques as applied to the monitoring of patients suffering from Alzheimer's disease as well as to shed light on possible future research topics. This work reviews more than 90 papers in the existing literature and focuses on the main feature extraction techniques and classification methods used. In order to guide researchers interested in working in this area, the most frequently used data repositories are also given. Likewise, it identifies the most clinically relevant results and the current lines developed in the field. Automatic speech analysis, within the Health 4.0 framework, offers the possibility of assessing these patients, without the need for a specific infrastructure, by means of non-invasive, fast and inexpensive techniques as a complement to the current diagnostic methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300397",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Complement (music)",
      "Complementation",
      "Computer science",
      "Data science",
      "Disease",
      "Feature extraction",
      "Field (mathematics)",
      "Gene",
      "Mathematics",
      "Medicine",
      "Natural language processing",
      "Pathology",
      "Phenotype",
      "Pure mathematics"
    ],
    "authors": [
      {
        "surname": "Pulido",
        "given_name": "María Luisa Barragán"
      },
      {
        "surname": "Hernández",
        "given_name": "Jesús Bernardino Alonso"
      },
      {
        "surname": "Ballester",
        "given_name": "Miguel Ángel Ferrer"
      },
      {
        "surname": "González",
        "given_name": "Carlos Manuel Travieso"
      },
      {
        "surname": "Mekyska",
        "given_name": "Jiří"
      },
      {
        "surname": "Smékal",
        "given_name": "Zdeněk"
      }
    ]
  },
  {
    "title": "Improving artificial algae algorithm performance by predicting candidate solution quality",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113298",
    "abstract": "The success of optimization algorithms is most of the time directly proportional to the number of fitness evaluations. However, not all fitness evaluations lead to successful fitness updates. Besides, the maximum number of fitness evaluations is limited and also balance of exploration and exploitation is still challenging. Best possible solution should be found in a reasonable time. Surely it can be said more fitness evaluation takes more time. Since methods are tested under fixed numbers of maximum fitness evaluation and the duration of each fitness evaluation of a problem may vary depending on the characteristic of the problem, finding best result with fewer fitness evaluations is challenging in optimization algorithms. For that reason in this study, we proposed a new method that predicts the quality of a candidate solution before evaluation of its fitness employing Gaussian-based Naïve Bayes probabilistic model. If the candidate solution is predicted to generate good result then that solution is evaluated by the objective function. Otherwise new candidate solution is created as usual. The primary purpose of the proposed method is improving the performance of AAA and at the same time preventing unnecessary fitness evaluation. The proposed method is evaluated using standard benchmark functions and CEC’05 test suite. The obtained results suggests that the new method outperformed the basic AAA and other state-of-the-art meta-heuristic algorithms with fewer fitness evaluations. Thus, the new method can be extended to cost sensitive industrial problems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420301238",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Fitness approximation",
      "Fitness function",
      "Genetic algorithm",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Regression analysis",
      "Test case",
      "Test suite"
    ],
    "authors": [
      {
        "surname": "Yibre",
        "given_name": "Abdulkerim Mohammed"
      },
      {
        "surname": "Koçer",
        "given_name": "Barış"
      }
    ]
  },
  {
    "title": "An evolutionary Pentagon Support Vector finder method",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113284",
    "abstract": "In dealing with big data, we need effective algorithms; effectiveness that depends, among others, on the ability to remove outliers from the data set, especially when dealing with classification problems. To this aim, support vector finder algorithms have been created to save just the most important data in the data pool. Nevertheless, existing classification algorithms, such as Fuzzy C-Means (FCM), suffer from the drawback of setting the initial cluster centers imprecisely. In this paper, we avoid existing shortcomings and aim to find and remove unnecessary data in order to speed up the final classification task without losing vital samples and without harming final accuracy; in this sense, we present a unique approach for finding support vectors, named evolutionary Pentagon Support Vector (PSV) finder method. The originality of the current research lies in using geometrical computations and evolutionary algorithms to make a more effective system, which has the advantage of higher accuracy on some data sets. The proposed method is subsequently tested with seven benchmark data sets and the results are compared to those obtained from performing classification on the original data (classification before and after PSV) under the same conditions. The testing returned promising results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420301093",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Big data",
      "Computer science",
      "Data mining",
      "Data set",
      "Economics",
      "Evolutionary algorithm",
      "Fuzzy logic",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Management",
      "Outlier",
      "Programming language",
      "Set (abstract data type)",
      "Support vector machine",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Mousavi",
        "given_name": "Seyed Muhammad Hossein"
      },
      {
        "surname": "Charles",
        "given_name": "Vincent"
      },
      {
        "surname": "Gherman",
        "given_name": "Tatiana"
      }
    ]
  },
  {
    "title": "Predicting online shopping behaviour from clickstream data using deep learning",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113342",
    "abstract": "Clickstream data is an important source to enhance user experience and pursue business objectives in e-commerce. The paper uses clickstream data to predict online shopping behavior and target marketing interventions in real-time. Such AI-driven targeting has proven to save huge amounts of marketing costs and raise shop revenue. Previous user behavior prediction models rely on supervised machine learning (SML). Conceptually, SML is less suitable because it cannot account for the sequential structure of clickstream data. The paper proposes a methodology capable of unlocking the full potential of clickstream data using the framework of recurrent neural networks (RNNs). An empirical evaluation based on real-world e-commerce data systematically assesses multiple RNN classifiers and compares them to SML benchmarks. To this end, the paper proposes an approach to measure the revenue impact of a targeting model. Estimates of revenue impact together with results of standard classifier performance metrics evidence the viability of RNN-based clickstream modeling and guide employing deep recurrent learners for campaign targeting. Given that the empirical analysis shows RNN-based and conventional classifiers to capture different patterns in clickstream data, a specific recommendation is to combine sequence and conventional classifiers in an ensemble. The paper shows such an ensemble to consistently outperform the alternative models considered in the study.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420301676",
    "keywords": [
      "Accounting",
      "Artificial intelligence",
      "Business",
      "Classifier (UML)",
      "Clickstream",
      "Computer science",
      "Data mining",
      "Empirical research",
      "Epistemology",
      "Machine learning",
      "Online advertising",
      "Philosophy",
      "Revenue",
      "The Internet",
      "Web API",
      "Web modeling",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Koehn",
        "given_name": "Dennis"
      },
      {
        "surname": "Lessmann",
        "given_name": "Stefan"
      },
      {
        "surname": "Schaal",
        "given_name": "Markus"
      }
    ]
  },
  {
    "title": "A scatter search method for heterogeneous fleet vehicle routing problem with release dates under lateness dependent tardiness costs",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113302",
    "abstract": "In this paper, we introduce a heterogeneous fleet vehicle routing problem with release and due dates in the presence of consolidation of customer orders and limited warehousing capacity. Tardiness cost for an order depends upon the magnitude of lateness and the type of the order. A mixed-integer programming model is proposed to minimize the sum of inventory holding, transportation, tardiness, and backorder costs. We demonstrate that customer order characteristics and limited warehousing capacity influence optimal vehicle routes significantly. As the problem is NP-hard, a scatter search (SS) method with strategic oscillation is designed to solve the large-sized instances. Problem-specific characteristics are used to customize the improvement method. Computational experiments suggest that SS finds excellent quality solutions compared to CPLEX in significantly lesser time. SS also performs better than the iterated local search used in the prior literature for the large-sized instances.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420301275",
    "keywords": [
      "Accounting",
      "Algorithm",
      "Business",
      "Computer network",
      "Computer science",
      "Consolidation (business)",
      "Economics",
      "Finance",
      "Holding cost",
      "Integer programming",
      "Iterated local search",
      "Job shop scheduling",
      "Mathematical optimization",
      "Mathematics",
      "Operations research",
      "Order (exchange)",
      "Routing (electronic design automation)",
      "Tabu search",
      "Tardiness",
      "Vehicle routing problem"
    ],
    "authors": [
      {
        "surname": "Soman",
        "given_name": "Jaikishan T."
      },
      {
        "surname": "Patil",
        "given_name": "Rahul J"
      }
    ]
  },
  {
    "title": "Motor imagery EEG recognition based on conditional optimization empirical mode decomposition and multi-scale convolutional neural network",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113285",
    "abstract": "Electroencephalogram (EEG) signals classification plays a crucial role in brain computer interfaces (BCIs) system. However, the inherent complex properties of EEG signals make it challenging to get them analyzed and modeled. In this paper, a novel method based on conditional empirical mode decomposition (CEMD) and one-dimensional multi-scale convolutional neural network (1DMSCNN) is proposed to recognize motor imagery (MI) EEG signals. In the CEMD algorithm, the correlation coefficient between the original EEG signal and each intrinsic modal component (IMF) is used as the first condition to select IMFs, and the relative energy occupancy rates between the IMFs are the second condition. The CEMD algorithm is applied to remove the noise of EEG signals. Then, an EEG signals combination method is proposed to encode event-related synchronization/de-synchronization (ERS/ERD) information between the channels. Finally, a model called 1DMSCNN is built to classify the processed EEG signals. The proposed method is applied to the dataset collected in our laboratory and BCI competition IV dataset 2b. The results indicate that the proposed method can achieve higher accuracy for EEG signals classification, compared with other state-of-the-art works. In addition, the proposed algorithm is applied to the online recognition of EEG signals, a BCI system that directly interacts with brain and wheelchair is designed and implemented. This system can directly command wheelchair to turn left and right through EEG signals. The online experimental results indicate that the designed intelligent wheelchair system is a feasible BCI application. It verifies the proposed algorithm can be used in expert and intelligent systems. Our method can provide a stimulus to the development of human-robot interaction.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742030110X",
    "keywords": [
      "Artificial intelligence",
      "Brain–computer interface",
      "Channel (broadcasting)",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Electroencephalography",
      "Filter (signal processing)",
      "Hilbert–Huang transform",
      "Image (mathematics)",
      "Motor imagery",
      "Noise (video)",
      "Pattern recognition (psychology)",
      "Psychiatry",
      "Psychology",
      "Speech recognition",
      "Synchronization (alternating current)",
      "Wheelchair",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Tang",
        "given_name": "Xianlun"
      },
      {
        "surname": "Li",
        "given_name": "Wei"
      },
      {
        "surname": "Li",
        "given_name": "Xingchen"
      },
      {
        "surname": "Ma",
        "given_name": "Weichang"
      },
      {
        "surname": "Dang",
        "given_name": "Xiaoyuan"
      }
    ]
  },
  {
    "title": "Anomaly explanation with random forests",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113187",
    "abstract": "Anomaly detection has become an important topic in many domains with many different solutions proposed until now. Despite that, there are only a few anomaly detection methods trying to explain how the sample differs from the rest. This work contributes to filling this gap because knowing why a sample is considered anomalous is critical in many application domains. The proposed solution uses a specific type of random forests to extract rules explaining the difference, which are then filtered and presented to the user as a set of classification rules sharing the same consequent, or as the equivalent rule with an antecedent in a disjunctive normal form. The quality of that solution is documented by comparison with the state of the art algorithms on 34 real-world datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300130",
    "keywords": [
      "Anomaly (physics)",
      "Anomaly detection",
      "Antecedent (behavioral psychology)",
      "Artificial intelligence",
      "Cardiology",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Condensed matter physics",
      "Data mining",
      "Developmental psychology",
      "Epistemology",
      "Medicine",
      "Philosophy",
      "Physics",
      "Programming language",
      "Psychology",
      "Quality (philosophy)",
      "Random forest",
      "Rest (music)",
      "Sample (material)",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Kopp",
        "given_name": "Martin"
      },
      {
        "surname": "Pevný",
        "given_name": "Tomáš"
      },
      {
        "surname": "Holeňa",
        "given_name": "Martin"
      }
    ]
  },
  {
    "title": "Novel trajectory privacy-preserving method based on clustering using differential privacy",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113241",
    "abstract": "With the development of location-aware technology, a large amount of location data of users is collected by the trajectory database. If these trajectory data are directly used for data mining without being processed, it will pose a threat to the user's personal privacy. At the moment, differential privacy is favored by experts and scholars because of its strict mathematical rigor, but how to apply differential privacy technology to trajectory clustering analysis is a difficult problem. To solve the problems in which existing trajectory privacy-preserving models have poor data availability or difficulty to resist complex privacy attacks, we devise novel trajectory privacy-preserving method based on clustering using differential privacy. More specifically, Laplacian noise is added to the count of trajectory location in the cluster to resist the continuous query attack. Then, radius-constrained Laplacian noise is added to the trajectory location data in the cluster to avoid too much noise affecting the clustering effect. According to the noise location data and the count of noise location, the noise clustering center in the cluster is obtained. Finally, it is considered that the attacker can associate the user trajectory with other information to form secret reasoning attack, and secret reasoning attack model is proposed. And we use the differential privacy technology to give corresponding resistance. Experimental results using the open data show that the proposed algorithm can not only effectively protect the private information of the trajectory data, but also ensure the data availability in cluster analysis. And compared with other algorithms, our algorithm has good effect on some evaluation indicators.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300671",
    "keywords": [
      "Artificial intelligence",
      "Astronomy",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Differential privacy",
      "Image (mathematics)",
      "Noise (video)",
      "Physics",
      "Trajectory"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Xiaodong"
      },
      {
        "surname": "Pi",
        "given_name": "Dechang"
      },
      {
        "surname": "Chen",
        "given_name": "Junfu"
      }
    ]
  },
  {
    "title": "An interval-stochastic programming based approach for a fully uncertain multi-objective and multi-mode resource investment project scheduling problem with an application to ERP project implementation",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113189",
    "abstract": "Most of the real-life project scheduling cases may involve different types of uncertainties simultaneously such as randomness, fuzziness and dynamism. Based on this motivation, the present paper proposes a novel interval programming and chance constrained optimization based hybrid solution approach for a fully uncertain, multi-objective and multi-mode resource investment project scheduling problem (MRIPSP). The classical discrete-time binary integer programming formulation of the problem is extended by incorporating both the interval-valued and interval-stochastic project parameters as well as variables. In addition to the uncertain project parameters/inputs, the completion times of the activities which represent the project schedule and the availabilities of the renewable project resources are also stated as uncertain project variables and represented by interval numbers. Then, the proposed interval-stochastic multi-mode resource investment project scheduling (IS-MRIPSP) model is converted into its crisp equivalent form by using the proposed approach. The proposed approach is also able to consider different types of project scheduling risks and produces more reliable and risk-free solutions according to the project manager's attitude toward risks. Furthermore, in addition to the classical makespan objective, effective and efficient utilization of the renewable project resources, i.e., human resources, is also targeted. The efficiency and reliability factors of the human resources are also taken into consideration. In order to generate balanced project schedules which tradeoff between the project time and total human resource costs, compromise programming approach is adapted. Finally, in order to test the validity and practicality of the proposed approach, a real-life application is presented for an enterprise resource planning (ERP) implementation project scheduling problem of an international industrial software company.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300154",
    "keywords": [
      "Algorithm",
      "Combinatorics",
      "Computer science",
      "Engineering",
      "Integer programming",
      "Interval (graph theory)",
      "Job shop scheduling",
      "Mathematical optimization",
      "Mathematics",
      "Operating system",
      "Operations research",
      "Project management",
      "Schedule",
      "Scheduling (production processes)",
      "Stochastic programming",
      "Systems engineering"
    ],
    "authors": [
      {
        "surname": "Subulan",
        "given_name": "Kemal"
      }
    ]
  },
  {
    "title": "Resolving data sparsity and cold start problem in collaborative filtering recommender system using Linked Open Data",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113248",
    "abstract": "The web contains a huge volume of data, and it's populating every moment to the point that human beings cannot deal with the vast amount of data manually or via traditional tools. Hence an advanced tool is required to filter such massive data and mine the valuable information. Recommender systems are among the most excellent tools for such a purpose in which collaborative filtering is widely used. Collaborative filtering (CF) has been extensively utilized to offer personalized recommendations in electronic business and social network websites. In that, matrix factorization is an efficient technique; however, it depends on past transactions of the users. Hence, there will be a data sparsity problem. Another issue with the collaborative filtering method is the cold start issue, which is due to the deficient information about new entities. A novel method is proposed to overcome the data sparsity and the cold start problem in CF. For cold start issue, Recommender System with Linked Open Data (RS-LOD) model is designed and for data sparsity problem, Matrix Factorization model with Linked Open Data is developed (MF-LOD). A LOD knowledge base “DBpedia” is used to find enough information about new entities for a cold start issue, and an improvement is made on the matrix factorization model to handle data sparsity. Experiments were done on Netflix and MovieLens datasets show that our proposed techniques are superior to other existing methods, which mean recommendation accuracy is improved.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300737",
    "keywords": [
      "Aerospace engineering",
      "Cold start (automotive)",
      "Collaborative filtering",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Eigenvalues and eigenvectors",
      "Engineering",
      "Filter (signal processing)",
      "Gaussian",
      "Information retrieval",
      "Linked data",
      "Matrix decomposition",
      "MovieLens",
      "Physics",
      "Quantum mechanics",
      "Recommender system",
      "Semantic Web",
      "Sparse matrix"
    ],
    "authors": [
      {
        "surname": "Natarajan",
        "given_name": "Senthilselvan"
      },
      {
        "surname": "Vairavasundaram",
        "given_name": "Subramaniyaswamy"
      },
      {
        "surname": "Natarajan",
        "given_name": "Sivaramakrishnan"
      },
      {
        "surname": "Gandomi",
        "given_name": "Amir H."
      }
    ]
  },
  {
    "title": "Convolution on neural networks for high-frequency trend prediction of cryptocurrency exchange rates using technical indicators",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113250",
    "abstract": "This study explores the suitability of neural networks with a convolutional component as an alternative to traditional multilayer perceptrons in the domain of trend classification of cryptocurrency exchange rates using technical analysis in high frequencies. The experimental work compares the performance of four different network architectures -convolutional neural network, hybrid CNN-LSTM network, multilayer perceptron and radial basis function neural network- to predict whether six popular cryptocurrencies -Bitcoin, Dash, Ether, Litecoin, Monero and Ripple- will increase their value vs. USD in the next minute. The results, based on 18 technical indicators derived from the exchange rates at a one-minute resolution over one year, suggest that all series were predictable to a certain extent using the technical indicators. Convolutional LSTM neural networks outperformed all the rest significantly, while CNN neural networks were also able to provide good results specially in the Bitcoin, Ether and Litecoin cryptocurrencies.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300750",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer security",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Cryptocurrency",
      "Deep learning",
      "Econometrics",
      "Economics",
      "Exchange rate",
      "Finance",
      "Machine learning",
      "Mathematics",
      "Multilayer perceptron",
      "Pattern recognition (psychology)",
      "Perceptron",
      "Technical analysis"
    ],
    "authors": [
      {
        "surname": "Alonso-Monsalve",
        "given_name": "Saúl"
      },
      {
        "surname": "Suárez-Cetrulo",
        "given_name": "Andrés L."
      },
      {
        "surname": "Cervantes",
        "given_name": "Alejandro"
      },
      {
        "surname": "Quintana",
        "given_name": "David"
      }
    ]
  },
  {
    "title": "A novel tourism recommender system in the context of social commerce",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113301",
    "abstract": "Web 2.0 and its services, such as social networks, have significantly influenced various businesses, including e-commerce. As a result, we face a new generation of e-commerce called Social Commerce. On the other hand, in the tourism industry, a variety of services and products are provided. The dramatic rise in the number of options in travel packages, hotels, tourist attractions, etc. put users in a difficult situation to find what they need. For a reason, tourism recommender systems have been considered by researchers and businesses as a solution. Since tourist attractions are often the reason for travelling, this research proposes a social-hybrid recommender system in the context of social commerce that recommends tourist attractions. The purpose of the research is presenting a personalized list of tourist attractions for each tourist based on the similarity of users' desires and interests, trust, reputation, relationships, and social communities. Compared with the traditional methods, collaborative filtering, content-based, and hybrid, the advantage of the proposed method is the use of various factors and the inclusion of trust factors in recommendation resources, (such as outlier detection in user ratings), and employing social relationships among individuals. The experimental results show the superiority of the proposed method over other common methods. The proposed method can also be used to recommend other products and services in the tourism industry and other social commerce.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420301263",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Business",
      "Collaborative filtering",
      "Computer science",
      "Context (archaeology)",
      "E-commerce",
      "Law",
      "Marketing",
      "Paleontology",
      "Political science",
      "Recommender system",
      "Reputation",
      "Social commerce",
      "Social media",
      "Social science",
      "Sociology",
      "Tourism",
      "Variety (cybernetics)",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Esmaeili",
        "given_name": "Leila"
      },
      {
        "surname": "Mardani",
        "given_name": "Shahla"
      },
      {
        "surname": "Golpayegani",
        "given_name": "Seyyed Alireza Hashemi"
      },
      {
        "surname": "Madar",
        "given_name": "Zeinab Zanganeh"
      }
    ]
  },
  {
    "title": "A hybrid safe semi-supervised learning method",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113295",
    "abstract": "Within the past few years, Safe Semi-Supervised Learning (S3L) has become a hot topic in the machine learning field and many related S3L methods have been proposed to safely exploit the unlabeled information. However, these methods only considered the risk from a single level, such as the instance or model level. They can not reduce the adverse effects of both the risky unlabeled instances and inappropriate learning models. Therefore, it is important to investigate a novel effective S3L method. In this paper, we present a hybrid S3L method which can inherit the merits of both the instance-level and model-level approaches. In our algorithm, multiple Graph-based SSL (GSSL) classifiers are firstly trained and used to predict the unlabeled instances. The risk degrees of the unlabeled instances and the qualities of the constructed graphs are then estimated through the predictions of multiple GSSL classifiers. Finally, we build two regularization terms to constrain the predictions of the unlabeled instances and adaptively select the graphs with high qualities. These regularization terms aim at reducing the negative effect of both the risky unlabeled instances and inappropriate learning models with low-quality graphs. Experimental results on different real-world datasets verify the effectiveness of our algorithm by comparisons to the state-of-the-art Supervised Learning (SL), SSL and S3L methods. In conclusion, our algorithm can not only enrich the research of S3L, but enlarge the practical scope of SSL in the expert and intelligent systems to a certain extent.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420301202",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer security",
      "Exploit",
      "Graph",
      "Machine learning",
      "Regularization (linguistics)",
      "Semi-supervised learning",
      "Supervised learning",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Gan",
        "given_name": "Haitao"
      },
      {
        "surname": "Guo",
        "given_name": "Li"
      },
      {
        "surname": "Xia",
        "given_name": "Siyu"
      },
      {
        "surname": "Wang",
        "given_name": "Tao"
      }
    ]
  },
  {
    "title": "A new approach for instance selection: Algorithms, evaluation, and comparisons",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113297",
    "abstract": "Several approaches for instance selection have been put forward as a primary step to increase the efficiency and accuracy of algorithms applied to mine big data. The instance selection task scales indeed big data down by removing irrelevant, redundant, and unreliable data, which, in turn, reduces the computational resources necessary for completing the mining task. The local density-based approaches are recently acknowledged as feasible approaches in terms of reduction rate, effectiveness, and computation time metrics. However, these approaches endure low classification accuracy results compared with other approaches. In this manuscript, we propose a new layered and operational approach to address these limitations as well as advance the state-of-the-art by balancing among classification accuracy, reduction rate, and time complexity. We commence by designing a new algorithm (called GDIS) that selects most relevant instances using a global density and relevance functions. This enable us to consider a global view overall a data set to get a better classification accuracy results than current density-based approaches. We design another novel algorithm (called EGDIS), which maintains the effectiveness results of the GDIS algorithm while improving reduction rate results. Moreover, we compare our algorithms against three state-of-the-art algorithms to validate their performance. We develop a Java toolkit called ISTK on the top of the GDIS and EGDIS algorithms, the density-based approaches, and the state-of-the-art algorithms. We also develop a suitable user interface and its management and validation capabilities to ease-of-use and visualize results and data sets. We evaluate and test the performance of our algorithms in terms of four metrics (reduction rate, classification accuracy, effectiveness, and computation time) using twenty-four standard data sets and conduct an intensive set of experiments. The experimental results proved that the GDIS algorithm outperforms the density-based approaches in terms of classification accuracy and effectiveness, the EGDIS algorithm outperforms the density-based approaches in terms of reduction rate and effectiveness, and the GDIS and EGDIS algorithms outperform the state-of-the-art algorithms in terms of achieving a good results in both the effectiveness and computation time metrics. We finally test the scalability and compute experimentally the polynomial-time complexity of our algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420301226",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Big data",
      "Computation",
      "Computer science",
      "Data mining",
      "Economics",
      "Geometry",
      "Law",
      "Machine learning",
      "Management",
      "Mathematics",
      "Political science",
      "Programming language",
      "Reduction (mathematics)",
      "Relevance (law)",
      "Selection (genetic algorithm)",
      "Set (abstract data type)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Malhat",
        "given_name": "Mohamed"
      },
      {
        "surname": "Menshawy",
        "given_name": "Mohamed El"
      },
      {
        "surname": "Mousa",
        "given_name": "Hamdy"
      },
      {
        "surname": "Sisi",
        "given_name": "Ashraf El"
      }
    ]
  },
  {
    "title": "Classification of human hand movements based on EMG signals using nonlinear dimensionality reduction and data fusion techniques",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113281",
    "abstract": "Surface electromyography (EMG) is non-invasive signal acquisition technique that plays a central role in many application, including clinical diagnostics, control for prosthetic devices and for human-machine interactions. The processing typically begins with a feature extraction step, which may be followed by the application of a dimensionality reduction technique. The obtained reduced features are input for a machine learning classifier. The constructed machine learning model may then classify new recorded movements. The features extracted for EMG signals usually capture information both from the time and from the frequency domain. Short time Fourier transform (STFT) is commonly used for signal processing and in particular for EMG processing since it captures the temporal and the frequency characteristics of the data. Since the number of calculated STFT features is large, a common approach in signal processing and machine learning applications is to apply a linear or a nonlinear dimensionality reduction technique for simplifying the feature space. Another aspect that arises in medical applications in general and in EMG based hand classification in particular, is the large variability between subjects. Due to this variability, many studies focus on single subject classification. This requires acquiring a large training set for each tested participant which is not practical in real life application. The objectives of this study were first to compare between the performances of a nonlinear dimensionality technique to a standard linear dimensionality method when applied for single subject EMG based hand movement classification, and to examined their performances in case of limited amount of training data samples. The second objective was to propose an algorithm for multi-subjects classification that utilized a data alignment step for overcoming the large variability between subjects. The data set included EMG signals from 5 subjects who perform 6 different hand movements. STFT was calculated for feature extraction, principal component analysis (PCA) and diffusion maps (DM) were compared for dimension reductions. An affine transformation for aligning between the reduced feature spaces of two subjects, was investigated. K-nearest neighbors (KNN) was used for single and multi-subject classification. The results of this study clearly show that the DM outperformed the PCA in case of limited training data. In addition, the multi-subject classification approach, which utilizes dimension reduction methods along with an alignment algorithm enable robust classification of a new subject based on another subjects’ data sets. The proposed framework is general and can be adopted for many EMG classification task.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420301068",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer hardware",
      "Computer science",
      "Curse of dimensionality",
      "Digital signal processing",
      "Dimensionality reduction",
      "Feature extraction",
      "Feature vector",
      "Fourier analysis",
      "Fourier transform",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Short-time Fourier transform",
      "Signal processing",
      "Speech recognition",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Rabin",
        "given_name": "Neta"
      },
      {
        "surname": "Kahlon",
        "given_name": "Maayan"
      },
      {
        "surname": "Malayev",
        "given_name": "Sarit"
      },
      {
        "surname": "Ratnovsky",
        "given_name": "Anat"
      }
    ]
  },
  {
    "title": "An improved firefly algorithm for global continuous optimization problems",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113340",
    "abstract": "Global continuous optimization is populated by its implementation in many real-world applications. Such optimization problems are often solved by nature-inspired and meta-heuristic algorithms, including the firefly algorithm (FA), which offers fast exploration and exploitation. To further strengthen FA’s search for global optimum, a Levy-flight FA (LF-FA) has been developed through sampling from a Levy distribution instead of the traditional uniform one. However, due to its poor exploitation in local areas, the LF-FA does not guarantee fast convergence. To address this problem, this paper provides an adaptive logarithmic spiral-Levy FA (AD-IFA) that strengthens the LF-FA’s local exploitation and accelerates its convergence. Our AD-IFA is integrated with logarithmic-spiral guidance to its fireflies’ paths, and adaptive switching between exploration and exploitation modes during the search process. Experimental results show that the AD-IFA presented in this paper consistently outperforms the standard FA and LF-FA for 29 test functions and 6 real cases of global optimization problems in terms of both computation speed and derived optimum.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420301652",
    "keywords": [
      "Algorithm",
      "Computation",
      "Computer science",
      "Computer vision",
      "Convergence (economics)",
      "Derivative-free optimization",
      "Economic growth",
      "Economics",
      "Filter (signal processing)",
      "Firefly algorithm",
      "Global optimization",
      "Heuristic",
      "Lévy flight",
      "Mathematical optimization",
      "Mathematics",
      "Metaheuristic",
      "Multi-swarm optimization",
      "Optimization problem",
      "Particle swarm optimization",
      "Random walk",
      "Sampling (signal processing)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Jinran"
      },
      {
        "surname": "Wang",
        "given_name": "You-Gan"
      },
      {
        "surname": "Burrage",
        "given_name": "Kevin"
      },
      {
        "surname": "Tian",
        "given_name": "Yu-Chu"
      },
      {
        "surname": "Lawson",
        "given_name": "Brodie"
      },
      {
        "surname": "Ding",
        "given_name": "Zhe"
      }
    ]
  },
  {
    "title": "An Automatic Nucleus Segmentation and CNN Model based Classification Method of White Blood Cell",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113211",
    "abstract": "White blood cells (WBCs) play a remarkable role in the human immune system. To diagnose blood-related diseases, pathologists need to consider the characteristics of WBC. The characteristics of WBC can be defined based on the morphological properties of WBC nucleus. Therefore, nucleus segmentation plays a vital role to classify the WBC image and it is an important part of the medical diagnosis system. In this study, color space conversion and k-means algorithm based new WBC nucleus segmentation method is proposed. Then we localize the WBC based on the location of segmented nucleus to separate them from the entire blood smear image. To classify the localized WBC image, we propose a new convolutional neural network (CNN) model by combining the concept of fusing the features of first and last convolutional layers, and propagating the input image to the convolutional layer. We also use a dropout layer for preventing the model from overfitting problem. We show the effectiveness of our proposed nucleus segmentation method by evaluating with seven quality metrics and comparing with other methods on four public databases. We achieve average accuracy of 98.61% and more than 97% on each public database. We also evaluate our proposed CNN model by using nine classification metrics and achieve an overall accuracy of 96% on BCCD test database. To validate the generalization capability of our proposed CNN model, we show the training and testing accuracy and loss curves for random test set of BCCD database. Further, we compare the performance of our proposed CNN model with four state-of-the-art CNN models (biomedical image classifier) by measuring the value of evaluation metrics.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300373",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Neuroscience",
      "Nucleus",
      "Pattern recognition (psychology)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Banik",
        "given_name": "Partha Pratim"
      },
      {
        "surname": "Saha",
        "given_name": "Rappy"
      },
      {
        "surname": "Kim",
        "given_name": "Ki-Doo"
      }
    ]
  },
  {
    "title": "Study of Hellinger Distance as a splitting metric for Random Forests in balanced and imbalanced classification datasets",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113264",
    "abstract": "Hellinger Distance (HD) is a splitting metric that has been shown to have an excellent performance for imbalanced classification problems for methods based on Bagging of trees, while also showing good performance for balanced problems. Given that Random Forests (RF) use Bagging as one of two fundamental techniques to create diversity in the ensemble, it could be expected that HD is also effective for this ensemble method. The main aim of this article is to carry out an extensive investigation on important aspects about the use of HD in RF, including handling of multi-class problems, hyper-parameter optimization, metrics comparison, probability estimation, and metrics combination. In particular, HD is compared to other commonly used splitting metrics (Gini and Gain Ratio) in several contexts: balanced/imbalanced and two-class/multi-class. Two aspects related to classification problems are assessed: classification itself and probability estimation. HD is defined for two-class problems, but there are several ways in which it can be extended to deal with multi-class and this article studies the performance of the available options. Finally, even though HD can be used as an alternative to other splitting metrics, there is no reason to limit RF to use just one of them. Therefore, the final study of this article is to determine whether selecting the splitting metric using cross-validation on the training data can improve results further. Results show HD to be a robust measure for RF, with some weakness for balanced multi-class datasets (especially for probability estimation). Combination of metrics is able to result in a more robust performance. However, experiments of HD with text datasets show Gini to be more suitable than HD for this kind of problems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300890",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Data mining",
      "Economics",
      "Hellinger distance",
      "Machine learning",
      "Management",
      "Mathematics",
      "Metric (unit)",
      "Operations management",
      "Pattern recognition (psychology)",
      "Performance metric",
      "Random forest",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Aler",
        "given_name": "Ricardo"
      },
      {
        "surname": "Valls",
        "given_name": "José M."
      },
      {
        "surname": "Boström",
        "given_name": "Henrik"
      }
    ]
  },
  {
    "title": "A multi-objective genetic algorithm for text feature selection using the relative discriminative criterion",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113276",
    "abstract": "With exponentially increasing the number of digital documents, text classification has become a major task in data science applications. Selecting discriminative features highly relevant to class labels while having low levels of redundancy is essential to improve the performance of text classification methods. In this paper, we propose a novel multi-objective algorithm for text feature selection, called Multi-Objective Relative Discriminative Criterion (MORDC), which balances minimal redundant features against those maximally relevant to the target class. The proposed method employs a multi-objective evolutionary framework to search through the solution space. The first objective function measures the relevance of the text features to the target class, whereas the second one evaluates the correlation between the features. None of these objectives use learning to evaluate the goodness of the selected features; thus, the proposed method can be classified as a multivariate filter method. In order to assess the effectiveness of the proposed method, several experiments are performed on three real-world datasets. Comparisons with state-of-the-art feature selection methods show that in most cases MORDC results in better classification performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420301019",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Discriminative model",
      "Feature selection",
      "Filter (signal processing)",
      "Machine learning",
      "Operating system",
      "Pattern recognition (psychology)",
      "Redundancy (engineering)"
    ],
    "authors": [
      {
        "surname": "Labani",
        "given_name": "Mahdieh"
      },
      {
        "surname": "Moradi",
        "given_name": "Parham"
      },
      {
        "surname": "Jalili",
        "given_name": "Mahdi"
      }
    ]
  },
  {
    "title": "Efficient algorithms for discrete resource allocation problems under degressively proportional constraints",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113293",
    "abstract": "The problem of a fair distribution is considered in relation to many areas and phenomena. The most deeply rooted in the theory of justice are proportional divisions. However, they may be perceived as unfair for common ventures, where strong participants should not dominate the weaker ones. The European Parliament composition and the cost sharing problem of a common infrastructure development are examples. In this paper, we propose an expert system that is based on a mathematical model describing discussed issues as the discrete resource allocation problem under degressively proportional constraints. This approach involves advantages of degressive proportionality to prevent mentioned domination and a proportional division generally perceived as fair to determine an unambiguous allocation. The decision making process is carried out by solving the formulated optimization problem using our highly scalable parallel branch and bound algorithm and the computationally efficient metaheuristic. The experiments prove that our approach can be successfully applied for the considered cases studies.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420301184",
    "keywords": [
      "Algorithm",
      "Computer network",
      "Computer science",
      "Database",
      "Fair division",
      "Law",
      "Mathematical economics",
      "Mathematical optimization",
      "Mathematics",
      "Metaheuristic",
      "Operations research",
      "Political science",
      "Proportionality (law)",
      "Resource allocation",
      "Scalability"
    ],
    "authors": [
      {
        "surname": "Rudek",
        "given_name": "Radosław"
      },
      {
        "surname": "Heppner",
        "given_name": "Izabela"
      }
    ]
  },
  {
    "title": "Artificial electric field algorithm for engineering optimization problems",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113308",
    "abstract": "Nature-inspired optimization algorithms have attracted significant attention from researchers during the past decades due to their applicability to solving the challenging optimization problems, efficiently. Many intelligent systems require an excellent constrained optimization scheme to act as an artificially intelligent system. Artificial electric field algorithm (AEFA) is an intelligently designed artificial system that deals with the purpose of function optimization. AEFA works on the principle of Coulombs’ law of electrostatic force and Newtons’ law of motion. The present article extends the AEFA algorithm for constrained optimization problems by introducing the new velocity and position bound strategies. These bounds lead the particle to interact with each other within the domain of the problem, and they are allowed to learn from the problem space individually. They also help to make a better balance between exploration and exploitation by controlling the position update of the particles. The challenging IEEE CEC 2017 constrained benchmark set of 28 problems, and five multidimensional non-linear structural design optimization problems are solved using AEFA-C, which tests the effectiveness and the efficiency of the proposed scheme. The comparative study of AEFA-C is performed with nine state-of-art algorithms, including some IEEE CEC 2017 competitors. The comparative study, statistical analysis, and the findings suggest that the proposed AEFA-C is an efficient constrained optimizer.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420301330",
    "keywords": [
      "Algorithm",
      "Benchmark (surveying)",
      "Computer science",
      "Domain (mathematical analysis)",
      "Economics",
      "Engineering optimization",
      "Field (mathematics)",
      "Finance",
      "Geodesy",
      "Geography",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Optimization problem",
      "Position (finance)",
      "Pure mathematics"
    ],
    "authors": [
      {
        "surname": "Anita",
        "given_name": ""
      },
      {
        "surname": "Yadav",
        "given_name": "Anupam"
      },
      {
        "surname": "Kumar",
        "given_name": "Nitin"
      }
    ]
  },
  {
    "title": "A Choquet integral based fuzzy logic approach to solve uncertain multi-criteria decision making problem",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113303",
    "abstract": "Nowadays, the fuzzy measures and fuzzy integrals have been successfully implemented to solve a variety of uncertain multi-criteria decision-making problems. However, with the growing complexity of the decision-making environment and the diversity of linguistic information in the decision-making process, defining the appropriate and reasonable measures and integrals in the fuzzy logic applications becomes increasingly challenging. As the commonly used interval-valued Sugeno probability space is capable of representing the linguistic information in a more accurate way, in this work, we combine the Choquet integrals with interval-valued Sugeno probability space to develop a new interval-valued function to solve the uncertain multi-criteria decision-making problems. This work first defines the interval-valued Sugeno probability measure based on σ − λ rules and proposes the Choquet integrals in the interval-valued Sugeno probability space, and then a relevant solution approach is developed to solve the uncertain multi-criteria decision-making problems. The refrigerator components end-of-life strategy determination problem is used as a case study to illustrate the application.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420301287",
    "keywords": [
      "Artificial intelligence",
      "Choquet integral",
      "Combinatorics",
      "Computer science",
      "Data mining",
      "Fuzzy logic",
      "Fuzzy measure theory",
      "Fuzzy number",
      "Fuzzy set",
      "Interval (graph theory)",
      "Mathematical optimization",
      "Mathematics",
      "Measure (data warehouse)"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Li"
      },
      {
        "surname": "Duan",
        "given_name": "Gang"
      },
      {
        "surname": "Wang",
        "given_name": "SuYun"
      },
      {
        "surname": "Ma",
        "given_name": "JunFeng"
      }
    ]
  },
  {
    "title": "Using a neural network-based feature extraction method to facilitate citation screening for systematic reviews",
    "journal": "Expert Systems with Applications: X",
    "year": "2020",
    "doi": "10.1016/j.eswax.2020.100030",
    "abstract": "Citation screening is a labour-intensive part of the process of a systematic literature review that identifies citations eligible for inclusion in the review. In this paper, we present an automatic text classification approach that aims to prioritise eligible citations earlier than ineligible ones and thus reduces the manual labelling effort that is involved in the screening process. e.g. by automatically excluding lower ranked citations. To improve the performance of the text classifier, we develop a novel neural network-based feature extraction method. Unlike previous approaches to citation screening that employ unsupervised feature extraction methods to address a supervised classification task, our proposed method extracts document features in a supervised setting. In particular, our method generates a feature representation for documents, which is explicitly optimised to discriminate between eligible and ineligible citations. The generated document representation is subsequently used to train a text classifier. Experiments show that our feature extraction method obtains average workload savings of 56% when evaluated across 23 medical systematic reviews. The proposed method outperforms 10 baseline feature extraction methods by approximately 6% in terms of the WSS@95% metric.",
    "link": "https://www.sciencedirect.com/science/article/pii/S2590188520300093",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Citation",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Engineering",
      "Feature (linguistics)",
      "Feature extraction",
      "Information retrieval",
      "Linguistics",
      "Machine learning",
      "Metric (unit)",
      "Operations management",
      "Pattern recognition (psychology)",
      "Philosophy",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Kontonatsios",
        "given_name": "Georgios"
      },
      {
        "surname": "Spencer",
        "given_name": "Sally"
      },
      {
        "surname": "Matthew",
        "given_name": "Peter"
      },
      {
        "surname": "Korkontzelos",
        "given_name": "Ioannis"
      }
    ]
  },
  {
    "title": "Optimization of coverage mission for lightweight unmanned aerial vehicles applied in crop data acquisition",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113227",
    "abstract": "The crop data acquisition with unmanned aerial vehicles is a popularized alternative to manage the agricultural processes, due to data emerging from portable sensors for image-gathering. Nevertheless, most unmanned aerial vehicles for data acquisition excess the cost of hundreds of dollars, making them inappropriate for small agricultural producers. In this paper, we proposed to achieve crop data acquisition using a Lightweight Unmanned Aerial Vehicle (LUAV), available at a reasonable cost. However, a LUAV has less flight time and robustness than the professional vehicles. To overcome the limitations, we designed a LUAV agent with the goal of optimizing coverage paths using a heuristic strategy in known areas. The path to follow can be selected from three algorithms, Wavefront, Dijkstra or Spiral, which are compared to define an option for the crop under study. A second goal is to improve the LUAV robustness, which was resolved from planning by selecting the start of the coverage mission in order to the flight lines cross the direction of the wind. We complemented the robustness of outdoors positioning using a Kalman Filter extension to specify movements during missions. Finally, using an AR Drone 2.0 quadcopter, we developed a prototype of the LUAV agent to obtain the mosaic of a grass crop. The results respect to optimized coverage mission showed that the Spiral algorithm with a Backtracking technique and avoiding areas of little interest, got the balanced score between revisits, turns, coverage percent and traveled distance. About the LUAV robustness in the presence of wind, the results stated an error of less than 2 m, considered acceptable for image-acquisition purposes. The developed work is simple but effective, and makes evident the viability for that any LUAV type can support the precision agriculture processes in favorable costs.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300531",
    "keywords": [
      "Aerospace engineering",
      "Artificial intelligence",
      "Biochemistry",
      "Biology",
      "Chemistry",
      "Computer science",
      "Drone",
      "Engineering",
      "Gene",
      "Genetics",
      "Motion planning",
      "Quadcopter",
      "Real-time computing",
      "Robot",
      "Robustness (evolution)",
      "Simulation",
      "Waypoint"
    ],
    "authors": [
      {
        "surname": "Campo",
        "given_name": "Liseth Viviana"
      },
      {
        "surname": "Ledezma",
        "given_name": "Agapito"
      },
      {
        "surname": "Corrales",
        "given_name": "Juan Carlos"
      }
    ]
  },
  {
    "title": "Alleviating the data sparsity problem of recommender systems by clustering nodes in bipartite networks",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113346",
    "abstract": "Recommender systems help users to find information that fits their preferences in an overloaded search space. Collaborative filtering systems suffer from increasingly severe data sparsity problem because more and more products are sold in commercial websites, which largely constrains the performance of recommendation algorithms. User clustering has already been applied to recommendation on sparse data in the literature, but in a completely different way. In most existing works, user clustering is directly used to identify the similar users of the target user to whom we want to make recommendation. More specifically, the users who are clustered in the same group of the target user are considered as similar users. However, in this paper we use user clustering to reconstruct the user-item bipartite network such that the network density is significantly improved. The recommendation made on this dense network thus can achieve much higher accuracy than on the original sparse network. The experimental results on three benchmark data sets demonstrate that, when facing the problem of data sparsity, our proposed recommendation algorithm based on node clustering achieves a significant improvement in accuracy and coverage of recommendation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420301718",
    "keywords": [
      "Benchmark (surveying)",
      "Bipartite graph",
      "Cluster analysis",
      "Collaborative filtering",
      "Computer science",
      "Data mining",
      "Engineering",
      "Geodesy",
      "Geography",
      "Graph",
      "Information retrieval",
      "Machine learning",
      "Node (physics)",
      "Recommender system",
      "Structural engineering",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Fuguo"
      },
      {
        "surname": "Qi",
        "given_name": "Shumei"
      },
      {
        "surname": "Liu",
        "given_name": "Qihua"
      },
      {
        "surname": "Mao",
        "given_name": "Mingsong"
      },
      {
        "surname": "Zeng",
        "given_name": "An"
      }
    ]
  },
  {
    "title": "A multimodal particle swarm optimization-based approach for image segmentation",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113233",
    "abstract": "Color image segmentation is a fundamental challenge in the field of image analysis and pattern recognition. In this paper, a novel automated pixel clustering and color image segmentation algorithm is presented. The proposed method operates in three successive stages. In the first stage, a three-dimensional histogram of pixel colors based on the RGB model is smoothened using a Gaussian filter. This process helps to eliminate unreliable and non-dominating peaks that are too close to one another in the histogram. In the next stage, the peaks representing different clusters in the histogram are identified using a multimodal particle swarm optimization algorithm. Finally, pixels are assigned to the most appropriate cluster based on Euclidean distance. Determining the number of clusters to be used is often a manual process left for a user and represents a challenge for various segmentation algorithms. The proposed method is designed to determine an appropriate number of clusters, in addition to the actual peaks, automatically. Experiments confirm that the proposed approach yields desirable results, demonstrating that it can find an appropriate set of clusters for a set of well-known benchmark images.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300592",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Computer vision",
      "Histogram",
      "Image (mathematics)",
      "Image histogram",
      "Image segmentation",
      "Image texture",
      "Pattern recognition (psychology)",
      "Pixel",
      "RGB color model",
      "Region growing",
      "Scale-space segmentation",
      "Segmentation",
      "Segmentation-based object categorization"
    ],
    "authors": [
      {
        "surname": "Farshi",
        "given_name": "Taymaz Rahkar"
      },
      {
        "surname": "Drake",
        "given_name": "John H."
      },
      {
        "surname": "Özcan",
        "given_name": "Ender"
      }
    ]
  },
  {
    "title": "Solving complex problems using model transformations: from set constraint modeling to SAT instance solving",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113243",
    "abstract": "On the one hand, solvers for the propositional satisfiability problem (SAT) can deal with huge instances composed of millions of variables and clauses. On the other hand, Constraint Satisfaction Problems (CSP) can model problems as constraints over a set of variables with non-empty domains. They require combinatorial search methods as well as heuristics to be solved in a reasonable time. In this article, we present a technique that benefits from both expressive CSP modeling and efficient SAT solving. We model problems as CSP set constraints. Then, a propagation algorithm reduces the domains of variables by removing values that cannot participate in any valid assignment. The reduced CSP set constraints are transformed into a set of suitable SAT instances. They may be simplified by a preprocessing method before applying a standard SAT solver for computing their solutions. The practical usefulness of this technique is illustrated with two well-known problems: a) the Social Golfer, and b) the Sports Tournament Scheduling. We obtained competitive results either compared with ad hoc solvers or with hand-written SAT instances. Compared with direct SAT modeling, the proposed technique offers higher expressiveness, is less error-prone, and is relatively simpler to apply. The automatically generated propositional satisfiability instances are rather small in terms of clauses and variables. Hence, applying the constraint propagation phase, even huge instances of our problems can be tackled and efficiently solved.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300695",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Backtracking",
      "Boolean function",
      "Boolean satisfiability problem",
      "Computer science",
      "Constraint (computer-aided design)",
      "Constraint programming",
      "Constraint satisfaction",
      "Constraint satisfaction problem",
      "Geometry",
      "Heuristics",
      "Mathematical optimization",
      "Mathematics",
      "Maximum satisfiability problem",
      "Probabilistic logic",
      "Programming language",
      "Satisfiability",
      "Set (abstract data type)",
      "Solver",
      "Stochastic programming",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Lardeux",
        "given_name": "Frédéric"
      },
      {
        "surname": "Monfroy",
        "given_name": "Éric"
      },
      {
        "surname": "Rodriguez-Tello",
        "given_name": "Eduardo"
      },
      {
        "surname": "Crawford",
        "given_name": "Broderick"
      },
      {
        "surname": "Soto",
        "given_name": "Ricardo"
      }
    ]
  },
  {
    "title": "Classification of brain MRI using hyper column technique with convolutional neural network and feature selection method",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113274",
    "abstract": "A proper and certain brain tumor MRI classification has a significant role in current clinical diagnosis, decision making as well as managing the treatment programs. In clinical practice, the examination is performed visually by the specialists, this is a labor-intensive and error-prone process. Therefore, the computer-based systems are in demand so as to carry out objectively this process. In the traditional machine learning approaches, the low-level and high-level handcrafted features used to describe the brain tumor MRI are extracted and classified to overcome the mentioned drawbacks. Considering the recent advances in deep learning, we propose a novel convolutional neural network (CNN) model that is combined with the hypercolumn technique, pretrained AlexNet and VGG-16 networks, recursive feature elimination (RFE), and support vector machine (SVM) in this study. One of the great advantages of the proposed model is that with the help of the hypercolumn technique, it can keep the local discriminative features, which are extracted from the layers located at the different levels of the deep architectures. In addition, the proposed model exploits the generalization abilities of both AlexNet and VGG-16 networks by fusing the deep features achieved from the last fully-connected layers of the networks. Furthermore, the discriminative capacity of the proposed model is enhanced using RFE and thus the most effective deep features are revealed. As a result, the proposed model yielded an accuracy of 96.77% without using any handcrafted feature engine. A fully automated consistent and effective diagnostic model is ensured for the brain tumor MRI classification. Consequently, the proposed model can contribute to realizing a more objective evaluation in the clinics, supporting the decision-making process of the experts, and reducing misdiagnosis rates.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300993",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Discriminative model",
      "Feature (linguistics)",
      "Feature selection",
      "Generalization",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Process (computing)",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Toğaçar",
        "given_name": "Mesut"
      },
      {
        "surname": "Cömert",
        "given_name": "Zafer"
      },
      {
        "surname": "Ergen",
        "given_name": "Burhan"
      }
    ]
  },
  {
    "title": "Fuzzy hidden Markov-switching portfolio selection with capital gain tax",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113304",
    "abstract": "A fuzzy portfolio selection model is considered with a view to incorporating ambiguity about model and data structure. The model features the uncertainty about the exit time of each risky asset within a pre-specified investment horizon and also the presence of transaction costs. However, departing from the traditional paradigm where the transaction costs are often assumed to be unrelated to holding periods, we introduce the capital gain tax of which the realized tax rate is decreasing with respect to the holding periods with a view to encouraging the long-term investment. Meanwhile, the regime switching property of the market state is introduced to fuzzy portfolio selection, where fuzzy random variables are employed to model uncertain returns of risky assets in a Markov-regime switching market. An adjusted L − R fuzzy number is introduced and some of its mathematical properties are studied. In addition, a bi-objective mean-variance model is formulated, and a time varying numerical integral-based particle swarm optimization algorithm (TVNIPSO) is designed to obtain the efficient frontier of the portfolio in the sense of Pareto dominance. Finally, some numerical experiments are provided to validate the effectiveness of the model and the TVNIPSO.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420301299",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Econometrics",
      "Economics",
      "Efficient frontier",
      "Finance",
      "Fuzzy logic",
      "Fuzzy number",
      "Fuzzy set",
      "Mathematical optimization",
      "Mathematics",
      "Portfolio",
      "Portfolio optimization"
    ],
    "authors": [
      {
        "surname": "Guo",
        "given_name": "Sini"
      },
      {
        "surname": "Ching",
        "given_name": "Wai-Ki"
      },
      {
        "surname": "Li",
        "given_name": "Wai-Keung"
      },
      {
        "surname": "Siu",
        "given_name": "Tak-Kuen"
      },
      {
        "surname": "Zhang",
        "given_name": "Zhiwen"
      }
    ]
  },
  {
    "title": "A novel dice similarity measure for IFSs and its applications in pattern and face recognition",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113245",
    "abstract": "In the theory of Intuitionistic fuzzy set (IFS), similarity measure is an effective instrument to measure similarity between IFSs. In this paper, we mention limitations of various existing similarity measures and propose a novel dice similarity measure for IFSs. Proposed dice similarity measure is based on inner product and overcomes limitations of existing similarity measures. In order to show the suitability and applicability of proposed dice similarity measure, we implement it on various classification problems of pattern recognition and medical diagnosis. Experimental results show that this does not only overcome limitations of existing similarity measures but also outperforms in pattern recognition and medical diagnosis problems. In this paper, we also propose an algorithm for face recognition problem using proposed dice similarity measure. This algorithm is demonstrated by an example and performance is compared with few existing methods for face recognition problems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300713",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Dice",
      "Face (sociological concept)",
      "Facial recognition system",
      "Image (mathematics)",
      "Machine learning",
      "Mathematics",
      "Measure (data warehouse)",
      "Pattern recognition (psychology)",
      "Programming language",
      "Set (abstract data type)",
      "Similarity (geometry)",
      "Similarity measure",
      "Social science",
      "Sociology",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Singh",
        "given_name": "Akanksha"
      },
      {
        "surname": "Kumar",
        "given_name": "Sanjay"
      }
    ]
  },
  {
    "title": "Blind watermarking for color images using EMMQ based on QDFT",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113225",
    "abstract": "In this study, we developed a novel scheme for the blind watermarking of color images. The proposed scheme incorporates extreme pixel adjustment (EPA), multi-bit partly sign-altered mean modulation (MPSAM), mixed modulation (MM), and particle swarm optimization (PSO) within a scheme based on crisscross inter-block quaternion discrete Fourier transform (QDFT). Accordingly, the proposed scheme employing EPA, MPSAM, MM, and QDFT is referred to as EMMQ. The image is separated into non-overlapping 8 × 8 pixel blocks, whereupon MPSAM is used to map multiple bits within a single block using multiple coefficients in one of the four transformed components of QDFT. The use of MM to embed the watermark allows for superior image quality and strong resistance to image processing attacks. Our use of PSO also makes it possible to optimize the EMMQ parameters, thereby enabling outstanding robustness without compromising imperceptibility. Experiment results demonstrate the efficacy of the proposed scheme in resisting a variety of image-processing attacks, with performance superior to that of existing watermarking schemes in terms of imperceptibility and robustness for a given payload capacity.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300518",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Block (permutation group theory)",
      "Chemistry",
      "Color image",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Digital watermarking",
      "Gene",
      "Geometry",
      "Image (mathematics)",
      "Image processing",
      "Image quality",
      "Mathematics",
      "Network packet",
      "Particle swarm optimization",
      "Pattern recognition (psychology)",
      "Payload (computing)",
      "Pixel",
      "Robustness (evolution)",
      "Watermark"
    ],
    "authors": [
      {
        "surname": "Hsu",
        "given_name": "Ling-Yuan"
      },
      {
        "surname": "Hu",
        "given_name": "Hwai-Tsu"
      }
    ]
  },
  {
    "title": "A new hierarchical multi group particle swarm optimization with different task allocations inspired by holonic multi agent systems",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113292",
    "abstract": "Nowadays expert systems have been used in different fields. They must be able to operate as quickly and efficiently as possible. So, they need optimization mechanism in their different parts and optimization is a critical part of almost all expert systems. Because of difficulties in real world problems, traditional optimization techniques commonly cannot solve them. Therefore, stochastic algorithms are used to do the optimization in expert systems. Particle swarm optimization (PSO) is one the most famous stochastic optimization algorithms. But this algorithm has some difficulties like losing diversity, premature convergence, trapping in local optimums and imbalance between exploration and exploitation. To overcome these drawbacks, inspired by holonic organization in multi agent systems, a new hierarchical multi group structure for PSO is presented in this paper. Considering the particles in PSO as simple agents, PSO is a kind of multi agent system. Existence of different facilities and organizations in multi agent systems and their great impact on performance encouraged us to use them. So, inspired by holonic multi agent systems, a new structure for PSO is presented. This work has been done for the first time in the literature. Meanwhile, to promote exploration and exploitation ability of proposed structure and create a suitable balance between them, different tasks are assigned to different groups of this structure. So, a holonic PSO with different task allocations (HPSO-DTA) is created. It provides the opportunity to employ all aspects for empowering PSO including parameter settings, neighborhood topologies and learning strategies to enhance the ability of it unlike other versions of PSO that use only one of these aspects to improve their solutions. This structure provides a lot of advantages for PSO. It is a new topological structure that improves the performance of PSO. It provides several leaders with efficient information to guide the particles in the search space. Also, it helps to control suitable information flow between groups and particles in order to preserve diversity and prevent from trapping in local optimums. Meanwhile, with assigning different tasks to different groups of proposed structure, an appropriate balance between exploration and exploitation is created to enhance the performance of the algorithm. In each group, based on its assigned task, particles use different parameters settings, different dynamic neighborhood topologies and different learning strategies which are proposed in this paper to enhance the performance of algorithm. A set of thirty four benchmark functions are used to evaluate the performance of proposed structure. Proposed algorithm is compared with a set of well-known PSO algorithms that their efficiency have been proved. Experimental results and comparative analysis demonstrate good performance of HPSO-DTA compared to other algorithms. Its solution accuracy, convergence speed and robustness is completely appropriate especially in more complicated benchmarks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420301172",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Convergence (economics)",
      "Economic growth",
      "Economics",
      "Engineering",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Multi-agent system",
      "Optimization problem",
      "Particle swarm optimization",
      "Premature convergence",
      "Swarm behaviour",
      "Systems engineering",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Roshanzamir",
        "given_name": "Mahdi"
      },
      {
        "surname": "Balafar",
        "given_name": "Mohammad Ali"
      },
      {
        "surname": "Razavi",
        "given_name": "Seyed Naser"
      }
    ]
  },
  {
    "title": "Diversity-preserving quantum particle swarm optimization for the multidimensional knapsack problem",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113310",
    "abstract": "Quantum particle swarm optimization is a population-based metaheuristic that becomes popular in recent years in the field of binary optimization. In this paper, we investigate a novel quantum particle swarm optimization algorithm, which integrates a distanced-based diversity-preserving strategy for population management and a local optimization method based on variable neighborhood descent for solution improvement. We evaluate the proposed method on the classic NP-hard 0–1 multidimensional knapsack problem. We present extensive computational results on the 270 benchmark instances commonly used in the literature to show the competitiveness of the proposed algorithm compared to several state-of-the-art algorithms. The ideas of using the diversity-preserving strategy and the probabilistic application of a local optimization procedure are of general interest and can be used to reinforce other quantum particle swarm algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420301354",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Demography",
      "Geodesy",
      "Geography",
      "Imperialist competitive algorithm",
      "Knapsack problem",
      "Mathematical optimization",
      "Mathematics",
      "Metaheuristic",
      "Multi-swarm optimization",
      "Optimization problem",
      "Parallel metaheuristic",
      "Particle swarm optimization",
      "Physics",
      "Population",
      "Probabilistic logic",
      "Quantum",
      "Quantum mechanics",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Lai",
        "given_name": "Xiangjing"
      },
      {
        "surname": "Hao",
        "given_name": "Jin-Kao"
      },
      {
        "surname": "Fu",
        "given_name": "Zhang-Hua"
      },
      {
        "surname": "Yue",
        "given_name": "Dong"
      }
    ]
  },
  {
    "title": "Chimp optimization algorithm",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113338",
    "abstract": "This paper proposes a novel metaheuristic algorithm called Chimp Optimization Algorithm (ChOA) inspired by the individual intelligence and sexual motivation of chimps in their group hunting, which is different from the other social predators. ChOA is designed to further alleviate the two problems of slow convergence speed and trapping in local optima in solving high-dimensional problems. In this paper, a mathematical model of diverse intelligence and sexual motivation of chimps is proposed. In this regard, four types of chimps entitled attacker, barrier, chaser, and driver are employed for simulating the diverse intelligence. Moreover, four main steps of hunting, i.e. driving, chasing, blocking, and attacking, are implemented. The proposed ChOA algorithm is evaluated in 3 main phases. First, a set of 30 mathematical benchmark functions is utilized to investigate various characteristics of ChOA. Secondly, ChOA was tested by 13 high-dimensional test problems. Finally, 10 real-world optimization problems were used to evaluate the performance of ChOA. The results are compared to several newly proposed meta-heuristic algorithms in terms of convergence speed, the probability of getting stuck in local minimums, and exploration, exploitation. Also, statistical tests were employed to investigate the significance of the results. The results indicate that the ChOA outperforms the other benchmark optimization algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420301639",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Convergence (economics)",
      "Economic growth",
      "Economics",
      "Geodesy",
      "Geography",
      "Heuristic",
      "Local optimum",
      "Mathematical optimization",
      "Mathematics",
      "Metaheuristic",
      "Optimization algorithm",
      "Particle swarm optimization",
      "Programming language",
      "Set (abstract data type)",
      "Swarm intelligence"
    ],
    "authors": [
      {
        "surname": "Khishe",
        "given_name": "M."
      },
      {
        "surname": "Mosavi",
        "given_name": "M.R."
      }
    ]
  },
  {
    "title": "Memetic niching-based evolutionary algorithms for solving nonlinear equation system",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113261",
    "abstract": "In numerical computation, finding multiple roots of nonlinear equation systems (NESs) in a single run is a fundamental and difficult problem. Recently, evolutionary algorithms (EAs) have been applied to solve NESs. However, due to the diversity preservation mechanism that EAs use, the accuracy of the roots may be reduced. To remedy this drawback, we propose a generic framework of memetic niching-based EA, referred to as MENI-EA. The main features of the framework are: i) the numerical method for a NES is integrated into an EA to obtain highly accurate roots; ii) the niching technique is employed to improve the diversity of the population; iii) different roots of the NESs are located simultaneously in a singe run; and iv) different numerical methods and different niching techniques can be used in the framework. To evaluate the performance of our approach, thirty NESs were chosen from the literature as the test suite. Experimental results show that the proposed approach is capable of yielding promising performance for different NESs in both the root ratio and success rate.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300865",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computation",
      "Computer science",
      "Demography",
      "Evolutionary algorithm",
      "Evolutionary computation",
      "Mathematical optimization",
      "Mathematics",
      "Memetic algorithm",
      "Nonlinear system",
      "Physics",
      "Population",
      "Quantum mechanics",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Liao",
        "given_name": "Zuowen"
      },
      {
        "surname": "Gong",
        "given_name": "Wenyin"
      },
      {
        "surname": "Wang",
        "given_name": "Ling"
      }
    ]
  },
  {
    "title": "The Self-Organizing Restricted Boltzmann Machine for Deep Representation with the Application on Classification Problems",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113286",
    "abstract": "Recently, deep learning is proliferating in the field of representation learning. A deep belief network (DBN) consists of a deep network architecture that can generate multiple features of input patterns, using restricted Boltzmann machines (RBMs) as a building block of DBN. A deep learning model can achieve extremely high accuracy in many applications that depend on the model structure. However, specifying various parameters of deep network architecture like the number of hidden layers and neurons is a difficult task even for expert designers. Besides, the number of hidden layers and neurons is typically set manually, while this method is costly in terms of time and computational cost, especially in big data. In this paper, we introduce an approach to determine the number of hidden layers and neurons of the deep network automatically during the learning process. To this end, the input vector is transformed from the feature space with a low dimension into the new feature space with a high dimension in a hidden layer of RBM. In the following, new features are ranked according to their discrimination power between classes in the new space, using the Separability-correlation measure for feature importance ranking algorithm. The algorithm uses the mean of weights as a threshold, so the neurons whose weights exceed the threshold are retained, and the others are removed in the hidden layer. The number of retained neurons is presented as a reasonable number of neurons. The number of layers is also determined in the deep model, using the validation data. The proposed approach acts as a regularization method since the neurons whose weights are lower than the threshold are removed; thus, RBM learns to copy input merely approximate. It also prevents over-fitting with a suitable number of hidden layers and neurons. Eventually, DBN can determine its structure according to the input data and is the self-organizing model. The experimental results on benchmark datasets confirm the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420301111",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Boltzmann machine",
      "Computer science",
      "Data mining",
      "Deep belief network",
      "Deep learning",
      "Dimension (graph theory)",
      "Feature (linguistics)",
      "Feature vector",
      "Law",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Measure (data warehouse)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Programming language",
      "Pure mathematics",
      "Ranking (information retrieval)",
      "Representation (politics)",
      "Restricted Boltzmann machine",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Pirmoradi",
        "given_name": "Saeed"
      },
      {
        "surname": "Teshnehlab",
        "given_name": "Mohammad"
      },
      {
        "surname": "Zarghami",
        "given_name": "Nosratollah"
      },
      {
        "surname": "Sharifi",
        "given_name": "Arash"
      }
    ]
  },
  {
    "title": "Enhanced deep learning algorithm development to detect pain intensity from facial expression images",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113305",
    "abstract": "Automated detection of pain intensity from facial expressions, especially from face images that show a patient's health, remains a significant challenge in the medical diagnostics and health informatics area. Expert systems that prudently analyse facial expression images, utilising an automated machine learning algorithm, can be a promising approach for pain intensity analysis in health domain. Deep neural networks and emerging machine learning techniques have made significant progress in both the feature identification, mapping and the modelling of pain intensity from facial images, with great potential to aid health practitioners in the diagnosis of certain medical conditions. Consequently, there has been significant research within the pain recognition and management area that aim to adopt facial expression datasets into deep learning algorithms to detect the pain intensity in binary classes, and also to identify pain and non-pain faces. However, the volume of research in identifying pain intensity levels in multi-classes remains rather limited. This paper reports on a new enhanced deep neural network framework designed for the effective detection of pain intensity, in four-level thresholds using a facial expression image. To explore the robustness of the proposed algorithms, the UNBC-McMaster Shoulder Pain Archive Database, comprised of human facial images, was first balanced, then used for the training and testing of the classification model, coupled with the fine-tuned VGG-Face pre-trainer as a feature extraction tool. To reduce the dimensionality of the classification model input data and extract most relevant features, Principal Component Analysis was applied, improving its computational efficiency. The pre-screened features, used as model inputs, are then transferred to produce a new enhanced joint hybrid CNN-BiLSTM (EJH-CNN-BiLSTM) deep learning algorithm comprised of convolutional neural networks, that were then linked to the joint bidirectional LSTM, for multi-classification of pain. The resulting EJH-CNN-BiLSTM classification model, tested to estimate four different levels of pain, revealed a good degree of accuracy in terms of different performance evaluation techniques. The results indicated that the enhanced EJH-CNN-BiLSTM classification algorithm was explored as a potential tool for the detection of pain intensity in multi-classes from facial expression images, and therefore, can be adopted as an artificial intelligence tool in the medical diagnostics for automatic pain detection and subsequent pain management of patients.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420301305",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Classifier (UML)",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Facial expression",
      "Facial recognition system",
      "Feature extraction",
      "Gene",
      "Histogram",
      "Image (mathematics)",
      "Local binary patterns",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Robustness (evolution)",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Bargshady",
        "given_name": "Ghazal"
      },
      {
        "surname": "Zhou",
        "given_name": "Xujuan"
      },
      {
        "surname": "Deo",
        "given_name": "Ravinesh C."
      },
      {
        "surname": "Soar",
        "given_name": "Jeffrey"
      },
      {
        "surname": "Whittaker",
        "given_name": "Frank"
      },
      {
        "surname": "Wang",
        "given_name": "Hua"
      }
    ]
  },
  {
    "title": "A weight perturbation-based regularisation technique for convolutional neural networks and the application in medical imaging",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113196",
    "abstract": "A convolutional neural network has the capacity to learn multiple representation levels and abstraction in order to provide a better understanding of image data. In addition, a good multi-level representation of data typically results in a better generalisation capability. This fact emphasises the importance of concentrating on the regularity information of training data in order to improve generalisation. However, the training data contain erroneous information owing to noise and outliers. In this paper, we propose a new regularisation approach for convolutional neural networks with better generalisation properties. Specifically, the weights of the convolution layers are perturbed by additive noise in each learning iteration. The approach provides a better model for prediction, as shown by the experimental results on a number of medical benchmark data sets. Furthermore, the effectiveness and accuracy of the proposed convolutional neural network are demonstrated by comparing with several recent perturbation techniques.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300221",
    "keywords": [
      "Abstraction",
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Computer science",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Data mining",
      "Epistemology",
      "External Data Representation",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Law",
      "Machine learning",
      "Noise (video)",
      "Outlier",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Khatami",
        "given_name": "Amin"
      },
      {
        "surname": "Nazari",
        "given_name": "Asef"
      },
      {
        "surname": "Khosravi",
        "given_name": "Abbas"
      },
      {
        "surname": "Lim",
        "given_name": "Chee Peng"
      },
      {
        "surname": "Nahavandi",
        "given_name": "Saeid"
      }
    ]
  },
  {
    "title": "NN-SSTA: A deep neural network approach for statistical static timing analysis",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113309",
    "abstract": "Discrete statistical static timing analysis (SSTA) performs the timing analysis by using statistical maximum and convolution operations. The maximum is basically a non-linear operator and it is not a simple task to capture the skewness introduced by it. On the other hand, the convolution has a potential to “blow-up” the number of discrete samples as we going deep inside the timing graph and hence, results in exponential timing complexity. Therefore, in this paper we present novel deep neural network based operations which can accurately approximate the signal arrival-time's distributions with linear-time complexity. The various deep neural network (DNN) architectures have been used to implement both the maximum and the convolution operations using proper training dataset. Simulation results on various benchmark circuits (ISCAS 85, ISCAS 89, and ITC 99) show that the proposed method estimate the mean and standard deviation (STD) of critical path delay distribution with an average error of 0.75% and 2.56% as compared to Monte Carlo (MC), respectively. Our SSTA speeds up the traditional discrete approach by a factor of 20.7x on average. Furthermore, the PDF obtained from our method matches the ones obtained from MC with a reasonable error. Furthermore, we have proposed multi-wise maximum operations to reduce the arrival-time computational complexity at multi-inputs gates. Comparing to MC, the proposed method shows 0.97% and 2.58% average error in mean and STD respectively and the speeding up factor reaches 24.4x on average for all benchmarks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420301342",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Computer science",
      "Convolution (computer science)",
      "Embedded system",
      "Geodesy",
      "Geography",
      "Mathematics",
      "Medicine",
      "Radiology",
      "Skewness",
      "Standard deviation",
      "Static timing analysis",
      "Statistics",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Savari",
        "given_name": "M. Amin"
      },
      {
        "surname": "Jahanirad",
        "given_name": "H."
      }
    ]
  },
  {
    "title": "Integrating complex event processing and machine learning: An intelligent architecture for detecting IoT security attacks",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113251",
    "abstract": "The Internet of Things (IoT) is growing globally at a fast pace: people now find themselves surrounded by a variety of IoT devices such as smartphones and wearables in their everyday lives. Additionally, smart environments, such as smart healthcare systems, smart industries and smart cities, benefit from sensors and actuators interconnected through the IoT. However, the increase in IoT devices has brought with it the challenge of promptly detecting and combating the cybersecurity attacks and threats that target them, including malware, privacy breaches and denial of service attacks, among others. To tackle this challenge, this paper proposes an intelligent architecture that integrates Complex Event Processing (CEP) technology and the Machine Learning (ML) paradigm in order to detect different types of IoT security attacks in real time. In particular, such an architecture is capable of easily managing event patterns whose conditions depend on values obtained by ML algorithms. Additionally, a model-driven graphical tool for security attack pattern definition and automatic code generation is provided, hiding all the complexity derived from implementation details from domain experts. The proposed architecture has been applied in the case of a healthcare IoT network to validate its ability to detect attacks made by malicious devices. The results obtained demonstrate that this architecture satisfactorily fulfils its objectives.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300762",
    "keywords": [
      "Architecture",
      "Art",
      "Artificial intelligence",
      "Complex event processing",
      "Computer science",
      "Computer security",
      "Denial-of-service attack",
      "Domain (mathematical analysis)",
      "Embedded system",
      "Event (particle physics)",
      "Malware",
      "Mathematical analysis",
      "Mathematics",
      "Operating system",
      "Physics",
      "Process (computing)",
      "Quantum mechanics",
      "The Internet",
      "Variety (cybernetics)",
      "Visual arts",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Roldán",
        "given_name": "José"
      },
      {
        "surname": "Boubeta-Puig",
        "given_name": "Juan"
      },
      {
        "surname": "Luis Martínez",
        "given_name": "José"
      },
      {
        "surname": "Ortiz",
        "given_name": "Guadalupe"
      }
    ]
  },
  {
    "title": "Automatic lecture video skimming using shot categorization and contrast based features",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113341",
    "abstract": "Video skimming is one of the recently, getting popular technique for preparing preview for long watching video sequences. Most of the video skimming techniques developed in the literature uses manual intervention of users to prepare the review. Mostly the literature reported video skimming for sports and movie industries. In sports the portion of video where audience claps are used and in movie important contents are manually selected for preparing the preview. However in literature rarely any work reported for skimming of lecture video sequences. Lecture videos are generally, recorded indoor, low illuminated, noisy environment condition and contents of the scene rarely changes much. Hence designing an automatic skimming scheme is quite difficult task. In this article, we put forward an intelligent expert video skimming technique for lecture video sequences, where human intervention is not required. In the proposed scheme, initially the lecture video is segmented into a number of shots. We proposed the use of radiometric correlation technique for lecture video segmentation or finding the shot transitions. After getting the shot transitions in a video, the shots are recognized. The fuzzy K-nearest neighborhood technique is proposed to recognize the shots in a video. The shots are recognized into three categories: title slides, written texts/displayed slides and talking heads/writing hands. Three contrast based features: one existing i.e., average sharpness (AS) and two newly proposed: relative height (RH) and edge potential (EP) are used to find the contents of a frame. The frames with different contrast values are categorized to prepare the video skimming or the capsule. The media recreation is achieved by selecting a set of frames around these selected content frames. The effectiveness of the proposed scheme is demonstrated in this paper using five test sequences, including three NPTEL and two non NPTEL. It is also observed that the capsule prepared by the proposed scheme, provides a better preview of the actual sequence. The performance of the proposed scheme is tested by comparing it against three state-of-the-art techniques. The evaluation of the proposed scheme is carried out by using three evaluation measures. It is also observed that the proposed scheme is found to be better than that of the existing schemes.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420301664",
    "keywords": [
      "Artificial intelligence",
      "Categorization",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Contrast (vision)",
      "Frame (networking)",
      "Multimedia",
      "Organic chemistry",
      "Segmentation",
      "Shot (pellet)",
      "Telecommunications",
      "Video processing",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Subudhi",
        "given_name": "Badri Narayan"
      },
      {
        "surname": "Veerakumar",
        "given_name": "Thangaraj"
      },
      {
        "surname": "Esakkirajan",
        "given_name": "Sankaralingam"
      },
      {
        "surname": "Chaudhury",
        "given_name": "Santanu"
      }
    ]
  },
  {
    "title": "Driver behavior detection and classification using deep convolutional neural networks",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113240",
    "abstract": "Driver behavior monitoring system as Intelligent Transportation Systems (ITS) have been widely exploited to reduce the traffic accidents risk. Most previous methods for monitoring the driver behavior are rely on computer vision techniques. Such methods suffer from violation of privacy and the possibility of spoofing. This paper presents a novel yet efficient deep learning method for analyzing the driver behavior. We have used the driving signals, including acceleration, gravity, throttle, speed, and Revolutions Per Minute (RPM) to recognize five types of driving styles, including normal, aggressive, distracted, drowsy, and drunk driving. To take the advantages of successful deep neural networks on images, we learn a 2D Convolutional Neural Network (CNN) on images constructed from driving signals based on recurrence plot technique. Experimental results confirm that the proposed method can efficiently detect the driver behavior.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742030066X",
    "keywords": [
      "Acceleration",
      "Artificial intelligence",
      "Artificial neural network",
      "Automotive engineering",
      "Civil engineering",
      "Classical mechanics",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Convolutional neural network",
      "Deep learning",
      "Engineering",
      "Intelligent transportation system",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Physics",
      "Real-time computing",
      "Spoofing attack",
      "Throttle"
    ],
    "authors": [
      {
        "surname": "Shahverdy",
        "given_name": "Mohammad"
      },
      {
        "surname": "Fathy",
        "given_name": "Mahmood"
      },
      {
        "surname": "Berangi",
        "given_name": "Reza"
      },
      {
        "surname": "Sabokrou",
        "given_name": "Mohammad"
      }
    ]
  },
  {
    "title": "A Bayesian learning model for design-phase service mashup popularity prediction",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113231",
    "abstract": "Using web services as building blocks to develop software applications, i.e., service mashups, not only reuses software development efforts to minimize development cost, but also leverages user groups and marketing efforts of those services to attract users and improve profits. This has significantly encouraged the development of a large number of service mashups in various domains. However, using existing services, even popular ones, does not guarantee the success of a mashup. In fact, a large portion of existing mashups fail to attract a good number of users, making the mashup development effort less effective. Design-phase popularity prediction can help avoid unpromising mashup developments by providing early-on insight into the potential popularity of a mashup. In this paper, we investigate the factors that can affect the popularity of a mashup through a comprehensive analysis on one of the largest mashup repository (i.e., ProgrammableWeb). We further propose a novel Bayesian approach that offers early-on insight to developers into the potential popularity of a mashup using design-phase features only. Besides identifying those relevant features, the Bayesian learning model can provide a confidence level for each prediction. This provides useful guidance to developers for successful mashup development. Experimental results demonstrate that the proposed approach achieves high prediction accuracy and outperforms competitive models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300579",
    "keywords": [
      "Artificial intelligence",
      "Business",
      "Computer science",
      "Machine learning",
      "Marketing",
      "Mashup",
      "Popularity",
      "Programming language",
      "Psychology",
      "Service (business)",
      "Social psychology",
      "Software",
      "Web development",
      "Web service",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Alshangiti",
        "given_name": "Moayad"
      },
      {
        "surname": "Shi",
        "given_name": "Weishi"
      },
      {
        "surname": "Liu",
        "given_name": "Xumin"
      },
      {
        "surname": "Yu",
        "given_name": "Qi"
      }
    ]
  },
  {
    "title": "Anomaly pattern detection for streaming data",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113252",
    "abstract": "Outlier detection aims to find a data sample that is different from most other data samples. While outlier detection is performed at an individual instance level, anomaly pattern detection on a data stream means detecting a time point where a pattern to generate data is unusual and significantly different from normal behavior. Beyond predicting the outlierness of individual data samples in a data stream, it can be very useful to detect the occurrence of anomalous patterns in real time. In this paper, we propose a method for anomaly pattern detection in a data stream based on binary classification for outliers and statistical tests on a data stream of binary labels of normal or an outlier. In the first step, by applying the clustering-based outlier detection method, we transform a data stream into a stream of binary values where 0 stands for the prediction as normal data and 1 for outlier prediction. In the second step, anomaly pattern detection is performed on a stream of binary values by two approaches: testing the equality of parameters in the binomial distributions of a reference window and a detection window, and using control charts for the fraction defective. The proposed method obtained the average true positive detection rate of 94% in simulated experiments using real and artificial data. The experimental results also show that anomaly pattern occurrence can be detected reliably even when outlier detection performance is relatively low.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300774",
    "keywords": [
      "Anomaly (physics)",
      "Anomaly detection",
      "Arithmetic",
      "Artificial intelligence",
      "Binary number",
      "Cluster analysis",
      "Computer science",
      "Condensed matter physics",
      "Data mining",
      "Data stream",
      "Mathematics",
      "Outlier",
      "Pattern recognition (psychology)",
      "Physics",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Kim",
        "given_name": "Taegong"
      },
      {
        "surname": "Park",
        "given_name": "Cheong Hee"
      }
    ]
  },
  {
    "title": "Cascaded deep learning-based efficient approach for license plate detection and recognition",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113280",
    "abstract": "Automatic license plate (ALP) detection and recognition is an important task for both traffic surveillance and parking management systems, as well as being crucial to maintaining the flow of modern civic life. Various ALP detection and recognition methods have been proposed to date. These methods generally use various image processing and machine learning techniques. In this paper, a cascaded deep learning approach is proposed in order to construct an efficient ALP detection and recognition system for the vehicles of northern Iraq. The license plates in northern Iraq contain three regions, namely a plate number, a city region, and a country region. The proposed method initially employs several preprocessing techniques such as Gaussian filtering and adaptive image contrast enhancement to make the input images more suited to further processing. Then, a deep semantic segmentation network is used in order to determine the three license plate regions of the input image. Segmentation is then carried out via deep encoder-decoder network architecture. The determined license plate regions are fed into two separate convolutional neural network (CNN) models for both Arabic number recognition and the city determination. For Arabic number recognition, an end-to-end CNN model was constructed and trained, whilst for the city recognition, a pretrained CNN model was further fine-tuned. A new license plate dataset was also constructed and used in the experimental works of the study. The performance of the proposed method was evaluated both in terms of detection and recognition. For detection, recall, precision and F-measure scores were used, and for recognition, classification accuracy was used. The obtained results showed the proposed method to be efficient in both license plate detection and recognition. The calculated recall, precision and F-measure scores were 92.10%, 94.43%, and 91.01%, respectively. Moreover, the classification accuracies for Arabic numbers and city labels were shown to be 99.37% and 92.26%, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420301056",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Deep learning",
      "License",
      "Operating system",
      "Pattern recognition (psychology)",
      "Preprocessor",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Omar",
        "given_name": "Naaman"
      },
      {
        "surname": "Sengur",
        "given_name": "Abdulkadir"
      },
      {
        "surname": "Al-Ali",
        "given_name": "Salim Ganim Saeed"
      }
    ]
  },
  {
    "title": "Developing a deep learning framework with two-stage feature selection for multivariate financial time series forecasting",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113237",
    "abstract": "Intelligent financial forecasting modeling plays an important role in facilitating investment-related decision-making activities in financial markets. However, accurate multivariate financial time series forecasting remains a challenge due to its complex nonlinear pattern. Aiming to fill the gap in the field, a novel forecasting framework, based on a two-stage feature selection model, deep learning model, and error correction model, is presented in this study, aiming at effectively capturing the nonlinearity inherent in multivariate financial time series. Concretely, the proposed two-stage feature selection model is utilized to determine the optimal feature set to further improve the generalization of the proposed deep learning model based on three deep learning units. Meanwhile, the error correction model is used to correct the forecasts and improve the accuracy further. To validate the performance of the forecasting framework, the case studies and the corresponding sensitivity analysis are carried out, consequently demonstrating its superiority, as compared to 16 benchmarks considered.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300634",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Economics",
      "Feature (linguistics)",
      "Feature selection",
      "Field (mathematics)",
      "Finance",
      "Generalization",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Multivariate statistics",
      "Philosophy",
      "Pure mathematics",
      "Selection (genetic algorithm)",
      "Time series"
    ],
    "authors": [
      {
        "surname": "Niu",
        "given_name": "Tong"
      },
      {
        "surname": "Wang",
        "given_name": "Jianzhou"
      },
      {
        "surname": "Lu",
        "given_name": "Haiyan"
      },
      {
        "surname": "Yang",
        "given_name": "Wendong"
      },
      {
        "surname": "Du",
        "given_name": "Pei"
      }
    ]
  },
  {
    "title": "A feature selection algorithm for intrusion detection system based on Pigeon Inspired Optimizer",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113249",
    "abstract": "Feature selection plays a vital role in building machine learning models. Irrelevant features in data affect the accuracy of the model and increase the training time needed to build the model. Feature selection is an important process to build Intrusion Detection System (IDS). In this paper, a wrapper feature selection algorithm for IDS is proposed. This algorithm uses the pigeon inspired optimizer to utilize the selection process. A new method to binarize a continuous pigeon inspired optimizer is proposed and compared to the traditional way for binarizing continuous swarm intelligent algorithms. The proposed algorithm was evaluated using three popular datasets: KDDCUP 99, NLS-KDD and UNSW-NB15. The proposed algorithm outperformed several feature selection algorithms from state-of-the-art related works in terms of TPR, FPR, accuracy, and F-score. Also, the proposed cosine similarity method for binarizing the algorithm has a faster convergence than the sigmoid method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300749",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Convergence (economics)",
      "Cosine similarity",
      "Data mining",
      "Economic growth",
      "Economics",
      "Feature (linguistics)",
      "Feature selection",
      "Image (mathematics)",
      "Intrusion detection system",
      "Linguistics",
      "Machine learning",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Process (computing)",
      "Selection (genetic algorithm)",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Alazzam",
        "given_name": "Hadeel"
      },
      {
        "surname": "Sharieh",
        "given_name": "Ahmad"
      },
      {
        "surname": "Sabri",
        "given_name": "Khair Eddin"
      }
    ]
  },
  {
    "title": "GNM: GridCell navigational model",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113217",
    "abstract": "It has been shown that grid cell firing patterns in the medial entorhinal cortex, can be used as a mapping reference for spatial navigation in mice and other mammalian species. In this paper, we propose a novel computational model for patterns of grid cells and combine it with a mechanism to tune the weights of cells, which we use to create a decision-making process for robot navigation. The method is used as an unsupervised method for uninformed online search with unknown goal positions and unknown environments, such as finding the exit out of a maze or help a robot to find its way in a jungle where there is no clue about the exit. The results of this approach in simulated and real environments show superior algorithmic steps over current search methods. In addition, the typical size of the memory can be reduced without compromising completeness of the method. Our results show that the number of steps is stable in terms of variations in memory allocations.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300439",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Cognition",
      "Completeness (order theory)",
      "Computer science",
      "Geometry",
      "Grid",
      "Grid cell",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Neuroscience",
      "Operating system",
      "Process (computing)",
      "Robot",
      "Spatial memory",
      "Working memory"
    ],
    "authors": [
      {
        "surname": "Naghizadeh",
        "given_name": "Alireza"
      },
      {
        "surname": "Berenjian",
        "given_name": "Samaneh"
      },
      {
        "surname": "Margolis",
        "given_name": "David J."
      },
      {
        "surname": "Metaxas",
        "given_name": "Dimitris N."
      }
    ]
  },
  {
    "title": "Accelerating the Miller–Tucker–Zemlin model for the asymmetric traveling salesman problem",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113229",
    "abstract": "In this article, we present an easy to implement algorithmic approach that improves the computational performance of the Miller–Tucker–Zemlin (MTZ) model for the asymmetric traveling salesman problem (ATSP) by efficiently generating valid inequalities from fractional solutions. Computational experiments show that the proposed approach enhances considerably the performance of MTZ-based formulations reported in the literature. By adding facet-defining inequalities of the underline ATSP-polytope, the number of nodes in the branch-and-bound tree is drastically reduced, and the convergence of the MTZ-type formulations is accelerated. We also extend this idea to solve the multiple asymmetric traveling salesman problem (mATSP). This approach can help practitioners to solve real-life problems to near optimality using a standard optimization solver and may be useful to solve a variety of routing problems that use MTZ-type of subtour elimination constraints.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300555",
    "keywords": [
      "Artificial intelligence",
      "Combinatorial optimization",
      "Combinatorics",
      "Computer science",
      "Convergence (economics)",
      "Economic growth",
      "Economics",
      "Mathematical optimization",
      "Mathematics",
      "Polytope",
      "Solver",
      "Travelling salesman problem",
      "Variety (cybernetics)"
    ],
    "authors": [
      {
        "surname": "Campuzano",
        "given_name": "Giovanni"
      },
      {
        "surname": "Obreque",
        "given_name": "Carlos"
      },
      {
        "surname": "Aguayo",
        "given_name": "Maichel M."
      }
    ]
  },
  {
    "title": "Nonnegative matrix factorization for link prediction in directed complex networks using PageRank and asymmetric link clustering information",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113290",
    "abstract": "The aim of link prediction is to predict missing links in current networks or new links in future networks. Almost all the existing directed link prediction algorithms only take into account the links direction formation but ignored the abundant network topological information such as local and global structures. Therefore, how to preserve both local and global structure information is an important issue for directed link prediction. To solve this problem, in this paper, we are motivated to propose a novel Nonnegative Matrix Factorization via Asymmetric link clustering and PageRank model, namely NMF-AP. Specifically, we utilize the PageRank algorithm to calculate the influence score of the node, which captures the global network structure information. While we employ the asymmetric link clustering method to calculate the link clustering coefficient score, which preserves the local network structure information. By jointly optimizing them in the nonnegative matrix factorization model, our model can preserve both the local and global information at the same time. Besides, we provide an effective the multiplicative updating rules to learn the parameter of NMF-AP. Extensive experiments are conducted on ten real-world directed networks, experiment results demonstrate that the method NMF-AP outperforms state-of-the-art link prediction methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420301159",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Clustering coefficient",
      "Complex network",
      "Computer network",
      "Computer science",
      "Data mining",
      "Eigenvalues and eigenvectors",
      "Engineering",
      "Link (geometry)",
      "Link analysis",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Matrix decomposition",
      "Multiplicative function",
      "Node (physics)",
      "Non-negative matrix factorization",
      "PageRank",
      "Physics",
      "Quantum mechanics",
      "Structural engineering",
      "Theoretical computer science",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Guangfu"
      },
      {
        "surname": "Xu",
        "given_name": "Chen"
      },
      {
        "surname": "Wang",
        "given_name": "Jingyi"
      },
      {
        "surname": "Feng",
        "given_name": "Jianwen"
      },
      {
        "surname": "Feng",
        "given_name": "Jiqiang"
      }
    ]
  },
  {
    "title": "Learning alternative ways of performing a task",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113263",
    "abstract": "A common way of learning to perform a task is to observe how it is carried out by experts. However, it is well known that for most tasks there is no unique way to perform them. This is especially noticeable the more complex the task is because factors such as the skill or the know-how of the expert may well affect the way she solves the task. In addition, learning from experts also suffers of having a small set of training examples generally coming from several experts (since experts are usually a limited and expensive resource), being all of them positive examples (i.e. examples that represent successful executions of the task). Traditional machine learning techniques are not useful in such scenarios, as they require extensive training data. Starting from very few executions of the task presented as activity sequences, we introduce a novel inductive approach for learning multiple models, with each one representing an alternative strategy of performing a task. By an iterative process based on generalisation and specialisation, we learn the underlying patterns that capture the different styles of performing a task exhibited by the examples. We illustrate our approach on two common activity recognition tasks: a surgical skills training task and a cooking domain. We evaluate the inferred models with respect to two metrics that measure how well the models represent the examples and capture the different forms of executing a task showed by the examples. We compare our results with the traditional process mining approach and show that a small set of meaningful examples is enough to obtain patterns that capture the different strategies that are followed to solve the tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300889",
    "keywords": [
      "Artificial intelligence",
      "Computer network",
      "Computer science",
      "Domain (mathematical analysis)",
      "Economics",
      "Machine learning",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Multi-task learning",
      "Operating system",
      "Process (computing)",
      "Programming language",
      "Resource (disambiguation)",
      "Set (abstract data type)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Nieves",
        "given_name": "D."
      },
      {
        "surname": "Ramírez-Quintana",
        "given_name": "MJ."
      },
      {
        "surname": "Monserrat",
        "given_name": "C."
      },
      {
        "surname": "Ferri",
        "given_name": "C."
      },
      {
        "surname": "Hernández-Orallo",
        "given_name": "J."
      }
    ]
  },
  {
    "title": "Dimensionality reduction for multi-criteria problems: An application to the decommissioning of oil and gas installations",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113236",
    "abstract": "This paper is motivated by decommissioning studies in the field of oil and gas, which comprise a very large number of installations and are of interest to a large number of stakeholders. Generally, the problem gives rise to complicated multi-criteria decision aid tools that rely upon the costly evaluation of multiple criteria for every piece of equipment. We propose the use of machine learning techniques to reduce the number of criteria by feature selection, thereby reducing the number of required evaluations and producing a simplified decision aid tool with no sacrifice in performance. In addition, we also propose the use of machine learning to explore the patterns of the multi-criteria decision aid tool in a training set. Hence, we predict the outcome of the analysis for the remaining pieces of equipment, effectively replacing the multi-criteria analysis by the computational intelligence acquired from running it in the training set. Computational experiments illustrate the effectiveness of the proposed approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300622",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Curse of dimensionality",
      "Dimensionality reduction",
      "Engineering",
      "Field (mathematics)",
      "Geometry",
      "Machine learning",
      "Mathematics",
      "Nuclear decommissioning",
      "Programming language",
      "Pure mathematics",
      "Reduction (mathematics)",
      "Set (abstract data type)",
      "Waste management"
    ],
    "authors": [
      {
        "surname": "Martins",
        "given_name": "Isabelle D."
      },
      {
        "surname": "Bahiense",
        "given_name": "Laura"
      },
      {
        "surname": "Infante",
        "given_name": "Carlos E.D."
      },
      {
        "surname": "Arruda",
        "given_name": "Edilson F."
      }
    ]
  },
  {
    "title": "Aspect-based sentiment analysis using adaptive aspect-based lexicons",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113234",
    "abstract": "Reviews expressed in e-commerce websites have formed an important source of information for both consumers and enterprises. Text sentiment analysis approaches aim to detect the sentiments of written reviews in order to achieve a better understanding of public opinion towards entities. Aspect-based sentiment analysis deals with capturing sentiments expressed towards each aspect of entities. A common approach in sentiment analysis problems is to take advantage of lexicons to generate features for classification of reviews. Existing aspect-based approaches fail to properly adapt general lexicons to the context of aspect-based datasets which results in reduced performance. To address this problem, this paper proposes extensions of two lexicon generation methods for aspect-based problems; one using statistical methods, and another using a genetic algorithm presented in our previous works. The aforementioned lexicons are then fused with prominent static lexicons to classify the aspects in reviews; this outperforms our previous works according to the t-test results (with a p-value of less than 0.001). Experimental results indicate that the proposed approach outperforms baseline methods in aspect-based polarity classification on Bing Liu's customer review datasets and improves precision, recall and F-measure by 6.0, 1.0, and 7.4 percentage points respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300609",
    "keywords": [
      "Artificial intelligence",
      "Baseline (sea)",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Data mining",
      "Geology",
      "Lexicon",
      "Machine learning",
      "Measure (data warehouse)",
      "Natural language processing",
      "Oceanography",
      "Paleontology",
      "Precision and recall",
      "Sentiment analysis"
    ],
    "authors": [
      {
        "surname": "Mowlaei",
        "given_name": "Mohammad Erfan"
      },
      {
        "surname": "Saniee Abadeh",
        "given_name": "Mohammad"
      },
      {
        "surname": "Keshavarz",
        "given_name": "Hamidreza"
      }
    ]
  },
  {
    "title": "Discrete symbiotic organisms search method for solving large-scale time-cost trade-off problem in construction scheduling",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113230",
    "abstract": "Construction projects are becoming increasingly larger and more complex in terms of size and cost. An optimization tool is necessary for the construction management system to develop the desired construction schedule to save time and cost. However, only a few efforts have been made to deal with the time-cost trade-off problem (TCTP) in the large-scale construction projects, and the existing optimization methods are slightly limited by the trouble of parameter tuning. As TCTP is known to be an NP-hard problem, this paper aims to introduce a new variant of Symbiotic Organisms Search (SOS) algorithm that does not contain control parameters, called DSOS (Discrete Symbiotic Organisms Search) which generates the parasite organism using a heuristic rule based on the network levels. This enhancement helps to improve the exploration phase and avoid premature stagnation. Performances are evaluated on project instances with different numbers of activities varying from 180 to 6300, as well as nine newly generated project instances with 720 activities but different network structures. The obtained results show a good performance of DSOS in terms of robustness and deviation from optimum in comparison with other meta-heuristics and variants of DSOS without using the heuristic rule. The good performance implies that DSOS is sufficient to serve as an effective tool to generate an optimized construction schedule.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300567",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Gene",
      "Heuristic",
      "Heuristics",
      "Mathematical optimization",
      "Mathematics",
      "Operating system",
      "Operations research",
      "Robustness (evolution)",
      "Schedule",
      "Scheduling (production processes)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Dian"
      },
      {
        "surname": "Li",
        "given_name": "Heng"
      },
      {
        "surname": "Wang",
        "given_name": "Hongwei"
      },
      {
        "surname": "Qi",
        "given_name": "Chao"
      },
      {
        "surname": "Rose",
        "given_name": "Timothy"
      }
    ]
  },
  {
    "title": "Development of an integrated decision making model for location selection of logistics centers in the Spanish autonomous communities",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113208",
    "abstract": "Logistics centers are those areas where all the national and international logistics and transportation operations are managed and directed to various business operations. One of the essential elements of an urban development system is to identify the appropriate location for a logistics center. In practice, the issue of evaluating and selecting the most suitable geographical area for a logistics center is considered as a complex decision making problem that can be well formulated through analytical and mathematical models. An exhaustive review of literature indicates that no concrete study has still proposed any integrated evaluation approach for logistics center selection. Thus, this paper aims in developing a two-stage decision making model to find out the most preferred zone in the autonomous communities of Spain for establishment of logistics centers. In the first stage, the considered communities are compared based on five evaluation criteria using data envelopment analysis (DEA) to identify the efficient and inefficient alternatives. In the second stage, a model is designed to evaluate the performance of the efficient communities using rough full consistency (R-FUCOM) and combined compromise solution (R-CoCoSo) methods. The adopted model allows capturing the uncertainty and vagueness in the decision makers’ judgments as involved in the evaluation process with the use of rough set theory (RST). The R-FUCOM method is utilized to obtain the optimal weights of the criteria, while R-CoCoSo method is finally used to rank the efficient communities. In addition, sensitivity analyses are performed to validate the robustness of the derived results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300348",
    "keywords": [
      "Analytic hierarchy process",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Combinatorics",
      "Computer science",
      "Consistency (knowledge bases)",
      "Data envelopment analysis",
      "Fuzzy logic",
      "Gene",
      "Logistics center",
      "Mathematical optimization",
      "Mathematics",
      "Multiple-criteria decision analysis",
      "Operations research",
      "Rank (graph theory)",
      "Robustness (evolution)",
      "Vagueness"
    ],
    "authors": [
      {
        "surname": "Yazdani",
        "given_name": "Morteza"
      },
      {
        "surname": "Chatterjee",
        "given_name": "Prasenjit"
      },
      {
        "surname": "Pamucar",
        "given_name": "Dragan"
      },
      {
        "surname": "Chakraborty",
        "given_name": "Shankar"
      }
    ]
  },
  {
    "title": "The evolution of knowledge navigator model: The construction and application of KNM 2.0",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113209",
    "abstract": "The knowledge management (KM) maturity model provides a framework against which both old and new KM initiatives can be assessed to determine whether they are capable of generating new knowledge. Knowledge Navigator Model (KNM) proposed in 2009 (Hsieh et al., 2009) has been promoted in Taiwan to assist organizations to evaluate their KM status, and also aided to be a diffusion platform for government, academic and practice to exchange their KM experience. However, for the past 10 years, the industrial environment has been constantly changing that has brought the developing of KM practice. This study described the evolution of KNM to become KNM 2.0 in terms of construction and application. Qualitative and quantitative research methods were conducted to construct the proposed three modules of KNM 2.0. A 139 cases survey was employed and the results reveal the applicability of KNM 2.0. Our research contributes to the body of concepts on KM maturity models that could evaluate the readiness of service-oriented knowledge economy, big data and smart factory, and strategic KM performance. The proposed KNM 2.0, with these features which are novel and rapidly expanding fields, has been developed as an online Knowledge Management Evaluation (KM evaluation) website available for the practice to use to better understand their overall value brought by contemporary KM. Moving forward, by using KNM 2.0 to continually collect the data from the industries in Taiwan, it is expected to obtain more information about the practical KM that keeps developing with the trends, and also keep playing the role to distribute the contemporary concepts of KM. The KM experience could be referenced and the methodology could be applied to other countries.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742030035X",
    "keywords": [
      "Business",
      "Capability Maturity Model",
      "Computer science",
      "Construct (python library)",
      "Engineering",
      "Engineering management",
      "Factory (object-oriented programming)",
      "Government (linguistics)",
      "Knowledge management",
      "Law",
      "Linguistics",
      "Marketing",
      "Maturity (psychological)",
      "Operations research",
      "Philosophy",
      "Political science",
      "Programming language",
      "Service (business)",
      "Software"
    ],
    "authors": [
      {
        "surname": "Hsieh",
        "given_name": "Ping Jung"
      },
      {
        "surname": "Lin",
        "given_name": "Chinho"
      },
      {
        "surname": "Chang",
        "given_name": "Shofang"
      }
    ]
  },
  {
    "title": "Group teaching optimization algorithm: A novel metaheuristic method for solving global optimization problems",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113246",
    "abstract": "In last 30 years, many metaheuristic algorithms have been developed to solve optimization problems. However, most existing metaheuristic algorithms have extra control parameters except the essential population size and stopping criterion. Considering different characteristics of different optimization problems, how to adjust these extra control parameters is a great challenge for these algorithms in solving different optimization problems. In order to address this challenge, a new metaheuristic algorithm called group teaching optimization algorithm (GTOA) is presented in this paper. The proposed GTOA is inspired by group teaching mechanism. To adapt group teaching to be suitable for using as an optimization technique, without loss of generality, four simple rules are first defined. Then a group teaching model is built under the guide of the four rules, which consists of teacher allocation phase, ability grouping phase, teacher phase and student phase. Note that GTOA needs only the essential population size and stopping criterion without extra control parameters, which has great potential to be used widely. GTOA is first examined over 28 well-known unconstrained benchmark problems and the optimization results are compared with nine state-of-the-art algorithms. Experimental results show the superior performance of the proposed GTOA for these problems in terms of solution quality, convergence speed and stability. Furthermore, GTOA is used to solve four constrained engineering design optimization problems in the real world. Simulation results demonstrate the proposed GTOA can find better solutions with faster speed compared with the reported optimizers.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300725",
    "keywords": [
      "Algorithm",
      "Benchmark (surveying)",
      "Computer science",
      "Continuous optimization",
      "Convergence (economics)",
      "Demography",
      "Economic growth",
      "Economics",
      "Engineering optimization",
      "Generality",
      "Geodesy",
      "Geography",
      "Global optimization",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Meta-optimization",
      "Metaheuristic",
      "Multi-swarm optimization",
      "Optimization problem",
      "Parallel metaheuristic",
      "Population",
      "Psychology",
      "Psychotherapist",
      "Sociology",
      "Stability (learning theory)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Yiying"
      },
      {
        "surname": "Jin",
        "given_name": "Zhigang"
      }
    ]
  },
  {
    "title": "A swarm optimization-based search algorithm for the quadratic knapsack problem with conflict Graphs",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113224",
    "abstract": "The knapsack problem arises in a variety of real world applications such as railway stations, flexible manufacturing systems, multimedia, cryptography and hydrological studies. In this paper, a special case of the knapsack problem is tackled: the quadratic knapsack problem with conflict graphs. This problem is solved by using a population-based search algorithm, which is inspired from the binary particle swarm optimization combined with a quick and efficient local search. The particle swarm optimization generates a population of particles while the local search procedure tries either to repair the infeasibility of each binary solution or to improve its quality. The performance of the proposed method is evaluated on a set of benchmark instances taken from the literature (containing medium and large-scale instances), where its achieved results are compared to those published in the literature containing the bounds realized with GLPK, Cplex and those achieved by more recent methods. The proposed method remains competitive, where encouraging results have been obtained.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300506",
    "keywords": [
      "Algorithm",
      "Change-making problem",
      "Combinatorial optimization",
      "Computer science",
      "Continuous knapsack problem",
      "Geometry",
      "Knapsack problem",
      "Mathematical optimization",
      "Mathematics",
      "Metaheuristic",
      "Quadratic assignment problem",
      "Quadratic equation",
      "Quadratic programming",
      "Swarm behaviour"
    ],
    "authors": [
      {
        "surname": "Dahmani",
        "given_name": "Isma"
      },
      {
        "surname": "Hifi",
        "given_name": "Mhand"
      },
      {
        "surname": "Saadi",
        "given_name": "Toufik"
      },
      {
        "surname": "Yousef",
        "given_name": "Labib"
      }
    ]
  },
  {
    "title": "Characterizing basal-like triple negative breast cancer using gene expression analysis: A data mining approach",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113253",
    "abstract": "Triple-negative breast cancer (TNBC) is characterized by the absence of expression of the estrogen receptor, progesterone receptor and human epidermal growth factor receptor 2 (HER2). Therefore, TNBC is unresponsive to targeted hormonal therapies, which limits treatment options to nonselective chemotherapeutic agents. Basal-like breast cancers (BLBCs) represent a subset of about 70% of TNBCs, more frequently affecting younger patients, being more prevalent in African-American women and significantly more aggressive than tumors of other molecular subtypes, with high rates of proliferation and extremely poor clinical outcomes. Proper classification of BLBCs using current pathological tools has been a major challenge. Although TNBCs have many BLBC characteristics, the relationship between clinically defined TNBC and the gene expression profile of BLBC is not fully examined. The purpose of this study is to process publicly-available TNBC gene expression datasets generated by Affymetrix gene chips and define a set of genes, or gene signature, that can classify TNBC samples between BLBC and Non-BLBC subtypes. We used over 3500 breast cancer gene expression profiles from several individual publicly available datasets and extracted Affymetrix gene expression data for 580 TNBC cases. Several popular data mining methods along with dimensionality reduction and feature selection techniques were applied to the resultant dataset to build predictive models to understand molecular characteristics and mechanisms associated with BLBCs and to classify them more accurately according to important features extracted through microarray data analysis of BLBC and Non-BLBC cases. Our result can lead to proper identification and diagnosis of BLBCs, which can potentially direct clinical implications by dictating the most effective therapy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300786",
    "keywords": [
      "Biology",
      "Breast cancer",
      "Cancer",
      "Computational biology",
      "Estrogen receptor",
      "Gene",
      "Gene expression",
      "Gene expression profiling",
      "Genetics",
      "Internal medicine",
      "Medicine",
      "Microarray",
      "Microarray analysis techniques",
      "Oncology",
      "Progesterone receptor",
      "Triple-negative breast cancer"
    ],
    "authors": [
      {
        "surname": "Hassan Zadeh",
        "given_name": "Amir"
      },
      {
        "surname": "Alsabi",
        "given_name": "Qamar"
      },
      {
        "surname": "Ramirez-Vick",
        "given_name": "Jaime E."
      },
      {
        "surname": "Nosoudi",
        "given_name": "Nasim"
      }
    ]
  },
  {
    "title": "Applying Dempster–Shafer theory for developing a flexible, accurate and interpretable classifier",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113262",
    "abstract": "Two approaches have traditionally been identified for developing artificial intelligence systems supporting decision-making: Machine Learning, which applies general techniques based on statistical analysis and optimization methods to extract information from a large amount of data looking for possible relations among them, and Expert Systems, which codify experts knowledge in rules, which are then applied to a specific situation. One of the main advantages of the first approach is its greater accuracy and wider generality for the application of the methods developed which can be used in various scenarios. By contrast, expert systems are usually more restricted and often applicable only to the domain for which they were originally developed. However, the machine learning approach requires the availability of large chunks of data, and it is much more complicated to interpret the results of the statistical methods to obtain some explanation of why the system decides, classifies, or evaluates a situation in a certain way. This issue may become very important in areas such as medicine, where it is relevant to know why the system recommends a certain treatment or diagnoses a certain illness. Likewise, in the financial sector, it might be legally required to explain that a decision to reject the granting of a mortgage loan to a person is not due to discriminatory causes such as gender or race. In order to be able to have interpretability and extract knowledge of available data we developed a classification method based on Dempster-Shafer’s Plausibility Theory. Mass assignment functions (MAF) must be established to apply this theory and they assign a weight or probability to all subsets of the possible outcomes, given the presence of a certain fact on a decision scenario. Thus MAF assignments encode expert knowledge. The method learns optimal values for the weights of each MAF using the Gradient Descent method. The presented method allows combination of MAF which have been generated by the method itself or defined by an expert with those that are derived from a set of available data. The developed method was first applied to controlled scenarios and traditional data sets to ensure that classifications and explanations are correct. Results show that the model can classify with an accuracy which is comparable to other statistical classification methods, being also able to extract the most important decision rules from the data.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300877",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Dempster–Shafer theory",
      "Machine learning",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Peñafiel",
        "given_name": "Sergio"
      },
      {
        "surname": "Baloian",
        "given_name": "Nelson"
      },
      {
        "surname": "Sanson",
        "given_name": "Horacio"
      },
      {
        "surname": "Pino",
        "given_name": "José A."
      }
    ]
  },
  {
    "title": "Stacking ensemble based deep neural networks modeling for effective epileptic seizure detection",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113239",
    "abstract": "Electroencephalography signals obtained from the brain‘s electrical activity are commonly used for the diagnosis of neurological diseases. These signals indicate the electrical activity in the brain and contain information about the brain. Epilepsy, one of the most important diseases in the brain, manifests itself as a result of abnormal pathological oscillating activity of a group of neurons in the brain. Automated systems that employed the electroencephalography signals are being developed for the assessment and diagnosis of epileptic seizures. The aim of this study is to focus on the effectiveness of stacking ensemble approach based model for predicting whether there is epileptic seizure or not. So, this study enables the readers and researchers to examine the proposed stacking ensemble model. The benchmark clinical dataset provided by Bonn University was used to assess the proposed model. Comparative experiments were conducted by utilizing the proposed model and the base deep neural networks model to show the effectiveness of the proposed model for seizure detection. Experiments show that the proposed model is proven to be competitive to base DNN model. The results indicate that the performance of the epileptic seizure detection by the stacking ensemble based deep neural networks model is high; especially the average accuracy value of 97.17%. Also, its average sensitivity with 93.11% is superior to the base DNN model. Thus, it can be said that the proposed model can be included in an expert system or decision support system. In this context, this system would be precious for the clinical diagnosis and treatment of epilepsy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300658",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Electroencephalography",
      "Electronic engineering",
      "Engineering",
      "Ensemble forecasting",
      "Epilepsy",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Neuroscience",
      "Nuclear magnetic resonance",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Physics",
      "Psychology",
      "Sensitivity (control systems)",
      "Stacking"
    ],
    "authors": [
      {
        "surname": "Akyol",
        "given_name": "Kemal"
      }
    ]
  },
  {
    "title": "Predicting sentence-level polarity labels of financial news using abnormal stock returns",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113223",
    "abstract": "Expert systems for automatic processing of financial news commonly operate at the document-level by counting positive and negative term-frequencies. This, however, limits their usefulness for investors and financial practitioners seeking specific positive and negative information on a more fine-grained level. For this purpose, this paper develops a novel machine learning approach for the prediction of sentence-level polarity labels in financial news. The method uses distributed text representations in combination with multi-instance learning to transfer information from the document-level to the sentence-level. This has two key advantages: (1) it captures semantic information of the textual data and thereby prevents the loss of information caused by bag-of-words approaches; (2) it is solely trained based on historic stock market reactions following the publication of news items without the need for any kind of manual labeling. Our experiments on a manually-labeled dataset of sentences from financial news yield a predictive accuracy of up to 71.20%, exceeding the performance of alternative approaches significantly by at least 5.10 percentage points. Hence, the proposed approach provides accurate decision support for investors and may assist investor relations departments in communicating their messages as intended. Furthermore, it presents promising avenues for future research aiming at studying communication patterns in financial news.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742030049X",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Business",
      "Computer science",
      "Computer security",
      "Engineering",
      "Finance",
      "Financial ratio",
      "Horse",
      "Information retrieval",
      "Key (lock)",
      "Machine learning",
      "Mechanical engineering",
      "Natural language processing",
      "Paleontology",
      "Sentence",
      "Sentiment analysis",
      "Stock (firearms)",
      "Stock market"
    ],
    "authors": [
      {
        "surname": "Lutz",
        "given_name": "Bernhard"
      },
      {
        "surname": "Pröllochs",
        "given_name": "Nicolas"
      },
      {
        "surname": "Neumann",
        "given_name": "Dirk"
      }
    ]
  },
  {
    "title": "Multi-objective flexibility-complexity trade-off problem in batch production systems using fuzzy goal programming",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113266",
    "abstract": "Several factors change the flexibility and the complexity of production systems. The flexibility of the production system means to meet the changing needs of the customers. As flexibility increases the complexity also increases. In this paper, a multi-objective linear programming is proposed to model the trade-off of the complexity and flexibility of a batch production system (BPS). Strategic, tactical and operational decision variables have been considered. Seven objective functions of the proposed model are assumed as flexibility and complexity. Sixteen tactical decision variables are defined to determine the level of the dimensions of flexibilities and complexities. Thirty-four operational decision variables are defined to tune the shop-floor operations. Several sets of constraints considering aspects of flexibility and complexity as well as the conditions of batch production systems have been considered. As the achievement to the objective functions is not possible simultaneously, and there is no unique and concise relation between these objective functions in a typical batch production system so, a fuzzy goal programming (FGP) approach is proposed to solve the model. Moreover, goal programming (GP), Fuzzy GP, multi-choice goal programming (MCGP) and fuzzy MCGP are proposed and used to compare the performance of solution procedures. The superior solution approach among GP, MCGP, FGP, and FMCGP is FMCGP which concurrently considers several aspiration levels for objective functions, maximization of the achievement level of objective functions, and satisfying uncertain preference of fuzzy objectives. An evolutionary algorithm, called non-dominated sorting genetic algorithm (NSGA-II) and a random weighted version of FMCGP are customized to regenerate several non-dominated designs for flexibility-complexity trade-off problem in BPS. The results are promising and the proposed model is capable to set the strategic, tactical and operational variables of a BPS.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300919",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Economics",
      "Flexibility (engineering)",
      "Fuzzy logic",
      "Goal programming",
      "Linear programming",
      "Macroeconomics",
      "Mathematical optimization",
      "Mathematics",
      "Production (economics)",
      "Sorting",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Javid",
        "given_name": "Nasser"
      },
      {
        "surname": "Khalili-Damghani",
        "given_name": "Kaveh"
      },
      {
        "surname": "Makui",
        "given_name": "Ahmad"
      },
      {
        "surname": "Abdi",
        "given_name": "Farshid"
      }
    ]
  },
  {
    "title": "The damped oscillator model (DOM) and its application in the prediction of emotion development of online public opinions",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113268",
    "abstract": "Online public opinions refer to the attitude of the public towards certain events or topics in social media and have informative significance for social governance and the formulation of public policies. Previous studies have proved the important role played by online public opinions in expert systems from which the prediction of online public opinions has drawn increasing attention from scholars. Though the development of public opinion events is largely driven by the emotions of users, few studies have regarded the issues of emotion development forecasting, and existing emotional dynamic models are relatively simple, not real-time predicting, and with no considerations of people's self-emotion-changing mechanisms. In order to fill this gap, this paper proposes a Damped Oscillator Model (DOM). Compared to existing models, the proposed model has two main merits: first, the Mass-Spring-Damper system in the physics area is used to measure the self-decaying process of human emotion which has not been included in previous opinion dynamics models; second, a self-emotional adaptation mechanism is introduced and final states with no obviously established opinions can be reached, which is common in real online public opinion cases. Simulation experiments are conducted to discuss the impacts of critical parameters contained in the model on opinion dynamics. Two real online public opinion events were studied using the proposed model, with critical parameters extracted by the particle swarm optimization algorithm. The predictive results have outperformed several previous well-known models. With its real-time predictive capacity, this model can provide substantial auxiliary support to the expert and intelligent system for making wise decisions in advance, especially during fast developing public opinion crises.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300932",
    "keywords": [
      "Artificial intelligence",
      "Business",
      "Cognitive psychology",
      "Computer science",
      "Data science",
      "Finance",
      "Law",
      "Machine learning",
      "Operating system",
      "Order (exchange)",
      "Particle swarm optimization",
      "Political science",
      "Politics",
      "Process (computing)",
      "Psychology",
      "Public opinion",
      "Social media",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Dong",
        "given_name": "Xuefan"
      },
      {
        "surname": "Lian",
        "given_name": "Ying"
      },
      {
        "surname": "Tang",
        "given_name": "Xianyi"
      },
      {
        "surname": "Liu",
        "given_name": "Yijun"
      }
    ]
  },
  {
    "title": "Group-of-features relevance in multinomial kernel logistic regression and application to human interaction recognition",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113247",
    "abstract": "We propose an approach for human interaction recognition (HIR) in videos using multinomial kernel logistic regression with group-of-features relevance (GFR-MKLR). Our approach couples kernel and group sparsity modelling to ensure highly precise interaction classification. The group structure in GFR-MKLR is chosen to reflect a representation of interactions at the level of gestures, which ensures more robustness to intra-class variability due to occlusions and changes in subject appearance, body size and viewpoint. The groups consist of motion features extracted from tracking interacting persons joints over time. We encode group sparsity in GFR-MKLR through relevance weights reflecting each group (gesture) discrimination capability between different interaction categories. These weights are automatically estimated during GFR-MKLR training using gradient descent minimisation. Our model is computationally efficient and can be trained on a small training dataset while maintaining a good generalization and interpretation capabilities. Experiments on the well-known UT-Interaction dataset have demonstrated the performance of our approach by comparison with state-of-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300683",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Combinatorics",
      "Computer science",
      "ENCODE",
      "Gene",
      "Gesture",
      "Kernel (algebra)",
      "Logistic regression",
      "Machine learning",
      "Mathematics",
      "Multinomial logistic regression",
      "Pattern recognition (psychology)",
      "Regression",
      "Robustness (evolution)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Ouyed",
        "given_name": "Ouiza"
      },
      {
        "surname": "Allili",
        "given_name": "Mohand Said"
      }
    ]
  },
  {
    "title": "Discriminative dimensionality reduction for sensor drift compensation in electronic nose: A robust, low-rank, and sparse representation method",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113238",
    "abstract": "Sensor drift, which is a critical issue in the field of sensor measurements, has plagued the sensor community in the past several decades. How to tackle the sensor drift problem using expert and intelligent systems has gained increasing attention. Most sensor drift compensation methods ignore the sparse and low-rank characteristics of sensor signals. In this paper, we propose a discriminative dimensionality reduction method for sensor drift compensation in the electronic nose. The proposed method consists of four major components. (1) The distribution discrepancy between source data and target data is alleviated via projecting all the data into a common subspace. (2) A sparse and low-rank reconstruction coefficient matrix is employed to preserve the global and local structures of sensor signals. (3) An error matrix is introduced to deal with outliers. (4) The source data label information is taken into consideration to avoid overlapping of samples with different labels in the common subspace. The formulated minimization problem with constraints can be solved in an iterative manner. The effectiveness of the proposed method has been verified by conducting experiments on two sensor drift datasets. The proposed method may provide new insights into the gas sensor drift compensation systems or other expert and intelligent systems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300646",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Dimensionality reduction",
      "Discriminative model",
      "Electronic nose",
      "Outlier",
      "Pattern recognition (psychology)",
      "Sparse approximation",
      "Subspace topology"
    ],
    "authors": [
      {
        "surname": "Yi",
        "given_name": "Zhengkun"
      }
    ]
  },
  {
    "title": "Big Data with deep learning for benchmarking profitability performance in project tendering",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113194",
    "abstract": "A reliable benchmarking system is crucial for the contractors to evaluate the profitability performance of project tenders. Existing benchmarks are ineffective in the tender evaluation task for three reasons. Firstly, these benchmarks are mostly based on the profit margins as the only key performance indicator (KPI) while there are other KPIs fit to drive the evaluation process. Secondly, these benchmarks don’t take project context into account, thereby restricts their predictive accuracy. And finally, these benchmarks are obtained from small subsets of data, making it hard to generalise. As a result, estimators cannot probe into tenders to judge the strengths and weaknesses of their bids. This advancement is critical for not only choosing more lucrative opportunities but also driving negotiations during the tendering process. This study aims to develop a benchmarking system for tender evaluation using Big Data of 1.2 terabytes, comprising 5.7 million cells. A holistic list of seventeen (17) KPIs is identified from the email data using Text Mining approaches. Besides, eight (8) key project attributes are chosen for ensuring context-aware benchmarking using Focused Group Interviews (FGIs). At the crux of this work lies the proposition of a deep ensemble learner based on the decomposition-integration methodology. In the decomposition stage, the model predicts several attribute-specific benchmarks for each KPI using our proposed context-aware algorithm. In the integration stage, deep neural network-based learners are trained to generate final project-sensitive KPI benchmark. The learner is deployed in the Spring tool to support the tender evaluation of power infrastructure projects. A tender of 60km underground cabling project is evaluated using the proposed learner. The system spontaneously identified KPIs in the tender that require further attention to achieve greater profitability performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300208",
    "keywords": [
      "Benchmark (surveying)",
      "Benchmarking",
      "Biology",
      "Business",
      "Computer science",
      "Context (archaeology)",
      "Engineering",
      "Finance",
      "Geodesy",
      "Geography",
      "Marketing",
      "Operating system",
      "Operations research",
      "Paleontology",
      "Performance indicator",
      "Process (computing)",
      "Process management",
      "Procurement",
      "Profitability index"
    ],
    "authors": [
      {
        "surname": "Bilal",
        "given_name": "Muhammad"
      },
      {
        "surname": "Oyedele",
        "given_name": "Lukumon O."
      }
    ]
  },
  {
    "title": "Intelligent personal assistants: A systematic literature review",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113193",
    "abstract": "Natural Language Interfaces allow human-computer interaction through the translation of human intention into devices’ control commands, analyzing the user’s speech or gestures. This novel interaction mode arises from advancements of artificial intelligence, expert systems, speech recognition, semantic web, dialog systems, and natural language processing, bringing the concept of Intelligent Personal Assistant (IPA). There is currently a vast literature on this subject. However, in the best of our knowledge, there is no thorough analysis of the state-of-the-art in the field. In this context, we present in this article a survey of the field, discussing the main trends, critical areas, and challenges of an IPA. Another contribution is the proposition of a taxonomy for IPA classification. The method used to achieve these objectives consisted of a systematic literature review based on the population, intervention, comparison, outcome, and context (PICOC) criteria. As a result, we started from more than 3472 scientific articles published in the last six years, searched on a set of databases chosen to increase the probability of finding highly relevant articles. The review selected the 58 most significant articles, identifying challenges and open questions. We also discuss in the article the current status, usage, security and privacy issues, types, and architectures regarding an IPA. We conclude that usability, security, and privacy directly affect the confidence of the user in adopting an IPA.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300191",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Demography",
      "Dialog box",
      "Field (mathematics)",
      "Human–computer interaction",
      "Law",
      "MEDLINE",
      "Mathematics",
      "Paleontology",
      "Political science",
      "Population",
      "Pure mathematics",
      "Sociology",
      "Systematic review",
      "Usability",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "de Barcelos Silva",
        "given_name": "Allan"
      },
      {
        "surname": "Gomes",
        "given_name": "Marcio Miguel"
      },
      {
        "surname": "da Costa",
        "given_name": "Cristiano André"
      },
      {
        "surname": "da Rosa Righi",
        "given_name": "Rodrigo"
      },
      {
        "surname": "Barbosa",
        "given_name": "Jorge Luis Victoria"
      },
      {
        "surname": "Pessin",
        "given_name": "Gustavo"
      },
      {
        "surname": "De Doncker",
        "given_name": "Geert"
      },
      {
        "surname": "Federizzi",
        "given_name": "Gustavo"
      }
    ]
  },
  {
    "title": "Applying genetic algorithms with speciation for optimization of grid template pattern detection in financial markets",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113191",
    "abstract": "This paper presents a new computational finance approach. It combines a grid pattern recognition technique allied to an evolutionary computation optimization kernel based on Genetic Algorithms, creating a dynamic way to attribute a score to the signal that takes volatility into consideration and normalizing the pattern detection by fixing the grid size with the ultimate goal of reduce risk and increase profits. For pattern matching, a template based approach using a fixed size grid of weights is adopted to describe the desired trading patterns, taking not only the closing price into consideration, but also the variation of price in each considered time interval of the time series. The scores assigned to the grid of weights will be optimized by the Genetic Algorithm and, at the same time, the genetic diversity of possible solutions will be preserved using a speciation technique, giving time for individuals to be optimized within their own niche. The adoption of this approach has the goal of reducing the investment risk and check if it outperforms similar approaches. This system was tested against state-of-the-art solutions, namely the existing adaptable grid of weights and a non speciated approach, considering real data from the stock market. The developed approach using the grid of weights had 21.3% of average return over the testing period against 10.9% of the existing approach and the use of speciation improved some of the training results as genetic diversity was taken into consideration.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300178",
    "keywords": [
      "Algorithm",
      "Computation",
      "Computer science",
      "Data mining",
      "Economics",
      "Finance",
      "Genetic algorithm",
      "Geometry",
      "Grid",
      "Machine learning",
      "Mathematics",
      "Volatility (finance)"
    ],
    "authors": [
      {
        "surname": "Martins",
        "given_name": "Tiago Mousinho"
      },
      {
        "surname": "Neves",
        "given_name": "Rui Ferreira"
      }
    ]
  },
  {
    "title": "Multiple premises entailment recognition based on attention and gate mechanism",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113214",
    "abstract": "Multi-premise natural language inference provides important technical support for automatic question answering, machine reading comprehension and other application fields. Existing approaches for Multiple Premises Entailment (MPE) task are to convert MPE data into Single Premise Entailment (SPE) data format, then MPE is handled in the same way as SPE. This process ignores the unique characteristics of multi-premise, which will result in loss of semantics. This paper proposes a mechanism based on Attention and Gate Fusion Network (AGNet). AGNet adopts a “Local Matching-Integration” strategy to consider the characteristics of multi-premise. In this process, an attention mechanism combined with a matching gate mechanism can fully describe the relationship between the premise and hypothesis. A self-attention mechanism and a fusion gate mechanism can deeply exploit the relationship from the multi-premise. In order to avoid over-fitting problem, we propose a pre-training method for our model. In terms of computational complexity, AGNet has good parallelism, reduces the time complexity to O(1) in the process of matching. The experiments show that our model has achieved new state-of-the-art results on MPE test set.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300403",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Epistemology",
      "Fusion",
      "Fusion mechanism",
      "Linguistics",
      "Lipid bilayer fusion",
      "Logical consequence",
      "Matching (statistics)",
      "Mathematics",
      "Mechanism (biology)",
      "Natural language processing",
      "Philosophy",
      "Premise",
      "Process (computing)",
      "Programming language",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Pin"
      },
      {
        "surname": "Lei",
        "given_name": "Zhidan"
      },
      {
        "surname": "Zhou",
        "given_name": "Quan"
      },
      {
        "surname": "Zhu",
        "given_name": "Rukang"
      },
      {
        "surname": "Chang",
        "given_name": "Xuting"
      },
      {
        "surname": "Sun",
        "given_name": "Junwu"
      },
      {
        "surname": "Zhang",
        "given_name": "Wenjie"
      },
      {
        "surname": "Guo",
        "given_name": "Yike"
      }
    ]
  },
  {
    "title": "Effect of dimensionality reduction on stock selection with cluster analysis in different market situations",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113226",
    "abstract": "Dimensionality reduction is inevitable in stock selection with cluster analysis. Considering relations among dimensionality reduction, noise trading, and market situations, we empirically investigate the effect of dimensionality-reduction methods–principal component analysis, stacked autoencoder, and stacked restricted Boltzmann machine–on stock selection with cluster analysis in different market situations. Based on the index fluctuation, the market is divided into sideways and trend situations. For the CSI 100 and Nikkei 225 constituent stocks, experimental results show that: (1) In sideways situations, dimensionality reduction hardly improves the performance of stock selection with cluster analysis; (2) the advantage of dimensionality reduction is mainly reflected in trend situations, but whether it is in an up or down trend depends on the market analyzed. More importantly, according to the above findings and assuming that the dimensionality-reduction effect will continue, we propose a rotation strategy with and without dimensionality reduction. The results of experiments show that the proposed rotation strategy outperforms the stock market indices as well as the stock-selection strategies based on dimensionality reduction and cluster analysis. These findings offer practical insights into how dimensionality reduction can be efficiently used for stock selection.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742030052X",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Biology",
      "Chemistry",
      "Cluster (spacecraft)",
      "Computer science",
      "Curse of dimensionality",
      "Dimensionality reduction",
      "Gene",
      "Genotype",
      "Geometry",
      "Horse",
      "Mathematics",
      "Multifactor dimensionality reduction",
      "Paleontology",
      "Programming language",
      "Reduction (mathematics)",
      "Selection (genetic algorithm)",
      "Single-nucleotide polymorphism",
      "Stock market"
    ],
    "authors": [
      {
        "surname": "Han",
        "given_name": "Jingti"
      },
      {
        "surname": "Ge",
        "given_name": "Zhipeng"
      }
    ]
  },
  {
    "title": "Evolutionary multiobjective optimization to target social network influentials in viral marketing",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113183",
    "abstract": "Marketers have an important asset if they effectively target social networks’ influentials. They can advertise products or services with free items or discounts to spread positive opinions to other consumers (i.e., word-of-mouth). However, main research on choosing the best influentials to target is single-objective and mainly focused on maximizing sales revenue. In this paper we propose a multiobjective approach to the influence maximization problem with the aim of increasing the revenue of viral marketing campaigns while reducing the costs. By using local social network metrics to locate influentials, we apply two evolutionary multiobjective optimization algorithms, NSGA-II and MOEA/D, a multiobjective adaptation of a single-objective genetic algorithm, and a greedy algorithm. Our proposal uses a realistic agent-based market framework to evaluate the fitness of the chromosomes by simulating the viral campaigns. The framework also generates, in a single run, a set of non-dominated solutions that allows marketers to consider multiple targeting options . The algorithms are evaluated on five network topologies and a real data-generated social network, showing that both MOEA/D and NSGA-II outperform the single-objective and the greedy approaches. More interestingly, we show a clear correlation between the algorithms’ performance and the diffusion features of the social networks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300099",
    "keywords": [
      "Artificial intelligence",
      "Business",
      "Computer science",
      "Machine learning",
      "Marketing",
      "Multi-objective optimization",
      "Social marketing",
      "Social media",
      "Social network (sociolinguistics)",
      "Viral marketing",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Robles",
        "given_name": "Juan Francisco"
      },
      {
        "surname": "Chica",
        "given_name": "Manuel"
      },
      {
        "surname": "Cordon",
        "given_name": "Oscar"
      }
    ]
  },
  {
    "title": "Reliable shortest path finding in stochastic time-dependent road network with spatial-temporal link correlations: A case study from Beijing",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113192",
    "abstract": "In view of the time-dependent characteristic of travel times in road networks and the travel time reliability (TTR) requirements by different travelers, it is complicated and time-consuming to determine the reliable shortest path (RSP) in large-scale road networks. To search the RSP in stochastic and time-dependent (STD) network with spatial-temporal correlated link travel times, an efficient path finding algorithm is presented. First, the fitting test results based on floating car data show that it is more appropriate to characterize the travel time distributions (TTDs) of links using lognormal distributions. In order to quantify spatial-temporal correlations between links, correlation coefficients of link travel times are calculated. Also, influences of spatial distance (counted by the number of links), temporal distance (counted by the number of time intervals) and road type on link correlations is analyzed. Afterwards, the dynamic moment-matching method (DMM) is used to calculate the approximate path TTD when correlated link travel times are considered. Accounting for different travelers' risk tolerance, a dynamic-moment-matching-based A* algorithm (STCRSP-DMA*) is proposed to provide personalized path navigation for individual travelers. Last, numerical case studies based on abundant floating car data as well as a subsistent road network in Beijing are conducted to demonstrate the applicability and the computational advantage of the devised algorithm in solving RSP searching problems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742030018X",
    "keywords": [
      "Algorithm",
      "Archaeology",
      "Average path length",
      "Beijing",
      "China",
      "Classical mechanics",
      "Computer science",
      "Data mining",
      "Engineering",
      "Geography",
      "Graph",
      "Matching (statistics)",
      "Mathematics",
      "Moment (physics)",
      "Path (computing)",
      "Physics",
      "Power (physics)",
      "Programming language",
      "Quantum mechanics",
      "Reliability (semiconductor)",
      "Shortest path problem",
      "Statistics",
      "Theoretical computer science",
      "Transport engineering",
      "Travel time"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Peng"
      },
      {
        "surname": "Tong",
        "given_name": "Rui"
      },
      {
        "surname": "Yu",
        "given_name": "Bin"
      },
      {
        "surname": "Wang",
        "given_name": "Yunpeng"
      }
    ]
  },
  {
    "title": "Symbiotic Organisms Search Algorithm for multilevel thresholding of images",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113210",
    "abstract": "Thresholding is a frequently used method in image processing because of its consistency and low computational cost. Otsu's and Kapur's methods are two important techniques that were proved to be best thresholding methods. However, they have high computational complexity when extended to multilevel thresholding because of their exhaustively search. Recently, meta-heuristic algorithms have been successfully applied for thresholding problems. In this study, six different meta-heuristic algorithms based on Otsu's and Kapur's functions; Particle Swarm Optimization (PSO), Firefly Algorithm (FA), Symbiotic Organisms Search (SOS), Artifical Bee Colony (ABC), Genetic Algorithm (GA) and grey Wolf Optimizer (GWO) were used for multilevel thresholding problem and compared. Experimental results suggest that SOS, PSO and FA algorithms often have higher fitness values than other algorithms. Especially when more than two threshold values are determined, SOS algorithm mostly gives higher fitness values. PSNR and SSIM results of the algorithms are similar. In terms of computational complexity, the GWO algorithm has the fastest convergence. For standard deviations of objective functions; more stable results were obtained with SOS based on Kapur's function, SOS and PSO based on Otsu's function. Also, SOS based on Kapur's function was found to be the most successful algorithm in the Friedman test. As a result, although the GWO approached faster, the SOS algorithm produced more consistent results for both objective functions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300361",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Balanced histogram thresholding",
      "Computational complexity theory",
      "Computer science",
      "Firefly algorithm",
      "Fitness function",
      "Genetic algorithm",
      "Heuristic",
      "Histogram",
      "Histogram equalization",
      "Image (mathematics)",
      "Mathematical optimization",
      "Mathematics",
      "Otsu's method",
      "Particle swarm optimization",
      "Thresholding"
    ],
    "authors": [
      {
        "surname": "Küçükuğurlu",
        "given_name": "Büşranur"
      },
      {
        "surname": "Gedikli",
        "given_name": "Eyüp"
      }
    ]
  },
  {
    "title": "Ensemble learning by means of a multi-objective optimization design approach for dealing with imbalanced data sets",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113232",
    "abstract": "Ensemble learning methods have already shown to be powerful techniques for creating classifiers. However, when dealing with real-world engineering problems, class imbalance is usually found. In such scenario, canonical machine learning algorithms may not present desirable solutions, and techniques for overcoming this problem must be used. In addition to using learning algorithms that alleviate the imbalance between classes, multi-objective optimization design (MOOD) approaches can be used to improve the prediction performance of ensembles of classifiers. This paper proposes a study of different MOOD approaches for ensemble learning. First, a taxonomy on multi-objective ensemble learning (MOEL) is proposed. In it, four types of existing approaches are defined: multi-objective ensemble member generation, multi-objective ensemble member selection, multi-objective ensemble member combination, and multi-objective ensemble member selection and combination. Additionally, new approaches can be derived by combining the previous ones, such as multi-objective ensemble member generation and selection, multi-objective ensemble member generation and combination and multi-objective ensemble member generation, selection and combination. With the given taxonomy, two experiments are conducted for comparing (1) the performance of the MOEL techniques for generating and aggregating base models on several imbalanced benchmark problems and (2) the performance of MOEL techniques against other machine learning techniques in a real-world imbalanced drinking-water quality anomaly detection problem. Finally, results indicate that MOOD is able to improve the predictive performance of existing ensemble learning techniques.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300580",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Ensemble forecasting",
      "Ensemble learning",
      "Geodesy",
      "Geography",
      "Machine learning"
    ],
    "authors": [
      {
        "surname": "Alves Ribeiro",
        "given_name": "Victor Henrique"
      },
      {
        "surname": "Reynoso-Meza",
        "given_name": "Gilberto"
      }
    ]
  },
  {
    "title": "An empirical evaluation of the inferential capacity of defeasible argumentation, non-monotonic fuzzy reasoning and expert systems",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113220",
    "abstract": "Several non-monotonic formalisms exist in the field of Artificial Intelligence for reasoning under uncertainty. Many of these are deductive and knowledge-driven, and also employ procedural and semi-declarative techniques for inferential purposes. Nonetheless, limited work exist for the comparison across distinct techniques and in particular the examination of their inferential capacity. Thus, this paper focuses on a comparison of three knowledge-driven approaches employed for non-monotonic reasoning, namely expert systems, fuzzy reasoning and defeasible argumentation. A knowledge-representation and reasoning problem has been selected: modelling and assessing mental workload. This is an ill-defined construct, and its formalisation can be seen as a reasoning activity under uncertainty. An experimental work was performed by exploiting three deductive knowledge bases produced with the aid of experts in the field. These were coded into models by employing the selected techniques and were subsequently elicited with data gathered from humans. The inferences produced by these models were in turn analysed according to common metrics of evaluation in the field of mental workload, in specific validity and sensitivity. Findings suggest that the variance of the inferences of expert systems and fuzzy reasoning models was higher, highlighting poor stability. Contrarily, that of argument-based models was lower, showing a superior stability of its inferences across knowledge bases and under different system configurations. The originality of this research lies in the quantification of the impact of defeasible argumentation. It contributes to the field of logic and non-monotonic reasoning by situating defeasible argumentation among similar approaches of non-monotonic reasoning under uncertainty through a novel empirical comparison.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300464",
    "keywords": [
      "Argumentation framework",
      "Argumentation theory",
      "Artificial intelligence",
      "Computer science",
      "Deductive reasoning",
      "Defeasible estate",
      "Epistemology",
      "Field (mathematics)",
      "Fuzzy logic",
      "Geometry",
      "Inference",
      "Knowledge representation and reasoning",
      "Machine learning",
      "Mathematics",
      "Model-based reasoning",
      "Non-monotonic logic",
      "Philosophy",
      "Pure mathematics",
      "Qualitative reasoning",
      "Rotation formalisms in three dimensions"
    ],
    "authors": [
      {
        "surname": "Rizzo",
        "given_name": "Lucas"
      },
      {
        "surname": "Longo",
        "given_name": "Luca"
      }
    ]
  },
  {
    "title": "Using a segmenting description approach in multiple criteria decision aiding",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113186",
    "abstract": "We propose a new method for analyzing a set of parameters in a multiple criteria ranking method. Unlike the existing techniques, we do not use any optimization method, instead incorporating and extending a Segmenting Description (SD) approach. The algorithm rewrites a set of initial inequalities into an equivalent SD system by means of three fundamental operations in a way that makes it possible to either easily analyze all feasible solutions or identify all sources of inconsistency. While considering a value-based preference disaggregation method, we demonstrate the usefulness of the introduced method in a multi-purpose decision analysis exploiting a system of inequalities that models the Decision Maker’s (DM’s) preferences. We focus on the contexts where informative results can be obtained through the analysis of (in)feasibility of such systems. Specifically, we discuss how SD can be applied for verifying the consistency between the revealed and estimated preferences as well as for identifying the reasons of potential incoherence. Moreover, we employ the method for conducting robustness analysis, i.e., discovering a set of all compatible parameter values and verifying the stability of suggested recommendation in view of multiplicity of feasible solutions. In addition, we make clear its suitability for generating arguments about the validity of outcomes and the role of particular criteria. Overall, the SD approach confirms its usefulness in analyzing the relationships between the DM’s preference information, the compatible parameters of an assumed model, and the value-driven ranking procedure. We also discuss the favourable characteristics of the proposed method enhancing its suitability for use in Multiple Criteria Decision Aiding. These include possibility of studying any system of inequalities without an objective function, keeping in memory an entire process of transforming a system of inequalities that makes identification of potential incoherence straightforward, and avoiding the need for processing the inequalities contained in the basic system which is subsequently enriched with some hypothesis to be verified. The latter is substantial for saving time when a multitude of hypotheses need to be checked in the same conditions. The applicability and the advantages of using the proposed method in the decision aiding context are clearly exemplified on a numerical study.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300129",
    "keywords": [],
    "authors": [
      {
        "surname": "Kadziński",
        "given_name": "Miłosz"
      },
      {
        "surname": "Badura",
        "given_name": "Jan"
      },
      {
        "surname": "Figueira",
        "given_name": "José Rui"
      }
    ]
  },
  {
    "title": "SLDeep: Statement-level software defect prediction using deep-learning model on static code features",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113156",
    "abstract": "Software defect prediction (SDP) seeks to estimate fault-prone areas of the code to focus testing activities on more suspicious portions. Consequently, high-quality software is released with less time and effort. The current SDP techniques however work at coarse-grained units, such as a module or a class, putting some burden on the developers to locate the fault. To address this issue, we propose a new technique called as Statement-Level software defect prediction using Deep-learning model (SLDeep). The significance of SLDeep for intelligent and expert systems is that it demonstrates a novel use of deep-learning models to the solution of a practical problem faced by software developers. To reify our proposal, we defined a suite of 32 statement-level metrics, such as the number of binary and unary operators used in a statement. Then, we applied as learning model, long short-term memory (LSTM). We conducted experiments using 119,989 C/C++ programs within Code4Bench. The programs comprise 2,356,458 lines of code of which 292,064 lines are faulty. The benchmark comprises a diverse set of programs and versions, written by thousands of developers. Therefore, it tends to give a model that can be used for cross-project SDP. In the experiments, our trained model could successfully classify the unseen data (that is, fault-proneness of new statements) with average performance measures 0.979, 0.570, and 0.702 in terms of recall, precision, and accuracy, respectively. These experimental results suggest that SLDeep is effective for statement-level SDP. The impact of this work is twofold. Working at statement-level further alleviates developer's burden in pinpointing the fault locations. Second, cross-project feature of SLDeep helps defect prediction research become more industrially-viable.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419308735",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Code (set theory)",
      "Combinatorics",
      "Computer science",
      "Data mining",
      "Deep learning",
      "Geodesy",
      "Geography",
      "History",
      "Law",
      "Machine learning",
      "Mathematics",
      "Political science",
      "Precision and recall",
      "Programming language",
      "Set (abstract data type)",
      "Software",
      "Software bug",
      "Software development",
      "Software metric",
      "Software quality",
      "Source lines of code",
      "Statement (logic)",
      "Suite",
      "Unary operation"
    ],
    "authors": [
      {
        "surname": "Majd",
        "given_name": "Amirabbas"
      },
      {
        "surname": "Vahidi-Asl",
        "given_name": "Mojtaba"
      },
      {
        "surname": "Khalilian",
        "given_name": "Alireza"
      },
      {
        "surname": "Poorsarvi-Tehrani",
        "given_name": "Pooria"
      },
      {
        "surname": "Haghighi",
        "given_name": "Hassan"
      }
    ]
  },
  {
    "title": "A novel decision-making method using R-Norm concept and VIKOR approach under picture fuzzy environment",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113228",
    "abstract": "The Picture Fuzzy Sets (PFS) are well suitable to capture inconsistent, imprecise and uncertain information in multiple-criteria decision-making problems. This communication is intended to introduce one such information measure defined on PFSs called R-norm picture fuzzy information measure. Moreover, a new set of axioms is proposed as a criteria for picture fuzzy entropy. Besides establishing the validity of proposed R-norm picture fuzzy information measure, some of its major properties are also discussed. In application part, the proposed information measure is applied in predicting the outcome of elections in a poll bound country through opinion polls and to solve an investment problem.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300543",
    "keywords": [
      "Artificial intelligence",
      "Axiom",
      "Computer science",
      "Data mining",
      "Entropy (arrow of time)",
      "Fuzzy logic",
      "Fuzzy set",
      "Geometry",
      "Law",
      "Mathematical optimization",
      "Mathematics",
      "Measure (data warehouse)",
      "Norm (philosophy)",
      "Physics",
      "Political science",
      "Quantum mechanics",
      "VIKOR method"
    ],
    "authors": [
      {
        "surname": "Joshi",
        "given_name": "Rajesh"
      }
    ]
  },
  {
    "title": "Gender recognition using motion data from multiple smart devices",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113195",
    "abstract": "Using multiple smart devices, such as smartphone and smartwatch simultaneously, is becoming a popular life style with the popularity of wearables. This multiple-sensor setting provides new opportunities for enhanced user trait analysis via multiple data fusion. In this study, we explore the task of gender recognition by using motion data collected from multiple smart devices. Specifically, motion data are collected from smartphone and smart band simultaneously. Motion features are extracted from the collected motion data according to three aspects: time, frequency, and wavelet domains. We present a feature selection method considering the redundancies between motion features. Gender recognition is performed using four supervised learning methods. Experimental results demonstrate that using motion data collected from multiple smart devices can significantly improve the accuracy of gender recognition. Evaluation of our method on a dataset of 56 subjects shows that it can reach an accuracy of 98.7% compared with the accuracies of 93.7% and 88.2% when using smartphone and smart band individually.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742030021X",
    "keywords": [
      "Activity recognition",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Embedded system",
      "Feature selection",
      "Machine learning",
      "Mobile device",
      "Motion (physics)",
      "Motion sensors",
      "Operating system",
      "Pattern recognition (psychology)",
      "Sensor fusion",
      "Smartwatch",
      "Wearable computer",
      "Wearable technology"
    ],
    "authors": [
      {
        "surname": "Dong",
        "given_name": "Jianmin"
      },
      {
        "surname": "Du",
        "given_name": "Youtian"
      },
      {
        "surname": "Cai",
        "given_name": "Zhongmin"
      }
    ]
  },
  {
    "title": "Spatially weighted order binary pattern for color texture classification",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113167",
    "abstract": "In this paper, we propose a novel descriptor called spatially weighted order binary pattern (SWOBP) for color texture classification. The SWOBP descriptor not only encodes color order information in different channels but also encodes color order relationships in the spatial domain. To achieve these goals, we introduce a color gradient channel to complement the traditional color channels and explore a multi-channel color order pattern to jointly encode inter-channel features. Furthermore, we decompose local color differences into spatially weighted binary templates and use them to encode color order information in a local neighborhood. Finally, we aggregate all the encoded features into image histograms as texture descriptor. Experiments on five benchmark databases demonstrate that the proposed SWOBP descriptor achieves the state-of-the-art performance for color texture classification.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741741930884X",
    "keywords": [
      "Arithmetic",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Binary number",
      "Biochemistry",
      "Channel (broadcasting)",
      "Chemistry",
      "Color histogram",
      "Color image",
      "Complement (music)",
      "Complementation",
      "Computer network",
      "Computer science",
      "Computer vision",
      "ENCODE",
      "Gene",
      "Geodesy",
      "Geography",
      "Histogram",
      "Image (mathematics)",
      "Image processing",
      "Local binary patterns",
      "Local color",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Phenotype",
      "Texture (cosmology)"
    ],
    "authors": [
      {
        "surname": "Song",
        "given_name": "Tiecheng"
      },
      {
        "surname": "Feng",
        "given_name": "Jie"
      },
      {
        "surname": "Wang",
        "given_name": "Shiyan"
      },
      {
        "surname": "Xie",
        "given_name": "Yurui"
      }
    ]
  },
  {
    "title": "Combining a recursive approach via non-negative matrix factorization and Gini index sparsity to improve reliable detection of wheezing sounds",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113212",
    "abstract": "Auscultation constitutes a fast, non-invasive and low-cost tool widely used to diagnose respiratory diseases in most of the health centres. However, the acoustic training and expertise acquired by the physician is still crucial to provide a reliable diagnosis of the status of the lung. Each wrong diagnosis increases the risk to the health of patients and the costs associated with the treatment of the disease detected. A wheezing detection system can be useful to the physician to minimize the subjectivity of the interpretation of the breathing sounds, misdiagnoses due to stress and elucidating complex acoustic scenes (such as louder background noises). Highlight that the presence of wheeze sounds is one of the main indicators of respiratory disorders from airway obstructions. This work presents an expert and intelligent system to detect wheeze sounds based on a recursive algorithm that combines orthogonal non-negative matrix factorization (ONMF) and the sparsity descriptor Gini index. The recursive algorithm is composed of four stages. The first stage is based on ONMF modelling to factorize the spectral bases as dissimilar as possible. The second stage clusters the ONMF bases into two categories: wheezing and normal breath. The third stage proposes a novel stopping criterion that controls the loss of wheezing spectral content at the expense of removing normal breath content in the recursive algorithm. Finally, the fourth stage determines the patient’s condition to locate the temporal intervals in which wheeze sounds are active for unhealthy patients. Experimental results report that the proposed method: (i) provides the best detection performance compared to the recent state-of-the-art wheezing detection approaches, achieving the highest robustness in noisy environments; and (ii) reliably distinguishes the patient’s condition (healthy/unhealthy). The strengths of the proposed method are the following: (i) its unsupervised nature since it does not depend on any training stage to learn in advanced the sounds of interest (wheezing). This fact could make this method attractive to be used in clinical settings because wheezing sound databases are often unavailable; and (ii) the modelling of the spectral behaviour by means of a common feature, the sparsity, that represents the typically energy distributions shown by most of the wheeze and normal breath sounds.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300385",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Asthma",
      "Auscultation",
      "Computer science",
      "Eigenvalues and eigenvectors",
      "Factorization",
      "Internal medicine",
      "Matrix decomposition",
      "Medicine",
      "Non-negative matrix factorization",
      "Physics",
      "Quantum mechanics",
      "Radiology",
      "Respiratory sounds",
      "Speech recognition",
      "Wheeze"
    ],
    "authors": [
      {
        "surname": "De La Torre Cruz",
        "given_name": "Juan"
      },
      {
        "surname": "Cañadas Quesada",
        "given_name": "Francisco Jesús"
      },
      {
        "surname": "Carabias Orti",
        "given_name": "Julio José"
      },
      {
        "surname": "Vera Candeas",
        "given_name": "Pedro"
      },
      {
        "surname": "Ruiz Reyes",
        "given_name": "Nicolás"
      }
    ]
  },
  {
    "title": "Reducing distance computations for distance-based outliers",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113215",
    "abstract": "The mining task of outlier detection is essential in many expert and intelligent systems exploited in a wide range of applications, from intrusion detection to molecular biology. In some of such applications the ability to process large amounts of data in a very short time can be critical, for instance in intrusion and fraud detection. This paper explores a solution for the optimisation of an exact, unsupervised outlier detection method by avoiding unnecessary computations, and therefore reducing the running time and making the method usable also in settings where response times are crucial. In particular, we enhance the SolvingSet-based approach by using a mechanism that exploits the knowledge learned during the algorithm execution and avoids a large amount of distance computations. We demonstrate the strength of the proposed solution, named FastSolvingSet, through both theoretical and experimental analysis.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300415",
    "keywords": [
      "Algorithm",
      "Anomaly detection",
      "Artificial intelligence",
      "Composite material",
      "Computation",
      "Computer science",
      "Computer security",
      "Data mining",
      "Economics",
      "Exploit",
      "Intrusion detection system",
      "Machine learning",
      "Management",
      "Materials science",
      "Operating system",
      "Outlier",
      "Process (computing)",
      "Range (aeronautics)",
      "Task (project management)",
      "USable",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Angiulli",
        "given_name": "Fabrizio"
      },
      {
        "surname": "Basta",
        "given_name": "Stefano"
      },
      {
        "surname": "Lodi",
        "given_name": "Stefano"
      },
      {
        "surname": "Sartori",
        "given_name": "Claudio"
      }
    ]
  },
  {
    "title": "Learn#: A Novel incremental learning method for text classification",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113198",
    "abstract": "Deep learning is an effective method for extracting the underlying information in text. However, it performs better on closed datasets and is less effective in real-world scenarios for text classification. As the data is updated and the amount of data increases, the models need to be retrained, in what is often a long training process. Therefore, we propose a novel incremental learning strategy to solve these problems. Our method, called Learn#, includes four components: a Student model, a reinforcement learning (RL) module, a Teacher model, and a discriminator model. The Student models first extract the features from the texts, then the RL module filters the results of multiple Student models. After that, the Teacher model reclassifies the filtered results to obtain the final texts category. To avoid increasing the Student models unlimitedly as the number of samples increases, the discriminator model is used to filter the Student models based on their similarity. The Learn# method has the advantage of a shorter training time than the One-Time model, because it only needs to train a new Student model each time, without changing the existing Student models. Furthermore, it can also obtain feedback during application and tune the models parameters over time. Experiments on different datasets show that our method for text classification outperforms many traditional One-Time methods, reducing training time by nearly 80%.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300245",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Detector",
      "Discriminator",
      "Filter (signal processing)",
      "Image (mathematics)",
      "Machine learning",
      "Operating system",
      "Process (computing)",
      "Reinforcement learning",
      "Similarity (geometry)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Shan",
        "given_name": "Guangxu"
      },
      {
        "surname": "Xu",
        "given_name": "Shiyao"
      },
      {
        "surname": "Yang",
        "given_name": "Li"
      },
      {
        "surname": "Jia",
        "given_name": "Shengbin"
      },
      {
        "surname": "Xiang",
        "given_name": "Yang"
      }
    ]
  },
  {
    "title": "Swash: A collective personal name matching framework",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113115",
    "abstract": "Having a unique personal identifier is a prerequisite to run person-centric analytical queries and data mining tasks, such as fraud detection, expert finding, and credit scoring. Personal names are the most commonly used identifier of individuals in datasets; however, the name of a person may not be unique across the dataset's records, especially where data are integrated from various sources. Intelligent systems utilize name matching methods to identify different name representations of persons. The performance of previous name matching methods is inadequate since they solely consider name similarities and ignore dissimilarities. Unavailability of Part of Name (PON, e.g., first name and last name) is an important limitation of dissimilarity consideration. To address this issue, this paper proposes an unsupervised personal name matching framework, namely Swash. This framework can model the information gatherable from a name dataset into a layered Heterogeneous Information Network, which facilitates control over the learning process. Swash predicts PON tags using a self-trainable algorithm and then collectively clusters the name vertices on the network. Evaluations on three public bibliographic datasets (i.e., CiteSeer, ArXiv, and DBLP) recognize the significance of the proposed framework. The results showed that Swash outperformed F1 of the state-of-the-art method up to 38%.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419308322",
    "keywords": [
      "Computer science",
      "Data mining",
      "Geology",
      "Geomorphology",
      "Identifier",
      "Information retrieval",
      "Matching (statistics)",
      "Mathematics",
      "Operating system",
      "Process (computing)",
      "Programming language",
      "Statistics",
      "Swash",
      "Unique identifier",
      "WordNet",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Raeesi",
        "given_name": "Mohsen"
      },
      {
        "surname": "Asadpour",
        "given_name": "Masoud"
      },
      {
        "surname": "Shakery",
        "given_name": "Azadeh"
      }
    ]
  },
  {
    "title": "Modeling and solving cloud service purchasing in multi-cloud environments",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113165",
    "abstract": "Nowadays, the range of cloud services offered by cloud providers varies sharply and poses a challenge for cloud consumers aiming for the most cost-effective and compliant solutions, especially when operating highly scalable microservice architectures. A remedy can be provided by a cloud brokerage intelligent mechanism selecting cloud services across multiple clouds on behalf of consumers by considering individual goals and requirements. In this paper, we present the Cloud Service Purchasing Problem (CSPP) which aims to minimize costs while incorporating specific consumer and application task requirements. To solve this problem, we propose a mixed-integer programming model and two large neighborhood search approaches. Using well-defined problem instances incorporating data from real cloud providers, we conduct several computational experiments to evaluate the performance of the proposed algorithms. They exhibit a competitive performance providing solutions for all scenarios within short computational times. Finally, the significance of this problem and decision support approaches is analyzed by comparing usage patterns with respect to different nowadays cloud providers and offered virtual machine types for various scenarios.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419308826",
    "keywords": [
      "Algorithm",
      "Cloud computing",
      "Computer science",
      "Database",
      "Distributed computing",
      "Economics",
      "Economy",
      "Engineering",
      "Integer programming",
      "Operating system",
      "Operations management",
      "Purchasing",
      "Scalability",
      "Service (business)",
      "Service provider",
      "Systems engineering",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Heilig",
        "given_name": "Leonard"
      },
      {
        "surname": "Lalla-Ruiz",
        "given_name": "Eduardo"
      },
      {
        "surname": "Voß",
        "given_name": "Stefan"
      }
    ]
  },
  {
    "title": "Topological data analysis in investment decisions",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113222",
    "abstract": "This article explores the applications of Topological Data Analysis (TDA) in the finance field, especially addressing the primordial problem of asset allocation. Firstly, we build a rationale on why TDA can be a better alternative to traditional risk indicators such as standard deviation using real data sets. We apply Takens embedding theorem to reconstruct the time series of returns in a high dimensional space. We adopt the sliding window approach to draw the time-dependent point cloud data sets and associate a topological space with them. We then apply the persistent homology to discover the topological patterns that appear in the multidimensional time series. The temporal changes in the persistence landscapes, which are the real-valued functions that encode the persistence of topological patterns, are captured via Lp norm. The time series of the Lp norms shows that it is better at measuring the dynamics of returns than the standard deviation. Inspired by our findings, we explore an application of TDA in Enhanced Indexing (EI) that aims to build a portfolio of fewer assets than that in the index to outperform the latter. We propose a two-step procedure to accomplish this task. In step one, we utilize the Lp norms of the assets to propose a filtration technique of selecting a few assets from a larger pool of assets. In step two, we propose an optimization model to construct an optimal portfolio from the class of filtered assets for EI. To test the efficiency of this enhanced algorithm, experiments are carried out on ten data sets from financial markets across the globe. Our extensive empirical analysis exhibits that the proposed strategy delivers superior performance on several measures, including excess mean returns from the benchmark index and tail reward-risk ratios than some of the existing models of EI in the literature. The proposed filtering strategy is also noted to be beneficial for both risk-seeking and risk-averse investors.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300488",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Data mining",
      "Embedding",
      "Paleontology",
      "Persistent homology",
      "Series (stratigraphy)",
      "Topological data analysis"
    ],
    "authors": [
      {
        "surname": "Goel",
        "given_name": "Anubha"
      },
      {
        "surname": "Pasricha",
        "given_name": "Puneet"
      },
      {
        "surname": "Mehra",
        "given_name": "Aparna"
      }
    ]
  },
  {
    "title": "IOTA: Interlinking of heterogeneous multilingual open fiscal DaTA",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113135",
    "abstract": "Open budget data are among the most frequently published datasets of the open data ecosystem, intended to improve public administrations and government transparency. Unfortunately, the prospects of analysis across different open budget data remain limited due to schematic and linguistic differences. Budget and spending datasets are published together with descriptive classifications. Various public administrations typically publish the classifications and concepts in their regional languages. These classifications can be exploited to perform a more in-depth analysis, such as comparing similar items across different, cross-lingual datasets. However, in order to enable such analysis, a mapping across the multilingual classifications of datasets is required. In this paper, we present the framework for Interlinking of Heterogeneous Multilingual Open Fiscal DaTA (IOTA). IOTA makes use of machine translation followed by string similarities to map concepts across different datasets. To the best of our knowledge, IOTA is the first framework to offer scalable implementation of string similarity using distributed computing. The results demonstrate the applicability of the proposed multilingual matching, the scalability of the proposed framework, and an in-depth comparison of string similarity measures.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419308528",
    "keywords": [
      "Advertising",
      "Artificial intelligence",
      "Business",
      "Computer science",
      "Computer security",
      "Data mining",
      "Data science",
      "Database",
      "Electronic engineering",
      "Engineering",
      "Image (mathematics)",
      "Information retrieval",
      "Linked data",
      "Machine translation",
      "Open data",
      "Open government",
      "Physics",
      "Publication",
      "Quantum mechanics",
      "Scalability",
      "Schematic",
      "Semantic Web",
      "Similarity (geometry)",
      "String (physics)",
      "Transparency (behavior)",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Musyaffa",
        "given_name": "Fathoni A."
      },
      {
        "surname": "Vidal",
        "given_name": "Maria-Esther"
      },
      {
        "surname": "Orlandi",
        "given_name": "Fabrizio"
      },
      {
        "surname": "Lehmann",
        "given_name": "Jens"
      },
      {
        "surname": "Jabeen",
        "given_name": "Hajira"
      }
    ]
  },
  {
    "title": "Data-level information enhancement: Motion-patch-based Siamese Convolutional Neural Networks for human activity recognition in videos",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113203",
    "abstract": "Data augmentation is critical for deep learning-based human activity recognition (HAR) systems. However, conventional data augmentation methods, such as random-cropping, may generate bad samples that are unrelated to a particular activity (e.g. the background patches without saliency motion information). As a result, the random-cropping based data augmentation may affect negatively the overall performance of HAR systems. Humans, in turn, tend to pay more attention to motion information when recognizing activities. In this work, we attempt to enhance the motion information in HAR systems and mitigate the influence of bad samples through a Siamese architecture, termed as Motion-patch-based Siamese Convolutional Neural Network (MSCNN). The term motion patch is defined as a specific square region that includes critical motion information in the video. We propose a simple yet effective method for selecting those regions. To evaluate the proposed MSCNN, we conduct a number of experiments on the popular datasets UCF-101 and HMDB-51. The mathematical model and experimental results show that the proposed architecture is capable of enhancing the motion information and achieves comparable performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300294",
    "keywords": [
      "Activity recognition",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Deep learning",
      "Human motion",
      "Machine learning",
      "Motion (physics)",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Yujia"
      },
      {
        "surname": "Man Po",
        "given_name": "Lai"
      },
      {
        "surname": "Liu",
        "given_name": "Mengyang"
      },
      {
        "surname": "Ur Rehman",
        "given_name": "Yasar Abbas"
      },
      {
        "surname": "Ou",
        "given_name": "Weifeng"
      },
      {
        "surname": "Zhao",
        "given_name": "Yuzhi"
      }
    ]
  },
  {
    "title": "A fuzzy logic system for the home assessment of freezing of gait in subjects with Parkinsons disease",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113197",
    "abstract": "Gait dysfunctions are pathognomonic, progressive and, generally, continuous in Parkinson’s Disease (PD). The Freezing of Gait (FoG) is an episodic gait disorder involving up to 70% of people with PD, within 10 years of clinical onset, and associated with an increased risk for falls and immobility, which in turn, contributes to greater disability. Automatic and objective monitoring of FoG may help clinicians to understand and treat this phenomenon. In this work, a smartphone app for real-time FoG detection is presented and tested both in a laboratory setting and at patients’ home. The app implements a novel fuzzy logic algorithm that uses important spatio-temporal parameters of gait and is built according to clinical knowledge about FoG. The app includes a gait detection function and the evaluation of two important clinical statistics, i.e. FoG time and FoG number. The app FoG detection performance was assessed against clinicians evaluation and compared with the Moore–Bachlin FoG detection algorithm through ROC analysis, the calculation of confusion matrix, and FoG hit rate. The proposed algorithm achieved better results with respect to the Moore–Bachlin algorithm. Home reports were compared with respect to the FoG Questionnaire and laboratory reports; results indicated significant correlations for both FoG time and FoG number. The results confirm the reliability and accuracy of this app for FoG detection, supporting its wide use for diagnostic and therapeutic purposes.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300233",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Confusion",
      "Confusion matrix",
      "Fuzzy logic",
      "Gait",
      "Medicine",
      "Physical medicine and rehabilitation",
      "Physics",
      "Power (physics)",
      "Psychoanalysis",
      "Psychology",
      "Quantum mechanics",
      "Reliability (semiconductor)"
    ],
    "authors": [
      {
        "surname": "Pepa",
        "given_name": "Lucia"
      },
      {
        "surname": "Capecci",
        "given_name": "Marianna"
      },
      {
        "surname": "Andrenelli",
        "given_name": "Elisa"
      },
      {
        "surname": "Ciabattoni",
        "given_name": "Lucio"
      },
      {
        "surname": "Spalazzi",
        "given_name": "Luca"
      },
      {
        "surname": "Ceravolo",
        "given_name": "Maria Gabriella"
      }
    ]
  },
  {
    "title": "New Chebyshev distance measures for Pythagorean fuzzy sets with applications to multiple criteria decision analysis using an extended ELECTRE approach",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113164",
    "abstract": "This paper aims to propose novel Chebyshev distance measures for Pythagorean membership grades and establish their based elimination and choice translating reality method (ELECTRE) for addressing multiple criteria decision-making problems under uncertainty of Pythagorean fuzziness. Pythagorean fuzzy (PF) sets have a significant effect on fuzzy modeling for intelligent informatics and decision support because the degrees of membership, non-membership, and indeterminacy, strength of commitment, and direction of commitment featured by PF information are extended for a wider coverage of information span. The theory of PF sets is a powerful tool in dealing with imprecise and ambiguous evaluations for realistic problems and modeling intelligent decision making for complex systems. This paper focuses on both theory and applications of the Chebyshev metric for PF contexts, and special attention is devoted to the theoretical development of Chebyshev distances in connection with Pythagorean membership grades based on various types of representations. To surmount the difficulties confronted by the existing measures, such as low comprehensivity, incomparability in scaling, ignorance of square degrees in metric specification, double weighting, and inappropriate normalization, this paper makes a comprehensive comparison to validate the effectiveness and superiority of the proposed Chebyshev distance measures. To support decision making within complicated PF environments, this paper develops an extended ELECTRE approach based on the Chebyshev distance measure to conduct multiple criteria decision analysis involving PF information for determining partial and complete rankings of candidate alternatives. In particular, this paper constructs novel Chebyshev metric-based preference functions depending on the individual characteristics of the criteria. The developed PF ELECTRE approach leads to using all the information that characterizes a PF set via the concepts of the scalar function and various Chebyshev metric-based comparison indices, such as (net) concordance indices, (net) discordance indices, and the overall precedence index. Practical applications with a comparative analysis in the field of bridge-superstructure construction are conducted to examine the usefulness and advantages of the proposed methodology in the real world.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419308802",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Chebyshev filter",
      "Chebyshev polynomials",
      "Computer science",
      "Computer vision",
      "Data mining",
      "ELECTRE",
      "Economics",
      "Fuzzy logic",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Measure (data warehouse)",
      "Metric (unit)",
      "Multiple-criteria decision analysis",
      "Operations management"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Ting-Yu"
      }
    ]
  },
  {
    "title": "A hybrid algorithm based on particle filter and genetic algorithm for target tracking",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113188",
    "abstract": "The particle filter (PF) is an influential instrument for visual tracking; it relies on the Monte Carlo Chain Framework and Bayesian probability that is of tremendous importance for smart monitoring systems. The current study introduces a particle filter based upon genetic resampling. In the suggested method called Reduced Particle Filter based upon Genetic Algorithm (RPFGA), particles with the highest weights are chosen and go through evolution using a GA in the resampling phase of PF algorithm. Moreover, this study aims to introduce the ideas of marking (marking the target by user (observer) in the first frame of a video sequence) and decreasing image size. Applying both ideas leads to reduced number of particles, the processing time of each frame, and the total tracking time. Additionally, the performance of the offered RPFGA method to tackle the occlusion problem is enhanced by the marking idea. According to the results obtained in challenges, such as Occlusions (OCC), deformation (DEF), low resolution (LR), scale variations(SV), Fast Motions (FM), In-Plane Rotation (IPR), Out-Of-Plane Rotation (OPR), Motion Blur (MB), Illumination Variation (IV) and color similarity between the target and the background, and regarding precision and tracking time, the recommended hybrid approach only with a few particles overtakes the generic particle filter, Particle Swarm Optimization particle filter (PSO-PF) and the particle filter based upon improved cuckoo search (ICS-PF). The suggested method can be applied for real time video objects tracking.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300142",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Auxiliary particle filter",
      "Computer science",
      "Computer vision",
      "Ensemble Kalman filter",
      "Extended Kalman filter",
      "Filter (signal processing)",
      "Frame (networking)",
      "Kalman filter",
      "Monte Carlo localization",
      "Particle filter",
      "Particle swarm optimization",
      "Pedagogy",
      "Psychology",
      "Resampling",
      "Telecommunications",
      "Tracking (education)"
    ],
    "authors": [
      {
        "surname": "Moghaddasi",
        "given_name": "Somayyeh Sadegh"
      },
      {
        "surname": "Faraji",
        "given_name": "Neda"
      }
    ]
  },
  {
    "title": "Dynamic portfolio optimization based on grey relational analysis approach",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113207",
    "abstract": "This research focuses on the possibilities of Grey Relational Analysis (GRA) as a tool in portfolio selection. The main goal of the paper was to empirically evaluate possibilities of dynamic portfolio (re)structuring with respect to the results from ranking stocks when using the GRA approach. The contributions of this research include: utilizing the GRA in accordance with finance and investor's utility theory; performing a battery of comparisons of investment strategies which are, again, based on finance theory. Results from the analysis indicate that there exist possibilities of exploiting the advantages of GRA methodology in order to form stock portfolios. This is shown by comparing the portfolio performance, which has been calculated based on several important measures. This performance indicates that based on the investor's preferences, certain gains can be achieved (both in terms of risk and return). Thus, the importance of this research is found in combining the GRA approach as a tool for achieving widely known investment goals more efficiently, in quicker time and even with the inclusion of transaction costs.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300336",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Database",
      "Database transaction",
      "Economics",
      "Engineering",
      "Finance",
      "Grey relational analysis",
      "Investment strategy",
      "Market liquidity",
      "Mathematical economics",
      "Mathematics",
      "Mechanical engineering",
      "Microeconomics",
      "Modern portfolio theory",
      "Operations research",
      "Order (exchange)",
      "Portfolio",
      "Portfolio optimization",
      "Ranking (information retrieval)",
      "Selection (genetic algorithm)",
      "Stock (firearms)",
      "Structuring",
      "Technical analysis",
      "Transaction cost"
    ],
    "authors": [
      {
        "surname": "Škrinjarić",
        "given_name": "Tihana"
      }
    ]
  },
  {
    "title": "Evolution of communities in dynamic social networks: An efficient map-based approach",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113221",
    "abstract": "The expanded domain of expert system applications has risen the impact of modeling and analysis of community evolution in social networks as an important part of the decision-making process. Social networks are time-variant systems, evolving through entities joining or leaving networks and establishing or terminating relationships. In this article, we study evolution of social networks at the level of community structure, by tracking different transformations of communities over time. Upon experimentation, we observed that a considerable portion of community evolution is partial events such as partial merge. Therefore, we define a broader set of community evolution to include partial events. Furthermore, we introduce ICEM, a novel method for Identification of Community Evolution by Mapping. ICEM determines community evolution by tracking community members, implemented with a hash-map. ICEM maps each member to a (t, c) pair, specifying it is last observed in time window t and community c. We evaluated our proposed approach with seventeen publicly available social network datasets and compared its performance against other well-known methods in the literature. Our experimental results indicated the performance superiority of our proposed solution. Additionally, we conducted separate comprehensive experiments using three community detection algorithms to highlight the effect of choosing different community discovery methods on community evolution results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300476",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Combinatorics",
      "Community structure",
      "Computer science",
      "Computer security",
      "Data mining",
      "Data science",
      "Domain (mathematical analysis)",
      "Hash function",
      "Identification (biology)",
      "Information retrieval",
      "Mathematical analysis",
      "Mathematics",
      "Merge (version control)",
      "Operating system",
      "Process (computing)",
      "Programming language",
      "Set (abstract data type)",
      "Social media",
      "Social network (sociolinguistics)",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Kadkhoda Mohammadmosaferi",
        "given_name": "Kaveh"
      },
      {
        "surname": "Naderi",
        "given_name": "Hassan"
      }
    ]
  },
  {
    "title": "A fusion method based on Deep Learning and Case-Based Reasoning which improves the resulting medical image segmentations",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113200",
    "abstract": "The fusion of multiple segmentations of different biological structures is inevitable in the case where each structure has been segmented individually for performance reasons. However, when aggregating these structures for a final segmentation, conflicting pixels may appear. These conflicts can be solved by artificial intelligence techniques. Our system, integrated into the SAIAD project, carries out the fusion of deformed kidneys and nephroblastoma segmentations using the combination of Deep Learning and Case-Based Reasoning. The performances of our method were evaluated on 9 patients affected by nephroblastoma, and compared with other AI and non-AI methods adapted from the literature. The results demonstrate its effectiveness in resolving the conflicting pixels and its ability to improve the resulting segmentations.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300269",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Fusion",
      "Image (mathematics)",
      "Image segmentation",
      "Linguistics",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pixel",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Corbat",
        "given_name": "Lisa"
      },
      {
        "surname": "Nauval",
        "given_name": "Mohammad"
      },
      {
        "surname": "Henriet",
        "given_name": "Julien"
      },
      {
        "surname": "Lapayre",
        "given_name": "Jean-Christophe"
      }
    ]
  },
  {
    "title": "Z-ERM DEA integrated approach for evaluation of banks & financial institutes in stock exchange",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113218",
    "abstract": "The purpose of this study is to evaluate efficiency and effectiveness of banks, financial and credit institutes active in stock and super-stock exchange organizations using new Fuzzy DEA model which is the modified, improved and enhanced version of Russell's measurement model. In order to make it more efficient, we used the Z-numbers theory through which the calculations of uncertainty in the decision-making process can be showed more accurately. To do so, the literature of this industry and financial data of the research companies during 2012–2017 were initially overviewed and 16 indicators (indices/indexes) and the main criteria related to their performance evaluation were selected. Afterward, the productivity of banks and financial and credit institutions in different levels of α − cut were specified and compared using integrated approach of ERM and calculations of Z-numbers as well as considering opinions provided by the financial experts with possibility of concurrent evaluation of the efficiency and effectiveness in a model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300440",
    "keywords": [
      "Actuarial science",
      "Artificial intelligence",
      "Business",
      "Computer science",
      "Data envelopment analysis",
      "Econometrics",
      "Economics",
      "Engineering",
      "Finance",
      "Fuzzy logic",
      "Mathematics",
      "Mechanical engineering",
      "Order (exchange)",
      "Statistics",
      "Stock (firearms)",
      "Stock exchange"
    ],
    "authors": [
      {
        "surname": "Mohtashami",
        "given_name": "Ali"
      },
      {
        "surname": "Ghiasvand",
        "given_name": "Bahram Mohammadkhani"
      }
    ]
  },
  {
    "title": "Data integrity assessment for maritime anomaly detection",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113219",
    "abstract": "In the last years, systems broadcasting mobility data underwent a rise in cyberthreats, jeopardising their normal use and putting both users and their environment at risk. In this respect, anomaly detection methods are needed to ensure an assessment of such systems. In this article, we propose a rule-based method for data integrity assessment, with rules built from the system technical specifications and by domain experts, and formalised by a logic-based framework, resulting in the triggering of situation-specific alerts. A use case is proposed on the Automatic Identification System, a worldwide localisation system for vessels, based on its poor level of security which allows errors, falsifications and spoofing scenarios. The discovery of abnormal reporting cases aims to assist marine traffic surveillance, preserve the human life at sea and mitigate hazardous behaviours against ports, off-shore structures and the environment.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300452",
    "keywords": [
      "Anomaly detection",
      "Biology",
      "Botany",
      "Computer science",
      "Computer security",
      "Data mining",
      "Domain (mathematical analysis)",
      "Engineering",
      "Hazardous waste",
      "Identification (biology)",
      "Mathematical analysis",
      "Mathematics",
      "Medicine",
      "Risk analysis (engineering)",
      "Spoofing attack",
      "Waste management"
    ],
    "authors": [
      {
        "surname": "Iphar",
        "given_name": "Clément"
      },
      {
        "surname": "Ray",
        "given_name": "Cyril"
      },
      {
        "surname": "Napoli",
        "given_name": "Aldo"
      }
    ]
  },
  {
    "title": "Driver fatigue transition prediction in highly automated driving using physiological features",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113204",
    "abstract": "One of the main causes of traffic accidents is driver fatigue due to monotonous driving, sleep deprivation, boredom, or a combination of these. Thus, fatigue detection systems have been proposed to alert drivers. However, how early driver fatigue can be detected often determines the effectiveness of the system. Traditional approaches aim to detect driver fatigue in real time, which can be too late in many critical situations, such as the takeover transition period in highly automated driving. Therefore, in this research, we aim to predict the driver's transition from non-fatigue to fatigue in highly automated driving using physiological features. First, we capitalized on PERCLOS (i.e., PERcent of time the eyelids CLOSure) as the ground truth of driver fatigue. Next, we selected the most important physiological features to predict driver fatigue proactively. Finally, using these critical physiological features, we built prediction models that were able to predict the fatigue transition at least 13.8 s ahead of time using a technique called nonlinear autoregressive exogenous network. The accuracy of fatigue transition prediction was promising for highly automated driving (F 1 measure = 97.4% and 99.1% for two types of models), which demonstrated the potential of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300300",
    "keywords": [
      "Artificial intelligence",
      "Autoregressive model",
      "Biochemistry",
      "Boredom",
      "Chemistry",
      "Circadian rhythm",
      "Computer science",
      "Econometrics",
      "Gene",
      "Mathematics",
      "Neuroscience",
      "Psychology",
      "Simulation",
      "Sleep deprivation",
      "Social psychology",
      "Transition (genetics)"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Feng"
      },
      {
        "surname": "Alsaid",
        "given_name": "Areen"
      },
      {
        "surname": "Blommer",
        "given_name": "Mike"
      },
      {
        "surname": "Curry",
        "given_name": "Reates"
      },
      {
        "surname": "Swaminathan",
        "given_name": "Radhakrishnan"
      },
      {
        "surname": "Kochhar",
        "given_name": "Dev"
      },
      {
        "surname": "Talamonti",
        "given_name": "Walter"
      },
      {
        "surname": "Tijerina",
        "given_name": "Louis"
      },
      {
        "surname": "Lei",
        "given_name": "Baiying"
      }
    ]
  },
  {
    "title": "Multiclass imbalanced learning with one-versus-one decomposition and spectral clustering",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113152",
    "abstract": "In many real-world applications, an algorithm needs to learn multiclass classification models from data with imbalanced class distributions. Multiclass imbalanced learning is currently receiving increased attention from researchers. In contrast to traditional imbalanced learning on binary datasets, multiclass imbalanced learning faces great challenges from the variety of changes in the class distributions as well as the inadequate performance of multiclass classification algorithms. In this paper, we propose a novel data preprocessing-based method to solve this problem. The proposed method combines a one-versus-one (OVO) decomposition of class pairs and a spectral clustering technique. This method first decomposes a multiclass dataset into several binary-class datasets. Then, it uses spectral clustering to divide the minority classes of binary-class subsets into subspaces and oversamples them according to the characteristics of the data. Sampling based on spectral clustering takes into account the distribution of the data and effectively avoids oversampling outliers. After the data approximately reaches the equilibrium point, multiclass classifiers can be trained from these rebalanced data. We compared the proposed method with five state-of-the-art multiclass imbalanced learning methods on seven multiclass datasets, using multiclass area under the ROC curve (MAUC), the precision of minor classes (Pmin ) and the average precision of all classes (Pavg ) as the performance metrics. The experimental results show that our proposed method has the best overall performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419308693",
    "keywords": [
      "Arithmetic",
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Binary classification",
      "Binary number",
      "Class (philosophy)",
      "Cluster analysis",
      "Computer network",
      "Computer science",
      "Data mining",
      "Machine learning",
      "Mathematics",
      "Multiclass classification",
      "Outlier",
      "Oversampling",
      "Pattern recognition (psychology)",
      "Preprocessor",
      "Spectral clustering",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Qianmu"
      },
      {
        "surname": "Song",
        "given_name": "Yanjun"
      },
      {
        "surname": "Zhang",
        "given_name": "Jing"
      },
      {
        "surname": "Sheng",
        "given_name": "Victor S."
      }
    ]
  },
  {
    "title": "SHyFTOO, an object-oriented Monte Carlo simulation library for the modeling of Stochastic Hybrid Fault Tree Automaton",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113139",
    "abstract": "Dependability assessment is a crucial activity to ensure the correct operation of complex systems. The output of dependability assessment activities include the quantification of reliability, availability, maintenance and safety related metrics. These metrics can assist in the identification of the system weak points or in the conception of mitigation strategies to increase the system dependability level. The development of advanced computer-aided methodologies to support dependability assessment activities is essential to automate and reduce the efforts implied by this process and similarly, the development of accurate dependability assessment methods is very important to increase the quality of the results. In this context, it is possible to identify different contributions that improve the dependability assessment through general-purpose modeling methodologies. However, existing solutions are ad-hoc applications specified with low-level stochastic formalisms and this complicates their adoption in the industry. Accordingly, this paper presents Stochastic Hybrid Fault Tree Automaton (SHyFTA) based simulation algorithm that allows the accurate dependability analysis of repairable multi-state systems. SHyFTA integrates the stochastic and deterministic operation of the system under study as well as their interactions. The algorithm is formalized through an object-oriented software architecture, which is developed as a software library for the modeling and simulation of repairable SHyFTA models. Following the proposed architecture, a Matlab® implementation of this library, SHyFTOO, has been developed and validated with a thorough test campaign. In order to provide a guideline to the end-users and show the potential of the SHyFTOO library, the case study of a feed-water pumping system is implemented in detail and it is used to evaluate different preventive maintenance policies. The SHyFTOO library can open the way to further investigations that address the interactions between the failure behavior and the functional operation of a system and their combined effect on system dependability.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419308565",
    "keywords": [
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Dependability",
      "Distributed computing",
      "Engineering",
      "Fault tree analysis",
      "Paleontology",
      "Petri net",
      "Reliability block diagram",
      "Reliability engineering",
      "Software engineering",
      "Stochastic Petri net"
    ],
    "authors": [
      {
        "surname": "Chiacchio",
        "given_name": "Ferdinando"
      },
      {
        "surname": "Aizpurua",
        "given_name": "Jose Ignacio"
      },
      {
        "surname": "Compagno",
        "given_name": "Lucio"
      },
      {
        "surname": "D'Urso",
        "given_name": "Diego"
      }
    ]
  },
  {
    "title": "An improved exact algorithm for a territory design problem with p-center-based dispersion minimization",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113150",
    "abstract": "Territory design deals with the discrete assignment of geographical units into territories with restrictions defined by planning criteria. We propose an exact solution method based on an integer programming model with the objective of minimizing a p-center dispersion measure. The solution approach is an iterative algorithm that uses different subproblems to validate if, for given values of the objective function of the original problem, it is possible to find feasible solutions with at most p territories. This change allows testing various candidate distance values as lower bounds on the optimal solution of the original problem. The aim is to improve these lower bounds at each iteration as we add the necessary constraints to reach a feasible solution. The proposed algorithm performs significantly faster than the best-known exact solution method for this model. Tests for both methodologies were done using a new set of instances with up to 300 nodes.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741741930867X",
    "keywords": [
      "Algorithm",
      "Biology",
      "Center (category theory)",
      "Chemistry",
      "Computer science",
      "Crystallography",
      "Data mining",
      "Evolutionary biology",
      "Exact solutions in general relativity",
      "Function (biology)",
      "Integer (computer science)",
      "Integer programming",
      "Iterative method",
      "Linear programming",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Measure (data warehouse)",
      "Minification",
      "Programming language",
      "Set (abstract data type)",
      "Solution set"
    ],
    "authors": [
      {
        "surname": "Sandoval",
        "given_name": "M. Gabriela"
      },
      {
        "surname": "Díaz",
        "given_name": "Juan A."
      },
      {
        "surname": "Ríos-Mercado",
        "given_name": "Roger Z."
      }
    ]
  },
  {
    "title": "Multiplex community detection in complex networks using an evolutionary approach",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113184",
    "abstract": "Multiplex networks are the general representative of complex systems composed of distinct interactions between the same entities on multiple layers. Community detection in the multiplex networks is the problem of finding a shared structure under all layers, which combines the information of the entire network. Most of the existing methods for community detection in the single-layer networks cannot be well applied to detect shared communities in multiplex networks. In this paper, we employ a multi-objective evolutionary approach, namely Multi-Objective Evolutionary Algorithm based on Decomposition with Tabu Search (MOEA/D-TS), to detect shared communities in multiplex networks. Also, we have improved the MOEA/D-TS using a social networks analysis measure named Clustering Coefficient (CC) in terms of the generation of the initial population. This hybrid algorithm employs the parallel computing capacity of the Multi-Objective Evolutionary Algorithm based on Decomposition (MOEA/D) along with the neighborhood search authority of Tabu Search (TS) for discovering Pareto optimal solutions. Extensive experiments on a variety of single-layer and multiplex real-world data sets show the superiority of the proposed method in comparison to state-of-the-art algorithms and its capability for producing improved results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300105",
    "keywords": [
      "Artificial intelligence",
      "Bioinformatics",
      "Biology",
      "Chemistry",
      "Cluster analysis",
      "Complex network",
      "Computer science",
      "Data mining",
      "Demography",
      "Evolutionary algorithm",
      "Layer (electronics)",
      "Multiplex",
      "Organic chemistry",
      "Population",
      "Sociology",
      "Tabu search",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Karimi",
        "given_name": "Fatemeh"
      },
      {
        "surname": "Lotfi",
        "given_name": "Shahriar"
      },
      {
        "surname": "Izadkhah",
        "given_name": "Habib"
      }
    ]
  },
  {
    "title": "DPDnet: A robust people detector using deep learning with an overhead depth camera",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113168",
    "abstract": "This paper proposes a deep learning-based method to detect multiple people from a single overhead depth image with high precision. Our neural network, called DPDnet, is composed by two fully-convolutional encoder-decoder blocks built with residual layers. The main block takes a depth image as input and generates a pixel-wise confidence map, where each detected person in the image is represented by a Gaussian-like distribution, The refinement block combines the depth image and the output from the main block, to refine the confidence map. Both blocks are simultaneously trained end-to-end using depth images and ground truth head position labels. The paper provides a rigorous experimental comparison with some of the best methods of the state-of-the-art, being exhaustively evaluated in different publicly available datasets. DPDnet proves to outperform all the evaluated methods with statistically significant differences, and with accuracies that exceed 99%. The system was trained on one of the datasets (generated by the authors and available to the scientific community) and evaluated in the others without retraining, proving also to achieve high accuracy with varying datasets and experimental conditions. Additionally, we made a comparison of our proposal with other CNN-based alternatives that have been very recently proposed in the literature, obtaining again very high performance. Finally, the computational complexity of our proposal is shown to be independent of the number of users in the scene and runs in real time using conventional GPUs.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419308851",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Deep learning",
      "Encoder",
      "Gaussian",
      "Geometry",
      "Ground truth",
      "Image (mathematics)",
      "Mathematics",
      "Operating system",
      "Overhead (engineering)",
      "Pattern recognition (psychology)",
      "Physics",
      "Pixel",
      "Quantum mechanics",
      "Residual"
    ],
    "authors": [
      {
        "surname": "Fuentes-Jimenez",
        "given_name": "David"
      },
      {
        "surname": "Martin-Lopez",
        "given_name": "Roberto"
      },
      {
        "surname": "Losada-Gutierrez",
        "given_name": "Cristina"
      },
      {
        "surname": "Casillas-Perez",
        "given_name": "David"
      },
      {
        "surname": "Macias-Guarasa",
        "given_name": "Javier"
      },
      {
        "surname": "Luna",
        "given_name": "Carlos A."
      },
      {
        "surname": "Pizarro",
        "given_name": "Daniel"
      }
    ]
  },
  {
    "title": "ReDMark: Framework for residual diffusion watermarking based on deep networks",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113157",
    "abstract": "Due to the rapid growth of machine learning tools and specifically deep networks in various computer vision and image processing areas, applications of Convolutional Neural Networks for watermarking have recently emerged. In this paper, we propose a deep end-to-end diffusion watermarking framework (ReDMark) which can learn a new watermarking algorithm in any desired transform space. The framework is composed of two Fully Convolutional Neural Networks with residual structure which handle embedding and extraction operations in real-time. The whole deep network is trained end-to-end to conduct a blind secure watermarking. The proposed framework simulates various attacks as a differentiable network layer to facilitate end-to-end training. The watermark data is diffused in a relatively wide area of the image to enhance security and robustness of the algorithm. Comparative results versus recent state-of-the-art researches highlight the superiority of the proposed framework in terms of imperceptibility, robustness and speed. The source codes of the proposed framework are publicly available at Github 1 1 https://github.com/MahdiShAhmadi/ReDMark/tree/master/ .",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419308759",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Convolutional neural network",
      "Data mining",
      "Deep learning",
      "Differentiable function",
      "Digital watermarking",
      "Embedding",
      "Gene",
      "Image (mathematics)",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Residual",
      "Robustness (evolution)",
      "Watermark"
    ],
    "authors": [
      {
        "surname": "Ahmadi",
        "given_name": "Mahdi"
      },
      {
        "surname": "Norouzi",
        "given_name": "Alireza"
      },
      {
        "surname": "Karimi",
        "given_name": "Nader"
      },
      {
        "surname": "Samavi",
        "given_name": "Shadrokh"
      },
      {
        "surname": "Emami",
        "given_name": "Ali"
      }
    ]
  },
  {
    "title": "Classification of user competency levels using EEG and convolutional neural network in 3D modelling application",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113202",
    "abstract": "Competency classification is one of the main challenging tasks for the development of state-of-the-art next generation computer-aided design (CAD) system. To develop a futuristic system that can accommodate the lack of competency, the system needs to adapt to the competency level of the user. To solve this problem, we have presented a deep convolutional neural network (CNN) model that uses the Electroencephalography (EEG) of the user to classify the level of competency in 3D modeling task. The five competency levels were defined based on the task completion time, final 3D model rating and previous modeling experience. This is the first study that classifies user competency and employs the CNN model for the analysis of EEG signals in the design application. In this work, a 14-layer deep CNN model was implemented to classify competency into five different levels. The proposed technique achieved an accuracy, specificity, and sensitivity of > 88%, > 90% and > 70% respectively with 5-fold cross-validation. The results showed the applicability of a CNN model to classify the user competency and can be used as a first step in developing state-of-the-art adaptive 3D modeling systems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300282",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Convolutional neural network",
      "Electroencephalography",
      "Human–computer interaction",
      "Machine learning",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Psychology"
    ],
    "authors": [
      {
        "surname": "Baig",
        "given_name": "Muhammad Zeeshan"
      },
      {
        "surname": "Kavakli",
        "given_name": "Manolya"
      }
    ]
  },
  {
    "title": "Fingerprint indexing for wrinkled fingertips immersed in liquids",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113153",
    "abstract": "Wrinkled fingerprint recognition has been a challenging problem because of changing the position of fingertip features. This change significantly degrades the fingerprint recognition accuracy. Contactless dry three-dimensional (3D) fingerprints have the advantages of reducing the position change of fingertip features presented in both contact-based and contactless dry 2D fingerprints. Unfortunately, in contrast to the contactless dry 3D fingerprints, the position of features in the contactless wrinkled 3D fingerprints will be changed. Furthermore, identifying a fingerprint in a voluminous database is another challenge. With an increasing the number of individuals and inserting their fingerprints into the enrollment database, the cost of identification will increase and can become critical. Fingerprint indexing is a prominent method to reduce the response time of a probe in a large-scale database. The indexing approaches powerfully boost the recognition efficiency of dry fingerprints, but the accuracy of wrinkled fingerprints cannot be guaranteed. Moreover, previous indexing approaches only focused on dry 2D fingerprints and did not consider 3D fingerprints and the problems of wrinkled fingerprints. This paper proposes a 3D fingerprint reconstruction technique based on multi-view contactless wrinkled fingerprint images. In the proposed system, we use two cameras to acquire the frontal image. A dual camera can get more details of an image and is useful to acquire wrinkled fingerprints. In this paper, we propose a rectification technique for wrinkled 2D and 3D fingerprints as well. This paper also proposes a wrinkled fingerprint indexing approach to overcome the problems of wrinkled fingerprints. Our proposal employs minutiae quadruplets, ellipse properties, and a k-means clustering to index and retrieve fingerprints. Finally, a voting technique based on minutiae quality is proposed. Experimental results validate our approach and demonstrate the effectiveness of proposed method. Some new outlooks on the topics of 3D fingerprint acquisition, rectification and 3D reconstruction of wrinkled fingerprints, and fingerprint indexing are revealed. The main impact of this work is that more researchers will be attracted in related areas for wrinkled fingerprint recognition. The main significance is that the diversity of expert systems should be well promoted in addressing reconstruction of 3D shape and rectification of deformed fingerprints.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741741930870X",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Computer science",
      "Computer vision",
      "Economics",
      "Finance",
      "Fingerprint (computing)",
      "Fingerprint recognition",
      "Identification (biology)",
      "Liquid-crystal display",
      "Minutiae",
      "Operating system",
      "Pattern recognition (psychology)",
      "Position (finance)",
      "Search engine indexing",
      "Viewing angle"
    ],
    "authors": [
      {
        "surname": "Khodadoust",
        "given_name": "Javad"
      },
      {
        "surname": "Khodadoust",
        "given_name": "Ali Mohammad"
      },
      {
        "surname": "Mirkamali",
        "given_name": "Seyed Saeid"
      },
      {
        "surname": "Ayat",
        "given_name": "Saeed"
      }
    ]
  },
  {
    "title": "Hyper-heuristic method for multilevel thresholding image segmentation",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113201",
    "abstract": "In digital image processing, one of the most relevant tasks is to classify pixels depending on their intensity level. To perform this process there exist different traditional methods as Otsu or Kapur, such methods are used to compute the thresholds that divide the histogram of the image into different groups. These methods are easy to implement for a single threshold; however, the computational effort is affected when more thresholds are required. Therefore, different meta-heuristic based approaches have been proposed, but each of them has its properties and limitations. So, this paper introduces an alternative concept to the image segmentation which is called hyper-heuristic that at each iteration determines the optimal execution sequence of meta-heuristic algorithms that provides the optimal thresholds. The proposed method consists of two layers, in the first layer, the genetic algorithm (GA) is used to determine the execution sequence of the meta-heuristic algorithms. While the second layer contains the set of four meta-heuristic algorithms that executed in a specific order, assigned by the current solution of GA, to update the threshold population. In order to evaluate the performance of the proposed approach, it has been tested over a set of benchmark images and the results provide a good performance in terms of quality of segmentation. Moreover, experimental comparisons support that the proposed hyper-heuristic is able to find more accurate solutions than other algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300270",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Genetic algorithm",
      "Geodesy",
      "Geography",
      "Heuristic",
      "Histogram",
      "Image (mathematics)",
      "Image segmentation",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Programming language",
      "Segmentation",
      "Set (abstract data type)",
      "Thresholding"
    ],
    "authors": [
      {
        "surname": "Elaziz",
        "given_name": "Mohamed Abd"
      },
      {
        "surname": "Ewees",
        "given_name": "Ahmed A."
      },
      {
        "surname": "Oliva",
        "given_name": "Diego"
      }
    ]
  },
  {
    "title": "A stochastic approximation approach to spatio-temporal anchorage planning with multiple objectives",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113170",
    "abstract": "Globalization and subsequent increase in seaborne trade have necessitated efficient planning and management of world’s anchorage areas. These areas serve as a temporary stay area for commercial vessels for various reasons such as waiting for passage or port, fuel services, and bad weather conditions. The research question we consider in this study is how to place these vessels inside a polygon-shaped anchorage area in a dynamic fashion as they arrive and depart, which seems to be the first of its kind in the literature. We specifically take into account the objectives of (1) anchorage area utilization, (2) risk of vessel collisions, and (3) fuel consumption performance. These three objectives define our objective function in a weighted sum scheme. We present a spatio-temporal methodology for this multi-objective anchorage planning problem where we use Monte Carlo simulations to measure the effect of any particular combination of planning metrics (measured in real time for an incoming vessel) on the objective function (measured in steady state). We resort to the Simultaneous Perturbation Stochastic Approximation (SPSA) algorithm for identifying the linear combination of the planning metrics that optimizes the objective function. We present computational experiments on a major Istanbul Straight anchorage, which is one of the busiest in the world, as well as synthetic anchorages. Our results indicate that our methodology significantly outperforms comparable algorithms in the literature for daily anchorage planning. For the Istanbul Straight anchorage, for instance, reduction in risk was 42% whereas reduction in fuel costs was 45% when compared the best of the current state-of-the-art methods. Our methodology can be utilized within a planning expert system that intelligently places incoming vessels inside the anchorage so as to optimize multiple strategic goals. Given the flexibility of our approach in terms of the planning objectives, it can easily be adapted to more general variants of multi-objective spatio-temporal planning problems where certain objects need to be dynamically placed inside two or even-three dimensional spaces in an intelligent manner.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419308875",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Mathematical optimization",
      "Mathematics"
    ],
    "authors": [
      {
        "surname": "Madadi",
        "given_name": "Bahman"
      },
      {
        "surname": "Aksakalli",
        "given_name": "Vural"
      }
    ]
  },
  {
    "title": "Robust FCM clustering algorithm with combined spatial constraint and membership matrix local information for brain MRI segmentation",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113159",
    "abstract": "This paper presents a robust fuzzy clustering algorithm for the segmentation of brain tissues in magnetic resonance imaging (MRI). The proposed method incorporates context-aware spatial constraint and local information of the membership matrix into the fuzzy c-means (FCM) clustering algorithm. Based upon this approach, an FCM clustering algorithm with joint spatial constraint and membership matrix local information (FCMS-MLI) for brain MRI segmentation is presented, which is more robust against noise and other artifacts. The proposed spatial constraint considers both local spatial and gray-level information adaptively, and to the best of the authors’ knowledge for the first time, the membership matrix local information (MLI) of fuzzy clustering is extracted to be utilized besides the spatial constraint. The proposed method solves two significant drawbacks of spatial constraint-based FCM approaches, which are ineffectiveness in preserving image details as well as confronting noise and intensity non-uniformity (INU) simultaneously. These problems are caused due to utilizing spatial constraints solely. The presented context-aware spatial constraint makes the method robust against a high level of noise while preserving image details. Furthermore, employing the MLI technique improves segmentation results in the presence of noise concurrently with INU. In contrast to spatial constraint-based methods, which just use local information in the image domain, the FCMS-MLI technique utilizes information in both image and coefficient domains. Hence, the proposed method benefits from two different sources of information. Finally, several types of images, including synthetic images, simulated and real brain MR images are utilized to make a comparison among the performances of popular FCMS types (i.e. FCM algorithms with spatial constraint), some new methods and the proposed algorithm. Experimental results prove efficiency and robustness of the FCMS-MLI algorithm confronting different levels of noise and INU.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419308772",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Cluster analysis",
      "Computer science",
      "Constraint (computer-aided design)",
      "Context (archaeology)",
      "Data mining",
      "Fuzzy clustering",
      "Fuzzy logic",
      "Geometry",
      "Image (mathematics)",
      "Image segmentation",
      "Mathematics",
      "Noise (video)",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Segmentation",
      "Spatial analysis",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Kouhi",
        "given_name": "Abolfazl"
      },
      {
        "surname": "Seyedarabi",
        "given_name": "Hadi"
      },
      {
        "surname": "Aghagolzadeh",
        "given_name": "Ali"
      }
    ]
  },
  {
    "title": "Automatic multiple moving humans detection and tracking in image sequences taken from a stationary thermal infrared camera",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113171",
    "abstract": "Several Particle Filter (PF)-based methods for human tracking in thermal IR image sequences have been proposed in the literature. Unfortunately, the majority of these methods are developed for tracking only a single human. Moreover, this human is manually pre-selected in the first frame of the image sequence, which is not practical for the real case of intelligent and efficient video surveillance system that needs tracking more than one human and without any external operator intervention. To contribute to addressing this need, in this paper, we propose a novel PF-based method that detects and tracks multiple moving humans using a thermal IR camera, without prior knowledge about their number and initial locations in the monitored scene. This method consists of three main parts. In the first one, all the moving objects are extracted from the image sequence by using the Gaussian Mixture Model (GMM) and then, for each extracted object, a combined shape, appearance, spatial and temporal-based similarity function that allows us to detect a human without any prior training of a mathematical model is calculated. The second part consists in tracking the human previously detected by using a PF and an adaptive combination of spatial, intensity, texture and motion velocity cues. In each cue, a model for the detected human is created, and when new observations arrive in the next frames, the similarity distances between each created model and the observed moving regions are calculated. The human tracking is achieved by combining individual similarity distances using adaptive weights, into a PF algorithm. The third part is devoted to detect and handle occlusions by using simple heuristic rules and grayscale Vertical Projection Histogram (VPH). Each part of the proposed method was separately tested on a set of real-world thermal IR image sequences containing background clutters, appearance and disappearance of multiple moving objects, occlusions, illumination and scales changes. A comparative study with several state-of-the-art methods has shown that the proposed method performs consistently better in terms of Center Location Error (CLE) and the Success Rate (SR), and it can also run at speed of about 15 Frame Per Second per human, which is considerably enough for real-time applications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419308887",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Filter (signal processing)",
      "Gaussian",
      "Image (mathematics)",
      "Object (grammar)",
      "Particle filter",
      "Pattern recognition (psychology)",
      "Pedagogy",
      "Physics",
      "Psychology",
      "Quantum mechanics",
      "Similarity (geometry)",
      "Tracking (education)",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Younsi",
        "given_name": "Merzouk"
      },
      {
        "surname": "Diaf",
        "given_name": "Moussa"
      },
      {
        "surname": "Siarry",
        "given_name": "Patrick"
      }
    ]
  },
  {
    "title": "Towards automatically filtering fake news in Portuguese",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113199",
    "abstract": "In the last years, the popularity of smartphones and social networks has been contributing to the spread of fake news. Through these electronic media, this type of news can deceive thousands of people in a short time and cause great harm to individuals, companies, or society. Fake news has the potential to change a political scenario, to contribute to the spread of diseases, and even to cause deaths. Despite the efforts of several studies on fake news detection, most of them only cover English language news. There is a lack of labeled datasets of fake news in other languages and, moreover, important questions still remain open. For example, there is no consensus on what are the best classification strategies and sets of features to be used for automatic fake news detection. To answer this and other important open questions, we present a new public and real dataset of labeled true and fake news in Portuguese, and we perform a comprehensive analysis of machine learning methods for fake news detection. The experiments were performed using different sets of features and employing different types of classification methods. A careful analysis of the results provided sufficient evidence to respond appropriately to the open questions. The various evaluated scenarios and the drawn conclusions from the results shed light on the potentiality of the methods and on the challenges that fake news detection presents.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300257",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Cover (algebra)",
      "Data science",
      "Engineering",
      "Fake news",
      "Harm",
      "Information retrieval",
      "Internet privacy",
      "Law",
      "Linguistics",
      "Mechanical engineering",
      "Philosophy",
      "Political science",
      "Popularity",
      "Portuguese",
      "Social media",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Silva",
        "given_name": "Renato M."
      },
      {
        "surname": "Santos",
        "given_name": "Roney L.S."
      },
      {
        "surname": "Almeida",
        "given_name": "Tiago A."
      },
      {
        "surname": "Pardo",
        "given_name": "Thiago A.S."
      }
    ]
  },
  {
    "title": "A-Stacking and A-Bagging: Adaptive versions of ensemble learning algorithms for spoof fingerprint detection",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113160",
    "abstract": "Stacking and bagging are widely used ensemble learning approaches that make use of multiple classifier systems. Stacking focuses on building an ensemble of heterogeneous classifiers while bagging constructs an ensemble of homogenous classifiers. There exist some applications where it is essential for learning algorithms to be adaptive towards the training data. We propose A-Stacking and A-Bagging; adaptive versions of stacking and bagging respectively that take into consideration the similarity inherently present in the dataset. One of the main motives of ensemble learning is to generate an ensemble of multiple “experts” that are weakly correlated. We achieve this by producing a set of disjoint experts where each expert is trained on a different subset of the dataset. We show the working mechanism of the proposed algorithms on spoof fingerprint detection. The proposed versions of these algorithms are adaptive as they conform to the features extracted from the live and spoof fingerprint images. From our experimental results, we establish that A-Stacking and A-Bagging give competitive results on both balanced and imbalanced datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419308784",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Cascading classifiers",
      "Classifier (UML)",
      "Combinatorics",
      "Computer science",
      "Data mining",
      "Disjoint sets",
      "Ensemble learning",
      "Fingerprint (computing)",
      "Machine learning",
      "Mathematics",
      "Nuclear magnetic resonance",
      "Pattern recognition (psychology)",
      "Physics",
      "Random subspace method",
      "Stacking"
    ],
    "authors": [
      {
        "surname": "Agarwal",
        "given_name": "Shivang"
      },
      {
        "surname": "Chowdary",
        "given_name": "C. Ravindranath"
      }
    ]
  },
  {
    "title": "A novel wrapper feature selection algorithm based on iterated greedy metaheuristic for sentiment classification",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113176",
    "abstract": "In recent years, sentiment analysis is becoming more and more important as the number of digital text resources increases in parallel with the development of information technology. Feature selection is a crucial sub-stage for the sentiment analysis as it can improve the overall predictive performance of a classifier while reducing the dimensionality of a problem. In this study, we propose a novel wrapper feature selection algorithm based on Iterated Greedy (IG) metaheuristic for sentiment classification. We also develop a selection procedure that is based on pre-calculated filter scores for the greedy construction part of the IG algorithm. A comprehensive experimental study is conducted on commonly-used sentiment analysis datasets to assess the performance of the proposed method. The computational results show that the proposed algorithm achieves 96.45% and 90.74% accuracy rates on average by using Multinomial Naïve Bayes classifier for 9 public sentiment and 4 Amazon product reviews datasets, respectively. The results also reveal that our algorithm outperforms state-of-the-art results for the 9 public sentiment datasets. Moreover, the proposed algorithm produces highly competitive results with state-of-the-art feature selection algorithms for 4 Amazon datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300026",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Feature selection",
      "Greedy algorithm",
      "Machine learning",
      "Metaheuristic",
      "Naive Bayes classifier",
      "Pattern recognition (psychology)",
      "Sentiment analysis",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Gokalp",
        "given_name": "Osman"
      },
      {
        "surname": "Tasci",
        "given_name": "Erdal"
      },
      {
        "surname": "Ugur",
        "given_name": "Aybars"
      }
    ]
  },
  {
    "title": "Ensemble learning with label proportions for bankruptcy prediction",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113155",
    "abstract": "Corporate bankruptcy prediction is an interesting and important research topic that can be conceived in many practical applications. Recently, machine learning based methods have been widely proposed to solve the problem of bankruptcy prediction. However, the existing models do not consider that large amounts of instance-level labeled training data are hard to be obtained in practice. In this paper, we propose to address bankruptcy prediction problem from the perspective of learning with label proportions, where the unlabeled training data are provided in different bags and only giving the bag-level proportion of instances belonging to a particular class. Then, we contribute two novel prediction methods, termed as Bagged-pSVM and Boosted-pSVM, based on proportion support vector machines and ensemble strategies including bagging and boosting. The proposed methods can not only explicitly model the unknown instance-level labels and the known label proportions under a large-margin framework, but also improve the performance through introducing ensemble learning strategies. Extensive experiments on the benchmark datasets demonstrate their efficiency and superiority on solving the problem of bankruptcy prediction.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419308723",
    "keywords": [
      "Artificial intelligence",
      "Bankruptcy",
      "Bankruptcy prediction",
      "Benchmark (surveying)",
      "Boosting (machine learning)",
      "Computer science",
      "Economics",
      "Ensemble forecasting",
      "Ensemble learning",
      "Finance",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Margin (machine learning)",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Zhensong"
      },
      {
        "surname": "Chen",
        "given_name": "Wei"
      },
      {
        "surname": "Shi",
        "given_name": "Yong"
      }
    ]
  },
  {
    "title": "Combining novelty and popularity on personalised recommendations via user profile learning",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113149",
    "abstract": "Recommender systems have been widely used by large companies in the e-commerce segment as aid tools in the search for relevant contents according to the user’s particular preferences. A wide variety of algorithms have been proposed in the literature aiming at improving the process of generating recommendations; in particular, a collaborative, diffusion-based hybrid algorithm has been proposed in the literature to solve the problem of sparse data, which affects the quality of recommendations. This algorithm was the basis for several others that effectively solved the sparse data problem. However, this family of algorithms does not differentiate users according to their profiles. In this paper, a new algorithm is proposed for learning the user profile and, consequently, generating personalised recommendations through diffusion, combining novelty with the popularity of items. Experiments performed in well-known datasets show that the results of the proposed algorithm outperform those from both diffusion-based hybrid algorithm and traditional collaborative filtering algorithm, in the same settings.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419308668",
    "keywords": [
      "Artificial intelligence",
      "Collaborative filtering",
      "Computer science",
      "Data mining",
      "Epistemology",
      "Machine learning",
      "Novelty",
      "Operating system",
      "Philosophy",
      "Popularity",
      "Process (computing)",
      "Psychology",
      "Quality (philosophy)",
      "Recommender system",
      "Social psychology",
      "Theology",
      "Variety (cybernetics)"
    ],
    "authors": [
      {
        "surname": "Bertani",
        "given_name": "Ricardo Mitollo"
      },
      {
        "surname": "A. C. Bianchi",
        "given_name": "Reinaldo"
      },
      {
        "surname": "Costa",
        "given_name": "Anna Helena Reali"
      }
    ]
  },
  {
    "title": "Evidential two-step tree species recognition approach from leaves and bark",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113154",
    "abstract": "The contribution of this paper is twofold. First, this paper aims at developing an intelligent system that emulates the decision-making ability of a botanist expert in the recognition of tree species from their leaves and bark. The main challenges of this recognition problem are related to the high diversity of trees in nature, the interspecies similarity and the intra-species variability. Therefore, similarities between species cause several confusions during recognition. The proposed decision system is designed to solve this complex problem of tree species recognition by reasoning with knowledge sets where the inference engine is based on belief functions theory, which reduces confusion between species and achieves greater accuracy. Secondly, this paper proposes a practical solution that can be embedded in the user's smartphone without any need for an internet connection. Therefore, our approach is adapted for smartphone limits, i.e. limits related to memory and computation capacity. Once in nature, everybody should appreciate the idea of having a mobile application that reflects the skills and know-how of a botanist. Building an application to make the potential of tree species recognition accessible and easy to use is a challenging problem. From methodological perspectives, the suggested method is a two-step recognition approach that identifies the leaf in a first step and refines the results using the bark in the second step. In fact, the first step is used to reduce the dimensionality of the problem through the identification of a subset of most probable species. The second step is performed using a modified evidential k Nearest Neighbors (EkNN) algorithm that recognizes the bark from the output of the first step. A set of experiments on real-world data is presented in order to study the accuracy of the proposed solution against existing ones.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419308711",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Data mining",
      "Decision tree",
      "Ecology",
      "Identification (biology)",
      "Image (mathematics)",
      "Inference",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Similarity (geometry)",
      "Tree (set theory)"
    ],
    "authors": [
      {
        "surname": "Jendoubi",
        "given_name": "Siwar"
      },
      {
        "surname": "Coquin",
        "given_name": "Didier"
      },
      {
        "surname": "Boukezzoula",
        "given_name": "Reda"
      }
    ]
  },
  {
    "title": "Ensemble belief rule base modeling with diverse attribute selection and cautious conjunctive rule for classification problems",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113161",
    "abstract": "Belief rule-based systems have demonstrated its advantages in solving complicated problems with uncertain information. However, the rule combinatorial explosion problem is still a great challenge for belief rule bases (BRBs) when a problem involves a large number of attributes, because existing attempts have not addressed this challenge adequately, e.g., utilization of single attribute selection method to downsize BRBs without considering its inherent weakness, or adjustment of referential values to optimize BRBs without attribute selection. Thus, inspired by ensemble learning, the objective of this paper is to propose an ensemble BRB modeling method to deal with classification problems. First, six attribute selection methods that have different advantages are introduced to select diverse sets of antecedent attributes for constructing multiple BRBs, and all of these BRBs are further trained by parameter learning for diverse belief rule-based systems. Second, due to the fact that each belief rule-based system has different importance and hardly satisfies the assumption of independence, a weight learning method is proposed to determine the weight of each belief rule-based system, and a new analytical cautious conjunctive rule (CCR) is deduced from the recursive CCR, that is suitable for the combination of non- independent individuals, to combine the outputs of all belief rule-based systems. Eight classification datasets from the well- known UCI database are adopted to verify the effectiveness of the proposed BRB modeling method in comparison with the belief rule-based systems constructed by single attribute selection, conventional fuzzy rule-based classifiers, and machine learning-based classifiers.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419308796",
    "keywords": [
      "Antecedent (behavioral psychology)",
      "Artificial intelligence",
      "Base (topology)",
      "Belief structure",
      "Computer science",
      "Data mining",
      "Developmental psychology",
      "Fuzzy logic",
      "Fuzzy rule",
      "Fuzzy set",
      "Independence (probability theory)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Psychology",
      "Rule-based system",
      "Selection (genetic algorithm)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Long-Hao"
      },
      {
        "surname": "Ye",
        "given_name": "Fei-Fei"
      },
      {
        "surname": "Wang",
        "given_name": "Ying-Ming"
      }
    ]
  },
  {
    "title": "Informative top-k class associative rule for cancer biomarker discovery on microarray data",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113169",
    "abstract": "The discovery of reliable cancer biomarkers is crucial for accurate early detection and clinical diagnosis. One of the strategies is by identifying expression-based cancer biomarkers through integrative microarray data analysis. Microarray is a powerful high-throughput technology, which allows a genome-wide analysis of human genes with various biological information. Nevertheless, more studies are needed on improving the predictability of the discovered gene biomarkers, as well as their reproducibility and interpretability, to qualify them for clinical use. This paper proposes an informative top-k class associative rule (iTCAR) method in an integrative framework for identifying candidate genes of specific cancers. iTCAR introduces an enhanced associative classification algorithm that integrates microarray data with biological information from gene ontology, KEGG pathways, and protein-protein interactions to generate informative class associative rules. A new interestingness measurement is used to rank and select class associative rules for building accurate classifiers. The experimental results show that iTCAR has excellent predictability by achieving the average classification accuracy above 90% and the average area under the curve above 0.8. Besides, iTCAR has significant reproducibility and interpretability through functional enrichment analysis and retrieval of meaningful cancer terms. These promising results suggest the proposed method has great potential in identifying candidate genes, which can be further investigated as biomarkers for cancer diseases.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419308863",
    "keywords": [
      "Artificial intelligence",
      "Associative property",
      "Biology",
      "Biomarker discovery",
      "Class (philosophy)",
      "Computational biology",
      "Computer science",
      "Data mining",
      "Gene",
      "Gene expression",
      "Gene ontology",
      "Genetics",
      "Interpretability",
      "KEGG",
      "Machine learning",
      "Mathematics",
      "Microarray analysis techniques",
      "Proteomics",
      "Pure mathematics"
    ],
    "authors": [
      {
        "surname": "Ong",
        "given_name": "Huey Fang"
      },
      {
        "surname": "Mustapha",
        "given_name": "Norwati"
      },
      {
        "surname": "Hamdan",
        "given_name": "Hazlina"
      },
      {
        "surname": "Rosli",
        "given_name": "Rozita"
      },
      {
        "surname": "Mustapha",
        "given_name": "Aida"
      }
    ]
  },
  {
    "title": "Data-driven construction of SPARQL queries by approximate question graph alignment in question answering over knowledge graphs",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113205",
    "abstract": "As increasingly more semantic real-world data is stored in knowledge graphs, providing intuitive and effective query methods for end-users is a fundamental and challenging task. Since there is a gap between the plain natural language question (NLQ) and structured data, most RDF question/answering (Q/A) systems construct SPARQL queries from NLQs and obtain precise answers from knowledge graphs. A major challenge is how to disambiguate the mapping of phrases and relations in a question to the dataset items, especially in complex questions. In this paper, we propose a novel data-driven graph similarity framework for RDF Q/A to extract the query graph patterns directly from the knowledge graph instead of constructing them with semantically mapped items. An uncertain question graph is presented to model the interpretations of an NLQ, based on which our problem is reduced to a graph alignment problem. In formulating the alignment, both the lexical and structural similarity of graphs are considered, hence, the target RDF subgraph is used as a query graph pattern to construct the final query. We create a pruned entity graph dynamically based on the complexity of an input question to reduce the search space on the knowledge graph. Moreover, to reduce the calculating cost of the graph similarity, we compute the similarity scores only for same-distance graph elements and equip the process with an edge association-aware surface form extraction method. Empirical studies over real datasets indicate that our proposed approach is flexible and effective as it outperforms state-of-the-art methods significantly.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300312",
    "keywords": [
      "Computer science",
      "Graph",
      "Graph database",
      "Information retrieval",
      "Knowledge graph",
      "RDF",
      "SPARQL",
      "Semantic Web",
      "Semantic similarity",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Bakhshi",
        "given_name": "Mahdi"
      },
      {
        "surname": "Nematbakhsh",
        "given_name": "Mohammadali"
      },
      {
        "surname": "Mohsenzadeh",
        "given_name": "Mehran"
      },
      {
        "surname": "Rahmani",
        "given_name": "Amir Masoud"
      }
    ]
  },
  {
    "title": "An efficient binary social spider algorithm for feature selection problem",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113185",
    "abstract": "The social spider algorithm (SSA) is a heuristic algorithm created on spider behaviors to solve continuous problems. In this paper, firstly a binary version of the social spider algorithm called binary social spider algorithm (BinSSA) is proposed. Currently, there is insufficient focus on the binary version of SSA in the literature. The main part of the binary version is the transfer function. The transfer function is responsible for mapping continuous search space to binary search space. In this study, eight of the transfer functions divided into two families, S-shaped and V-shaped, are evaluated. BinSSA is obtained from SSA, by transforming constant search space to binary search space with eight different transfer functions (S-Shapes and V-Shaped). Thus, eight different variations of BinSSA are formed as BinSSA1, BinSSA2, BinSSA3, BinSSA4, BinSSA5, BinSSA6, BinSSA7, and BinSSA8. For increasing, exploration and exploitation capacity of BinSSA, a crossover operator is added as BinSSA-CR. In secondly, the performances of BinSSA variations are tested on feature selection task. The optimal subset of features is a challenging problem in the process of feature selection. In this paper, according to different comparison criteria (mean of fitness values, the standard deviation of fitness values, the best of fitness values, the worst of fitness values, accuracy values, the mean number of the selected features, CPU time), the best BinSSA variation is detected. In the feature selection problem, the K-nearest neighbor (K-NN) and support vector machines (SVM) are used as classifiers. A detailed study is performed for the fixed parameter values used in the fitness function. BinSSA is evaluated on low-scaled, middle-scaled and large-scaled twenty-one well-known UCI datasets and obtained results are compared with state-of-art algorithms in the literature. Obtained results have shown that BinSSA and BinSSA-CR show superior performance and offer quality and stable solutions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300117",
    "keywords": [
      "Algorithm",
      "Arithmetic",
      "Artificial intelligence",
      "Binary number",
      "Binary search algorithm",
      "Computer science",
      "Crossover",
      "Feature (linguistics)",
      "Feature selection",
      "Fitness function",
      "Genetic algorithm",
      "Heuristic",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Search algorithm"
    ],
    "authors": [
      {
        "surname": "BAŞ",
        "given_name": "Emine"
      },
      {
        "surname": "ÜLKER",
        "given_name": "Erkan"
      }
    ]
  },
  {
    "title": "ILWAANet: An Interactive Lexicon-Aware Word-Aspect Attention Network for aspect-level sentiment classification on social networking",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113065",
    "abstract": "An Interactive Lexicon-Aware Word-Aspect Attention Network (ILWAAN) is proposed for aspect-level sentiment classification which deals with identifying the sentiment polarity of a specific aspect in its context and have potential application on social networking. In this model, effective multiple attention mechanisms (intra-attention and interactive-attention mechanisms) integrated with sentiment lexicon information are developed to form an aspect-specific representation at two levels: Phrase-level and Aggregation-level information. Specifically, an aspect and its context are fused with the sentiment lexicon information and learn their relationship representations by lexicon-aware attention operations. This allows the model to tries to incorporate the aspect information into the deep neural networks and learn to attend the correct sentiment context words conditioned on the informative aspect words. To evaluate the performance, we evaluate our model in three benchmark data: Twitter, Laptop, and Restaurant. The experimental results indicate that our models improve the performance for aspect-level sentiment classification.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419307821",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Geodesy",
      "Geography",
      "Laptop",
      "Law",
      "Lexicon",
      "Linguistics",
      "Natural language processing",
      "Negation",
      "Operating system",
      "Paleontology",
      "Philosophy",
      "Phrase",
      "Political science",
      "Politics",
      "Programming language",
      "Representation (politics)",
      "Sentiment analysis",
      "Social media",
      "Word (group theory)",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Nguyen",
        "given_name": "Huy-Thanh"
      },
      {
        "surname": "Nguyen",
        "given_name": "Le-Minh"
      }
    ]
  },
  {
    "title": "The path cover problem: Formulation and a hybrid metaheuristic",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113107",
    "abstract": "This paper introduces the path cover problem (PCP) that is a variant of the capacitated vehicle routing problem (CVRP). In contrast to CVRP, a vehicle route in PCP involves a set of vertex-disjoint paths where a vehicle starts at a customer and finishes at another customer without traveling to the depot. PCP is defined to find a set of service paths to serve a set of customers that aiming to minimize the total traveling cost and hiring cost. This paper develops a hybrid approach that integrates variable neighborhood search (VNS) and tabu search (TS) to solve the problem. This algorithm presents an approach to explore the solution space by utilizing multi-set neighborhood and applying the systematic changing neighborhood of VNS. TS is incorporated into the algorithm to guide the search toward diverse regions. The computational results indicate that the proposed algorithm efficiently solves PCP.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419308243",
    "keywords": [
      "Algorithm",
      "Approximation algorithm",
      "Combinatorics",
      "Computer network",
      "Computer science",
      "Cover (algebra)",
      "Disjoint sets",
      "Engineering",
      "Graph",
      "Mathematical optimization",
      "Mathematics",
      "Mechanical engineering",
      "Metaheuristic",
      "Path (computing)",
      "Programming language",
      "Routing (electronic design automation)",
      "Set (abstract data type)",
      "Set cover problem",
      "Tabu search",
      "Theoretical computer science",
      "Variable neighborhood search",
      "Vehicle routing problem",
      "Vertex (graph theory)",
      "Vertex cover"
    ],
    "authors": [
      {
        "surname": "Yu",
        "given_name": "Vincent F."
      },
      {
        "surname": "Redi",
        "given_name": "A. A. N. Perwira"
      },
      {
        "surname": "Halim",
        "given_name": "Christine"
      },
      {
        "surname": "Jewpanya",
        "given_name": "Parida"
      }
    ]
  },
  {
    "title": "A grid-quadtree model selection method for support vector machines",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113172",
    "abstract": "In this paper, a new model selection approach for Support Vector Machine (SVM), which integrates the quadtree technique with the grid search, denominated grid-quadtree (GQ) is proposed. The developed method is the first in the literature to apply the quadtree for the SVM parameters optimization. The SVM is a machine-learning technique for pattern recognition whose performance relies on its parameters determination. Thus, the model selection problem for SVM is an important field of study and requires expert and intelligent systems to solve it. Real classification data sets involve a huge number of instances and features, and the greater is the training data set dimension, the larger is the cost of a recognition system. The grid search (GS) is the most popular and the simplest method to select parameters for SVM. However, it is time-consuming, which limits its application for big-sized problems. With this in mind, the main idea of this research is to apply the quadtree technique to the GS to make it faster. Hence, this may lower computational time cost for solving problems such as bio-identification, bank credit risk and cancer detection. Based on the asymptotic behaviors of the SVM, it was noticeably observed that the quadtree is able to avoid the GS full search space evaluation. As a consequence, the GQ carries out fewer parameters analysis, solving the same problem with much more efficiency. To assess the GQ performance, ten classification benchmark data set were used. The obtained results were compared with the ones of the traditional GS. The outcomes showed that the GQ is able to find parameters that are as good as the GS ones, executing 78.8124% to 85.8415% fewer operations. This research points out that the adoption of quadtree expressively reduces the computational time of the original GS, making it much more efficient to deal with high dimensional and large data sets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419308899",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Data mining",
      "Dimension (graph theory)",
      "Geodesy",
      "Geography",
      "Geometry",
      "Grid",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pure mathematics",
      "Quadtree",
      "Selection (genetic algorithm)",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Beltrami",
        "given_name": "Monica"
      },
      {
        "surname": "da Silva",
        "given_name": "Arinei Carlos Lindbeck"
      }
    ]
  },
  {
    "title": "Corrigendum to “Likelihood ratio equivalence and imbalanced binary classification” [Expert Systems with Applications, Volume 130 (2019), Pages 84--96]",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113299",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742030124X",
    "keywords": [
      "Arithmetic",
      "Binary number",
      "Computer science",
      "Discrete mathematics",
      "Equivalence (formal languages)",
      "Mathematics",
      "Physics",
      "Thermodynamics",
      "Volume (thermodynamics)"
    ],
    "authors": [
      {
        "surname": "Benítez-Buenache",
        "given_name": "Alexander"
      },
      {
        "surname": "Álvarez-Pérez",
        "given_name": "Lorena"
      },
      {
        "surname": "Mathews",
        "given_name": "V. John"
      },
      {
        "surname": "Figueiras-Vidal",
        "given_name": "Aníbal R."
      }
    ]
  },
  {
    "title": "A hybrid discrete water wave optimization algorithm for the no-idle flowshop scheduling problem with total tardiness criterion",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113166",
    "abstract": "The no-idle flowshop has attracted enormous attention owing to its widespread application in the manufacturing industry domain. In this paper, a hybrid discrete water wave optimization algorithm, named HWWO, is presented to solve the NIFSP with total tardiness. In order to improve the quality of a population, an initialize method based on a new priority rule combined with the modified NEH method is proposed to generate a population. In the propagation phase, a self-adaption selection neighborhood search structure is introduced to amplify the search range of waves and balance the exploration and exploitation ability of the HWWO. Afterwards, a variable neighborhood search is adopted to strengthen the local search and maintain the diversity of the population in the breaking phase. In the refraction operation, a perturbation sequence is generated and combined with the local optimal solution found by the breaking operation, in order to generate a new solution, and prevent the algorithm from becoming trapped in the local optimum. Furthermore, the control parameters and time complexity are analyzed. The experimental results and comparisons with the other state-of-the-art algorithms evaluated on Taillard's and Ruiz's benchmark sets reveal that the effectiveness and efficiency of the HWWO outperformed the compared algorithms for solving the NIFSP.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419308838",
    "keywords": [
      "Algorithm",
      "Benchmark (surveying)",
      "Computer science",
      "Demography",
      "Geodesy",
      "Geography",
      "Job shop scheduling",
      "Local optimum",
      "Local search (optimization)",
      "Mathematical optimization",
      "Mathematics",
      "Metaheuristic",
      "Operating system",
      "Population",
      "Schedule",
      "Sociology",
      "Tardiness",
      "Variable neighborhood search"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Fuqing"
      },
      {
        "surname": "Zhang",
        "given_name": "Lixin"
      },
      {
        "surname": "Zhang",
        "given_name": "Yi"
      },
      {
        "surname": "Ma",
        "given_name": "Weimin"
      },
      {
        "surname": "Zhang",
        "given_name": "Chuck"
      },
      {
        "surname": "Song",
        "given_name": "Houbin"
      }
    ]
  },
  {
    "title": "Improving consensus clustering with noise-induced ensemble generation",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113138",
    "abstract": "Because of the negative perception towards noise, it is commonly eliminated in the process of data cleansing prior to the analysis process. Some studies attempt to employ tolerant or robust algorithms to achieve a reliable outcome. One way or another, the impact of noise might be minimized, thus preserving the integrity of discovered knowledge. On the other hand, making good use of noise has recently been investigated and exploited in different contexts, such as in privacy-preserving data mining, single clustering and consensus clustering. Given our initial study of employing uniform random noise in the process of ensemble generation as a way to increase diversity within an ensemble, improved clustering goodness can be obtained at specific levels of noise. To consolidate the aforementioned finding, this paper investigates a rich collection of random noise functions, which can be used to form perturbed data variation within the framework of noise-induced ensemble generation. The effectiveness of this approach which uses different cases for random noise is demonstrated over benchmark datasets from the UCI repository. The results suggest that the noise-induced strategy is generally better than the baseline counterpart, whilst showing uneven improvement with different data patterns. As such, a guideline is provided to make the best use of the proposed method with any new set of data.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419308553",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "CURE data clustering algorithm",
      "Cluster analysis",
      "Computer science",
      "Consensus clustering",
      "Correlation clustering",
      "Data mining",
      "Data set",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Machine learning",
      "Noise (video)",
      "Noise measurement",
      "Noise reduction",
      "Operating system",
      "Pattern recognition (psychology)",
      "Process (computing)",
      "Programming language",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Panwong",
        "given_name": "Patcharaporn"
      },
      {
        "surname": "Boongoen",
        "given_name": "Tossapon"
      },
      {
        "surname": "Iam-On",
        "given_name": "Natthakan"
      }
    ]
  },
  {
    "title": "A bi-phased multi-objective genetic algorithm based classifier",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113163",
    "abstract": "This paper presents a novel Bi-Phased Multi-Objective Genetic Algorithm (BPMOGA) based classification method. It is a Learning Classifier System (LCS) designed for supervised learning tasks. Here we have used Genetic Algorithms (GAs) to discover optimal classifiers from data sets. The objective of the work is to find out a classifier or Complete Rule (CR) which comprises of several Class Specific Rules (CSRs). Phase-I of BPMOGA extracts optimized CSRs in I F − T H E N form by following Michigan approach, without considering interaction among the rules. Phase-II of BPMOGA builds optimized CRs from CSRs by following Pittsburgh way. It combines the advantages of both approaches. Extracted CRs help to build CSRs for the next run of phase-I. Hence, phase-I and phase-II are cyclically related, which is one of the uniqueness of BPMOGA. With the help of twenty one benchmark data sets from the University of California at Irvine (UCI) machine learning repository we have compared performance of BPMOGA based classifier with fourteen GA and non-GA based classifiers. Statistical test shows that the performance of the proposed classifier is either superior or comparable to other classifiers.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419308814",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Genetic algorithm",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Quadratic classifier",
      "Random subspace method"
    ],
    "authors": [
      {
        "surname": "Dutta",
        "given_name": "Dipankar"
      },
      {
        "surname": "Sil",
        "given_name": "Jaya"
      },
      {
        "surname": "Dutta",
        "given_name": "Paramartha"
      }
    ]
  },
  {
    "title": "Model validation failure in class imbalance problems",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2020.113190",
    "abstract": "For a classification task, multiple classification models can be built from the training set in various ways. In general, the best-performing model is selected for deployment through a model validation procedure. However, even if the dataset is sufficiently large, model validation is difficult when the minority class is too rare in an absolute sense in the validation set. Under such an extreme absolute rarity condition, the validation performance of a model is more affected by randomness in the model so that it would misleadingly estimate the generalization ability of the model. In this regard, even a random guessing model, which will eventually fail to accurately classify new data, can yield a considerably high validation performance by chance. This implies that the selected model may not perform well during its deployment. In this study, the effect of absolute rarity on the inherent difficulty of model validation is investigated. We demonstrate that the higher degree of absolute rarity in the validation set as well as comparing a larger number of models during model validation contribute to an increased likelihood of model validation failure. Finally, a practical guideline is suggested to evaluate model validation results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420300166",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Cross-validation",
      "Data mining",
      "Data science",
      "Generalization",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Model validation",
      "Programming language",
      "Randomness",
      "Set (abstract data type)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Kang",
        "given_name": "Seokho"
      }
    ]
  },
  {
    "title": "Hybrid multiobjective evolutionary algorithm with fast sampling strategy-based global search and route sequence difference-based local search for VRPTW",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113151",
    "abstract": "The vehicle routing problem with time windows (VRPTW) is an important and widely studied combinatorial optimization problems. This paper aims at VRPTW with the objectives of reducing the number of vehicles and minimizing the time-wasting during the delivery process caused by early arrival. To solve this NP-hard problem, a hybrid multiobjective evolutionary algorithm with fast sampling strategy-based global search and route sequence difference-based local search (HMOEA-GL) is proposed. Firstly, fast sampling strategy-based global search (FSS-GS) of HMOEA-GL extensively explores the entire solution space to quickly guide the search direction towards the center and edge areas of Pareto frontier. Secondly, route sequence difference-based local search (RSD-LS) is executed on the individuals with poor performance in the population obtained by FSS-GS to enhance the search ability of HMOEA-GL. In addition, the suitable coding method and proper genetic operators are designed, especially, a simple insertion search is used to reduce the number of vehicles in VRPTW. Comparing with NSGA-II, SPEA2, and MOEA/D, experimental results on 12 Solomon benchmark test problems indicate that the proposed HMOEA-GL is effective, and more excellent in convergence, while maintaining a satisfying distribution performance. HMOEA-GL could be an effective intelligent algorithm for expert and intelligent decision support system to help logistics companies to make decisions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419308681",
    "keywords": [
      "Algorithm",
      "Benchmark (surveying)",
      "Biology",
      "Computer network",
      "Computer science",
      "Demography",
      "Evolutionary algorithm",
      "Genetics",
      "Geodesy",
      "Geography",
      "Guided Local Search",
      "Local search (optimization)",
      "Mathematical optimization",
      "Mathematics",
      "Multi-objective optimization",
      "Pareto principle",
      "Population",
      "Routing (electronic design automation)",
      "Sequence (biology)",
      "Sociology",
      "Vehicle routing problem"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Wenqiang"
      },
      {
        "surname": "Yang",
        "given_name": "Diji"
      },
      {
        "surname": "Zhang",
        "given_name": "Guohui"
      },
      {
        "surname": "Gen",
        "given_name": "Mitsuo"
      }
    ]
  },
  {
    "title": "Trendlets: A novel probabilistic representational structures for clustering the time series data",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113119",
    "abstract": "Time series data is a sequence of values recorded systematically over a period which are mostly used for prediction, clustering, and analysis. The two essential features of a time series data are trend and seasonality. Preprocessing of the time series data is necessary for performing prediction tasks. In most of the cases, the trend and the seasonality are removed before applying the regression algorithms. The accuracy of such algorithms depends upon the functions used for the removal of trend and seasonality. Clustering of an unlabeled time series data with the presence of trend and seasonality is challenging. In this paper, we propose a probabilistic representational learning method for grouping the time series data. We introduce five terminologies in our method of clustering namely the trendlets, uplets, downlets, equalets and trendlet string. These elements are the representational building blocks of our proposed method. Experiments on the proposed algorithm are performed with the renewable energy data on the electricity supply system of continental Europe which includes the demand and inflow of renewable energy for the term 2012 to 2014 and UCR-2018 time series archive containing 128 datasets. We compared our proposed representational method with various clustering algorithms using the silhouette score. Mini-batch k-means and agglomerative hierarchical clustering algorithms show better performance in terms of quality, logical accordance with data and time taken for clustering.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741741930836X",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "CURE data clustering algorithm",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Data stream clustering",
      "Fuzzy clustering",
      "Hierarchical clustering",
      "Machine learning",
      "Paleontology",
      "Probabilistic logic",
      "Seasonality",
      "Series (stratigraphy)",
      "Time series"
    ],
    "authors": [
      {
        "surname": "C",
        "given_name": "Johnpaul"
      },
      {
        "surname": "Prasad",
        "given_name": "Munaga V.N.K."
      },
      {
        "surname": "Nickolas",
        "given_name": "S."
      },
      {
        "surname": "Gangadharan",
        "given_name": "G.R."
      }
    ]
  },
  {
    "title": "Rough set based lattice structure for knowledge representation in medical expert systems: Low back pain management case study",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113084",
    "abstract": "The aim of medical knowledge representation is to capture detailed domain knowledge in a clinically efficient manner, and to offer reliable resolution with the acquired knowledge. The knowledge base should allow incremental growth with inclusion of updated knowledge over the time. Accommodating the knowledge acquired from a variety of knowledge sources by different knowledge engineers may lead to a redundant and inconsistent design of the knowledge base, increasing the storage size and the time for knowledge retrieval. In this paper, we have proposed a rough set based lattice framework for representation of knowledge in medical expert systems which overcomes the problem of redundancy and inconsistency in knowledge and offers computational efficiency with respect to both time and space. The proposed knowledge representation offers a flexible scheme for expressing diverse possibilities of inter-relatedness among the symptoms and diseases in a systematic manner within the lattice structures. Through our design, we generate an optimal set of decision rules for use during inference. Reliability of each rule is measured using a new metric called credibility factor, and the certainty and coverage factors of a decision rule have been re-defined. During inference, the medical expert system considers the highly reliable and certain rules first, and the possible and uncertain rules at the later stages, if recommended by physicians. The proposed scheme ensures completeness, consistency, integrity, non-redundancy, and ease of access. These qualities are automatically preserved in the designed knowledge base while being updated with the new knowledge of medical advancements. As a result, the overall maintenance cost of a medical expert system is significantly reduced. The proposed knowledge representation technique has been illustrated using an example from the domain of low back pain. Though the proposed technique has been demonstrated for low back pain, it offers a generalized scheme for knowledge representation and may be adopted by medical expert systems (e.g. pathology, psychology, and many other specialties) having complexity similar to this one; no application-specific knowledge representation technique needs to be designed. The medical expert system developed using this scheme may act as a standardized example for development of different unexplored or semi-explored health care automated systems. This scheme may also be applied for pattern recognition, rule mining, and conflict analysis in complex medical data domains. Our design will have a wide ranging impact towards finding low-cost, reliable and available healthcare solutions for different diseases, especially in primary care units where expert physicians are scarce.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419308012",
    "keywords": [
      "Artificial intelligence",
      "Association rule learning",
      "Computer science",
      "Data mining",
      "Decision table",
      "Domain knowledge",
      "Expert system",
      "Inference",
      "Inference engine",
      "Knowledge acquisition",
      "Knowledge base",
      "Knowledge representation and reasoning",
      "Knowledge-based systems",
      "Legal expert system",
      "Machine learning",
      "Medical diagnosis",
      "Medicine",
      "Operating system",
      "Pathology",
      "Redundancy (engineering)",
      "Rough set"
    ],
    "authors": [
      {
        "surname": "Santra",
        "given_name": "Debarpita"
      },
      {
        "surname": "Basu",
        "given_name": "Swapan Kumar"
      },
      {
        "surname": "Mandal",
        "given_name": "Jyotsna Kumar"
      },
      {
        "surname": "Goswami",
        "given_name": "Subrata"
      }
    ]
  },
  {
    "title": "Boosting salp swarm algorithm by sine cosine algorithm and disrupt operator for feature selection",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113103",
    "abstract": "Features Selection (FS) plays an important role in enhancing the performance of machine learning techniques in terms of accuracy and response time. As FS is known to be an NP-hard problem, the aim of this paper is to introduce basically a new variant of Salp Swarm Optimizer (SSA) for FS (called ISSAFD (Improved Followers of Salp swarm Algorithm using Sine Cosine algorithm and Disrupt Operator), that updates the position of followers (F) in SSA using sinusoidal mathematical functions that were inspired from the Sine Cosine Algorithm (SCA). This enhancement helps to improve the exploration phase and to avoid stagnation in a local area. Moreover, the Disruption Operator (Dop ) is applied for all solutions, in order to enhance the population diversity and to maintain the balance between exploration and exploitation processes. Two other variants of SSA are developed based on SCA called ISSALD (Improved Leaders of Salp swarm Algorithm using Sine Cosine algorithm and Disrupt Operator) and ISSAF (Improved Followers of Salp swarm Algorithm using Sine Cosine algorithm). The updating process in consists to update the leaders (L) position by SCA and applying (Dop ), whereas in ISSAF, the Dop is omitted and the position of followers is updated by SCA. Experimental results are evaluated on twenty datasets where four of them represent high dimensionality with a small number of instances. The obtained results show a good performance of ISSAFD in terms of accuracy, sensitivity, specificity, and the number of selected features in comparison with other metaheuristics (MH).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419308206",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Economics",
      "Finance",
      "Gene",
      "Geometry",
      "Mathematical optimization",
      "Mathematics",
      "Operator (biology)",
      "Position (finance)",
      "Repressor",
      "Sine",
      "Swarm behaviour",
      "Transcription factor",
      "Trigonometric functions"
    ],
    "authors": [
      {
        "surname": "Neggaz",
        "given_name": "Nabil"
      },
      {
        "surname": "Ewees",
        "given_name": "Ahmed A."
      },
      {
        "surname": "Elaziz",
        "given_name": "Mohamed Abd"
      },
      {
        "surname": "Mafarja",
        "given_name": "Majdi"
      }
    ]
  },
  {
    "title": "A long-term fleet renewal problem under uncertainty: A simulation-based optimization approach",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113158",
    "abstract": "In this paper, we model and solve a strategic problem of fleet renewal to meet future operational needs under uncertain conditions. The fleet renewal problem focuses on mainly strategic decisions involving from fleet size, fleet mix and timing of replacement, yet it is essential to consider a significant amount of detail regarding short-term decisions to prevent inferior or infeasible strategies. In this direction, we develop a hybrid simulation model by combining system dynamics (SD) and discrete event simulation (DES) approaches. The standalone use of this model enables the decision maker to analyze the effects of both short- and long-term decisions on availability by simulating the processes that the fleet undertakes through its life-cycle from asset acquisition to retirement. Nevertheless, the simulation neither suggests nor seeks the best renewal strategy(ies). To alleviate this difficulty, we propose a simulation-based optimization that uses a genetic algorithm (GA) to effectively search a very large set of feasible fleet renewal strategies and uses the developed hybrid simulation model to evaluate candidate strategies found by GA. To provide a decision context where the approach has been developed and applied, we use a naval fleet renewal application. The extensive numerical experiments show that the proposed approach not only finds good and robust renewal strategies but also identify critical resources that influence the fleet’s availability. Finally, the robustness of optimized strategies under uncertainty is tested by sensitivity analysis, and mappings between implemented strategies and the fleet performance are constructed by scenario discovery analysis to provide insights for decision makers.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419308760",
    "keywords": [
      "Asset (computer security)",
      "Biochemistry",
      "Biology",
      "Chemistry",
      "Computer science",
      "Computer security",
      "Context (archaeology)",
      "Decision maker",
      "Discrete event simulation",
      "Engineering",
      "Gene",
      "Industrial engineering",
      "Mathematical optimization",
      "Mathematics",
      "Operations research",
      "Paleontology",
      "Physics",
      "Programming language",
      "Quantum mechanics",
      "Robust optimization",
      "Robustness (evolution)",
      "Set (abstract data type)",
      "Simulation",
      "Term (time)"
    ],
    "authors": [
      {
        "surname": "Turan",
        "given_name": "Hasan Hüseyin"
      },
      {
        "surname": "Elsawah",
        "given_name": "Sondoss"
      },
      {
        "surname": "Ryan",
        "given_name": "Michael J."
      }
    ]
  },
  {
    "title": "Multi-depot vehicle routing problem with risk mitigation: Model and solution algorithm",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113099",
    "abstract": "In practice, the execution of plans with vehicle routing components is often subjected to external events since the transporting vehicles can be exposed to various risk factors. This may lead to delivery failure, vehicle breakdown, commodity loses, etc. In this setting, the stakeholders can benefit from logistic planning techniques whereby potential vehicle breakdown and cargo delivery failure can be mitigated by limiting vehicle risk exposure and prioritizing deliveries of larger payloads. In this paper, we propose a cost effective learning-based heuristic technique to minimize the routing cost along with the potential cost due to the risk of vehicle breakdown and cargo delivery failure. The approach is elaborated by means of an illustrative case study, and it is accompanied by benchmark results along with a comparative study. The heuristic solution generation approach can be used to mitigate vehicle routing risk at the planning stage as well as during various proactive and reactive plan adaptation activities in response to the occurrence of exogenous events.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419308164",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Business",
      "Computer network",
      "Computer science",
      "Engineering",
      "Geodesy",
      "Geography",
      "Heuristic",
      "History",
      "Limiting",
      "Mechanical engineering",
      "Operations research",
      "Plan (archaeology)",
      "Reliability engineering",
      "Risk analysis (engineering)",
      "Routing (electronic design automation)",
      "Vehicle routing problem"
    ],
    "authors": [
      {
        "surname": "Soeanu",
        "given_name": "Andrei"
      },
      {
        "surname": "Ray",
        "given_name": "Sujoy"
      },
      {
        "surname": "Berger",
        "given_name": "Jean"
      },
      {
        "surname": "Boukhtouta",
        "given_name": "Abdeslem"
      },
      {
        "surname": "Debbabi",
        "given_name": "Mourad"
      }
    ]
  },
  {
    "title": "A hierarchical structure based on Stacking approach for skin lesion classification",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113127",
    "abstract": "Malignant melanoma is the most dangerous type of skin cancer. The diagnosis of melanoma in the early stages can greatly increase the possibility of its successful treatment. In the recent years, automated systems have played an pivotal role in increasing the skin cancer diagnosis rate. The main objective of this paper was to improve the performance of a skin cancer automated diagnostic system by introducing a new approach to combining classifiers in the classification stage. Therefore, the Stacking Ensemble Method based on the Meta Learning algorithm was proposed for the skin lesion classification. To classify skin lesions as melanoma, dysplastic and benign, two new hybrid approaches of Structure Based on Stacking (SBS) and Hierarchical Structure Based on Stacking (HSBS) were introduced to combine the heterogeneous classifiers. The proposed methods for skin lesions classification were implemented and evaluated based on the dermoscopic images of two PH2 and Ganster datasets using the Five Fold Cross Validation procedure and different numbers of the selected features. The results showed that the SBS approach had a good performance in diagnosing melanoma lesions from non-melanoma lesions for both datasets. Moreover, the results indicated that the HSBS method compared to the SBS approach and other works on the same dataset offers a far better performance in classifying skin lesions as benign, dysplastic and melanoma.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419308449",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Cancer",
      "Cancer research",
      "Computer science",
      "Dermatology",
      "Internal medicine",
      "Medicine",
      "Melanoma",
      "Melanoma diagnosis",
      "Nuclear magnetic resonance",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Physics",
      "Skin cancer",
      "Skin lesion",
      "Stacking",
      "Stage (stratigraphy)"
    ],
    "authors": [
      {
        "surname": "Ghalejoogh",
        "given_name": "Ghasem Shakourian"
      },
      {
        "surname": "Kordy",
        "given_name": "Hussain Montazery"
      },
      {
        "surname": "Ebrahimi",
        "given_name": "Farideh"
      }
    ]
  },
  {
    "title": "Synthetic accessibility assessment using auxiliary responses",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113106",
    "abstract": "Despite the recent advances in computational approaches to discovering new chemical compounds, accessibility assessment of designed compounds has still been a difficult task to automate because it is a heavily knowledge intensive task. A promising solution to such “AI-hard” tasks is collective intelligence approaches that aggregate opinions of a group of human non-experts or semi-experts. However, the existing aggregation methods rely only on synthetic accessibility evaluation scores given by humans, and they do not exploit auxiliary information obtained as byproducts of human evaluations such as that related to chemical structures. In this paper, we propose to exploit such auxiliary responses to obtain better aggregations. We introduce a new two-stage aggregation method of semi-expert judgments consisting of synthetic accessibility evaluation scores along with auxiliary responses that select substructures of targets obstructive to their synthesis. The first stage divides both semi-experts and substructures into clusters using stochastic block models to identify similar skills or properties. The second stage aggregates judgments while considering groups of semi-experts and substructures, and predicts synthetic accessibility. Our experiments show that the use of auxiliary responses improves the prediction performance and gives insight into evaluators and the structure of evaluated compounds.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419308231",
    "keywords": [
      "Computer science"
    ],
    "authors": [
      {
        "surname": "Ito",
        "given_name": "Shun"
      },
      {
        "surname": "Baba",
        "given_name": "Yukino"
      },
      {
        "surname": "Isomura",
        "given_name": "Tetsu"
      },
      {
        "surname": "Kashima",
        "given_name": "Hisashi"
      }
    ]
  },
  {
    "title": "A real-time map merging strategy for robust collaborative reconstruction of unknown environments",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113109",
    "abstract": "The development of collaborative techniques for exploring and mapping environments has been rising in the last decade. These techniques, known as multi-robot SLAM (MRSLAM), aim to extend the use of autonomous mobile robots to autonomous multi-agent systems. The MRSLAM technique presented here consists mainly of a robust map merging algorithm and a decision-making algorithm that controls agents in the field. On the one hand, the proposed merging algorithm performs a consistent and robust map fusion in real time. It consists of an own corner detector, a cylindrical descriptor, a matching technique and the RANSAC algorithm. On the other hand, once the fusion of maps is performed, the decision-making algorithm is responsible for controlling the robot operation in the field, based on the general current state of the multi-robot system. The main contribution of this MRSLAM technique is the robust map merging algorithm, since it was implemented and validated in simulated and real scenarios, resulting in collaborative maps that are consistent with the environment and obtained in less than 280 ms. This technique also achieves a significant decrease in reconstruction time when two or three robots are used: up to 35% in a simulated scenario and up to 49% in a real one. The proposed MRSLAM technique shows important similarities to expert multi-agent systems, as it is able to control and organize a team of robots in order to collaboratively explore and map an unknown environment. This approach was developed under the ROS framework to be used and tested by the scientific and academic community.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419308267",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Field (mathematics)",
      "Global Map",
      "Image (mathematics)",
      "Matching (statistics)",
      "Mathematics",
      "Mobile robot",
      "Pure mathematics",
      "RANSAC",
      "Robot",
      "Sensor fusion",
      "Simultaneous localization and mapping",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Velásquez Hernández",
        "given_name": "Carlos Alberto"
      },
      {
        "surname": "Prieto Ortiz",
        "given_name": "Flavio Augusto"
      }
    ]
  },
  {
    "title": "TDR: Two-stage deep recommendation model based on mSDA and DNN",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113116",
    "abstract": "Recently, deep learning techniques have been widely used in recommendation tasks and have attained record performance. However, the input quality of the deep learning model has a great influence on the recommendation performance. In this work, an efficient and effective input optimization method is proposed. Specifically, we propose an integrated recommendation framework based on two-stage deep learning. In the first stage, with user and item features as the original input, a low-cost marginalized stacked denoising auto-encoder (mSDA) model is used to learn the latent factors of users and items. In the second stage, the resulting latent factors are combined and used as input vector to the DNN model for fast and accurate prediction. Using the latent factor vector as the input to the deep learning-based recommendation model not only captures the high-order feature interaction, but also reduces the burden of the hidden layer, and also avoids the model training falling into local optimum. Extensive experiments with real-world datasets show that the proposed model shows much better performance than the state-of-the-art recommendation methods in terms of prediction accuracy, parameter space and training speed.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419308334",
    "keywords": [
      "Artificial intelligence",
      "Autoencoder",
      "Computer science",
      "Deep learning",
      "Feature (linguistics)",
      "Feature vector",
      "Linguistics",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Recommender system"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Ruiqin"
      },
      {
        "surname": "Jiang",
        "given_name": "Yunliang"
      },
      {
        "surname": "Lou",
        "given_name": "Jungang"
      }
    ]
  },
  {
    "title": "A new approach to solve opinion dynamics on complex networks",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113132",
    "abstract": "In this paper, we use the differential equations to establish the continuous opinion models on the complex networks, so as to observe how the nodes holding different opinions influence each other and how the opinion dynamics evolve. One of nodes in the complex network is selected randomly as a social outcast who opposes the opinions of other nodes and is reacted by the opponents equally. The analytical framework of dynamics on weighted-directed complex networks is proposed. We investigate how the strength and position of social outcast impact the opinion evolution. Then, we analyze how the network structures impact the result of dynamical process. Meanwhile, we propose a periodic description and an analytical method to solve the equation-based models. Finally, a series of experiments are conducted on five kinds of typical weighted-directed complex networks to demonstrate the feasibility and effectiveness of our proposed approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419308498",
    "keywords": [
      "Acoustics",
      "Artificial intelligence",
      "Complex dynamics",
      "Complex network",
      "Complex system",
      "Computer science",
      "Dynamics (music)",
      "Economics",
      "Finance",
      "Law",
      "Mathematical analysis",
      "Mathematics",
      "Operating system",
      "Opinion leadership",
      "Physics",
      "Political science",
      "Position (finance)",
      "Process (computing)",
      "Social media",
      "Social network (sociolinguistics)",
      "Theoretical computer science",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Wen-Ting"
      },
      {
        "surname": "He",
        "given_name": "Yu-Lin"
      },
      {
        "surname": "Huang",
        "given_name": "Joshua Zhexue"
      },
      {
        "surname": "Ma",
        "given_name": "Li-Heng"
      }
    ]
  },
  {
    "title": "Use of support vector machines with a parallel local search algorithm for data classification and feature selection",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113133",
    "abstract": "Over the last decade, the number of studies on machine learning has significantly increased. One of the most widely researched areas of machine learning is data classification. Most big data systems require a large amount of information storage for analytic purposes; however, this involves some disadvantages, such as the costs of processing and collecting data. Thus, many researchers and practitioners are working on effectively reducing the number of features used in classification. This paper proposes a method which jointly optimizes both feature selection and classification. A survey of the relevant literature shows that the vast majority of studies focus on either feature selection or classification. In this study, the proposed parallel local search algorithm both selects features and finds a classifier with high rates of accuracy. Moreover, the proposed method is capable of finding solutions for problems that have extremely high numbers of features within a reasonable computation time.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419308504",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Big data",
      "Classifier (UML)",
      "Computation",
      "Computer science",
      "Data classification",
      "Data mining",
      "Feature (linguistics)",
      "Feature selection",
      "Focus (optics)",
      "Linguistics",
      "Machine learning",
      "Optics",
      "Philosophy",
      "Physics",
      "Selection (genetic algorithm)",
      "Statistical classification",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Cura",
        "given_name": "Tunchan"
      }
    ]
  },
  {
    "title": "A new technique with improved control quality of nonlinear systems using an optimized fuzzy logic controller",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113148",
    "abstract": "Fuzzy logic controllers are increasingly applied to control complex systems because they have several advantages. The objective of this work is to propose a new technique to optimize a Takagi-Sugeno fuzzy logic controller with quality using the Non-dominated Sorting Genetic Algorithm-II by optimizing three objectives functions which are a cost function, the number of fuzzy inference rules, and the maximum instantaneous quadratic error. In this technique, the Multi-Criteria Decision-Making approach is used to choose one of the best controllers from the Pareto set of the last generation of the genetic algorithm. The proposed technique ensure: (i) that the output of the controlled system correctly follows the desired reference. (ii) the acceleration of the control process and (iii) avoid the existence of large overshoots; which are usually observed when applying commands to complex processes with variable behavior. At the end of the control process, a robustness test is performed to verify the efficiency of the proposed technique. It is shown here that the optimization of the third objective function, allows the improvement of the control quality. This new technique can be used to improve expert and intelligent systems based on fuzzy rules to control high complex systems with variable behavior which they have disturbing overshoots during their control. This technique; allows to accelerate the calculation of the control law of expert systems based on fuzzy rules; while ensuring that the quality of the control and the output signal are good.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419308656",
    "keywords": [
      "Adaptive neuro fuzzy inference system",
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Control (management)",
      "Control theory (sociology)",
      "Defuzzification",
      "Fuzzy control system",
      "Fuzzy logic",
      "Fuzzy number",
      "Fuzzy set",
      "Gene",
      "Mathematical optimization",
      "Mathematics",
      "Robustness (evolution)",
      "Sorting"
    ],
    "authors": [
      {
        "surname": "Lamamra",
        "given_name": "Kheireddine"
      },
      {
        "surname": "Batat",
        "given_name": "Farida"
      },
      {
        "surname": "Mokhtari",
        "given_name": "Fouad"
      }
    ]
  },
  {
    "title": "Splitting the fitness and penalty factor for temporal diversity increase in practical problem solving",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113126",
    "abstract": "In this paper, we propose an optimization system based on an evolutionary algorithm developed to solve a real-life optimization problem related to the optimization of the computer networks. In particular, we focus on an NP-hard flow allocation problem in computer networks related to network survivability and addressing various constraints specific to computer networks. The constrained problems are usually handled by evolutionary methods by the introduction of the so-called penalty factor. However, such techniques raise difficulties regardless of their simple working principle. The more oppressive the penalty factor is, the more likely the evolutionary method is to stuck in the local optima with feasible solutions and propose final results of low-quality. On the other hand, a low penalty factor may lead the method to regions of high-rated solutions that violate the constraints and are useless due to their infeasibility. Therefore, we address the issue of penalty handling by proposing Ranking-based Fitness and Penalty Weighting (RFPW). RFPW is inspired by a theory-driven penalty parameter estimation using a bi-objective and weighted sum approach. It separates the objective and penalty functions and removes the necessity of hand-made weighting. RFPW was introduced to the recent proposition of an evolutionary method dedicated to solving hard NP-complete practical problem. The performed experiments related to the flow allocation problem confirm that employing RFPW may lead to a significant improvement in results quality. In our opinion, the proposed RFPW framework of the penalty function handling has the potential to be adapted to a wide range of optimization systems that utilize the idea of penalty function. RFPW allows the optimization method to reduce or increase the strength of a convergence automatically. Therefore, RFPW has the potential of results quality improvement and may replace expensive commonly-used diversity preservation techniques (e.g., island models).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419308437",
    "keywords": [
      "Artificial intelligence",
      "Computer network",
      "Computer science",
      "Epistemology",
      "Evolutionary algorithm",
      "Factor (programming language)",
      "Mathematical optimization",
      "Mathematics",
      "Medicine",
      "Optimization problem",
      "Penalty method",
      "Philosophy",
      "Programming language",
      "Quality (philosophy)",
      "Radiology",
      "Ranking (information retrieval)",
      "Survivability",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Przewozniczek",
        "given_name": "Michal Witold"
      },
      {
        "surname": "Datta",
        "given_name": "Rituparna"
      },
      {
        "surname": "Walkowiak",
        "given_name": "Krzysztof"
      },
      {
        "surname": "Komarnicki",
        "given_name": "Marcin Michal"
      }
    ]
  },
  {
    "title": "A hybrid approach for portfolio selection with higher-order moments: Empirical evidence from Shanghai Stock Exchange",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113104",
    "abstract": "Skewness and kurtosis, the third and fourth order moments, are statistics to summarize the shape of a distribution function. Recent studies show that investors would take these higher-order moments into consideration to make a profitable investment decision. Unfortunately, due to the difficulties in solving the multi-objective problem with higher-order moments, the literature on portfolio selection problem with higher-order moments is few. This paper proposes a new hybrid approach to solve the portfolio selection problem with skewness and kurtosis, which includes not only the multi-objective optimization but also the data-driven asset selection and return prediction, where the techniques of two-stage clustering, radial basis function neural network and genetic algorithm are employed. With the historical data from Shanghai stock exchange, we find that the out-of-sample performance of our model with higher-order moments is significantly better than that of traditional mean-variance model and verify the robustness of our hybrid algorithm.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419308218",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Cluster analysis",
      "Computer science",
      "Econometrics",
      "Economics",
      "Finance",
      "Gene",
      "Kurtosis",
      "Mathematical optimization",
      "Mathematics",
      "Portfolio",
      "Portfolio optimization",
      "Robustness (evolution)",
      "Skewness",
      "Statistics",
      "Stock exchange"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Bilian"
      },
      {
        "surname": "Zhong",
        "given_name": "Jingdong"
      },
      {
        "surname": "Chen",
        "given_name": "Yuanyuan"
      }
    ]
  },
  {
    "title": "Market segmentation using high-dimensional sparse consumers data",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113136",
    "abstract": "Good segmentation contributes towards a better understanding of the market and customer demands. This study aims to develop a new methodological approach, integrating “Recency, Frequency and Monetary” with the sparse K-means clustering algorithm of Witten and Tibshirani (2010). The proposed approach is suitable for handling large, high-dimensional and sparse consumer data. Drawing on the proposed methodology, alongside data collection from the Chinese mobile telecommunications market, and considering specific services, our treatment is further assessed empirically and appears to provide robust results when compared to the Dolnicar, Kaiser, Lazarevski and Leisch (2012) biclustering of customers method. Following the attainment of a clear and robust market segmentation structure, our theoretical treatment and its empirical analysis provide a useful tool and valid methodology for marketers, and decision makers in general, to accurately determine the most profitable market segments.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741741930853X",
    "keywords": [
      "Artificial intelligence",
      "Business",
      "Computer science",
      "Machine learning",
      "Market segmentation",
      "Marketing",
      "Pattern recognition (psychology)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Jian"
      },
      {
        "surname": "Zhai",
        "given_name": "Linli"
      },
      {
        "surname": "Pantelous",
        "given_name": "Athanasios A."
      }
    ]
  },
  {
    "title": "Sensitivity analysis of Bayesian networks to parameters of the conditional probability model using a Beta regression approach",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113130",
    "abstract": "Ensuring the validity and credibility of Bayesian Belief Network (BBN) as a modelling tool for expert systems requires appropriate methods for sensitivity analysis (SA), in order to test the robustness of the BBN diagnostic and prognostic with respect to the parameterisation of the conditional probability model (CPM). Yet, the most widely used techniques (based on sensitivity functions for discrete BBNs) only provide a local insight on the CPM influence, i.e. by varying only one CPM parameter at a time (or a few of them) while keeping the other ones unchanged. To overcome this limitation, the present study proposes an approach for global SA relying on Beta Regression using gradient boosting (potentially combined with stability selection analysis): it presents the benefit of keeping the presentation intuitive through a graph-based approach, while being applicable to a large number of CPM parameters. The implementation of this approach is investigated for three cases, which cover a large spectrum of situations: (1) a small discrete BBN, used to capture medical knowledge, demonstrates the proposed approach; (2) a linear Gaussian BBN, used to assess the damage of reinforced concrete structures, exemplifies a case where the number of parameters is too large to be easily processed and interpreted (>40 parameters); (3) a discrete BBN, used for reliability analysis of nuclear power plant, exemplifies a case where analytical solutions for sensitivity can hardly be derived. Finally, provided that the validity of the BBR model is carefully checked, we show that the proposed approach can provide richer information than traditional SA methods at different levels: (i) it selects the most influential parameters; (ii) it provides the functional relation between the CPM parameter and the result of the probabilistic query; and (iii) it identifies how the CPM parameters can lead to situations of high probability, while quantifying the confidence in the occurrence of these situations.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419308474",
    "keywords": [
      "Artificial intelligence",
      "Bayesian network",
      "Bayesian probability",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Data mining",
      "Electronic engineering",
      "Engineering",
      "Gene",
      "Machine learning",
      "Posterior probability",
      "Robustness (evolution)",
      "Sensitivity (control systems)"
    ],
    "authors": [
      {
        "surname": "Rohmer",
        "given_name": "Jeremy"
      },
      {
        "surname": "Gehl",
        "given_name": "Pierre"
      }
    ]
  },
  {
    "title": "Improved Salp Swarm Algorithm based on opposition based learning and novel local search algorithm for feature selection",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113122",
    "abstract": "Many fields such as data science, data mining suffered from the rapid growth of data volume and high data dimensionality. The main problems which are faced by these fields include the high computational cost, memory cost, and low accuracy performance. These problems will occur because these fields are mainly used machine learning classifiers. However, machine learning accuracy is affected by the noisy and irrelevant features. In addition, the computational and memory cost of the machine learning is mainly affected by the size of the used datasets. Thus, to solve these problems, feature selection can be used to select optimal subset of features and reduce the data dimensionality. Feature selection represents an important preprocessing step in many intelligent and expert systems such as intrusion detection, disease prediction, and sentiment analysis. An improved version of Salp Swarm Algorithm (ISSA) is proposed in this study to solve feature selection problems and select the optimal subset of features in wrapper-mode. Two main improvements were included into the original SSA algorithm to alleviate its drawbacks and adapt it for feature selection problems. The first improvement includes the use of Opposition Based Learning (OBL) at initialization phase of SSA to improve its population diversity in the search space. The second improvement includes the development and use of new Local Search Algorithm with SSA to improve its exploitation. To confirm and validate the performance of the proposed improved SSA (ISSA), ISSA was applied on 18 datasets from UCI repository. In addition, ISSA was compared with four well-known optimization algorithms such as Genetic Algorithm, Particle Swarm Optimization, Grasshopper Optimization Algorithm, and Ant Lion Optimizer. In these experiments four different assessment criteria were used. The rdemonstrate that ISSA outperforms all baseline algorithms in terms of fitness values, accuracy, convergence curves, and feature reduction in most of the used datasets. The wrapper feature selection mode can be used in different application areas of expert and intelligent systems and this is confirmed from the obtained results over different types of datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419308395",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Curse of dimensionality",
      "Data mining",
      "Data pre-processing",
      "Dimensionality reduction",
      "Feature selection",
      "Initialization",
      "Machine learning",
      "Preprocessor",
      "Programming language"
    ],
    "authors": [
      {
        "surname": "Tubishat",
        "given_name": "Mohammad"
      },
      {
        "surname": "Idris",
        "given_name": "Norisma"
      },
      {
        "surname": "Shuib",
        "given_name": "Liyana"
      },
      {
        "surname": "Abushariah",
        "given_name": "Mohammad A.M."
      },
      {
        "surname": "Mirjalili",
        "given_name": "Seyedali"
      }
    ]
  },
  {
    "title": "Robust adaptive multivariate Hotelling's T 2 control chart based on kernel density estimation for intrusion detection system",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113105",
    "abstract": "The utilization of conventional multivariate control chart in network intrusion detection will deal with two main problems. First, the high false alarm occurs due to the distribution of network traffic data that is not following the theory. Second, the inability of the control chart to detect outliers caused by the masking effect. To overcome these problems, the multivariate control chart based on the fast minimum covariance determinant (MCD) algorithm and kernel density estimation (KDE) is proposed in this paper. The employment of KDE technique is expected to adaptively follow the network traffic data pattern, thereby reducing the occurrence of false alarms. Meanwhile, the usage of Fast-MCD will improve the capabilities of the proposed control chart to quickly and accurately detect the outliers. For the simulated data, the proposed chart shows a better level of accuracy when it is compared to conventional T 2 and other robust T 2 based on successive difference covariate matrix (SDSM) charts. For the data generated from some distributions, the proposed chart shows its adaptability by producing low false alarm with high detection rate. The proposed chart shows excellent performance to monitor the KDD99 dataset with 98.61% accuracy, NSL-KDD dataset with 91.71% accuracy, and UNSW-NB 15 dataset with 91.02% accuracy. The proposed method has consistent performance when monitoring the small subset of the datasets, which can minimize the computational time by more than 90% without decreasing its level of accuracy and precision. Also, the performance from the proposed chart surpasses the other benchmarks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741741930822X",
    "keywords": [
      "Anomaly detection",
      "Artificial intelligence",
      "Chart",
      "Computer science",
      "Constant false alarm rate",
      "Control chart",
      "Data mining",
      "EWMA chart",
      "Estimator",
      "False alarm",
      "Intrusion detection system",
      "Kernel density estimation",
      "Machine learning",
      "Mathematics",
      "Multivariate statistics",
      "Operating system",
      "Outlier",
      "Process (computing)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Ahsan",
        "given_name": "Muhammad"
      },
      {
        "surname": "Mashuri",
        "given_name": "Muhammad"
      },
      {
        "surname": "Lee",
        "given_name": "Muhammad Hisyam"
      },
      {
        "surname": "Kuswanto",
        "given_name": "Heri"
      },
      {
        "surname": "Prastyo",
        "given_name": "Dedy Dwi"
      }
    ]
  },
  {
    "title": "Fuzzy optimization model for electric vehicle routing problem with time windows and recharging stations",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113123",
    "abstract": "As fuel prices increase and emission regulations become increasingly strict, electric vehicles have been used in various logistics distribution activities. Most studies have focused on the electric vehicle routing problem under a deterministic environment, neglecting the effects of uncertain factors in practical logistics distribution. Thus, a novel fuzzy electric vehicle routing problem with time windows and recharging stations (FEVRPTW) is investigated in this study, and a fuzzy optimization model is established based on credibility theory for this problem. In the presented model, fuzzy numbers are used to denote the uncertainties of service time, battery energy consumption, and travel time. Moreover, the partial recharge is allowed under the uncertain environment. To solve the model, an adaptive large neighborhood search (ALNS) algorithm enhanced with the fuzzy simulation method is proposed. In the proposed ALNS algorithm, four new removal algorithms are designed and integrated for addressing the FEVRPTW. To further improve the algorithmic performance, the variable neighborhood descent algorithm is embedded into the proposed ALNS algorithm and five local search operators are applied. The experiments were conducted to verify the effectiveness of the proposed ALNS algorithm for solving the presented model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419308401",
    "keywords": [
      "Artificial intelligence",
      "Computer network",
      "Computer science",
      "Credibility theory",
      "Electric vehicle",
      "Fuzzy logic",
      "Mathematical optimization",
      "Mathematics",
      "Physics",
      "Power (physics)",
      "Quantum mechanics",
      "Routing (electronic design automation)",
      "Vehicle routing problem"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Shuai"
      },
      {
        "surname": "Chen",
        "given_name": "Mingzhou"
      },
      {
        "surname": "Zhang",
        "given_name": "Wenyu"
      },
      {
        "surname": "Zhuang",
        "given_name": "Xiaoyu"
      }
    ]
  },
  {
    "title": "Iterated greedy with variable neighborhood search for a multiobjective waste collection problem",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113101",
    "abstract": "In the last few years, the application of decision making to logistic problems has become crucial for public and private organizations. Efficient decisions clearly contribute to improve operational aspects such as cost reduction or service improvement. The particular case of waste collection service considered in this paper involves a set of economic, labor and environmental issues that translate into difficult operational problems. They pose a challenge to nowadays optimization technologies since they have multiple constraints and multiple objectives that may be in conflict. We therefore need to resort to multiobjective approaches to model and solve this problem, providing efficient solutions in short computational times. In particular, we consider four different objectives to model the waste collection problem: travel cost, route length balance, route time balance, and number of routes. We propose an iterated greedy algorithm coupled with a variable neighborhood search to minimize an achievement function to determine a good approximation to the Pareto front. The performance of our method is empirically analyzed on a set of instances (both generated and real), and compared with the well-known NSGA-II and SPEA2 methods. The comparison favors our proposal.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419308188",
    "keywords": [
      "Computer science",
      "Economics",
      "Economy",
      "Greedy algorithm",
      "Iterated local search",
      "Local search (optimization)",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Multi-objective optimization",
      "Operations research",
      "Pareto principle",
      "Programming language",
      "Service (business)",
      "Set (abstract data type)",
      "Variable (mathematics)"
    ],
    "authors": [
      {
        "surname": "Delgado-Antequera",
        "given_name": "Laura"
      },
      {
        "surname": "Caballero",
        "given_name": "Rafael"
      },
      {
        "surname": "Sánchez-Oro",
        "given_name": "Jesús"
      },
      {
        "surname": "Colmenar",
        "given_name": "J Manuel"
      },
      {
        "surname": "Martí",
        "given_name": "Rafael"
      }
    ]
  },
  {
    "title": "Hybrid enhanced discrete fruit fly optimization algorithm for scheduling blocking flow-shop in distributed environment",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113147",
    "abstract": "Scheduling in distributed production environments is becoming widespread in recent years due to the increasing advantages of multi-factory manufacture. This paper investigates the distributed blocking flow-shop scheduling problem (DBFSP) with the objective of minimizing the makespan. To solve this problem, a hybrid enhanced discrete fruit fly optimization algorithm (HEDFOA) is proposed. In the proposed algorithm, an effective constructive heuristic is developed based on a new assignment rule of jobs and an insertion-based improvement procedure to initialize the common central location of all fruit fly swarms. In the smell-based foraging, an effective insertion-based neighborhood operator is designed for exploration in global scope. In the vision-based foraging, a local search is embedded to intensify the exploitation ability of algorithm in local region. Meanwhile, a simulated annealing-like acceptance criterion is employed to help algorithm escape from the local optimum. Finally, an extensive computational experiment is conducted. Experimental results show that the proposed HEDFOA is more effective than the existing state-of-the-art methods. Furthermore, 516 best known solutions out of 720 benchmark instances are also updated.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419308644",
    "keywords": [
      "Algorithm",
      "Benchmark (surveying)",
      "Biology",
      "Computer science",
      "Distributed computing",
      "Ecology",
      "Flow shop scheduling",
      "Foraging",
      "Geodesy",
      "Geography",
      "Job shop scheduling",
      "Local search (optimization)",
      "Mathematical optimization",
      "Mathematics",
      "Operating system",
      "Schedule",
      "Scheduling (production processes)",
      "Simulated annealing"
    ],
    "authors": [
      {
        "surname": "Shao",
        "given_name": "Zhongshi"
      },
      {
        "surname": "Pi",
        "given_name": "Dechang"
      },
      {
        "surname": "Shao",
        "given_name": "Weishi"
      }
    ]
  },
  {
    "title": "A new clustering method based on morphological operations",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113102",
    "abstract": "With the booming development of data science, many clustering methods have been proposed. All clustering methods have inherent merits and deficiencies. Therefore, they are only capable of clustering some specific types of data robustly. In addition, the accuracies of the clustering methods rely heavily on the characteristics of the data. In this paper, we propose a new clustering method based on the morphological operations. The morphological dilation is used to connect the data points based on their adjacency and form different connected domains. The iteration of the morphological dilation process stops when the number of connected domains equals the number of the clusters or when the maximum number of iteration is reached. The morphological labeling is then used to label the connected domains. The Euclidean distance between each data point and the points in each labeled connected domain is calculated. For each data point, there is a labeled connected domain that contains a point that yields the smallest Euclidean distance. The data point is assigned with the same labeling number as the labeled connected domain. We evaluate and compare the proposed method with state of the art clustering methods with different types of data. Experimental results are favorable.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741741930819X",
    "keywords": [
      "Adjacency list",
      "Algorithm",
      "Artificial intelligence",
      "Cluster analysis",
      "Clustering high-dimensional data",
      "Combinatorics",
      "Computer science",
      "Correlation clustering",
      "Data mining",
      "Data point",
      "Dilation (metric space)",
      "Domain (mathematical analysis)",
      "Euclidean distance",
      "Euclidean geometry",
      "Fuzzy clustering",
      "Geometry",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Point (geometry)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Zhenzhou"
      }
    ]
  },
  {
    "title": "A harmonic estimator design with evolutionary operators equipped grey wolf optimizer",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113125",
    "abstract": "Harmonic estimation is a challenging design problem in power networks. Accurate estimation of the inter, power and sub harmonics in networks can be a helpful aspect for designing potential solutions for elimination of these harmonics. Harmonic estimation design problem has been considered as an optimization problem with the amalgamation of least square algorithm in past. In this paper, we first propose an Evolutionary Operators Equipped Grey Wolf Optimizer (E-GWO). In this proposal a sinusoidal function enabled bridging is proposed and along with this tournament selection operator and crossover and mutation operation are incorporated at position updation phase. The variant is first benchmarked on latest CEC-2017 functions and then this design problem is addressed. After a meaningful comparison with the previously published approaches, we arrive at the conclusion that proposed modifications have positive implications on the performance of GWO. Proposed harmonic designs are robust when tested with different operating conditions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419308425",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Crossover",
      "Electric power system",
      "Electrical engineering",
      "Engineering",
      "Estimator",
      "Evolutionary algorithm",
      "Gene",
      "Harmonic",
      "Harmonics",
      "Mathematical optimization",
      "Mathematics",
      "Operator (biology)",
      "Optimization problem",
      "Physics",
      "Power (physics)",
      "Quantum mechanics",
      "Repressor",
      "Statistics",
      "Tournament selection",
      "Transcription factor",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Saxena",
        "given_name": "Akash"
      },
      {
        "surname": "Kumar",
        "given_name": "Rajesh"
      },
      {
        "surname": "Mirjalili",
        "given_name": "Seyedali"
      }
    ]
  },
  {
    "title": "Automated domain-specific healthcare knowledge graph curation framework: Subarachnoid hemorrhage as phenotype",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113120",
    "abstract": "To derive meaningful insights from voluminous healthcare data, it is essential to convert it into machine understandable knowledge. Currently, machine understandable domain specific healthcare knowledge curation framework does not exist for complex neurological diseases such as subarachnoid hemorrhage stroke. We envisage futuristic clinical decision support systems and tools backed with such knowledge will aide in complex neurological disease prognosis, diagnosis, and treatment. Existing knowledge graphs (KGs) only contain concepts and relationships between them and offer this knowledge to information extraction and knowledge management applications. However, the proposed domain-specific automated KG curation framework enables extraction of concepts, relationships, individual and cohort graphs, and predictive knowledge. By employing ontology-based information extraction, ensemble learning and word embedding based on skip-gram techniques on structured and unstructured data from electronic health records of 1025 patients with an intracranial aneurysm, this paper proposes a novel fully automated framework to curate knowledge graph, consisting of concepts, different hierarchical and non-hierarchical relationships, and predictive rules for prediction of subarachnoid hemorrhage. The evaluation shows that proposed framework achieves 78% precision and 71% recall respectively, for concept extraction from clinical text. Taxonomic relationships evaluation had precision and recall of 68%, and 95%, respectively. Evaluation of knowledge to predict unruptured status using validation dataset shows accuracy, precision, recall, of 73%, 76%, and 90% respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419308371",
    "keywords": [
      "Artificial intelligence",
      "Clinical decision support system",
      "Computer science",
      "Decision support system",
      "Domain knowledge",
      "Economic growth",
      "Economics",
      "Epistemology",
      "Health care",
      "Information extraction",
      "Internal medicine",
      "Knowledge extraction",
      "Knowledge graph",
      "Linguistics",
      "Machine learning",
      "Medicine",
      "Natural language processing",
      "Ontology",
      "Philosophy",
      "Precision and recall",
      "Recall",
      "Subarachnoid hemorrhage"
    ],
    "authors": [
      {
        "surname": "Malik",
        "given_name": "Khalid Mahmood"
      },
      {
        "surname": "Krishnamurthy",
        "given_name": "Madan"
      },
      {
        "surname": "Alobaidi",
        "given_name": "Mazen"
      },
      {
        "surname": "Hussain",
        "given_name": "Maqbool"
      },
      {
        "surname": "Alam",
        "given_name": "Fakhare"
      },
      {
        "surname": "Malik",
        "given_name": "Ghaus"
      }
    ]
  },
  {
    "title": "A multiple search strategies based grey wolf optimizer for solving multi-objective optimization problems",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113134",
    "abstract": "In this paper, a novel multi-objective grey wolf optimizer (MOGWO) based on multiple search strategies (i.e., adaptive chaotic mutation strategy, boundary mutation strategy, and elitism strategy) which we shall call MMOGWO is proposed to solve multi-objective optimization problems (MOPs). The algorithm uses a fixed-sized external archive that is adaptively maintained according to a grid-based approach to preserve the non-dominated solutions found during the search process. Then, the archive is used to define the social hierarchy and simulate the hunting behaviors of grey wolves. In the proposed algorithm, an adaptive chaotic mutation strategy based on a chaotic cubic map and modified generational distance (GD) is applied to the archive to dynamically adjust the convergence speed and balance the exploration and exploitation. To prevent the population diversity loss, a boundary mutation strategy based on the concept of multi-level parallel is employed to manage boundary constraint violations. Moreover, a non-dominated sorting and crowding distance-based elitism strategy is also incorporated into the algorithm for exploiting more potential Pareto optimal solutions and preserve the diversity of solutions in the approximated set. The proposed algorithm is evaluated on a wide range of multi-objective optimization problems (MOPs), and compared with other state-of-the-art multi-objective optimization algorithms in terms of often-used performance metrics with the help of statistical analysis, average ranks test and Wilcoxon Signed-Rank Test (WSRT). It is revealed by the experimental results that the algorithm is highly competitive and significantly outperforms other well-known algorithms on most of the test problems. On obtaining satisfactory performance for test problems, to investigate the performance of the MMOGWO for solving real-world optimization problems with various constraints, MMOGWO is further applied to handle the multi-objective optimal scheduling problem (MOOSP) of cascade hydropower stations (CHSs) based on a novel constraints handling method designed in this paper. Simulation results indicate that, compared with other algorithms, MMOGWO can produce better quality solutions and it can be considered as a promising alternative tool to deal with multi-objective real-life engineering problems with complex constraints by equipping with constraints handling methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419308516",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chaotic",
      "Chemistry",
      "Composite material",
      "Computer science",
      "Demography",
      "Gene",
      "Local search (optimization)",
      "Materials science",
      "Mathematical optimization",
      "Mathematics",
      "Multi-objective optimization",
      "Mutation",
      "Optimization problem",
      "Particle swarm optimization",
      "Population",
      "Range (aeronautics)",
      "Sociology",
      "Sorting"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Junfeng"
      },
      {
        "surname": "Yang",
        "given_name": "Zhe"
      },
      {
        "surname": "Li",
        "given_name": "Dingfang"
      }
    ]
  },
  {
    "title": "Executive summaries of uncertain values close to the gain/loss threshold – linguistic modelling perspective",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113108",
    "abstract": "In this paper we propose a novel method for the assessment of linguistic approximation of fuzzy outputs of decision-support and evaluation models in the presence of thresholds. The method provides graphical and numerical summaries of performance of different distance/similarity measures in combination with various linguistic scales in the process of assigning linguistic labels to the outputs of expert systems and decision-support models. We assume the existence of a specific threshold on the output scale that splits the outputs in two categories, i.e. gains/losses, acceptable/unacceptable values, better/worse than average values etc. This way a framing of the outputs can be obtained by labelling them linguistically. We consider numerical outputs in monetary units and assume zero to be the threshold value, splitting the universe into gains and losses. Based on a numerical analysis and yet without the knowledge of the most fitting linguistic label, the proposed analytical method is able to identify the cases where a clearly incorrect label is assigned (a loss label for a gain an/or vice versa) and hence the combinations of linguistic scales and distance/similarity measures of fuzzy numbers not to be used for the given purpose. We can also analyze specific features of some similarity/distance and linguistic scale combinations. The proposed method and its outputs is intended for the design of such expert systems and decision-support models, where a linguistic level of communicating the results to the users of these models is of importance, e.g., for the creation of executive summaries of outputs of mathematical models and results of financial data analyses. The method brings together the mathematical analysis of the linguistic approximation tools and the behavioral aspect of framing of the outputs e.g., as gains or losses prior to the final decision-making step. This way it provides much need guidance for the selection of reasonable distance/similarity measures of fuzzy numbers and reasonable linguistic scale for linguistic approximation. As such it is a useful tool for the design of system-user interfaces including a linguistic level of description.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419308255",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Fuzzy logic",
      "Image (mathematics)",
      "Machine learning",
      "Mathematics",
      "Perspective (graphical)",
      "Physics",
      "Quantum mechanics",
      "Scale (ratio)",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Stoklasa",
        "given_name": "Jan"
      },
      {
        "surname": "Talášek",
        "given_name": "Tomáš"
      },
      {
        "surname": "Stoklasová",
        "given_name": "Jana"
      }
    ]
  },
  {
    "title": "Automatic detonator code recognition via deep neural network",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113121",
    "abstract": "Detonators are hazardous and must be strictly controlled to strengthen public security since they contain explosive. To deal with the stressful management of detonators by manually recording detonator codes, an expert and intelligent system for detonator code recognition is an alternative management scheme for automatically recording detonator codes. How to achieve accurate and reliable recognition performance is also a challenging problem in such an intelligent system. In this study, we develop an intelligent vision-based system based on deep learning for identifying detonator codes. Specifically, we design a detonator image acquisition subsystem to construct the detonator code image dataset, and an intelligent image processing subsystem named as automatic detonator code recognition network (ADCR-Net). The ADCR-Net is a pipeline cascading a deep localization network and a deep recognition network. The multi-scale concatenation block and the features integration module are proposed for the localization network of the ADCR-Net to improve the features expressions. The multi-level progressive self-attention block is put forward for the recognition network of the ADCR-Net to help with focusing on multi-level activation maps. Also, a multi-label loss function for the recognition network is designed to balance the significance of the classifiers of the intelligent system during training. Experiments on our released dataset demonstrate the effectiveness and efficiency of the proposed expert and intelligent system, which achieves 99.18% accuracy in end-to-end recognition. Our work provides an effective resolution to the expert and intelligent system for automatic detonator code recognition. Also, we provide an alternative framework to deal with the tradeoff between inference time and recognition accuracy in deep learning which is becoming a popular method in many expert and intelligent systems. Furthermore, the released dataset is available for distribution to the related researchers to develop their expert systems in the field of industrial optical character recognition.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419308383",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Block (permutation group theory)",
      "Chemistry",
      "Code (set theory)",
      "Computer science",
      "Data mining",
      "Deep learning",
      "Detonator",
      "Explosive material",
      "Geometry",
      "Machine learning",
      "Mathematics",
      "Operating system",
      "Organic chemistry",
      "Pipeline (software)",
      "Programming language",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Jixiu"
      },
      {
        "surname": "Cai",
        "given_name": "Nian"
      },
      {
        "surname": "Li",
        "given_name": "Feiyang"
      },
      {
        "surname": "Jiang",
        "given_name": "Huiwen"
      },
      {
        "surname": "Wang",
        "given_name": "Han"
      }
    ]
  },
  {
    "title": "An Efficient JAYA Algorithm with Lévy Flight for Non-linear Channel Equalization",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112970",
    "abstract": "Neural network (NN) based equalizers are known to outperform the linear equalizers based on finite impulse response (FIR) adaptive filter for highly dispersive and non-linear channels. To overcome the limitations of the back-propagation (BP) algorithm, metaheuristic algorithms are emerging as promising alternatives for training the NN based equalizers. JAYA is a simple and efficient metaheuristic algorithm. Hence, its application to channel equalization problem is worth investigating. Despite its simplicity and efficiency, the JAYA algorithm has problems such as being trapped in local minima due to insufficient diversity of population and weak exploration capability. To alleviate these issues, in this paper the concept of Lévy flight (LF) and greedy selection scheme has been incorporated into the basic JAYA algorithm. The LF concept enhances the population diversity and thus avoids the state of stagnation. The greedy selection scheme is employed to improve the exploitation ability without loss of population diversity. Furthermore, in order to maintain the balance between the exploration and exploitation capabilities of the algorithm, an adaptive Lévy index is proposed based on a linear control parameter strategy. An extensive simulation-based sensitivity analysis of proposed method called JAYA algorithm with Lévy flight (JAYALF) with respect to key parameters is carried out to select the optimized values for these parameters. In order to validate the local optima avoidance ability, exploitation and convergence rate of the proposed JAYALF algorithm, it is tested on seventeen well-known unimodal and multimodal benchmark functions and to verify the effectiveness of the JAYALF for non-linear channel equalization problem, three wireless communication channels with two different nonlinearities have been considered for simulation. In addition, the non-parametric pairwise Wilcoxon rank-sum test has been employed to test the statistical validity of the results obtained from JAYALF. The results of experiments and statistical test demonstrate that the proposed algorithm significantly outperforms JAYA, variants of JAYA, state-of-the-art algorithms and BP algorithm in terms of solution quality, convergence speed, and robustness. Furthermore, the results of experimental analyses conducted indicate that proposed JAYALF algorithm has a better exploration ability and converges quickly without getting trapped into local optima.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419306888",
    "keywords": [
      "Algorithm",
      "Benchmark (surveying)",
      "Channel (broadcasting)",
      "Computer network",
      "Computer science",
      "Demography",
      "Geodesy",
      "Geography",
      "Local optimum",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Maxima and minima",
      "Metaheuristic",
      "Population",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Ingle",
        "given_name": "Kishor Kisan"
      },
      {
        "surname": "Jatoth",
        "given_name": "Dr. Ravi Kumar"
      }
    ]
  },
  {
    "title": "A simulation-optimisation genetic algorithm approach to product allocation in vending machine systems",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113110",
    "abstract": "In recent years, vending machines have seen increasing levels of popularity. In a fast-paced world where convenience and accessibility of products is highly sought after the vending industry has provided a suitable solution. Although the economic impact of the vending industry is indisputable, it is not without challenges, especially when it comes to the efficiency of the vending logistics operations. The optimisation of logistic vending machine systems is decidedly complex. Product allocation to columns in a vending machine, replenishment points of products, product thresholds at vending machines, and vehicle routes for inventory replenishments are all essential challenges in vending machine system management and operation. If all facets of the problem were to be addressed, it would require techniques such as forecasting, machine learning, data mining, combinatorial optimization and vehicle routing, among others. In the past, these approaches have been explored individually despite their intrinsic interdependence within the problem. This paper aims to help to fill in this gap and proposes a model for the optimisation of product allocation within a vending machine under the constraint of fixed restocking instances. The optimal product allocation is based on the definition of product profitability which accounts for the net revenue earned after the cost of restock, as opposed to the revenue earned until first stock-out to prevent arbitrary extension of the stock-out period. The whole approach is encompassed in the simulation-optimisation framework that utilises a Genetic Algorithm, with fitness evaluated as simulated revenue, to determine the optimal product allocation. The acceptable threshold of missed sales for a machine is also determined as a means to make intelligent restocking decisions. Overall, the proposed approach allows the strengths of mathematically robust optimization algorithms and the implementation of analytic solutions to be combined and applied to realistic scenarios where uncertainty may rule out some high quality analytic solutions. It respects problem intricacies proper to vending and addresses the interdependence between routing and portfolio optimisation. The proposal is application-driven and stems from a collaboration with an industry partner. The model is validated against an authentic data set supplied by the partner. The case study results revealed a network-wide improvement in net revenue of approximately 3.4%, with varied efficacy based on machine popularity. The method of optimisation was found to be significantly more effective for higher performing machines, with median improvements as high as 6%. Our framework based on the optimization-simulation model yields clear benefits to vending logistics operations management. The simulation component provides the decision maker with a more comprehensive view on the actual implementation of the solution. Effectively, the joint use of simulation and optimization methods provides managers with enhanced information to help decide on both: (i) the most beneficial product portfolio, and (ii) the quality of the proposed restocking schedules. Simulation-optimisation based approach is a powerful technique used to address stochastic problems. However, it was yet to be applied specifically to logistic vending machine systems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419308279",
    "keywords": [
      "Archaeology",
      "Clothing",
      "Computer science",
      "Economics",
      "Engineering",
      "Fast fashion",
      "Finance",
      "Genetic algorithm",
      "Geometry",
      "History",
      "Machine learning",
      "Mathematics",
      "Operations research",
      "Product (mathematics)",
      "Profitability index",
      "Revenue"
    ],
    "authors": [
      {
        "surname": "Grzybowska",
        "given_name": "Hanna"
      },
      {
        "surname": "Kerferd",
        "given_name": "Briscoe"
      },
      {
        "surname": "Gretton",
        "given_name": "Charles"
      },
      {
        "surname": "Travis Waller",
        "given_name": "S."
      }
    ]
  },
  {
    "title": "Bottleneck feature supervised U-Net for pixel-wise liver and tumor segmentation",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113131",
    "abstract": "Liver cancer is one of the most common cancer types with high death rate. Doctors diagnose cancer by examining the CT images, which can be time-consuming and prone to error. Therefore, an automatic segmentation method is desired for clinical practice. In the literature, many U-Net-based models were proposed. But few of them focus on the bottleneck feature vectors, which are low dimensional representations of the input. In this paper, we propose a bottleneck feature supervised (BS) U-Net model and apply it to liver and tumor segmentation. Our main contributions are: (1) we propose a variation of the original U-Net that has better performance with a smaller number of parameters; (2) we propose a bottleneck feature supervised (BS) U-Net that contains an encoding U-Net and a segmentation U-Net. The encoding U-Net is first trained as an auto-encoder to get encodings of the label maps, which are subsequently used as additional supervision to train the segmentation U-Net. Compared with most U-Net-based models in the literature that only use the pair information between images and label maps, BS U-Net additionally uses the information extracted from the label maps as supervision. The model is evaluated on the liver and tumor segmentation (LiTS) competition. 2D BS U-Net achieves dice per case (DPC) 96.1% for liver segmentation and 56.9% for tumor segmentation. This result is better than most state-of-the-art 2D UNet-based networks in both tasks. Furthermore, the idea of bottleneck feature supervision can also be generalized to other U-Net-based models, making it have good potential for future development.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419308486",
    "keywords": [
      "Artificial intelligence",
      "Bottleneck",
      "Computer science",
      "Embedded system",
      "Encoder",
      "Encoding (memory)",
      "Feature (linguistics)",
      "Geometry",
      "Image segmentation",
      "Linguistics",
      "Mathematics",
      "Net (polyhedron)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Scale-space segmentation",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "LI",
        "given_name": "Song"
      },
      {
        "surname": "TSO",
        "given_name": "Geoffrey K.F."
      },
      {
        "surname": "HE",
        "given_name": "Kaijian"
      }
    ]
  },
  {
    "title": "Leveraging feature selection to detect potential tax fraudsters",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113128",
    "abstract": "Tax evasion is any act that knowingly or unknowingly, legally or unlawfully, leads to non-payment or underpayment of tax due. Enforcing the correct payment of taxes by taxpayers is fundamental in maintaining investments that are necessary and benefits a society as a whole. Indeed, without taxes it is not possible to guarantee basic services such as health-care, education, sanitation, transportation, infrastructure, among other services essential to the population. This issue is especially relevant in developing countries such as Brazil. In this work we consider a real-world case study involving the Treasury Office of the State of Ceará (SEFAZ-CE, Brazil), the agency in charge of supervising more than 300,000 active taxpayers companies. SEFAZ-CE maintains a very large database containing vast amounts of information concerning such companies. Its enforcement team struggles to perform thorough inspections on taxpayers accounts as the underlying traditional human-based inspection processes involve the evaluation of countless fraud indicators (i.e., binary features), thus requiring burdensome amounts of time and being potentially prone to human errors. On the other hand, the vast amount of taxpayer information collected by fiscal agencies opens up the possibility of devising novel techniques able to tackle fiscal evasion much more effectively than traditional approaches. In this work we address the problem of using feature selection to select the most relevant binary features to improve the classification of potential tax fraudsters. Finding out possible fraudsters from taxpayer data with binary features presents several challenges. First, taxpayer data typically have features with low linear correlation between themselves. Also, tax frauds may originate from intricate illicit tactics, which in turn requires to uncover non-linear relationships between multiple features. Finally, few features may be correlated with the targeted class. In this work we propose Alicia, a new feature selection method based on association rules and propositional logic with a carefully crafted graph centrality measure that attempts to tackle the above challenges while, at the same time, being agnostic to specific classification techniques. Alicia is structured in three phases: first, it generates a set of relevant association rules from a set of fraud indicators (features). Subsequently, from such association rules Alicia builds a graph, which structure is then used to determine the most relevant features. To achieve this Alicia applies a novel centrality measure we call the Feature Topological Importance. We perform an extensive experimental evaluation to assess the validity of our proposal on four different real-world datasets, where we compare our solution with eight other feature selection methods. The results show that Alicia achieves F-measure scores up to 76.88%, and consistently outperforms its competitors.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419308450",
    "keywords": [
      "Agency (philosophy)",
      "Archaeology",
      "Business",
      "Certification",
      "Computer science",
      "Computer security",
      "Database",
      "Demography",
      "Economics",
      "Enforcement",
      "Epistemology",
      "Finance",
      "History",
      "Law",
      "Macroeconomics",
      "Payment",
      "Philosophy",
      "Political science",
      "Population",
      "Sociology",
      "Taxpayer",
      "Treasury",
      "Very large database"
    ],
    "authors": [
      {
        "surname": "Matos",
        "given_name": "Tales"
      },
      {
        "surname": "Macedo",
        "given_name": "Jose Antonio"
      },
      {
        "surname": "Lettich",
        "given_name": "Francesco"
      },
      {
        "surname": "Monteiro",
        "given_name": "Jose Maria"
      },
      {
        "surname": "Renso",
        "given_name": "Chiara"
      },
      {
        "surname": "Perego",
        "given_name": "Raffaele"
      },
      {
        "surname": "Nardini",
        "given_name": "Franco Maria"
      }
    ]
  },
  {
    "title": "A hybrid recommendation system for Q&A documents",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113088",
    "abstract": "Question and answer (Q&A) documents are a new type of knowledge document composed of a question part and an answer part. The questions represent knowledge needs, and the answers contain the knowledge that meets these knowledge needs. An overload of accumulated Q&A documents decreases the reuse of valuable knowledge. In this paper, we propose a novel hybrid system to recommend Q&A documents to alleviate overload. First, knowledge needs are partitioned, and current knowledge needs are identified by sequentially clustering the Q&A documents. Second, a content-based (CB) recommendation method, a collaborative filtering (CF) recommendation method and a complementarity-based recommendation method are used to find the Q&A documents that are potentially helpful for the user. Third, the three initial recommendation lists of Q&A documents derived from the three recommendation methods are combined to form a more comprehensive recommendation based on the Fermat point. Because reading all Q&A documents in the recommendation list consumes an enormous amount of time and users prefer to read Q&A documents one by one starting from the top, a novel ranking mechanism is proposed to ensure that users obtain comprehensive knowledge to the greatest extent possible from the limited number of Q&A documents at the top of the list. The proposed approach is evaluated and compared based on an experimental dataset. Our experimental results show that the approach is feasible, performs well, and provides a more effective way to recommend Q&A documents.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741741930805X",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Cluster analysis",
      "Collaborative filtering",
      "Computer science",
      "Ecology",
      "Information overload",
      "Information retrieval",
      "Ranking (information retrieval)",
      "Recommender system",
      "Reuse",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Ming"
      },
      {
        "surname": "Li",
        "given_name": "Ying"
      },
      {
        "surname": "Lou",
        "given_name": "Wangqin"
      },
      {
        "surname": "Chen",
        "given_name": "Lisheng"
      }
    ]
  },
  {
    "title": "A new approach using fuzzy DEA models to reduce search space and eliminate replications in simulation optimization problems",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113137",
    "abstract": "This article proposes a new combination of methods to increase optimization simulation efficiency and reliability, utilizing orthogonal arrays, fuzzy-data envelopment analysis (FDEA) with linear membership function, and discrete event simulation (DES). Considering a simulation optimization problem, experimental matrices are generated using orthogonal arrays and which simulation runs (scenarios) will be executed are defined, followed by FDEA to analyze and rank the scenarios in terms of their efficiency (considering occurrence of uncertainty). In this way, it is possible to reduce the search space of scenarios to be simulated, and avoid the need for replications in DES, without impairing the quality of the final solution. Six real cases that were solved by the proposed approach are presented. In order to highlight the efficiency of the proposed method, in Cases 5 and 6, all viable solutions of each of these problems were tested, ie, 100% of the search space was analyzed, and it was found that the solution obtained by the new method was statistically equal to the overall optimal solution. Note that for the other real cases solved, the solutions obtained by the proposed method were also statistically equal to those obtained from the original search space, and that analyzing 100% of the viable solutions space would be computationally impossible or impractical. These results confirmed the reliability and applicability of the proposed method, since it enabled a significant reduction in the search space for the simulation application compared to conventional simulation optimization techniques.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419308541",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Fuzzy logic",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Operating system",
      "Space (punctuation)"
    ],
    "authors": [
      {
        "surname": "Marins",
        "given_name": "Fernando Augusto Silva"
      },
      {
        "surname": "da Silva",
        "given_name": "Aneirson Francisco"
      },
      {
        "surname": "Miranda",
        "given_name": "Rafael de Carvalho"
      },
      {
        "surname": "Montevechi",
        "given_name": "José Arnaldo Barra"
      }
    ]
  },
  {
    "title": "DNNRec: A novel deep learning based hybrid recommender system",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113054",
    "abstract": "We propose a novel deep learning hybrid recommender system to address the gaps in Collaborative Filtering systems and achieve the state-of-the-art predictive accuracy using deep learning. While collaborative filtering systems are popular with many state-of-the-art achievements in recommender systems, they suffer from the cold start problem, when there is no history about the users and items. Further, the latent factors learned by these methods are linear in nature. To address these gaps, we describe a novel hybrid recommender system using deep learning. The solution uses embeddings for representing users and items to learn non-linear latent factors. The solution alleviates the cold start problem by integrating side information about users and items into a very deep neural network. The proposed solution uses a decreasing learning rate in conjunction with increasing weight decay, the values cyclically varied across epochs to further improve accuracy. The proposed solution is benchmarked against existing methods on both predictive accuracy and running time. Predictive Accuracy is measured by Mean Squared Error (MSE), Root Mean Squared Error (RMSE), Mean Absolute Error (MAE) and R-squared. Running time is measured by the mean and standard deviation across seven runs. Comprehensive experiments are conducted on several datasets such as the MovieLens 100 K, FilmTrust, Book-Crossing and MovieLens 1 M. The results show that the proposed technique outperforms existing methods in both non-cold start and cold start cases. The proposed solution framework is generic from the outperformance on four different datasets and can be leveraged for other ratings prediction datasets in recommender systems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419307717",
    "keywords": [
      "Aerospace engineering",
      "Artificial intelligence",
      "Artificial neural network",
      "Cold start (automotive)",
      "Collaborative filtering",
      "Computer science",
      "Data mining",
      "Deep learning",
      "Engineering",
      "Machine learning",
      "Mathematics",
      "Mean absolute error",
      "Mean squared error",
      "MovieLens",
      "Recommender system",
      "Standard deviation",
      "Statistics",
      "Word error rate"
    ],
    "authors": [
      {
        "surname": "R",
        "given_name": "Kiran"
      },
      {
        "surname": "Kumar",
        "given_name": "Pradeep"
      },
      {
        "surname": "Bhasker",
        "given_name": "Bharat"
      }
    ]
  },
  {
    "title": "The solution of the concurrent layout scheduling problem in the job-shop environment through a local neighborhood search algorithm",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113096",
    "abstract": "The concurrent layout and scheduling problem is an extension of the well-known job-shop scheduling problem with transport delays, in which, in addition to the decisions taken in the classic problem, the location of machines must be selected from a set of possible sites. The aim of this study was to solve this problem by using a local neighborhood search algorithm (LNSA). This algorithm used a random local neighborhood, where neighbors were produced by simple operators commonly used in metaheuristics for scheduling problems. The solution coding used in the LNSA enabled all the calculated solutions to be valid schedules for the problem. The findings of the study were compared with those obtained by Ranjbar, who proposed the benchmark problems. Our method obtained a minor average relative percentage compared to the best-known results. Furthermore, it achieved a smaller makespan in the problems with higher computational complexity, which may aid companies that require customized manufacturing.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419308139",
    "keywords": [
      "Algorithm",
      "Benchmark (surveying)",
      "Computer science",
      "Flow shop scheduling",
      "Geodesy",
      "Geography",
      "Job shop",
      "Job shop scheduling",
      "Local search (optimization)",
      "Mathematical optimization",
      "Mathematics",
      "Metaheuristic",
      "Operating system",
      "Schedule",
      "Scheduling (production processes)"
    ],
    "authors": [
      {
        "surname": "Hernández-Gress",
        "given_name": "Eva Selene"
      },
      {
        "surname": "Seck-Tuoh-Mora",
        "given_name": "Juan Carlos"
      },
      {
        "surname": "Hernández-Romero",
        "given_name": "Norberto"
      },
      {
        "surname": "Medina-Marín",
        "given_name": "Joselito"
      },
      {
        "surname": "Lagos-Eulogio",
        "given_name": "Pedro"
      },
      {
        "surname": "Ortíz-Perea",
        "given_name": "Javier"
      }
    ]
  },
  {
    "title": "Personalized itinerary recommendation: Deep and collaborative learning with textual information",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113070",
    "abstract": "Personalized itinerary recommendation is a complicated and challenging task, which aims to construct and recommend a visit sequence consists of multiple Points of Interest (POIs) with the constraints that maximizing user satisfaction while adhering time budget. User interests, therefore, becomes the most crucial element in the recommendation task, determining the POIs and their visit durations in the itinerary. In this paper, we propose a novel framework named DCC-PersIRE to infer the user interests and recommend personalized itinerary consists of POIs, visit durations and visit sequence. Specifically, we employ an unsupervised deep learning model to embed the POI textual contents, and then propose a DCC model, which seamlessly integrates the embedded POI textual contents with the traditional and widely used user-POI visits and POI categories, to predict the user interests as well as the visit durations. Then, after formulating itinerary construction as a variant of the Orienteering Problem, an Iterated Local Search based algorithm is proposed to calculate the visit sequence with maximized satisfaction consists of multiple POIs and personalized POI visit durations, i.e., the optimal itinerary. Extensive experiments on eight real-world datasets validate the effectiveness of DCC-PersIRE. The experimental results show that our algorithm delivers a more fine-grained prediction of user interests and outperforms various state-of-the-art baselines in tour itinerary recommendation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419307870",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Economics",
      "Genetics",
      "History",
      "Information retrieval",
      "Machine learning",
      "Management",
      "Orienteering",
      "Point of interest",
      "Sequence (biology)",
      "Task (project management)",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Lei"
      },
      {
        "surname": "Zhang",
        "given_name": "Lu"
      },
      {
        "surname": "Cao",
        "given_name": "Shanshan"
      },
      {
        "surname": "Wu",
        "given_name": "Zhiang"
      },
      {
        "surname": "Cao",
        "given_name": "Jie"
      }
    ]
  },
  {
    "title": "Leveraging deep graph-based text representation for sentiment polarity applications",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113090",
    "abstract": "Over the last few years, machine learning over graph structures has manifested a significant enhancement in text mining applications such as event detection, opinion mining, and news recommendation. One of the primary challenges in this regard is structuring a graph that encodes and encompasses the features of textual data for the effective machine learning algorithm. Besides, exploration and exploiting of semantic relations is regarded as a principal step in text mining applications. However, most of the traditional text mining methods perform somewhat poor in terms of employing such relations. In this paper, we propose a sentence-level graph-based text representation which includes stop words to consider semantic and term relations. Then, we employ a representation learning approach on the combined graphs of sentences to extract the latent and continuous features of the documents. Eventually, the learned features of the documents are fed into a deep neural network for the sentiment classification task. The experimental results demonstrate that the proposed method substantially outperforms the related sentiment analysis approaches based on several benchmark datasets. Furthermore, our method can be generalized on different datasets without any dependency on pre-trained word embeddings.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419308073",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Deep learning",
      "Graph",
      "Machine learning",
      "Natural language processing",
      "Sentence",
      "Sentiment analysis",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Bijari",
        "given_name": "Kayvan"
      },
      {
        "surname": "Zare",
        "given_name": "Hadi"
      },
      {
        "surname": "Kebriaei",
        "given_name": "Emad"
      },
      {
        "surname": "Veisi",
        "given_name": "Hadi"
      }
    ]
  },
  {
    "title": "The bi-objective periodic closed loop network design problem",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113068",
    "abstract": "Reverse supply chains are becoming a crucial part of retail supply chains given the recent reforms in the consumers’ rights and the regulations by governments. This has motivated companies around the world to adopt zero-landfill goals and move towards circular economy to retain the product’s value during its whole life cycle. However, designing an efficient closed loop supply chain is a challenging undertaking as it presents a set of unique challenges, mainly owing to the need to handle pickups and deliveries at the same time and the necessity to meet the customer requirements within a certain time limit. In this paper, we model this problem as a bi-objective periodic location routing problem with simultaneous pickup and delivery as well as time windows and examine the performance of two procedures, namely NSGA-II and NRGA, to solve it. The goal is to find the best locations for a set of depots, allocation of customers to these depots, allocation of customers to service days and the optimal routes to be taken by a set of homogeneous vehicles to minimise the total cost and to minimise the overall violation from the customers’ defined time limits. Our results show that while there is not a significant difference between the two algorithms in terms of diversity and number of solutions generated, NSGA-II outperforms NRGA when it comes to spacing and runtime.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419307857",
    "keywords": [
      "Artificial intelligence",
      "Business",
      "Computer network",
      "Computer science",
      "Economics",
      "Geometry",
      "Image (mathematics)",
      "Limit (mathematics)",
      "Management",
      "Marketing",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Microeconomics",
      "Operations research",
      "Pickup",
      "Product (mathematics)",
      "Production (economics)",
      "Programming language",
      "Reverse logistics",
      "Routing (electronic design automation)",
      "Service (business)",
      "Set (abstract data type)",
      "Supply chain",
      "Time limit",
      "Vehicle routing problem"
    ],
    "authors": [
      {
        "surname": "Mamaghani",
        "given_name": "Elham Jelodari"
      },
      {
        "surname": "Davari",
        "given_name": "Soheil"
      }
    ]
  },
  {
    "title": "Geometric probabilistic evolutionary algorithm",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113080",
    "abstract": "In this paper we introduce a crossover operator and a mutation operator, called Bernoulli Reflection Search Operator (BRSO) and Cauchy Distributed Inversion Search Operator (CDISO) respectively, in order to define the search mechanism of a new evolutionary algorithm for global continuous optimisation, namely the Geometric Probabilistic Evolutionary Algorithm (GPEA). Both operators have been motivated by geometric transformations, namely inversions with respect to hyperspheres and reflections with respect to a hyperplanes, but are implemented stochastically. The design of the new operators follows statistical analyses of the search mechanisms (Inversion Search Operator (ISO) and Reflection Search Operator (RSO)) of the Spherical Evolutionary Algorithm (SEA). From the statistical analyses, we concluded that the non-linearity of the ISO can be imitated stochastically, avoiding the calculation of several parameters such as the radius of hypersphere and acceptable regions of application. In addition, a new mutation based on a normal distribution is included in CDISO in order to guide the exploration. On the other hand, the BRSO imitates the mutation of individuals using reflections with respect to hyperplanes and complements the CDISO. In order to evaluate the proposed method, we use the benchmark functions of the special session on real-parameter optimisation of the CEC 2013 competition. We compare GPEA against 12 state-of-the-art methods, and present a statistical analysis using the Wilcoxon signed rank and the Friedman tests. According to the numerical experiments, GPEA exhibits a competitive performance against a variety of sophisticated contemporary algorithms, particularly in higher dimensions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419307973",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Cauchy distribution",
      "Chemistry",
      "Combinatorics",
      "Computer science",
      "Crossover",
      "Evolutionary algorithm",
      "Gene",
      "Hyperplane",
      "Mathematical optimization",
      "Mathematics",
      "Operator (biology)",
      "Probabilistic logic",
      "Repressor",
      "Statistics",
      "Transcription factor"
    ],
    "authors": [
      {
        "surname": "Segovia-Domínguez",
        "given_name": "Ignacio"
      },
      {
        "surname": "Herrera-Guzmán",
        "given_name": "Rafael"
      },
      {
        "surname": "Serrano-Rubio",
        "given_name": "Juan Pablo"
      },
      {
        "surname": "Hernández-Aguirre",
        "given_name": "Arturo"
      }
    ]
  },
  {
    "title": "SCGSA: A sine chaotic gravitational search algorithm for continuous optimization problems",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113118",
    "abstract": "Gravitational search algorithm (GSA), as one of the novel meta-heuristic optimization algorithms inspired by the law of gravity and mass interactions, is however prone to local optima stagnation due to heavier gravity. Hence, an enhanced version, chaotic gravitational constants for the gravitational search algorithm (CGSA), was proposed to improve the exploration ability through various chaotic maps. In this paper, with insightful utilization of sine cosine algorithm, we put forward sine chaotic gravitational search algorithm (SCGSA) as a further step of CGSA to escape from its local optima stagnation. The experiments show remarkable results in both the speed of convergence and the ability of finding global optima in 30 benchmark functions (CEC 2014), thus proving a better balance between exploration and exploitation in SCGSA compared with CGSA.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419308358",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Channel (broadcasting)",
      "Chaotic",
      "Classical mechanics",
      "Computer network",
      "Computer science",
      "Convergence (economics)",
      "Economic growth",
      "Economics",
      "Geodesy",
      "Geography",
      "Geometry",
      "Gravitation",
      "Gravitational search algorithm",
      "Heuristic",
      "Local optimum",
      "Mathematical optimization",
      "Mathematics",
      "Particle swarm optimization",
      "Physics",
      "Rate of convergence",
      "Sine"
    ],
    "authors": [
      {
        "surname": "Jiang",
        "given_name": "Jianhua"
      },
      {
        "surname": "Jiang",
        "given_name": "Ran"
      },
      {
        "surname": "Meng",
        "given_name": "Xianqiu"
      },
      {
        "surname": "Li",
        "given_name": "Keqin"
      }
    ]
  },
  {
    "title": "Influential spreaders identification in complex networks with improved k-shell hybrid method",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113092",
    "abstract": "Identifying influential spreaders in a complex network has practical and theoretical significance. In applications such as disease spreading, virus infection in computer networks, viral marketing, immunization, rumor containment, among others, the main strategy is to identify the influential nodes in the network. Hence many different centrality measures evolved to identify central nodes in a complex network. The degree centrality is the most simple and easy to compute whereas closeness and betweenness centrality are complex and more time-consuming. The k-shell centrality has the problem of placing too many nodes in a single shell. Over the time many improvements over k-shell have been proposed with pros and cons. The k-shell hybrid (ksh) method has been recently proposed with promising results but with a free parameter that is set empirically which may cause some constraints to the performance of the method. This paper presents an improvement of the ksh method by providing a mathematical model for the free parameter based on standard network parameters. Experiments on real and artificially generated networks show that the proposed method outperforms the ksh method and most of the state-of-the-art node indexing methods. It has a better performance in terms of ranking performance as measured by the Kendall’s rank correlation, and in terms of ranking efficiency as measured by the monotonicity value. Due to the absence of any empirically set free parameter, no time-consuming preprocessing is required for optimal parameter value selection prior to actual ranking of nodes in a large network.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419308097",
    "keywords": [
      "Artificial intelligence",
      "Betweenness centrality",
      "Biology",
      "Botany",
      "Centrality",
      "Computer science",
      "Data mining",
      "Engineering",
      "Identification (biology)",
      "Mathematical optimization",
      "Mathematics",
      "Node (physics)",
      "Preprocessor",
      "Programming language",
      "Ranking (information retrieval)",
      "Set (abstract data type)",
      "Statistics",
      "Structural engineering"
    ],
    "authors": [
      {
        "surname": "Maji",
        "given_name": "Giridhar"
      },
      {
        "surname": "Namtirtha",
        "given_name": "Amrita"
      },
      {
        "surname": "Dutta",
        "given_name": "Animesh"
      },
      {
        "surname": "Curado Malta",
        "given_name": "Mariana"
      }
    ]
  },
  {
    "title": "SCGSA: A sine chaotic gravitational search algorithm for continuous optimization problems",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113118",
    "abstract": "Gravitational search algorithm (GSA), as one of the novel meta-heuristic optimization algorithms inspired by the law of gravity and mass interactions, is however prone to local optima stagnation due to heavier gravity. Hence, an enhanced version, chaotic gravitational constants for the gravitational search algorithm (CGSA), was proposed to improve the exploration ability through various chaotic maps. In this paper, with insightful utilization of sine cosine algorithm, we put forward sine chaotic gravitational search algorithm (SCGSA) as a further step of CGSA to escape from its local optima stagnation. The experiments show remarkable results in both the speed of convergence and the ability of finding global optima in 30 benchmark functions (CEC 2014), thus proving a better balance between exploration and exploitation in SCGSA compared with CGSA.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419308358",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Channel (broadcasting)",
      "Chaotic",
      "Classical mechanics",
      "Computer network",
      "Computer science",
      "Convergence (economics)",
      "Economic growth",
      "Economics",
      "Geodesy",
      "Geography",
      "Geometry",
      "Gravitation",
      "Gravitational search algorithm",
      "Heuristic",
      "Local optimum",
      "Mathematical optimization",
      "Mathematics",
      "Particle swarm optimization",
      "Physics",
      "Rate of convergence",
      "Sine"
    ],
    "authors": [
      {
        "surname": "Jiang",
        "given_name": "Jianhua"
      },
      {
        "surname": "Jiang",
        "given_name": "Ran"
      },
      {
        "surname": "Meng",
        "given_name": "Xianqiu"
      },
      {
        "surname": "Li",
        "given_name": "Keqin"
      }
    ]
  },
  {
    "title": "A Music Classification model based on metric learning applied to MP3 audio files",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113071",
    "abstract": "The development of models for learning music similarity from audio media files is an increasingly important task for the entertainment industry. This work proposes a novel music classification model based on metric learning whose main objective is to learn a personalized metric for each customer. The metric learning process considers the learning of a set of parameterized distances employing a structured prediction approach from a set of MP3 audio files containing several music genres according to the users taste. The structured prediction solution aims to maximize the separation margin between genre centroids and to minimize the overall intra-cluster distances. To extract the acoustic information we use the Mel-Frequency Cepstral Coecient (MFCC) and made a dimensionality reduction using Principal Components Analysis (PCA). We attest the model validity performing a set of experiments and comparing the training and testing results with baseline algorithms, such as K-means and Soft Margin Linear Support Vector Machine (SVM). Also, to prove the prediction capacity, we compare our results with two recent works with good prediction results on the GTZAN dataset. Experiments show promising results and encourage the future development of an online version of the learning model to be applied in streaming platforms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419307882",
    "keywords": [
      "Artificial intelligence",
      "Centroid",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Dimensionality reduction",
      "Economics",
      "Feature extraction",
      "Machine learning",
      "Margin (machine learning)",
      "Mel-frequency cepstrum",
      "Metric (unit)",
      "Operations management",
      "Pattern recognition (psychology)",
      "Principal component analysis",
      "Programming language",
      "Set (abstract data type)",
      "Speech recognition",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Silva",
        "given_name": "Angelo Cesar Mendes da"
      },
      {
        "surname": "Coelho",
        "given_name": "Maurício Archanjo Nunes"
      },
      {
        "surname": "Neto",
        "given_name": "Raul Fonseca"
      }
    ]
  },
  {
    "title": "Incremental and dynamic graph construction with application to image classification",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113117",
    "abstract": "In this paper, we propose a dynamic graph construction technique that inserts new samples into a previously constructed graph, which reduces the computational time and complexity of the classic batch construction schemes. The basic assumption of the proposed method is that by adding one sample into a graph, only its close nodes will be affected, hence, it is not necessary to update the whole graph but only the weights of close nodes. The proposed method has two steps of insertion and updating. In the insertion phase, the similarity between the new sample and the available data is calculated. The similarity vector is retrieved either from a distance function or a coding scheme. Then, in the update phase, by evaluating the similarity vector of the new sample, close nodes which will be affected are identified and the graph weights of these close (similar) nodes are updated. By adopting this scenario, in each insertion of a node (or a set of nodes), only the weights of very few nodes have to be updated. It is worthy to mention that since the proposed method does not invoke labels of the samples, it can be adopted by any unsupervised, semi-supervised or supervised technique. A set of extensive experiments for the task of classification on different image datasets show that, in various post-graph learning tasks (i.e., Label propagation and Manifold learning), the graph which is constructed by the proposed method, even after hundreds of insertions and updates, has a similar performance (in some cases it can be better) compared to the graph which is constructed from scratch.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419308346",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Coding (social sciences)",
      "Computer science",
      "Data mining",
      "Graph",
      "Image (mathematics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Similarity (geometry)",
      "Statistics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Bosaghzadeh",
        "given_name": "Alireza"
      },
      {
        "surname": "Dornaika",
        "given_name": "Fadi"
      }
    ]
  },
  {
    "title": "Mathematically optimized, recursive prepartitioning strategies for k-anonymous microaggregation of large-scale datasets",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113086",
    "abstract": "The technical contents of this work fall within the statistical disclosure control (SDC) field, which concerns the postprocessing of the demographic portion of the statistical results of surveys containing sensitive personal information, in order to effectively safeguard the anonymity of the participating respondents. A widely known technique to solve the problem of protecting the privacy of the respondents involved beyond the mere suppression of their identifiers is the k-anonymous microaggregation. Unfortunately, most microaggregation algorithms that produce competitively low levels of distortions exhibit a superlinear running time, typically scaling with the square of the number of records in the dataset. This work proposes and analyzes an optimized prepartitioning strategy to reduce significantly the running time for the k-anonymous microaggregation algorithm operating on large datasets, with mild loss in data utility with respect to that of MDAV, the underlying method. The optimization strategy is based on prepartitioning a dataset recursively until the desired k-anonymity parameter is achieved. Traditional microaggregation algorithms have quadratic computational complexity in the form Θ(n 2). By using the proposed method and fixing the number of recurrent prepartitions we obtain subquadratic complexity in the form Θ(n 3/2), Θ(n 4/3), ..., depending on the number of prepartitions. Alternatively, fixing the ratio between the size of the microcell and the macrocell on each prepartition, quasilinear complexity in the form Θ(nlog n) is achieved. Our method is readily applicable to large-scale datasets with numerical demographic attributes.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419308036",
    "keywords": [
      "Algorithm",
      "Anonymity",
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Computer science",
      "Computer security",
      "Data mining",
      "Field (mathematics)",
      "Geometry",
      "Identification (biology)",
      "Information loss",
      "Mathematics",
      "Physics",
      "Pure mathematics",
      "Quadratic equation",
      "Quantum mechanics",
      "Scale (ratio)",
      "Time complexity",
      "k-anonymity"
    ],
    "authors": [
      {
        "surname": "Pallarès",
        "given_name": "Esteve"
      },
      {
        "surname": "Rebollo-Monedero",
        "given_name": "David"
      },
      {
        "surname": "Rodríguez-Hoyos",
        "given_name": "Ana"
      },
      {
        "surname": "Estrada-Jiménez",
        "given_name": "José"
      },
      {
        "surname": "Mezher",
        "given_name": "Ahmad Mohamad"
      },
      {
        "surname": "Forné",
        "given_name": "Jordi"
      }
    ]
  },
  {
    "title": "Advanced orthogonal learning-driven multi-swarm sine cosine optimization: Framework and case studies",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113113",
    "abstract": "Sine cosine algorithm (SCA) is a widely used nature-inspired algorithm that is simple in structure and involves only a few parameters. For some complex tasks, especially high-dimensional problems and multimodal problems, the basic method may have problems in harmonic convergence or trapped into local optima. To efficiently alleviate this deficiency, an improved variant of basic SCA is proposed in this paper. The orthogonal learning, multi-swarm, and greedy selection mechanisms are utilized to improve the global exploration and local exploitation powers of SCA. In preference, the orthogonal learning procedure is introduced into the conventional method to expand its neighborhood searching capabilities. Next, the multi-swarm scheme with three sub-strategies is adopted to enhance the global exploration capabilities of the algorithm. Also, a greedy selection strategy is applied to the conventional approach to improve the qualities of the search agents. Based on these three strategies, we called the improved SCA as OMGSCA. The proposed OMGSCA is compared with a comprehensive set of meta-heuristic algorithms including six other improved SCA variants, basic version, and ten advanced meta-heuristic algorithms. We employed thirty IEEE CEC2014 benchmark functions, and eight advanced meta-heuristic algorithms on seventeen real-world benchmark problems from IEEE CEC2011. Also, non-parametric statistical Wilcoxon sign rank and the Friedman tests are performed to monitor the performance of the proposed method. The obtained experimental results demonstrate that the introduced strategies can significantly improve the exploratory and exploitative inclinations of the basic algorithm. The convergence speed of the original method has also been improved, substantially. The results suggest the proposed OMGSCA can be used as an effective and efficient auxiliary tool for solving complex optimization problems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419308309",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Convergence (economics)",
      "Economic growth",
      "Economics",
      "Geodesy",
      "Geography",
      "Geometry",
      "Greedy algorithm",
      "Heuristic",
      "Local optimum",
      "Mathematical optimization",
      "Mathematics",
      "Sine",
      "Swarm behaviour"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Hao"
      },
      {
        "surname": "Heidari",
        "given_name": "Ali Asghar"
      },
      {
        "surname": "Zhao",
        "given_name": "Xuehua"
      },
      {
        "surname": "Zhang",
        "given_name": "Lejun"
      },
      {
        "surname": "Chen",
        "given_name": "Huiling"
      }
    ]
  },
  {
    "title": "Unusual customer response identification and visualization based on text mining and anomaly detection",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113111",
    "abstract": "The Vehicle Dependability Study (VDS) is a survey study on customer satisfaction for vehicles that have been sold for three years. VDS data analytics plays an important role in the vehicle development process because it can contribute to enhancing the brand image and sales of an automobile company by properly reflecting customer requirements retrieved from the analysis results when developing the vehicle’s next model. Conventional approaches to analyzing the voice of customers (VOC) data, such as VDS, have focused on finding the mainstream of customer responses, many of which are already known to the enterprise. However, detecting and visualizing notable opinions from a large amount of VOC data are important in responding to customer complaints. In this study, we propose a framework for identifying unusual but significant customer responses and frequently used words therein based on distributed document representation, local outlier factor, and TF–IDF methods. We also propose a procedure that can provide useful information to vehicle engineers by visualizing the main results of the framework. This unusual customer response detection and visualization framework can accelerate the efficiency and effectiveness of many VOC data analytics.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419308280",
    "keywords": [
      "Analytics",
      "Anomaly detection",
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Business",
      "Computer science",
      "Customer retention",
      "Customer satisfaction",
      "Data mining",
      "Data science",
      "Identification (biology)",
      "Marketing",
      "Operating system",
      "Outlier",
      "Process (computing)",
      "Service (business)",
      "Service quality",
      "Visualization",
      "Voice of the customer"
    ],
    "authors": [
      {
        "surname": "Seo",
        "given_name": "Seungwan"
      },
      {
        "surname": "Seo",
        "given_name": "Deokseong"
      },
      {
        "surname": "Jang",
        "given_name": "Myeongjun"
      },
      {
        "surname": "Jeong",
        "given_name": "Jaeyun"
      },
      {
        "surname": "Kang",
        "given_name": "Pilsung"
      }
    ]
  },
  {
    "title": "A batch informed sampling-based algorithm for fast anytime asymptotically-optimal motion planning in cluttered environments",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113124",
    "abstract": "Practical applications favor anytime asymptotically-optimal algorithms that find and improve an initial solution toward the optimal solution as quickly as possible due to the algorithms may be terminated at any time. We present Batch-to-batch Informed Fast Marching Tree (BBI-FMT*), an anytime asymptotically-optimal sampling-based algorithm that is designed for solving complex motion planning problems. The proposed algorithm has the ability to fast find an initial low-cost solution by the batch sampling-based incremental search and the “lazy” optimal search, then it employs the batch informed sampling-based incremental search and the anytime optimal search to quickly improve the tree and achieve the optimal solution. The proposed anytime optimal search strategy integrates the “lazy” and “non-lazy” optimal search to efficiently improve the tree to the minimum-cost spanning tree in cluttered environments. This paper theoretically analyzes the proposed algorithm in depth and evaluates it by numerical experiments under a few challenging scenarios. The experimental results show that BBI-FMT* outperforms the state-of-the-art algorithms in the self-adaptability, robustness, convergence rate, and success rate of the planning. The proposed algorithm can be widely applied to intelligent robots with expert systems to improve the efficiency and stability of the motion planning and navigation modules which are the core modules in the expert systems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419308413",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Asymptotically optimal algorithm",
      "Computer science",
      "Computer vision",
      "Filter (signal processing)",
      "Mathematical optimization",
      "Mathematics",
      "Motion (physics)",
      "Motion planning",
      "Robot",
      "Sampling (signal processing)"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Jing"
      },
      {
        "surname": "Song",
        "given_name": "Kechen"
      },
      {
        "surname": "Dong",
        "given_name": "Hongwen"
      },
      {
        "surname": "Yan",
        "given_name": "Yunhui"
      }
    ]
  },
  {
    "title": "Distributed version of hybrid swarm intelligence-Nelder Mead algorithm for DOA estimation in WSN",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113112",
    "abstract": "Distributed direction of arrival (DOA) estimation based on maximum likelihood (ML) is an energy-efficient source localization technique that is vital to systems such as wireless sensor networks (WSN). Due to the multimodal nature of the ML function, the distributed approach uses swarm intelligence (SI) algorithms. However, this approach has slow convergence and incurs significant communication overhead for more than two sources. Hence, to obtain accurate DOA estimates with faster convergence, this paper proposes a distributed hybrid version of SI and Nelder-Mead (NM) simplex algorithm. In this approach, a modified evolutionary population dynamics based distributed Grey Wolf optimization SI algorithm is proposed to obtain initial DOA estimates. Then NM provides accurate DOA estimates with faster convergence and thereby reduces the communication overhead. Detailed simulation analysis shows that by using Quasi-Opposition based population initialization and sensor node degree combiner coefficients, the proposed distributed hybrid algorithm converges to theoretical Cramer-Rao lower bound with small communication overhead. Thus DOA estimation can be performed in an energy-efficient way by incorporating a computationally intelligent distributed hybrid approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419308292",
    "keywords": [
      "Algorithm",
      "Computer network",
      "Computer science",
      "Convergence (economics)",
      "Demography",
      "Distributed algorithm",
      "Distributed computing",
      "Economic growth",
      "Economics",
      "Engineering",
      "Initialization",
      "Linear programming",
      "Mathematical optimization",
      "Mathematics",
      "Node (physics)",
      "Operating system",
      "Overhead (engineering)",
      "Population",
      "Programming language",
      "Simplex algorithm",
      "Sociology",
      "Structural engineering",
      "Wireless sensor network"
    ],
    "authors": [
      {
        "surname": "Maruthi",
        "given_name": "Shree Prasad"
      },
      {
        "surname": "Panigrahi",
        "given_name": "Trilochan"
      },
      {
        "surname": "Jagannath",
        "given_name": "Ravi Prasad K."
      }
    ]
  },
  {
    "title": "Transform domain representation-driven convolutional neural networks for skin lesion segmentation",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113129",
    "abstract": "Automated diagnosis systems provide a huge improvement in early detection of skin cancer, and consequently, contribute to successful treatment. Recent research on convolutional neural network has achieved enormous success in segmentation and object detection tasks. However, these networks require large amount of data that is a big challenge in medical domain where often have insufficient data and even a pretrained model on medical images can be hardly found. Lesion segmentation as the initial step of skin cancer analysis remains a challenging issue since datasets are small and include a variety of images in terms of light, color, scale, and marks which have led researchers to use extensive augmentation and preprocessing techniques or fine tuning the network with a pretrained model on irrelevant images. A segmentation model based on convolutional neural networks is proposed in this study for the tasks of skin lesion segmentation and dermoscopic feature segmentation. The network is trained from scratch and despite the small size of datasets neither excessive data augmentation nor any preprocessing to remove artifacts or enhance the images are applied. Alternatively, we investigated incorporating image representations of the transform domain to the convolutional neural network and compared to a model with more convolutional layers that resulted in 6% higher Jaccard index and has shorter training time. The model improved by applying CIELAB color space and the performance of the final proposed architecture is evaluated on publicly available datasets from ISBI challenges in 2016 and 2017. The proposed model has resulted in an improvement of as much as 7% for the segmentation metrics and 17% for the feature segmentation, which demonstrates the robustness of this unique hybrid framework and its future applications as well as further improvement.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419308462",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Feature (linguistics)",
      "Geodesy",
      "Geography",
      "Image segmentation",
      "Jaccard index",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Preprocessor",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Pezhman Pour",
        "given_name": "Mansoureh"
      },
      {
        "surname": "Seker",
        "given_name": "Huseyin"
      }
    ]
  },
  {
    "title": "Globally-biased BIRECT algorithm with local accelerators for expensive global optimization",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113052",
    "abstract": "In this paper, black-box global optimization problem with expensive function evaluations is considered. This problem is challenging for numerical methods due to the practical limits on computational budget often required by intelligent systems. For its efficient solution, a new DIRECT-type hybrid technique is proposed. The new algorithm incorporates a novel sampling on diagonals and bisection strategy (instead of a trisection which is commonly used in the existing DIRECT-type algorithms), embedded into the globally-biased framework, and enriched with three different local minimization strategies. The numerical results on a test set of almost 900 problems from the literature and on a real-life application regarding nonlinear regression show that the new approach effectively addresses well-known DIRECT weaknesses, has beneficial effects on the overall performance, and on average, gives significantly better results compared to several DIRECT-type methods widely used in decision-making expert systems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419307699",
    "keywords": [
      "Algorithm",
      "Computer science",
      "Diagonal",
      "Geometry",
      "Mathematical optimization",
      "Mathematics",
      "Minification",
      "Nonlinear system",
      "Physics",
      "Programming language",
      "Quantum mechanics",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Paulavičius",
        "given_name": "Remigijus"
      },
      {
        "surname": "Sergeyev",
        "given_name": "Yaroslav D."
      },
      {
        "surname": "Kvasov",
        "given_name": "Dmitri E."
      },
      {
        "surname": "Žilinskas",
        "given_name": "Julius"
      }
    ]
  },
  {
    "title": "BPDET: An effective software bug prediction model using deep representation and ensemble learning techniques",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113085",
    "abstract": "In software fault prediction systems, there are many hindrances for detecting faulty modules, such as missing values or samples, data redundancy, irrelevance features, and correlation. Many researchers have built a software bug prediction (SBP) model, which classify faulty and non-faulty module which are associated with software metrics. Till now very few works has been done which addresses the class imbalance problem in SBP. The main objective of this paper is to reveal the favorable result by feature selection and machine learning methods to detect defective and non-defective software modules. We propose a rudimentary classification based framework Bug Prediction using Deep representation and Ensemble learning (BPDET) techniques for SBP. It combinedly applies by ensemble learning (EL) and deep representation(DR). The software metrics which are used for SBP are mostly conventional. Staked denoising auto-encoder (SDA) is used for the deep representation of software metrics, which is a robust feature learning method. Propose model is mainly divided into two stages: deep learning stage and two layers of EL stage (TEL). The extraction of the feature from SDA in the very first step of the model then applied TEL in the second stage. TEL is also dealing with the class imbalance problem. The experiment mainly performed NASA (12) datasets, to reveal the efficiency of DR, SDA, and TEL. The performance is analyzed in terms of Mathew co-relation coefficient (MCC), the area under the curve (AUC), precision-recall area (PRC), F-measure and Time. Out of 12 dataset MCC values over 11 datasets, ROC values over 6 datasets, PRC values overall 12 datasets and F-measure over 8 datasets surpass the existing state of the art bug prediction methods. We have tested BPDET using Wilcoxon rank sum test which rejects the null hypothesis at α = 0.025. We have also tested the stability of the model over 5, 8, 10, 12, and 15 fold cross-validation and got similar results. Finally, we conclude that BPDET is a stable and outperformed on most of the datasets compared with EL and another state of the art techniques.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419308024",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Data mining",
      "Deep learning",
      "Ensemble learning",
      "Feature (linguistics)",
      "Feature selection",
      "Linguistics",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Precision and recall",
      "Programming language",
      "Software"
    ],
    "authors": [
      {
        "surname": "Pandey",
        "given_name": "Sushant Kumar"
      },
      {
        "surname": "Mishra",
        "given_name": "Ravi Bhushan"
      },
      {
        "surname": "Tripathi",
        "given_name": "Anil Kumar"
      }
    ]
  },
  {
    "title": "M3DNet: A manifold-based discriminant feature learning network for hyperspectral imagery",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113089",
    "abstract": "Feature extraction (FE) is an effective method for learning discriminant features from hyperspectral image (HSI). Recently, graph embedding (GE) framework has been widely applied in FE of HSI data. GE unifies many classical FE methods and explores the low-dimensional embedding of high-dimensional data by a projection matrix generated from undirected weighted graphs. However, GE is unable to adaptively optimize projection matrix due to the absence of an iterative strategy in a single mapping process. To address this issue, a unified optimization method termed manifold-based maximization margin discriminant network (M3DNet) was proposed to improve the performance of traditional FE methods. In M3DNet, an initial projection matrix is obtained from original FE method, and then a maximal manifold margin criterion (M3C) is proposed to maximize the margins among different classes, which enhances the discriminative ability of embedding features. After that, an iterative strategy is designed to optimize the projection matrix. Experiments on real-world HSI data sets indicate that the proposed M3DNet performs significantly better than some state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419308061",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Dimensionality reduction",
      "Discriminant",
      "Engineering",
      "Feature (linguistics)",
      "Hyperspectral imaging",
      "Linear discriminant analysis",
      "Linguistics",
      "Manifold (fluid mechanics)",
      "Manifold alignment",
      "Mathematics",
      "Mechanical engineering",
      "Nonlinear dimensionality reduction",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Zhengying"
      },
      {
        "surname": "Huang",
        "given_name": "Hong"
      },
      {
        "surname": "Li",
        "given_name": "Yuan"
      },
      {
        "surname": "Pan",
        "given_name": "Yinsong"
      }
    ]
  },
  {
    "title": "Corrigendum to “A novel sparse representation model for pedestrian abnormal trajectory understanding” [Expert Systems with Applications, Volume 138, 30 December 2019, 112753]",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113093",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419308103",
    "keywords": [
      "Artificial intelligence",
      "Astronomy",
      "Computer science",
      "Engineering",
      "Law",
      "Machine learning",
      "Pedestrian",
      "Physics",
      "Political science",
      "Politics",
      "Quantum mechanics",
      "Representation (politics)",
      "Trajectory",
      "Transport engineering",
      "Volume (thermodynamics)"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Zhijun"
      },
      {
        "surname": "Cai",
        "given_name": "Hao"
      },
      {
        "surname": "Zhang",
        "given_name": "Yishi"
      },
      {
        "surname": "Wu",
        "given_name": "Chaozhong"
      },
      {
        "surname": "Mu",
        "given_name": "Mengchao"
      },
      {
        "surname": "Li",
        "given_name": "Zhixiong"
      },
      {
        "surname": "Sotelo",
        "given_name": "Miguel Angel"
      }
    ]
  },
  {
    "title": "Intelligent traffic control for autonomous vehicle systems based on machine learning",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113074",
    "abstract": "This study aimed to resolve a real-world traffic problem in a large-scale plant. Autonomous vehicle systems (AVSs), which are designed to use multiple vehicles to transfer materials, are widely used to transfer wafers in semiconductor manufacturing. Traffic control is a significant challenge with AVSs because all vehicles must be monitored and controlled in real time, to cope with uncertainties such as congestion. However, existing traffic control systems, which are primarily designed and controlled by human experts, are insufficient to prevent heavy congestion that impedes production. In this study, we developed a traffic control system based on machine learning predictions, and a routing method that dynamically determines AVS routes with reduced congestion rates. We predicted congestion for critical bottleneck areas, and utilized the predictions for adaptive routing control of all vehicles to avoid congestion. We conducted an experimental evaluation to compare the predictive performance of four popular algorithms. We performed a simulation study based on data from semiconductor fabrication to demonstrate the utility and superiority of the proposed method. The experimental results showed that AVSs with the proposed approach outperformed the existing approach in terms of delivery time, transfer time, and queuing time. We found that adopting machine learning-based traffic control can enhance the performance of existing AVSs and reduce the burden on the human experts who monitor and control AVSs.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419307912",
    "keywords": [
      "Artificial intelligence",
      "Bottleneck",
      "Computer network",
      "Computer science",
      "Control (management)",
      "Embedded system",
      "Engineering",
      "Intelligent transportation system",
      "Network congestion",
      "Network packet",
      "Queueing theory",
      "Real-time computing",
      "Routing (electronic design automation)",
      "Simulation",
      "Telecommunications",
      "Throughput",
      "Traffic congestion",
      "Transport engineering",
      "Wireless"
    ],
    "authors": [
      {
        "surname": "Lee",
        "given_name": "Sangmin"
      },
      {
        "surname": "Kim",
        "given_name": "Younghoon"
      },
      {
        "surname": "Kahng",
        "given_name": "Hyungu"
      },
      {
        "surname": "Lee",
        "given_name": "Soon-Kyo"
      },
      {
        "surname": "Chung",
        "given_name": "Seokhyun"
      },
      {
        "surname": "Cheong",
        "given_name": "Taesu"
      },
      {
        "surname": "Shin",
        "given_name": "Keeyong"
      },
      {
        "surname": "Park",
        "given_name": "Jeehyuk"
      },
      {
        "surname": "Kim",
        "given_name": "Seoung Bum"
      }
    ]
  },
  {
    "title": "Privacy-preserving in association rule mining using an improved discrete binary artificial bee colony",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113097",
    "abstract": "Association Rule Hiding (ARH) is the process of protecting sensitive knowledge using data transformation. Although there are some evolutionary-based ARH algorithms, they mostly focus on the itemset hiding instead of the rule hiding. Besides, unstable convergence to the global optimum solution and designing long solutions make them inappropriate in reducing side effects. They use the basic versions of evolutionary approaches, resulting in inappropriate performance in ARH domain where the search space is large and the algorithms easily get trapped in local optima. To deal with these problems, we propose a new rule hiding algorithm based on a binary Artificial Bee Colony (ABC) approach which has good exploration. However, we improve the binary ABC algorithm to enhance its poor exploitation by designing a new neighborhood generation mechanism to balance between exploration and exploitation. We called this algorithm Improved Binary ABC (IBABC). IBABC approach is coupled with our proposed rule hiding algorithm, called ABC4ARH, to select sensitive transactions for modification. To choose victim items, ABC4ARH formulates a heuristic. The performance of ABC4ARH algorithm on the side effects is demonstrated using extensive experiments conducted on five real datasets. Furthermore, the effectiveness of IBABC is verified using the uncapacitated facility location problem and 0–1 knapsack problem.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419308140",
    "keywords": [
      "Algorithm",
      "Arithmetic",
      "Artificial bee colony algorithm",
      "Artificial intelligence",
      "Association rule learning",
      "Binary number",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Convergence (economics)",
      "Data mining",
      "Domain (mathematical analysis)",
      "Economic growth",
      "Economics",
      "Evolutionary algorithm",
      "Gene",
      "Heuristic",
      "Knapsack problem",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Operating system",
      "Process (computing)",
      "Transformation (genetics)"
    ],
    "authors": [
      {
        "surname": "Telikani",
        "given_name": "Akbar"
      },
      {
        "surname": "Gandomi",
        "given_name": "Amir H."
      },
      {
        "surname": "Shahbahrami",
        "given_name": "Asadollah"
      },
      {
        "surname": "Naderi Dehkordi",
        "given_name": "Mohammad"
      }
    ]
  },
  {
    "title": "Characterizing Complexity and Self-Similarity Based on Fractal and Entropy Analyses for Stock Market Forecast Modelling",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113098",
    "abstract": "Complex systems constitute components that interact with one another and involve phenomena which are not always easy to understand in terms of their components and interactions. Alternative mathematical models have been developed so that the users’ tasks can be facilitated and an actual assistance can be provided for decision-making processes in case of every encountered incident which requires critical decision-making. Within this framework, financial systems can be regarded as complex systems with their volatile and vulnerable nature along with various parameters and interactions involved. Forecasting shifts in stock indices is crucial to validate the potential strategies of monetary mechanisms. Therefore, forecasting is an essential step in financial decision-making to manage data selection and attain robust prediction. Our purpose is to optimize the stock indices’ forecasting model in the stock indices dataset, constructed from the daily values. The following steps were applied to demonstrate the critical significance of Hurst exponent (HE) computed by Rescaled Range (R/S) fractal analysis when used as indicator in conjunction with Shannon entropy (SE) and Renyi entropy (RE) for the future forecasting ability of the stock indices. With this aim, the following stages were performed with indicators obtained from the applications and added into the dataset in the respective order. The first stage consists of: i) HE indicator, ii) Entropy based SE and RE indicators, iii) HE, SE and RE indicators. As the second stage, stock indices day-to-day valuation was evaluated using Multi Layer Regression (MLR), Support Vector Regression (SVR) and Feed Forward Back Propagation (FFBP) algorithms, applied for each indicator for comparative analysis. When compared with earlier works, no relevant work exists in the literature in which the algorithms and above-mentioned indicators have been used in conjunction with one another. This paper, through the multistage methodology and proposed model, demonstrates that HE is obviously a significant and critical determining indicator compared to RE and SE indicators for forecasting purposes. Consequently, experimental results demonstrate the accuracy and applicability of the proposed method. Thus, this study attempts to illustrate a new frontier in domains concerning critical decision-making processes in non-linear, dynamic, and volatile environments.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419308152",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Data mining",
      "Econometrics",
      "Economics",
      "Engineering",
      "Entropy (arrow of time)",
      "Finance",
      "Fractal",
      "Horse",
      "Hurst exponent",
      "Mathematical analysis",
      "Mathematics",
      "Mechanical engineering",
      "Paleontology",
      "Physics",
      "Quantum mechanics",
      "Statistics",
      "Stock (firearms)",
      "Stock market",
      "Stock market index",
      "Valuation (finance)"
    ],
    "authors": [
      {
        "surname": "Karaca",
        "given_name": "Yeliz"
      },
      {
        "surname": "Zhang",
        "given_name": "Yu-Dong"
      },
      {
        "surname": "Muhammad",
        "given_name": "Khan"
      }
    ]
  },
  {
    "title": "Discriminative globality and locality preserving graph embedding for dimensionality reduction",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113079",
    "abstract": "Graph embedding in dimensionality reduction has attracted much attention in the high-dimensional data analysis. Graph construction in graph embedding plays an important role in the quality of dimensionality reduction. However, the discrimination information and the geometrical distributions of data samples are not fully exploited for discovering the essential geometrical and discriminant structures of data and strengthening the pattern discrimination in graph constructions of graph embedding. To overcome the limitations of graph constructions, in this article we propose a novel graph-based dimensionality reduction method entitled discriminative globality and locality preserving graph embedding (DGLPGE) by designing the informative globality and locality preserving graph constructions. In the constructed graphs, bidirectional weights of edges are newly defined by considering both the geometrical distributions of each point of edges and the class discrimination. Using the adjacent weights of graphs, we characterize the intra-class globality preserving scatter, the inter-class globality preserving scatter and the locality preserving scatter to formulate the objective function of DGLPGE in order to optimize the projection of dimensionality reduction. Extensive experiments demonstrate that the proposed DGLPGE often outperforms the state-of-the-art dimensionality reduction methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419307961",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Dimensionality reduction",
      "Discriminative model",
      "Economics",
      "Embedding",
      "Globality",
      "Globalization",
      "Graph",
      "Graph embedding",
      "Linguistics",
      "Locality",
      "Market economy",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Gou",
        "given_name": "Jianping"
      },
      {
        "surname": "Yang",
        "given_name": "Yuanyuan"
      },
      {
        "surname": "Yi",
        "given_name": "Zhang"
      },
      {
        "surname": "Lv",
        "given_name": "Jiancheng"
      },
      {
        "surname": "Mao",
        "given_name": "Qirong"
      },
      {
        "surname": "Zhan",
        "given_name": "Yongzhao"
      }
    ]
  },
  {
    "title": "An expert system gap analysis and empirical triangulation of individual differences, interventions, and information technology applications in alertness of railroad workers",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113081",
    "abstract": "In this abstract we would like to provide some exciting concrete information including the article's main impact and significance on expert and intelligent systems. The main impact is that the PTC expert intelligent system fills in the gaps between the human and software decision making processes. This gap analysis is analyzed via empirical triangulation of rail worker data collected from its groups, individuals and the rail industry itself. We utilize an expert intelligent system PTC information technology application to both measure and to improve the alertness of the groups and workers in order to improve the overall safety of the railways through reduced human errors and failures to prevent accidents. Many individual differences in alertness among military, railroad, and other industry workers stem from a lack of sufficient sleep. This continues to be a concern in the railroad industry, even with the implementation of positive train control (PTC) expert system technology. Information technology aids such as PTC cannot prevent all accidents, and errors and failures with PTC may occur. Furthermore, drug interventions are a short-term solution for improving alertness. This study investigated the effect of sleep deprivation on the alertness of railroad signalmen at work, individual differences in alertness, and the information technology available to improve alertness. We investigated various information and communication technology control systems that can be used to maintain operational safety in the railroad industry in the face of incompatible circadian rhythms due to irregular hours, weekend work, and night operations. To fully explain individual differences after the adoption of technology, our approach posits the necessary parameters that one must consider for reason-oriented action, sequential updating, feedback, and technology acceptance in a unified model. This triangulation can help manage workers by efficiently increasing their productivity and improving their health. In our analysis we used R statistical software and Tableau. To test our theory, we issued an Apple watch to a locomotive engineer. The perceived usefulness, perceived ease of use, and actual use he reported led to an analysis of his sleep patterns that eventually ended in his adoption of a sleep apnea device and an improvement in his alertness and effectiveness. His adoption of the technology also resulted in a decrease in his use of chemical interventions to increase his alertness. Our model shows that the alertness of signalmen can be predicted. Therefore, we recommend that the alertness of all railroad workers be predicted given the safety limitations of PTC.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419307985",
    "keywords": [
      "Alertness",
      "Applied psychology",
      "Artificial intelligence",
      "Business",
      "Computer science",
      "Control (management)",
      "Empirical research",
      "Engineering",
      "Epistemology",
      "Information technology",
      "Mechanical engineering",
      "Operating system",
      "Philosophy",
      "Psychiatry",
      "Psychological intervention",
      "Psychology",
      "Risk analysis (engineering)",
      "Work (physics)"
    ],
    "authors": [
      {
        "surname": "Rodger",
        "given_name": "James A."
      }
    ]
  },
  {
    "title": "Drawing openness to experience from user generated contents: An interpretable data-driven topic modeling approach",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113073",
    "abstract": "Openness to experience, one of the essential individual characteristics, is of great theoretical and practical value in psychological and behavioral domains. Although typical machine learning methods can be utilized to extract individuals’ openness to experience from the large-scale textual data like the unprecedented massive user generated contents (UGCs), they are often regarded as “black boxes” because they are unable to provide knowledge about the influential factors of openness to experience. This is of no help for us to investigate why a particular level of openness to experience is predicted for an individual. In addition, high dimensionality and sparseness of textual data impairs the performance of the typical machine learning method in extracting individuals’ characteristics. In this study, we propose an interpretable data-driven mixture method for qualified modeling and predicting individuals’ openness to experience. The proposed method extends the latent Dirichlet allocation (LDA) to overcome the problem of high dimensionality and sparseness in modeling the textual data, and can effectively extract two influential variables, namely, the topic preference and the expressed emotional intensity, to make an accurate prediction and to help us fully understand individuals’ openness to experience lurking in the textual data. Experimental results indicate the effectiveness of the proposed method in drawing individuals’ openness to experience, and also validate the predictive ability of topic preference and expressed emotional intensity which are indicated in psychological literature to be influential factors of openness to experience.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419307900",
    "keywords": [
      "Artificial intelligence",
      "Cognitive psychology",
      "Computer science",
      "Curse of dimensionality",
      "Latent Dirichlet allocation",
      "Machine learning",
      "Mathematics",
      "Openness to experience",
      "Preference",
      "Psychology",
      "Social psychology",
      "Statistics",
      "Topic model"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Yishi"
      },
      {
        "surname": "Wei",
        "given_name": "Haiying"
      },
      {
        "surname": "Ran",
        "given_name": "Yaxuan"
      },
      {
        "surname": "Deng",
        "given_name": "Yang"
      },
      {
        "surname": "Liu",
        "given_name": "Dan"
      }
    ]
  },
  {
    "title": "An explainable AI decision-support-system to automate loan underwriting",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113100",
    "abstract": "Widespread adoption of automated decision making by artificial intelligence (AI) is witnessed due to specular advances in computation power and improvements in optimization algorithms especially in machine learning (ML). Complex ML models provide good prediction accuracy; however, the opacity of ML models does not provide sufficient assurance for their adoption in the automation of lending decisions. This paper presents an explainable AI decision-support-system to automate the loan underwriting process by belief-rule-base (BRB). This system can accommodate human knowledge and can also learn from historical data by supervised learning. The hierarchical structure of BRB can accommodates factual and heuristic rules. The system can explain the chain of events leading to a decision for a loan application by the importance of an activated rule and the contribution of antecedent attributes in the rule. A business case study on automation of mortgage underwriting is demonstrated to show that the BRB system can provide a good trade-off between accuracy and explainability. The textual explanation produced by the activation of rules could be used as a reason for denial of a loan. The decision-making process for an application can be comprehended by the significance of rules in providing the decision and contribution of its antecedent attributes.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419308176",
    "keywords": [
      "Artificial intelligence",
      "Automation",
      "Casualty insurance",
      "Computer science",
      "Decision rule",
      "Decision support system",
      "Economics",
      "Engineering",
      "Finance",
      "Heuristic",
      "Insurance policy",
      "Loan",
      "Machine learning",
      "Mechanical engineering",
      "Mortgage insurance",
      "Mortgage underwriting",
      "Operating system",
      "Process (computing)",
      "Underwriting"
    ],
    "authors": [
      {
        "surname": "Sachan",
        "given_name": "Swati"
      },
      {
        "surname": "Yang",
        "given_name": "Jian-Bo"
      },
      {
        "surname": "Xu",
        "given_name": "Dong-Ling"
      },
      {
        "surname": "Benavides",
        "given_name": "David Eraso"
      },
      {
        "surname": "Li",
        "given_name": "Yang"
      }
    ]
  },
  {
    "title": "Knowledge-based problem solving in physical product development––A methodological review",
    "journal": "Expert Systems with Applications: X",
    "year": "2020",
    "doi": "10.1016/j.eswax.2020.100025",
    "abstract": "The manufacturing of products at low maturity levels (referred to as physical product development) requires knowledge intensive nonconformance problem solving, yet constituting a major difficulty in industry. Due to the exponential increase of failure cost during the product development process however, problems have to be effectively remedied as early as possible. Facing shortened innovation cycles, problem solving efficiency simultaneously constitutes a competitive factor. The purpose of this theoretical review is therefore the analysis of relevant approaches contributing to knowledge-based problem solving in physical product development, to synthesize a comprehensive construct as well as to derive novel conceptualizations. The latter demonstrably emerges from natural language processing, case ontologies and machine-/deep learning support, embedded in a distributed case-based reasoning architecture. Building on this, we likewise encourage researchers and professionals to propose new studies dedicated to the field of problem solving in physical product development.",
    "link": "https://www.sciencedirect.com/science/article/pii/S2590188520300044",
    "keywords": [
      "Architecture",
      "Art",
      "Artificial intelligence",
      "Business",
      "Computer science",
      "Construct (python library)",
      "Developmental psychology",
      "Engineering",
      "Field (mathematics)",
      "Geometry",
      "Knowledge management",
      "Management science",
      "Marketing",
      "Mathematics",
      "Maturity (psychological)",
      "New product development",
      "Operating system",
      "Process (computing)",
      "Product (mathematics)",
      "Programming language",
      "Psychology",
      "Pure mathematics",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Burggräf",
        "given_name": "Peter"
      },
      {
        "surname": "Wagner",
        "given_name": "Johannes"
      },
      {
        "surname": "Weißer",
        "given_name": "Tim"
      }
    ]
  },
  {
    "title": "A neural network for semantic labelling of structured information",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113053",
    "abstract": "Intelligent systems rely on rich sources of information to make informed decisions. Using information from external sources requires establishing correspondences between the information and known information classes. This can be achieved with semantic labelling, which assigns known labels to structured information by classifying it according to computed features. The existing proposals have explored different sets of features, without focusing on what classification techniques are used. In this paper we present three contributions: first, insights on architectural issues that arise when using neural networks for semantic labelling; second, a novel implementation of semantic labelling that uses a state-of-the-art neural network classifier which achieves significantly better results than other four traditional classifiers; third, a comparison of the results obtained by the former network when using different subsets of features, comparing textual features to structural ones, and domain-dependent features to domain-independent ones. The experiments were carried away with datasets from three real world sources. Our results show that there is a need to develop more semantic labelling proposals with sophisticated classification techniques and large features catalogues.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419307705",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Classifier (UML)",
      "Computer science",
      "Criminology",
      "Domain (mathematical analysis)",
      "Labelling",
      "Mathematical analysis",
      "Mathematics",
      "Natural language processing",
      "Semantic network",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Ayala",
        "given_name": "Daniel"
      },
      {
        "surname": "Borrego",
        "given_name": "Agustín"
      },
      {
        "surname": "Hernández",
        "given_name": "Inma"
      },
      {
        "surname": "Ruiz",
        "given_name": "David"
      }
    ]
  },
  {
    "title": "Real-time purchase behavior recognition system based on deep learning-based object detection and tracking for an unmanned product cabinet",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113063",
    "abstract": "We propose a system to recognize purchasing behavior by detecting and tracking products in real time using only camera sensors in an unmanned product cabinet. To detect and track products in real time, we focused on the simultaneous pre-processing of videos from multiple cameras for robust product detection. After synchronizing multiple videos, unnecessary frames with relatively little information are removed based on change detection. An object score is measured on a frame-by-frame basis to select the most significant frames. Next, the target products are detected and tracked in the selected frames. Finally, the purchasing behavior of the detected product is recognized based on the tracking information. These processes were used to design an end-to-end recognition framework. The contribution of this paper is significant in that by redesigning the existing deep neural networks a real-time integrated system for a practical application was successfully realized without any bottleneck from multi-camera inputs to final object recognition process. Furthermore, the proposed object detection network shows comparable performance with the state-of-the-art methods. We performed intensive experiments to evaluate pure object detection performance as well as to evaluate various purchase/return scenarios. For example, for a basic purchase/return scenario, the proposed system achieved about 92% or more accuracy, which can be the actual level of commercialization.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419307808",
    "keywords": [
      "Artificial intelligence",
      "Bottleneck",
      "Computer science",
      "Computer vision",
      "Economics",
      "Embedded system",
      "Frame (networking)",
      "Object detection",
      "Operating system",
      "Operations management",
      "Pattern recognition (psychology)",
      "Process (computing)",
      "Purchasing",
      "Real-time computing",
      "Synchronizing",
      "Telecommunications",
      "Transmission (telecommunications)"
    ],
    "authors": [
      {
        "surname": "Kim",
        "given_name": "Dae Ha"
      },
      {
        "surname": "Lee",
        "given_name": "Seunghyun"
      },
      {
        "surname": "Jeon",
        "given_name": "Jungho"
      },
      {
        "surname": "Song",
        "given_name": "Byung Cheol"
      }
    ]
  },
  {
    "title": "Oblique Decision Tree Ensemble via Twin Bounded SVM",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113072",
    "abstract": "Ensemble methods with “perturb and combine” strategy have shown improved performance in the classification problems. Recently, random forest algorithm was ranked one among 179 classifiers evaluated on 121 UCI datasets. Motivated by this, we propose a new approach for the generation of oblique decision trees. At each non-leaf node, the training data samples are grouped in two categories based on the Bhattachrayya distance with randomly selected feature subset. Then, twin bounded support vector machine (TBSVM) is used to get two clustering hyperplanes such that each hyperplane is closer to data points of one group and as far as possible from the data points of other group. Based on these hyperplanes, each non-leaf node is splitted to generate the decision tree. In this paper, we used different base models like random forest (RaF), rotation forest (RoF), random sub rotation forest (RRoF) to generate the different oblique decision tree forests named as TBRaF, TBRoF and TBRRoF, respectively. In earlier oblique decision trees, like multisurface proximal support vector machine (MPSVM) based oblique decision trees, matrices are semi-positive definite and hence different regularization methods are required. However, no explicit regularization techniques need to be applied to the primal problems as the matrices in the proposed TBRaF, TBRoF and TBRRoF are positive definite. We evaluated the performance of the proposed models (TBRaF, TBRoF and TBRRoF) on 49 datasets taken from the UCI repository and on some real-world biological datasets (not in UCI). The experimental results and statistical tests conducted show that TBRaF and TBRRoF outperform other baseline methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419307894",
    "keywords": [
      "Artificial intelligence",
      "Bounded function",
      "Combinatorics",
      "Computer science",
      "Data mining",
      "Decision tree",
      "Hyperplane",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Oblique case",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Random forest",
      "Regularization (linguistics)",
      "Support vector machine",
      "Tree (set theory)"
    ],
    "authors": [
      {
        "surname": "Ganaie",
        "given_name": "M.A."
      },
      {
        "surname": "Tanveer",
        "given_name": "M."
      },
      {
        "surname": "Suganthan",
        "given_name": "P.N."
      }
    ]
  },
  {
    "title": "Portfolio formation with preselection using deep learning from long-term financial data",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113042",
    "abstract": "Portfolio theory is an important foundation for portfolio management which is a well-studied subject yet not fully conquered territory. This paper proposes a mixed method consisting of long short-term memory networks and mean-variance model for optimal portfolio formation in conjunction with the asset preselection, in which long-term dependences of financial time-series data can be captured. The experiment uses a large volume of sample data from the UK Stock Exchange 100 Index between March 1994 and March 2019. In the first stage, long short-term memory networks are used to forecast the return of assets and select assets with higher potential returns. After comparing the outcomes of the long short-term memory networks against support vector machine, random forest, deep neural networks, and autoregressive integrated moving average model, we discover that long short-term memory networks are appropriate for financial time-series forecasting, to beat the other benchmark models by a very clear margin. In the second stage, based on selected assets with higher returns, the mean-variance model is applied for portfolio optimisation. The validation of this methodology is carried out by comparing the proposed model with the other five baseline strategies, to which the proposed model clearly outperforms others in terms of the cumulative return per year, Sharpe ratio per triennium as well as average return to the risk per month of each triennium. i.e. potential returns and risks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419307596",
    "keywords": [
      "Asset allocation",
      "Autoregressive model",
      "Benchmark (surveying)",
      "Biology",
      "Computer science",
      "Econometrics",
      "Economics",
      "Finance",
      "Geodesy",
      "Geography",
      "Horse",
      "Management",
      "Modern portfolio theory",
      "Paleontology",
      "Physics",
      "Portfolio",
      "Portfolio optimization",
      "Project management",
      "Project portfolio management",
      "Quantum mechanics",
      "Rate of return on a portfolio",
      "Sharpe ratio",
      "Stock market",
      "Stock market index",
      "Term (time)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Wuyu"
      },
      {
        "surname": "Li",
        "given_name": "Weizi"
      },
      {
        "surname": "Zhang",
        "given_name": "Ning"
      },
      {
        "surname": "Liu",
        "given_name": "Kecheng"
      }
    ]
  },
  {
    "title": "Models and an exact method for the Unrelated Parallel Machine scheduling problem with setups and resources",
    "journal": "Expert Systems with Applications: X",
    "year": "2020",
    "doi": "10.1016/j.eswax.2020.100022",
    "abstract": "This paper deals with the Unrelated Parallel Machine scheduling problem with Setups and Resources (UPMSR) with the objective of minimizing makespan. Processing times and setups depend on machine and job. The necessary resources could be: specific resources for processing, needed for processing a job on a machine; specific resources for setups, needed to do the previous setup before a job is processed on a machine; shared resources, understanding these as unspecific resources that could also be needed in both processing or setup. The number of scarce resources depends on machine and job. As an industrial example, in a plastic processing plant molds are the specific resource for processing machines, cleaning equipment is the specific resource for setups and workers are the unspecific shared resource to operate processing machines and setup cleaning equipment. A mixed integer linear program is presented to model this problem. Also a three phase algorithm based on mathematical exact method is introduced. Model and algorithm are tested in a comprehensive and extensive computational campaign. Tests show good results for different combinations of useE of resources and in most cases come to less than 2.7% of gap against lower bound for instances of 400 jobs.",
    "link": "https://www.sciencedirect.com/science/article/pii/S2590188520300019",
    "keywords": [
      "Algorithm",
      "Computer science",
      "Distributed computing",
      "Embedded system",
      "Integer programming",
      "Job shop scheduling",
      "Mathematical optimization",
      "Mathematics",
      "Routing (electronic design automation)",
      "Scheduling (production processes)"
    ],
    "authors": [
      {
        "surname": "Fanjul-Peyro",
        "given_name": "Luis"
      }
    ]
  },
  {
    "title": "Efficient approach for incremental weighted erasable pattern mining with list structure",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113087",
    "abstract": "Erasable pattern mining is one of the important fields of frequent pattern mining. It diagnoses and solves the economic problems that arise in the manufacturing industry. The real-world database is continually accumulated over time, and each item has a different importance. Therefore, if we use conventional erasable pattern mining without considering the characteristics of the real-world database, less meaningful patterns can be extracted. Also, when mining a real-world database, the algorithm must be able to process operations quickly and efficiently. In this paper, in order to meet these requirements, we propose an algorithm which is implemented as a list structure for mining erasable patterns in an incremental database with weighted condition. Compared to existing state-of-the-art mining algorithms, the proposed algorithm performs pattern pruning by applying weighted condition to a dynamic database, so it extracts fewer candidate patterns and shows fast performance. We test our algorithms and the algorithms previously presented with various real datasets and synthetic datasets and obtained results such as run time, memory usage, scalability, and accuracy tests. By analyzing and comparing these experimental results, we show that the proposed algorithm has outstanding performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419308048",
    "keywords": [
      "Agronomy",
      "Algorithm",
      "Apriori algorithm",
      "Association rule learning",
      "Biology",
      "Computer science",
      "Data mining",
      "Database",
      "GSP Algorithm",
      "Operating system",
      "Process (computing)",
      "Pruning",
      "Scalability"
    ],
    "authors": [
      {
        "surname": "Nam",
        "given_name": "Hyoju"
      },
      {
        "surname": "Yun",
        "given_name": "Unil"
      },
      {
        "surname": "Yoon",
        "given_name": "Eunchul"
      },
      {
        "surname": "Lin",
        "given_name": "Jerry Chun-Wei"
      }
    ]
  },
  {
    "title": "PAROT: Translating natural language to SPARQL",
    "journal": "Expert Systems with Applications: X",
    "year": "2020",
    "doi": "10.1016/j.eswax.2020.100024",
    "abstract": "This paper provides a dependency based framework for converting natural language to SPARQL. We present a tool known as PAROT (which echos answers from ontologies) which is able to handle user’s queries that contain compound sentences, negation, scalar adjectives and numbered list. PAROT employs a number of dependency based heuristics to convert user’s queries to user’s triples. The user’s triples are then processed by the lexicon into ontology triples. It is these ontology triples that are used to construct SPARQL queries. From the experiments conducted, PAROT provides state of the art results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S2590188520300032",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Computer science",
      "History",
      "Linguistics",
      "Named graph",
      "Natural (archaeology)",
      "Natural language processing",
      "Philosophy",
      "RDF",
      "SPARQL",
      "Semantic Web"
    ],
    "authors": [
      {
        "surname": "Ochieng",
        "given_name": "Peter"
      }
    ]
  },
  {
    "title": "An efficient pattern growth approach for mining fault tolerant frequent itemsets",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113046",
    "abstract": "Mining fault tolerant (FT) frequent itemsets from transactional databases are computationally more expensive than mining exact matching frequent itemsets. Previous algorithms mine FT frequent itemsets using Apriori heuristic. Apriori-like algorithms generate exponential number of candidate itemsets including the itemsets that do not exist in the database. These algorithms require multiple scans of database for counting the support of candidate FT itemsets. In this paper we present a novel algorithm, which mines FT frequent itemsets using frequent pattern growth approach (FT-PatternGrowth). FT-PatternGrowth adopts a divide-and-conquer technique and recursively projects transactional database into a set of smaller projected transactional databases and mines FT frequent itemsets in each projected database by exploring only locally frequent items. This mines the complete set of FT frequent itemsets and substantially reduces those candidate itemsets that do not exist in the database. FT-PatternGrowth stores the transactional database in a highly condensed much smaller data structure called frequent pattern tree (FP-tree). The support of candidate itemsets are counted directly from the FP-tree without scanning the original database multiple times. This improves the processing speed of algorithm. Our experiments on benchmark databases indicates mining FT frequent itemsets using FT-PatternGrowth is highly efficient than Apriori-like algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419307638",
    "keywords": [
      "A priori and a posteriori",
      "Apriori algorithm",
      "Artificial intelligence",
      "Association rule learning",
      "Benchmark (surveying)",
      "Computer science",
      "Data mining",
      "Database",
      "Epistemology",
      "Geodesy",
      "Geography",
      "Heuristic",
      "Matching (statistics)",
      "Mathematical analysis",
      "Mathematics",
      "Philosophy",
      "Programming language",
      "Set (abstract data type)",
      "Statistics",
      "Tree (set theory)",
      "Very large database"
    ],
    "authors": [
      {
        "surname": "Bashir",
        "given_name": "Shariq"
      }
    ]
  },
  {
    "title": "Lexicon-Grammar based open information extraction from natural language sentences in Italian",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112954",
    "abstract": "In the last decade, the quantity of readily accessible text has grown rapidly and enormously, long exceeding the capacity of humans to read and understand it. One of the most interesting strategies proposed to fulfill this need is known as Open Information Extraction (OIE). It is essentially devised to read in sentences and rapidly extract one or more domain-independent coherent propositions, each represented by a verb relation and its arguments. Even though many OIE approaches exist for English, no significant research has been conducted about OIE on Italian texts. Due to the usage of language-specific features, OIE systems operating in other languages are not directly applicable for Italian. Therefore, this paper proposes, as first contribution, a novel approach to perform OIE for Italian language, based on standard linguistic structures to analyze sentences and on a set of verbal behavior patterns to extract information from them. These patterns are built combining a solid linguistic theoretical framework, i.e. Lexicon-Grammar (LG), and distributional profiles extracted from a contemporary Italian corpus, i.e. itWaC. Starting from simple sentences, the approach is able to determine elementary tuples, then, all their permutations, by adding complements and adverbials, and, finally, n-ary propositions, by granting syntactic invariance, preserving the overall grammaticality and also respecting some syntactic constraints and selection preferences, thus approximating a first level of semantic acceptability. As second contribution of this work, a gold standard dataset for the Italian language has been built from the itWaC corpus, aimed at being widely used to enable the experimental validation of OIE solutions. It has been manually and independently labeled by four Italian native speakers with all the n-ary propositions that can be extracted, following the criteria of grammaticality and acceptability, i.e. granting syntactic well-formedness and meaningfulness in the context. Finally, the proposed approach has been experimented and quantitatively validated on this gold standard dataset, also in comparison with an indirect approach translating input sentences and output propositions from Italian to English and vice versa and embedding an OIE approach for English, as well as with an OIE system for Italian previously presented by the authors. The results obtained have shown the effectiveness of the proposed approach in generating propositions with respect to these criteria of grammaticality and acceptability. Even if the approach has been evaluated for the Italian language, it is essentially based on linguistic resources produced by LG, which exist for many languages besides Italian and a representative corpus for the language under consideration. Given these premises, it has a general basis from a methodological perspective and can be proficiently extended also to other languages.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419306724",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Grammar",
      "Grammaticality",
      "Lexicon",
      "Linguistics",
      "Natural language",
      "Natural language processing",
      "Philosophy",
      "Programming language",
      "Selection (genetic algorithm)",
      "Set (abstract data type)",
      "Verb"
    ],
    "authors": [
      {
        "surname": "Guarasci",
        "given_name": "Raffaele"
      },
      {
        "surname": "Damiano",
        "given_name": "Emanuele"
      },
      {
        "surname": "Minutolo",
        "given_name": "Aniello"
      },
      {
        "surname": "Esposito",
        "given_name": "Massimo"
      },
      {
        "surname": "De Pietro",
        "given_name": "Giuseppe"
      }
    ]
  },
  {
    "title": "A feature transfer enabled multi-task deep learning model on medical imaging",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112957",
    "abstract": "Object detection, segmentation, and classification are three common tasks in medical image analysis. Multi-task deep learning (MTL) tackles these three tasks jointly, which provides two advantages—saving computational cost and improving robustness against overfitting. Existing multi-task deep models start with learning each task as an individual objective in parallel and then integrate the tasks at the end of the architecture with one cost function. Such architecture fails to take advantage of the combined power of the features from each individual task at an early stage of the training. In this research, we propose a new architecture, FT-MTL-Net, an MTL model enabled by feature transfer. Traditional transfer learning deals with the same or similar task (e.g., classification) from different data sources (a.k.a. domain). The underlying assumption is that the knowledge gained from various source domains may help the learning task on the target domain. Our proposed FT-MTL-Net utilizes the different tasks from the same domain. Considering that features from the tasks are different views of the domain, the combined feature maps can be well exploited using knowledge from multiple views to enhance the generalizability. To evaluate the validity of the proposed approach, FT-MTL-Net is compared with models from literature including eight classification models, four detection models, and three segmentation models using a publicly available Full Filed Digital Mammogram dataset for breast cancer diagnosis. Experimental results show that the proposed FT-MTL-Net outperforms the competing models in classification and detection and has comparable results in segmentation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741741930675X",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Deep learning",
      "Economics",
      "Feature (linguistics)",
      "Gene",
      "Generalizability theory",
      "Linguistics",
      "Machine learning",
      "Management",
      "Mathematics",
      "Multi-task learning",
      "Overfitting",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Robustness (evolution)",
      "Segmentation",
      "Statistics",
      "Task (project management)",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Gao",
        "given_name": "Fei"
      },
      {
        "surname": "Yoon",
        "given_name": "Hyunsoo"
      },
      {
        "surname": "Wu",
        "given_name": "Teresa"
      },
      {
        "surname": "Chu",
        "given_name": "Xianghua"
      }
    ]
  },
  {
    "title": "A Lagrangean based solution algorithm for the knapsack problem with setups",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113077",
    "abstract": "We consider the knapsack problem with setups which is a generalization of the classical knapsack problem where the items belong to families and an item can be placed in the knapsack only if its family is selected. The problem has received increasing attention by researchers because of its theoretical significance and practical applications related to resource allocation. This paper presents an algorithm based on a Lagrangean relaxation of the problem that produces solutions whose quality can be assessed automatically with the algorithm itself without ever knowing the optimal solutions. We report results of an extensive computational study which show that the method can solve near optimally very large instances of the problem with up to 500 families and 2,000,000 items in reasonable amount of time. This study shows the merit of using the Lagrangean relaxation method to solve the current problem when the constraints to relax are chosen properly.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419307948",
    "keywords": [
      "Algorithm",
      "Change-making problem",
      "Computer science",
      "Continuous knapsack problem",
      "Cutting stock problem",
      "Epistemology",
      "Generalization",
      "Generalized assignment problem",
      "Knapsack problem",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Optimization problem",
      "Philosophy",
      "Psychology",
      "Quality (philosophy)",
      "Relaxation (psychology)",
      "Social psychology"
    ],
    "authors": [
      {
        "surname": "Amiri",
        "given_name": "Ali"
      }
    ]
  },
  {
    "title": "Keystroke dynamics obfuscation using key grouping",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113091",
    "abstract": "Keystroke dynamics is one of the most widely adopted identity verification techniques in remote systems. It is based on modeling users’ specific patterns of typing on the keyboard. When utilized in conjunction with the commonly used passwords, the use of keystroke dynamics can dramatically increase the level of security without interfering with the user experience. However, aspects of keystroke dynamics that applied on passwords, such as processing keystroke events and storing feature vectors or user models, can expose users to identity theft and a new set of privacy risks, thus questioning the added value of keystroke dynamics. In addition, common encryption techniques will be unable to mitigate these threats, since the user's behavior changes from one session to another. In this paper, we suggest key grouping as an obfuscation method to ensure keystroke dynamics privacy. When applied on the keystroke events, the key grouping dramatically reduces the possibility of password theft. To perform the key grouping optimally, we present a novel method which produces groups that can integrated with any keystroke dynamics algorithm. Our method divides the keys into groups using hierarchical clustering with dedicated statistical heuristics algorithm. We tested our method's key grouping output on five keystroke dynamics algorithms using a public dataset and managed to show a consistent improvement of up to 7% in the AUC over other, more intuitive key groupings and random key groupings.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419308085",
    "keywords": [
      "Authentication (law)",
      "Computer science",
      "Computer security",
      "Data mining",
      "Encryption",
      "Key (lock)",
      "Keystroke dynamics",
      "Keystroke logging",
      "Obfuscation",
      "Password",
      "S/KEY"
    ],
    "authors": [
      {
        "surname": "Hazan",
        "given_name": "Itay"
      },
      {
        "surname": "Margalit",
        "given_name": "Oded"
      },
      {
        "surname": "Rokach",
        "given_name": "Lior"
      }
    ]
  },
  {
    "title": "Text document summarization using word embedding",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112958",
    "abstract": "Automatic text summarization essentially condenses a long document into a shorter format while preserving its information content and overall meaning. It is a potential solution to the information overload. Several automatic summarizers exist in the literature capable of producing high-quality summaries, but they do not focus on preserving the underlying meaning and semantics of the text. In this paper, we capture and preserve the semantics of text as the fundamental feature for summarizing a document. We propose an automatic summarizer using the distributional semantic model to capture semantics for producing high-quality summaries. We evaluated our summarizer using ROUGE on DUC-2007 dataset and compare our results with other four different state-of-the-art summarizers. Our system outperforms the other reference summarizers leading us to the conclusion that usage of semantic as a feature for text summarization provides improved results and helps to further reduce redundancies from the input source.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419306761",
    "keywords": [
      "Artificial intelligence",
      "Automatic summarization",
      "Computer science",
      "Embedding",
      "Feature (linguistics)",
      "Focus (optics)",
      "Information overload",
      "Information retrieval",
      "Linguistics",
      "Meaning (existential)",
      "Natural language processing",
      "Optics",
      "Philosophy",
      "Physics",
      "Programming language",
      "Psychology",
      "Psychotherapist",
      "Semantics (computer science)",
      "Word (group theory)",
      "Word embedding",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Mohd",
        "given_name": "Mudasir"
      },
      {
        "surname": "Jan",
        "given_name": "Rafiya"
      },
      {
        "surname": "Shah",
        "given_name": "Muzaffar"
      }
    ]
  },
  {
    "title": "Video on demand recommender system for internet protocol television service based on explicit information fusion",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113045",
    "abstract": "Internet protocol television (IPTV) provides video on demand (VOD), internet service, and real-time broadcasting to users as a service that combines broadcasting and communication technology. Among various services, the sales of VOD are profitable because VODs offer relatively strong direct revenue models in IPTV services. However, the development of a VOD recommender system for IPTV service is highly challenging owing to the lack of explicit preference information of users in an IPTV environment. Previous studies for IPTV VOD recommender systems have attempted to solve the data sparsity problem through implicit preference information; however, it is better to utilize explicit preference information to improve the performance of system. Recently, IPTV service providers have provided their own over-the-top (OTT) services such that explicit preference information of users for items can be combined. Therefore, we proposed a novel information fusion method for an IPTV VOD recommender system that integrates the explicit information of both IPTV and OTT services. In addition, we utilized the probabilistic matrix factorization, that guarantees high performance in most recommender systems, as a recommender algorithm in this study. Finally, we conducted comparative evaluations based on various metrics and validated that the information fusion of IPTV and OTT services contribute to the IPTV VOD recommender system.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419307626",
    "keywords": [
      "Broadcasting (networking)",
      "Computer network",
      "Computer science",
      "Economics",
      "Economy",
      "IPTV",
      "Microeconomics",
      "Preference",
      "Recommender system",
      "Service (business)",
      "Service provider",
      "The Internet",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Seo",
        "given_name": "Young-Duk"
      },
      {
        "surname": "Lee",
        "given_name": "Euijong"
      },
      {
        "surname": "Kim",
        "given_name": "Young-Gab"
      }
    ]
  },
  {
    "title": "Simultaneous feature selection and heterogeneity control for SVM classification: An application to mental workload assessment",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112988",
    "abstract": "In this study, an expert system is presented for analyzing the mental workload of interacting with a mobile phone while facing common daily tasks. Psychophysiological signals were collected from various devices, each characterized by a different cost and obtrusiveness. To deal with user-level signal data, a support vector machine-based feature selection approach is proposed. Given the limited person-level information available, our goal was to construct robust models by pooling population-level information across users (as a heterogeneity control). A single optimization problem that combines four objectives is proposed: model, margin maximization, feature selection, and heterogeneity control. The costs of using the devices were estimated, leading to a decision tool that allowed experiment designers to evaluate the marginal benefit of using a given device in terms of performance and its cost.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419307067",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Construct (python library)",
      "Control (management)",
      "Data mining",
      "Demography",
      "Economics",
      "Feature (linguistics)",
      "Feature selection",
      "Linguistics",
      "Machine learning",
      "Margin (machine learning)",
      "Maximization",
      "Microeconomics",
      "Mobile phone",
      "Operating system",
      "Philosophy",
      "Pooling",
      "Population",
      "Programming language",
      "Selection (genetic algorithm)",
      "Sociology",
      "Support vector machine",
      "Telecommunications",
      "Workload"
    ],
    "authors": [
      {
        "surname": "Maldonado",
        "given_name": "Sebastián"
      },
      {
        "surname": "López",
        "given_name": "Julio"
      },
      {
        "surname": "Jimenez-Molina",
        "given_name": "Angel"
      },
      {
        "surname": "Lira",
        "given_name": "Hernán"
      }
    ]
  },
  {
    "title": "A hybrid localization model using node segmentation and improved particle swarm optimization with obstacle-awareness for wireless sensor networks",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113044",
    "abstract": "Other than energy consumption, precision is of the utmost importance in node localization. Various wireless-sensor-network applications require the accurate information of sensor nodes’ locations. For instance, an enemy intrusion detection system (e.g., geo-fencing) needs accurate sensor nodes’ locations to detect where intruding enemies are located. As practical examples, forest fire, landslide, and water quality monitoring systems require the early identification of root causes’ exact locations before they can widely spread. In general, range-based localization techniques often yield higher accuracies because the localization estimation can be directly derived from the distance between hops and can leverage received signal strength indicator (RSSI) values but require model approximation of various hops and distances as in range-free localization techniques. However, the important factor that affects the accuracies is sensor node positioning, especially when sensor nodes (SNs) are spread across areas filled with obstructions causing less localization accuracy. Due to the diffraction caused by obstructions, the approximate distances between pairs of anchor nodes and unknown nodes using RSSI can differ substantially from the actual values. This research, therefore, aims to improve sensor node localization in situations where SNs are in areas with obstructions. We propose a novel technique, node segmentation with improved particle swarm optimization (NS-IPSO) that divides SNs into segments to improve the accuracy of the estimated distances between pairs of anchor nodes and unknown nodes. First, we determine candidate sensor nodes that could potentially be used to segment anchor nodes in the area. Such sensor nodes (STs) are those on the shortest paths between anchor nodes that appear more often than the average appearances of all sensor nodes. Then, segment nodes (SMs, sensor nodes for segmenting the anchor nodes) are selected from all the other STs based on certain specified conditions. To further improve the localization precision, we enhance the fitness function for each anchor node by taking into account the number of hops between each anchor node and unknown nodes. Furthermore, we enhance particle swarm optimization (PSO) by considering only particles that do not change positions to possibly reduce the chance of the local optimal trap. In this research, we test our proposed scheme's performance considering three forms of sensor node positioning: C-shape, H-shape, and S-shape. The simulation results show that the proposed scheme achieves higher accuracy in comparison with the recent state-of-the-art methods, i.e., hybrid discrete PSO (HDPSO), Hybrid PSO, approximate distances node localization (ADNL), the weight-search localization algorithm (WSLA), and min-max PSO techniques, particularly the situation where sensor nodes are in areas with obstacles.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419307614",
    "keywords": [
      "Acoustics",
      "Artificial intelligence",
      "Computer network",
      "Computer science",
      "Law",
      "Machine learning",
      "Mobile robot",
      "Node (physics)",
      "Obstacle",
      "Obstacle avoidance",
      "Particle swarm optimization",
      "Physics",
      "Political science",
      "Real-time computing",
      "Robot",
      "Segmentation",
      "Swarm behaviour",
      "Telecommunications",
      "Wireless",
      "Wireless sensor network"
    ],
    "authors": [
      {
        "surname": "Phoemphon",
        "given_name": "Songyut"
      },
      {
        "surname": "So-In",
        "given_name": "Chakchai"
      },
      {
        "surname": "Leelathakul",
        "given_name": "Nutthanon"
      }
    ]
  },
  {
    "title": "Top-K interesting preference rules mining based on MaxClique",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113043",
    "abstract": "In order to fully considered context constraints and eliminate the redundancy of preferences in the personalized queries in the database, a Top-K conditional preference mining framework based on maximal clique is proposed. Firstly, a conditional constraint model i + ≻ i − ∣X is proposed, which means that users prefer i + to i − under the constraint of X. Based on this model, we design a MPM algorithm to efficiently obtain users contextual preferences by using graphic model and new pruning strategies. Then, we propose an updated belief system and the concept of common belief. The belief system is constructed by using users common preferences, and combined with Bayesian approach to acquire users’ Top-K preference rules. Finally, the experiment result indicates that MPM algorithm is far more efficient than three classic or leading-edge mining algorithms in mining all rules and has comparable performance with 2 state-of-the-art Top-K methods. Moreover, the experiment results on Recall, precision and F1-Measure show that MPM algorithm comprehensively outperforms than six kinds various algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419307602",
    "keywords": [
      "Agronomy",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Constraint (computer-aided design)",
      "Context (archaeology)",
      "Data mining",
      "Geometry",
      "Machine learning",
      "Mathematics",
      "Operating system",
      "Paleontology",
      "Precision and recall",
      "Preference",
      "Pruning",
      "Redundancy (engineering)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Tan",
        "given_name": "Zheng"
      },
      {
        "surname": "Yu",
        "given_name": "Hang"
      },
      {
        "surname": "Wei",
        "given_name": "Wei"
      },
      {
        "surname": "Liu",
        "given_name": "Jinglei"
      }
    ]
  },
  {
    "title": "Customized and knowledge-centric service design model integrating case-based reasoning and TRIZ",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113062",
    "abstract": "Aiming at accelerating customized innovative service design, a novel knowledge-centric innovative service design (KISD) model is proposed through integrating memory-oriented-method case-based reasoning (CBR) and non-memory-oriented-method theory of inventive problem solving (TRIZ) to generate abundant ideas efficiently. Based on KISD, three key design approach phases are integrated: domain requirement acquisition (DRA), knowledge-centric resolution generation (KRG), and customized design knowledge-reasoning (CDK) phases. In the DRA phase, customer knowledge hierarchy is adopted to elicit customer requirements. In the KRG phase, CBR and TRIZ contradiction analysis are conducted and innovation principles are generated for constructing a functional knowledge hierarchy. In the CDK phase, quality function deployment analysis is conducted to evaluate and determine suitable new service functions. The case study of designing and developing a new shopping navigation service system for a shopping mall are presented to demonstrate the approach in practice. An empirical verification is conducted to verify the feasibility of the proposed approach and the satisfaction of the service.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419307791",
    "keywords": [
      "Artificial intelligence",
      "Business",
      "Case-based reasoning",
      "Computer science",
      "Domain knowledge",
      "Economics",
      "Economy",
      "Engineering",
      "Knowledge management",
      "Marketing",
      "New product development",
      "Process management",
      "Quality function deployment",
      "Service (business)",
      "Software engineering",
      "Systems engineering",
      "TRIZ"
    ],
    "authors": [
      {
        "surname": "Lee",
        "given_name": "Ching-Hung"
      },
      {
        "surname": "Chen",
        "given_name": "Chun-Hsien"
      },
      {
        "surname": "Li",
        "given_name": "Fan"
      },
      {
        "surname": "Shie",
        "given_name": "An-Jin"
      }
    ]
  },
  {
    "title": "Merged Tree-CAT: A fast method for building precise computerized adaptive tests based on decision trees",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113066",
    "abstract": "Over the last few years, there has been an increasing interest in the creation of Computerized Adaptive Tests (CATs) based on Decision Trees (DTs). Among the available methods, the Tree-CAT method has been able to demonstrate a mathematical equivalence between both techniques. However, this method has the inconvenience of requiring a high performance cluster while taking a few days to perform its computations. This article presents the Merged Tree-CAT method, which extends the Tree-CAT technique, to create CATs based on DTs in just a few seconds in a personal computer. In order to do so, the Merged Tree-CAT method controls the growth of the tree by merging those branches in which both the distribution and the estimation of the latent level are similar. The performed experiments show that the proposed method obtains estimations of the latent level which are comparable to the obtained by the state-of-the-art techniques, while drastically reducing the computational time.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419307833",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computation",
      "Computer science",
      "Data mining",
      "Decision tree",
      "Discrete mathematics",
      "Equivalence (formal languages)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Tree (set theory)",
      "Tree traversal",
      "k-d tree"
    ],
    "authors": [
      {
        "surname": "Rodríguez-Cuadrado",
        "given_name": "Javier"
      },
      {
        "surname": "Delgado-Gómez",
        "given_name": "David"
      },
      {
        "surname": "Laria",
        "given_name": "Juan C."
      },
      {
        "surname": "Rodríguez-Cuadrado",
        "given_name": "Sara"
      }
    ]
  },
  {
    "title": "Reward-driven U-Net training for obstacle avoidance drone",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113064",
    "abstract": "Along with the fast progress in deep learning, an autonomous drone with obstacle avoidance capability has been studied mainly by two machine learning paradigms, i.e. supervised learning, and reinforcement learning. The former has some advantages since the trained network is light and fast, but it needs a large amount of data that requires laborious manual labeling. With the latter, such a drawback can be overcome as an agent learns by itself in a simulated environment, although the gap between the real and simulated one has to be minimized in the end. This study proposes a new framework where a supervised segmentation network is trained with labels made by an actor-critic network in a reward-driven manner, wherein this U-Net based network infers the next moving direction from the sequence of input images. For the actor-critic part, several recent policy gradient algorithms have been tested for controlling the drone with the continuous action space. After training in the Airsim simulation environment, the model is transferred to a Bebop drone flying in the real environment, built as a reconfigurable maze using panels and a hoop. The result suggests that our network enables the drone to navigate through the obstacles using only monocular RGB input in the trained environment as well as in the reconfigured ones without retraining.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741741930781X",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Business",
      "Computer science",
      "Computer vision",
      "Drone",
      "Genetics",
      "International trade",
      "Law",
      "Machine learning",
      "Meteorology",
      "Mobile robot",
      "Obstacle",
      "Obstacle avoidance",
      "Physics",
      "Political science",
      "Real-time computing",
      "Reinforcement learning",
      "Retraining",
      "Robot",
      "Segmentation",
      "Training (meteorology)"
    ],
    "authors": [
      {
        "surname": "Shin",
        "given_name": "Sang-Yun"
      },
      {
        "surname": "Kang",
        "given_name": "Yong-Won"
      },
      {
        "surname": "Kim",
        "given_name": "Yong-Guk"
      }
    ]
  },
  {
    "title": "Role of inventory and assets in shareholder value creation",
    "journal": "Expert Systems with Applications: X",
    "year": "2020",
    "doi": "10.1016/j.eswax.2020.100027",
    "abstract": "In this study is examined the role of inventories and assets in the financial and shareholder value creation of a company. Research builds several Data Envelopment Analysis (DEA) models (staged), and tests their connections with each other. Research concerns publicly traded manufacturing and trade companies of Finland and three Baltic States (Estonia, Latvia and Lithuania) during the years 2010–2018. Logical and in two stages proceeding DEA efficiency model gets statistical significance, and there is support that inventory and asset related measures will lead to revenue, profits and cash flow, which together will eventually result in higher shareholder value (like stated in operations and supply chain management theories such as theory of constraints). However, this finding has weakness as explanation power is low, and there is a lot of noise. It could also be so that inventories and assets are part of bunch of other inputs, which together directly create shareholder value. Therefore, it remains as an open question whether inventory and assets should be managed through classical and logical stages in companies through organization hierarchy, or if inventory and assets should be just a part of group of factors, which together aim to increase shareholder value.",
    "link": "https://www.sciencedirect.com/science/article/pii/S2590188520300068",
    "keywords": [
      "Asset (computer security)",
      "Business",
      "Cash flow",
      "Computer science",
      "Computer security",
      "Corporate governance",
      "Economics",
      "Finance",
      "Hierarchy",
      "Industrial organization",
      "Machine learning",
      "Market economy",
      "Microeconomics",
      "Revenue",
      "Shareholder",
      "Shareholder value",
      "Value (mathematics)"
    ],
    "authors": [
      {
        "surname": "Hilmola",
        "given_name": "Olli-Pekka"
      }
    ]
  },
  {
    "title": "Acoustic scene classification using deep CNN with fine-resolution feature",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113067",
    "abstract": "Convolutional neural networks with spectrogram feature representation for acoustic scene classification are attracting more and more attentions due to its favorable performance. However, most of the existing methods are still restricted to the tradeoff between the minimum coverage area across time-frequency feature representation, i.e. time-frequency feature resolution, and the depth of CNN models. Thus, it is unfeasible to improve the performance by simply deepening networks. In this paper, fine-resolution convolutional neural network (FRCNN) is proposed to embrace the progress in very deep architecture, feature fusion and convolutional operation. Specifically, lateral construction is applied to generate a fine-resolution feature map with semantic information, and depth-wise separable convolution is utilized to reduce the number of trainable parameters. Extensive experiments demonstrate that the proposed FRCNN exhibits high performance on several metrics, with low computational complexity.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419307845",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Deep learning",
      "Feature (linguistics)",
      "Feature extraction",
      "Law",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Resolution (logic)",
      "Spectrogram"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Tao"
      },
      {
        "surname": "Liang",
        "given_name": "Jinhua"
      },
      {
        "surname": "Ding",
        "given_name": "Biyun"
      }
    ]
  },
  {
    "title": "A comprehensive survey on the biometric recognition systems based on physiological and behavioral modalities",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113114",
    "abstract": "Biometrics is the branch of science that deals with the identification and verification of an individual based on the physiological and behavioral traits. These traits or identifiers are permanent, unique and can separate one individual from another. Biometric recognition systems integrate complex definitional, technological and operational selection under various contexts. The systems are not going to replace the authentication tools and technologies, but the combination of biometric approaches and authentication methods to help in improving the security aspects of the applications where user cooperation can be inferred. Biometric based recognition methods and tools have become popular for the development of many useful, challenging and widely accepted applications such as security issues, surveillance, forensic investigations, fraudulent technologies, identity access management and access control. These systems also help to identify an individual in group of industrial networks, home/office building and control system. For the successful implementation of the biometric systems, deep artificial neural networks are in great demand. These systems can be built up either on the single modality or multiple modalities. This article explicates the comprehensive and deep survey that compactly and systematically summarizes the literature work done on unimodal and multimodal biometric systems and analyzes the feature extraction techniques, classifiers, datasets, results, efficiency and reliability of the system with high and multi-dimensional perspectives. This article also justifies in detail the classical methods, influential methods and taxonomy based on the biometric attributes. The goal is to aware the researchers of this area regarding various dimensions for the development of biometric systems to enhance the security aspects. The article begins with the fundamentals, types, need of system, challenges, uncertainties, motivations and then to the survey work. The tabular representation prepares for each biometric trait shows the author, year, major findings and results achieved with the synthesis analysis and the evaluation. The article finally ends up with the 3D biometric, a future perspective and concluding remarks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419308310",
    "keywords": [
      "Access control",
      "Artificial intelligence",
      "Authentication (law)",
      "Biology",
      "Biometrics",
      "Botany",
      "Computer science",
      "Computer security",
      "Data science",
      "Identification (biology)",
      "Identity management",
      "Iris recognition",
      "Modalities",
      "Modality (human–computer interaction)",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Dargan",
        "given_name": "Shaveta"
      },
      {
        "surname": "Kumar",
        "given_name": "Munish"
      }
    ]
  },
  {
    "title": "Distracting users as per their knowledge: Combining linked open data and word embeddings to enhance history learning",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113051",
    "abstract": "Organizations that preserve and promote heritage must meet the expectatives of sophisticated visitors who, far from wanting simply to be informed, desire to explore engaging and innovative technology-driven experiences which consider their particular interests and encourage them to discover more. We describe an approach based on quiz games that can be exploited in the deployment of such challenging experiences. The game consists of raising multiple-choice questions about a particular theme which is introduced by a Humanities expert through a brief narrative. Given the input text, a question and its right answer, our strategy provides the expert with a set of wrong alternatives (called distractors). These options are chosen from a (semi)automatically-built tailor-made corpus of documents by considering each player’s level of knowledge on the game theme and exploiting Linked Open Data initiatives and natural language processing. On the one hand, automatic selection of distractors assists the Humanities expert to create games about very diverse topics without needing to be a specialist in all of them. On the other one, distractors are related to the right answer of each question in an appealing and meaningful way, which contributes to arouse the visitors’ curiosity and their possible interest in exploring similar experiences in future visits. The work has been experimentally validated, achieving better results than a previous distractor identification strategy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419307687",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Computer science",
      "Curiosity",
      "Identification (biology)",
      "Linguistics",
      "Narrative",
      "Operating system",
      "Philosophy",
      "Programming language",
      "Psychology",
      "Selection (genetic algorithm)",
      "Set (abstract data type)",
      "Social psychology",
      "Software deployment",
      "Theme (computing)",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Blanco-Fernández",
        "given_name": "Yolanda"
      },
      {
        "surname": "Gil-Solla",
        "given_name": "Alberto"
      },
      {
        "surname": "Pazos-Arias",
        "given_name": "José J."
      },
      {
        "surname": "Ramos-Cabrer",
        "given_name": "Manuel"
      },
      {
        "surname": "Daif",
        "given_name": "Abdullah"
      },
      {
        "surname": "López-Nores",
        "given_name": "Martín"
      }
    ]
  },
  {
    "title": "SOINN+, a Self-Organizing Incremental Neural Network for Unsupervised Learning from Noisy Data Streams",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113069",
    "abstract": "The goal of continuous learning is to acquire and fine-tune knowledge incrementally without erasing already existing knowledge. How to mitigate this erasure, known as catastrophic forgetting, is a grand challenge for machine learning, specifically when systems are trained on evolving data streams. Self-organizing incremental neural networks (SOINN) are a class of neural networks that are designed for continuous learning from non-stationary data. Here, we propose a novel method, SOINN+, which is fundamentally different from the previous generations of SOINN with respect to how “forgetting” is modeled. To achieve a more graceful forgetting, we developed three new concepts: idle time, trustworthiness, and unutility of a node. SOINN+ learns a topology-preserving mapping of the input data to a network structure, which reveals clusters of arbitrary shapes in streams of noisy data. Our experiments with synthetic and real-world data sets showed that SOINN+ can maintain discovered structures even under sudden and recurring concept drifts.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419307869",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Data mining",
      "Data stream",
      "Data stream mining",
      "Forgetting",
      "Linguistics",
      "Machine learning",
      "Philosophy",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Wiwatcharakoses",
        "given_name": "Chayut"
      },
      {
        "surname": "Berrar",
        "given_name": "Daniel"
      }
    ]
  },
  {
    "title": "Improving performances of Top-N recommendations with co-clustering method",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113078",
    "abstract": "Collaborative filtering has been widely used in many applications. The typical idea is to identify preferences of users by utilizing their interaction data over the whole items. However, this is sometimes inaccurate since users might share different preferences with different sets of items. In this paper, we put forward a new recommendation method based on collaborative filtering called User-Item Community Detection based Recommendation (UICDR) method. The new parameter-free and scalable community detection method is modified from our previous work. We derive a unipartite form of bipartite modularity and put forward a new network representation. By constructing a bipartite network with user-item interaction data, we first partitions users and items into several subgroups. After getting clusters with tightly linked users and items, traditional collaborative filtering models can be trained for each cluster. The results on four real-world data sets show that, the proposed UICDR significantly improves the performances of Top-N recommendations of several traditional collaborative filtering methods. In addition, UICDR is helpful to cold-start problem.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741741930795X",
    "keywords": [
      "Aerospace engineering",
      "Artificial intelligence",
      "Biology",
      "Bipartite graph",
      "Cluster analysis",
      "Cold start (automotive)",
      "Collaborative filtering",
      "Computer science",
      "Data mining",
      "Database",
      "Engineering",
      "Genetics",
      "Graph",
      "Information retrieval",
      "Law",
      "Modularity (biology)",
      "Political science",
      "Politics",
      "Recommender system",
      "Representation (politics)",
      "Scalability",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Feng",
        "given_name": "Liang"
      },
      {
        "surname": "Zhao",
        "given_name": "Qianchuan"
      },
      {
        "surname": "Zhou",
        "given_name": "Cangqi"
      }
    ]
  },
  {
    "title": "Real-time biomechanical modeling of the liver using Machine Learning models trained on Finite Element Method simulations",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113083",
    "abstract": "The development of accurate real-time models of the biomechanical behavior of different organs and tissues still poses a challenge in the field of biomechanical engineering. In the case of the liver, specifically, such a model would constitute a great leap forward in the implementation of complex applications such as surgical simulators, computed-assisted surgery or guided tumor irradiation. In this work, a relatively novel approach for developing such a model is presented. It consists in the use of a machine learning algorithm, which provides real-time inference, trained on tens of thousands of simulations of the biomechanical behavior of the liver carried out by the finite element method on more than 100 different liver geometries. Considering a target accuracy threshold of 3 mm for the Euclidean Error, four different scenarios were modeled and assessed: a single liver with an arbitrary force applied (99.96% of samples within the accepted error range), a single liver with two simultaneous forces applied (99.84% samples in range), a single liver with different material properties and an arbitrary force applied (98.46% samples in range), and a much more general model capable of modeling the behavior of any liver with an arbitrary force applied (99.01% samples in range for the median liver). The results show that the Machine Learning models perform extremely well on all the scenarios, managing to keep the Mean Euclidean Error under 1 mm in all cases. Furthermore, the proposed model achieves working frequencies above 100Hz on modest hardware (with frequencies above 1000Hz being easily achievable on more powerful GPUs) thus fulfilling the real-time requirements. These results constitute a remarkable improvement in this field and may involve a prompt implementation in clinical practice.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419308000",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Composite material",
      "Computer science",
      "Engineering",
      "Euclidean distance",
      "Euclidean geometry",
      "Finite element method",
      "Geometry",
      "Inference",
      "Machine learning",
      "Materials science",
      "Mathematics",
      "Range (aeronautics)",
      "Simulation",
      "Structural engineering"
    ],
    "authors": [
      {
        "surname": "Pellicer-Valero",
        "given_name": "Oscar J."
      },
      {
        "surname": "Rupérez",
        "given_name": "María José"
      },
      {
        "surname": "Martínez-Sanchis",
        "given_name": "Sandra"
      },
      {
        "surname": "Martín-Guerrero",
        "given_name": "José D."
      }
    ]
  },
  {
    "title": "Portfolio management via two-stage deep learning with a joint cost",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113041",
    "abstract": "Portfolio management is a series of processes that maximize returns and minimize risk by allocating assets efficiently. Along with the developments in machine learning technology, it has been studied to apply machine learning methods to prediction-based portfolio management. However, such methods have a few limitations. First, they do not consider the relations between assets for the prediction. In addition, the studies commonly focus on the prediction accuracy, neglecting the construction of portfolios. Furthermore, the methods have usually been evaluated with index data, which hardly represent actual prices to buy or sell an asset. To overcome these problems, Exchange Traded Funds (ETFs) are employed for base assets for the evaluation, and we propose a two-stage deep learning framework, called Grouped-ETFs Model (GEM), with a joint cost function. The GEM is designed to learn the features of inter-asset and groups in each stage. Also, the proposed joint cost can consider relative returns for the training while the relative returns are a crucial factor to construct a portfolio. The results of a rigorous evaluation with global ETF data indicate that the proposed GEM with the joint cost outperforms the equally weighted portfolio and the ordinary deep learning model by 33.7% and 30.1%, respectively. An additional experiment using sector ETFs verifies the generality of the proposed model where the results accord with those of the previous experiment.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419307584",
    "keywords": [
      "Artificial intelligence",
      "Asset (computer security)",
      "Asset management",
      "Computer science",
      "Computer security",
      "Deep learning",
      "Econometrics",
      "Economics",
      "Finance",
      "Generality",
      "Machine learning",
      "Management",
      "Portfolio",
      "Portfolio optimization",
      "Project management",
      "Project portfolio management"
    ],
    "authors": [
      {
        "surname": "Yun",
        "given_name": "Hyungbin"
      },
      {
        "surname": "Lee",
        "given_name": "Minhyeok"
      },
      {
        "surname": "Kang",
        "given_name": "Yeong Seon"
      },
      {
        "surname": "Seok",
        "given_name": "Junhee"
      }
    ]
  },
  {
    "title": "Using the European Commission country recommendations to predict sovereign ratings: A topic modeling approach",
    "journal": "Expert Systems with Applications: X",
    "year": "2020",
    "doi": "10.1016/j.eswax.2020.100026",
    "abstract": "This paper examines the role of textual and unstructured data in the credit risk assessment of sovereigns. Specifically, in this paper, a novel approach to understand and predict sovereign ratings is proposed. For that purpose, information embedded in the annual country reports issued by the European Commission is used. The model employs a neural-network-based document embedding known as document to vector (Doc2Vec) to convert each country report into a numerical vector, which is then used as features into a logistic regression. The model is trained using information from 2011 to 2019 and it correctly predicts the 70.27% of country ratings in the test sample, improving slightly the results obtained using only macroeconomic variables.",
    "link": "https://www.sciencedirect.com/science/article/pii/S2590188520300056",
    "keywords": [
      "Business",
      "Commission",
      "European commission",
      "European union",
      "International trade",
      "Law",
      "Political science",
      "Politics",
      "Sovereignty"
    ],
    "authors": [
      {
        "surname": "Sanz",
        "given_name": "Ivan Pastor"
      }
    ]
  },
  {
    "title": "DSTP-RNN: A dual-stage two-phase attention-based recurrent neural network for long-term and multivariate time series prediction",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113082",
    "abstract": "Long-term prediction of multivariate time series is still an important but challenging problem. The key to solve this problem is capturing (1) the spatial correlations at the same time, (2) the spatio-temporal relationships at different times, and (3) long-term dependency of the temporal relationships between different series. Attention-based recurrent neural networks (RNN) can effectively represent and learn the dynamic spatio-temporal relationships between exogenous series and target series, but they only perform well in one-step time prediction and short-term time prediction. In this paper, inspired by human attention mechanism including the dual-stage two-phase (DSTP) model and the influence mechanism of target information and non-target information, we propose DSTP-based RNN (DSTP-RNN) and DSTP-RNN-Ⅱ respectively for long-term time series prediction. Specifically, we first propose the DSTP-based structure to enhance the spatial correlations between exogenous series. The first phase produces violent but decentralized response weight, while the second phase leads to stationary and concentrated response weight. Then, we employ multiple attentions on target series to boost the long-term dependency. Finally, we study the performance of deep spatial attention mechanism and provide interpretation. Experimental results demonstrate that the present work can be successfully used to develop expert or intelligent systems for a wide range of applications, with state-of-the-art performances superior to nine baseline methods on four datasets in the fields of energy, finance, environment and medicine, respectively. Overall, the present work carries a significant value not merely in the domain of machine intelligence and deep learning, but also in the fields of many applications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419307997",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Dependency (UML)",
      "Dual (grammatical number)",
      "Literature",
      "Machine learning",
      "Multivariate statistics",
      "Paleontology",
      "Physics",
      "Quantum mechanics",
      "Recurrent neural network",
      "Series (stratigraphy)",
      "Term (time)",
      "Time domain",
      "Time series"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Yeqi"
      },
      {
        "surname": "Gong",
        "given_name": "Chuanyang"
      },
      {
        "surname": "Yang",
        "given_name": "Ling"
      },
      {
        "surname": "Chen",
        "given_name": "Yingyi"
      }
    ]
  },
  {
    "title": "Multiple writer retrieval systems based on language independent dissimilarity learning",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113023",
    "abstract": "Retrieval based on query images supports interesting applications in handwritten document analysis, such as checking manuscripts originality, and authorship. In this respect, writer retrieval systems aim to automatically find all manuscripts belonging to the same author. Presently, we propose a new combination scheme for multiple writer retrieval systems that employ different features and dissimilarities. The proposed combination is founded on writer-independent, SVM dissimilarity learning. For experimental evaluation, three individual systems are proposed each of which, has its specific features. To develop the first system, we propose the Multiscale Histogram Of Templates (M-HOT). For the second system, we introduce the so-called Multi-Gradient Elongated Quinary Pattern (MG-EQP) as new descriptor for handwriting characterization. The third system uses the well-known Run Length Features. Retrieval tests are performed on CVL, ICDAR-2011, ICDAR-2013 and ICDAR-2017 datasets. Furthermore, to highlight the language-independence aspect, experiments are performed on KHATT dataset that contains Arabic handwritten documents. Results obtained evince the effectiveness of the proposed features as well as the combination scheme, which outperforms both individual systems and the state of the art.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419307407",
    "keywords": [
      "Arabic",
      "Artificial intelligence",
      "Computer science",
      "Handwriting",
      "Histogram",
      "Image (mathematics)",
      "Image retrieval",
      "Independence (probability theory)",
      "Linguistics",
      "Machine learning",
      "Margin (machine learning)",
      "Mathematical analysis",
      "Mathematics",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Scheme (mathematics)",
      "Statistics",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Bouibed",
        "given_name": "Mohamed Lamine"
      },
      {
        "surname": "Nemmour",
        "given_name": "Hassiba"
      },
      {
        "surname": "Chibani",
        "given_name": "Youcef"
      }
    ]
  },
  {
    "title": "Portfolio management via two-stage deep learning with a joint cost",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113041",
    "abstract": "Portfolio management is a series of processes that maximize returns and minimize risk by allocating assets efficiently. Along with the developments in machine learning technology, it has been studied to apply machine learning methods to prediction-based portfolio management. However, such methods have a few limitations. First, they do not consider the relations between assets for the prediction. In addition, the studies commonly focus on the prediction accuracy, neglecting the construction of portfolios. Furthermore, the methods have usually been evaluated with index data, which hardly represent actual prices to buy or sell an asset. To overcome these problems, Exchange Traded Funds (ETFs) are employed for base assets for the evaluation, and we propose a two-stage deep learning framework, called Grouped-ETFs Model (GEM), with a joint cost function. The GEM is designed to learn the features of inter-asset and groups in each stage. Also, the proposed joint cost can consider relative returns for the training while the relative returns are a crucial factor to construct a portfolio. The results of a rigorous evaluation with global ETF data indicate that the proposed GEM with the joint cost outperforms the equally weighted portfolio and the ordinary deep learning model by 33.7% and 30.1%, respectively. An additional experiment using sector ETFs verifies the generality of the proposed model where the results accord with those of the previous experiment.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419307584",
    "keywords": [
      "Artificial intelligence",
      "Asset (computer security)",
      "Asset management",
      "Computer science",
      "Computer security",
      "Deep learning",
      "Econometrics",
      "Economics",
      "Finance",
      "Generality",
      "Machine learning",
      "Management",
      "Portfolio",
      "Portfolio optimization",
      "Project management",
      "Project portfolio management"
    ],
    "authors": [
      {
        "surname": "Yun",
        "given_name": "Hyungbin"
      },
      {
        "surname": "Lee",
        "given_name": "Minhyeok"
      },
      {
        "surname": "Kang",
        "given_name": "Yeong Seon"
      },
      {
        "surname": "Seok",
        "given_name": "Junhee"
      }
    ]
  },
  {
    "title": "Deep 1D-Convnet for accurate Parkinson disease detection and severity prediction from gait",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113075",
    "abstract": "Diagnosing Parkinson’s disease is a complex task that requires the evaluation of several motor and non-motor symptoms. During diagnosis, gait abnormalities are among the important symptoms that physicians should consider. However, gait evaluation is challenging and relies on the expertise and subjectivity of clinicians. In this context, the use of an intelligent gait analysis algorithm may assist physicians in order to facilitate the diagnosis process. This paper proposes a novel intelligent Parkinson detection system based on deep learning techniques to analyze gait information. We used 1D convolutional neural network (1D-Convnet) to build a Deep Neural Network (DNN) classifier. The proposed model processes 18 1D-signals coming from foot sensors measuring the vertical ground reaction force (VGRF). The first part of the network consists of 18 parallel 1D-Convnet corresponding to system inputs. The second part is a fully connected network that connects the concatenated outputs of the 1D-Convnets to obtain a final classification. We tested our algorithm in Parkinson’s detection and in the prediction of the severity of the disease with the Unified Parkinson’s Disease Rating Scale (UPDRS). Our experiments demonstrate the high efficiency of the proposed method in the detection of Parkinson disease based on gait data. The proposed algorithm achieved an accuracy of 98.7%. To our knowledge, this is the state-of-the-start performance in Parkinson’s gait recognition. Furthermore, we achieved an accuracy of 85.3% in Parkinson’s severity prediction. To the best of our knowledge, this is the first algorithm to perform a severity prediction based on the UPDRS. These results show that the model is able to learn intrinsic characteristics from gait data and to generalize to unseen subjects, which could be helpful in a clinical diagnosis.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419307924",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Disease",
      "Gait",
      "Machine learning",
      "Medicine",
      "Parkinson's disease",
      "Pathology",
      "Pattern recognition (psychology)",
      "Physical medicine and rehabilitation"
    ],
    "authors": [
      {
        "surname": "El Maachi",
        "given_name": "Imanne"
      },
      {
        "surname": "Bilodeau",
        "given_name": "Guillaume-Alexandre"
      },
      {
        "surname": "Bouachir",
        "given_name": "Wassim"
      }
    ]
  },
  {
    "title": "Discovering generalized design knowledge using a multi-objective evolutionary algorithm with generalization operators",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113025",
    "abstract": "The early-phase design of complex systems is a challenging task, as a decision maker has to take into account the intricate relationships among different design variables. A popular way to help decision makers easily identify important design features is to use data mining. However, many of the existing algorithms output design features that are too complex (e.g., conjunction of many literals with unrelated predicates), making it difficult for a user to understand, remember, and apply these features to find better designs. In this paper, we introduce a new data mining method that extracts compact design features through knowledge generalization. The proposed method performs a search over the space of features using a multi-objective evolutionary algorithm that contains a set of generalization operators in addition to conventional evolutionary operators. Both variables and feature types are generalized by using an ontology defining a set of domain-specific concepts and relationships. Generalization leads to more compact and insightful features, as generalized knowledge encompasses wider concepts. A comparative experiment is conducted on a real-world system architecting problem to demonstrate the gain in compactness of the extracted features without significant reductions in predictive power.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419307420",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Domain (mathematical analysis)",
      "Domain knowledge",
      "Economics",
      "Epistemology",
      "Evolutionary algorithm",
      "Feature (linguistics)",
      "Generalization",
      "Knowledge extraction",
      "Linguistics",
      "Machine learning",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Ontology",
      "Philosophy",
      "Programming language",
      "Rough set",
      "Set (abstract data type)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Bang",
        "given_name": "Hyunseung"
      },
      {
        "surname": "Selva",
        "given_name": "Daniel"
      }
    ]
  },
  {
    "title": "Detecting malware evolution using support vector machines",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113022",
    "abstract": "Malware families typically evolve over a period of time. Differences between malware samples within a single family can originate from various code modifications designed to evade detection, or changes that are made to alter the functionality of the malware itself. Thus, malware samples from the same family from different time periods can exhibit significantly different behavior. In this research, we apply feature ranking—based on linear support vector machine (SVM) weights—to identify changes within malware families. We analyze numerous malware families over extended periods of time. Our goal is to demonstrate that we can detect evolutionary changes within malware families using an automated and quantifiable machine learning based technique.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419307390",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Data mining",
      "Feature (linguistics)",
      "Linguistics",
      "Machine learning",
      "Malware",
      "Malware analysis",
      "Philosophy",
      "Ranking (information retrieval)",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Wadkar",
        "given_name": "Mayuri"
      },
      {
        "surname": "Di Troia",
        "given_name": "Fabio"
      },
      {
        "surname": "Stamp",
        "given_name": "Mark"
      }
    ]
  },
  {
    "title": "A multi-objective optimization procedure for solving the high-order epistasis detection problem",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113000",
    "abstract": "There are multiple research works that establish a relationship between Single Nucleotide Polymorphisms (SNPs) and complex diseases. In many cases, these diseases are caused by the interaction of two or more SNPs (epistasis). Therefore, it is important to identify which SNPs lead to the emergence of diseases. However, this problem gets harder when the epistasis order and the number of SNPs in the dataset under study are increased. To tackle this problem, this work presents an implementation of a multi-objective optimization algorithm with a problem-aware offspring process. Furthermore, due to the large amount of data in the SNPs datasets, we have also parallelized this algorithm. We experimentally validate the quality of the proposal under epistasis sizes of 2, 5, and 8 loci, although the application is not limited to those values. Moreover, a thorough comparative study of biological performance with six state-of-the-art methods has been conducted, obtaining better results than all the other biological methods. Our results confirm that, using the proposed approach, large datasets with high-order epistasis sizes can be processed in reasonable time, obtaining solutions of good quality. According to the literature review performed until the acceptance of this article, there are no other authors’ works that can process epistasis sizes above 6 loci in large datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419307171",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Data mining",
      "Epistasis",
      "Epistemology",
      "Gene",
      "Genetics",
      "Genotype",
      "Machine learning",
      "Operating system",
      "Philosophy",
      "Process (computing)",
      "Quality (philosophy)",
      "Single-nucleotide polymorphism"
    ],
    "authors": [
      {
        "surname": "Granado-Criado",
        "given_name": "José M."
      },
      {
        "surname": "Santander-Jiménez",
        "given_name": "Sergio"
      },
      {
        "surname": "Vega-Rodríguez",
        "given_name": "Miguel A."
      },
      {
        "surname": "Rubio-Largo",
        "given_name": "Álvaro"
      }
    ]
  },
  {
    "title": "SLNet: Stereo face liveness detection via dynamic disparity-maps and convolutional neural network",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113002",
    "abstract": "Current state-of-the-art dual camera-based face liveness detection methods utilize either hand-crafted features, such as disparity, or deep texture features to classify a live face and face Presentation Attack (PA). However, these approaches limit the effectiveness of classifiers, particularly deep Convolutional Neural Networks (CNN) to unknown face PA in adverse scenarios. In contrast to these approaches, in this paper, we show that supervising a deep CNN classifier by learning disparity features using the existing CNN layers improves the performance and robustness of CNN to unknown types of face PA. For this purpose, we propose to supervise a CNN classifier by introducing a disparity layer within CNN to learn the dynamic disparity-maps. Subsequently, the rest of the convolutional layers, following the disparity layer, in the CNN are supervised using the learned dynamic disparity-maps for face liveness detection. We further propose a new video-based stereo face anti-spoofing database with various face PA and different imaging qualities. Experiments on the proposed stereo face anti-spoofing database are performed using various test case scenarios. The experimental results indicate that our proposed system shows promising performance and has good generalization ability.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419307195",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Classifier (UML)",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Deep learning",
      "Face (sociological concept)",
      "Facial recognition system",
      "Gene",
      "Liveness",
      "Pattern recognition (psychology)",
      "Programming language",
      "Robustness (evolution)",
      "Social science",
      "Sociology",
      "Spoofing attack"
    ],
    "authors": [
      {
        "surname": "Rehman",
        "given_name": "Yasar Abbas Ur"
      },
      {
        "surname": "Po",
        "given_name": "Lai-Man"
      },
      {
        "surname": "Liu",
        "given_name": "Mengyang"
      }
    ]
  },
  {
    "title": "Feedback control loop design for workload change detection in self-tuning NoSQL wide column stores",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112973",
    "abstract": "Database management systems are the main part of information systems that the size and complexity of these systems are increased in recent years. Due to the growing complexity of DBMSs, database administrators (DBAs) face increasingly more problems and challenges, and so managing these systems are difficult and laborious. More over the main part of the total cost of ownership includes the cost of expert database administrator who can manage these large and complicated systems. Autonomic database, by providing self-management functionality, leads to a reduction in the total cost of ownership for the database system. Self-management decisions such as automated schema database tuning are dependent on the database workload. Therefore, one of the important issues in realizing the database automated tuning is workload monitoring and analysis for changes detection and schema re-tuning with this changes. In this paper, a feedback control loop is designed for continuous monitoring and light-weight workload analysis in NoSQL wide column stores. This loop describes a design pattern for the self-tuning feature and it is used to detect workload changes that are necessary for the automated schema database re-tuning. Our concept is based on workload model construction using reconfigurable colored petri-net model. The results of the experiments show the effectiveness of the proposed approach in discovering significant workload changes.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419306918",
    "keywords": [
      "Computer science",
      "Data mining",
      "Database",
      "Database administrator",
      "Database design",
      "Database schema",
      "Database tuning",
      "Machine learning",
      "NoSQL",
      "Operating system",
      "Scalability",
      "Schema (genetic algorithms)",
      "View",
      "Workload"
    ],
    "authors": [
      {
        "surname": "Mozaffari",
        "given_name": "Maryam"
      },
      {
        "surname": "Nazemi",
        "given_name": "Eslam"
      },
      {
        "surname": "Eftekhari-Moghadam",
        "given_name": "Amir Masoud"
      }
    ]
  },
  {
    "title": "MGFS: A multi-label graph-based feature selection algorithm via PageRank centrality",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113024",
    "abstract": "In multi-label data, each instance corresponds to a set of labels instead of one label whereby the instances belonging to a label in the corresponding column of that label are assigned 1, while instances that do not belong to that label are assigned 0 in the data set. This type of data is usually considered as high-dimensional data, so many methods, using machine learning algorithms, seek to choose the best subset of features for reducing the dimensionality of data and then to create an acceptable model for classification. In this paper, we have designed a fast algorithm for feature selection on the multi-label data using the PageRank algorithm, which is an effective method used to calculate the importance of web pages on the Internet. This algorithm, which is called multi-label graph-based feature selection (MGFS), first constructs an M × L matrix, called Correlation Distance Matrix (CDM), where M is the number of features and L represents the number of class labels. Then, MGFS creates a complete weighted graph, called Feature-Label Graph (FLG), where each feature is considered as a vertex, and the weight between two vertices (or features) represents their Euclidean distance in CDM. Finally, the importance of each graph vertex (or feature) is estimated via the PageRank algorithm. In the proposed method, the number of features can be determined by the user. To prove the performance of the proposed algorithm, we have tested this algorithm with several methods for multi-label feature selection and on several multi-label datasets with different dimensions. The results show the superiority of the proposed method in the classification criteria and run-time.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419307419",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Feature (linguistics)",
      "Feature selection",
      "Graph",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Hashemi",
        "given_name": "Amin"
      },
      {
        "surname": "Dowlatshahi",
        "given_name": "Mohammad Bagher"
      },
      {
        "surname": "Nezamabadi-pour",
        "given_name": "Hossein"
      }
    ]
  },
  {
    "title": "Efficient multi-population outpost fruit fly-driven optimizers: Framework and advances in support vector machines",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112999",
    "abstract": "The original fruit fly algorithm (FOA) in simple structure is easy to understand, but it has a slow convergence rate and tends to be trapped in the local optimal solutions. In order to improve the convergence rate and efficacy of FOA, two new mechanisms are integrated with the exploratory and exploitative strategies of the original FOA: the outpost mechanism and the multi-population mechanism. The outpost mechanism consists of two parts: greedy selection and Gaussian mutation, which is mainly used to improve the convergence rate of the algorithm. The multi-swarm mechanism divides the population of agents into several sub-swarms and selects several individuals from sub-swarm with a random probability. Then, the selected individuals are remapped into the feature space to expand the exploratory capabilities. To illustrate the performance of the proposed method, a comprehensive set of benchmark functions, including the unimodal, multimodal, and composition functions were chosen for testing tasks. Also, the proposed MOFOA is compared against the state-of-the-art improved FOA algorithms and other well-known swarm-based methods. The experimental results have shown that MOFOA can outperform all the competitors involved in this study in terms of convergence speed and solution quality in a significant manner. Furthermore, MOFOA is also employed to optimize two critical parameters of the support vector machine (SVM) for classification tasks. The results demonstrate that the proposed MOFOA can also achieve a better performance than other swarm-based methods in dealing with the optimization of the SVM in dealing with several financial datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741741930716X",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Computer security",
      "Convergence (economics)",
      "Demography",
      "Economic growth",
      "Economics",
      "Feature vector",
      "Geodesy",
      "Geography",
      "Key (lock)",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Population",
      "Rate of convergence",
      "Sociology",
      "Support vector machine",
      "Swarm behaviour"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Huiling"
      },
      {
        "surname": "Li",
        "given_name": "Shimin"
      },
      {
        "surname": "Asghar Heidari",
        "given_name": "Ali"
      },
      {
        "surname": "Wang",
        "given_name": "Pengjun"
      },
      {
        "surname": "Li",
        "given_name": "Jiawei"
      },
      {
        "surname": "Yang",
        "given_name": "Yutao"
      },
      {
        "surname": "Wang",
        "given_name": "Mingjing"
      },
      {
        "surname": "Huang",
        "given_name": "Changcheng"
      }
    ]
  },
  {
    "title": "Identification of influential users in social network using gray wolf optimization algorithm",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112971",
    "abstract": "A challenging issue in viral marketing is to effectively identify a set of influential users. By sending the advertising messages to this set, one can reach out the largest area of the network. In this paper, we formulate the influence maximization problem as an optimization problem with cost functions as the influentiality of the nodes and the distance between them. Maximizing the distance between the seed nodes guarantees reaching to different parts of the network. We use gray wolf optimization algorithm to solve the problem. Our experimental results on three real-world networks show that proposed method outperforms state-of-the-art influence maximization algorithms. Furthermore, it has lower computational time than other meta-heuristic methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741741930689X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Computer science",
      "Heuristic",
      "Identification (biology)",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Maximization",
      "Optimization algorithm",
      "Optimization problem",
      "Programming language",
      "Set (abstract data type)",
      "Social media",
      "Social network (sociolinguistics)",
      "Viral marketing",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Zareie",
        "given_name": "Ahmad"
      },
      {
        "surname": "Sheikhahmadi",
        "given_name": "Amir"
      },
      {
        "surname": "Jalili",
        "given_name": "Mahdi"
      }
    ]
  },
  {
    "title": "Website categorization: A formal approach and robustness analysis in the case of e-commerce detection",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113001",
    "abstract": "Website categorization has recently emerged as a very important task in several contexts. A huge amount of information is freely available through websites, and it could be used for example to accomplish statistical surveys, saving in costs. However, the information of interest for the specific categorization has to be mined among that huge amount. This turns out to be a difficult task in practice. In this work we propose a practically viable procedure to perform website categorization, based on the automatic generation of data records summarizing the content of each entire website. This is obtained by using web scraping and optical character recognition, followed by a number of nontrivial text mining and feature engineering steps. When such records have been produced, we use classification algorithms to categorize the websites according to the aspect of interest. We compare in this task Convolutional Neural Networks, Support Vector Machines, Random Forest and Logistic classifiers. Since in many practical cases the training set labels are physiologically noisy, we analyze the robustness of each technique with respect to the presence of misclassified training records. We present results on real-world data for the problem of the detection of websites providing e-commerce facilities, however our approach is not structurally limited to this case.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419307183",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Categorization",
      "Chemistry",
      "Computer science",
      "Convolutional neural network",
      "Data mining",
      "Deep learning",
      "Economics",
      "Feature engineering",
      "Gene",
      "Information retrieval",
      "Machine learning",
      "Management",
      "Random forest",
      "Robustness (evolution)",
      "Support vector machine",
      "Task (project management)",
      "Text categorization",
      "Training set"
    ],
    "authors": [
      {
        "surname": "Bruni",
        "given_name": "Renato"
      },
      {
        "surname": "Bianchi",
        "given_name": "Gianpiero"
      }
    ]
  },
  {
    "title": "A novel vehicle lateral positioning methodology based on the integrated deep neural network",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112991",
    "abstract": "Many intelligent transportation system (ITS) applications increasingly depend on accurate and reliable positioning performance. How to achieve such performance using low-cost sensors is one of the main challenges for land vehicles. Traditional positioning sensor such as the Global Positioning System (GPS) may fail to obtain satisfactory performance in challenging situations. The main reason is that satellite signals along the vehicle's lateral direction are obstructed by obstacles and therefore the lateral position error will be increased. This paper proposes a novel methodology to achieve lane-level lateral positioning based on the integrated deep neural network (IDNet) which integrates the deep convolutional neural network (DCNN) with the multi-layer neural network. Specifically, IDNet consists of two carefully designed networks that are connected in series. The first one aims to extract the road area efficiently. The other is integrated with the former and makes full use of the road area to predict lateral position accurately. The design concept of IDNet is inspired by the mechanism of human eyes’ lateral positioning, which utilizes the location information hidden in road areas to predict the lateral position. Benefiting from this novel mechanism, the proposed methodology applies to many urban road scenes including well-marked, poorly marked and unmarked roads. To evaluate this approach, road experiments with typical scenarios were performed in urban streets. The results validate its effectiveness and reliability. From the methodological perspective, the proposed method simulates the process of human eyes’ lateral positioning, which indicates that artificial intelligence provides an effective means to simulate human perception and behavior for domain experts. From the application point of view, the proposed methodology expands the application fields of artificial intelligence, such as the field of positioning involved in this paper.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419307080",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Deep learning",
      "Economics",
      "Finance",
      "Global Positioning System",
      "Operating system",
      "Physics",
      "Position (finance)",
      "Power (physics)",
      "Process (computing)",
      "Quantum mechanics",
      "Reliability (semiconductor)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Zheng",
        "given_name": "Zhiyong"
      },
      {
        "surname": "Li",
        "given_name": "Xu"
      }
    ]
  },
  {
    "title": "iProStruct2D: Identifying protein structural classes by deep learning via 2D representations",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113019",
    "abstract": "In this paper, we address the problem of protein classification, starting from multi-view 2D snapshots of proteins. Using JMol, a well-known protein visualization software, a set of multi-view 2D representations including 13 different types of protein visualizations are rendered. The 13 visualization types are used to emphasize specific properties of protein structure (e.g. a backbone visualization that displays the backbone structure of the protein as a trace of the Cα atom); while different points of view in the 3D space are used to visualize the protein shapes. Given this set of 2D snapshots for each protein, deep learning is used to perform protein classification starting from the 2D images. Each type of representation is used to train a different Convolutional Neural Network (CNN), and the fusion of these CNNs is shown to be able to exploit the diversity of different types of representations to improve classification performance. The multi-view projections, obtained by uniformly rotating the protein structure around its central X, Y, and Z viewing axes, are used as a kind of data augmentation during the training and testing phases. The resulting approach, named iProStruct2D, is different from most of existing methods in the literature, which are based on protein alignment or on measuring the distance between 3D representation of the protein. Experimental evaluation of the proposed approach on two datasets demonstrates the strength of iProStruct2D with respect to other state-of-the-art approaches. The MATLAB code used in this paper is available at https://github.com/LorisNanni.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419307365",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Convolutional neural network",
      "Data mining",
      "Deep learning",
      "Law",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Programming language",
      "Representation (politics)",
      "Set (abstract data type)",
      "Visualization"
    ],
    "authors": [
      {
        "surname": "Nanni",
        "given_name": "Loris"
      },
      {
        "surname": "Lumini",
        "given_name": "Alessandra"
      },
      {
        "surname": "Pasquali",
        "given_name": "Federica"
      },
      {
        "surname": "Brahnam",
        "given_name": "Sheryl"
      }
    ]
  },
  {
    "title": "The development of a video retrieval system using a clinician-led approach",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112992",
    "abstract": "Patient video taken at home can provide valuable insights into the recovery progress during a programme of physical therapy, but is very time consuming for clinician review. Our work focussed on (i) enabling any patient to share information about progress at home, simply by sharing video and (ii) building intelligent systems to support Physical Therapists (PTs) in reviewing this video data and extracting the necessary detail. This paper reports the development of the system, appropriate for future clinical use without reliance on a technical team, and the clinician involvement in that development. We contribute an interactive content-based video retrieval system that significantly reduces the time taken for clinicians to review videos, using human head movement as an example. The system supports query-by-movement (clinicians move their own body to define search queries) and retrieves the essential fine-grained movements needed for clinical interpretation. This is done by comparing sequences of image-based pose estimates (here head rotations) through a distance metric (here Fréchet distance) and presenting a ranked list of similar movements to clinicians for review. In contrast to existing intelligent systems for retrospective review of human movement, the system supports a flexible analysis where clinicians can look for any movement that interests them. Evaluation by a group of PTs with expertise in training movement control showed that 96% of all relevant movements were identified with time savings of as much as 99.1% compared to reviewing target videos in full. The novelty of this contribution includes retrospective progress monitoring that preserves context through video, and content-based video retrieval that supports both fine-grained human actions and query-by-movement. Future research, including large clinician-led studies, will refine the technical aspects and explore the benefits in terms of patient outcomes, PT time, and financial savings over the course of a programme of therapy. It is anticipated that this clinician-led approach will mitigate the reported slow clinical uptake of technology with resulting patient benefit.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419307092",
    "keywords": [
      "Aesthetics",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Economics",
      "Information retrieval",
      "Metric (unit)",
      "Movement (music)",
      "Multimedia",
      "Novelty",
      "Operations management",
      "Paleontology",
      "Philosophy",
      "Psychology",
      "Social psychology"
    ],
    "authors": [
      {
        "surname": "Darby",
        "given_name": "John"
      },
      {
        "surname": "Sánchez",
        "given_name": "María B."
      },
      {
        "surname": "Bew",
        "given_name": "Sarah"
      },
      {
        "surname": "Loram",
        "given_name": "Ian"
      },
      {
        "surname": "Butler",
        "given_name": "Penelope"
      }
    ]
  },
  {
    "title": "A hybrid project portfolio selection procedure with historical performance consideration",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113003",
    "abstract": "A novel hybrid portfolio selection procedure consisting of three phases is proposed for choosing an appropriate project portfolio based on a set of criteria having fuzzy measurements. Different from other existing selection procedures, in which decisions are based mainly on the projects’ characteristics at decision points, the proposed procedure takes into account projects’ historical performances. In particular, projects are first evaluated and assigned comparable scores, based on which a multi-objective model is built. The model selects a portfolio with a view to maximizing its expected value and development trend, while minimizing its risk with other selected projects. Three important elements are considered: (1) criteria weighting over each other, (2) uncertainty in decision making, and (3) projects’ historical performances. The aim is to select more robust project portfolios in the long run, which lead to fewer replacements. Comparative studies are performed to verify the validity of the approach. It is shown that our method can lead to more robust portfolios in a shorter time, which can aid decision makers throughout the selection process.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419307201",
    "keywords": [
      "Artificial intelligence",
      "Business",
      "Computer science",
      "Economics",
      "Engineering",
      "Financial economics",
      "Fuzzy logic",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Medicine",
      "Operating system",
      "Operations research",
      "Portfolio",
      "Process (computing)",
      "Programming language",
      "Project management",
      "Project portfolio management",
      "Radiology",
      "Risk analysis (engineering)",
      "Selection (genetic algorithm)",
      "Set (abstract data type)",
      "Systems engineering",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Xiaoxiong"
      },
      {
        "surname": "Fang",
        "given_name": "Liping"
      },
      {
        "surname": "Hipel",
        "given_name": "Keith W."
      },
      {
        "surname": "Ding",
        "given_name": "Song"
      },
      {
        "surname": "Tan",
        "given_name": "Yuejin"
      }
    ]
  },
  {
    "title": "Imbalance learning using heterogeneous ensembles",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113005",
    "abstract": "In binary classification, class-imbalance problem occurs when the number of samples in one class is much larger than that of the other class. In such cases, the performance of a classifier is generally poor on the minority class. Classifier ensembles are used to tackle this problem where each member is trained using a different balanced dataset that is computed by randomly undersampling the majority class and/or randomly oversampling the minority. Although the primary target of imbalance learning is the minority class, downsampling-based schemes employ the same minority sample set for all members whereas oversampling the minority is challenging due to its unclear structure. On the other hand, heterogeneous ensembles utilizing multiple learning algorithms have a higher potential in generating diverse members than homogeneous ones. In this study, the use of heterogeneous ensembles for imbalance learning is addressed. Experiments are conducted on 66 datasets to explore the relation between the heterogeneity of the ensemble and performance scores using AUC and F 1 measures. The results obtained have shown that the performance scores improve as the number of classification methods is increased from one to five. Moreover, when compared with homogeneous ensembles, significantly higher scores are achieved using heterogeneous ones. Also, it is observed that multiple balancing schemes contribute to the performance scores of some homogeneous and heterogeneous ensembles. However, the improvements are not significant for either approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419307225",
    "keywords": [
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Binary classification",
      "Class (philosophy)",
      "Classifier (UML)",
      "Combinatorics",
      "Computer network",
      "Computer science",
      "Ensemble learning",
      "Homogeneous",
      "Image (mathematics)",
      "Machine learning",
      "Mathematics",
      "Oversampling",
      "Support vector machine",
      "Undersampling",
      "Upsampling"
    ],
    "authors": [
      {
        "surname": "Ghaderi Zefrehi",
        "given_name": "Hossein"
      },
      {
        "surname": "Altınçay",
        "given_name": "Hakan"
      }
    ]
  },
  {
    "title": "IACS-HCSP: Improved ant colony optimization for large-scale home care scheduling problems",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112994",
    "abstract": "The home care and scheduling problem (HCSP) consists on the design of a set of routes to be used by caregivers that provide daily assistance at specific times to patients located in a definite geographic area. In this study we propose a modified version of Ant Colony Optimization (ACO), called IACS-HCSP, to approach this task. In order to be used in this problem, ACO requires modifications in the problem representation and additional mechanisms to deal with constraints. We propose a dynamic neighborhood graph and an improved method that constructs the solution that improves its exploration capability over deterministic or greedy heuristic methods. This technique has been applied to a very large real world instance of HCSP for which results are available for comparison. IACS-HCSP is able to improve the previous results on this specific instance in terms of cost. At the same time it can be used to help decision making when there is a choice between competing objectives, because it finds a full range of feasible solutions with different equilibria between time and size of the required labor force.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419307110",
    "keywords": [
      "Algorithm",
      "Ant colony",
      "Ant colony optimization algorithms",
      "Artificial intelligence",
      "Computer network",
      "Computer science",
      "Economics",
      "Graph",
      "Greedy algorithm",
      "Heuristic",
      "Job shop scheduling",
      "Management",
      "Mathematical optimization",
      "Mathematics",
      "Routing (electronic design automation)",
      "Scheduling (production processes)",
      "Task (project management)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Martin",
        "given_name": "Emilio"
      },
      {
        "surname": "Cervantes",
        "given_name": "Alejandro"
      },
      {
        "surname": "Saez",
        "given_name": "Yago"
      },
      {
        "surname": "Isasi",
        "given_name": "Pedro"
      }
    ]
  },
  {
    "title": "Toward graph-based semi-supervised face beauty prediction",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112990",
    "abstract": "Assessing beauty using facial images analysis is an emerging computer vision problem. To the best of our knowledge, all existing methods for automatic facial beauty scoring rely on fully supervised schemes. In this paper, we introduce the use of semi-supervised learning schemes for solving the problem of face beauty scoring when the image descriptor is holistic and the score is given by a real number. The paper has two main contributions. Firstly, we introduce the use of graph-based semi-supervised learning for face beauty scoring. The proposed method is based on texture and utilizes continuous scores in a full range. Secondly, we adapt and kernelize an existing linear Flexible Manifold Embedding scheme (that works with discrete classes) to the case of real scores propagation. The resulting model can be used for transductive and inductive settings. The proposed semi-supervised schemes were evaluated on three recent public datasets for face beauty analysis: SCUT-FBP, M2B, and SCUT-FBP5500. The obtained experimental results, as well as many comparisons with fully supervised methods, demonstrate that the nonlinear semi-supervised scheme compares favorably with many supervised schemes. The proposed semi-supervised scoring framework paves the way to virtually all applications to adopt continuous scores instead of the usual discrete labels.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419307079",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Beauty",
      "Computer science",
      "Embedding",
      "Epistemology",
      "Face (sociological concept)",
      "Graph",
      "Graph embedding",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Scheme (mathematics)",
      "Social science",
      "Sociology",
      "Supervised learning",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Dornaika",
        "given_name": "Fadi"
      },
      {
        "surname": "Wang",
        "given_name": "Kunwei"
      },
      {
        "surname": "Arganda-Carreras",
        "given_name": "Ignacio"
      },
      {
        "surname": "Elorza",
        "given_name": "Anne"
      },
      {
        "surname": "Moujahid",
        "given_name": "Abdelmalik"
      }
    ]
  },
  {
    "title": "Path-based reasoning approach for knowledge graph completion using CNN-BiLSTM with attention mechanism",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112960",
    "abstract": "Knowledge graphs are valuable resources for building intelligent systems such as question answering or recommendation systems. However, most knowledge graphs are impaired by missing relationships between entities. Embedding methods that translate entities and relations into a low-dimensional space achieve great results, but they only focus on the direct relations between entities and neglect the presence of path relations in graphs. On the contrary, path-based embedding methods consider a single path to make inferences. It also relies on simple recurrent neural networks while highly efficient neural network models are available for processing sequence data. We propose a new approach for knowledge graph completion that combines bidirectional long short-term memory (BiLSTM) and convolutional neural network modules with an attention mechanism. Given a candidate relation and two entities, we encode paths that connect the entities into a low-dimensional space using a convolutional operation followed by BiLSTM. Then, an attention layer is applied to capture the semantic correlation between a candidate relation and each path between two entities and attentively extract reasoning evidence from the representation of multiple paths to predict whether the entities should be connected by the candidate relation. We extend our model to perform multistep reasoning over path representations in an embedding space. A recurrent neural network is designed to repeatedly interact with an attention module to derive logical inference from the representation of multiple paths. We perform link prediction tasks on several knowledge graphs and show that our method achieves better performance compared with recent state-of-the-art path-reasoning methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419306785",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Data mining",
      "Embedding",
      "Graph",
      "Graph embedding",
      "Inference",
      "Knowledge representation and reasoning",
      "Law",
      "Path (computing)",
      "Political science",
      "Politics",
      "Programming language",
      "Relation (database)",
      "Representation (politics)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Jagvaral",
        "given_name": "Batselem"
      },
      {
        "surname": "Lee",
        "given_name": "Wan-Kon"
      },
      {
        "surname": "Roh",
        "given_name": "Jae-Seung"
      },
      {
        "surname": "Kim",
        "given_name": "Min-Sung"
      },
      {
        "surname": "Park",
        "given_name": "Young-Tack"
      }
    ]
  },
  {
    "title": "Efficient methods for mining weighted clickstream patterns",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112993",
    "abstract": "Pattern mining has been an attractive topic for many researchers since its first introduction. Clickstream mining, a specific version of sequential pattern mining, has been shown to be important in the age of the Internet. However, most previous works have simply exploited and applied existing sequential pattern algorithms to the mining of clickstream patterns, and few have studied clickstreams with weights, which also have a wide range of application. In this paper, we address this problem by proposing an approach based on the average weight measure for clickstream pattern mining and adapting a previous state-of-the-art algorithm to deal with the problem of weighted clickstream pattern mining. Following this, we propose an improved method named Compact-SPADE to enhance both the efficiency and memory consumption. Through various tests on both real-life and synthetic databases, we show that our proposed algorithms outperform state-of-the-art alternatives in terms of efficiency, memory requirements and scalability.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419307109",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Association rule learning",
      "Clickstream",
      "Computer science",
      "Data mining",
      "Database",
      "Machine learning",
      "Scalability",
      "Sequential Pattern Mining",
      "State (computer science)",
      "The Internet",
      "Web API",
      "Web modeling",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Huynh",
        "given_name": "Huy M."
      },
      {
        "surname": "Nguyen",
        "given_name": "Loan T.T."
      },
      {
        "surname": "Vo",
        "given_name": "Bay"
      },
      {
        "surname": "Nguyen",
        "given_name": "Anh"
      },
      {
        "surname": "Tseng",
        "given_name": "Vincent S."
      }
    ]
  },
  {
    "title": "Predicting auction price of vehicle license plate with deep recurrent neural network",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113008",
    "abstract": "In Chinese societies, superstition is of paramount importance, and vehicle license plates with desirable numbers can fetch very high prices in auctions. Unlike other valuable items, license plates are not allocated an estimated price before auction. I propose that the task of predicting plate prices can be viewed as a natural language processing (NLP) task, as the value depends on the meaning of each individual character on the plate and its semantics. I construct a deep recurrent neural network (RNN) to predict the prices of vehicle license plates in Hong Kong, based on the characters on a plate. I demonstrate the importance of having a deep network and of retraining. Evaluated on 13 years of historical auction prices, the deep RNN’s predictions can explain over 80% of price variations, outperforming previous models by a significant margin. I also demonstrate how the model can be extended to become a search engine for plates and to provide estimates of the expected price distribution.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419307250",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Common value auction",
      "Computer science",
      "Construct (python library)",
      "Deep neural networks",
      "Economics",
      "License",
      "Machine learning",
      "Management",
      "Margin (machine learning)",
      "Microeconomics",
      "Operating system",
      "Programming language",
      "Semantics (computer science)",
      "Task (project management)",
      "Value (mathematics)"
    ],
    "authors": [
      {
        "surname": "Chow",
        "given_name": "Vinci"
      }
    ]
  },
  {
    "title": "A refined selection method for project portfolio optimization considering project interactions",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112952",
    "abstract": "This paper proposes two innovative approaches to address the two challenges of project portfolio selection: (i) Determining how project interactions influence the final values of project portfolios, and (ii) selecting the best solution from non-dominated project portfolios. First, based on the dependence relationship between projects and technologies, we construct a project co-utilization network indicating project interactions using the co-citation network method. We also consider project interaction as a regularization item of the value function to indicate the influence of project interactions on the value of project portfolios. Second, we propose a refined selection strategy. All the projects are ranked in the order of their appearance in the ranked association rules. Project portfolios having no highly ranked projects are deleted successively until only one of them remains. Finally, a case of multi-objective project portfolio selection is studied and an optimal project portfolio recommended. A comparison shows the availability and advantages of the proposed approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419306700",
    "keywords": [
      "Biology",
      "Business",
      "Computer science",
      "Engineering",
      "Evolutionary biology",
      "Finance",
      "Function (biology)",
      "Machine learning",
      "Mathematics",
      "Operations research",
      "Portfolio",
      "Project management",
      "Project portfolio management",
      "Selection (genetic algorithm)",
      "Systems engineering"
    ],
    "authors": [
      {
        "surname": "Wei",
        "given_name": "Hechuan"
      },
      {
        "surname": "Niu",
        "given_name": "Caiyun"
      },
      {
        "surname": "Xia",
        "given_name": "Boyuan"
      },
      {
        "surname": "Dou",
        "given_name": "Yajie"
      },
      {
        "surname": "Hu",
        "given_name": "Xuejun"
      }
    ]
  },
  {
    "title": "TechNet: Technology semantic network based on patent data",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112995",
    "abstract": "The growing developments in general semantic networks, knowledge graphs and ontology databases have motivated us to build a large-scale comprehensive semantic network of technology-related data for engineering knowledge discovery, technology search and retrieval, and artificial intelligence for engineering design and innovation. Specially, we constructed a technology semantic network (TechNet) that covers the elemental concepts in all domains of technology and their semantic associations by mining the complete U.S. patent database from 1976. To derive the TechNet, natural language processing techniques were utilized to extract terms from massive patent texts and recent word embedding algorithms were employed to vectorize such terms and establish their semantic relationships. We report and evaluate the TechNet for retrieving terms and their pairwise relevance that is meaningful from a technology and engineering design perspective. The TechNet may serve as an infrastructure to support a wide range of applications, e.g., technical text summaries, search query predictions, relational knowledge discovery, and design ideation support, in the context of engineering and technology, and complement or enrich existing semantic databases. To enable such applications, the TechNet is made public via an online interface and APIs for public users to retrieve technology-related terms and their relevancies.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419307122",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Data science",
      "Embedding",
      "Epistemology",
      "Information retrieval",
      "Law",
      "Ontology",
      "Ontology engineering",
      "Paleontology",
      "Philosophy",
      "Political science",
      "Relevance (law)",
      "Semantic Web",
      "Semantic compression",
      "Semantic computing",
      "Semantic network",
      "Semantic search",
      "Semantic technology",
      "Upper ontology",
      "Word embedding"
    ],
    "authors": [
      {
        "surname": "Sarica",
        "given_name": "Serhad"
      },
      {
        "surname": "Luo",
        "given_name": "Jianxi"
      },
      {
        "surname": "Wood",
        "given_name": "Kristin L."
      }
    ]
  },
  {
    "title": "A multi-strategy fusion artificial bee colony algorithm with small population",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112921",
    "abstract": "Although artificial bee colony (ABC) algorithm is more and more popular in solving complex problems, slow convergence rate limits its wide application. ABC with small population can use the limited function evaluation times more efficiently since it can avoid unnecessary searches. However, ABC with small population cannot ensure population diversity, and when the algorithm is weak or unstable, it may fall into local optimum easily. So based on the latest research, we are motivated to propose a stabler and more efficient algorithm design to improve the search ability of ABC with small population by the fusion of multiple search strategies, which used together for the employed bees and the onlooker bees. Firstly we select and design multiple strategies with different search abilities of exploration and exploitation. Secondly, we propose an evolution ratio, which is an indicator to fully reflect the adaptability of the search strategy. Thirdly, we design different fusion methods according to the characteristics of the strategies, in which the search strategy with high exploration is maintained at a certain frequency throughout the whole search process of the employed bees, and the selections of the other two search strategies are adjusted according to evolution ratio adaptively in the employed bee phase and the onlooker bee phase. In the end, a novel algorithm called MFABC is proposed, which can realize efficiently multi-strategy cooperative search according to the requirements of different problems and different search stages. Experimental results on a set of benchmark functions have shown the accuracy, stability, efficiency and convergence rate of MFABC.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419306396",
    "keywords": [
      "Adaptability",
      "Algorithm",
      "Artificial bee colony algorithm",
      "Artificial intelligence",
      "Bees algorithm",
      "Benchmark (surveying)",
      "Biology",
      "Computer science",
      "Convergence (economics)",
      "Demography",
      "Ecology",
      "Economic growth",
      "Economics",
      "Geodesy",
      "Geography",
      "Local search (optimization)",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Metaheuristic",
      "Operating system",
      "Particle swarm optimization",
      "Population",
      "Process (computing)",
      "Programming language",
      "Set (abstract data type)",
      "Sociology",
      "Stability (learning theory)",
      "Swarm intelligence"
    ],
    "authors": [
      {
        "surname": "Song",
        "given_name": "Xiaoyu"
      },
      {
        "surname": "Zhao",
        "given_name": "Ming"
      },
      {
        "surname": "Xing",
        "given_name": "Shuangyun"
      }
    ]
  },
  {
    "title": "Opinion leader detection using whale optimization algorithm in online social network",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113016",
    "abstract": "In the current digital era, optimization is one of the most significant problems in the social network. Most of the issues related to optimization are NP-complete and not possible to solve them in polynomial time. Detection of opinion leader based on their optimized centrality measure is a critical issue. The opinion leaders have a non-trivial influence on the other user's decision-making process and can solve various problems related to the diffusion of new products and innovations in the real world. In this paper, we proposed a new Social Network-based Whale Optimization Algorithm (SNWOA) to find the top-N opinion leaders by measuring the reputation of the user using various standard optimization function in the network. The proposed algorithm is advantageous to determine the opinion leaders because it based on the bubble-net hunting behavior of humpback whales. The algorithm found the best possible solution as the number of users raises progressively in the network; therefore, the general complexity of the algorithm remains unchanged. Besides, we also proposed a new approach to categorize the communities based on the similarity index comprising neighbor similarity and clustering coefficient as the significant components. Initially, we computed the objective function of each user by using their centralities and deployed the proposed algorithm with different optimization functions to identify the local and universal opinion leaders. We implemented the proposed algorithm on the real and synthesized datasets and compared the result based on the accuracy, precision, recall, and F1-score. The result indicates that the proposed algorithms give a better result as compared to the other standard Social Network Analysis (SNA) measures. We also concluded that the community partitioning algorithm is even better than the other community detection algorithms based on different parameters and computational time.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741741930733X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Centrality",
      "Cluster analysis",
      "Combinatorics",
      "Computer science",
      "Data mining",
      "Evolutionary biology",
      "Function (biology)",
      "Image (mathematics)",
      "Mathematics",
      "Opinion leadership",
      "Optimization problem",
      "Particle swarm optimization",
      "Political science",
      "Public relations",
      "Reputation",
      "Similarity (geometry)",
      "Similarity measure",
      "Social media",
      "Social network (sociolinguistics)",
      "Social science",
      "Sociology",
      "Time complexity",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Jain",
        "given_name": "Lokesh"
      },
      {
        "surname": "Katarya",
        "given_name": "Rahul"
      },
      {
        "surname": "Sachdeva",
        "given_name": "Shelly"
      }
    ]
  },
  {
    "title": "Image Classification Benchmark (ICB)",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112998",
    "abstract": "During any investigative work, it is necessary to confront our solutions to the already existing ones. This requires much work, namely to try to recreate the competing solutions and to use the same experimental conditions in order to obtain an objective comparison. Benchmarking is an activity of comparing performance metrics of systems in order to rank them or the activity of comparing a specific system to state-of-the-art alternatives. It would be convenient to have an image benchmarking ecosystem, not only to evaluate a personal solution but also to compare it with other previously proposed solutions. Upon designing a new or improved image classification pipeline, a convolution neural network or a specific algorithm modifying some detail, a user can add it to the benchmarking ecosystem and get a report on the performance of the solution, which can be compared to other solutions that were previously benchmarked under the same conditions. In this paper we describe the ICB - “Image Classification Benchmark” -, a prototype of a benchmarking ecosystem created to enable this vision. Besides describing how it works and how it was made flexible to incorporate any algorithm, we apply it to a specific comparison, as a proof-of-concept.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419307158",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Geodesy",
      "Geology",
      "Machine learning",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Caldeira",
        "given_name": "Manuel"
      },
      {
        "surname": "Martins",
        "given_name": "Pedro"
      },
      {
        "surname": "Costa",
        "given_name": "Rogério Luís C."
      },
      {
        "surname": "Furtado",
        "given_name": "Pedro"
      }
    ]
  },
  {
    "title": "A novel pruning algorithm for mining long and maximum length frequent itemsets",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.113004",
    "abstract": "Frequent itemset mining is today one of the most popular data mining techniques. Its application is, however, hindered by the high computational cost in many real-world datasets, especially for smaller values of support thresholds. In many cases, moreover, the large number of frequent itemsets discovered is overwhelming. In some real-world applications, it is sufficient to find a smaller subset of frequent itemsets, such as identifying the frequent itemsets with a maximum length. In this paper, we present a pruning algorithm, called LengthSort, that reduces the search space effectively and improves the efficiency of mining frequent itemsets with a maximum length. LengthSort prunes both the items and the transactions before constructing a Frequent Pattern tree structure. Our experiments on several datasets show that the proposed pruning techniques reduce the time needed to discover the frequent itemsets with a maximum length. The proposed pruning algorithm can also be applied to efficiently discover frequent itemsets that are longer than a user-specified threshold.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419307213",
    "keywords": [
      "Agronomy",
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Data mining",
      "Mathematical analysis",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Pruning",
      "Space (punctuation)",
      "Tree (set theory)"
    ],
    "authors": [
      {
        "surname": "Lessanibahri",
        "given_name": "Sina"
      },
      {
        "surname": "Gastaldi",
        "given_name": "Luca"
      },
      {
        "surname": "González Fernández",
        "given_name": "Camino"
      }
    ]
  },
  {
    "title": "Wildfire detection using transfer learning on augmented datasets",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112975",
    "abstract": "Wildfire detection is a time-critical application as the difficulty to pinpoint ignition locations in a short time-frame often leads to the escalation of the severity of fire events. This problem has motivated considerable interest from expert systems research to develop accurate early-warning applications and the breakthroughs in deep learning in complex visual understanding tasks open novel research opportunities. However, despite the improvements in performance demonstrated in the current literature, a comprehensive study of the challenges and limitations of this approach is still a gap in the state-of-the-art. To address this issue, the contributions of this work are threefold. First, we overview recent works to identify common difficulties and shortcomings of these approaches, and assess issues related to the quality of the databases. Second, to overcome data limitations, this work proposes a transfer learning approach coupled with data augmentation techniques tested under a tenfold cross-validation scheme. The proposed framework enables leveraging an open-source dataset featuring images from more than 35 real fire events, which unlike video-based works offers higher variability between samples, allowing evaluating the approach in an extensive set of real scenarios. Third, this article presents an in-depth study of the limitations, providing a comprehensive analysis of the patterns causing misclassifications. The key insights gained in this analysis provide relevant takeaways to guide future research towards the implementation of expert systems in decision support systems in firefighting and civil protection operations.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419306931",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Data science",
      "Frame (networking)",
      "Key (lock)",
      "Machine learning",
      "Medicine",
      "Open research",
      "Programming language",
      "Risk analysis (engineering)",
      "Set (abstract data type)",
      "Telecommunications",
      "Transfer of learning",
      "Warning system",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Sousa",
        "given_name": "Maria João"
      },
      {
        "surname": "Moutinho",
        "given_name": "Alexandra"
      },
      {
        "surname": "Almeida",
        "given_name": "Miguel"
      }
    ]
  },
  {
    "title": "A multirobot target searching method based on bat algorithm in unknown environments",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112945",
    "abstract": "Multirobot target searching in unknown environments is a currently trending topic of discussion. In this paper, an improved bat algorithm (BA) for multirobot target searching in unknown environments, named adaptive robotic bat algorithm (ARBA), is proposed; it acts as the controlling mechanism for robots. The obstacle avoidance problem is considered in the proposed ARBA. The adaptive inertial weight strategy helps ARBA improve its diversity and provides an effective mechanism for escaping from local optima. In addition, the Doppler effect is introduced to improve ARBA; the effect can be adaptively compensated when the robot moves and helps robots avoid premature convergence. Moreover, the location of the target in an unknown environment is unknown, and a multi-swarm strategy is introduced into the ARBA to improve the diversity and expand the search space of robots so that robots can find the location of the target as well as the target itself faster than the existing algorithms. Experiments were conducted in three aspects to verify the effectiveness and efficiency of ARBA. We compared ARBA with the other algorithms in this field; the experimental results demonstrate that ARBA exhibits better performance in multirobot target searching and can be applied to multirobot intelligent systems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419306633",
    "keywords": [
      "Algorithm",
      "Archaeology",
      "Artificial intelligence",
      "Computer science",
      "Convergence (economics)",
      "Economic growth",
      "Economics",
      "Geography",
      "Obstacle",
      "Robot",
      "Swarm behaviour"
    ],
    "authors": [
      {
        "surname": "Tang",
        "given_name": "Hongwei"
      },
      {
        "surname": "Sun",
        "given_name": "Wei"
      },
      {
        "surname": "Yu",
        "given_name": "Hongshan"
      },
      {
        "surname": "Lin",
        "given_name": "Anping"
      },
      {
        "surname": "Xue",
        "given_name": "Min"
      }
    ]
  },
  {
    "title": "DGStream: High quality and efficiency stream clustering algorithm",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112947",
    "abstract": "Recently as applications produce overwhelming data streams, the need for strategies to analyze and cluster streaming data becomes an urgent and a crucial research area for knowledge discovery. The main objective and the key aim of data stream clustering is to gain insights into incoming data. Recognizing all probable patterns in this boundless data which arrives at varying speeds and structure and evolves over time, is very important in this analysis process. The existing data stream clustering strategies so far, all suffer from different limitations, like the inability to find the arbitrary shaped clusters and handling outliers in addition to requiring some parameter information for data processing. For fast, accurate, efficient and effective handling for all these challenges, we proposed DGStream, a new online-offline grid and density-based stream clustering algorithm. We conducted many experiments and evaluated the performance of DGStream over different simulated databases and for different parameter settings where a wide variety of concept drifts, novelty, evolving data, number and size of clusters and outlier detection are considered. Our algorithm is suitable for applications where the interest lies in the most recent information like stock market, or if the analysis of existing information is required as well as cases where both the old and the recent information are all equally important. The experiments, over the synthetic and real datasets, show that our proposed algorithm outperforms the other algorithms in efficiency.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419306657",
    "keywords": [
      "Algorithm",
      "Anomaly detection",
      "Artificial intelligence",
      "CURE data clustering algorithm",
      "Cluster analysis",
      "Computer science",
      "Computer security",
      "Correlation clustering",
      "Data mining",
      "Data stream",
      "Data stream clustering",
      "Data stream mining",
      "Key (lock)",
      "Machine learning",
      "Operating system",
      "Outlier",
      "Process (computing)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Ahmed",
        "given_name": "Rowanda"
      },
      {
        "surname": "Dalkılıç",
        "given_name": "Gökhan"
      },
      {
        "surname": "Erten",
        "given_name": "Yusuf"
      }
    ]
  },
  {
    "title": "An ensemble based on a bi-objective evolutionary spectral algorithm for graph clustering",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112911",
    "abstract": "Graph clustering is a challenging pattern recognition problem whose goal is to identify vertex partitions with high intra-group connectivity. This paper investigates a bi-objective problem that maximizes the number of intra-cluster edges of a graph and minimizes the expected number of inter-cluster edges in a random graph with the same degree sequence as the original one. The difference between the two investigated objectives is the definition of the well-known measure of graph clustering quality: the modularity. We introduce a spectral decomposition hybridized with an evolutionary heuristic, called MOSpecG, to approach this bi-objective problem and an ensemble strategy to consolidate the solutions found by MOSpecG into a final robust partition. The results of computational experiments with real and artificial LFR networks demonstrated a significant improvement in the results and performance of the introduced method in regard to another bi-objective algorithm found in the literature. The crossover operator based on the geometric interpretation of the modularity maximization problem to match the communities of a pair of individuals was of utmost importance for the good performance of MOSpecG. Hybridizing spectral graph theory and intelligent systems allowed us to define significantly high-quality community structures.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419306293",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Crossover",
      "Graph",
      "Graph partition",
      "Mathematical optimization",
      "Mathematics",
      "Maximization",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Tautenhain",
        "given_name": "Camila P.S."
      },
      {
        "surname": "Nascimento",
        "given_name": "Mariá C.V."
      }
    ]
  },
  {
    "title": "Learning to compose over tree structures via POS tags for sentence representation",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112917",
    "abstract": "Recursive Neural Network (RecNN), a type of model which composes words or phrases recursively over syntactic tree structures, has been proven to have superior ability to obtain sentence representation for a variety of NLP tasks. However, RecNN is born with a thorny problem that a shared compositional function for each node of trees can’t capture the complex semantic compositionality so that the expressive power of model is limited. In this paper, in order to address this problem, we propose Tag-Guided HyperRecNN/TreeLSTM (TG-HRecNN/TreeLSTM), which introduces hypernetwork into RecNNs to take as inputs Part-of-Speech (POS) tags of word/phrase and generate the semantic composition parameters dynamically. Experimental results on five datasets for two typical NLP tasks show proposed models both obtain significant improvement compared with RecNN and TreeLSTM consistently. Our TG-HTreeLSTM outperforms all existing RecNN-based models and achieves or is competitive with state-of-the-art on four sentence classification benchmarks. The effectiveness of our models is also demonstrated by qualitative analysis.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419306359",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Engineering",
      "Law",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Natural language processing",
      "Node (physics)",
      "Philosophy",
      "Phrase",
      "Political science",
      "Politics",
      "Principle of compositionality",
      "Representation (politics)",
      "Sentence",
      "Structural engineering",
      "Tree (set theory)",
      "Variety (cybernetics)",
      "Word (group theory)"
    ],
    "authors": [
      {
        "surname": "Shen",
        "given_name": "Gehui"
      },
      {
        "surname": "Deng",
        "given_name": "Zhi-Hong"
      },
      {
        "surname": "Huang",
        "given_name": "Ting"
      },
      {
        "surname": "Chen",
        "given_name": "Xi"
      }
    ]
  },
  {
    "title": "Identifying sufficient deception in military logistics",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112974",
    "abstract": "In the context of modern warfare, military forces are faced with challenges that threaten their power projection operations. Adversarial strategies could hinder the actions of the forces within operational areas by inferring and anticipating a military’s potential mission objectives including military logistics and transportation activities. Thus, this research investigates the potential ways of incorporating two military deception strategies into the military logistics decision making process: (i) falsifying signifies including empty convoys on routes on which commodities are being hauled, and (ii) hiding refers to concealing routes in the logistics network. Assuming that the adversaries can monitor the shipments in the transportation network, we develop a two-stage integer linear programming model that quantifies the sufficient amounts of each deception needed to deceive the adversaries. In addition, the mathematical model accurately captures several other military logistics challenges, such as satisfying the demands for military commodities at destinations subject to limited capacities at supply points, deciding the best route option, and respecting the time limit constraint. However, the two-stage model is non-convex and therefore, reformulation and linearization steps are applied to convert it to a convex single-stage model. Due to the computational complexity in solving the single-stage model for realistic-size instances, two greedy heuristics are developed that can efficiently solve those instances. Moreover, the computational experiments reveal that the greedy heuristics obtain near optimal solutions when compared to the solutions of the smaller instances solved to optimality via a commercial solver.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741741930692X",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Constraint (computer-aided design)",
      "Context (archaeology)",
      "Deception",
      "Geometry",
      "Heuristics",
      "Law",
      "Mathematical optimization",
      "Mathematics",
      "Operating system",
      "Operations research",
      "Paleontology",
      "Political science",
      "Process (computing)",
      "Programming language",
      "Solver"
    ],
    "authors": [
      {
        "surname": "Ausseil",
        "given_name": "Rosemonde"
      },
      {
        "surname": "Gedik",
        "given_name": "Ridvan"
      },
      {
        "surname": "Bednar",
        "given_name": "Amy"
      },
      {
        "surname": "Cowan",
        "given_name": "Mark"
      }
    ]
  },
  {
    "title": "A guided population archive whale optimization algorithm for solving multiobjective optimization problems",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112972",
    "abstract": "This paper proposes a new multiobjective algorithm by extending the recently Whale Optimization Algorithm (WOA) to solve multiobjective optimization problems. Our algorithm which we shall call Guided Population Archive Whale Optimization Algorithm (GPAWOA) is based on Pareto dominance and uses an external archive to store the non-dominated solutions found during the optimization process. The leaders are selected from the archive to guide the population towards promising regions of the search space, also, the mechanism of crowding distance is incorporated into the standard WOA to maintain the diversity. Our proposed algorithm is evaluated on twelve benchmark functions and is applied to four multi-objective engineering design problems: 4-bar truss design, gear train problem, disk brake design and welded beam design and compared against three well-known algorithms: Multiobjective Evolutionary Algorithm Based on Decomposition (MOEA/D), Multi-Objective Grey Wolf Optimizer (MOGWO), and Multi-Objective Particle Swarm Optimization (MOPSO). The experimental results indicate that the proposed GPAWOA is highly competitive and outperforms the selected state-of-the-art multiobjective optimization algorithms, being able to provide an excellent approximation of Pareto front in terms of convergence and diversity.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419306906",
    "keywords": [
      "Algorithm",
      "Benchmark (surveying)",
      "Computer science",
      "Convergence (economics)",
      "Demography",
      "Economic growth",
      "Economics",
      "Evolutionary algorithm",
      "Geodesy",
      "Geography",
      "Mathematical optimization",
      "Mathematics",
      "Multi-objective optimization",
      "Optimization problem",
      "Pareto principle",
      "Particle swarm optimization",
      "Population",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Got",
        "given_name": "Adel"
      },
      {
        "surname": "Moussaoui",
        "given_name": "Abdelouahab"
      },
      {
        "surname": "Zouache",
        "given_name": "Djaafar"
      }
    ]
  },
  {
    "title": "A comparison of separation routines for benders optimality cuts for two-level facility location problems",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112928",
    "abstract": "This paper studies two-level uncapacitated facility location problems, a class of discrete location problems that consider different hierarchies of facilities and their interactions. Benders reformulations for both single and multiple assignment variants and while several separation procedures for three classes of Benders cuts are presented: standard optimality cuts, lifted optimality cuts, and non-dominated optimality cuts. Extensive computational experiments are performed on difficult and large-scale benchmark instances to assess the performance of the considered separation routines.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419306463",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Benders' decomposition",
      "Class (philosophy)",
      "Computer science",
      "Facility location problem",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Separation (statistics)"
    ],
    "authors": [
      {
        "surname": "de Oliveira",
        "given_name": "Paganini Barcellos"
      },
      {
        "surname": "Contreras",
        "given_name": "Ivan"
      },
      {
        "surname": "de Camargo",
        "given_name": "Ricardo Saraiva"
      },
      {
        "surname": "de Miranda Júnior",
        "given_name": "Gilberto"
      }
    ]
  },
  {
    "title": "Maximising goals achievement through abstract argumentation frameworks: An optimal approach",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112930",
    "abstract": "Argumentation is a prominent AI research area, focused on approaches and techniques for performing common-sense reasoning, that is of paramount importance in a wide range of real-world applications, such as decision support and recommender systems. In this work we introduce an approach for updating an abstract Argumentation Framework (AF) so that achievement with respect to a given set of goals is maximised. The set of goals identifies arguments for which a specific acceptability status (a labelling) will be pursued, distinguishing between “ in ” and “ out ” goals. Given an AF, a set of goals and a set of available actions allowing to add or remove arguments and attacks from the AF, our approach will select the strategy (set of actions) that should be applied in order to obtain a new AF where the goals achievement is maximised. Moreover, the selected strategy will be optimal with respect to the number of actions to be applied. In the context of argumentation-based expert and intelligent systems, our approach will provide tools allowing the user to interact with the argumentative reasoning process carried out by the system, learning how the strategy she undertakes will affect the recommendations she receives. For that, we propose an encoding of the AF, the available actions and goals as weighted Boolean formulas, and rely on MaxSAT techniques for selecting the optimal strategy. We provide an experimental analysis of our approach, and formally show that the results we obtain correspond to the optimal strategy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419306487",
    "keywords": [
      "Argumentation framework",
      "Argumentation theory",
      "Argumentative",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Economics",
      "Epistemology",
      "Law",
      "Machine learning",
      "Management science",
      "Operating system",
      "Paleontology",
      "Philosophy",
      "Political science",
      "Process (computing)",
      "Programming language",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Cohen",
        "given_name": "Andrea"
      },
      {
        "surname": "Gottifredi",
        "given_name": "Sebastian"
      },
      {
        "surname": "Vallati",
        "given_name": "Mauro"
      },
      {
        "surname": "García",
        "given_name": "Alejandro J."
      },
      {
        "surname": "Antoniou",
        "given_name": "Grigoris"
      }
    ]
  },
  {
    "title": "Bagging of credal decision trees for imprecise classification",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112944",
    "abstract": "The Credal Decision Trees (CDT) have been adapted for Imprecise Classification (ICDT). However, no ensembles of imprecise classifiers have been proposed so far. The reason might be that it is not a trivial question to combine the predictions made by multiple imprecise classifier. In fact, if the combination method used is not appropriate, the ensemble method could even worse the performance of one single classifier. On the other hand, the Bagging scheme has shown to provide satisfactory results in precise classification, specially when it is used with CDTs, which are known to be very weak and unstable classifiers. For these reasons, in this research, it is proposed a new Bagging scheme with ICDTs. It is presented a new technique for combining predictions made by imprecise classifiers that tries to maximize the precision of the bagging classifier. If the procedure for such a combination is too conservative it is easy to obtain few information and worse the results of a single classifier. Our proposal considers only the states with the minimum level of non-dominance. An exhaustive experimentation carried out in this work has shown that the Bagging of ICDTs, with our proposed combination technique, performs clearly better than a single ICDT.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419306621",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Decision tree",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Random subspace method"
    ],
    "authors": [
      {
        "surname": "Moral-García",
        "given_name": "S."
      },
      {
        "surname": "Mantas",
        "given_name": "Carlos J."
      },
      {
        "surname": "Castellano",
        "given_name": "Javier G."
      },
      {
        "surname": "Benítez",
        "given_name": "María D."
      },
      {
        "surname": "Abellán",
        "given_name": "Joaquín"
      }
    ]
  },
  {
    "title": "A new history-guided multi-objective evolutionary algorithm based on decomposition for batching scheduling",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112920",
    "abstract": "In this paper, a multi-objective scheduling problem on parallel batching machines is investigated with three objectives, the minimization of the makespan, the total weighted earliness/tardiness penalty and the total energy consumption, simultaneously. It is known that the batch scheduling problem is a type of NP-hard problems and the solutions to this problem have quite valuable structural features that are difficult to be formulated. One of the main issues is to make full use of the structural features of the existing solutions. Aiming at this issue, two effective strategies, local competition and internal replacement, are designed. Firstly, the local competition searches for the competitive neighboring solutions to accelerate convergence, through adjusting job positions based on two structural indicators. Secondly, the internal replacement uniformly retains half of the population as elites by elitist preservation based on decomposition. Thereafter, the other half of the population is replaced by the new solutions generated under the guidance of historical information. Moreover, the historical information is updated with the structural features extracted from the elites. As a result, a history-guided evolutionary algorithm based on decomposition with the above two strategies is proposed. To verify the performance of the proposed algorithm, extensive experiments are conducted on 18 groups of instances, in comparison with four state-of-the-art multi-objective optimization algorithms. Experimental results demonstrate that the proposed algorithm shows considerable competitiveness in addressing the studied multi-objective scheduling problems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419306384",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Decomposition",
      "Ecology",
      "Evolutionary algorithm",
      "Mathematical optimization",
      "Mathematics",
      "Scheduling (production processes)"
    ],
    "authors": [
      {
        "surname": "Jia",
        "given_name": "Zhao-hong"
      },
      {
        "surname": "Gao",
        "given_name": "Le-yang"
      },
      {
        "surname": "Zhang",
        "given_name": "Xing-yi"
      }
    ]
  },
  {
    "title": "Knowledge map construction for question and answer archives",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112923",
    "abstract": "The rapid increase in the number of community-based question-and-answer services has built up large archives of questions and answers. These archives deliver a plethora of valuable knowledge to users who primarily browse to locate information. Question–answer pairs, which not only include knowledge content but also indicate knowledge needs, are a new form of information presentation. To facilitate browsing question and answer archives and alleviate information overload during the browsing process, this paper constructs a knowledge map for question and answer archives by exploiting question–answer pair characteristics to determine a more precise location of question–answer pairs. First, the questions and answers are modeled and then the knowledge map structure is completed. Questions and answers are the two main dimensions of this knowledge map, and their intersection comprises a cluster of corresponding question–answer pairs. The knowledge map can be widely and deeply extended to provide a comprehensive representation of the question and answer archive. When labeling the knowledge map to aid in interpretation and understanding, the key words are identified, and typical question–answer pair extraction methods are proposed. Finally, we conduct extensive experiments on a real dataset, and the results show that the proposed approach is feasible and performs well.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419306414",
    "keywords": [
      "Aerospace engineering",
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Construct (python library)",
      "Data mining",
      "Data science",
      "Engineering",
      "Information retrieval",
      "Interpretation (philosophy)",
      "Intersection (aeronautics)",
      "Key (lock)",
      "Knowledge extraction",
      "Knowledge representation and reasoning",
      "Law",
      "Medicine",
      "Operating system",
      "Political science",
      "Politics",
      "Presentation (obstetrics)",
      "Process (computing)",
      "Programming language",
      "Question answering",
      "Questions and answers",
      "Radiology",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Ming"
      },
      {
        "surname": "Lu",
        "given_name": "Xiuzhi"
      },
      {
        "surname": "Chen",
        "given_name": "Lisheng"
      },
      {
        "surname": "Wang",
        "given_name": "Jun"
      }
    ]
  },
  {
    "title": "Robust regression with deep CNNs for facial age estimation: An empirical study",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112942",
    "abstract": "Recent works have shown that deep Convolutional Neural Networks (CNNs) can be very effective for image-based age estimation. However, the proposed approaches significantly vary, and there are still some open problems. Almost all deep regression networks for age estimation have exploited the Mean Square Error loss only. These deep networks have not considered the influence of aberrant and outlier observations on the final model. In this letter, we introduce the use of robust loss functions in order to learn deep regression networks for age estimation. More precisely, we explore the use of two robust regression functions: (i) the ℓ1 norm error, and (ii) the adaptive loss function that retains the advantages of the ℓ1 and ℓ2 norms. Experimental results obtained on four public databases demonstrate that learning a deep CNN with robust losses can improve the age estimation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419306608",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Economics",
      "Estimation",
      "Machine learning",
      "Management",
      "Mathematics",
      "Mean squared error",
      "Outlier",
      "Pattern recognition (psychology)",
      "Regression",
      "Regression analysis",
      "Robust regression",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Dornaika",
        "given_name": "F."
      },
      {
        "surname": "Bekhouche",
        "given_name": "SE."
      },
      {
        "surname": "Arganda-Carreras",
        "given_name": "I."
      }
    ]
  },
  {
    "title": "A two-phase approach for unexpected pattern mining",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112946",
    "abstract": "A typical mining task is to retrieve all frequent patterns from a multi-dimensional dataset. Those patterns give us a basic idea of how the data look like and the hidden inherent regularities. However, this is only useful for an unfamiliar dataset, while for datasets that are analyzed periodically, “unexpected” patterns are more interesting (e.g., some customers decided to subscribe to long-term deposits despite the burden of housing loan). In this paper, we propose a new mining job, unexpected mining, which targets at retrieving frequent patterns that are not valid in a reference dataset, but are significant enough in a specific subgroup. Given a reference dataset, we step by step generate all unexpected patterns for all subgroups. We extend existing mining approaches to support the new mining job efficiently. In particular, our scheme consists of an offline process and an online process. Offline process generates candidate patterns and builds an index table. Online process can retrieve unexpected patterns from user-defined subgroups and a given support. Experiments on real datasets show that our approach can find interesting patterns and is very efficient compared to existing approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419306645",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Data mining",
      "Organic chemistry",
      "Phase (matter)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Jingtian"
      },
      {
        "surname": "Shou",
        "given_name": "Lidan"
      },
      {
        "surname": "Wu",
        "given_name": "Sai"
      },
      {
        "surname": "Chen",
        "given_name": "Gang"
      },
      {
        "surname": "Chen",
        "given_name": "Ke"
      }
    ]
  },
  {
    "title": "Energy-efficient flexible flow shop scheduling with worker flexibility",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112902",
    "abstract": "The classical flexible flow shop scheduling problem (FFSP) only considers machine flexibility. Thus far, the relevant literature has not studied FFSPs with worker flexibility, which is widely seen in practical manufacturing systems. Worker flexibility may greatly affect production efficiency and productivity. Furthermore, with the increase of environmental pollution and energy consumption, manufacturers require innovative methods to improve energy efficiency. In this paper, we propose an energy-efficient FFSP with worker flexibility (EFFSPW), in which the flexibility of machines and workers as well as the processing time, energy consumption and worker cost related factors are considered simultaneously. A hybrid evolutionary algorithm (HEA) is then presented to solve the proposed EFFSPW, where some effective operators and a new variable neighborhood search approach are designed. Comprehensive experiments including 54 benchmark instances of the EFFSPW are carried out, and Taguchi analysis is used to determine the best combination of key parameters for the HEA. Experimental results show that the proposed HEA can obtain better solutions for most of these benchmark instances compared to two other well-known algorithms, demonstrating its superior performance in terms of both solution quality and computational efficiency.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419306207",
    "keywords": [
      "Computer science",
      "Embedded system",
      "Engineering",
      "Flexibility (engineering)",
      "Flow (mathematics)",
      "Flow shop scheduling",
      "Industrial engineering",
      "Job shop scheduling",
      "Mathematical optimization",
      "Mathematics",
      "Mechanics",
      "Physics",
      "Routing (electronic design automation)",
      "Scheduling (production processes)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Gong",
        "given_name": "Guiliang"
      },
      {
        "surname": "Chiong",
        "given_name": "Raymond"
      },
      {
        "surname": "Deng",
        "given_name": "Qianwang"
      },
      {
        "surname": "Han",
        "given_name": "Wenwu"
      },
      {
        "surname": "Zhang",
        "given_name": "Like"
      },
      {
        "surname": "Lin",
        "given_name": "Wenhui"
      },
      {
        "surname": "Li",
        "given_name": "Kexin"
      }
    ]
  },
  {
    "title": "OR-Skip-Net: Outer residual skip network for skin segmentation in non-ideal situations",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112922",
    "abstract": "Skin segmentation is one of the most important tasks for human activity recognition, video monitoring, face detection, hand gesture recognition, content-based detection, adult content filtering, human tracking, and robotic surgeries. Skin segmentation in ideal situations is easy to accomplish because it is with similar backgrounds. However, the skin segmentation in non-ideal situations is complicated due to difficult background illuminations, the presence of skin-like pixels, and environmental changes. The current studies are handling the mentioned challenges by adding the preprocessing stages in their methods, which increases the overall cost of the system. In addition, prevailing segmentation studies have ignored black skin and mainly focused on white skin for their experiments. To deal with skin segmentation in challenging environments irrespective of skin color, and to eliminate the cost of the preprocessing, this paper proposes an outer residual skip connection-based deep convolutional neural network (OR-Skip-Net) which innovatively empowers the features by transferring the direct edge information from the initial layer to the end of the network. Experiments were performed on the following eight open datasets for skin segmentation in different environments: hand gesture recognition dataset, event detection dataset, laboratoire d'informatique en image et systèmes d'information dataset, in-house dataset, UT-interaction dataset, augmented multi-party interaction dataset, Pratheepan dataset, and black skin people dataset. In addition, two other experiments were performed for gland segmentation from colon cancer histology images for the diagnosis of colorectal cancer using the Warwick-QU dataset and for iris segmentation using the Noisy Iris Challenge Evaluation - Part II dataset to explore the possibility of applying our method to different applications. Experimental results showed that the proposed OR-Skip-Net outperformed existing methods in terms of skin, gland, and iris segmentation accuracies.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419306402",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Deep learning",
      "Image segmentation",
      "Pattern recognition (psychology)",
      "Pixel",
      "Preprocessor",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Arsalan",
        "given_name": "Muhammad"
      },
      {
        "surname": "Kim",
        "given_name": "Dong Seop"
      },
      {
        "surname": "Owais",
        "given_name": "Muhammad"
      },
      {
        "surname": "Park",
        "given_name": "Kang Ryoung"
      }
    ]
  },
  {
    "title": "Multi-class multi-level classification algorithm for skin lesions classification using machine learning techniques",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112961",
    "abstract": "Skin diseases remain a major cause of disability worldwide and contribute approximately 1.79% of the global burden of disease measured in disability-adjusted life years. In the United Kingdom alone, 60% of the population suffer from skin diseases during their lifetime. In this paper, we propose an intelligent digital diagnosis scheme to improve the classification accuracy of multiple diseases. A Multi-Class Multi-Level (MCML) classification algorithm inspired by the “divide and conquer” rule is explored to address the research challenges. The MCML classification algorithm is implemented using traditional machine learning and advanced deep learning approaches. Improved techniques are proposed for noise removal in the traditional machine learning approach. The proposed algorithm is evaluated on 3672 classified images, collected from different sources and the diagnostic accuracy of 96.47% is achieved. To verify the performance of the proposed algorithm, its metrics are compared with the Multi-Class Single-Level classification algorithm which is the main algorithm used in most of the existing literature. The results also indicate that the MCML classification algorithm is capable of enhancing the classification performance of multiple skin lesions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419306797",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Class (philosophy)",
      "Classification scheme",
      "Computer science",
      "Divide and conquer algorithms",
      "Machine learning",
      "Multiclass classification",
      "Statistical classification",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Hameed",
        "given_name": "Nazia"
      },
      {
        "surname": "Shabut",
        "given_name": "Antesar M."
      },
      {
        "surname": "Ghosh",
        "given_name": "Miltu K."
      },
      {
        "surname": "Hossain",
        "given_name": "M.A."
      }
    ]
  },
  {
    "title": "Fast optimised ridesharing: Objectives, reformulations and driver flexibility",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112914",
    "abstract": "Ridesharing schemes, which match passengers to private car journeys with spare seats, are becoming popular for improving access, and reducing costs, emissions and congestion. Current deployed ridesharing systems offer matches to participants based on trip details and on preferences. They do not optimise for system-wide objectives that generally incur a high computational cost. Academic studies tend to focus on specific single objectives, while stakeholders and policy-makers for practical deployments may have many different, possibly vague, objectives. In this paper, we try to reduce the gap between research and deployed applications, and tackle the problem of computing global plans for a flexible ridesharing scheme, for different objectives. In a flexible scheme, some drivers are willing to leave their cars and become passengers of other drivers. We consider objectives to improve access (maximising served passengers), reduce emissions and congestion (minimising total distance), and to sustain the long-term viability of the system (maximising served users). In the studied scheme, riders reach a meeting point and are picked-up along the intended routes of the drivers. We show that when drivers can change roles and become passengers, a state-of-the-art solver is not able to solve quickly ridesharing schemes optimizing for global sustainable objectives. To handle this issue, we introduce new spatio-temporal constraints, based on the notion of time-consistent sets of riders, from which we derive new logical rules and routing properties. We encode the new properties and the derived logical rules within new ridesharing problem formulations. We show that solving the introduced formulations encoding the routing properties and the logical rules are up to one order of magnitude faster than the basic model, and that even for the larger instances with 1000s of users, achieves results within 2% and 5% of the optimal value within a time limit of 300 seconds. Our study also shows the interaction between the different objectives and to which extent satisfying one objective also satisfies other objectives. This study demonstrates that the introduced method that integrates spatio-temporal constraints encoding the drivers’ shared paths using logical rules within ridesharing problem formulations, help to solve faster, larger ridesharing schemes satisfying various global sustainable objectives. The methods developed in this study are of particular interest to developers and researchers interesting in reformulation techniques using logical rules to speed-up the solving of real world applications. The outcomes of this study will also benefit a variety of stakeholders (users, companies, public institutes) that aim at offering or maintaining ridesharing schemes as a sustainable solutions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419306323",
    "keywords": [
      "Computer science",
      "Computer security",
      "Engineering",
      "Flexibility (engineering)",
      "Mathematical analysis",
      "Mathematics",
      "Operations research",
      "Programming language",
      "Scheme (mathematics)",
      "Solver",
      "Statistics",
      "Traffic congestion",
      "Transport engineering"
    ],
    "authors": [
      {
        "surname": "Armant",
        "given_name": "Vincent"
      },
      {
        "surname": "Brown",
        "given_name": "Kenneth N."
      }
    ]
  },
  {
    "title": "Multivariable data imputation for the analysis of incomplete credit data",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112926",
    "abstract": "Missing data significantly reduce the accuracy and usability of credit scoring models, especially in multivariate missing cases. Most credit scoring models address this problem by deleting the missing instances from the dataset or imputing missing values with the mean, mode, or regression values. However, these methods often result in a significant loss of information or a bias. We proposed a novel method called BNII to impute missing values, which can be helpful for intelligent credit scoring systems. The proposed BNII algorithm consisted of two stages: the preparatory stage and the imputation stage. In the first stage, a Bayesian network with all of the attributes in the original dataset was constructed from the complete dataset so that both the network structure that implied the dependencies between variables and the parameters at each variable's conditional distributions could be learned. In the second stage, multivariables with missing values were iteratively imputed using Bayesian network models from the first stage. The algorithm was found to be monotonically convergent. The most significant advantages of the method include, it exploits the inherent probability-dependent relationship between variables, but without a specific probability distribution hypothesis, and it is suitable for multivariate missing cases. Three datasets were used for experiments: one was the real dataset from a famous P2P financial company in China, and the other two were benchmark datasets provided by UCI. The experimental results showed that BNII performed significantly better than the other well-known imputation techniques. This suggested that the proposed method can be used to improve the performance of a credit scoring system and to be extended to other expert and intelligent systems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741741930644X",
    "keywords": [
      "Artificial intelligence",
      "Categorical variable",
      "Computer science",
      "Data mining",
      "Imputation (statistics)",
      "Machine learning",
      "Mathematics",
      "Missing data",
      "Multivariate statistics",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Lan",
        "given_name": "Qiujun"
      },
      {
        "surname": "Xu",
        "given_name": "Xuqing"
      },
      {
        "surname": "Ma",
        "given_name": "Haojie"
      },
      {
        "surname": "Li",
        "given_name": "Gang"
      }
    ]
  },
  {
    "title": "Hexagon Based Compressed Diamond Algorithm for motion estimation and its dedicated VLSI system for HD videos",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112919",
    "abstract": "Motion estimation (ME) is crucial to the performance of a video codec as it recognizes and mitigates temporal redundancies between consecutive frames of a video sequence. It is one of the most computationally intensive blocks in the video encoder. Block matching algorithm (BMA) is considered as one of the best approaches for performing ME due to its ease of implementation and efficiency. Several ME algorithms are based on fixed search patterns while the others are adaptive in nature and exploit the correlation present between neighboring macroblocks of a video frame. This paper introduces an efficient ME algorithm named Hexagon Based Compressed Diamond Algorithm (HCDA) that tries to reduce the complexity of ME by lowering the number of search points in the process. The algorithm uses an early termination technique and an adaptive search pattern that can uniformly deal with slow and fast motion content in a video. The proposed HCDA uses dual threshold technique based on the available spatial information. This further helps in terms of clock cycles or hardware resources during VLSI implementation. Moreover, VLSI design for HCDA can explore the adaptive nature of the algorithm and the adopted search methodology is handled by interleaved memory organization that enhances the operational speed. Supportive data re-use scheme with effective block for SAD computation contributes to a fair trade-off between speed and on-chip area. The designs can work at a maximum frequency of 262 MHz to process 76 HD (1280 × 720) frames per second. The corresponding gate count comes out to be 35.8 K gate equivalent. The encouraging experimental results obtained under various defined metrics indicate suitability of the proposed hardware architecture for its inclusions in portable battery powered consumer video devices.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419306372",
    "keywords": [
      "Algorithm",
      "Block (permutation group theory)",
      "Block-matching algorithm",
      "Codec",
      "Computer engineering",
      "Computer hardware",
      "Computer science",
      "Embedded system",
      "Encoder",
      "Geometry",
      "Mathematics",
      "Motion estimation",
      "Operating system",
      "Parallel computing",
      "Process (computing)",
      "Real-time computing",
      "Search algorithm",
      "Speedup",
      "Very-large-scale integration",
      "Video processing",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Mukherjee",
        "given_name": "Rohan"
      },
      {
        "surname": "Vinod",
        "given_name": "Indubu Gaana"
      },
      {
        "surname": "Chakrabarti",
        "given_name": "Indrajit"
      },
      {
        "surname": "Kumar Dutta",
        "given_name": "Pranab"
      },
      {
        "surname": "Kumar Ray",
        "given_name": "Ajoy"
      }
    ]
  },
  {
    "title": "Possibility degree and divergence degree based method for interval-valued intuitionistic fuzzy multi-attribute group decision making",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112929",
    "abstract": "This paper investigates an order relation of interval-valued intuitionistic fuzzy values (IVIFVs) based on possibility degree and divergence degree and applies it to multi-attribute group decision making (MAGDM) problems. Firstly, based on the possibility degree of intuitionistic fuzzy values, the possibility degree and divergence degree of IVIFVs are defined to provide a new order relation of IVIFVs. Subsequently in interval-valued intuitionistic fuzzy MAGDM, an intuitionistic fuzzy linear programming model is constructed to derive decision makers' weights. Depending on the construction of membership and non-membership functions, three approaches are developed to solve the constructed intuitionistic fuzzy linear programming model, including the optimistic, pessimistic and mixed approaches. Utilizing decision makers’ weights, the collective decision matrix is obtained. Combining the possibility and divergence degrees of alternatives on attributes, the adjusted possibility distribution matrix for each attribute is constructed and proved to be an interval fuzzy preference relation. Then the attribute weights are determined by the negative distances and the positive distances of adjusted possibility distribution matrices. Using the interval-valued intuitionistic weighted arithmetic aggregation operator, the collective comprehensive values of alternatives are obtained. The ranking order of alternatives is generated according to the proposed order relation of IVIFVs. Therefore, an interval-valued intuitionistic fuzzy MAGDM method is proposed. Finally, the proposed method is implemented on some examples. Validation discussion and comparative analyses of this method is also done to show the effectiveness of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419306475",
    "keywords": [
      "Acoustics",
      "Artificial intelligence",
      "Chemistry",
      "Combinatorics",
      "Composite material",
      "Computer science",
      "Data mining",
      "Decision matrix",
      "Degree (music)",
      "Divergence (linguistics)",
      "Fuzzy logic",
      "Group (periodic table)",
      "Group decision-making",
      "Interval (graph theory)",
      "Law",
      "Linguistics",
      "Materials science",
      "Mathematical economics",
      "Mathematical optimization",
      "Mathematics",
      "Matrix (chemical analysis)",
      "Organic chemistry",
      "Philosophy",
      "Physics",
      "Political science",
      "Preference",
      "Preference relation",
      "Ranking (information retrieval)",
      "Relation (database)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Feng"
      },
      {
        "surname": "Wan",
        "given_name": "Shuping"
      }
    ]
  },
  {
    "title": "Hash polynomial two factor decision tree using IoT for smart health care scheduling",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112924",
    "abstract": "The steady growth of an aging population and increased frequency of chronic disease led to the development of Smart Health Care (SHC) systems. While patient prioritization is the core of any SHC system, handling the response time by medical practitioners is a prevailing challenge. With advancements in information technology, the concept of the Internet of Things (IoT) has made it possible to integrate SHC systems with the Cloud environment to not only ensure patient prioritization according to disease prevalence, but also to minimize response time. In this work, an IoT-based scheduling method, called the Hash Polynomial Two-factor Decision Tree (HP-TDT) is proposed to increase scheduling efficiency and reduce response time by classifying patients as being normal or in a critical state in minimal time. The HP-TDT scheduling method involves three stages including the registration stage, the data collection stage, and the scheduling stage. The registration phase is carried out through Open Address Hashing (OAH) model for reducing the key generation response time. Next, the data collection stage is performed using the Polynomial Data Collection (PDC) algorithm. By incorporating PDC, computation overhead is reduced because a number of operations are considered during data collection. Finally, scheduling is performed by applying two-factor, entropy and information gain according to a decision tree. With this, scheduling efficiency is improved due to the classification of patients as being normal or in a critical state. The proposed method minimizes response time, computational overhead, and improves essential scheduling efficiency.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419306426",
    "keywords": [
      "Computer network",
      "Computer science",
      "Data mining",
      "Decision tree",
      "Distributed computing",
      "Dynamic priority scheduling",
      "Fair-share scheduling",
      "Mathematical optimization",
      "Mathematics",
      "Quality of service",
      "Round-robin scheduling",
      "Scheduling (production processes)"
    ],
    "authors": [
      {
        "surname": "Manikandan",
        "given_name": "Ramachandran"
      },
      {
        "surname": "Patan",
        "given_name": "Rizwan"
      },
      {
        "surname": "Gandomi",
        "given_name": "Amir H."
      },
      {
        "surname": "Sivanesan",
        "given_name": "Perumal"
      },
      {
        "surname": "Kalyanaraman",
        "given_name": "Hariharan"
      }
    ]
  },
  {
    "title": "Cost-sensitive ensemble of stacked denoising autoencoders for class imbalance problems in business domain",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112918",
    "abstract": "Standard classification algorithms assume the class distribution of data to be roughly balanced. Class imbalance problem usually occurs in real-life applications, such as direct marketing, fraud detection and churn prediction. Class imbalance problem is referred to the issue that the number of examples belonging to a class is significantly higher than those of the others. When training a standard classifier with class imbalance data, the classifier is usually biased toward the majority class. In this work, we propose two novel cost-sensitive methods to address class imbalance problem, namely Cost-Sensitive Deep Neural Network (CSDNN) and Cost-Sensitive Deep Neural Network Ensemble (CSDE). CSDNN is a cost-sensitive version of Stacked Denoising Autoencoders. CSDE is an ensemble learning version of CSDNN. Random undersampling and layer-wise feature extraction from the hidden layers of the deep neural network are applied in CSDE to improve the generalization performance over CSDNN. In some literatures, various methods handling class imbalance problem were proposed. However, the experiments discussed in those studies were usually conducted on relatively small data sets and also on artificial data. The performance of those methods on modern real-life data sets, which are more complicated, is unclear. In our experiment, we examine the performance of our proposed methods and the other methods using six large real-life data sets in different business domains ranging from direct marketing, churn prediction, default payment to firm fraud detection. The results show that the proposed methods obtain promising results in handling class imbalance problem and also outperform all the other compared methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419306360",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Class (philosophy)",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Machine learning",
      "Noise reduction",
      "Pattern recognition (psychology)",
      "Undersampling"
    ],
    "authors": [
      {
        "surname": "Wong",
        "given_name": "Man Leung"
      },
      {
        "surname": "Seng",
        "given_name": "Kruy"
      },
      {
        "surname": "Wong",
        "given_name": "Pak Kan"
      }
    ]
  },
  {
    "title": "Fighting post-truth using natural language processing: A review and open challenges",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112943",
    "abstract": "Post-truth is a term that describes a distorting phenomenon that aims to manipulate public opinion and behavior. One of its key engines is the spread of Fake News. Nowadays most news is rapidly disseminated in written language via digital media and social networks. Therefore, to detect fake news it is becoming increasingly necessary to apply Artificial Intelligence (AI) and, more specifically Natural Language Processing (NLP). This paper presents a review of the application of AI to the complex task of automatically detecting fake news. The review begins with a definition and classification of fake news. Considering the complexity of the fake news detection task, a divide-and-conquer methodology was applied to identify a series of subtasks to tackle the problem from a computational perspective. As a result, the following subtasks were identified: deception detection; stance detection; controversy and polarization; automated fact checking; clickbait detection; and, credibility scores. From each subtask, a PRISMA compliant systematic review of the main studies was undertaken, searching Google Scholar. The various approaches and technologies are surveyed, as well as the resources and competitions that have been involved in resolving the different subtasks. The review concludes with a roadmap for addressing the future challenges that have emerged from the analysis of the state of the art, providing a rich source of potential work for the research community going forward.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741741930661X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Credibility",
      "Data science",
      "Deception",
      "Economics",
      "Fake news",
      "Internet privacy",
      "Key (lock)",
      "Law",
      "Management",
      "Open research",
      "Political science",
      "Psychology",
      "Sentiment analysis",
      "Social psychology",
      "Task (project management)",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Saquete",
        "given_name": "Estela"
      },
      {
        "surname": "Tomás",
        "given_name": "David"
      },
      {
        "surname": "Moreda",
        "given_name": "Paloma"
      },
      {
        "surname": "Martínez-Barco",
        "given_name": "Patricio"
      },
      {
        "surname": "Palomar",
        "given_name": "Manuel"
      }
    ]
  },
  {
    "title": "A new safe lane-change trajectory model and collision avoidance control method for automatic driving vehicles",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112953",
    "abstract": "Lane change maneuvers, are important contributors to road traffic accidents on highway. In this paper, we propose a new safe lane change trajectory model and collision avoidance control method for Society of Automotive Engineers (SAE)-level 2 automatic driving vehicles. First, a Gaussian distribution is used to describe the new trajectory model that uses pure steering and combined braking. According to regional and progressive states, a new safe lane change meaning is defined. Second, we design a new four-level automatic driving mode and an effective decision mechanism that considers safety and ergonomics. Moreover, a new trajectory tracking controller combined with a decision mechanism was designed using feedback linearization, verified using typical lane-change scenarios. Finally, based on a physical simulation platform, PreScan, the hardware-in-the-loop simulation result demonstrate the feasibility and effectiveness of our method. This paper provides a valuable reference on an expert and intelligent system methodology for automatic driving vehicles, which will be helpful for improving highway traffic safety and efficiency.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419306712",
    "keywords": [
      "Aerospace engineering",
      "Agronomy",
      "Artificial intelligence",
      "Astronomy",
      "Automotive engineering",
      "Automotive industry",
      "Biology",
      "Collision",
      "Collision avoidance",
      "Computer science",
      "Computer security",
      "Control (management)",
      "Control engineering",
      "Control theory (sociology)",
      "Controller (irrigation)",
      "Engineering",
      "Epistemology",
      "Linearization",
      "Mechanism (biology)",
      "Nonlinear system",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Simulation",
      "Trajectory"
    ],
    "authors": [
      {
        "surname": "Peng",
        "given_name": "Tao"
      },
      {
        "surname": "Su",
        "given_name": "Lili"
      },
      {
        "surname": "Zhang",
        "given_name": "Ronghui"
      },
      {
        "surname": "Guan",
        "given_name": "Zhiwei"
      },
      {
        "surname": "Zhao",
        "given_name": "Honglin"
      },
      {
        "surname": "Qiu",
        "given_name": "Zhijun"
      },
      {
        "surname": "Zong",
        "given_name": "Changfu"
      },
      {
        "surname": "Xu",
        "given_name": "Hongguo"
      }
    ]
  },
  {
    "title": "Robust weighted SVD-type latent factor models for rating prediction",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112885",
    "abstract": "Recommending system is a popular tool in many commercial or social platforms which finds interesting products for users based on their preference history. Predicting the ratings of items, such as movies, plays an essential role in the recommending system. In this context, we develop a new type of latent factor models by attaching weights to the entries of the incomplete ratings matrix. The weights are computed after estimating the user/item mean errors caused by the basic SVD model under the low-rank assumption on the ratings matrix. To accelerate the optimization process of our proposed models and other existing SVD-type models, a special design of the initial guess is suggested. In the experiments on real-world datasets, the proposed weighted models outperform other SVD-type methods, and the usage of the special initial guess improves the optimization significantly, obtaining lower MRSEs within fixed number of iterations, in comparison with the random initial guess. Furthermore, artificially noised datasets are taken to evaluate the methods, where the weighted models still perform better than other SVD-type models, implying their effectiveness and robustness in noised environment.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419305950",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Biology",
      "Chemistry",
      "Combinatorics",
      "Computer science",
      "Context (archaeology)",
      "Data mining",
      "Factor (programming language)",
      "Factor analysis",
      "Gene",
      "Machine learning",
      "Mathematics",
      "Paleontology",
      "Programming language",
      "Rank (graph theory)",
      "Recommender system",
      "Robustness (evolution)",
      "Singular value decomposition"
    ],
    "authors": [
      {
        "surname": "Gu",
        "given_name": "Yiqi"
      },
      {
        "surname": "Yang",
        "given_name": "Xi"
      },
      {
        "surname": "Peng",
        "given_name": "Mengjiao"
      },
      {
        "surname": "Lin",
        "given_name": "Guang"
      }
    ]
  },
  {
    "title": "An imprecise deep forest for classification",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112978",
    "abstract": "An imprecise deep forest classifier, which can be regarded as a modification of the deep forest proposed by Zhou and Feng, is presented in the paper. In the proposed classifier, the precise class probabilities at leaf nodes of decision trees in the deep forest are replaced with interval-valued probabilities produced by Walley’s imprecise Dirichlet model. The obtained sets of probabilities are averaged over all trees in a random forest as the imprecise probability masses. In order to use the stacking algorithm in the deep forest, a number of the class probability distributions are generated from sets of the random forest class probabilities such that decision trees of the next level of the deep forest cascade are trained on the basis of the extended training set which is added by new generated class distributions. Numerical examples illustrate how the incorporated imprecision may lead to outperforming results, especially, when the training set is rather small. The proposed classifier is the first modification of the deep forest, which efficiently deals with small datasets and takes into account a lack of sufficient training data. The ideas underlying the classifier may be a basis for development of a set on new machine learning models for small datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419306967",
    "keywords": [
      "Artificial intelligence",
      "Boundary value problem",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Decision tree",
      "Deep learning",
      "Dirichlet distribution",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Random forest"
    ],
    "authors": [
      {
        "surname": "Utkin",
        "given_name": "Lev V."
      }
    ]
  },
  {
    "title": "SAX-ARM: Deviant event pattern discovery from multivariate time series using symbolic aggregate approximation and association rule mining",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112950",
    "abstract": "The discovery of event patterns from multivariate time series is important to academics and practitioners. In particular, we consider the event patterns related to anomalies such as outliers and deviations, which are important factors in system monitoring for manufacturing processes. In this paper, we propose a method for discovering the rules to describe deviant event patterns from multivariate time series, called SAX-ARM (association rule mining based on symbolic aggregate approximation). Inverse normal transformation (INT) is first adopted for converting the distribution of time series to the normal distribution. Then, symbolic aggregate approximation (SAX) is applied to symbolize time series, and association rule mining (ARM) is used for discovering frequent rules among the symbols of deviant events. The experimental results show the discovery of informative rules among deviant events in a multivariate time series from a die-casting manufacturing process that has ten variables with 1,437 lengths. We also present the results of sensitivity analysis, which demonstrates that significant rules can be discovered with different settings of the SAX parameters. The results describe the usefulness of the proposed method to identify deviant event among multivariate time series with high complexity.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419306682",
    "keywords": [
      "Aggregate (composite)",
      "Artificial intelligence",
      "Association (psychology)",
      "Association rule learning",
      "Biology",
      "Composite material",
      "Computer science",
      "Data mining",
      "Econometrics",
      "Event (particle physics)",
      "Machine learning",
      "Materials science",
      "Mathematics",
      "Multivariate analysis",
      "Multivariate statistics",
      "Outlier",
      "Paleontology",
      "Physics",
      "Psychology",
      "Psychotherapist",
      "Quantum mechanics",
      "Series (stratigraphy)"
    ],
    "authors": [
      {
        "surname": "Park",
        "given_name": "Hoonseok"
      },
      {
        "surname": "Jung",
        "given_name": "Jae-Yoon"
      }
    ]
  },
  {
    "title": "A novel modified bat algorithm hybridizing by differential evolution algorithm",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112949",
    "abstract": "The bat algorithm (BA) is one of the metaheuristic algorithms that are used to solve optimization problems. The differential evolution (DE) algorithm is also applied to optimization problems and has successful exploitation ability. In this study, an advanced modified BA (MBA) algorithm was initially proposed by making some modifications to improve the exploration and exploitation abilities of the BA. A hybrid system (MBADE), involving the use of the MBA in conjunction with the DE, was then suggested in order to further improve the exploitation potential and provide superior performance in various test problem clusters. The proposed hybrid system uses a common population, and the algorithm to be applied to the individual is selected on the basis of a probability value, which is calculated in accordance with the performance of the algorithms; thus, the probability of applying a successful algorithm is increased. The performance of the proposed method was tested on functions that have frequently been studied, such as classical benchmark functions, small-scale CEC 2005 benchmark functions, large-scale CEC 2010 benchmark functions, and CEC 2011 real-world problems. The obtained results were compared with the results obtained from the standard BA and other findings in the literature and interpreted by means of statistical tests. The developed hybrid system showed superior performance to the standard BA in all test problem sets and produced more acceptable results when compared to the published data for the existing algorithms. In addition, the contribution of the MBA and DE algorithms to the hybrid system was examined.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419306670",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Bat algorithm",
      "Benchmark (surveying)",
      "Computer science",
      "Constraint logic programming",
      "Constraint satisfaction",
      "Demography",
      "Differential evolution",
      "Geodesy",
      "Geography",
      "Hybrid algorithm (constraint satisfaction)",
      "Mathematical optimization",
      "Mathematics",
      "Particle swarm optimization",
      "Physics",
      "Population",
      "Probabilistic logic",
      "Quantum mechanics",
      "Scale (ratio)",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Yildizdan",
        "given_name": "Gülnur"
      },
      {
        "surname": "Baykan",
        "given_name": "Ömer Kaan"
      }
    ]
  },
  {
    "title": "Extending association rules with graph patterns",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112897",
    "abstract": "We propose a general class of graph-pattern association rules ( GPARs ) for social network analysis. Extending association rules for itemsets, GPARs can help us discover associations among entities in social networks and identify potential customers. Despite the benefits, GPARs bring us challenges: the problem of GPARs discovery is already intractable, not to mention mining over large social networks. Nonetheless, we show that it is still feasible to discover GPARs from large social networks. We first formalize the GPARs mining problem and decompose it into two subproblems: Frequent pattern mining and rule generation. To address two subproblems, we develop a parallel algorithm along with an optimization strategy to construct DFS code graphs, whose nodes correspond to frequent patterns. We also provide efficient algorithms to generate (resp. representative) GPARs by using (resp. maximal) frequent patterns. Using real-life and synthetic graphs, we experimentally verify that our algorithms not only scale well but can also identify interesting GPARs with high quality among social entities.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741741930613X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Association (psychology)",
      "Association rule learning",
      "Class (philosophy)",
      "Computer science",
      "Construct (python library)",
      "Data mining",
      "Efficient algorithm",
      "Epistemology",
      "Graph",
      "Philosophy",
      "Programming language",
      "Social media",
      "Social network (sociolinguistics)",
      "Theoretical computer science",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Xin"
      },
      {
        "surname": "Xu",
        "given_name": "Yang"
      },
      {
        "surname": "Zhan",
        "given_name": "Huayi"
      }
    ]
  },
  {
    "title": "Combined fuzzy clustering and firefly algorithm for privacy preserving in social networks",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112968",
    "abstract": "In recent years, an explosive growth of social networks has been made publicly available for understanding the behavior of users and data mining purposes. The main challenge in sharing the social network databases is protecting public released data from individual identification. The most common privacy preserving technique is anonymizing data by removing or changing some information, while the anonymized data should retain as much information as possible of the original data. K-anonymity and its extensions (e.g., L-diversity and T-closeness) have widely been used for data anonymization. The main drawback of the existing anonymity techniques is the lack of protection against attribute/link disclosure and similarity attacks. Moreover, they suffer from high amount of information loss in the released database. In order to overcome these drawbacks, this paper proposes a combined anonymizing algorithm based on K-member Fuzzy Clustering and Firefly Algorithm (KFCFA) to protect the anonymized database against identity disclosure, attribute disclosure, link disclosure, and similarity attacks, and significantly minimize the information loss. In KFCFA, at first, a modified K-member version of fuzzy c-means is utilized to create balanced clusters with at least K members in each cluster. Then, firefly algorithm is performed for further optimizing the primary clusters and anonymizing the network graph and data. To achieve this purpose, a constrained multi-objective function is introduced to simultaneously minimize the clustering error rate and the generated information loss, while satisfying the defined anonymity constraints. The proposed methodology can be utilized for both network graph structures and micro data. Simulation results over four social network databases from Facebook, Google+, Twitter and YouTube demonstrate the efficiency of the proposed KFCFA algorithm to minimize the information loss of the published data and graph, while satisfying K-anonymity, L-diversity and T-closeness conditions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419306864",
    "keywords": [
      "Algorithm",
      "Anonymity",
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Closeness",
      "Cluster analysis",
      "Computer science",
      "Computer security",
      "Data mining",
      "Firefly algorithm",
      "Graph",
      "Identification (biology)",
      "Image (mathematics)",
      "Information sensitivity",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Particle swarm optimization",
      "Similarity (geometry)",
      "Social media",
      "Social network (sociolinguistics)",
      "Theoretical computer science",
      "World Wide Web",
      "k-anonymity"
    ],
    "authors": [
      {
        "surname": "Langari",
        "given_name": "Rohulla Kosari"
      },
      {
        "surname": "Sardar",
        "given_name": "Soheila"
      },
      {
        "surname": "Amin Mousavi",
        "given_name": "Seyed Abdollah"
      },
      {
        "surname": "Radfar",
        "given_name": "Reza"
      }
    ]
  },
  {
    "title": "Android-GAN: Defending against android pattern attacks using multi-modal generative network as anomaly detector",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112964",
    "abstract": "Android pattern lock system is a popular form of user authentication extensively used in mobile phones today. However, it is vulnerable to potential security attacks such as shoulder surfing, camera attack and smudge attack. This study proposes a new kind of authentication system based on a generative deep neural network that can defend any attacks by imposters except a registered user. This network adopts the anomaly detection paradigm where only normal data is used while training the network. For this purpose, we utilize both Generative Adversarial Networks as an anomaly detector and Long Short Term Memory that processes 1D time varying signals converted from 2D Android patterns. To handle the stability problem of GANs during the training, Replay Buffer, which has been effectively used in Deep Q-Networks, is also utilized. Evaluation of the proposed method was carried out thoroughly and the accuracy reached to 0.95 in term of the Area Under Curve. Although training this network requires extensive computing resources, it runs on a mobile phone well since the testing version is very light. Further experiments conducted using a group of mobile phone users, including posture variation study, provided comparable performance as well. Results suggest that the proposed system has a potential for real world application.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419306827",
    "keywords": [
      "Android (operating system)",
      "Anomaly detection",
      "Artificial intelligence",
      "Artificial neural network",
      "Authentication (law)",
      "Chemistry",
      "Computer science",
      "Computer security",
      "Detector",
      "Mobile phone",
      "Modal",
      "Operating system",
      "Polymer chemistry",
      "Real-time computing",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Shin",
        "given_name": "Sang-Yun"
      },
      {
        "surname": "Kang",
        "given_name": "Yong-Won"
      },
      {
        "surname": "Kim",
        "given_name": "Yong-Guk"
      }
    ]
  },
  {
    "title": "SoTaRePo: Society-Tag Relationship Protocol based architecture for UIP construction",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112955",
    "abstract": "As with the advancement of web services, there has been a rapid proliferation in web size and number of web users, where, each user holds a different viewpoint towards the same information. This, in turn, has become a big challenge for the web search platforms to interpret the preferences of the users and provide the desired information to them. The most suitable solution to the problem of search platforms is personalization of web search. A personalization system is a kind of expert and intelligent system which can automatically learn about the preferences of a user so that the system can provide the search results as per their relevance to a user. The process of acquiring knowledge about user’s preferences by a personalization system is known as User Interest Profile (UIP). In the field of search personalization, it can also not be denied that only an efficient and complete UIP can lead to an effective and high performing web search personalization methodology design. But most of the studies conducted for web search personalization have only focused on UIP modeling without any thought about the quality of UIP. Rather limited attention has been paid to sparsity issue of UIP modeling. In this paper, we propose a novel protocol based architecture model to create an efficient UIP by exploiting direct and indirect interest of a user. Direct interest aims at mining user’s preferences from his own activities on a social information platform. The explicitly defined society and real-world activity relationships of a user on a social platform are used to predict his indirect interest as UIP constructed solely on the basis of direct interest is sparse and ineffective. In order to unearth user’s activity relationships the concept of semantic relatedness, computed using Word2vec model, has been used. Moreover, different trust levels in society relationships have also been incorporated into the proposed model to facilitate the prediction of user’s indirect interest. A series of experiments have been conducted on a del.icio.us dataset to evaluate the effectiveness of the proposed model. The results show that the model has outperformed each and every baseline in relation to complete and efficient UIP construction.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419306736",
    "keywords": [
      "Alternative medicine",
      "Architecture",
      "Art",
      "Artificial intelligence",
      "Computer science",
      "Medicine",
      "Pathology",
      "Protocol (science)",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Goel",
        "given_name": "Shubham"
      },
      {
        "surname": "Kumar",
        "given_name": "Ravinder"
      }
    ]
  },
  {
    "title": "Automating the expansion of a knowledge graph",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112965",
    "abstract": "In order to make computers understand human languages and to reason, human knowledge needs to be represented and stored in a form that can be processed by computers. Knowledge graphs have been developed for use as a form of the knowledge base for words and general relationships among words. However, they have two limitations. One is that the knowledge graph is limited in size and scope for most of the human languages. Another is that they are not able to deal with neologisms that form a part of the human common sense. Addressing these problems, we have developed and validated PolarisX which can automatically expand a knowledge graph, by crawling and analyzing the news sites and social media in real-time. We utilize and fine-tune the pre-trained multilingual BERT model for the construction of knowledge graphs without language dependencies. We extract new relationships using the BERT-based relation extraction model and integrate them into the knowledge graph. We verify the novelty and accuracy of PolarisX. It deals with neologisms and does not have language dependencies.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419306839",
    "keywords": [
      "Artificial intelligence",
      "Commonsense knowledge",
      "Computer science",
      "Conceptual graph",
      "Graph",
      "Information retrieval",
      "Knowledge base",
      "Knowledge extraction",
      "Knowledge graph",
      "Knowledge management",
      "Knowledge representation and reasoning",
      "Natural language processing",
      "Novelty",
      "Open Knowledge Base Connectivity",
      "Organizational learning",
      "Personal knowledge management",
      "Philosophy",
      "Theology",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Yoo",
        "given_name": "SoYeop"
      },
      {
        "surname": "Jeong",
        "given_name": "OkRan"
      }
    ]
  },
  {
    "title": "Modified smoothing data association for target tracking in clutter",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112969",
    "abstract": "Tracking an unknown target in noisy environment is difficult especially when the target is maneuvering and has unknown trajectory. Smoother uses measurements from future scans to estimate the target state in past scan. This requires the fusion of forward and backward prediction. However, due to uncertain target motions and low detection probabilities, backward prediction could not associate with forward prediction which results in inequitable fusion pair and thus, smoothing performance could not be improved. To cope with these difficulties, the proposed algorithm modifies the fixed-lag smoothing data association based on the integrated probabilistic data association (IPDA) algorithm and a new algorithm called modified smoothing IPDA (MSIPDA) is developed. MSIPDA utilizes two IPDA filters to obtain forward IPDA (fIPDA) track and backward (bIPDA) track estimation in each scan. Each fIPDA prediction generates multiple fusion pairs in association with bIPDA multi-track prediction. These fusion pairs are created in the form of components. As a result, multiple smoothing components are formed with their smoothing component data association probabilities for computing MSIPDA track components state estimate. In addition, the smoothing data association probabilities upgrade the forward track which makes the forward track more powerful for target tracking in clutter. The numerical assessment of MSIPDA is verified using simulations. The result shows significant false track discrimination performance in comparison to existing algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419306876",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Astronomy",
      "Clutter",
      "Computer science",
      "Computer vision",
      "Data association",
      "Operating system",
      "Pedagogy",
      "Physics",
      "Probabilistic logic",
      "Psychology",
      "Radar",
      "Sensor fusion",
      "Smoothing",
      "Telecommunications",
      "Track (disk drive)",
      "Tracking (education)",
      "Trajectory"
    ],
    "authors": [
      {
        "surname": "Memon",
        "given_name": "Sufyan Ali"
      },
      {
        "surname": "Song",
        "given_name": "Taek Lyul"
      },
      {
        "surname": "Memon",
        "given_name": "Kashif Hussain"
      },
      {
        "surname": "Ullah",
        "given_name": "Ihsan"
      },
      {
        "surname": "Khan",
        "given_name": "Uzair"
      }
    ]
  },
  {
    "title": "Gaussian mutational chaotic fruit fly-built optimization and feature selection",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112976",
    "abstract": "To cope with the potential shortcomings of classical fruit fly optimization algorithm (FOA), a new version of FOA with Gaussian mutation operator and the chaotic local search strategy (MCFOA) is proposed in this research. First, the Gaussian mutation operator is introduced into the basic FOA to avoid premature convergence and improve the exploitative tendencies in the algorithm (MFOA). Then, chaotic local search method is adopted for enhancing the local searching ability of the swarm of agents (CFOA). To substantiate the efficiency of three proposed methods, a comprehensive comparison has been completed using 23 benchmark functions with different characteristics. The best version of FOA among them is the MCFOA, which is extensively compared with the notable swarm-intelligence algorithms like bat algorithm (BA), particle swarm optimization algorithm (PSO), and several advanced FOA-based methods such as chaotic FOA (CIFOA), improved FOA (IFOA), multi-swarm FOA (swarm_MFOA) and differential evolution based FOA (DFOA). Numerical results show that two embedded strategies will effectively boost the performance of FOA for optimization tasks. In addition, MCFOA is also applied to feature selection problems. The results also prove that MCFOA can obtain the optimal classification accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419306943",
    "keywords": [
      "Adaptive mutation",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biochemistry",
      "Chaotic",
      "Chemistry",
      "Computer science",
      "Convergence (economics)",
      "Differential evolution",
      "Economic growth",
      "Economics",
      "Feature (linguistics)",
      "Feature selection",
      "Gaussian",
      "Gene",
      "Genetic algorithm",
      "Geodesy",
      "Geography",
      "Linguistics",
      "Local search (optimization)",
      "Mathematical optimization",
      "Mathematics",
      "Multi-swarm optimization",
      "Mutation",
      "Operator (biology)",
      "Particle swarm optimization",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Repressor",
      "Selection (genetic algorithm)",
      "Swarm behaviour",
      "Swarm intelligence",
      "Transcription factor"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Xiang"
      },
      {
        "surname": "Xu",
        "given_name": "Yueting"
      },
      {
        "surname": "Yu",
        "given_name": "Caiyang"
      },
      {
        "surname": "Heidari",
        "given_name": "Ali Asghar"
      },
      {
        "surname": "Li",
        "given_name": "Shimin"
      },
      {
        "surname": "Chen",
        "given_name": "Huiling"
      },
      {
        "surname": "Li",
        "given_name": "Chengye"
      }
    ]
  },
  {
    "title": "A review: Knowledge reasoning over knowledge graph",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112948",
    "abstract": "Mining valuable hidden knowledge from large-scale data relies on the support of reasoning technology. Knowledge graphs, as a new type of knowledge representation, have gained much attention in natural language processing. Knowledge graphs can effectively organize and represent knowledge so that it can be efficiently utilized in advanced applications. Recently, reasoning over knowledge graphs has become a hot research topic, since it can obtain new knowledge and conclusions from existing data. Herein we review the basic concept and definitions of knowledge reasoning and the methods for reasoning over knowledge graphs. Specifically, we dissect the reasoning methods into three categories: rule-based reasoning, distributed representation-based reasoning and neural network-based reasoning. We also review the related applications of knowledge graph reasoning, such as knowledge graph completion, question answering, and recommender systems. Finally, we discuss the remaining challenges and research opportunities for knowledge graph reasoning.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419306669",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Conceptual graph",
      "Graph",
      "Knowledge engineering",
      "Knowledge extraction",
      "Knowledge graph",
      "Knowledge management",
      "Knowledge representation and reasoning",
      "Model-based reasoning",
      "Open Knowledge Base Connectivity",
      "Opportunistic reasoning",
      "Organizational learning",
      "Personal knowledge management",
      "Qualitative reasoning",
      "Reasoning system",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Xiaojun"
      },
      {
        "surname": "Jia",
        "given_name": "Shengbin"
      },
      {
        "surname": "Xiang",
        "given_name": "Yang"
      }
    ]
  },
  {
    "title": "Modeling of electrical resistivity of soil based on geotechnical properties",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112966",
    "abstract": "Determining the relationship between the electrical resistivity of soil and its geotechnical properties is an important engineering problem. This study aims to develop methodology for finding the best model that can be used to predict the electrical resistivity of soil, based on knowing its geotechnical properties. The research develops several linear models, three non-linear models, and three artificial neural network models (ANN). These models are applied to the experimental data set comprises 864 observations and five variables. The results show that there are significant exponential negative relationships between the electrical resistivity of soil and its geotechnical properties. The most accurate prediction values are obtained using the ANN model. The cross-validation analysis confirms the high precision of the selected predictive model. This research is the first rigorous systematic analysis and comparison of difference methodologies in ground electrical resistivity studies. It provides practical guidelines and examples of design, development and testing non-linear relationships in engineering intelligent systems and applications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419306840",
    "keywords": [
      "Artificial neural network",
      "Computer science",
      "Electrical engineering",
      "Electrical resistivity and conductivity",
      "Engineering",
      "Exponential function",
      "Geology",
      "Geotechnical engineering",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Soil science"
    ],
    "authors": [
      {
        "surname": "Alsharari",
        "given_name": "Bandar"
      },
      {
        "surname": "Olenko",
        "given_name": "Andriy"
      },
      {
        "surname": "Abuel-Naga",
        "given_name": "Hossam"
      }
    ]
  },
  {
    "title": "A fully-automated deep learning pipeline for cervical cancer classification",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112951",
    "abstract": "Cervical cancer ranks the fourth most common cancer among females worldwide with roughly 528, 000 new cases yearly. Around 85% of the new cases occurred in less-developed countries. In these countries, the high fatality rate is mainly attributed to the lack of skilled medical staff and appropriate medical pre-screening procedures. Images capturing the cervical region, known as cervigrams, are the gold-standard for the basic evaluation of cervical cancer presence. Cervigrams have high inter-rater variability especially among less skilled medical specialists. In this paper, we develop a fully-automated pipeline for cervix detection and cervical cancer classification from cervigram images. The proposed pipeline consists of two pre-trained deep learning models for the automatic cervix detection and cervical tumor classification. The first model detects the cervix region 1000 times faster than state-of-the-art data-driven models while achieving a detection accuracy of 0.68 in terms of intersection of union (IoU) measure. Self-extracted features are used by the second model to classify the cervix tumors. These features are learned using two lightweight models based on convolutional neural networks (CNN). The proposed deep learning classifier outperforms existing models in terms of classification accuracy and speed. Our classifier is characterized by an area under the curve (AUC) score of 0.82 while classifying each cervix region 20 times faster. Finally, the pipeline accuracy, speed and lightweight architecture make it very appropriate for mobile phone deployment. Such deployment is expected to drastically enhance the early detection of cervical cancer in less-developed countries.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419306694",
    "keywords": [
      "Artificial intelligence",
      "Cancer",
      "Cervical cancer",
      "Cervix",
      "Classifier (UML)",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Internal medicine",
      "Machine learning",
      "Medicine",
      "Pipeline (software)",
      "Programming language"
    ],
    "authors": [
      {
        "surname": "Alyafeai",
        "given_name": "Zaid"
      },
      {
        "surname": "Ghouti",
        "given_name": "Lahouari"
      }
    ]
  },
  {
    "title": "DSPNet: Deep scale purifier network for dense crowd counting",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112977",
    "abstract": "Crowd counting has produced considerable concern in recent years. However, crowd counting in highly congested scenes is a challenging problem owing to scale variation. To remedy this issue, we propose a novel deep scale purifier network (DSPNet) that can encode multiscale features and reduce the loss of contextual information for dense crowd counting. Our proposed method has two strong points. First, the DSPNet model consists of a frontend and a backend. The frontend is a conventional deep convolutional neural network, while the unified deep neural network backend adopts a “maximal ratio combining” strategy to learn complementary scale information at different levels. The scale purifier module, which improves scale representations, can effectively fuse multiscale features. Second, DSPNet performs the whole RGB image-based inference to facilitate model learning and decrease contextual information loss. Our customized network is end-to-end and has a fully convolutional architecture. We demonstrate the generalization ability of our approach by cross-scene evaluation. Extensive experiments on three publicly available crowd counting benchmarks (i.e., UCF-QNRF, ShanghaiTech, and UCF_CC_50 datasets) show that our DSPNet delivers superior performance against state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419306955",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer network",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "ENCODE",
      "Electrical engineering",
      "Engineering",
      "Fuse (electrical)",
      "Gene",
      "Generalization",
      "Inference",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Network architecture",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "RGB color model",
      "Scale (ratio)"
    ],
    "authors": [
      {
        "surname": "Zeng",
        "given_name": "Xin"
      },
      {
        "surname": "Wu",
        "given_name": "Yunpeng"
      },
      {
        "surname": "Hu",
        "given_name": "Shizhe"
      },
      {
        "surname": "Wang",
        "given_name": "Ruobin"
      },
      {
        "surname": "Ye",
        "given_name": "Yangdong"
      }
    ]
  },
  {
    "title": "Precise drone location and tracking by adaptive matched filtering from a top-view ToF camera",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112989",
    "abstract": "Nowadays the use of drones is rapidly growing, and the autonomous navigation capability depends on knowing their position at any time. The precision and robustness of a local positioning system for these devices is of particular importance in takeoff and landing maneuvers, especially in GPS-denied environments. The main contribution of this work lies in the development of a precise local positioning and tracking system for drones. For this purpose, a ToF camera has been installed on the ceiling in order to make use of its depth maps. Taking as a reference the disturbance caused by a quadrotor in one of these maps, a novel 2D matched filter has been designed based on a Gaussian wavelet. This filter allows the system to quickly detect all drones flying in the scene. Moreover, it is dynamically adapted to the image portion that they occupy, taking into account the variation of this parameter with their flying altitude, which has also been theoretically determined. The whole algorithm leads to a precise 3D drone positioning. In addition, the proposed system is also robust against short-time occlusions, since the measurements history can be used to predict future positions, thus avoiding the complete loss of tracking.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419307055",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Biology",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Drone",
      "Filter (signal processing)",
      "Gene",
      "Genetics",
      "Global Positioning System",
      "Robustness (evolution)",
      "Telecommunications",
      "Tracking system"
    ],
    "authors": [
      {
        "surname": "Paredes",
        "given_name": "José A."
      },
      {
        "surname": "Álvarez",
        "given_name": "Fernando J."
      },
      {
        "surname": "Aguilera",
        "given_name": "Teodoro"
      },
      {
        "surname": "Aranda",
        "given_name": "Fernando J."
      }
    ]
  },
  {
    "title": "GRASP algorithm for the unrelated parallel machine scheduling problem with setup times and additional resources",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112959",
    "abstract": "This paper provides practitioners with new approaches for solving realistic scheduling problems that consider additional resources, which can be implemented on expert and intelligent systems and help decision making in realistic settings. More specifically, we study the unrelated parallel machine scheduling problem with setup times and additional limited resources in the setups (UPMSR-S), with makespan minimization criterion. This is a more realistic extension of the traditional problem, in which the setups are assumed to be done without using additional resources (e.g. workers). We propose three metaheuristics following two approaches: a first approach that ignores the information about additional resources in the constructive phase, and a second approach that takes into account this information about the resources. Computational experiments are carried out over a benchmark of small and large instances. After the computational analysis we conclude that the second approach shows an excellent performance, overcoming the first approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419306773",
    "keywords": [
      "Algorithm",
      "Benchmark (surveying)",
      "Computer science",
      "Constructive",
      "GRASP",
      "Geodesy",
      "Geography",
      "Job shop scheduling",
      "Mathematical optimization",
      "Mathematics",
      "Metaheuristic",
      "Minification",
      "Operating system",
      "Process (computing)",
      "Programming language",
      "Schedule",
      "Scheduling (production processes)"
    ],
    "authors": [
      {
        "surname": "Yepes-Borrero",
        "given_name": "Juan C."
      },
      {
        "surname": "Villa",
        "given_name": "Fulgencia"
      },
      {
        "surname": "Perea",
        "given_name": "Federico"
      },
      {
        "surname": "Caballero-Villalobos",
        "given_name": "Juan Pablo"
      }
    ]
  },
  {
    "title": "Distributed mining of high utility time interval sequential patterns using mapreduce approach",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112967",
    "abstract": "High Utility Sequential Pattern mining (HUSP) algorithms aim to find all the high utility sequences from a sequence database. Due to the large explosion of data, recently few distributed algorithms have been designed for mining HUSPs based on the MapReduce framework. However, the existing HUSP algorithms such as USpan, HUS-Span and BigHUSP are able to predict only the order of items, they do not predict the time between the items, that is, they do not include the time intervals between the successive items. But in a real-world scenario, time interval patterns provide more valuable information than conventional high utility sequential patterns. Therefore, we propose a distributed high utility time interval sequential pattern mining (DHUTISP) algorithm using the MapReduce approach that is suitable for big data. DHUTISP creates a novel time interval utility linked list data structure (TIUL) to efficiently calculate the utility of the resulting patterns. Moreover, two utility upper bounds, namely, remaining utility upper bound (RUUB) and co-occurrence utility upper bound (CUUB) are proposed to prune the unpromising candidates. We conducted various experiments to prove the efficiency of the proposed algorithm over both the distributed and non-distributed approaches. The experimental results show the efficiency of DHUTISP over state-of-the-art algorithms, namely, BigHUSP, AHUS-P, PUSOM and UTMining_A.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419306852",
    "keywords": [
      "Algorithm",
      "Biology",
      "Combinatorics",
      "Computer science",
      "Data mining",
      "Distributed computing",
      "Execution time",
      "Genetics",
      "Interval (graph theory)",
      "Mathematical analysis",
      "Mathematics",
      "Running time",
      "Sequence (biology)",
      "Upper and lower bounds"
    ],
    "authors": [
      {
        "surname": "Sumalatha",
        "given_name": "Saleti"
      },
      {
        "surname": "Subramanyam",
        "given_name": "R.B.V."
      }
    ]
  },
  {
    "title": "Agent-based multi-edge network simulation model for knowledge diffusion through board interlocks",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112962",
    "abstract": "Interlocking directors play a crucial role in corporate decision making and profitability of an organization. They also act as information channels between corporate entities and various hypotheses have been proposed in the literature regarding effectiveness of board-interlocks. Generally these hypotheses are tested against country specific data over a given time period. Therefore, a network diffusion model of companies, with interlocking directors as the links, would serve as an expert system to test hypotheses and policies in a more general framework. Such a model will also be useful in decision making and identifying organizations and directors for board-interlock for optimal performance. In this paper, we propose a hybrid multi-edge directed weighted network model and an agent based simulation scheme for analysing the diffusion process through board interlocks. The general framework introduced in this work is capable of incorporating individual characteristics of directors such as personal identification with a corporate firm. We test the hypothesis that the organisational affinity of interlocking directors does not affect the rate of knowledge transfer using the Indian Company Affiliation Network (ICAN) and the results show that organisational affinity affects the diffusion rate in a statistically significant manner. This is further confirmed with three other random networks of different topologies. The model is also used to simulate knowledge diffusion of the Indian Company Affiliation Network (ICAN) and investigate the uniformity of diffusion rates across various clusters.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419306803",
    "keywords": [
      "Artificial intelligence",
      "Business",
      "Computer science",
      "Diffusion",
      "Electrical engineering",
      "Engineering",
      "Enhanced Data Rates for GSM Evolution",
      "Finance",
      "Interlock",
      "Interlocking",
      "Knowledge management",
      "Mechanical engineering",
      "Operating system",
      "Physics",
      "Process (computing)",
      "Profitability index",
      "Thermodynamics"
    ],
    "authors": [
      {
        "surname": "Sankar",
        "given_name": "C. Prem"
      },
      {
        "surname": "Thumba",
        "given_name": "Drisya Alex"
      },
      {
        "surname": "Ramamohan",
        "given_name": "T.R."
      },
      {
        "surname": "Chandra",
        "given_name": "S.S. Vinod"
      },
      {
        "surname": "Satheesh Kumar",
        "given_name": "K."
      }
    ]
  },
  {
    "title": "Learning parameters of Bayesian networks from datasets with systematically missing data: A meta–analytic approach",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112956",
    "abstract": "Previous research suggested that using additional data sources could improve parameter learning in Bayesian networks. However, when additional datasets do not include all network variables, neither standard Bayesian network learning techniques nor standard missing data methods can be applied. In such situations, the use of a meta–analytic approach is proposed. The performance of one such meta–analytic approach was evaluated by simulating several study results on two real–life biomedical examples (one discrete and one Gaussian Bayesian network). Regardless of the network type, the meta–analytic approach showed higher mean log–likelihood values, less sensitive to the presence of heterogeneity, than a single dataset analysis. The difference between the two methods was most pronounced when sample sizes were small ( N = 100 ). For the meta–analytic approach, the increase in log–likelihood was in most cases positively related to the number of nodes estimated with additional data. However, as in the case of single dataset analysis, care is needed when estimating rare event probabilities from small datasets due to the problems with unidentifiability and increased bias.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419306748",
    "keywords": [
      "Artificial intelligence",
      "Bayesian network",
      "Bayesian probability",
      "Computer science",
      "Data mining",
      "Machine learning",
      "Mathematics",
      "Missing data",
      "Sample size determination",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Kovačić",
        "given_name": "Jelena"
      }
    ]
  },
  {
    "title": "Application of deep reinforcement learning to intrusion detection for supervised problems",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112963",
    "abstract": "The application of new techniques to increase the performance of intrusion detection systems is crucial in modern data networks with a growing threat of cyber-attacks. These attacks impose a greater risk on network services that are increasingly important from a social end economical point of view. In this work we present a novel application of several deep reinforcement learning (DRL) algorithms to intrusion detection using a labeled dataset. We present how to perform supervised learning based on a DRL framework. The implementation of a reward function aligned with the detection of intrusions is extremely difficult for Intrusion Detection Systems (IDS) since there is no automatic way to identify intrusions. Usually the identification is performed manually and stored in datasets of network features associated with intrusion events. These datasets are used to train supervised machine learning algorithms for classifying intrusion events. In this paper we apply DRL using two of these datasets: NSL-KDD and AWID datasets. As a novel approach, we have made a conceptual modification of the classic DRL paradigm (based on interaction with a live environment), replacing the environment with a sampling function of recorded training intrusions. This new pseudo-environment, in addition to sampling the training dataset, generates rewards based on detection errors found during training. We present the results of applying our technique to four of the most relevant DRL models: Deep Q-Network (DQN), Double Deep Q-Network (DDQN), Policy Gradient (PG) and Actor-Critic (AC). The best results are obtained for the DDQN algorithm. We show that DRL, with our model and some parameter adjustments, can improve the results of intrusion detection in comparison with current machine learning techniques. Besides, the classifier obtained with DRL is faster than alternative models. A comprehensive comparison of the results obtained with other machine learning models is provided for the AWID and NSL-KDD datasets, together with the lessons learned from the application of several design alternatives to the four DRL models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419306815",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer science",
      "Data mining",
      "Evolutionary biology",
      "Function (biology)",
      "Geochemistry",
      "Geology",
      "Intrusion",
      "Intrusion detection system",
      "Machine learning",
      "Reinforcement learning",
      "Supervised learning"
    ],
    "authors": [
      {
        "surname": "Lopez-Martin",
        "given_name": "Manuel"
      },
      {
        "surname": "Carro",
        "given_name": "Belen"
      },
      {
        "surname": "Sanchez-Esguevillas",
        "given_name": "Antonio"
      }
    ]
  },
  {
    "title": "Human action recognition using double discriminative sparsity preserving projections and discriminant ridge-based classifier based on the GDWL-l1 graph",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112927",
    "abstract": "Human action recognition is defined as determining the actions of humans happening in video sequences. Human action recognition is one of the interesting topics which can play important role in intelligence, surveillance and health protection systems' performance; the performance of human action recognition methods is a vital issue that has been focused on many recent papers. Subspace learning and classification steps can impress the performance of the human action recognition methods. Accordingly, in this paper, new subspace learning and classification algorithms are proposed for human action recognition. Notably, graphs play important role to describe the relationship of data but most of the graph based methods used Euclidean distance metric. To overcome it, a geodesic distance based weighted LASSO l1-graph (GDWL-l1 graph) is proposed to extract the between-class and within-class graphs. Then, double discriminative sparsity preserving projections (DDSPP) algorithm is introduced to map the high-dimensional data to a new discriminant low-dimensional space using these graphs in order to have better discrimination besides sparsity and locality preserving in the mapped space. Subspace learning using DDSPP algorithm leads to a discriminative and sparse low-dimensional space. At the end, a discriminant ridge-based classifier (DRC) is introduced to inherit the grouping effect of the ridge regression besides incorporating the geodesic distance by defining a criterion of the classification. Experimental results show the suitable performance of the proposed method on HMDB51 and UCF101 datasets which are as 66.41% and 92.46%, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419306451",
    "keywords": [
      "Action recognition",
      "Artificial intelligence",
      "Class (philosophy)",
      "Classifier (UML)",
      "Computer science",
      "Discriminant",
      "Discriminative model",
      "Euclidean distance",
      "Geodesic",
      "Graph",
      "Linear discriminant analysis",
      "Linguistics",
      "Locality",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Subspace topology",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Rahimi",
        "given_name": "Sahere"
      },
      {
        "surname": "Aghagolzadeh",
        "given_name": "Ali"
      },
      {
        "surname": "Ezoji",
        "given_name": "Mehdi"
      }
    ]
  },
  {
    "title": "A framework for suspect face retrieval using linguistic descriptions",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112925",
    "abstract": "Facial sketch plays a vital role in suspect’s identification and apprehension by law enforcement agencies. The sketch artist develops sketches based on the memory of the eye witness. However, sketch artists are less in number and limited to availability. It is also observed that as time passes, the onlooker even forgets many of the critical attributes, which proves costly in time-sensitive investigations. State-of-the-art sketch-photo retrieval methods have used the sketch to retrieve the suspect’s photograph and ignore the importance of time sensitivity. In this paper, we have proposed a linguistic description based approach for suspect’s photo retrieval. In the proposed approach, the facial attributes and their descriptions are extracted from the input linguistic information using parts of speech and attributes ontology. Attributes are ranked according to their importance in the identification of a suspect. An ensemble classifier which aggregates the predictions of multiple classifiers is used to retrieve a suspect’s image. Experiments are performed on standard datasets. The performance of the proposed approach is evaluated by comparing it with the existing methods of linguistic sketch-based retrieval as well as with sketch to photo retrieval method. Experimental results illustrate that the proposed linguistic-base method using ensemble classification attains auspicious performance as compared to state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419306438",
    "keywords": [
      "Algorithm",
      "Apprehension",
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Classifier (UML)",
      "Cognitive psychology",
      "Computer science",
      "Identification (biology)",
      "Information retrieval",
      "Law",
      "Natural language processing",
      "Political science",
      "Programming language",
      "Psychology",
      "Sketch",
      "Suspect",
      "Witness"
    ],
    "authors": [
      {
        "surname": "Khan",
        "given_name": "Mohd. Aamir"
      },
      {
        "surname": "Jalal",
        "given_name": "Anand Singh"
      }
    ]
  },
  {
    "title": "Automatic design of specialized algorithms for the binary knapsack problem",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112908",
    "abstract": "Not all problem instances of a difficult combinatorial optimization problem have the same degree of difficulty for a given algorithm. Surprisingly, apparently similar problem instances may require notably different computational efforts to be solved. Few studies have explored the case that the algorithm that solves a combinatorial optimization problem is automatically designed. In consequence, the generation of the best algorithms may produce specialized algorithms according to the problem instances used during the constructive step. Following a constructive process based on genetic programming that combines heuristic components with an exact method, new algorithms for the binary knapsack problem are produced. We found that most of the automatically designed algorithms have better performance when solving instances of the same type used during construction, although the algorithms also perform well with other types of similar instances. The rest of the algorithms are partially specialized. We also found that the exact method that only solves a small knapsack problem has a key role in such results. When the algorithms are produced without considering such a method, the errors are higher. We observed this fact when the algorithms were constructed with a combination of instances from different types. These results suggest that the better the pre-classification of the instances of an optimization problem, the more specific and more efficient are the algorithms produced by the automatic generation of algorithms. Consequently, the method described in this article accelerates the search for efficient methods for NP-hard optimization problems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419306268",
    "keywords": [
      "Algorithm",
      "Arithmetic",
      "Artificial intelligence",
      "Binary number",
      "Combinatorial optimization",
      "Computer science",
      "Computer security",
      "Constructive",
      "Continuous knapsack problem",
      "Generalized assignment problem",
      "Heuristic",
      "Key (lock)",
      "Knapsack problem",
      "Mathematical optimization",
      "Mathematics",
      "Operating system",
      "Optimization problem",
      "Process (computing)"
    ],
    "authors": [
      {
        "surname": "Acevedo",
        "given_name": "Nicolás"
      },
      {
        "surname": "Rey",
        "given_name": "Carlos"
      },
      {
        "surname": "Contreras-Bolton",
        "given_name": "Carlos"
      },
      {
        "surname": "Parada",
        "given_name": "Victor"
      }
    ]
  },
  {
    "title": "Weighted aspect-based opinion mining using deep learning for recommender system",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112871",
    "abstract": "The main goal of Aspect-Based Opinion Mining is to extract product's aspects and the associated user opinions from the user text review. Although this serves as vital source information for enhancing rating prediction performance, few studies have attempted to fully utilize it for better accuracy of recommendation systems. Most of these studies typically assign equal weights to all aspects in the opinion mining process, however, in practices; users tend to give different priority on different aspects of the product when reaching overall ratings. In addition, most of the existing methods typically rely on handcrafted, rule-based or double propagation methods in the opinion mining process which are known to be time-consuming and often inclined to errors. This could affect the reliability and performance of the recommender systems (RS). Therefore, in this paper, we propose a weighted Aspect-based Opinion mining using Deep learning method for Recommender system (AODR) that can extract product's aspects and the underlying weighted user opinions from the review text using a deep learning method and then fuse them into extended collaborative filtering (CF) technique for improving the RS. The proposed method is basically comprised of two components: (1) Aspect-based opinion mining module which aims to extract the product aspects from the review text to generate aspect rating matrix. (2) Recommendation generation component that uses tensor factorization (TF) technique to compute weighted aspect ratings and finally infer the overall rating prediction. We evaluate the proposed model in terms of both aspect extraction and recommendation performance. Experiment results on different datasets show that our AODR model achieves better results compared to the baselines.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419305810",
    "keywords": [
      "Artificial intelligence",
      "Association rule learning",
      "Collaborative filtering",
      "Computer science",
      "Data mining",
      "Deep learning",
      "Electrical engineering",
      "Engineering",
      "Fuse (electrical)",
      "Geometry",
      "Information retrieval",
      "Machine learning",
      "Mathematics",
      "Operating system",
      "Physics",
      "Power (physics)",
      "Process (computing)",
      "Product (mathematics)",
      "Quantum mechanics",
      "Recommender system",
      "Reliability (semiconductor)",
      "Sentiment analysis"
    ],
    "authors": [
      {
        "surname": "Da'u",
        "given_name": "Aminu"
      },
      {
        "surname": "Salim",
        "given_name": "Naomie"
      },
      {
        "surname": "Rabiu",
        "given_name": "Idris"
      },
      {
        "surname": "Osman",
        "given_name": "Akram"
      }
    ]
  },
  {
    "title": "Eating and drinking gesture spotting and recognition using a novel adaptive segmentation technique and a gesture discrepancy measure",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112888",
    "abstract": "Despite the increasing developments on human activity recognition using wearable technology, there are still many open challenges in spotting and recognising sporadic gestures. As opposed to activities, which exhibit continuous behaviour, the difficulty of spotting gestures lies in their rather sparse nature. This paper proposes a novel solution to spot and recognise a set of similar eating and drinking gestures from continuous inertial data streams. First, potential segments containing an eating or a drinking gesture are found using a Crossings-based Adaptive Segmentation Technique (CAST). Second, further to the long-established range of features employed in previous human activities recognition research work, a gesture discrepancy measure is proposed to improve the classification performance of the system. At the final step, a range of state-of-the-art classification models is employed for evaluation. Various conclusions can be drawn from the results obtained. First, given the 100% recall achieved at the segmentation step, the CAST can be considered a reliable segmentation technique for spotting drinking and eating gestures which may be employed in future gesture spotting work. Second, the addition of gesture discrepancy as a feature descriptor consistently improves the classification performance of the system. Third, the reliability of the food and drink intake monitoring approach proposed in this work finds support on the out-performance of previous similar work.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419305986",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Embedded system",
      "Feature (linguistics)",
      "Gesture",
      "Gesture recognition",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Segmentation",
      "Spotting",
      "Wearable computer"
    ],
    "authors": [
      {
        "surname": "Anderez",
        "given_name": "Dario Ortega"
      },
      {
        "surname": "Lotfi",
        "given_name": "Ahmad"
      },
      {
        "surname": "Pourabdollah",
        "given_name": "Amir"
      }
    ]
  },
  {
    "title": "Simulation optimization approach for patient scheduling at destination medical centers",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112881",
    "abstract": "This study presents a simulation optimization approach for scheduling medical tourists who travel to destination medical centers (DMCs). The objective of this research is two-fold: (1) to minimize deviations from a patient's preferred start day and (2) to minimize the flow time of patients at the DMC. The duration of treatment is considered uncertain, and patients are scheduled over a multiday scheduling window. This problem is modeled as a flow-shop scheduling problem (FSP) where each patient should undergo a specific number of stages to complete treatment. The goal is to assign patients to different days within the time horizon such that the minimum deviation from the patient's preferred start day and the minimum average patient flow time are achieved. Due to the complex and stochastic nature of the problem, the simulation-optimization approach is used. This approach includes simulation evaluation at each iteration of the optimization procedure. The optimization procedures used in this research are tabu search (TS) and simulated annealing (SA). Finally, several test problems are used to evaluate the performance of the proposed simulation optimization approach. Based on our results, the total cost of the proposed model is significantly less than that of the problem without any admission planning and patient sequencing.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419305913",
    "keywords": [
      "Algorithm",
      "Computer science",
      "Flow shop scheduling",
      "Job shop scheduling",
      "Mathematical optimization",
      "Mathematics",
      "Operating system",
      "Schedule",
      "Scheduling (production processes)",
      "Simulated annealing",
      "Tabu search",
      "Time horizon"
    ],
    "authors": [
      {
        "surname": "Rezaeiahari",
        "given_name": "Mandana"
      },
      {
        "surname": "Khasawneh",
        "given_name": "Mohammad T."
      }
    ]
  },
  {
    "title": "Comparing of deep neural networks and extreme learning machines based on growing and pruning approach",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112875",
    "abstract": "Recently, the studies based on Deep Neural Networks and Extreme Learning Machines have become prominent. The models of parameters designed in these studies have been chosen randomly and the models have been designed in this direction. The main focus of this study is to determine the ideal parameters i.e. optimum hidden layer number, optimum hidden neuron number and activation function for Deep Neural Networks and Extreme Learning Machines architectures based on growing and pruning approach and to compare the performances of the models designed. The performances of the models are evaluated on two datasets; Parkinson and Self-Care Activities Dataset. Multi experiments have verified that the Deep Neural Networks architectures present a good prediction performance and this architecture outperforms the Extreme Learning Machines.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419305858",
    "keywords": [
      "Agronomy",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer science",
      "Deep learning",
      "Deep neural networks",
      "Extreme learning machine",
      "Focus (optics)",
      "Machine learning",
      "Optics",
      "Physics",
      "Pruning"
    ],
    "authors": [
      {
        "surname": "Akyol",
        "given_name": "Kemal"
      }
    ]
  },
  {
    "title": "Collaborative multi-depot logistics network design with time window assignment",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112910",
    "abstract": "In logistics operation, delivery times are often uncertain for customers, and accommodating this uncertainty poses operation challenges as well as extra cost for logistics service providers. The delivery time uncertainty is particularly an issue if there are multiple service providers in a logistics network. To address this issue, we formulate and solve a collaborative multi-depot vehicle routing problem with time window assignment (CMDVRPTWA) to effectively reduce the impact of changing time windows on operating costs. This paper establishes a bi-objective programming model that optimize the total operating cost and the total number of delivery vehicles. A hybrid heuristic algorithm consisting of K-means clustering, Clarke–Wright (CW) saving algorithm and an Extended Non-dominated Sorting Genetic Algorithm-II (E-NSGA-II) is presented to efficiently solve CMDVRPTWA. The clustering and CW saving algorithm are employed to increase the likelihood of finding the optimal vehicle routes by identifying a feasible initial solution. The E-NSGA-II procedure combines partial-mapped crossover (PMC), relocation, 2-opt* exchange and swap mutation operations to find the optimal solution with pre-defined iteration and termination rules. Profit allocation schemes are then analyzed using the Game Quadratic Programming (GQP) method, and the optimal sequences of joining coalitions are obtained based on the principle that coalition participants’ benefits should be non-decreasing when a new participant joins the coalition. We conduct three empirical studies on a small-scale example, on several benchmark datasets and on a large-scale logistics network in Chongqing city, China. Further comparative analysis indicates that E-NSGA-II outperforms most other algorithms in solving CMDVRPTWA. This novel approach identifies profit allocation strategies that ensure the stability and reliability of the collaborative coalitions in the context of flexible customer service time windows, and can be utilized to improve the efficiency of urban logistics and intelligent transportation networks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419306281",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer network",
      "Computer science",
      "Crossover",
      "Economics",
      "Economy",
      "Engineering",
      "Heuristics",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Operating system",
      "Operations research",
      "Routing (electronic design automation)",
      "Service (business)",
      "Service provider",
      "Vehicle routing problem"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Yong"
      },
      {
        "surname": "Zhang",
        "given_name": "Shuanglu"
      },
      {
        "surname": "Guan",
        "given_name": "Xiangyang"
      },
      {
        "surname": "Peng",
        "given_name": "Shouguo"
      },
      {
        "surname": "Wang",
        "given_name": "Haizhong"
      },
      {
        "surname": "Liu",
        "given_name": "Yong"
      },
      {
        "surname": "Xu",
        "given_name": "Maozeng"
      }
    ]
  },
  {
    "title": "Forecasting across time series databases using recurrent neural networks on groups of similar series: A clustering approach",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112896",
    "abstract": "With the advent of Big Data, nowadays in many applications databases containing large quantities of similar time series are available. Forecasting time series in these domains with traditional univariate forecasting procedures leaves great potentials for producing accurate forecasts untapped. Recurrent neural networks (RNNs), and in particular Long Short Term Memory (LSTM) networks, have proven recently that they are able to outperform state-of-the-art univariate time series forecasting methods in this context, when trained across all available time series. However, if the time series database is heterogeneous, accuracy may degenerate, so that on the way towards fully automatic forecasting methods in this space, a notion of similarity between the time series needs to be built into the methods. To this end, we present a prediction model that can be used with different types of RNN models on subgroups of similar time series, which are identified by time series clustering techniques. We assess our proposed methodology using LSTM networks, a widely popular RNN variant, together with various clustering algorithms, such as kMeans, DBScan, Partition Around Medoids (PAM), and Snob. Our method achieves competitive results on benchmarking datasets under competition evaluation procedures. In particular, in terms of mean sMAPE accuracy it consistently outperforms the baseline LSTM model, and outperforms all other methods on the CIF2016 forecasting competition dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419306128",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmarking",
      "Biology",
      "Business",
      "Cluster analysis",
      "Computer science",
      "Context (archaeology)",
      "Data mining",
      "Geography",
      "Machine learning",
      "Marketing",
      "Multivariate statistics",
      "Paleontology",
      "Recurrent neural network",
      "Series (stratigraphy)",
      "Time series",
      "Univariate"
    ],
    "authors": [
      {
        "surname": "Bandara",
        "given_name": "Kasun"
      },
      {
        "surname": "Bergmeir",
        "given_name": "Christoph"
      },
      {
        "surname": "Smyl",
        "given_name": "Slawek"
      }
    ]
  },
  {
    "title": "Efficient extraction of consistent bit locations from binarized iris features",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112884",
    "abstract": "Biometric systems which are operationally based on human irides are considered to be an effective way of verifying individual identities. However, features extracted from iris images are sometimes characterized by a low degree of stability for intra-class iris samples. In this work, we develop a robust mechanism for selecting the most consistent bits which are present within binary iris features. In the proposed model, we initially form dynamic clusters of invariant positions from some particular iris feature samples and then mark the centers of these clusters as the most consistent locations. Finally, error information from the corresponding iris masks is incorporated for eliminating any effects of noise from the extracted bits. Our work also introduces a formal criterion, termed as t-consistency, for defining the worst-case consistency of IrisCodes with respect to a tolerance threshold. We have tested our proposed model by extracting up to 300 bits from the CASIAv3 Interval and CASIAv4 Thousand databases, wherein we achieved 0.88-consistency and 0.65-consistency of the IrisCodes respectively. We have also obtained competitive recognition accuracy rates for our model in comparison to their baseline results. Hence our study introduces an efficient technique for extracting the most discriminatory binary features from raw iris images.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419305949",
    "keywords": [
      "Algorithm",
      "Arithmetic",
      "Artificial intelligence",
      "Binary number",
      "Biochemistry",
      "Biometrics",
      "Chemistry",
      "Computer science",
      "Consistency (knowledge bases)",
      "Feature (linguistics)",
      "Feature extraction",
      "Gene",
      "IRIS (biosensor)",
      "Invariant (physics)",
      "Iris recognition",
      "Linguistics",
      "Mathematical physics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Sadhya",
        "given_name": "Debanjan"
      },
      {
        "surname": "De",
        "given_name": "Kanjar"
      },
      {
        "surname": "Raman",
        "given_name": "Balasubramanian"
      },
      {
        "surname": "Roy",
        "given_name": "Partha Pratim"
      }
    ]
  },
  {
    "title": "Color image watermarking based on orientation diversity and color complexity",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112868",
    "abstract": "The Just Noticeable Distortion (JND) can reliably measure the perceptual strength in image watermarking, but, it remains a challenge to computationally model the process of embedding watermark without prior knowledge of the image contents. This paper proposed a novel color image watermarking scheme in discrete cosine transform (DCT) domain based on JND, which takes both orientation diversity and color complexity features into account. Firstly, two indicator was introduced which take into account the differences in the texture types and orientation diversity of the Human Visual System (HVS) in the proposed JND contrast masking (CM) processing. In addition, a novel color complexity weight from Cb-channel is used to guarantee the scheme robustness. Then, a novel JND model combined with the proposed contrast masking and color complexity is applied into quantization watermarking scheme. Compared with the state-of-the-art methods for color image watermarking, experimental results using publicly available images show that our proposed scheme is reliable and effective.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419305780",
    "keywords": [
      "Amplifier",
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Biochemistry",
      "Chemistry",
      "Color image",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Digital watermarking",
      "Discrete cosine transform",
      "Distortion (music)",
      "Embedding",
      "Gene",
      "Human visual system model",
      "Image (mathematics)",
      "Image processing",
      "Just-noticeable difference",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Robustness (evolution)",
      "Watermark"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Jun"
      },
      {
        "surname": "Wan",
        "given_name": "Wen Bo"
      },
      {
        "surname": "Li",
        "given_name": "Xiao Xiao"
      },
      {
        "surname": "Sun",
        "given_name": "Jian De"
      },
      {
        "surname": "Zhang",
        "given_name": "Hua Xiang"
      }
    ]
  },
  {
    "title": "Time-driven feature-aware jointly deep reinforcement learning for financial signal representation and algorithmic trading",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112872",
    "abstract": "Algorithmic trading is a continuous perception and decision making problem, where environment perception requires to learn feature representation from highly nonstationary and noisy financial time series, and decision making requires the algorithm to explore the environment and simultaneously make correct decisions in an online manner without any supervised information. To address these two problems, we propose a time-driven feature-aware jointly deep reinforcement learning model (TFJ-DRL) that integrates deep learning model and reinforcement learning model to improve the financial signal representation learning and action decision making in algorithmic trading. Concretely, we learn the environmental representation by adaptively selecting and reweighting various features of financial signals and summarize the attention values between historical information and changing trend depending on the current state. Besides, the supervised deep learning and reinforcement learning are jointly and iteratively trained to make full use of the supervised signals in the training data, and obtain more update information and stricter loss function constraints, thereby increasing investment returns. TFJ-DRL is evaluated on real-world financial data with different price trends (rising, falling and no obvious direction). A series of analysis show the robust superiority and the extensive applicability of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419305822",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Deep learning",
      "Economics",
      "Feature (linguistics)",
      "Feature learning",
      "Finance",
      "Law",
      "Linguistics",
      "Machine learning",
      "Philosophy",
      "Political science",
      "Politics",
      "Reinforcement learning",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Lei",
        "given_name": "Kai"
      },
      {
        "surname": "Zhang",
        "given_name": "Bing"
      },
      {
        "surname": "Li",
        "given_name": "Yu"
      },
      {
        "surname": "Yang",
        "given_name": "Min"
      },
      {
        "surname": "Shen",
        "given_name": "Ying"
      }
    ]
  },
  {
    "title": "DCR: A new distributed model for human activity recognition in smart homes",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112849",
    "abstract": "Human Activity Recognition (HAR) is an important research issue for pervasive computing that aims to identify human activities in smart homes. In the literature, most reasoning approaches for HAR are based on centralized approach where a central system is responsible for processing and reasoning about sensor data in order to recognize activities. Since sensor data are distributed, heterogeneous, and dynamic (i.e., whose characteristics are varying over time) in the smart home, reasoning process on these data for HAR needs to be distributed over a group of heterogeneous, autonomous and interacting entities in order to be more efficient. This paper proposes a main contribution, the DCR approach, a fully Distributed Collaborative Reasoning multi-agent approach where agents, with diverse classifiers, observe sensor data, make local predictions, communicate and collaborate to identify current activities. Then, an improved version of the DCR approach is proposed, the DCR-OL approach, a distributed Online Learning approach where learning agents learns from their collaborations to improve their own performance in activity recognition. Finally, we test our approaches by performing an evaluation study on Aruba dataset, that indicates an enhancement in terms of accuracy, F-measure and G-mean metrics compared to the centralized approach and also compared to a distributed approach existing in the literature.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419305512",
    "keywords": [
      "Activity recognition",
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Economics",
      "Embedded system",
      "Finance",
      "Human–computer interaction",
      "Internet of Things",
      "Machine learning",
      "Operating system",
      "Order (exchange)",
      "Process (computing)",
      "Smart environment"
    ],
    "authors": [
      {
        "surname": "Jarraya",
        "given_name": "Amina"
      },
      {
        "surname": "Bouzeghoub",
        "given_name": "Amel"
      },
      {
        "surname": "Borgi",
        "given_name": "Amel"
      },
      {
        "surname": "Arour",
        "given_name": "Khedija"
      }
    ]
  },
  {
    "title": "A constrained multi-swarm particle swarm optimization without velocity for constrained optimization problems",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112882",
    "abstract": "The original particle swarm optimization (PSO) is not able to tackle constrained optimization problems (COPs) due to the absence of constraint handling techniques. Furthermore, most existing PSO variants can only perform well in certain types of optimization problem and tend to suffer with premature convergence due to the limited search operator and directional information used to guide the search process. An improved PSO variant known as the constrained multi-swarm particle swarm optimization without velocity (CMPSOWV) is proposed in this paper to overcome the aforementioned drawbacks. Particularly, a constraint handling technique is first incorporated into CMPSOWV to guide population searching towards the feasible regions of search space before optimizing the objective function within the feasible regions. Two evolution phases known as the current swarm evolution and memory swarm evolution are also introduced to offer the multiple search operators for each CMPSOWV particle, aiming to improve the robustness of algorithm in solving different types of COPs. Finally, two diversity maintenance schemes of multi-swarm technique and probabilistic mutation operator are incorporated to prevent the premature convergence of CMPSOWV. The overall optimization performances of CMPSOWV in solving the CEC 2006 and CEC 2017 benchmark functions and real-world engineering design problems are compared with selected constrained optimization algorithms. Extensive simulation results report that the proposed CMPSOWV has demonstrated the best search accuracy among all compared methods in solving majority of problems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419305925",
    "keywords": [
      "Benchmark (surveying)",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Convergence (economics)",
      "Economic growth",
      "Economics",
      "Gene",
      "Geodesy",
      "Geography",
      "Mathematical optimization",
      "Mathematics",
      "Meta-optimization",
      "Metaheuristic",
      "Multi-swarm optimization",
      "Optimization problem",
      "Particle swarm optimization",
      "Premature convergence",
      "Robustness (evolution)",
      "Swarm behaviour"
    ],
    "authors": [
      {
        "surname": "Ang",
        "given_name": "Koon Meng"
      },
      {
        "surname": "Lim",
        "given_name": "Wei Hong"
      },
      {
        "surname": "Isa",
        "given_name": "Nor Ashidi Mat"
      },
      {
        "surname": "Tiang",
        "given_name": "Sew Sun"
      },
      {
        "surname": "Wong",
        "given_name": "Chin Hong"
      }
    ]
  },
  {
    "title": "FPO tree and DP3 algorithm for distributed parallel Frequent Itemsets Mining",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112874",
    "abstract": "Frequent Itemsets Mining is a fundamental mining model in Data Mining. It supports a vast range of application fields and can be employed as a key calculation phase in many other mining models such as Association Rules, Correlations, Classifications, etc. Many distributed parallel algorithms have been introduced to confront with very large-scale datasets of Big Data. However, the problems of running time and memory scalability still have not had adequate solutions for very large and “hard-to-mined” datasets. In this paper, we propose a distributed parallel algorithm named DP3 (Distributed PrePostPlus) which parallelizes the state-of-the-art algorithm PrePost+ and operates in Master-Slaves model. Slave machines mine and send local frequent itemsets and support counts to the Master for aggregations. In the case of tremendous numbers of itemsets transferred between the Slaves and Master, the computational load at the Master, therefore, is extremely heavy if there is not the support from our complete FPO tree (Frequent Patterns Organization) which can provide optimal compactness for light data transfers and highly efficient aggregations with pruning ability. Processing phases of the Slaves and Master are designed for memory scalability and shared-memory parallel in Work-Pool model so as to utilize the computational power of multi-core CPUs. We conducted experiments on both synthetic and real datasets, and the empirical results have shown that our algorithm far outperforms the well-known PFP and other three recently high-performance ones Dist-Eclat, BigFIM, and MapFIM.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419305846",
    "keywords": [
      "Agronomy",
      "Algorithm",
      "Association rule learning",
      "Biology",
      "Computer science",
      "Data mining",
      "Database",
      "Distributed memory",
      "Mathematical analysis",
      "Mathematics",
      "Parallel algorithm",
      "Parallel computing",
      "Physics",
      "Pruning",
      "Quantum mechanics",
      "Scalability",
      "Scale (ratio)",
      "Shared memory",
      "Theoretical computer science",
      "Tree (set theory)"
    ],
    "authors": [
      {
        "surname": "Huynh",
        "given_name": "Van Quoc Phuong"
      },
      {
        "surname": "Küng",
        "given_name": "Josef"
      }
    ]
  },
  {
    "title": "Complex Network based Supervised Keyword Extractor",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112876",
    "abstract": "In this paper, we present a supervised framework for automatic keyword extraction from single document. We model the text as complex network, and construct the feature set by extracting select node properties from it. Several node properties have been exploited by unsupervised, graph-based keyword extraction methods to discriminate keywords from non-keywords. We exploit the complex interplay of node properties to design a supervised keyword extraction method. The training set is created from the feature set by assigning a label to each candidate keyword depending on whether the candidate is listed as a gold-standard keyword or not. Since the number of keywords in a document is much less than non-keywords, the curated training set is naturally imbalanced. We train a binary classifier to predict keywords after balancing the training set. The model is trained using two public datasets from scientific domain and tested using three unseen scientific corpora and one news corpus. Comparative study of the results with several recent keyword and keyphrase extraction methods establishes that the proposed method performs better in most cases. This substantiates our claim that graph-theoretic properties of words are effective discriminators between keywords and non-keywords. We support our argument by showing that the improved performance of the proposed method is statistically significant for all datasets. We also evaluate the effectiveness of the pre-trained model on Hindi and Assamese language documents. We observe that the model performs equally well for the cross-language text even though it was trained only on English language documents. This shows that the proposed method is independent of the domain, collection, and language of the training corpora.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741741930586X",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Graph",
      "Information retrieval",
      "Keyword extraction",
      "Natural language processing",
      "Programming language",
      "Set (abstract data type)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Duari",
        "given_name": "Swagata"
      },
      {
        "surname": "Bhatnagar",
        "given_name": "Vasudha"
      }
    ]
  },
  {
    "title": "Rotated object detection with forward-looking sonar in underwater applications",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112870",
    "abstract": "Autonomous underwater vehicles (AUVs) are often used to inspect the condition of submerged structures in oil and gas fields. Because the use of global positioning systems to aid AUV navigation is not feasible, object detection is an alternative method of supporting underwater inspection missions by detecting landmarks. Objects are detected not only to plan the trajectory of the AUVs, but their inspection can be the ultimate goal of the mission. In both cases, detecting an object’s distance and orientation with respect to the AUV provides clues for the vehicle’s navigation. Accordingly, we introduce a novel multi-object detection system that outputs object position and rotation from sonar images to support AUV navigation. To achieve this aim, two novel convolutional neural network-based architectures are proposed to detect and estimate rotated bounding boxes: an end-to-end network (RBoxNet), and a pipeline comprised of two networks (YOLOv2+RBoxDNet). Both proposed networks are structured from one of three novel representations of rotated bounding boxes regressed deep inside. Experimental analyses were performed by comparing several configurations of our proposed methods (by varying the backbone, regression representation, and architecture) with state-of-the-art methods using real sonar images. Results showed that RBoxNet presents the optimum trade-off between accuracy and speed, reaching an averaged mAP@[.5,.95] of 90.3% at 8.58 frames per second (FPS), while YOLOv2+RBoxDNet is the fastest solution, running at 16.19 FPS but with a lower averaged mAP@[.5,.95] of 77.5%. Both proposed methods are robust to additive Gaussian noise variations, and can detect objects even when the noise level is up to 0.10.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419305809",
    "keywords": [
      "Artificial intelligence",
      "Astronomy",
      "Bounding overwatch",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Geology",
      "Geometry",
      "Mathematics",
      "Object (grammar)",
      "Object detection",
      "Oceanography",
      "Orientation (vector space)",
      "Pattern recognition (psychology)",
      "Physics",
      "Pipeline (software)",
      "Programming language",
      "Real-time computing",
      "Sonar",
      "Trajectory",
      "Underwater"
    ],
    "authors": [
      {
        "surname": "Neves",
        "given_name": "Gustavo"
      },
      {
        "surname": "Ruiz",
        "given_name": "Marco"
      },
      {
        "surname": "Fontinele",
        "given_name": "Jefferson"
      },
      {
        "surname": "Oliveira",
        "given_name": "Luciano"
      }
    ]
  },
  {
    "title": "A compositional model of multi-faceted trust for personalized item recommendation",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112880",
    "abstract": "Trust-based recommender systems improve rating prediction with respect to Collaborative Filtering by leveraging the additional information provided by a trust network among users to deal with the cold start problem. However, they are challenged by recent studies according to which people generally perceive the usage of data about social relations as a violation of their own privacy. In order to address this issue, we extend trust-based recommender systems with additional evidence about trust, based on public anonymous information, and we make them configurable with respect to the data that can be used in the given application domain: 1. We propose the Multi-faceted Trust Model (MTM) to define trust among users in a compositional way, possibly including or excluding the types of information it contains. MTM flexibly integrates social links with public anonymous feedback received by user profiles and user contributions in social networks. 2. We propose LOCABAL+, based on MTM, which extends the LOCABAL trust-based recommender system with multi-faceted trust and trust-based social regularization. Experiments carried out on two public datasets of item reviews show that, with a minor loss of user coverage, LOCABAL+ outperforms state-of-the art trust-based recommender systems and Collaborative Filtering in accuracy, ranking of items and error minimization both when it uses complete information about trust and when it ignores social relations. The combination of MTM with LOCABAL+ thus represents a promising alternative to state-of-the-art trust-based recommender systems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419305901",
    "keywords": [
      "Aerospace engineering",
      "Cold start (automotive)",
      "Collaborative filtering",
      "Computer science",
      "Engineering",
      "Information retrieval",
      "Ranking (information retrieval)",
      "Recommender system",
      "Social capital",
      "Social media",
      "Social network (sociolinguistics)",
      "Social science",
      "Social trust",
      "Sociology",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Ardissono",
        "given_name": "Liliana"
      },
      {
        "surname": "Mauro",
        "given_name": "Noemi"
      }
    ]
  },
  {
    "title": "Early author profiling on Twitter using profile features with multi-resolution",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112909",
    "abstract": "The Author Profiling (AP) task aims to predict demographic characteristics about the authors from documents (e.g., age, gender, native language). The research so far has focused only on forensic scenarios by performing post-analysis using all the available text evidence. This paper introduces the task of Early Author Profiling (EAP) in Twitter. The goal is to effectively recognize profiles using as few tweets as possible from the user history. The task is highly relevant to support social media analysis and different problems related to security and marketing, where prevention and anticipation is crucial. This work proposes a novel strategy that combines a state of the art representation for early text classification and specialized word-vectors for author profiling tasks. In this strategy we build prototypical features called Profile based Meta-Words, which allow us to model AP information at different levels of granularity. Our evaluation shows that the proposed methodology is well suited for profiling little text evidence (e.g., a handful of tweets) in early stages, but as more tweets become available other granularities better encode larger amounts of text in late stages. We evaluated the proposed ideas on gender and language variety identification for English and Spanish, and showed that the proposal outperforms state of the art methodologies.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741741930627X",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Data science",
      "ENCODE",
      "Gene",
      "Information retrieval",
      "Natural language processing",
      "Operating system",
      "Profiling (computer programming)",
      "Social media",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "López-Monroy",
        "given_name": "A. Pastor"
      },
      {
        "surname": "González",
        "given_name": "Fabio A."
      },
      {
        "surname": "Solorio",
        "given_name": "Thamar"
      }
    ]
  },
  {
    "title": "Prediction of Alzheimer's disease based on deep neural network by integrating gene expression and DNA methylation dataset",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112873",
    "abstract": "Motivation The molecular mechanism of Alzheimer's disease (AD) has not been clearly revealed and there is no clinically reliable genetic risk factor. Therefore, diagnosis of AD has been mostly performed by analyzing brain images such as magnetic resonance imaging and neuropsychological tests. Identifying the molecular-level mechanism of AD has been lacking data owing to the difficulty of sampling in the posterior brains of normal and AD patients; however, recent studies have produced and analyzed large-scale omics data for brain areas such as prefrontal cortex. Therefore, it is necessary to develop AD diagnosis or prediction methods based on these data. Results This paper proposed a deep learning-based model that can predict AD using large-scale gene expression and DNA methylation data. The most challenging problem in constructing a model to diagnose AD based on the multi-omics dataset is how to integrate different omics data and how to deal with high-dimensional and low-sample-size data. To solve this problem, we proposed a novel but simple approach to reduce the number of features based on a differentially expressed gene and a differentially methylated position in the multi-omics dataset. Moreover, we developed a deep neural network-based prediction model that improves performance compared to that of conventional machine learning algorithms. The feature selection method and the prediction model presented in this paper outperformed conventional machine learning algorithms, which utilize typical dimension reduction methods. In addition, we demonstrated that integrating gene expression and DNA methylation data could improve the prediction accuracy. Availability https://github.com/ChihyunPark/DNN_for_ADprediction.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419305834",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Bioinformatics",
      "Biology",
      "Computational biology",
      "Computer science",
      "DNA methylation",
      "Data mining",
      "Deep learning",
      "Feature selection",
      "Gene",
      "Gene expression",
      "Machine learning",
      "Neuroimaging",
      "Neuroscience"
    ],
    "authors": [
      {
        "surname": "Park",
        "given_name": "Chihyun"
      },
      {
        "surname": "Ha",
        "given_name": "Jihwan"
      },
      {
        "surname": "Park",
        "given_name": "Sanghyun"
      }
    ]
  },
  {
    "title": "Unbalanced breast cancer data classification using novel fitness functions in genetic programming",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112866",
    "abstract": "Breast Cancer is a common disease and to prevent it, the disease must be identified at earlier stages. Available breast cancer datasets are unbalanced in nature, i.e. there are more instances of benign (non-cancerous) cases then malignant (cancerous) ones. Therefore, it is a challenging task for most machine learning (ML) models to classify between benign and malignant cases properly, even though they have high accuracy. Accuracy is not a good metric to assess the results of ML models on breast cancer dataset because of biased results. To address this issue, we use Genetic Programming (GP) and propose two fitness functions. First one is F2 score which focuses on learning more about the minority class, which contains more relevant information, the second one is a novel fitness function known as Distance score (D score) which learns about both the classes by giving them equal importance and being unbiased. The GP framework in which we implemented D score is named as D-score GP (DGP) and the framework implemented with F2 score is named as F2GP. The proposed F2GP achieved a maximum accuracy of 99.63%, 99.51% and 100% for 60-40, 70-30 partition schemes and 10 fold cross validation scheme respectively and DGP achieves a maximum accuracy of 99.63%, 98.5% and 100% in 60-40, 70-30 partition schemes and 10 fold cross validation scheme respectively. The proposed models also achieves a recall of 100% for all the test cases. This shows that using a new fitness function for unbalanced data classification improves the performance of a classifier.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419305767",
    "keywords": [
      "Artificial intelligence",
      "Breast cancer",
      "Cancer",
      "Combinatorics",
      "Computer science",
      "Cross-validation",
      "Economics",
      "F1 score",
      "Fitness function",
      "Genetic algorithm",
      "Genetic programming",
      "Internal medicine",
      "Machine learning",
      "Mathematics",
      "Medicine",
      "Metric (unit)",
      "Operations management",
      "Partition (number theory)",
      "Precision and recall",
      "Programming language",
      "Test data"
    ],
    "authors": [
      {
        "surname": "Devarriya",
        "given_name": "Divyaansh"
      },
      {
        "surname": "Gulati",
        "given_name": "Cairo"
      },
      {
        "surname": "Mansharamani",
        "given_name": "Vidhi"
      },
      {
        "surname": "Sakalle",
        "given_name": "Aditi"
      },
      {
        "surname": "Bhardwaj",
        "given_name": "Arpit"
      }
    ]
  },
  {
    "title": "Text-line extraction from handwritten document images using GAN",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112916",
    "abstract": "Text-line extraction (TLE) from unconstrained handwritten document images is still considered an open research problem. Literature survey reveals that use of various rule-based methods is commonplace in this regard. But these methods mostly fail when the document images have touching and/or multi-skewed text lines or overlapping words/characters and non-uniform inter-line space. To encounter this problem, in this paper, we have used a deep learning-based method. In doing so, we have, for the first time in the literature, applied Generative Adversarial Networks (GANs) where we have considered TLE as image-to-image translation task. We have used U-Net architecture for the Generator, and Patch GAN architecture for the discriminator with different combinations of loss functions namely GAN loss, L1 loss and L2 loss. Evaluation is done on two datasets: handwritten Chinese text dataset HIT-MW and ICDAR 2013 Handwritten Segmentation Contest dataset. After exhaustive experimentations, it has been observed that U-Net architecture with combination of the said three losses not only produces impressive results but also outperforms some state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419306347",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Detector",
      "Discriminator",
      "Economics",
      "Generator (circuit theory)",
      "Geometry",
      "Image (mathematics)",
      "Line (geometry)",
      "Management",
      "Mathematics",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Physics",
      "Power (physics)",
      "Quantum mechanics",
      "Segmentation",
      "Task (project management)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Kundu",
        "given_name": "Soumyadeep"
      },
      {
        "surname": "Paul",
        "given_name": "Sayantan"
      },
      {
        "surname": "Kumar Bera",
        "given_name": "Suman"
      },
      {
        "surname": "Abraham",
        "given_name": "Ajith"
      },
      {
        "surname": "Sarkar",
        "given_name": "Ram"
      }
    ]
  },
  {
    "title": "Outlier Robust Extreme Machine Learning for multi-target regression",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112877",
    "abstract": "The popularity of algorithms based on Extreme Learning Machine (ELM), which can be used to train Single Layer Feedforward Neural Networks (SLFN), has increased in the past years. They have been successfully applied to a wide range of classification and regression tasks. The most commonly used methods are the ones based on minimizing the ℓ2 norm of the error, which is not suitable to deal with outliers, essentially in regression tasks. The use of ℓ1 norm was proposed in Outlier Robust ELM (OR-ELM), which is defined to one-dimensional outputs. In this paper, we generalize OR-ELM to deal with multi-target regression problems, using the error ℓ2,1 norm and the Elastic Net theory, which can result in a more sparse network, resulting in our method, Generalized Outlier Robust ELM (GOR-ELM). We use Alternating Direction Method of Multipliers (ADMM) to solve the resulting optimization problem. An incremental version of GOR-ELM is also proposed. We chose 15 public real-world multi-target regression datasets to test our methods. Our conducted experiments show that they are statistically better than other ELM-based techniques, when considering data contaminated with outliers, and equivalent to them, otherwise.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419305871",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Composite material",
      "Computer science",
      "Elastic net regularization",
      "Extreme learning machine",
      "Feature selection",
      "Law",
      "Machine learning",
      "Materials science",
      "Mathematics",
      "Norm (philosophy)",
      "Outlier",
      "Pattern recognition (psychology)",
      "Political science",
      "Range (aeronautics)",
      "Regression",
      "Robust regression",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Silva",
        "given_name": "Bruno Légora Souza da"
      },
      {
        "surname": "Inaba",
        "given_name": "Fernando Kentaro"
      },
      {
        "surname": "Salles",
        "given_name": "Evandro Ottoni Teatini"
      },
      {
        "surname": "Ciarelli",
        "given_name": "Patrick Marques"
      }
    ]
  },
  {
    "title": "Continuous control with Stacked Deep Dynamic Recurrent Reinforcement Learning for portfolio optimization",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112891",
    "abstract": "Recurrent reinforcement learning (RRL) techniques have been used to optimize asset trading systems and have achieved outstanding results. However, the majority of the previous work has been dedicated to systems with discrete action spaces. To address the challenge of continuous action and multi-dimensional state spaces, we propose the so called Stacked Deep Dynamic Recurrent Reinforcement Learning (SDDRRL) architecture to construct a real-time optimal portfolio. The algorithm captures the up-to-date market conditions and rebalances the portfolio accordingly. Under this general vision, Sharpe ratio, which is one of the most widely accepted measures of risk-adjusted returns, has been used as a performance metric. Additionally, the performance of most machine learning algorithms highly depends on their hyperparameter settings. Therefore, we equipped SDDRRL with the ability to find the best possible architecture topology using an automated Gaussian Process ( GP ) with Expected Improvement ( EI ) as an acquisition function. This allows us to select the best architectures that maximizes the total return while respecting the cardinality constraints. Finally, our system was trained and tested in an online manner for 20 successive rounds with data for ten selected stocks from different sectors of the S&P 500 from January 1st, 2013 to July 31st, 2017. The experiments reveal that the proposed SDDRRL achieves superior performance compared to three benchmarks: the rolling horizon Mean-Variance Optimization (MVO) model, the rolling horizon risk parity model, and the uniform buy-and-hold (UBAH) index.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419306074",
    "keywords": [
      "Artificial intelligence",
      "Bayesian optimization",
      "Computer science",
      "Economics",
      "Financial economics",
      "Gaussian",
      "Gaussian process",
      "Hyperparameter",
      "Machine learning",
      "Management",
      "Mathematical optimization",
      "Mathematics",
      "Performance metric",
      "Physics",
      "Portfolio",
      "Portfolio optimization",
      "Quantum mechanics",
      "Reinforcement learning"
    ],
    "authors": [
      {
        "surname": "Aboussalah",
        "given_name": "Amine Mohamed"
      },
      {
        "surname": "Lee",
        "given_name": "Chi-Guhn"
      }
    ]
  },
  {
    "title": "Continuous control with Stacked Deep Dynamic Recurrent Reinforcement Learning for portfolio optimization",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112891",
    "abstract": "Recurrent reinforcement learning (RRL) techniques have been used to optimize asset trading systems and have achieved outstanding results. However, the majority of the previous work has been dedicated to systems with discrete action spaces. To address the challenge of continuous action and multi-dimensional state spaces, we propose the so called Stacked Deep Dynamic Recurrent Reinforcement Learning (SDDRRL) architecture to construct a real-time optimal portfolio. The algorithm captures the up-to-date market conditions and rebalances the portfolio accordingly. Under this general vision, Sharpe ratio, which is one of the most widely accepted measures of risk-adjusted returns, has been used as a performance metric. Additionally, the performance of most machine learning algorithms highly depends on their hyperparameter settings. Therefore, we equipped SDDRRL with the ability to find the best possible architecture topology using an automated Gaussian Process ( GP ) with Expected Improvement ( EI ) as an acquisition function. This allows us to select the best architectures that maximizes the total return while respecting the cardinality constraints. Finally, our system was trained and tested in an online manner for 20 successive rounds with data for ten selected stocks from different sectors of the S&P 500 from January 1st, 2013 to July 31st, 2017. The experiments reveal that the proposed SDDRRL achieves superior performance compared to three benchmarks: the rolling horizon Mean-Variance Optimization (MVO) model, the rolling horizon risk parity model, and the uniform buy-and-hold (UBAH) index.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419306074",
    "keywords": [
      "Artificial intelligence",
      "Bayesian optimization",
      "Computer science",
      "Economics",
      "Financial economics",
      "Gaussian",
      "Gaussian process",
      "Hyperparameter",
      "Machine learning",
      "Management",
      "Mathematical optimization",
      "Mathematics",
      "Performance metric",
      "Physics",
      "Portfolio",
      "Portfolio optimization",
      "Quantum mechanics",
      "Reinforcement learning"
    ],
    "authors": [
      {
        "surname": "Aboussalah",
        "given_name": "Amine Mohamed"
      },
      {
        "surname": "Lee",
        "given_name": "Chi-Guhn"
      }
    ]
  },
  {
    "title": "A hybrid regularization approach for random vector functional-link networks",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112912",
    "abstract": "Neural networks have been widely applied to expert and intelligent systems in the fields of business, fault diagnosis, and forecasting. Especially, random vector functional-link networks (RVFL), an important structure, have also been considerably studied and used in recent years. This paper addresses in the investigation of regularization model for RVFL, and proposes a hybrid regularization approach for RVFL by adding the ℓ2 and ℓ1 norm penalty terms simultaneously. A novel iterative learning algorithm is developed by using a fixed point contractive map. Further, some theoretical properties, including the convergence, sparsity, and stability of the proposed algorithm, are discussed and analyzed concretely under reasonable assumptions. The proposed method greatly improves the learner’s sparsity and stability, guaranteeing the feasibility and effectiveness of network training. Experimental results on some benchmarks as well as face recognition database collected from the expert and intelligent systems, particularly the use of the statistical analysis strategy, verify the effectiveness and superiority of the proposed method, i.e. this new algorithm systematically outperforms the original RVFL and its variants in terms of the accuracy, sparsity, and stability of the solution.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741741930630X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Convergence (economics)",
      "Economic growth",
      "Economics",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Regularization (linguistics)",
      "Stability (learning theory)"
    ],
    "authors": [
      {
        "surname": "Ye",
        "given_name": "Hailiang"
      },
      {
        "surname": "Cao",
        "given_name": "Feilong"
      },
      {
        "surname": "Wang",
        "given_name": "Dianhui"
      }
    ]
  },
  {
    "title": "Fuzzy TODIM method based on alpha-level sets",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112899",
    "abstract": "Fuzzy TODIM method has been widely and successfully used to solve different decision making problems and consider decision maker’s (DM’s) psychological behavior during the decision process under uncertain environment. In the existing fuzzy TODIM methods, the fuzzy preferences are defuzzified into crisp values by using a distance measure between fuzzy numbers or variables. However, such an operation may imply a significant loss of information. It is argued that if fuzzy information is defuzzified into crisp values at the very beginning, then the superiority of considering fuzzy information is discounted. Therefore, it seems better to keep as much information as possible during the decision process rather than oversimplifying the fuzzy information by crisp values. An effective and proper way for keeping as much information as possible dealing with fuzzy preferences during the decision process is the use of alpha level sets. Several fuzzy multi-criteria decision making (MCDM) methods based on alpha level sets have been proposed and used to handle fuzzy information successfully, however, they neglected DM’s psychological behavior that plays a critical role in the real world decision processes. Up to now, there is not a fuzzy MCDM method employing alpha level sets to deal with fuzzy information together with considering DM’s psychological behavior. Motivated by previous limitations, this study proposes a novel fuzzy TODIM method based on alpha level sets, which keeps the fuzzy information longer and considers DM’s psychological behavior in the decision process. In addition, different ways to select the best alternative are provided in the proposed method. Comparisons with several MCDM methods are presented to show the improvements both of dealing with alpha level sets when psychological behavior is considered and the sensitivity of using such behavior regarding those MCDM methods that do not consider it. Through the comparison analysis, the proposed method is significant superiority to the existing approaches, which not only improves the current studies, but also enriches the way of coping with fuzzy information in the extant fuzzy MCDM methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419306153",
    "keywords": [
      "Alpha (finance)",
      "Artificial intelligence",
      "Computer science",
      "Construct validity",
      "Fuzzy logic",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Psychometrics",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Liang"
      },
      {
        "surname": "Wang",
        "given_name": "Ying-Ming"
      },
      {
        "surname": "Martínez",
        "given_name": "Luis"
      }
    ]
  },
  {
    "title": "Fog computing architecture for personalized recommendation of banking products",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112900",
    "abstract": "In this article, a novel Fog Computing solution is proposed, developed in the area of fintech. It integrates predictive systems in the process of delivery of personalized customer services for the recommendation of the products of a banking entity. The motivation behind this research is to improve aspects of customer support services, especially, achieve greater security, increased transparency and agility of processes as well as reduce entity management costs. The presented architecture includes fog nodes where data are processed by light intelligent agents allowing for the implementation of contextual recommendation systems together with the configuration of a Case Based Reasoning in the Cloud layer to improve the efficiency of the whole system over the time. The recommendation system is the cornerstone of architecture operating on banking products, such as mortgages, loans, retirement plans, etc., and it is developed by a hybrid method of recommendation: collaborative filtering combined with content-based filtering. The article analyzes the presented architecture while performing a verification and simulation of the data in the context of commercial banking. For this purpose, it shows the use of the proposed system of recommendations that represent the different communication channels as well as the possible devices. The proposed architecture offers the opportunity to improve the customer service in the bank’s physical channels and at the same time generate technological support to improve the resolution capacity of office managers, allowing employees to adopt a more versatile and flexible role. It also allows the evolution of the banking services model in offices while the processes that support it to follow a one-stop shop approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419306189",
    "keywords": [
      "Architecture",
      "Art",
      "Biology",
      "Business",
      "Cloud computing",
      "Computer science",
      "Computer security",
      "Context (archaeology)",
      "Cornerstone",
      "Marketing",
      "Operating system",
      "Paleontology",
      "Process (computing)",
      "Process management",
      "Recommender system",
      "Service (business)",
      "Transparency (behavior)",
      "Visual arts",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Hernández-Nieves",
        "given_name": "Elena"
      },
      {
        "surname": "Hernández",
        "given_name": "Guillermo"
      },
      {
        "surname": "Gil-González",
        "given_name": "Ana-Belén"
      },
      {
        "surname": "Rodríguez-González",
        "given_name": "Sara"
      },
      {
        "surname": "Corchado",
        "given_name": "Juan M."
      }
    ]
  },
  {
    "title": "A genetic programming hyper-heuristic approach for the multi-skill resource constrained project scheduling problem",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112915",
    "abstract": "Multi-skill resource-constrained project scheduling problem (MS-RCPSP) is one of the most investigated problems in operations research and management science. In this paper, a genetic programming hyper-heuristic (GP-HH) algorithm is proposed to address the MS-RCPSP. Firstly, a single task sequence vector is used to encode solution, and a repair-based decoding scheme is proposed to generate feasible schedules. Secondly, ten simple heuristic rules are designed to construct a set of low-level heuristics. Thirdly, genetic programming is utilized as a high-level strategy which can manage the low-level heuristics on the heuristic domain flexibly. In addition, the design-of-experiment (DOE) method is employed to investigate the effect of parameters setting. Finally, the performance of GP-HH is evaluated on the intelligent multi-objective project scheduling environment (iMOPSE) benchmark dataset consisting of 36 instances. Computational comparisons between GP-HH and the state-of-the-art algorithms indicate the superiority of the proposed GP-HH in computing feasible solutions to the problem.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419306335",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "ENCODE",
      "Gene",
      "Genetic algorithm",
      "Genetic programming",
      "Geodesy",
      "Geography",
      "Heuristic",
      "Heuristics",
      "Hyper-heuristic",
      "Job shop scheduling",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Mobile robot",
      "Operating system",
      "Robot",
      "Robot learning",
      "Schedule",
      "Scheduling (production processes)"
    ],
    "authors": [
      {
        "surname": "Lin",
        "given_name": "Jian"
      },
      {
        "surname": "Zhu",
        "given_name": "Lei"
      },
      {
        "surname": "Gao",
        "given_name": "Kaizhou"
      }
    ]
  },
  {
    "title": "Influence maximization across heterogeneous interconnected networks based on deep learning",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112905",
    "abstract": "With the fast development of online social networks, a large number of their members are involved in more than one social network. Finding most influential users is one of the interesting social network analysis tasks. The influence maximization (IM) problem aims to select a minimum set of users who maximize the influence spread on the underlying network. Most of the previous researches only focus on a single social networks, whereas in real world, users join to multiple social networks. Thus, influence can spread through common users on multiple networks. Besides, the existing works including simulation based, proxy based and sketch based approaches suffer from different issues including scalability, efficiency and feasibility due to the nature of these approaches for exploring networks and computation of their influence diffusion. Moreover, in the previous algorithms, several heuristics are employed to capture network topology for IM. But, these methods have information loss during network exploration because of their pruning strategies. In this paper, a new research direction is presented for studying IM problem on interconnected networks. The proposed approach employs deep learning techniques to learn the feature vectors of network nodes while preserving both local and global structural information. To the best of our knowledge, network embedding has not yet been used to solve IM problem. Indeed, our algorithm leverages deep learning techniques for feature engineering to extract all the appropriate information related to IM problem for single and interconnected networks. Moreover, we prove that the proposed algorithm is monotone and submodular, thus, an optimal solution is guaranteed by the proposed approach. The experimental results on two interconnected networks including DBLP and Twitter-Foursquare illustrate the efficiency of the proposed algorithm in comparison to state of the art IM algorithms. We also conduct some experiments on NetHept dataset to evaluate the performance of the proposed approach on single networks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419306232",
    "keywords": [
      "Artificial intelligence",
      "Complex network",
      "Computer science",
      "Database",
      "Distributed computing",
      "Evolving networks",
      "Feature (linguistics)",
      "Heuristics",
      "Linguistics",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Maximization",
      "Operating system",
      "Philosophy",
      "Scalability",
      "Social media",
      "Social network (sociolinguistics)",
      "Submodular set function",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Keikha",
        "given_name": "Mohammad Mehdi"
      },
      {
        "surname": "Rahgozar",
        "given_name": "Maseud"
      },
      {
        "surname": "Asadpour",
        "given_name": "Masoud"
      },
      {
        "surname": "Abdollahi",
        "given_name": "Mohammad Faghih"
      }
    ]
  },
  {
    "title": "Land use discovery based on Volunteer Geographic Information classification",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112892",
    "abstract": "Nowadays, cities are dynamic ecosystems where urban changes occur at a very fast pace. Hence, social sensing has become a powerful tool to uncover the actual land-use of a metropolis. However, current solutions for land-use discovery based on user-generated data usually rely on an information retrieval mechanism applied on a textual corpus. This causes ad-hoc place labelling with limited semantic meaning. In this line, the present work introduces a novel data-driven methodology that extends existing solutions by means of a classifier based on a pre-defined hierarchy of land categories. Two types of social networks –text-based and venue-based platforms– are utilized to train the classifier, which is then applied to infer the use of the land based on text data in areas where venue data are not available. The approach has been evaluated by using large datasets comprising two large cities, showing an accuracy above 90% in predicting the land-use categories.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419306086",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Data science",
      "Geodesy",
      "Geography",
      "Information retrieval",
      "Machine learning",
      "Pace"
    ],
    "authors": [
      {
        "surname": "Terroso-Saenz",
        "given_name": "Fernando"
      },
      {
        "surname": "Muñoz",
        "given_name": "Andrés"
      }
    ]
  },
  {
    "title": "A comprehensive overview of relevant methods of image cosegmentation",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112901",
    "abstract": "Segmenting the foreground objects from an image is an essential low-level step for many expert and intelligent systems, and the success of this key process largely depends on the amount of available training data. However, the cost for obtaining annotations is still a bottleneck, requiring enormous human effort. Thus, in order to obtain higher segmentation accuracy without the need of annotations, computer vision researchers attempted to segment simultaneously the common regions from collections of images. Indeed, when images that share common foreground objects are available, we talk about cosegmentation. The task of cosegmentation can be considered as weakly supervised segmentation, seen that the input image set is sharing common foreground objects. In fact, several methods have been proposed while displaying different frameworks, models and strategies of work, for enhancing the cosegmentation accuracy. This work presents a comprehensive review of relevant cosegmentation methods, carried out extensively and in depth and completed by performance measurements. Indeed, the existing cosegmentation methods are roughly classified in this paper into the following categories: Markov random fields-based cosegmentation, Co-saliency-based cosegmentation, Image decomposition-based cosegmentation, Random Walker-based cosegmentation, Maps-based cosegmentation, Active contours-based cosegmentation, Clustering-based cosegmentation and Deep learning-based cosegmentation. For each category, we discuss the most relevant methods while presenting the proposed problem to deal, the degree of supervision, the adopted features, the number and the complexity of input images as well as the presented models and frameworks. Then, in order to exhibit and evaluate objectively and comprehensively the most relevant state-of-the-art methods, we provide a comparative study on various challenging datasets (iCoseg, MSRC, Internet, FlickrMFC and PASCAL-VOC datasets). The objective is to show the performances and the limitations of these methods, while using different metrics and providing the computation cost for many methods. In addition, we discuss various cosegmentation challenges, issues and some applications, what can be useful and helpful to understand the cosegmentation problem. Finally, we suggest some research directions for future research on image cosegmentation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419306190",
    "keywords": [
      "Artificial intelligence",
      "Bottleneck",
      "Computer science",
      "Conditional random field",
      "Embedded system",
      "Image (mathematics)",
      "Image segmentation",
      "Markov random field",
      "Operating system",
      "Pattern recognition (psychology)",
      "Process (computing)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Merdassi",
        "given_name": "Hager"
      },
      {
        "surname": "Barhoumi",
        "given_name": "Walid"
      },
      {
        "surname": "Zagrouba",
        "given_name": "Ezzeddine"
      }
    ]
  },
  {
    "title": "Sparse and collaborative representation based kernel pairwise linear regression for image set classification",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112886",
    "abstract": "As an important part of expert and intelligent systems, image set classification has been widely applied to many real-life scenarios including surveillance videos, multi-view camera networks and personal albums. Compared with single image based classification, it is more promising and therefore has attracted significant research attention in recent years. Traditional pairwise linear regression classification (PLRC) introduces the unrelated subspace to increase the discriminative information, and it shows a demonstrated better performance on image set classification. However, the unrelated subspace constructed by PLRC is not optimal and PLRC may fail for well classifying image sets that are not linear separable, or when the axes of linear regression of class-specific samples of different classes have an intersection. In this paper, two new unrelated subspace construction strategies are proposed based on sparse and collaborative representation, respectively. Then, based on them, a new image set classification framework, kernel pairwise linear regression classification (KPLRC) is developed. KPLRC is a nonlinear extension of PLRC and can overcome the drawback of PLRC. Specifically, KPLRC embeds the related and unrelated gallery sets and probe sets into the high-dimensional Hilbert space, and in the kernel space, the data become more linear separable. Extensive experiments on four well-known databases prove that the classification accuracies of KPLRC are better than that of PLRC and several state-of-the-art classifiers. These quantitative assessments reinforce the significance as well as the importance of embedding the proposed method in other intelligent systems application areas.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419305962",
    "keywords": [
      "Aerospace engineering",
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Contextual image classification",
      "Discriminative model",
      "Engineering",
      "Image (mathematics)",
      "Intersection (aeronautics)",
      "Kernel (algebra)",
      "Kernel method",
      "Linear discriminant analysis",
      "Machine learning",
      "Mathematics",
      "Pairwise comparison",
      "Pattern recognition (psychology)",
      "Subspace topology",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Gao",
        "given_name": "Xizhan"
      },
      {
        "surname": "Sun",
        "given_name": "Quansen"
      },
      {
        "surname": "Xu",
        "given_name": "Haitao"
      },
      {
        "surname": "Gao",
        "given_name": "Jianqiang"
      }
    ]
  },
  {
    "title": "Time-varying hierarchical chains of salps with random weight networks for feature selection",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112898",
    "abstract": "Feature selection (FS) is considered as one of the most common and challenging tasks in Machine Learning. FS can be considered as an optimization problem that requires an efficient optimization algorithm to find its optimal set of features. This paper proposes a wrapper FS method that combines a time-varying number of leaders and followers binary Salp Swarm Algorithm (called TVBSSA) with Random Weight Network (RWN). In this approach, the TVBSSA is used as a search strategy, while RWN is utilized as an induction algorithm. The objective function is formulated in a manner to aggregate three objectives: maximizing the classification accuracy, maximizing the reduction rate of the selected features, and minimizing the complexity of generated RWN models. To assess the performance of the proposed approach, 20 well-known UCI datasets and a number of existing FS methods are employed. The comparative results show the ability of the proposed approach in outperforming similar algorithms in the literature and its merits to be used in systems that require FS.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419306141",
    "keywords": [],
    "authors": [
      {
        "surname": "Faris",
        "given_name": "Hossam"
      },
      {
        "surname": "Heidari",
        "given_name": "Ali Asghar"
      },
      {
        "surname": "Al-Zoubi",
        "given_name": "Ala’ M."
      },
      {
        "surname": "Mafarja",
        "given_name": "Majdi"
      },
      {
        "surname": "Aljarah",
        "given_name": "Ibrahim"
      },
      {
        "surname": "Eshtay",
        "given_name": "Mohammed"
      },
      {
        "surname": "Mirjalili",
        "given_name": "Seyedali"
      }
    ]
  },
  {
    "title": "A neuro-inspired computational model for adaptive fault diagnosis",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112879",
    "abstract": "Fault diagnosis is a key process to ensure reliable and cost-effective performance of time-critical engineered systems. This article develops a data-driven computational model for adaptive fault diagnosis by drawing an analogy with the neurobiological process of conscious attention—a dynamic process that brings only the most novel 0.01% of the signals we receive with our five senses to our conscious experience. A model of conscious attention based on the theory of dynamic core hypothesis is first outlined, followed by a computational model that mimics key stages of the conscious attention process. Convolutional neural networks serve as a basis for modeling perceptual categorization and concept formation through automatic feature extraction, due to their analogy with the processes of neural group selection and reentry in the brain. Further, the process of incremental learning and its impact on signal novelty are modeled via transfer learning. The model is tested on the NASA C-MAPSS turbofan engine model, which indicated 95–99% fault diagnosis accuracy. This study aims at familiarizing the engineering community with the neurobiological process of conscious attention and its applications for adaptive process monitoring and improvement in engineered systems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419305895",
    "keywords": [
      "Analogy",
      "Artificial intelligence",
      "Computational model",
      "Computer science",
      "Computer security",
      "Convolutional neural network",
      "Fault (geology)",
      "Geology",
      "Key (lock)",
      "Linguistics",
      "Machine learning",
      "Novelty",
      "Novelty detection",
      "Operating system",
      "Philosophy",
      "Process (computing)",
      "Seismology",
      "Theology"
    ],
    "authors": [
      {
        "surname": "Moghaddam",
        "given_name": "Mohsen"
      },
      {
        "surname": "Chen",
        "given_name": "Qiliang"
      },
      {
        "surname": "Deshmukh",
        "given_name": "Abhijit V."
      }
    ]
  },
  {
    "title": "A divide-and-conquer approach to geometric sampling for active learning",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112907",
    "abstract": "Active learning (AL) repeatedly trains the classifier with the minimum labeling budget to improve the current classification model. The training process is usually supervised by an uncertainty evaluation strategy. However, the uncertainty evaluation always suffers from performance degeneration when the initial labeled set has insufficient labels. To completely eliminate the dependence on the uncertainty evaluation sampling in AL, this paper proposes a divide-and-conquer idea that directly transfers the AL sampling as the geometric sampling over the clusters. By dividing the points of the clusters into cluster boundary and core points, we theoretically discuss their margin distance and hypothesis relationship. With the advantages of cluster boundary points in the above two properties, we propose a Geometric Active Learning (GAL) algorithm by knight’s tour. Experimental studies of the two reported experimental tasks including cluster boundary detection and AL classification show that the proposed GAL method significantly outperforms the state-of-the-art baselines.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419306256",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Boundary (topology)",
      "Classifier (UML)",
      "Cluster (spacecraft)",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Divide and conquer algorithms",
      "Filter (signal processing)",
      "Machine learning",
      "Margin (machine learning)",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Programming language",
      "Sampling (signal processing)"
    ],
    "authors": [
      {
        "surname": "Cao",
        "given_name": "Xiaofeng"
      }
    ]
  },
  {
    "title": "Analysis of multimodal physiological signals within and between individuals to predict psychological challenge vs. threat",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112890",
    "abstract": "Challenge and threat characterize distinct patterns of physiological response to a motivated performance task where the response patterns vary as a function of an individual's evaluation of task demands relative to his/her available resources to cope with the demands. Challenge and threat responses during motivated performance have been used to understand psychological, behavioral, and biological phenomena across many motivated performance domains. In this study, we aimed to investigate individual and group-level variations in physiological responding across a series of motivated performance tasks that vary in difficulty. The proposed approach is motivated by documented individual differences in physiological responses observed in motivated performance tasks, such that we first focus on individual differences in physiological responses rather than group-level comparisons. Then, through our analysis of individuals we identify sub-groups (i.e., clusters) of individuals that share common physiological patterns across tasks of varying difficulty and we perform across-subject analysis within each cluster. This is distinct from existing studies which typically do not examine individual vs. subgroup-specific patterns of physiological activity. Such an approach enables us to identify patterns in physiological responses that can be used to predict self-reported judgments of challenge vs. threat with higher accuracy in each subgroup compared to an analysis that includes the entire sample population as a single group. Specifically, three hypotheses were tested: (H1) individuals will have different sets of physiological patterns (features) across tasks of varying difficulty; (H2) there will be subgroups of individuals who share common salient physiological features across the subgroup clusters that differentiate their physiological responding across tasks of varying difficulty; and (H3) the accuracy of predicting self-reported judgments of challenge vs. threat across individuals will be higher within each subgroup with shared salient physiological features than across all subgroups or the entire sample with all computed features. To test these hypotheses, we developed an integrated analytic framework for multimodal physiological data analysis. We employed data from an existing experiment in which participants completed three mental arithmetic tasks of increasing difficulty during which different modalities of physiological data were collected. Analyses revealed three subgroups of participants who shared common features that best differentiated their within-individual physiological response patterns across tasks. Support vector machine (SVM) classifiers were trained using both shared features within each group and all computed features to predict challenge vs. threat states. Results showed that, the within-group classification model using group common features achieved higher self-report prediction accuracy compared to an alternative model trained on data from all participants without feature selection.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419306001",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Cognitive psychology",
      "Computer science",
      "Economics",
      "Environmental health",
      "Evolutionary biology",
      "Function (biology)",
      "Management",
      "Medicine",
      "Population",
      "Psychology",
      "Salient",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Khalaf",
        "given_name": "Aya"
      },
      {
        "surname": "Nabian",
        "given_name": "Mohsen"
      },
      {
        "surname": "Fan",
        "given_name": "Miaolin"
      },
      {
        "surname": "Yin",
        "given_name": "Yu"
      },
      {
        "surname": "Wormwood",
        "given_name": "Jolie"
      },
      {
        "surname": "Siegel",
        "given_name": "Erika"
      },
      {
        "surname": "Quigley",
        "given_name": "Karen S."
      },
      {
        "surname": "Barrett",
        "given_name": "Lisa Feldman"
      },
      {
        "surname": "Akcakaya",
        "given_name": "Murat"
      },
      {
        "surname": "Chou",
        "given_name": "Chun-An"
      },
      {
        "surname": "Ostadabbas",
        "given_name": "Sarah"
      }
    ]
  },
  {
    "title": "Decision-making techniques in supplier selection: Recent accomplishments and what lies ahead",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112903",
    "abstract": "Supplier selection (SS) is considered a sophisticated, application-oriented, decision-making (DM) problem and has received considerable attention. In the past two decades, DM theories and techniques continue to be incorporated into and contribute to the development of SS applications. Maintaining the pace of the rapid transitions in this field, this paper systematically reviews the relevant articles published between 2013 and 2018. Articles that orient various DM techniques are selected and analyzed under a well-established framework. State-of-the-art developments in the adoption of DM techniques are summarized in a SS process. We pay particular attention to promising directions that can dominate future research in this field. This paper further extends the history of several interacting fields, including big data and economic theories, toward methodological rather than application dimensions. The potential of such fields for SS is discussed from an interdisciplinary perspective.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419306219",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data science",
      "Economics",
      "Engineering",
      "Field (mathematics)",
      "Geodesy",
      "Geography",
      "Management science",
      "Mathematics",
      "Operating system",
      "Operations research",
      "Pace",
      "Perspective (graphical)",
      "Process (computing)",
      "Pure mathematics",
      "Selection (genetic algorithm)"
    ],
    "authors": [
      {
        "surname": "Chai",
        "given_name": "Junyi"
      },
      {
        "surname": "Ngai",
        "given_name": "Eric W.T."
      }
    ]
  },
  {
    "title": "Mitigating long tail effect in recommendations using few shot learning technique",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112887",
    "abstract": "Recommender system has been established as an effective tool for users in providing personalized suggestions in many domains, especially in e-commerce. In these domains, recommendations are provided based on the feedback (ratings) given by the users. However, recommendations provided by the traditional approaches are biased towards the popular items (items that receive more number of ratings). As a result, unpopular items are left out and these items remain un-recommended and unsold. These unpopular items form a “long tail” in the product space, resulting in a huge loss to the e-commerce industry. However, diminishing this long tail effect is a highly challenging and non-trivial task due to limited available rating information. Recommending long tail items helps in improving the item liquidation and recommendation diversity as well. In this paper, we propose a novel framework to mitigate the long tail effect and overcome the limited ratings problem using few shot learning techniques. Siamese network, a type of few shot learning technique is found to be performing well in many domains with a limited number of instances in the recent past. In the proposed framework, vital statistics of each user are computed and this information is provided to deep siamese network. The trained siamese network is used to identify the long tail items that are similar to the liked items of each user. Finally, the identified long tail items are recommended to the appropriate users. We introduce three novel performance metrics to evaluate the long tail item recommendations. The proposed framework is evaluated on two real world datasets (MovieLens 1M and Netflix) and the results demonstrate that the proposed framework outperforms the traditional approaches and existing long-tail recommendation techniques.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419305974",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Machine learning",
      "Materials science",
      "Metallurgy",
      "Shot (pellet)"
    ],
    "authors": [
      {
        "surname": "Sreepada",
        "given_name": "Rama Syamala"
      },
      {
        "surname": "Patra",
        "given_name": "Bidyut Kr."
      }
    ]
  },
  {
    "title": "Experimental analysis of multiple criteria for extractive multi-document text summarization",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112904",
    "abstract": "Automatic text summarization methods are increasingly needed in different fields of knowledge. In the scientific literature, generic extractive multi-document text summarization can be formulated as an optimization problem which involves several criteria. Only two criteria have been considered simultaneously, i.e., content coverage and redundancy reduction, whereas the other ones, relevance and coherence have been considered separately. Therefore, there is a lack of studies comparing the performance of different criteria. For this reason, a comparative study of the different criteria suitable for generic extractive multi-document text summarization is performed here. All possible combinations of two, three, and four criteria have been considered within a multi-objective optimization context. Experiments have been carried out based on datasets from Document Understanding Conferences (DUC), and the combinations of objective functions have been compared and evaluated with Recall-Oriented Understudy for Gisting Evaluation (ROUGE) metrics. Redundancy reduction has been demonstrated as an indispensable criterion, being the coherence the least significant and efficient criterion. The combination that includes content coverage, redundancy reduction, and relevance obtains the most balanced results in terms of average ROUGE and execution time.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419306220",
    "keywords": [
      "Artificial intelligence",
      "Automatic summarization",
      "Biology",
      "Coherence (philosophical gambling strategy)",
      "Computer science",
      "Context (archaeology)",
      "Data mining",
      "Information retrieval",
      "Law",
      "Mathematics",
      "Natural language processing",
      "Operating system",
      "Paleontology",
      "Political science",
      "Redundancy (engineering)",
      "Relevance (law)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Sanchez-Gomez",
        "given_name": "Jesus M."
      },
      {
        "surname": "Vega-Rodríguez",
        "given_name": "Miguel A."
      },
      {
        "surname": "Pérez",
        "given_name": "Carlos J."
      }
    ]
  },
  {
    "title": "A tabu search based algorithm for the optimal design of multi-objective multi-product supply chain networks",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.07.025",
    "abstract": "The optimal design of a supply chain network is a challenging problem, especially for large networks where there are multiple objectives. Such problems are usually formulated as mixed integer programs. Solving this type of network design problem takes a long time using exact algorithms and for large-scale problems it is not even possible. This has given rise to the use of meta-heuristic techniques. In this paper, an effective tabu search algorithm for solving multi-product, multi-objective, multi-stage supply chain design problems is proposed. The desirable characteristics of the algorithm are developed, coded and tested. The results of the developed algorithm are compared with the results obtained by an improved augmented ε-constraint algorithm embedded in the General Algebraic Modeling System (GAMS) software for small-scale, medium-scale, and large-scale instances of multi-objective supply chain problems. Experimental results have shown that the developed algorithm is capable of obtaining high quality solutions within a short computation time, in addition to performing well in other measures such as solution diversity.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419305044",
    "keywords": [
      "Algorithm",
      "Computation",
      "Computer science",
      "Constraint (computer-aided design)",
      "Geometry",
      "Heuristic",
      "Law",
      "Mathematical optimization",
      "Mathematics",
      "Physics",
      "Political science",
      "Product (mathematics)",
      "Quantum mechanics",
      "Scale (ratio)",
      "Supply chain",
      "Supply chain management",
      "Supply chain network",
      "Tabu search"
    ],
    "authors": [
      {
        "surname": "Mohammed",
        "given_name": "Awsan M."
      },
      {
        "surname": "Duffuaa",
        "given_name": "Salih O."
      }
    ]
  },
  {
    "title": "A hybrid Local Binary Pattern and wavelets based approach for EEG classification for diagnosing epilepsy",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112895",
    "abstract": "Epilepsy is one of the grave neurological ailments affecting approximately 70 million people globally. Detection of epileptic attack is commonly carried out by viewing and analysing long-duration multi-channel EEG records. To counter this time-consuming process, a hybrid Local Binary Pattern-Wavelet based approach, classifying EEG in epileptic patients, is adopted in this research. Epilepsy is characterized by multiple ictal patterns in the form of synchronous epileptiform discharge transients. This work attempts to classify seizure from normal EEG recordings using the low-frequency activity. In order to perform this classification, the EEG signal is filtered and then transformed using Local Binary Pattern (LBP) into a new signal. Discrete Wavelet Transform (DWT) is employed to decompose the obtained signal. Wavelet coefficients are calculated to 5 levels of decomposition. A combination of univariate and bivariate features forms the feature set for seizure detection. This feature set extracted from low-frequency band coefficients helps in bringing out the dispersion, symmetry, and peakedness present in the EEG signal. A novel LBP based Spatio-temporal analysis of the continuous EEG signal for epilepsy detection is carried out on 105 seizures from 14 randomly selected subjects of CHB-MIT EEG database. A sensitivity of 100% is achieved on the CHB-MIT database while long term EEG is being tested with Linear Discriminant Analysis (LDA) classifier. The algorithm works well to obtain a false detection rate (FP/Hour) of 0.59. The specificity of 99.8% is attained with a mean accuracy of 99.6% when tested on 498.9 h of EEG data.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419306116",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Discrete wavelet transform",
      "Electroencephalography",
      "Epilepsy",
      "Feature (linguistics)",
      "Histogram",
      "Ictal",
      "Image (mathematics)",
      "Linear discriminant analysis",
      "Linguistics",
      "Local binary patterns",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Psychiatry",
      "Psychology",
      "Speech recognition",
      "Wavelet",
      "Wavelet transform"
    ],
    "authors": [
      {
        "surname": "Khan",
        "given_name": "Kashif Ahmad"
      },
      {
        "surname": "P. P.",
        "given_name": "Shanir"
      },
      {
        "surname": "Khan",
        "given_name": "Yusuf Uzzaman"
      },
      {
        "surname": "Farooq",
        "given_name": "Omar"
      }
    ]
  },
  {
    "title": "An improved artificial neural network based on human-behaviour particle swarm optimization and cellular automata",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112862",
    "abstract": "Back-Propagation (BP) neural network, as a powerful and adaptive tool, has led to a tremendous surge in various expert systems. However, BP model has some deficiencies such as getting trapped in local minima and premature convergence. These weaknesses can be partly compensated by combining the ANN with Evolutionary Algorithms (EAs), i.e., at the same time, EAs also sufferred from their own characteristics, such as premature convergence in Particle Swarm Optimization (PSO). To gain a better trained weights in EAs-ANN, this paper proposes an improved ANN model based on HPSO and Cellular Automata (CA), which is called ANN-HPSO-CA. Firstly, to balance global exploration and local exploitation better and prevent particles from trapping in local optima, CA strategy is involved in HPSO algorithm, which is denoted as HPSO-CA. Then, the proposed HPSO-CA algorithm is combined with ANN to prevent ANN from trapping in local minima. Finally, to validate the performance of ANN-HPSO-CA, 15 benchmark complex and real-world datasets are used to compare with some well-known EA-based ANN models. Experimental results confirm that the proposed ANN-HPSO-CA algorithm outperforms the other predictive EA-based ANN models. The numerical comparison results will provide useful information and references for any future study for choosing proper EAs as ANN training algorithms. In addition, ANN-HPSO-CA algorithm provides a good theoretical basis for an expert system with good convergence and robustness.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741741930572X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Biochemistry",
      "Biology",
      "Cellular automaton",
      "Computer science",
      "Convergence (economics)",
      "Economic growth",
      "Economics",
      "Evolutionary algorithm",
      "Gene",
      "Geodesy",
      "Geography",
      "Local optimum",
      "Machine learning",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Maxima and minima",
      "Particle swarm optimization",
      "Premature convergence",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Yue"
      },
      {
        "surname": "Liu",
        "given_name": "Hao"
      },
      {
        "surname": "Yu",
        "given_name": "ZhongXin"
      },
      {
        "surname": "Tu",
        "given_name": "LiangPing"
      }
    ]
  },
  {
    "title": "A comparative study on network alignment techniques",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112883",
    "abstract": "Network alignment is a method to align nodes that belong to the same entity from different networks. A well-known application of network alignment is to map user accounts from different social networks that belong to the same person. As network alignment has a wide range of applications from recommendation to link prediction, there are several proposed approaches to aligning nodes from different networks. These techniques, however, have been rarely compared and analyzed under the same setting, rendering a right choice for a particular set of networks very difficult. Addressing this problem, this paper presents a benchmark that offers a comprehensive empirical study on the performance comparison of network alignment methods. Specifically, we integrate several state-of-the-art network alignment techniques in a comparable manner, and measure distinct characteristics of these techniques with various settings. We then provide in-depth analysis of the benchmark results, obtained by using both real data and synthetic data. We believe that the findings from the benchmark will serve as a practical guideline for potential applications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419305937",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Benchmarking",
      "Business",
      "Composite material",
      "Computer science",
      "Data mining",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Marketing",
      "Materials science",
      "Programming language",
      "Range (aeronautics)",
      "Rendering (computer graphics)",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Trung",
        "given_name": "Huynh Thanh"
      },
      {
        "surname": "Toan",
        "given_name": "Nguyen Thanh"
      },
      {
        "surname": "Vinh",
        "given_name": "Tong Van"
      },
      {
        "surname": "Dat",
        "given_name": "Hoang Thanh"
      },
      {
        "surname": "Thang",
        "given_name": "Duong Chi"
      },
      {
        "surname": "Hung",
        "given_name": "Nguyen Quoc Viet"
      },
      {
        "surname": "Sattar",
        "given_name": "Abdul"
      }
    ]
  },
  {
    "title": "A bi-objective dynamic collaborative task assignment under uncertainty using modified MOEA/D with heuristic initialization",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112844",
    "abstract": "The collaborative task assignment involved in Command and Control Systems is a key problem to be solved. The existing researches have their limitations to the natures of dynamic, uncertainty, flexibility and cooperation in a defensive scenario. Aiming at these, we formulate a bi-objective multi-stage task assignment model. The cooperation between sensor platforms and weapon platforms is considered. Also a Soyster robust model is introduced to handle uncertainty in a real time assignment process. Multi-objective evolutionary algorithm based on decomposition (MOEA/D) is adopted for the purpose of command flexibility. Currently, research focusing on multi-objective heuristics is relatively lacking. In this paper, we present a novel constructive heuristic for initializing the population. It successively adds quaternions into the assignment scheme to construct a solution set along the Pareto front, which is an interesting heuristic framework for multi-objective problems. We have also modified MOEA/D with nadir-based Tchebycheff and utilized the proposed neighbor matching strategy to gain better performance. Since algorithms are sensitive to their parameters, the Taguchi method with a novel response metric is utilized to calibrate the parameters. Numerical experiments demonstrate the superiority of the proposed algorithm and the necessity of a robust model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419305469",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Engineering",
      "Evolutionary algorithm",
      "Flexibility (engineering)",
      "Heuristic",
      "Heuristics",
      "Initialization",
      "Machine learning",
      "Matching (statistics)",
      "Mathematical optimization",
      "Mathematics",
      "Metric (unit)",
      "Multi-objective optimization",
      "Operating system",
      "Operations management",
      "Process (computing)",
      "Programming language",
      "Set (abstract data type)",
      "Statistics",
      "Systems engineering",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Wenqin"
      },
      {
        "surname": "Chen",
        "given_name": "Chen"
      },
      {
        "surname": "Ding",
        "given_name": "Shuxin"
      },
      {
        "surname": "Pardalos",
        "given_name": "Panos M."
      }
    ]
  },
  {
    "title": "Intelligent firefly-based algorithm with Levy distribution (FF-L) for multicast routing in vehicular communications",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112889",
    "abstract": "Vehicular ad hoc network (VANET) is an interesting technology used to attain inter-vehicle communication in intelligent transportation system (ITS). In VANET, the timely and unfailing vehicular communication is an important challenge to achieved good service. So, efficient routing techniques are highly needed for proper data transmission in VANET. In this paper, we concentrate on the quality of service (QoS) constrained multicast routing problem by considering it as an NP-complete problem. Several studies reported that bio-inspired algorithms work well than the traditional algorithms to resolve the NP-complete problem. So, a firefly with Levy distribution (FF-L) algorithm is presented to manage the multicast routing problem. To prevent the FF algorithm stuck into local optima, Levy distribution is incorporated to the FF algorithm. The FF-L algorithm for multicasting routing in VANET is validated using a set of experiments on three different scenarios. The experimental values verified that the FF-L algorithm identifies the optimal routes and it can be implemented in practical scenarios where the network structure remains quite stable.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419305998",
    "keywords": [
      "Algorithm",
      "Computer network",
      "Computer science",
      "Distributed computing",
      "Firefly algorithm",
      "Multicast",
      "Particle swarm optimization",
      "Quality of service",
      "Routing (electronic design automation)",
      "Telecommunications",
      "Transmission (telecommunications)",
      "Vehicular ad hoc network",
      "Wireless",
      "Wireless ad hoc network"
    ],
    "authors": [
      {
        "surname": "Elhoseny",
        "given_name": "Mohamed"
      }
    ]
  },
  {
    "title": "Type-1 recurrent intuitionistic fuzzy functions for forecasting",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112913",
    "abstract": "In this study, a novel forecasting method that employs intuitionistic fuzzy c-means clustering and a grey wolf optimizer in recurrent type-1 fuzzy functions (R-T1FFs) is introduced. R-T1FFs, which adapt to the moving average (MA) along with an autoregressive model (AR), were introduced recently to improve the forecasting performances of type-1 fuzzy functions, which adapted solely to the AR model. Because the objective function of R-T1FFs was non-derivative, particle swarm optimization was used to estimate the coefficients of the model. Because R-T1FFs calculate the disturbance terms for the MA model recursively, the model is computationally intensive. Therefore, the proposed method employs a grey wolf optimizer for training processes. Examples in the literature demonstrate that the grey wolf optimizer performs well when obtaining model coefficients because convergence is faster owing to a continuous reduction in the search space and reduced storage requirements. Another contribution of the proposed method is an improved forecasting accuracy, which is obtained by quantifying the hesitancy with which an observation belongs to a cluster given a certain degree of membership. A comparison of the proposed method with numerous existing methods is conducted for 12 practical time series datasets as applications. The accuracy of the proposed method is investigated using root-mean-squared error, mean absolute percentage error, and correlation coefficients; further, whether it produces significantly better outcomes than the existing methods is verified using paired t-tests. For example, it is clear by looking at the average RMSEs of the methods that the best forecasting performance is produced by the proposed method for Taiwan stock exchange datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419306311",
    "keywords": [
      "Algorithm",
      "Applied mathematics",
      "Artificial intelligence",
      "Autoregressive model",
      "Biology",
      "Cluster analysis",
      "Computer science",
      "Convergence (economics)",
      "Data mining",
      "Ecology",
      "Economic growth",
      "Economics",
      "Fuzzy logic",
      "Mathematical optimization",
      "Mathematics",
      "Mean squared error",
      "Paleontology",
      "Particle swarm optimization",
      "Series (stratigraphy)",
      "Statistics",
      "Type (biology)"
    ],
    "authors": [
      {
        "surname": "Tak",
        "given_name": "Nihat"
      }
    ]
  },
  {
    "title": "An artificial immune systems approach to Case-based Reasoning applied to fault detection and diagnosis",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112906",
    "abstract": "This work presents a hybrid model of Case-based Reasoning (CBR) and artificial immune systems (AIS), which is able to manage the processes of recovery, adaptation (reuse and revision) and retention of cases. The developed model also provides an alternative way of clustering cases, identifying high density areas, improve search efficiency in the case space and store relationships among similar cases. The proposed model is applied to a fault detection and diagnosis problem of direct currnet motor nenchmark and the obtained results are compared using specific CBR performance metrics showing promising perspectives for the proposed model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419306244",
    "keywords": [
      "Actuator",
      "Adaptation (eye)",
      "Artificial immune system",
      "Artificial intelligence",
      "Case-based reasoning",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Engineering",
      "Fault (geology)",
      "Fault detection and isolation",
      "Geology",
      "Machine learning",
      "Optics",
      "Physics",
      "Reuse",
      "Seismology",
      "Waste management"
    ],
    "authors": [
      {
        "surname": "Costa Silva",
        "given_name": "Guilherme"
      },
      {
        "surname": "Carvalho",
        "given_name": "Eduardo E.O."
      },
      {
        "surname": "Caminhas",
        "given_name": "Walmir Matos"
      }
    ]
  },
  {
    "title": "Structured learning for unsupervised feature selection with high-order matrix factorization",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112878",
    "abstract": "Feature selection aims at searching the most discriminative and relevant features from high-dimensional data to improve the performance of certain learning tasks. Whereas, irrelevant or redundant features may increase the over-fitting risk of consequent learning algorithms. Structured learning of feature selection is to embed intrinsic structures of data, such as geometric structures and manifold structures, resulting in the improvement of learning performance. In this paper, three types of structured regularizers are embedded into the feature selection framework and an iterative algorithm with proved convergence for feature selection problem is proposed. First, serving as crucial representation pipelines of local structures, three types of local learning regularizers, including graph Laplacian, neighborhood preservation and sparsity regularizer, are defined. Second, the local and global structures are integrated into one joint framework for the feature selection problem. Third, the framework is formulated as the canonical form of high-order matrix factorizations and then an efficient convergent iterative algorithm is proposed for the problem. Besides, the proposed framework is further extended to multi-view feature selection and fusion problems from an algorithmic view. Finally, the proposed algorithm is tested on eight publicly available datasets and compared to several state-of-the-art feature selection methods. Experimental results demonstrate the superiority of the proposed method against the compared algorithms in terms of clustering performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419305883",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Discriminative model",
      "Feature (linguistics)",
      "Feature learning",
      "Feature selection",
      "Graph",
      "Laplacian matrix",
      "Linguistics",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Shiping"
      },
      {
        "surname": "Chen",
        "given_name": "Jiawei"
      },
      {
        "surname": "Guo",
        "given_name": "Wenzhong"
      },
      {
        "surname": "Liu",
        "given_name": "Genggeng"
      }
    ]
  },
  {
    "title": "MultiD-CNN: A multi-dimensional feature learning approach based on deep convolutional networks for gesture recognition in RGB-D image sequences",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112829",
    "abstract": "Human gesture recognition has become a pillar of today’s intelligent Human-Computer Interfaces as it typically provides more comfortable and ubiquitous interaction. Such expert system has a promising prospect in various applications, including smart houses, gaming, healthcare, and robotics. However, recognizing human gestures in videos is one of the most challenging topics in computer vision, because of some irrelevant environmental factors like complex background, occlusion, lighting conditions, and so on. With the recent development of deep learning, many researchers have addressed this problem by building single deep networks to learn spatiotemporal features from video data. However, the performance is still unsatisfactory due to the limitation that the single deep networks are incapable of handling these challenges simultaneously. Hence, the extracted features cannot efficiently capture both relevant shape information and detailed spatiotemporal variation of the gestures. One solution to overcome the aforementioned drawbacks is to fuse multiple features from different models learned on multiple vision cues. Aiming at this objective, we present in this paper an effective multi-dimensional feature learning approach, termed as MultiD-CNN, for human gesture recognition in RGB-D videos. The key to our design is to learn high-level gesture representations by taking advantages from Convolutional Residual Networks (ResNets) for training extremely deep models and Convolutional Long Short-Term Memory Networks (ConvLSTM) for dealing with time-series connections. More specifically, we first construct an architecture to simultaneously learn the spatiotemporal features from RGB and depth sequences through 3D ResNets which are then linked to a ConvLSTM to capture the temporal dependencies between them, and we show that they better combine appearance and motion information effectively. Second, to alleviate distractions from background and other variations, we propose a method that encodes the temporal information into a motion representation, while a two-stream architecture based on 2D-ResNets is then employed to extract deep features from this representation. Third, we investigate different fusion strategies at different levels for blending the classification results, and we show that integrating multiple ways of encoding the spatial and temporal information leads to a robust and stable spatiotemporal feature learning with better generalization capability. Finally, we perform different experiments to evaluate the performance of the investigated architectures on four kinds of challenging datasets, demonstrating that our approach is particularly impressive where it outperforms prior arts in both accuracy and efficiency. The obtained results affirm also the importance of embedding the proposed approach in other intelligent systems application areas.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419305317",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Deep learning",
      "Feature (linguistics)",
      "Feature learning",
      "Gesture",
      "Gesture recognition",
      "Image (mathematics)",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "RGB color model"
    ],
    "authors": [
      {
        "surname": "Elboushaki",
        "given_name": "Abdessamad"
      },
      {
        "surname": "Hannane",
        "given_name": "Rachida"
      },
      {
        "surname": "Afdel",
        "given_name": "Karim"
      },
      {
        "surname": "Koutti",
        "given_name": "Lahcen"
      }
    ]
  },
  {
    "title": "Adaptive windows multiple deep residual networks for speech recognition",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112840",
    "abstract": "The hybrid convolutional neural network and hidden Markov model (CNN-HMM) has recently achieved considerable performance in speech recognition because deep neural networks, model complex correlations between features. Automatic speech recognition (ASR) as an input to many intelligent and expert systems has impacts in various fields such as evolving search engines (inclusion of speech recognition in search engines), healthcare industry (medical reporting by medical personnel, and disease diagnosis expert systems), service delivery, communication in service providers (to establish the callers demands and then direct them to the appropriate operator for assistance), etc. This paper introduces a method, which further reduces the recognition error rate. In this paper, we first propose adaptive windows convolutional neural network (AWCNN) to analyze joint temporal-spectral features variation. AWCNN makes the model more robust against both intra- and inter-speaker variations. We further propose a new residual learning, which leads to better utilization of information in deep layers and provides a better control on transferring input information. The proposed speech recognition system can be used as the vocal input for many artificial and expert systems. We evaluated the proposed method on TIMIT, FARSDAT, Switchboard, and CallHome datasets and one image database i.e. MNIST. The experimental results show that the proposed method reduces the absolute error rate by 7% compared with the state-of-the-art methods in some speech recognition tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419305421",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Hidden Markov model",
      "MNIST database",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Residual",
      "Speech recognition",
      "TIMIT",
      "Word error rate"
    ],
    "authors": [
      {
        "surname": "Zoughi",
        "given_name": "Toktam"
      },
      {
        "surname": "Homayounpour",
        "given_name": "Mohammad Mehdi"
      },
      {
        "surname": "Deypir",
        "given_name": "Mahmood"
      }
    ]
  },
  {
    "title": "An integrated feature learning approach using deep learning for travel time prediction",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112864",
    "abstract": "Travel time data is a vital factor for numbers of performance measures in transportation systems. Travel time prediction is both a challenging and interesting problem in ITS, because of the underlying traffic and events’ hidden patterns. In this study, we propose a multi-step deep-learning-based algorithm for predicting travel time. Our algorithm starts with data pre-processing. Then, the data is augmented by incorporating external datasets. Moreover, extensive feature learning and engineering such as spatiotemporal feature analysis, feature extraction, and clustering algorithms is applied to improve the feature space. Furthermore, for representing features we used a deep stacked autoencoder with dropout layer as regularizer. Finally, a deep multi-layer perceptron is trained to predict travel times. For testing our predictive accuracy, we used a 5-fold cross validation to test the generalization of our predictive model. As we observed, the performance of the proposed algorithm is on average 4 min better than applying the deep neural network to the initial feature space. Furthermore, we have noticed that representation learning using stacked autoencoders makes our learner robust to overfitting. Moreover, our algorithm is capable of capturing the general dynamics of the traffic, however further works need to be done for some rare events which impact travel time prediction significantly.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419305743",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Autoencoder",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Deep learning",
      "Dropout (neural networks)",
      "Feature (linguistics)",
      "Feature engineering",
      "Feature learning",
      "Generalization",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Overfitting",
      "Pattern recognition (psychology)",
      "Perceptron",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Abdollahi",
        "given_name": "Mohammad"
      },
      {
        "surname": "Khaleghi",
        "given_name": "Tannaz"
      },
      {
        "surname": "Yang",
        "given_name": "Kai"
      }
    ]
  },
  {
    "title": "Neighborhood information-based probabilistic algorithm for network disintegration",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112853",
    "abstract": "Many real-world applications can be modelled as complex networks, and such networks include the Internet, epidemic disease networks, transport networks, power grids, protein-folding structures and others. Network integrity and robustness are important to ensure that crucial networks are protected and undesired harmful networks can be dismantled. Network structure and integrity can be controlled by a set of key nodes, and to find the optimal combination of nodes in a network to ensure network structure and integrity can be an NP-complete problem. Despite extensive studies, existing methods have many limitations and there are still many unresolved problems. This paper presents a probabilistic approach based on neighborhood information and node importance, namely, neighborhood information-based probabilistic algorithm (NIPA). We also define a new centrality-based importance measure (IM), which combines the contribution ratios of the neighbor nodes of each target node and two-hop node information. Our proposed NIPA has been tested for different network benchmarks and compared with three other methods: optimal attack strategy (OAS), high betweenness first (HBF) and high degree first (HDF). Experiments suggest that the proposed NIPA is most effective among all four methods. In general, NIPA can identify the most crucial node combination with higher effectiveness, and the set of optimal key nodes found by our proposed NIPA is much smaller than that by heuristic centrality prediction. In addition, many previously neglected weakly connected nodes are identified, which become a crucial part of the newly identified optimal nodes. Thus, revised strategies for protection are recommended to ensure the safeguard of network integrity. Further key issues and future research topics are also discussed.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741741930555X",
    "keywords": [
      "Artificial intelligence",
      "Betweenness centrality",
      "Biochemistry",
      "Centrality",
      "Chemistry",
      "Combinatorics",
      "Computer science",
      "Computer security",
      "Data mining",
      "Engineering",
      "Gene",
      "Key (lock)",
      "Mathematics",
      "Node (physics)",
      "Probabilistic logic",
      "Robustness (evolution)",
      "Structural engineering"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Qian"
      },
      {
        "surname": "Liu",
        "given_name": "San-Yang"
      },
      {
        "surname": "Yang",
        "given_name": "Xin-She"
      }
    ]
  },
  {
    "title": "TAD: A trajectory clustering algorithm based on spatial-temporal density analysis",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112846",
    "abstract": "In this paper, a novel trajectory clustering algorithm - TAD - is proposed to extract trajectory Stays based on spatial-temporal density analysis of data. Two new metrics - NMAST (Neighbourhood Move Ability and Stay Time) density function and NT (Noise Tolerance) factor - are defined in this algorithm. Firstly, NMAST integrates the characteristics of Neighbourhood Move Ability (NMA, extended from the concept of Move Ability MA), Stay Time (ST), and evaluation factor Eμ to measure the spatial-temporal density of data. Secondly, NT utilizes the features of noise to dynamically evaluate and reduce the influence of noise. The experimental results on Geolife dataset shows that the distributions hidden in data are extracted more realistically, especially for various complex or special trajectories with long-duration gaps. Furthermore, our analytical method of trajectory data is particularly applied in the spectra of LAMOST survey to analyse the variation characteristics of sky-background. The results show a regular distribution on observational date which is relatively concentrated in the month of 1, 10, 11, 12 in each year. The laws discovered in this work would provide a reasonable support for the designation of observational plans, and the new trajectory analysis method would also provide the services for the astronomical data analysis and then for the further studies of formation and evolution of the universe.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419305482",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Astronomy",
      "Cluster analysis",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Image (mathematics)",
      "LAMOST",
      "Mathematical analysis",
      "Mathematics",
      "Neighbourhood (mathematics)",
      "Noise (video)",
      "Physics",
      "Stars",
      "Trajectory"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Yuqing"
      },
      {
        "surname": "Cai",
        "given_name": "Jianghui"
      },
      {
        "surname": "Yang",
        "given_name": "Haifeng"
      },
      {
        "surname": "Zhang",
        "given_name": "Jifu"
      },
      {
        "surname": "Zhao",
        "given_name": "Xujun"
      }
    ]
  },
  {
    "title": "Hybrid advanced player selection strategy based population search for global optimization",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112825",
    "abstract": "Motivation: Finding the optimum solution of real world complex optimization problem needs efficient solver system. Meta-heuristic like swarm, evolutionary or physics based methods essentially form an expert system which may strive for the search of optimum solution. However, these methods have their own merits and demerits. To supersede the performance of existing systems in context of real time and advanced problems and to propose new intelligent method having an optimal balance between exploration and exploitation phase have become a center of focus of the expert system design. Therefore, being inspired from such a scenario, a hybrid advanced player selection strategy based population search (HAPS-PS) technique is proposed and evaluated in this paper in context of modern applications. Methods: Sport is a multi-functional activity where different learning methods like cooperative, competitive and self-learning variants along with interactive environmental features help to improve the performance of the players. The properties of playing activity and the thematic learning process of sports, along with some features like, chaotic map based initialization, pool based topological structure selection, integration of blended laplacian operator etc. contributes in the design of HAPS-PS. Also a novel mechanism of team reformation through elimination of comparatively poor players and recruitment of new talents is employed. It provides diversification and regulates the search agents in a promising direction. Results: We have comprehensively evaluated the performance of the HAPS-PS by applying it on standard benchmark problems and on various shifted rotated, hybrid and composite problems of CEC2013 and CEC2014. Also some of the competitive problems of CEC 2017 are tested using the proposed method and they are also compared with other state-of-the-art algorithm to validate the efficacy of the system. Further, tests are carried out on some real-world problems from CEC2011 which includes applications related to parameter optimization of frequency-modulated (FM) sound wave, minimization of molecular potential energy of Lennard-Jones cluster, controlling the chemical reactors, and evaluation of the inter atomic tersoff potentials for covalent systems of silicon. The proposed method is also applied on spread spectrum radar polyphase code design problem as a part of real world application. The comparative results validate the effectiveness and efficiency of HAPS-PS.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419305275",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Demography",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Population",
      "Selection (genetic algorithm)",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Agarwalla",
        "given_name": "Prativa"
      },
      {
        "surname": "Mukhopadhyay",
        "given_name": "Sumitra"
      }
    ]
  },
  {
    "title": "A review on video-based active and assisted living technologies for automated lifelogging",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112847",
    "abstract": "Providing support for ageing and frail populations to extend their personal autonomy is desirable for their well-being as it is for the society at large, since it can ease the economic and social challenges caused by ever-ageing developed societies. Ambient-assisted living (AAL) technologies and services might be a solution to address those challenges. Recent improved capabilities in both ambient and wearable technologies, especially those related with video and lifelogging data, and huge advances in the accuracy of intelligent systems for AAL are leading to more valuable and trustworthy services for older people and their caregivers. These advances have been particularly relevant in the last years due to the appearance of RGB-D devices and the development of deep learning systems. This article reviews these latest developments in the intersection of AAL, intelligent systems, lifelogging, and computer vision. This paper provides a study of previous reviews in these fields, and later analyses newer intelligent techniques employed with different video-based lifelogging technologies in order to offer lifelogging services for AAL. Additionally, privacy and ethical issues associated with these technologies are discussed. This review aims at facilitating the understanding of the multiple fields involved.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419305494",
    "keywords": [
      "Assisted living",
      "Autonomy",
      "Computer science",
      "Data science",
      "Embedded system",
      "Gerontology",
      "Human–computer interaction",
      "Internet privacy",
      "Law",
      "Lifelog",
      "Medicine",
      "Political science",
      "Wearable computer"
    ],
    "authors": [
      {
        "surname": "Climent-Pérez",
        "given_name": "Pau"
      },
      {
        "surname": "Spinsante",
        "given_name": "Susanna"
      },
      {
        "surname": "Mihailidis",
        "given_name": "Alex"
      },
      {
        "surname": "Florez-Revuelta",
        "given_name": "Francisco"
      }
    ]
  },
  {
    "title": "Breast tumor segmentation and shape classification in mammograms using generative adversarial and convolutional neural network",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112855",
    "abstract": "Mammogram inspection in search of breast tumors is a tough assignment that radiologists must carry out frequently. Therefore, image analysis methods are needed for the detection and delineation of breast tumors, which portray crucial morphological information that will support reliable diagnosis. In this paper, we proposed a conditional Generative Adversarial Network (cGAN) devised to segment a breast tumor within a region of interest (ROI) in a mammogram. The generative network learns to recognize the tumor area and to create the binary mask that outlines it. In turn, the adversarial network learns to distinguish between real (ground truth) and synthetic segmentations, thus enforcing the generative network to create binary masks as realistic as possible. The cGAN works well even when the number of training samples are limited. As a consequence, the proposed method outperforms several state-of-the-art approaches. Our working hypothesis is corroborated by diverse segmentation experiments performed on INbreast and a private in-house dataset. The proposed segmentation model, working on an image crop containing the tumor as well as a significant surrounding area of healthy tissue (loose frame ROI), provides a high Dice coefficient and Intersection over Union (IoU) of 94% and 87%, respectively. In addition, a shape descriptor based on a Convolutional Neural Network (CNN) is proposed to classify the generated masks into four tumor shapes: irregular, lobular, oval and round. The proposed shape descriptor was trained on DDSM, since it provides shape ground truth (while the other two datasets does not), yielding an overall accuracy of 80%, which outperforms the current state-of-the-art.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419305573",
    "keywords": [
      "Aerospace engineering",
      "Arithmetic",
      "Artificial intelligence",
      "Artificial neural network",
      "Binary classification",
      "Binary number",
      "Breast cancer",
      "Cancer",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Engineering",
      "Generative adversarial network",
      "Generative grammar",
      "Ground truth",
      "Image segmentation",
      "Internal medicine",
      "Intersection (aeronautics)",
      "Machine learning",
      "Mammography",
      "Mathematics",
      "Medicine",
      "Pattern recognition (psychology)",
      "Segmentation",
      "Support vector machine",
      "Sørensen–Dice coefficient"
    ],
    "authors": [
      {
        "surname": "Singh",
        "given_name": "Vivek Kumar"
      },
      {
        "surname": "Rashwan",
        "given_name": "Hatem A."
      },
      {
        "surname": "Romani",
        "given_name": "Santiago"
      },
      {
        "surname": "Akram",
        "given_name": "Farhan"
      },
      {
        "surname": "Pandey",
        "given_name": "Nidhi"
      },
      {
        "surname": "Sarker",
        "given_name": "Md. Mostafa Kamal"
      },
      {
        "surname": "Saleh",
        "given_name": "Adel"
      },
      {
        "surname": "Arenas",
        "given_name": "Meritxell"
      },
      {
        "surname": "Arquez",
        "given_name": "Miguel"
      },
      {
        "surname": "Puig",
        "given_name": "Domenec"
      },
      {
        "surname": "Torrents-Barrena",
        "given_name": "Jordina"
      }
    ]
  },
  {
    "title": "A personal data store approach for recommender systems: enhancing privacy without sacrificing accuracy",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112858",
    "abstract": "Recommender systems have become extremely common in recent years, and are applied in a variety of domains. Existing recommender systems exhibit two major limitations: (1) Privacy - each service provider holds a database that contains information about all of its users; and (2) Partial view - when recommending to users, each such service can rely only on data that were collected by the service itself. The Open Personal Data Store (openPDS) architecture was recently suggested for storing personal data in a privacy preserving way. Inspired by openPDS, we suggest a novel architecture for recommender systems that overcomes the two limitations mentioned above. The suggested architecture allows the recommender system to utilize rich data collected about the user (possibly through other services) to produce more accurate recommendations, while allowing its users to manage and gain control over their own data. We evaluate the suggested architecture on two different use cases: movies and web browsing, and compare its performance with that of a popular non-privacy-aware collaborative-filtering algorithm. We find that in comparison to the alternative approach, our approach is able to enhance privacy significantly without sacrificing the accuracy level of the recommendations (and in some cases providing even higher level of accuracy).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419305688",
    "keywords": [
      "Architecture",
      "Art",
      "Artificial intelligence",
      "Collaborative filtering",
      "Computer science",
      "Computer security",
      "Economics",
      "Economy",
      "Information retrieval",
      "Personally identifiable information",
      "Recommender system",
      "Service (business)",
      "Service provider",
      "Variety (cybernetics)",
      "Visual arts",
      "Web service",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Mazeh",
        "given_name": "Itzik"
      },
      {
        "surname": "Shmueli",
        "given_name": "Erez"
      }
    ]
  },
  {
    "title": "Cautious relational clustering: A thresholding approach",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112837",
    "abstract": "We propose in this article a new relational clustering method that can return a partial answer (i.e., a set of clusterings) in some cases. Starting from relational or similarity data, we determine a partial equivalence relation defined on the set of objects (two objects are linked if they belong to the same cluster): the key idea is to allow the method to abstain on some pairwise links because they cannot be determined with enough certainty from the data. This cautious equivalence relation represents a set of possible hard clusterings which can be obtained by completing the partial relation. This formalization makes it possible to easily detect ambiguous links and to identify subsets of objects with uncertain relationship. We illustrate the potential interest of our approach as a tool for exploratory data analysis of synthetic and real data sets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419305391",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Discrete mathematics",
      "Equivalence relation",
      "Exploratory data analysis",
      "Image (mathematics)",
      "Mathematics",
      "Pairwise comparison",
      "Programming language",
      "Relation (database)",
      "Relational database",
      "Set (abstract data type)",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Masson",
        "given_name": "Marie-Hélène"
      },
      {
        "surname": "Quost",
        "given_name": "Benjamin"
      },
      {
        "surname": "Destercke",
        "given_name": "Sébastien"
      }
    ]
  },
  {
    "title": "Low resolution face recognition using a two-branch deep convolutional neural network architecture",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112854",
    "abstract": "We propose a novel coupled mappings method for low resolution face recognition using deep convolutional neural networks (DCNNs). The proposed architecture consists of two branches of DCNNs to map the high and low resolution face images into a common space with nonlinear transformations. The branch corresponding to transformation of high resolution images consists of 14 layers and the other branch which maps the low resolution face images to the common space includes a 5-layer super-resolution network connected to a 14-layer network. The distance between the features of corresponding high and low resolution images are backpropagated to train the networks. Our proposed method is evaluated on FERET, LFW, and MBGC datasets and compared with state-of-the-art competing methods. Our extensive experimental evaluations show that the proposed method significantly improves the recognition performance especially for very low resolution probe face images (5% improvement in recognition accuracy). Furthermore, it can reconstruct a high resolution image from its corresponding low resolution probe image which is comparable with the state-of-the-art super-resolution methods in terms of visual quality.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419305561",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Convolutional neural network",
      "Face (sociological concept)",
      "Facial recognition system",
      "Gene",
      "Geography",
      "High resolution",
      "Image (mathematics)",
      "Image resolution",
      "Low resolution",
      "Network architecture",
      "Pattern recognition (psychology)",
      "Programming language",
      "Remote sensing",
      "Resolution (logic)",
      "Set (abstract data type)",
      "Social science",
      "Sociology",
      "Transformation (genetics)"
    ],
    "authors": [
      {
        "surname": "Zangeneh",
        "given_name": "Erfan"
      },
      {
        "surname": "Rahmati",
        "given_name": "Mohammad"
      },
      {
        "surname": "Mohsenzadeh",
        "given_name": "Yalda"
      }
    ]
  },
  {
    "title": "A hybrid data mining approach for identifying the temporal effects of variables associated with breast cancer survival",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112863",
    "abstract": "Predicting breast cancer survival is crucial for practitioners to determine possible outcomes and make better treatment plans for the patients. In this study, a hybrid data mining based methodology was constructed to differentiate the variables whose importance for survival change over time. Therefore, the importance of variables was determined for three different time periods (i.e. one, five, and ten years). To conduct such an analysis, the most parsimonious models were constructed by employing one regression analysis method—Least Absolute Shrinkage and Selection Operator (LASSO), and one metaheuristic optimization method, namely a Genetic Algorithm (GA). Due to the high imbalance between the number of survivals and deaths, two well-known resampling procedures—Random Under-sampling (RUS) and Synthetic Minority Over-sampling Technique (SMOTE)—were applied to increase the performance of the classification models. In the final stage, two data mining models, namely Artificial Neural Networks (ANNs) and Logistic Regression (LR), were utilized along with 10-fold cross-validation. Sensitivity analysis (SA) was conducted for each model to identify the importance of each variable for a certain model and time period. The obtained results revealed that certain variables lose their importance over time, while others gain importance. This information can assist medical practitioners in identifying specific subsets of variables to focus on in different periods, which will in turn lead to a more effective and efficient cancer care. Moreover, the study findings indicate that extremely parsimonious models can be developed by adopting a purely data-driven approach, rather than eliminating the variables manually. Such methodology can also be applied in treating other types of cancer.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419305731",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Feature selection",
      "Filter (signal processing)",
      "Lasso (programming language)",
      "Logistic regression",
      "Machine learning",
      "Metaheuristic",
      "Regression analysis",
      "Resampling",
      "Sampling (signal processing)",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Simsek",
        "given_name": "Serhat"
      },
      {
        "surname": "Kursuncu",
        "given_name": "Ugur"
      },
      {
        "surname": "Kibis",
        "given_name": "Eyyub"
      },
      {
        "surname": "AnisAbdellatif",
        "given_name": "Musheera"
      },
      {
        "surname": "Dag",
        "given_name": "Ali"
      }
    ]
  },
  {
    "title": "Multi-objective item evaluation for diverse as well as novel item recommendations",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112857",
    "abstract": "Most of the traditional Recommendation Systems (RSs) focus on recommending only the popular items as they deal with a single objective precision/popularity. However, focusing on the diversity of the items in the recommendation list is also equally important to improve its relevance to the user, i.e., it is required to view RSs as a multi-objective optimization problem. Nevertheless, owing to popularity and diversity to be conflicting with each other, it degrades the accuracy of the recommendation list. Therefore, in this work, we use a multi-objective optimization method to maintain a trade-off between the popularity and the diversity and obtain multiple trade-off solutions in a single run. We first incorporate Bhattacharyya Coefficient in an existing nonlinear similarity computation model to create a new similarity model named as Bhat_sim to increase the prediction accuracy of the exiting rating evaluation methods. Further, we formulate a multi-parent crossover mechanism NewCross in the proposed multi-objective recommendation filtering NewCrossPMOEA which preserves the order and the frequency in the parents genes to bring good objectivity in the trade-off of recommending popular and diverse items in the recommendation list. The obtained results on the Movielens dataset demonstrate that the NewCrossPMOEA performs superior in terms of average precision, diversity, and novelty to its competing methods. Moreover, the Pareto-dominance concept of NewCrossPMOEA suggests multiple recommendation solutions of diverse and novel items to the target users in a single run.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419305597",
    "keywords": [
      "Artificial intelligence",
      "Bhattacharyya distance",
      "Collaborative filtering",
      "Computer science",
      "Crossover",
      "Data mining",
      "Image (mathematics)",
      "Information retrieval",
      "Machine learning",
      "MovieLens",
      "Popularity",
      "Psychology",
      "RSS",
      "Recommender system",
      "Similarity (geometry)",
      "Social psychology",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Jain",
        "given_name": "Ankush"
      },
      {
        "surname": "Singh",
        "given_name": "Pramod Kumar"
      },
      {
        "surname": "Dhar",
        "given_name": "Joydip"
      }
    ]
  },
  {
    "title": "A new fusion of grey wolf optimizer algorithm with a two-phase mutation for feature selection",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112824",
    "abstract": "Because of their high dimensionality, dealing with large datasets can hinder the data mining process. Thus, the feature selection is a pre-process mandatory phase for reducing the dimensionality of datasets through using the most informative features and at the same time maximizing the classification accuracy. This paper proposes a new Grey Wolf Optimizer algorithm integrated with a Two-phase Mutation to solve the feature selection for classification problems based on the wrapper methods. The sigmoid function is used to transform the continuous search space to the binary one in order to match the binary nature of the feature selection problem. The two-phase mutation enhances the exploitation capability of the algorithm. The purpose of the first mutation phase is to reduce the number of selected features while preserving high classification accuracy. The purpose of the second mutation phase is to attempt to add more informative features that increase the classification accuracy. As the mutation phase can be time-consuming, the two-phase mutation can be done with a small probability. The wrapper methods can give high-quality solutions so we use one of the most famous wrapper methods which called k-Nearest Neighbor (k-NN) classifier. The Euclidean distance is computed to search for the k-NN. Each dataset is split into training and testing data using K-fold cross-validation to overcome the overfitting problem. Several comparisons with the most famous and modern algorithms such as flower algorithm, particle swarm optimization algorithm, multi-verse optimizer algorithm, whale optimization algorithm, and bat algorithm are done. The experiments are done using 35 datasets. Statistical analyses are made to prove the effectiveness of the proposed algorithm and its outperformance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419305263",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Classifier (UML)",
      "Computer science",
      "Curse of dimensionality",
      "Data mining",
      "Euclidean distance",
      "Feature (linguistics)",
      "Feature selection",
      "Gene",
      "Genetic algorithm",
      "Linguistics",
      "Machine learning",
      "Mutation",
      "Overfitting",
      "Particle swarm optimization",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Abdel-Basset",
        "given_name": "Mohamed"
      },
      {
        "surname": "El-Shahat",
        "given_name": "Doaa"
      },
      {
        "surname": "El-henawy",
        "given_name": "Ibrahim"
      },
      {
        "surname": "de Albuquerque",
        "given_name": "Victor Hugo C."
      },
      {
        "surname": "Mirjalili",
        "given_name": "Seyedali"
      }
    ]
  },
  {
    "title": "Optimal integration of the facility location problem into the multi-project multi-supplier multi-resource Construction Supply Chain network design under the vendor managed inventory strategy",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112841",
    "abstract": "Using an expert system, this study is a pioneer in the formulation of an original Mixed Integer Linear Programming to integrate Vendor Managed Inventory strategy into the general multi-project multi-resource multi-supplier Construction Supply Chain (CSC) network design and facility location problems in a minimum cost. The framework is capable of dynamically scheduling resources in terms of timing and delivery as well as selecting appropriate suppliers and suitable candidate locations restricted to only authorized facilities in a capacitated network. Results show that there are different distributions for the different cost components in response to the different network sizes. Since changes in ratios of transportation cost and inventory holding cost to the total cost are mirroring each other, the total transportation and inventory costs in proportion to the total network's cost does not depend on the problem's size. In other point of view, when the network is large enough, changing its size results in a very little change in the cost components, hence better controlling the cost variability is achieved. Besides, increasing the number of projects may increase the total cost of the CSC with increasing rates, and the disparity between the number of projects and the number of suppliers increases the cost of the network, nonlinearly. Further, if the duration of the projects is given fixed, the greater the number of time periods for providing resources, the lower the transportation costs. Finally, the higher replenishment frequency results in the lower inventory cost and brings benefits for both sides of the CSC.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419305433",
    "keywords": [
      "Accounting",
      "Algorithm",
      "Business",
      "Computer network",
      "Computer science",
      "Economics",
      "Facility location problem",
      "Integer programming",
      "Marketing",
      "Mathematics",
      "Network planning and design",
      "Operations management",
      "Operations research",
      "Scheduling (production processes)",
      "Supply chain",
      "Supply chain management",
      "Supply chain network",
      "Total cost",
      "Vendor",
      "Vendor-managed inventory"
    ],
    "authors": [
      {
        "surname": "Golpîra",
        "given_name": "Hêriş"
      }
    ]
  },
  {
    "title": "A feature extraction model based on discriminative graph signals",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112861",
    "abstract": "Classification finds wide applications in artificial intelligence and expert systems. Feature extraction is a key step for classifier learning. However, the relation among samples is usually ignored in classical feature extraction models. Recently, feature extraction based on graph signal processing that makes use of the relation among samples has attracted great attention. It is a common assumption that the classification information is smooth and of low frequency in these studies. We point out that it is the discrimination ability that essentially makes a good classification feature instead of smoothness. This new perspective prompts us to introduce the concept of discriminative graph signal, and then, based on this concept, we propose a novel feature extraction model for supervised classification. To improve the classification ability for multi-class problems, a generalized model is proposed to extract multiple discriminative signals and an algorithm is also presented to compute the multiple discriminative signals simultaneously. On five publicly available UCI datasets, our proposed method outperforms the existing methods in terms of performance. Finally some drawbacks are discussed and future research directions are also provided.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419305718",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Discriminative model",
      "Feature extraction",
      "Graph",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Lin",
        "given_name": "Wuhong"
      },
      {
        "surname": "Huang",
        "given_name": "Jianfeng"
      },
      {
        "surname": "Suen",
        "given_name": "Ching Yee"
      },
      {
        "surname": "Yang",
        "given_name": "Lihua"
      }
    ]
  },
  {
    "title": "Resilient supplier selection in logistics 4.0 with heterogeneous information",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.07.016",
    "abstract": "Supplier selection problem has gained extensive attention in the prior studies. However, research based on Fuzzy Multi-Attribute Decision Making (F-MADM) approach in ranking resilient suppliers in logistic 4.0 is still in its infancy. Traditional MADM approach fails to address the resilient supplier selection problem in logistic 4.0 primarily because of the large amount of data concerning some attributes that are quantitative, yet difficult to process while making decisions. Besides, some qualitative attributes prevalent in logistic 4.0 entail imprecise perceptual or judgmental decision relevant information, and are substantially different than those considered in traditional suppler selection problems. This study develops a Decision Support System (DSS) that will help the decision maker to incorporate and process such imprecise heterogeneous data in a unified framework to rank a set of resilient suppliers in the logistic 4.0 environment. The proposed framework induces a triangular fuzzy number from large-scale temporal data using probability-possibility consistency principle. Large number of non-temporal data presented graphically are computed by extracting granular information that are imprecise in nature. Fuzzy linguistic variables are used to map the qualitative attributes. Finally, fuzzy based TOPSIS method is adopted to generate the ranking score of alternative suppliers. These ranking scores are used as input in a Multi-Choice Goal Programming (MCGP) model to determine optimal order allocation for respective suppliers. Finally, a sensitivity analysis assesses how the Supplier's Cost versus Resilience Index (SCRI) changes when differential priorities are set for respective cost and resilience attributes.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419304956",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Consistency (knowledge bases)",
      "Data mining",
      "Fuzzy logic",
      "Fuzzy set",
      "Machine learning",
      "Mathematics",
      "Operations research",
      "Physics",
      "Programming language",
      "Ranking (information retrieval)",
      "Resilience (materials science)",
      "Selection (genetic algorithm)",
      "Set (abstract data type)",
      "Thermodynamics"
    ],
    "authors": [
      {
        "surname": "Hasan",
        "given_name": "Md Mahmudul"
      },
      {
        "surname": "Jiang",
        "given_name": "Dizuo"
      },
      {
        "surname": "Ullah",
        "given_name": "A.M.M. Sharif"
      },
      {
        "surname": "Noor-E-Alam",
        "given_name": "Md."
      }
    ]
  },
  {
    "title": "Friend recommendation for cross marketing in online brand community based on intelligent attention allocation link prediction algorithm",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112839",
    "abstract": "Circle structure of online brand communities allows companies to conduct cross-marketing activities by the influence of friends in different circles and build strong and lasting relationships with customers. However, existing works on the friend recommendation in social network do not consider establishing friendships between users in different circles, which has the problems of network sparsity, neither do they study the adaptive generation of appropriate link prediction algorithms for different circle features. In order to fill the gaps in previous works, the intelligent attention allocation link prediction algorithm is proposed to adaptively build attention allocation index (AAI) according to the sparseness of the network and predict the possible friendships between users in different circles. The AAI reflects the amount of attention allocated to the user pair by their common friend in the triadic closure structure, which is decided by the friend count of the common friend. Specifically, for the purpose of overcoming the problem of network sparsity, the AAIs of both the direct common friends and indirect ones are developed. Next, the decision tree (DT) method is constructed to adaptively select the suitable AAIs for the circle structure based on the density of common friends and the dispersion level of common friends’ attention. In addition, for the sake of further improving the accuracy of the selected AAI, its complementary AAIs are identified with support vector machine model according to their similarity in value, direction, and ranking. Finally, the mutually complementary indices are combined into a composite one to comprehensively portray the attention distribution of common friends of users in different circles and predict their possible friendships for cross-marketing activities. Experimental results on Twitter and Google+ show that the model has highly reliable prediction performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741741930541X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Closure (psychology)",
      "Computer science",
      "Data mining",
      "Economics",
      "Image (mathematics)",
      "Machine learning",
      "Market economy",
      "Ranking (information retrieval)",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Shugang"
      },
      {
        "surname": "Song",
        "given_name": "Xuewei"
      },
      {
        "surname": "Lu",
        "given_name": "Hanyu"
      },
      {
        "surname": "Zeng",
        "given_name": "Linyi"
      },
      {
        "surname": "Shi",
        "given_name": "Miaojing"
      },
      {
        "surname": "Liu",
        "given_name": "Fang"
      }
    ]
  },
  {
    "title": "A new efficient hybrid algorithm for large scale multiple traveling salesman problems",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112867",
    "abstract": "Multiple traveling salesmen problem (MTSP) is not only a generalization of the traveling salesman problem (TSP), but also more suitable for modeling practical problems in the real life than TSP. For solving the MTSP with multiple depots, the requirement of minimum and maximum number of cities that each salesman should visit, a hybrid algorithm called ant colony-partheno genetic algorithms (AC-PGA) is provided by combining partheno genetic algorithms (PGA) and ant colony algorithms (ACO). The main idea in this paper is to divide the variables into two parts. In detail, it exploits PGA to comprehensively search the best value of the first part variables and then utilizes ACO to accurately determine the second part variables value. For comparative analysis, PGA, improved PGA (IPGA), two-part wolf pack search (TWPS), artificial bee colony (ABC) and invasive weed optimization (IWO) algorithms are adopted to solve MTSP and validated with publicly available TSPLIB benchmarks. The results of comparative experiments show that AC-PGA is sufficiently effective in solving large scale MTSP and has better performance than the existing algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419305779",
    "keywords": [
      "Algorithm",
      "Ant colony",
      "Ant colony optimization algorithms",
      "Artificial bee colony algorithm",
      "Artificial intelligence",
      "Computer science",
      "Extremal optimization",
      "Generalization",
      "Genetic algorithm",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Meta-optimization",
      "Metaheuristic",
      "Physics",
      "Quantum mechanics",
      "Scale (ratio)",
      "Travelling salesman problem"
    ],
    "authors": [
      {
        "surname": "Jiang",
        "given_name": "Chao"
      },
      {
        "surname": "Wan",
        "given_name": "Zhongping"
      },
      {
        "surname": "Peng",
        "given_name": "Zhenhua"
      }
    ]
  },
  {
    "title": "Expected margin–based pattern selection for support vector machines",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112865",
    "abstract": "Support Vector Machines (SVMs) are amongst the most powerful classification algorithms in machine learning and data mining. However, SVMs are limited by high training complexity when training with large datasets. Pattern selection methods have been proposed to reduce the training complexity by selecting a smaller subset of important patterns among all training patterns. In this paper, we propose a new pattern selection method called Expected Margin–based Pattern Selection (EMPS), which selects patterns based on an estimated margin for SVM classifiers. With the estimated margin, EMPS selects patterns that are likely to become support vectors located on the margin boundary and inside the margin region; however, other patterns including noise support vectors are discarded. The experimental results involving 15 benchmark datasets and one real–world semiconductor manufacturing dataset showed that EMPS exhibits excellent performance and stability.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419305755",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Data mining",
      "Decision boundary",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Margin (machine learning)",
      "Margin classifier",
      "Pattern recognition (psychology)",
      "Selection (genetic algorithm)",
      "Stability (learning theory)",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Kim",
        "given_name": "Dongil"
      },
      {
        "surname": "Kang",
        "given_name": "Seokho"
      },
      {
        "surname": "Cho",
        "given_name": "Sungzoon"
      }
    ]
  },
  {
    "title": "Borrow from rich cousin: transfer learning for emotion detection using cross lingual embedding",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112851",
    "abstract": "Performance of any natural language processing (NLP) system greatly depends on the amount of resources and tools available in a particular language or domain. Therefore, while solving any problem in low-resource setting, it is important to investigate techniques to leverage the resources and tools available in resource-rich languages. In this paper we propose an efficient technique to mitigate the problem of resource scarcity for emotion detection in Hindi by leveraging information from a resource-rich language like English. Our method follows a deep transfer learning framework which efficiently captures relevant information through the shared space of two languages, showing significantly better performance compared to the monolingual scenario that learns in the vector space of only one language. As base learning models, we use Convolution Neural Network (CNN) and Bi-Directional Long Short Term Memory (Bi-LSTM). As there are no available emotion labeled dataset for Hindi, we create a new dataset for emotion detection in disaster domain by annotating sentences of news documents with nine different classes based on Plutchikâ;;s wheel of emotions. To improve the performance of emotion classification in Hindi, we employ transfer learning to exploit the resources available in the related domains. The core of our approach lies in generating a cross-lingual word embedding representation of words in the shared embedding space. The neural networks are trained on the existing datasets, and then weights are fine-tuned following the four different transfer learning strategies for emotion classification in Hindi. We obtain a significant performance gain in our our proposed transfer learning techniques, achieving an F1-score of 0.53 (compared to 0.47)-thereby implying that knowledge from a resource-rich language can be transferred across language and domains. 1 1 codes and data available at https://github.com/zishanahmad1821cs18/crosslingual_transfer",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419305536",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Embedding",
      "Hindi",
      "Leverage (statistics)",
      "Machine learning",
      "Natural language processing",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Ahmad",
        "given_name": "Zishan"
      },
      {
        "surname": "Jindal",
        "given_name": "Raghav"
      },
      {
        "surname": "Ekbal",
        "given_name": "Asif"
      },
      {
        "surname": "Bhattachharyya",
        "given_name": "Pushpak"
      }
    ]
  },
  {
    "title": "Local distances preserving based manifold learning",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112860",
    "abstract": "In this paper a local manifold learning method based on the local distances preserving (LDP) is proposed. LDP focuses on extracting and preserving the local distances between the data points. In LDP, the coefficients between each data point and its neighbors are calculated based on the inverse of Euclidean distances between them. Then, a minimization function, which is in accordance with the calculated coefficients, is proposed for calculating the embedded data manifold in the low-dimensional representation space. Matching the way of calculation the coefficients and the proposed minimization function is the main significance of the proposed LDP method. In addition, the proposed LDP method shows less sensitivity to the initial parameters such as the number of neighbors and the value of variance. Also, a stochastic based extension of LDP (SLDP) manifold learning method is proposed. The proposed method is compared with the common manifold learning methods based on the achievable recognition rate and the power of the local distances preserving. The experiments have been done on two different kinds of databases: HODA Persian handwritten character database and ORL face image database. The results demonstrate the suitable performance of LDP and SLDP. Also, the results show the robustness of the proposed method to the number of neighbors and the value of variance parameter. Moreover, the proposed method of calculating the embedded data points in LDP and SLDP has less complexity than the similar local manifold learning methods, Laplacian eigenmaps (LEM) and stochastic LEM (SLEM).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419305706",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Dimensionality reduction",
      "Engineering",
      "Manifold (fluid mechanics)",
      "Manifold alignment",
      "Mechanical engineering",
      "Nonlinear dimensionality reduction",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Hajizadeh",
        "given_name": "Rassoul"
      },
      {
        "surname": "Aghagolzadeh",
        "given_name": "A."
      },
      {
        "surname": "Ezoji",
        "given_name": "M."
      }
    ]
  },
  {
    "title": "Using high-fidelity meta-models to improve performance of small dataset trained Bayesian Networks",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112830",
    "abstract": "Machine Learning (ML) is increasingly being used by companies like Google, Amazon and Apple to help identify market trends and predict customer behavior. Continuous improvement and maturing of these ML tools will help improve decision making across a number of industries. Unfortunately, before many ML strategies can be utilized the methods often require large amounts of data. For a number of realistic situations, however, only smaller subsets of data are available (i.e. hundreds to thousands of points). This work explores this problem by investigating the feasibility of using meta-models, specifically Kriging and Radial Basis Functions, to generate data for training a BN when only small amounts of original data are available. This paper details the meta-model creation process and the results of using Particle Swarm Optimization (PSO) for tuning parameters for four network structures trained using three relatively small data sets. Additionally, a series of experiments augment these small datasets by generating ten thousand, one-hundred thousand, and a million synthetic data points using the Kriging and RBF meta-models as well as intelligently establishing prior probabilities using PSO. Results show that augmenting limited existing datasets with meta-model generated data can dramatically affect network accuracy. Overall, the exploratory results presented in this paper demonstrate the feasibility of using meta-model generated data to increase the accuracy of small sample set trained BN. Further developing this method will help underserved areas with access to only small datasets make use of the powerful predictive analytics of ML.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419305329",
    "keywords": [
      "Artificial intelligence",
      "Bayesian optimization",
      "Bayesian probability",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Data mining",
      "Data set",
      "Economics",
      "Fidelity",
      "Kriging",
      "Machine learning",
      "Management",
      "Mathematics",
      "Meta learning (computer science)",
      "Operating system",
      "Particle swarm optimization",
      "Process (computing)",
      "Programming language",
      "Sample (material)",
      "Sample size determination",
      "Set (abstract data type)",
      "Statistics",
      "Task (project management)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "MacAllister",
        "given_name": "Anastacia"
      },
      {
        "surname": "Kohl",
        "given_name": "Adam"
      },
      {
        "surname": "Winer",
        "given_name": "Eliot"
      }
    ]
  },
  {
    "title": "Comparing the effectiveness of deep feedforward neural networks and shallow architectures for predicting stock price indices",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112828",
    "abstract": "Many existing learning algorithms suffer from limited architectural depth and the locality of estimators, making it difficult to generalize from the test set and providing inefficient and biased estimators. Deep architectures have been shown to appropriately learn correlation structures in time series data. This paper compares the effectiveness of a deep feedforward Neural Network (DNN) and shallow architectures (e.g., Support Vector Machine (SVM) and one-layer NN) when predicting a broad cross-section of stock price indices in both developed and emerging markets. An extensive evaluation is undertaken, using daily, hourly, minute and tick level data related to thirty-four financial indices from 32 countries across six years. Our evaluation results show a considerable advantage from training deep (cf. shallow) architectures, using a rectifier linear (RELU) activation function, across all thirty-four markets when ‘minute’ data is used. However, the predictive performance of DNN was not significantly better than that of shallower architectures when using tick level data. This result suggests that when training a DNN algorithm, the predictive accuracy peaks, regardless of training size. We also examine which activation function works best for stock price index data. Our results demonstrate that the RELU activation function performs better than TANH across all markets and time horizons when using DNN to predict stock price indices.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419305305",
    "keywords": [
      "Activation function",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer science",
      "Deep learning",
      "Estimator",
      "Horse",
      "Linguistics",
      "Locality",
      "Machine learning",
      "Mathematics",
      "Paleontology",
      "Philosophy",
      "Statistics",
      "Stock market",
      "Stock market index",
      "Support vector machine",
      "Test set"
    ],
    "authors": [
      {
        "surname": "Orimoloye",
        "given_name": "Larry Olanrewaju"
      },
      {
        "surname": "Sung",
        "given_name": "Ming-Chien"
      },
      {
        "surname": "Ma",
        "given_name": "Tiejun"
      },
      {
        "surname": "Johnson",
        "given_name": "Johnnie E.V."
      }
    ]
  },
  {
    "title": "Weighted kshell degree neighborhood: A new method for identifying the influential spreaders from a variety of complex network connectivity structures",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112859",
    "abstract": "Due to the fast and worldwide growth of the social network, it has become a potent platform for broadcasting any information. Through the network, people can easily reach to a mass, can easily propagate a piece of information within a short time. Considering the advantages, especially to accelerate the information spreading or controlling the spreading, the organizations want to exploit the social network to its best. However, as we know, the network is formed by connecting one node (i.e., user) to another node, and it is not that all the nodes will be effective equally in spreading. Because it depends on many factors and one of them is their topological position in the network. Automatically finding the effective nodes (the influential spreaders) from a network is a real challenge. In the literature, kshell decomposition and degree centrality are the two popular measures for identifying the influential spreaders from a network. Moreover, it is more challenging in identifying the influential spreaders when network connectivity structure varies from network to network. It has been found that the kshell decomposition method works better in the complete global network connectivity structures and neighbors’ degree method in the incomplete global network connectivity structures. But the degree of completeness of the network connectivity structures also vary. Under this circumstance, only the kshell method or only the neighbors’ degree method will not be able to obtain the best influential spreaders. To overcome this problem, this article proposes an indexing method weighted kshell degree neighborhood which is a composition of kshell and degree through tunable parameters. We have evaluated the effectiveness of the proposed method using different real networks and the Susceptible-Infected-Recovered (SIR) spreading epidemic model. The results show that the proposed method can significantly obtain the best spreading dynamics from different varieties of network connectivity structures and outperforms the other existing indexing methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741741930569X",
    "keywords": [
      "Acoustics",
      "Artificial intelligence",
      "Centrality",
      "Combinatorics",
      "Complex network",
      "Computer network",
      "Computer science",
      "Computer security",
      "Data mining",
      "Degree (music)",
      "Engineering",
      "Exploit",
      "Mathematics",
      "Network science",
      "Network topology",
      "Node (physics)",
      "Physics",
      "Search engine indexing",
      "Social media",
      "Social network (sociolinguistics)",
      "Structural engineering",
      "Theoretical computer science",
      "Weighted network",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Namtirtha",
        "given_name": "Amrita"
      },
      {
        "surname": "Dutta",
        "given_name": "Animesh"
      },
      {
        "surname": "Dutta",
        "given_name": "Biswanath"
      }
    ]
  },
  {
    "title": "Intelligent image-based colourimetric tests using machine learning framework for lateral flow assays",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112843",
    "abstract": "This paper aims to deliberately examine the scope of an intelligent colourimetric test that fulfils ASSURED criteria (Affordable, Sensitive, Specific, User-friendly, Rapid and robust, Equipment-free, and Deliverable) and demonstrate the claim as well. This paper presents an investigation into an intelligent image-based system to perform automatic paper-based colourimetric tests in real-time to provide a proof-of-concept for a dry-chemical based or microfluidic, stable and semi-quantitative assay using a larger dataset with diverse conditions. The universal pH indicator papers were utilised as a case study. Unlike the works done in the literature, this work performs multiclass colourimetric tests using histogram-based image processing and machine learning algorithm without any user intervention. The proposed image processing framework is based on colour channel separation, global thresholding, morphological operation and object detection. We have also deployed aserver-based convolutional neural network framework for image classification using inductive transfer learning on a mobile platform. The results obtained by both traditional machine learning and pre-trained model-based deep learning were critically analysed with the set evaluation criteria (ASSURED criteria). The features were optimised using univariate analysis and exploratory data analysis to improve the performance. The image processing algorithm showed >98% accuracy while the classification accuracy by Least Squares Support Vector Machine (LS-SVM) was 100%. On the other hand, the deep learning technique provided >86% accuracy, which could be further improved with a large amount of data. The k-fold cross-validated LS-SVM based final system, examined on different datasets, confirmed the robustness and reliability of the presented approach, which was further validated using statistical analysis. The understaffed and resource-limited healthcare system can benefit from such an easy-to-use technology to support remote aid workers, assist in elderly care and promote personalised healthcare by eliminating the subjectivity of interpretation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419305457",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Data mining",
      "Histogram",
      "Histogram of oriented gradients",
      "Image (mathematics)",
      "Image processing",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Hoque Tania",
        "given_name": "Marzia"
      },
      {
        "surname": "Lwin",
        "given_name": "Khin T."
      },
      {
        "surname": "Shabut",
        "given_name": "Antesar M."
      },
      {
        "surname": "Najlah",
        "given_name": "Mohammad"
      },
      {
        "surname": "Chin",
        "given_name": "Jeannette"
      },
      {
        "surname": "Hossain",
        "given_name": "M.A."
      }
    ]
  },
  {
    "title": "A new model for evaluating subjective online ratings with uncertain intervals",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112850",
    "abstract": "We formalize the information acquisition and choice structure of a decision maker (DM) when the main characteristics defining the alternatives are not directly observed but numerical evaluations of unknown quality are provided by external raters. The DM observes the overall numerical value assigned by the raters to an alternative and defines an uncertain interval within which the evaluation observed is contained. The width of the interval is determined by the subjective perception and evaluation differences existing between the DM and the raters transmitting the information. We analyze the incentives of the DM to improve upon an evaluation contained within an uncertain interval by retrieving further information from the raters of other alternatives. Different scenarios will be developed based on the ability of the DM to fully assimilate uncertainty and the introduction of heuristic approximations to account for the potential frictions arising from uncertainty. One of the main qualities of the current framework is its capacity to formalize interactions among alternatives determined by interval width differences across their characteristics, providing an analytical advantage over the operational complexity involved in the use of fuzzy and intuitionistic fuzzy sets. The same remark applies to the formalization of the interactions across attributes that must be considered when defining sequential decision processes or dynamical systems while dealing with multiple sources of uncertainty. Numerical simulations are provided to compare the different scenarios developed and describe the main consequences derived from ignoring the uncertainty inherent to the evaluations received. In particular, we illustrate the ranking consequences derived from increasing the spread of the evaluation uncertainty, an effect that can be easily combined with the risk attitude exhibited by DMs. The inclusion of both these features bridges the gap between economics, psychology and multiple criteria decision making, whose techniques do not generally account for these differences among DMs.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419305524",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Data mining",
      "Decision maker",
      "Epistemology",
      "Fuzzy logic",
      "Heuristic",
      "Interval (graph theory)",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Operations research",
      "Philosophy",
      "Quality (philosophy)",
      "Ranking (information retrieval)",
      "Value (mathematics)"
    ],
    "authors": [
      {
        "surname": "Santos-Arteaga",
        "given_name": "Francisco J."
      },
      {
        "surname": "Tavana",
        "given_name": "Madjid"
      },
      {
        "surname": "Di Caprio",
        "given_name": "Debora"
      }
    ]
  },
  {
    "title": "Refined selfish herd optimizer for global optimization problems",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112838",
    "abstract": "The selfish herds optimizer (SHO) is a recently developed swarm optimization algorithm for solving global optimization problems. SHO mimics the widely observed selfish herd behaviors of avoiding predation risks. In SHO, a set of unique evolutionary operators inspired by the prey-predator relationship are used in dealing with optimization problems. Although SHO can provide a good performance when finding an optimal solution, this algorithm has some oversights that cause problems in the exploitation ability of SHO. Additionally, SHO still has some shortcomings that influence the performance of exploration and avoiding stagnation in local optima. In this paper, some refinements and modifications are proposed for these oversights and shortcomings. In order to validate whether the refinements and modifications are appropriate for improving the performance of SHO, two suites of benchmark functions are employed to compare the proposed method with the original SHO and other well-known and recently developed algorithms, such as Standard Particle Swarm Optimization, Artificial Bee Colony algorithm, Differential Evolution, and Crow Search Algorithm, etc. Finally, the efficiency of the proposed method is justified using the nonparametric Wilcoxon rank-sum test. The experimental results show that the proposed modifications are appropriate for improving the performance of the original SHO. In addition, the proposed method can also give competitive results in finding global optima when compared with other algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419305408",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Differential evolution",
      "Evolutionary algorithm",
      "Geodesy",
      "Geography",
      "Global optimization",
      "Local optimum",
      "Mann–Whitney U test",
      "Mathematical optimization",
      "Mathematics",
      "Metaheuristic",
      "Particle swarm optimization",
      "Programming language",
      "Set (abstract data type)",
      "Statistics",
      "Wilcoxon signed-rank test"
    ],
    "authors": [
      {
        "surname": "Yimit",
        "given_name": "Adiljan"
      },
      {
        "surname": "Iigura",
        "given_name": "Koji"
      },
      {
        "surname": "Hagihara",
        "given_name": "Yoshihiro"
      }
    ]
  },
  {
    "title": "Map-Reduce based tipping point scheduler for parallel image processing",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112848",
    "abstract": "Nowadays, Big Data image processing is very much in need due to its proven success in the field of business information system, medical science and social media. However, as the days are passing by, the computation of Big Data images is becoming more complex which ultimately results in complex resource management and higher task execution time. Researchers have been using a combination of CPU and GPU based computing to cut down the execution time, however, when it comes to scaling of compute nodes, then the combination of CPU and GPU based computing still remains a challenge due to the high communication cost factor. In order to tackle this issue, the Map-Reduce framework has come out to be a viable option as its workflow optimization could be enhanced by changing its underlying job scheduling mechanism. This paper presents a comparative study of job scheduling algorithms which could be deployed over various Big Data based image processing application and also proposes a tipping point scheduling algorithm to optimize the workflow for job execution on multiple nodes. The evaluation of the proposed scheduling algorithm is done by implementing parallel image segmentation algorithm to detect lung tumor for up to 3GB size of image dataset. In terms of performance comprising of task execution time and throughput, the proposed tipping point scheduler has come out to be the best scheduler followed by the Map-Reduce based Fair scheduler. The proposed tipping point scheduler is 1.14 times better than Map-Reduce based Fair scheduler and 1.33 times better than Map-Reduced based FIFO scheduler in terms of task execution time and throughput. In terms of speedup comparison between single node and multiple nodes, the proposed tipping point scheduler attained a speedup of 4.5 X for multi-node architecture.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419305500",
    "keywords": [
      "Big data",
      "Computer science",
      "Data mining",
      "Database",
      "Distributed computing",
      "Economics",
      "Operations management",
      "Parallel computing",
      "Real-time computing",
      "Scheduling (production processes)",
      "Workflow"
    ],
    "authors": [
      {
        "surname": "Akhtar",
        "given_name": "Mohammad Nishat"
      },
      {
        "surname": "Saleh",
        "given_name": "Junita Mohamad"
      },
      {
        "surname": "Awais",
        "given_name": "Habib"
      },
      {
        "surname": "Bakar",
        "given_name": "Elmi Abu"
      }
    ]
  },
  {
    "title": "Adaptive intrusion detection via GA-GOGMM-based pattern learning with fuzzy rough set-based attribute selection",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112845",
    "abstract": "In this paper, an adaptive network intrusion detection method using fuzzy rough set-based feature selection and GA-GOGMM-based pattern learning is presented. Based on the fuzzy rough set theory, the optimal attribute subset of network connection records is achieved by the information gain ratio criterion in advance. A greedy algorithm-based global optimal Gaussian mixture model (GMM) clustering method, termed GA-GOGMM, is introduced, to extract the intrinsic structure of network instances to achieve highly-discernable and stable normal and intrusion pattern libraries for the subsequent network intrusion detection (NID). GA-GOGMM-based pattern learning can achieve the optimal GMM of network traffic instances for the pattern clustering while avoiding the negative effect of the empirical initialization of clustering numbers and random initialization of clustering centers with a low computational complexity. An adaptive model updating mechanism is further introduced for the online updating of normal and intrusion pattern libraries to ensure the adaptability of the NID model. Extensive validation and comparative experiments, conducted on a benchmark dataset NSL-KDD and a self-built Nidsbench-based network simulation platform, show that the proposed ANID approach leads to a significant improvement in detection accuracies with low false alarms and missing reports on both known and unknown attacks. It can effectively adapt to the dynamic changing network environments with high detection accuracy and low false alarm rate as well as low missing reporting rate, which has significant application prospects.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419305470",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Cluster analysis",
      "Computer science",
      "Constant false alarm rate",
      "Data mining",
      "Feature selection",
      "Fuzzy clustering",
      "Fuzzy logic",
      "Geodesy",
      "Geography",
      "Initialization",
      "Intrusion detection system",
      "Machine learning",
      "Mixture model",
      "Pattern recognition (psychology)",
      "Programming language"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Jinping"
      },
      {
        "surname": "Zhang",
        "given_name": "Wuxia"
      },
      {
        "surname": "Tang",
        "given_name": "Zhaohui"
      },
      {
        "surname": "Xie",
        "given_name": "Yongfang"
      },
      {
        "surname": "Ma",
        "given_name": "Tianyu"
      },
      {
        "surname": "Zhang",
        "given_name": "Jingjing"
      },
      {
        "surname": "Zhang",
        "given_name": "Guoyong"
      },
      {
        "surname": "Niyoyita",
        "given_name": "Jean Paul"
      }
    ]
  },
  {
    "title": "Simulation-optimization for the management of the transshipment operations at maritime container terminals",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112852",
    "abstract": "Maritime container terminals are complex infrastructures designed specifically to handle a large number of containers, and which play a relevant role in international freight transport. Terminal managers must deal with a wide variety of interrelated logistic problems, and the effectiveness and productivity of the terminal depends on their solution. Management strategies are therefore necessary to increase their effectiveness and productivity, and thereby reducing the costs of these operations. This task is complicated by imprecise data and the need to satisfy several criteria, many of which are subjective, when evaluating the solutions. One of these logistic problems, the quay crane scheduling problem, has attracted the attention of many researchers since quay cranes are one of the most valuable resources in the port. Many proposals based on optimization algorithms have tackled this problem but the vast majority disregard the uncertainty inherent in this kind of systems and the impact of internal delivery vehicles. An intelligent system which integrates Artificial Intelligence techniques and simulation tools is proposed to aid terminal managers. The system combines an intelligent evolutionary algorithm to generate high quality schedules for the cranes with a simulation model that incorporates uncertainty and the impact of internal delivery vehicles. The joint use of these tools provides managers with enhanced information to decide on the quality and robustness of the proposed schedules, resulting in better solutions for everyday situations. Our intelligent system based on the optimization-simulation model provides clear benefits to maritime terminal management. The system efficiently identifies high quality schedules and can be used to evaluate its robustness. It is also flexible and can easily be adapted if other elements need to be introduced, which may affect the goodness of a schedule.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419305548",
    "keywords": [
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer security",
      "Container (type theory)",
      "Economics",
      "Electrical engineering",
      "Engineering",
      "Gene",
      "Mechanical engineering",
      "Medicine",
      "Operations management",
      "Operations research",
      "Port (circuit theory)",
      "Risk analysis (engineering)",
      "Robustness (evolution)",
      "Scheduling (production processes)",
      "Transshipment (information security)"
    ],
    "authors": [
      {
        "surname": "Castilla-Rodríguez",
        "given_name": "Iván"
      },
      {
        "surname": "Expósito-Izquierdo",
        "given_name": "Christopher"
      },
      {
        "surname": "Melián-Batista",
        "given_name": "Belén"
      },
      {
        "surname": "Aguilar",
        "given_name": "Rosa M."
      },
      {
        "surname": "Moreno-Vega",
        "given_name": "J. Marcos"
      }
    ]
  },
  {
    "title": "A study of two evolutionary/tabu search approaches for the generalized max-mean dispersion problem",
    "journal": "Expert Systems with Applications",
    "year": "2020",
    "doi": "10.1016/j.eswa.2019.112856",
    "abstract": "Evolutionary computing is a general and powerful framework for solving difficult optimization problems, including those arising in expert and intelligent systems. In this work, we investigate for the first time two hybrid evolutionary algorithms incorporating tabu search for solving the generalized max-mean dispersion problem (GMaxMeanDP) which has a variety of practical applications such as web page ranking, community mining, and trust networks. The proposed algorithms integrate innovative search strategies that help the search to explore the search space effectively. We report extensive computational results of the proposed algorithms on six types of 160 benchmark instances, demonstrating their effectiveness and usefulness. In addition to the GMaxMeanDP, the proposed algorithms can help to better solve other problems that can be formulated as the GMaxMeanDP.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417419305585",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Evolutionary algorithm",
      "Geodesy",
      "Geography",
      "Guided Local Search",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Ranking (information retrieval)",
      "Tabu search",
      "Variety (cybernetics)"
    ],
    "authors": [
      {
        "surname": "Lai",
        "given_name": "Xiangjing"
      },
      {
        "surname": "Hao",
        "given_name": "Jin-Kao"
      },
      {
        "surname": "Glover",
        "given_name": "Fred"
      }
    ]
  }
]