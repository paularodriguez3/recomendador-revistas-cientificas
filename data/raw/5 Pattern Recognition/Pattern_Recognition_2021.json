[
  {
    "title": "Deep tree-ensembles for multi-output prediction",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108211",
    "abstract": "Recently, deep neural networks have expanded the state-of-art in various scientific fields and provided solutions to long standing problems across multiple application domains. Nevertheless, they also suffer from weaknesses since their optimal performance depends on massive amounts of training data and the tuning of an extended number of parameters. As a countermeasure, some deep-forest methods have been recently proposed, as efficient and low-scale solutions. Despite that, these approaches simply employ label classification probabilities as induced features and primarily focus on traditional classification and regression tasks, leaving multi-output prediction under-explored. Moreover, recent work has demonstrated that tree-embeddings are highly representative, especially in structured output prediction. In this direction, we propose a novel deep tree-ensemble (DTE) model, where every layer enriches the original feature set with a representation learning component based on tree-embeddings. In this paper, we specifically focus on two structured output prediction tasks, namely multi-label classification and multi-target regression. We conducted experiments using multiple benchmark datasets and the obtained results confirm that our method provides superior results to state-of-the-art methods in both tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003927",
    "keywords": [],
    "authors": [
      {
        "surname": "Nakano",
        "given_name": "Felipe Kenji"
      },
      {
        "surname": "Pliakos",
        "given_name": "Konstantinos"
      },
      {
        "surname": "Vens",
        "given_name": "Celine"
      }
    ]
  },
  {
    "title": "DPNet: Detail-preserving network for high quality monocular depth estimation",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107578",
    "abstract": "Existing monocular depth estimation methods are unsatisfactory due to the inaccurate inference of depth details and the loss of spatial information. In this paper, we present a novel detail-preserving network (DPNet), i.e., a dual-branch network architecture that fully addresses the above problems and facilitates the depth map inference. Specifically, in contextual branch (CB), we propose an effective and efficient nonlocal spatial attention module by introducing non-local filtering strategy to explicitly exploit the pixel relationship in spatial domain, which can bring significant promotion on depth details inference. Meanwhile, we design a spatial branch (SB) to preserve the spatial information and generate high-resolution features from input color image. A refinement module (RM) is then proposed to fuse the heterogeneous features from both spatial and contextual branches to obtain a high quality depth map. Experimental results show that the proposed method outperforms SOTA methods on benchmark RGB-D datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303812",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Depth map",
      "Domain (mathematical analysis)",
      "Electrical engineering",
      "Engineering",
      "Exploit",
      "Fuse (electrical)",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Inference",
      "Mathematical analysis",
      "Mathematics",
      "Monocular",
      "Pattern recognition (psychology)",
      "RGB color model",
      "Spatial analysis",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Ye",
        "given_name": "Xinchen"
      },
      {
        "surname": "Chen",
        "given_name": "Shude"
      },
      {
        "surname": "Xu",
        "given_name": "Rui"
      }
    ]
  },
  {
    "title": "Learn to cycle: Time-consistent feature discovery for action recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2020.11.012",
    "abstract": "Generalizing over temporal variations is a prerequisite for effective action recognition in videos. Despite significant advances in deep neural networks, it remains a challenge to focus on short-term discriminative motions in relation to the overall performance of an action. We address this challenge by allowing some flexibility in discovering relevant spatio-temporal features. We introduce Squeeze and Recursion Temporal Gates (SRTG), an approach that favors inputs with similar activations with potential temporal variations. We implement this idea with a novel CNN block that uses an LSTM to encapsulate feature dynamics, in conjunction with a temporal gate that is responsible for evaluating the consistency of the discovered dynamics and the modeled features. We show consistent improvement when using SRTG blocks, with only a minimal increase in the number of GFLOPs. On Kinetics-700, we perform on par with current state-of-the-art models, and outperform these on HACS, Moments in Time, UCF-101 and HMDB-51. 1 1 The code for this project can be found at: https://git.io/JfuPi.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520304177",
    "keywords": [
      "Action recognition",
      "Algorithm",
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Class (philosophy)",
      "Computer science",
      "Consistency (knowledge bases)",
      "Deep learning",
      "Discriminative model",
      "FLOPS",
      "Feature (linguistics)",
      "Flexibility (engineering)",
      "Focus (optics)",
      "Geometry",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Optics",
      "Parallel computing",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Recursion (computer science)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Stergiou",
        "given_name": "Alexandros"
      },
      {
        "surname": "Poppe",
        "given_name": "Ronald"
      }
    ]
  },
  {
    "title": "Local minima found in the subparameter space can be effective for ensembles of deep convolutional neural networks",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107582",
    "abstract": "Ensembles of deep convolutional neural networks (CNNs), which integrate multiple deep CNN models to achieve better generalization for an artificial intelligence application, now play an important role in ensemble learning due to the dominant position of deep learning. However, the usage of ensembles of deep CNNs is still not adequate because the increasing complexity of deep CNN architectures and the emerging data with large dimensionality have made the training stage and testing stage of ensembles of deep CNNs inevitably expensive. To alleviate this situation, we propose a new approach that finds multiple models converging to local minima in subparameter space for ensembles of deep CNNs. The subparameter space here refers to the space constructed by a partial selection of parameters, instead of the entire set of parameters, of a deep CNN architecture. We show that local minima found in the subparameter space of a deep CNN architecture can in fact be effective for ensembles of deep CNNs to achieve better generalization. Moreover, finding local minima in the subparameter space of a deep CNN architecture is more affordable at the training stage, and the multiple models at the found local minima can also be selectively fused to achieve better ensemble generalization while limiting the expense to a single deep CNN model at the testing stage. Demonstrations of MobilenetV2, Resnet50 and InceptionV4 (deep CNN architectures from lightweight to complex) on ImageNet, CIFAR-10 and CIFAR-100, respectively, lead us to believe that finding local minima in the subparameter space of a deep CNN architecture could be leveraged to broaden the usage of ensembles of deep CNNs.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032030385X",
    "keywords": [
      "Aerospace engineering",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Convolutional neural network",
      "Curse of dimensionality",
      "Deep learning",
      "Engineering",
      "Generalization",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Maxima and minima",
      "NASA Deep Space Network",
      "Pattern recognition (psychology)",
      "Spacecraft"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Yongquan"
      },
      {
        "surname": "Lv",
        "given_name": "Haijun"
      },
      {
        "surname": "Chen",
        "given_name": "Ning"
      },
      {
        "surname": "Wu",
        "given_name": "Yang"
      },
      {
        "surname": "Zheng",
        "given_name": "Jiayi"
      },
      {
        "surname": "Zheng",
        "given_name": "Zhongxi"
      }
    ]
  },
  {
    "title": "Convolutional kernels with an element-wise weighting mechanism for identifying abnormal brain connectivity patterns",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107570",
    "abstract": "Deep learning based human brain network classification has gained increasing attention in recent years. However, current methods remain limited in exploring the topological structure information of a brain network. In this paper, we propose a kind of new convolutional kernels with an element-wise weighting mechanism (CKEW) to extract hierarchical topological features of brain networks, in which each weight is assigned to an element with a unique neuroscientific meaning. In addition, a novel classification framework based on CKEW is presented to diagnose brain diseases and explore the most important original features by a tracing feature analysis method efficiently. Experimental results on two autism spectrum disorder (ASD) datasets and an attention deficit hyperactivity disorder (ADHD) dataset with functional magnetic resonance imaging (fMRI) data demonstrate that our method can more accurately distinguish subject groups compared to several state-of-the-art methods in cerebral disease classification, and abnormal connectivity patterns and brain regions identified are more likely to become biomarkers associated with a cerebral disease.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303733",
    "keywords": [
      "Artificial intelligence",
      "Autism",
      "Autism spectrum disorder",
      "Computer science",
      "Convolutional neural network",
      "Developmental psychology",
      "Feature (linguistics)",
      "Functional magnetic resonance imaging",
      "Linguistics",
      "Machine learning",
      "Mechanism (biology)",
      "Medicine",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Psychology",
      "Quantum mechanics",
      "Radiology",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Ji",
        "given_name": "Junzhong"
      },
      {
        "surname": "Xing",
        "given_name": "Xinying"
      },
      {
        "surname": "Yao",
        "given_name": "Yao"
      },
      {
        "surname": "Li",
        "given_name": "Junwei"
      },
      {
        "surname": "Zhang",
        "given_name": "Xiaodan"
      }
    ]
  },
  {
    "title": "Implementing transfer learning across different datasets for time series forecasting",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107617",
    "abstract": "Due to the extensive practical value of time series prediction, many excellent algorithms have been proposed. Most of these methods are developed assuming that massive labeled training data are available. However, this assumption might be invalid in some actual situations. To address this limitation, a transfer learning framework with deep architectures is proposed. Since convolutional neural network (CNN) owns favorable feature extraction capability and can implement parallelization more easily, we propose a deep transfer learning method resorting to the architecture of CNN, termed as DTr-CNN for short. It can effectively alleviate the available labeled data absence and leverage useful knowledge to the current prediction. Notably, in our method, transfer learning process is implemented across different datasets. For a given target domain, in real-world scenarios, relativity of truly available potential source datasets may not be obvious, which is challenging and rarely referred to in most existing time series prediction methods. Aiming at this problem, the incorporation of Dynamic Time Warping (DTW) and Jensen-Shannon (JS) divergence is adopted for the selection of the appropriate source domain. Effectiveness of the proposed method is empirically underpinned by the experiments conducted on one group of synthetic and two groups of practical datasets. Besides, an additional experiment on NN5 dataset is conducted.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320304209",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Data mining",
      "Deep learning",
      "Dynamic time warping",
      "Leverage (statistics)",
      "Machine learning",
      "Operating system",
      "Process (computing)",
      "Time series",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Ye",
        "given_name": "Rui"
      },
      {
        "surname": "Dai",
        "given_name": "Qun"
      }
    ]
  },
  {
    "title": "Probabilistic homogeneity for document image segmentation",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107591",
    "abstract": "In this paper we propose a novel probabilistic framework for document segmentation exploiting human perceptual recognition of text regions from complicated layouts. In particular, we conceptualize text homogeneity as the Gestalt pattern displayed in text regions, characterized by proximately and symmetrically arranged units with similar morphological and texture features. We model this pattern in the local region of a connected component (CC) using an hierarchical formulation, which simulates a random walk-and-check on a graph encoding the neighborhood of the CC. The proposed formulation allows an effective computation of what we call the probabilistic local text homogeneity (PLTH) using a weighted summation of the weights of the graph, which are derived from a probabilistic description of the homogeneity between neighboring CCs and computed through Bayesian cue integration. The proposed PLTH enables a multi-aspect analysis, where various primitives such as geometrical configuration, morphological features, texture characterization and location priors are integrated in one computational probabilistic model. This enables an effective text and non-text classification of CCs preceding any grouping process, which is currently absent in document segmentation. Experimental results show that our segmentation method based on the proposed PLTH model improves upon the state-of-the-art.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303940",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Bayesian probability",
      "Biology",
      "Computation",
      "Computer science",
      "Connected component",
      "Gestalt psychology",
      "Graph",
      "Homogeneity (statistics)",
      "Image segmentation",
      "Machine learning",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Perception",
      "Prior probability",
      "Probabilistic logic",
      "Segmentation",
      "Statistical model",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Lu",
        "given_name": "Tan"
      },
      {
        "surname": "Dooms",
        "given_name": "Ann"
      }
    ]
  },
  {
    "title": "Nonlocal patch similarity based heterogeneous remote sensing change detection",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107598",
    "abstract": "Change detection of heterogeneous remote sensing images is an important and challenging topic, which has found a wide range of applications in many fields, especially in the emergency situation resulting from nature disaster. However, the difference in imaging mechanism of heterogeneous sensors makes it difficult to carry out a direct comparison of images. In this paper, we propose a new change detection method based on similarity measurement between heterogeneous images. The method constructs a graph for each patch based on the nonlocal patch similarity to establish a connection between heterogeneous data, and then measures the change level by measuring how much the graph structure of one image still conforms to that of the other image. The graph structures are compared in the same domain, so it can avoid the leakage of heterogeneous data and bring more robust change detection results. Experiments demonstrate the effective performance of the proposed nonlocal patch similarity based heterogeneous change detection method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320304015",
    "keywords": [
      "Artificial intelligence",
      "Change detection",
      "Composite material",
      "Computer science",
      "Data mining",
      "Graph",
      "Image (mathematics)",
      "Materials science",
      "Pattern recognition (psychology)",
      "Range (aeronautics)",
      "Similarity (geometry)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Sun",
        "given_name": "Yuli"
      },
      {
        "surname": "Lei",
        "given_name": "Lin"
      },
      {
        "surname": "Li",
        "given_name": "Xiao"
      },
      {
        "surname": "Sun",
        "given_name": "Hao"
      },
      {
        "surname": "Kuang",
        "given_name": "Gangyao"
      }
    ]
  },
  {
    "title": "An application of cyber-physical system and multi-agent technology to demand-side management systems",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2020.09.004",
    "abstract": "This paper presents a Cyber-Physical System (CPS) based on Multi-Agent System (MAS) technology to implement an intelligent Demand-Side Management system (DSM). This system has been applied to optimize the power consumption of a public building (a University Facility) powered by a photovoltaic plant with the purpose of maintaining the same comfort level for users. This CPS is composed of a sensor network where in each node there is a computing unit handling various sensors and actuators. Each physical node is installed in a room of the building and it is twinned with an intelligent agent in the virtual world where the MAS is implemented. In this MAS the various agents carry out a cooperative strategy to meet its own local goal (grant a good Human Thermal Comfort (HTC) level for users present in its environment) compatibly with the global one (minimize the power consumption). The results show that the proposed system can be the consumer's key to maximize the sustainability and profitability of self-production energy plants.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520303366",
    "keywords": [
      "Actuator",
      "Artificial intelligence",
      "Building management system",
      "Business",
      "Computer network",
      "Computer science",
      "Computer security",
      "Control (management)",
      "Cyber-physical system",
      "Electrical engineering",
      "Embedded system",
      "Energy (signal processing)",
      "Energy management",
      "Engineering",
      "Finance",
      "Key (lock)",
      "Mathematics",
      "Multi-agent system",
      "Node (physics)",
      "Operating system",
      "Photovoltaic system",
      "Profitability index",
      "Statistics",
      "Structural engineering",
      "Systems engineering",
      "Wireless sensor network"
    ],
    "authors": [
      {
        "surname": "Amato",
        "given_name": "Alberto"
      },
      {
        "surname": "Quarto",
        "given_name": "Alessandro"
      },
      {
        "surname": "Di Lecce",
        "given_name": "Vincenzo"
      }
    ]
  },
  {
    "title": "Multi-dimensional clustering through fusion of high-order similarities",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108108",
    "abstract": "Clustering objects with heterogeneous attributes captured from different dimensions remains challenging in integrating the multiple dimensional information. Most of the current multi-dimensional clustering models pin on direct sample-wised similarity and fail to exploit hidden mutual affinity among different sampling spaces. Thus, it is hard to capture a legible cluster structure. To tackle this issue, we propose a High-order multi-dimensional Spectral Clustering method (HSC). The proposed HSC aims to learn a high-order similarity to characterize the intrinsic relationship among different dimensional spaces instead of the ordinary similarity. It then performs a clustering task within a latent space by jointly learning the high-order similarity and ordinary similarity. Extensive experiments over synthetic and real-world data sets show that the proposed HSC outperforms benchmark multi-dimensional methods in most scenarios and is capable of revealing a reliable structure concealed across multi-dimensional spaces.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321002958",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Cluster analysis",
      "Clustering high-dimensional data",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Data mining",
      "Exploit",
      "Filter (signal processing)",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Sampling (signal processing)",
      "Similarity (geometry)",
      "Space (punctuation)",
      "Spectral clustering"
    ],
    "authors": [
      {
        "surname": "Peng",
        "given_name": "Hong"
      },
      {
        "surname": "Wang",
        "given_name": "Haiyan"
      },
      {
        "surname": "Hu",
        "given_name": "Yu"
      },
      {
        "surname": "Zhou",
        "given_name": "Weiwei"
      },
      {
        "surname": "Cai",
        "given_name": "Hongmin"
      }
    ]
  },
  {
    "title": "Video saliency prediction using enhanced spatiotemporal alignment network",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107615",
    "abstract": "Due to a variety of motions across different frames, it is highly challenging to learn an effective spatiotemporal representation for accurate video saliency prediction (VSP). To address this issue, we develop an effective spatiotemporal feature alignment network tailored to VSP, mainly including two key sub-networks: a multi-scale deformable convolutional alignment network (MDAN) and a bidirectional convolutional Long Short-Term Memory (Bi-ConvLSTM) network. The MDAN learns to align the features of the neighboring frames to the reference one in a coarse-to-fine manner, which can well handle various motions. Specifically, the MDAN owns a pyramidal feature hierarchy structure that first leverages deformable convolution (Dconv) to align the lower-resolution features across frames, and then aggregates the aligned features to align the higher-resolution features, progressively enhancing the features from top to bottom. The output of MDAN is then fed into the Bi-ConvLSTM for further enhancement, which captures the useful long-time temporal information along forward and backward timing directions to effectively guide attention orientation shift prediction under complex scene transformation. Finally, the enhanced features are decoded to generate the predicted saliency map. The proposed model is trained end-to-end without any intricate post processing. Extensive evaluations on four VSP benchmark datasets demonstrate that the proposed method achieves favorable performance against state-of-the-art methods. The source codes and all the results will be released at https://github.com/cj4L/ESAN-VSP.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320304180",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Encoding (memory)",
      "Feature (linguistics)",
      "Frame (networking)",
      "Gene",
      "Geodesy",
      "Geography",
      "Geometry",
      "Key (lock)",
      "Law",
      "Linguistics",
      "Mathematics",
      "Orientation (vector space)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Telecommunications",
      "Transformation (genetics)"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Jin"
      },
      {
        "surname": "Song",
        "given_name": "Huihui"
      },
      {
        "surname": "Zhang",
        "given_name": "Kaihua"
      },
      {
        "surname": "Liu",
        "given_name": "Bo"
      },
      {
        "surname": "Liu",
        "given_name": "Qingshan"
      }
    ]
  },
  {
    "title": "DenMune: Density peak based clustering using mutual nearest neighbors",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107589",
    "abstract": "Many clustering algorithms fail when clusters are of arbitrary shapes, of varying densities, or the data classes are unbalanced and close to each other, even in two dimensions. A novel clustering algorithm “DenMune” is presented to meet this challenge. It is based on identifying dense regions using mutual nearest neighborhoods of size K, where K is the only parameter required from the user, besides obeying the mutual nearest neighbor consistency principle. The algorithm is stable for a wide range of values of K. Moreover, it is able to automatically detect and remove noise from the clustering process as well as detecting the target clusters. It produces robust results on various low and high dimensional datasets relative to several known state of the art clustering algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303927",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "CURE data clustering algorithm",
      "Canopy clustering algorithm",
      "Cluster analysis",
      "Clustering high-dimensional data",
      "Composite material",
      "Computer science",
      "Consistency (knowledge bases)",
      "Correlation clustering",
      "Data mining",
      "Image (mathematics)",
      "Materials science",
      "Mathematics",
      "Nearest-neighbor chain algorithm",
      "Noise (video)",
      "Pattern recognition (psychology)",
      "Range (aeronautics)",
      "Single-linkage clustering",
      "k-nearest neighbors algorithm"
    ],
    "authors": [
      {
        "surname": "Abbas",
        "given_name": "Mohamed"
      },
      {
        "surname": "El-Zoghabi",
        "given_name": "Adel"
      },
      {
        "surname": "Shoukry",
        "given_name": "Amin"
      }
    ]
  },
  {
    "title": "Topological optimization of the DenseNet with pretrained-weights inheritance and genetic channel selection",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107608",
    "abstract": "Convolutional neural networks (CNNs) have been successfully applied in many computer vision applications [1], especially in image classification tasks, where most of the structures have been designed manually. With the aid of skip connection and dense connection, the depths of the models are becoming “deeper” and the filters of layers are getting “wider” in order to tackle the challenge of large-scale datasets. However, large-scale models in convolutional layers become inefficient due to the redundant channels from input feature maps. In this paper, we aim to automatically optimize the topology of the DenseNet, in which unnecessary convolutional kernels are reduced. To achieve this, we present a training pipeline that generates the network structure using a genetic algorithm. We first propose two encoding methods that can represent the structure of the model using a fixed-length binary string. A three-step based evolutionary process consisting of selection, crossover, and mutation is proposed to optimize the structure. We also present a pretrained weight inheritance method which can largely reduce the total time consumption of the genetic process. Experimental results have demonstrated that our proposed model can achieve comparable accuracy to the state-of-the-art models, across a wide range of image recognition and classification datasets, whilst significantly reducing the number of parameters.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320304118",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Channel (broadcasting)",
      "Combinatorics",
      "Computer science",
      "Gene",
      "Genetics",
      "Inheritance (genetic algorithm)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Selection (genetic algorithm)",
      "Telecommunications",
      "Topology (electrical circuits)"
    ],
    "authors": [
      {
        "surname": "Fang",
        "given_name": "Zhenyu"
      },
      {
        "surname": "Ren",
        "given_name": "Jinchang"
      },
      {
        "surname": "Marshall",
        "given_name": "Stephen"
      },
      {
        "surname": "Zhao",
        "given_name": "Huimin"
      },
      {
        "surname": "Wang",
        "given_name": "Song"
      },
      {
        "surname": "Li",
        "given_name": "Xuelong"
      }
    ]
  },
  {
    "title": "SLiKER: Sparse loss induced kernel ensemble regression",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107587",
    "abstract": "Kernel ridge regression (KRR) is an efficient method for regression task. However, KRR has a deficiency in finding appropriate type of kernel functions and their parameters. To overcome this shortcoming, a novel kernel ensemble framework is developed. In this ensemble framework, each kernel regressor is associated with a weight that can be adaptively determined according to its contribution to the regression result. By this way, more appropriate kernels and more accurate parameters can be learned directly from data without any manual intervention, which results in better performance in regression. In addition, to overcome the influence of existing outliers, the regressor loss is modeled as a sparse signal, thus a Sparse Loss induced Kernel Ensemble Regression (SLiKER) method is obtained. Experimental results on several UCI regression and computer vision datasets show that our proposed approach obtains best regression and classification performances among the state-of-art comparative methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303903",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Ensemble learning",
      "Kernel (algebra)",
      "Kernel method",
      "Kernel regression",
      "Machine learning",
      "Mathematics",
      "Outlier",
      "Pattern recognition (psychology)",
      "Regression",
      "Regression analysis",
      "Statistics",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Shen",
        "given_name": "Xiang-Jun"
      },
      {
        "surname": "Ni",
        "given_name": "ChengGong"
      },
      {
        "surname": "Wang",
        "given_name": "Liangjun"
      },
      {
        "surname": "Zha",
        "given_name": "Zheng-Jun"
      }
    ]
  },
  {
    "title": "Two-phase Dynamic Routing for Micro and Macro-level Equivariance in Multi-Column Capsule Networks",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107595",
    "abstract": "The capability of multi column convolutional networks in identifying local invariant features helps improve its performance on image classification tasks to a large extent. Suppression of non maximal activations in a convolutional network, however, can lead to loss of valuable information, as scalar activations typically only ,encode the presence (or absence) of a feature in an input image, providing no additional information. Capsule networks, on other hand, learn richer representations by propagating non-maximal activations to higher layers, encoding the agreement between neurons at various layers on the presence (or absence) of a feature into a fixed-length vector. Traditional capsule networks, however encodes agreements for micro and macro-level features of an input image with same precedence. Such an uniform agreement protocol can hinder the repsentation capability of a network, especially for datasets that contain objects with independently deformable components. To address this, we propose a novel two-phase dynamic routing protocol that computes agreements between neurons at various layers for micro and macro-level features, following a hierarchical learning paradigm. Experiments on seven publicly available datasets show that a multi-column capsule network that encodes an input image following our routing protocol performs competitively or better than contemporary multi-column convolutional architectures andtraditional capsule networks on a classification task.Implementations of the networks used in this paper have been made available at: github.com/DVLP-CMATERJU/TwoPhaseDynamicRouting.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303988",
    "keywords": [
      "Adaptive routing",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Column (typography)",
      "Computer network",
      "Computer science",
      "Convolutional neural network",
      "Data mining",
      "Distributed computing",
      "ENCODE",
      "Feature (linguistics)",
      "Frame (networking)",
      "Gene",
      "Invariant (physics)",
      "Linguistics",
      "Macro",
      "Mathematical physics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Routing (electronic design automation)",
      "Routing protocol",
      "Static routing"
    ],
    "authors": [
      {
        "surname": "Mandal",
        "given_name": "Bodhisatwa"
      },
      {
        "surname": "Sarkhel",
        "given_name": "Ritesh"
      },
      {
        "surname": "Ghosh",
        "given_name": "Swarnendu"
      },
      {
        "surname": "Das",
        "given_name": "Nibaran"
      },
      {
        "surname": "Nasipuri",
        "given_name": "Mita"
      }
    ]
  },
  {
    "title": "GridMix: Strong regularization through local context mapping",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107594",
    "abstract": "Recently developed regularization techniques improve the networks generalization by only considering the global context. Therefore, the network tends to focus on a few most discriminative subregions of an image for prediction accuracy, leading the network being sensitive to unseen or noisy data. To address this disadvantage, we introduce the concept of local context mapping by predicting patch-level labels and combine it with a method of local data augmentation by grid-based mixing, called GridMix. Through our analysis of intermediate representations, we show that our GridMix can effectively regularize the network model. Finally, our evaluation results indicate that GridMix outperforms state-of-the-art techniques in classification and adversarial robustness, and it achieves a comparable performance in weakly supervised object localization.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303976",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Discriminative model",
      "Gene",
      "Geometry",
      "Grid",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Regularization (linguistics)",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Baek",
        "given_name": "Kyungjune"
      },
      {
        "surname": "Bang",
        "given_name": "Duhyeon"
      },
      {
        "surname": "Shim",
        "given_name": "Hyunjung"
      }
    ]
  },
  {
    "title": "Combined center dispersion loss function for deep facial expression recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2020.11.002",
    "abstract": "We propose a combined center dispersion loss function to reduce the intra-class variations and inter-class similarities of facial expression datasets and achieve high accuracy in facial expression recognition. Because of the lack of data, we strategically combine four publicly available facial expression datasets for training. Moreover, we propose an incremental cosine annealing method for deploying multiple models trained with incremental learning rates and ensemble predictions for achieving better accuracy. This method also reduces the computational cost and yields ensemble predictions of varied models, instead of similar models, that are trained with the same learning rates. We train our methods using the VGGFace network and achieve an accuracy of 74.71% on the FER2013 test set.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520304074",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Class (philosophy)",
      "Computer science",
      "Deep learning",
      "Evolutionary biology",
      "Expression (computer science)",
      "Facial expression",
      "Facial expression recognition",
      "Facial recognition system",
      "Function (biology)",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Programming language",
      "Set (abstract data type)",
      "Simulated annealing",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Nanda",
        "given_name": "Abhilasha"
      },
      {
        "surname": "Im",
        "given_name": "Woobin"
      },
      {
        "surname": "Choi",
        "given_name": "Key-Sun"
      },
      {
        "surname": "Yang",
        "given_name": "Hyun Seung"
      }
    ]
  },
  {
    "title": "Learning modulation filter networks for weak signal detection in noise",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107590",
    "abstract": "Weak signal detection is a challenging yet significant problem in the field of radio communication. Although hand-crafted filters are widely used in signal processing, they are challenged by the weak signal detection task with unknown background noise especially in the range of 0-5dB. In this paper, we propose the learning modulation filter networks (LMFNs) to improve the detection performance. The approach is based on a two-stage optimization scheme which addresses filter learning, attention mechanism and classification in a unified framework. Modulation filters are built to enhance the capacity of the learned filters, and the attention mechanism further characterizes the saliency properties of the input signal. LMFNs reduce the storage size of the network while achieving the state-of-the-art performance by a significant margin compared to traditional cognitive radio approaches. We establish a weak signal dataset that contains unmanned aerial vehicle (UAV) communication signals in a real-terrain environment. The source code and dataset will be made publicly available soon.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303939",
    "keywords": [
      "Acoustics",
      "Artificial intelligence",
      "Code (set theory)",
      "Cognitive radio",
      "Computer science",
      "Computer vision",
      "Detection theory",
      "Detector",
      "Filter (signal processing)",
      "Image (mathematics)",
      "Matched filter",
      "Modulation (music)",
      "Noise (video)",
      "Physics",
      "Programming language",
      "SIGNAL (programming language)",
      "Set (abstract data type)",
      "Speech recognition",
      "Telecommunications",
      "Wireless"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Duona"
      },
      {
        "surname": "Ding",
        "given_name": "Wenrui"
      },
      {
        "surname": "Zhang",
        "given_name": "Baochang"
      },
      {
        "surname": "Liu",
        "given_name": "Chunhui"
      },
      {
        "surname": "Han",
        "given_name": "Jungong"
      },
      {
        "surname": "Doermann",
        "given_name": "David"
      }
    ]
  },
  {
    "title": "Towards using count-level weak supervision for crowd counting",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107616",
    "abstract": "Most existing crowd counting methods require object location-level annotation which is labor-intensive and time-consuming to obtain. In contrast, weaker annotations that only label the total count of objects can be easy to obtain in many practical scenarios. This paper focuses on the problem of weakly-supervised crowd counting which learns a model from a small amount of location-level annotations (fully-supervised) and a large amount of count-level annotations (weakly-supervised). Our study reveals that the most straightforward, that is, directly regressing the integral of density map to the object count, fails to provide satisfactory performance. As an alternative solution, we propose a method by taking advantage of the fact that the total count can be estimated via different-but-equivalent density maps. Our key idea is to enforce the consistency between those density maps and total object count on weakly labeled images as regularization terms. We realize this idea by using multiple density map estimation branches and a carefully devised asymmetry training strategy, called Multiple Auxiliary Tasks Training (MATT). Through extensive experiments on existing datasets and a newly proposed dataset, we validate the effectiveness of the proposed weakly-supervised method and demonstrate its superior performance over existing solutions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320304192",
    "keywords": [
      "Annotation",
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Consistency (knowledge bases)",
      "Contrast (vision)",
      "Data mining",
      "Key (lock)",
      "Machine learning",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Regularization (linguistics)"
    ],
    "authors": [
      {
        "surname": "Lei",
        "given_name": "Yinjie"
      },
      {
        "surname": "Liu",
        "given_name": "Yan"
      },
      {
        "surname": "Zhang",
        "given_name": "Pingping"
      },
      {
        "surname": "Liu",
        "given_name": "Lingqiao"
      }
    ]
  },
  {
    "title": "Clothes image caption generation with attribute detection and visual attention model",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2020.12.001",
    "abstract": "Fashion is a multi-billion-dollar industry, which is directly related to social, cultural, and economic implications in the real world. While computer vision has demonstrated remarkable success in the applications of the fashion domain, natural language processing technology has become contributed in the area, so that it can build the connection between clothes image and human semantic understandings. An element work for combing images and language understanding is how to generate a natural language sentence that accurately summarizes the contents of a clothes image. In this paper, we develop a joint attribute detection and visual attention framework for clothes image captioning. Specifically, in order to involve more attributes of clothes to learn, we first utilize a pre-trained Convolutional Neural Network (CNN) to learn the feature that can characterize more information about clothing attribute. Based on such learned feature, we then adopt an encoder/decoder framework, where we first encoder the feature of clothes and then and input it to a language Long Short-Term Memory(LSTM) model for decoding the clothes descriptions. The method greatly enhances the performance of clothes image captioning and reduces the misleading attention. Extensive simulations based on real-world data verify the effectiveness of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520304281",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Closed captioning",
      "Clothing",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Decoding methods",
      "Encoder",
      "Feature (linguistics)",
      "History",
      "Image (mathematics)",
      "Linguistics",
      "Natural language",
      "Natural language processing",
      "Operating system",
      "Philosophy",
      "Programming language",
      "Semantics (computer science)",
      "Sentence",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Xianrui"
      },
      {
        "surname": "Ye",
        "given_name": "Zhiling"
      },
      {
        "surname": "Zhang",
        "given_name": "Zhao"
      },
      {
        "surname": "Zhao",
        "given_name": "Mingbo"
      }
    ]
  },
  {
    "title": "A theoretical justification of warping generation for dewarping using CNN",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107621",
    "abstract": "Dewarping is a necessary preprocessing step to recognize text from a distorted camera captured document image. According to recent literature, deep learning-based approaches perform with higher accuracy in similar domains. The deep learning-based neural networks are not yet fully explored in the domain of dewarping. To fill this gap, we propose a dewarping approach based on the convolutional neural network. A large number of images are required to train such networks. However, it is a tedious job to capture such a large number of images. Hence, it is required to generate synthetic warped images for the training phase of the deep learning-based neural network. The existing synthetic warped image generation methods are heuristic-based. In this paper, we propose a novel mathematical model for the generation of warped images. The proposed model takes some parameters such as depth of the surface, camera angle, and camera position and generates the corresponding warped image. These parameters are the ground truth for that particular warped image. We use a Convolutional Neural Network (CNN) based model to estimate the warping parameters from a 2D warped image for dewarping. In the training phase of CNN based model, the synthetic images and their corresponding ground truth are used. Next, the trained model is used to dewarp the unknown warped images. The performance of the proposed warping model is analyzed. Finally, the proposed dewarping method is compared with existing approaches. In both cases, the results are encouraging.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320304246",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Deep learning",
      "Ground truth",
      "Heuristic",
      "Image (mathematics)",
      "Image warping",
      "Pattern recognition (psychology)",
      "Preprocessor"
    ],
    "authors": [
      {
        "surname": "Garai",
        "given_name": "Arpan"
      },
      {
        "surname": "Biswas",
        "given_name": "Samit"
      },
      {
        "surname": "Mandal",
        "given_name": "Sekhar"
      }
    ]
  },
  {
    "title": "Comparative analysis of image classification algorithms based on traditional machine learning and deep learning",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2020.07.042",
    "abstract": "Image classification is a hot research topic in today's society and an important direction in the field of image processing research. SVM is a very powerful classification model in machine learning. CNN is a type of feedforward neural network that includes convolution calculation and has a deep structure. It is one of the representative algorithms of deep learning. Taking SVM and CNN as examples, this paper compares and analyzes the traditional machine learning and deep learning image classification algorithms. This study found that when using a large sample mnist dataset, the accuracy of SVM is 0.88 and the accuracy of CNN is 0.98; when using a small sample COREL1000 dataset, the accuracy of SVM is 0.86 and the accuracy of CNN is 0.83. The experimental results in this paper show that traditional machine learning has a better solution effect on small sample data sets, and deep learning framework has higher recognition accuracy on large sample data sets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520302981",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Contextual image classification",
      "Convolutional neural network",
      "Deep learning",
      "Field (mathematics)",
      "Image (mathematics)",
      "MNIST database",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pure mathematics",
      "Sample (material)",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Pin"
      },
      {
        "surname": "Fan",
        "given_name": "En"
      },
      {
        "surname": "Wang",
        "given_name": "Peng"
      }
    ]
  },
  {
    "title": "Blind image deblurring based on the sparsity of patch minimum information",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107597",
    "abstract": "Blind image deblurring is a very challenging inverse problem due to the severe ill-posedness caused by the unknown kernel and the latent clear image. To tackle this problem, appropriate smoothing regularizations and image priors are usually employed and incorporated into the associated variational models to alleviate the inherent ill-posedness. In this paper, we first propose a strongly imposed zero patch minimum constraint for the latent image, which helps alleviate the ill-posedness of the inverse problem for blind image deblurring. Then, we retrieve important fine details by assigning the patch minimum information obtained from the blurred image back to the latent image to further enhance its structure. Finally, we introduce an adaptive regularizer which was shown to have significantly better edge-preserving property than the total variation regularizer for the image restoration of degraded images. Operator splitting techniques are used to accomplish an efficient numerical implementation of the proposed variational model. A number of numerical experiments and comparisons with some state-of-the-art methods are conducted to demonstrate the effective performance of the newly proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320304003",
    "keywords": [
      "Algorithm",
      "Applied mathematics",
      "Artificial intelligence",
      "Bayesian probability",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Constraint (computer-aided design)",
      "Deblurring",
      "Geometry",
      "Image (mathematics)",
      "Image processing",
      "Image restoration",
      "Inverse problem",
      "Kernel (algebra)",
      "Latent image",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Prior probability",
      "Smoothing",
      "Well-posed problem"
    ],
    "authors": [
      {
        "surname": "Hsieh",
        "given_name": "Po-Wen"
      },
      {
        "surname": "Shao",
        "given_name": "Pei-Chiang"
      }
    ]
  },
  {
    "title": "Parametric recurrence quantification analysis of autoregressive processes for pattern recognition in multichannel electroencephalographic data",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107572",
    "abstract": "Recurrence quantification analysis (RQA) is an acknowledged method for the characterization of experimental time series. We propose a parametric version of RQA, pRQA, allowing a fast processing of spatial arrays of time series, once each is modeled by an autoregressive stochastic process. This method relies on the analytical derivation of asymptotic expressions for five current RQA measures as a function of the model parameters. By avoiding the construction of the recurrence plot of the time series, pRQA is computationally efficient. As a proof of principle, we apply pRQA to pattern recognition in multichannel electroencephalographic (EEG) data from a patient with a brain tumor.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303757",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Autoregressive model",
      "Biology",
      "Computer science",
      "Electroencephalography",
      "Machine learning",
      "Mathematics",
      "Nonlinear system",
      "Paleontology",
      "Parametric statistics",
      "Pattern recognition (psychology)",
      "Physics",
      "Plot (graphics)",
      "Psychiatry",
      "Psychology",
      "Quantum mechanics",
      "Recurrence plot",
      "Recurrence quantification analysis",
      "Series (stratigraphy)",
      "Statistics",
      "Stochastic process",
      "Time series"
    ],
    "authors": [
      {
        "surname": "Ramdani",
        "given_name": "Sofiane"
      },
      {
        "surname": "Boyer",
        "given_name": "Anthony"
      },
      {
        "surname": "Caron",
        "given_name": "Stéphane"
      },
      {
        "surname": "Bonnetblanc",
        "given_name": "François"
      },
      {
        "surname": "Bouchara",
        "given_name": "Frédéric"
      },
      {
        "surname": "Duffau",
        "given_name": "Hugues"
      },
      {
        "surname": "Lesne",
        "given_name": "Annick"
      }
    ]
  },
  {
    "title": "Weakly supervised image classification and pointwise localization with graph convolutional networks",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107596",
    "abstract": "In computer vision, the research community has been looking to how to benefit from weakly supervised learning that utilizes easily obtained image-level labels to train neural network models. The existing deep convolutional neural networks for weakly supervised learning, however, generally do not fully exploit the label dependencies in an image. To make full use of this information, in this paper, we propose a new framework for weakly supervised learning of deep convolutional neural networks, introducing graph convolutional networks to capture the semantic label co-occurrence in an image. Moreover, we propose a novel initialization method for label embedding in graph convolutional networks, which enables a smoother optimization for interrelationships learning. Extensive experiments and comparisons on four public benchmark datasets (PASCAL VOC 2007, PASCAL VOC 2012, Microsoft COCO, and NUS-WIDE) show the superior performance of our approach in both image classification and weakly supervised pointwise object localization. These results lead us to conclude that the label dependencies in the input image can provide valuable evidence for learning strongly localized features.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032030399X",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Computer science",
      "Contextual image classification",
      "Convolutional neural network",
      "Deep learning",
      "Embedding",
      "Geodesy",
      "Geography",
      "Graph",
      "Image (mathematics)",
      "Initialization",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pascal (unit)",
      "Pattern recognition (psychology)",
      "Pointwise",
      "Programming language",
      "Supervised learning",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Yongsheng"
      },
      {
        "surname": "Chen",
        "given_name": "Wenyu"
      },
      {
        "surname": "Qu",
        "given_name": "Hong"
      },
      {
        "surname": "Mahmud",
        "given_name": "S.M. Hasan"
      },
      {
        "surname": "Miao",
        "given_name": "Kebin"
      }
    ]
  },
  {
    "title": "CADN: A weakly supervised learning-based category-aware object detection network for surface defect detection",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107571",
    "abstract": "Large-scale data with human annotations is of crucial importance for training deep convolutional neural network (DCNN) to ensure stable and reliable performance. However, accurate annotations, such as bounding box and pixel-level annotations, demand expensive labeling efforts, which has prevented wide application of DCNN in industries. Focusing on the problem of surface defect detection, this paper proposes a weakly supervised learning method named Category-Aware object Detection network (CADN) to tackle the dilemma. CADN is trained with image tag annotations only and performs image classification and defect localization simultaneously. The weakly supervised learning is achieved by extracting category-aware spatial information in a classification pipeline. CADN could be equipped with either a lighter or a larger backbone network as the feature extractor resulting in better real-time performance or higher accuracy. To address the two conflicting objectives simultaneously, both of which are significant concerns in industrial applications, knowledge distillation strategy is adopted to force the learned features of a lighter CADN to mimic that of a larger CADN. Accordingly, the accuracy of the lighter CADN is improved while high real-time performance is maintained. The proposed approach is verified on our own defect dataset as well as on an open-source defect dataset. As demonstrated, satisfied performance is achieved by the proposed method, which could meet industrial requirements completely. Meanwhile, the method minimizes human efforts involved in image labelling, thus promoting the applications of DCNN in industries.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303745",
    "keywords": [
      "Artificial intelligence",
      "Bounding overwatch",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Engineering",
      "Extractor",
      "Feature (linguistics)",
      "Image (mathematics)",
      "Linguistics",
      "Machine learning",
      "Minimum bounding box",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pipeline (software)",
      "Process engineering",
      "Programming language"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Jiabin"
      },
      {
        "surname": "Su",
        "given_name": "Hu"
      },
      {
        "surname": "Zou",
        "given_name": "Wei"
      },
      {
        "surname": "Gong",
        "given_name": "Xinyi"
      },
      {
        "surname": "Zhang",
        "given_name": "Zhengtao"
      },
      {
        "surname": "Shen",
        "given_name": "Fei"
      }
    ]
  },
  {
    "title": "Gradient clustering algorithm based on deep learning aerial image detection",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2020.09.032",
    "abstract": "In recent years, computer vision, especially deep learning, has been widely used in various fields. Through the deep learning aerial image detection gradient clustering algorithm automatic recognition, it can solve the limitations of manual shooting by humans, can shoot from a high altitude to a panoramic view of a specific area, and provide a more comprehensive solution. The traditional forest resource management and management work is mainly carried out by forestry personnel to carry out a large number of investigations and investigations on the forest. This method not only consumes a lot of manpower and material resources, but also does not have real-time nature. It is difficult to deal with all kinds of forest management. Problems, causing unnecessary losses. In this regard, this paper proposes an aerial image change detection algorithm based on H-KFCM, and designs related experiments to verify and demonstrate the performance of the algorithm. In this paper, we conduct a parallel study based on deep learning on the gradient clustering algorithm of deep learning in aerial image processing. By using CUDA (Compute Unified Device Architecture) to perform large-scale parallel processing of aerial data. Can greatly shorten the time to obtain results, improve the efficiency of relevant personnel. Experiment analysis. It can be seen from the results that the deep learning parallelization program implemented in this paper has a faster calculation speed and uses less time in high-resolution images, and has a good acceleration ratio compared to the CPU.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520303640",
    "keywords": [
      "Aerial image",
      "Algorithm",
      "Artificial intelligence",
      "CUDA",
      "Cluster analysis",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Image (mathematics)",
      "Image processing",
      "Machine learning",
      "Parallel computing"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Ning"
      },
      {
        "surname": "Guo",
        "given_name": "Bin"
      },
      {
        "surname": "Li",
        "given_name": "Xinju"
      },
      {
        "surname": "Min",
        "given_name": "Xiangyu"
      }
    ]
  },
  {
    "title": "A generic shift-norm-activation approach for deep learning",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107609",
    "abstract": "Deep learning has received increasing attention in the last decade. Its amazing success, is partly attributed to the evolution of normalization and activation techniques. However, less works have devoted to explore both modules together. This work, therefore, aims at pushing for a deeper understanding on the effect of normalization and activation together analytically. We design a generic method which integrates both normalization and activation together as a whole, named as the Generic Shift-Normalization-Activation Approach (GSNA), in reserving richer information propagation in neural networks. A rigorous mathematical analysis was performed to investigate the benefits of the designed method, such as its computation complexity, performance potential as well as optimization over trainable parameter initialization. Further, extensive experiments are conducted to demonstrate the superiority and generality of the designed method in many computer vision benchmarking tasks, such as CIFAR-10/100, SVHN, ImageNet32 × 32, etc. To explore its generality, we also conduct some experiments on natural language understanding tasks like text classification, natural language inference, and some variational generative task as well. More interestingly, GSNA can be naturally incorporated into the existing neural networks with arbitrary architectures, demonstrating its generic effectiveness in deep learning field.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032030412X",
    "keywords": [
      "Algorithm",
      "Anthropology",
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmarking",
      "Business",
      "Computation",
      "Computer science",
      "Deep learning",
      "Deep neural networks",
      "GRASP",
      "Generality",
      "Generative grammar",
      "Inference",
      "Initialization",
      "Machine learning",
      "Marketing",
      "Normalization (sociology)",
      "Programming language",
      "Psychology",
      "Psychotherapist",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Zhi"
      },
      {
        "surname": "Ho",
        "given_name": "Pin-Han"
      }
    ]
  },
  {
    "title": "A comprehensive survey of multi-view video summarization",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107567",
    "abstract": "There has been an exponential growth in the amount of visual data on a daily basis acquired from single or multi-view surveillance camera networks. This massive amount of data requires efficient mechanisms such as video summarization to ensure that only significant data are reported and the redundancy is reduced. Multi-view video summarization (MVS) is a less redundant and more concise way of providing information from the video content of all the cameras in the form of either keyframes or video segments. This paper presents an overview of the existing strategies proposed for MVS, including their advantages and drawbacks. Our survey covers the genericsteps in MVS, such as the pre-processing of video data, feature extraction, and post-processing followed by summary generation. We also describe the datasets that are available for the evaluation of MVS. Finally, we examine the major current issues related to MVS and put forward the recommendations for future research 1 [1] https://github.com/tanveer-hussain/MVS-Survey .",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303708",
    "keywords": [
      "Artificial intelligence",
      "Automatic summarization",
      "Computer science",
      "Data mining",
      "Feature (linguistics)",
      "Feature extraction",
      "Information retrieval",
      "Linguistics",
      "Multimedia",
      "Operating system",
      "Philosophy",
      "Redundancy (engineering)",
      "Video processing"
    ],
    "authors": [
      {
        "surname": "Hussain",
        "given_name": "Tanveer"
      },
      {
        "surname": "Muhammad",
        "given_name": "Khan"
      },
      {
        "surname": "Ding",
        "given_name": "Weiping"
      },
      {
        "surname": "Lloret",
        "given_name": "Jaime"
      },
      {
        "surname": "Baik",
        "given_name": "Sung Wook"
      },
      {
        "surname": "de Albuquerque",
        "given_name": "Victor Hugo C."
      }
    ]
  },
  {
    "title": "Weighted bilateral K-means algorithm for fast co-clustering and fast spectral clustering",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107560",
    "abstract": "Bipartite spectral graph partition (BSGP) is a school of the most well-known algorithms designed for the bipartite graph partition problem. It is also a fundamental mathematical model widely used in the tasks of co-clustering and fast spectral clustering. In BSGP, the key is to find the minimal normalized cuts (Ncuts) of bipartite graph. However, the convolutional BSGP algorithms usually need to use the singular value decomposition (SVD) to find the minimal Ncuts, which is computational prohibitive. Under this circumstance, the application range of those methods would be limited when the volume of the dataset is huge or the dimension of features is high. To overcome this problem, this paper proposes a novel weighted bilateral k-means (WBKM) algorithm and applies it for co-clustering and fast spectral clustering. Specifically, WBKM is a relaxation of the problem of finding the minimal Ncuts of bipartite graph, so it can be seen as a new solution for the minimal-Ncuts problem in bipartite graph. Different from the conventional relaxation ways, WBKM relaxes the minimal-Ncuts problem to a Non-negative decomposition problem which can be solved by an efficient iterative method. Therefore, the running speed of the proposed method is much faster. Besides, as our model can directly output the clustering results without any help of post-procedures, its solution tends to be more close to the ideal solution of the minimal-Ncuts problem than that of the conventional BSGP algorithms. To demonstrate the effectiveness and efficiency of the proposed method, extensive experiments on various types of datasets are conducted. Compared with other state-of-the-art methods, the proposed WBKM not only has faster computational speed, but also achieves more promising clustering results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303630",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Bipartite graph",
      "Cluster analysis",
      "Combinatorics",
      "Computer science",
      "Correlation clustering",
      "Graph",
      "Graph partition",
      "Mathematics",
      "Partition (number theory)",
      "Psychology",
      "Relaxation (psychology)",
      "Singular value decomposition",
      "Social psychology",
      "Spectral clustering",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Song",
        "given_name": "Kun"
      },
      {
        "surname": "Yao",
        "given_name": "Xiwen"
      },
      {
        "surname": "Nie",
        "given_name": "Feiping"
      },
      {
        "surname": "Li",
        "given_name": "Xuelong"
      },
      {
        "surname": "Xu",
        "given_name": "Mingliang"
      }
    ]
  },
  {
    "title": "Efficient densely connected convolutional neural networks",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107610",
    "abstract": "Recent works have shown that convolutional neural networks (CNNs) are parameter redundant, which limits the application of CNNs in Mobile devices with limited memory and computational resources. In this paper, two novel and efficient lightweight CNNs architectures are proposed, which are called DenseDsc and Dense2Net. Two proposed CNNs are densely connected and the dense connectivity facilitates feature re-use in the networks. Dense2Net adopts efficient group convolution and DenseDsc adopts more efficient depthwise separable convolution. The novel dense blocks of DenseDsc and Dense2Net improve the parameter efficiency. The proposed DenseDsc and Dense2Net are evaluated on highly competitive classification benchmark datasets (CIFAR and ImageNet). The experimental results show that DenseDsc and Dense2Net have higher accuracy than DenseNet with similar parameters or FLOPs. Compared with other efficient CNNs with less than 0.5 M parameters for CIFAR, Dense2Net and DenseDsc achieve state-of-the-art results on CIFAR-10 and CIFAR-100, respectively. DenseDsc and Dense2Net are very competitive in efficient CNNs with less than 1.0 M parameters on CIFAR. Furthermore, Dense2Net achieves state-of-the-art results on ImageNet in manual CNNs with less than 10 M parameters.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320304131",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Computer science",
      "Contextual image classification",
      "Convolution (computer science)",
      "Convolutional neural network",
      "FLOPS",
      "Feature (linguistics)",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Linguistics",
      "Parallel computing",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Guoqing"
      },
      {
        "surname": "Zhang",
        "given_name": "Meng"
      },
      {
        "surname": "Li",
        "given_name": "Jiaojie"
      },
      {
        "surname": "Lv",
        "given_name": "Feng"
      },
      {
        "surname": "Tong",
        "given_name": "Guodong"
      }
    ]
  },
  {
    "title": "Active k-labelsets ensemble for multi-label classification",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107583",
    "abstract": "The random k-labelsets ensemble (RAkEL) is a multi-label learning strategy that integrates many single-label learning models. Each single-label model is constructed using a label powerset (LP) technique based on a randomly generated size-k label subset. Although RAkEL can improve the generalization capability and reduce the complexity of the original LP method, the quality of the randomly generated label subsets could be low. On the one hand, the transformed classes may be difficult to separate in the feature space, negatively affecting the performance; on the other hand, the classes might be highly imbalanced, resulting in difficulties in using the existing single-label algorithms. To solve these problems, we propose an active k-labelsets ensemble (ACkEL) paradigm. Borrowing the idea of active learning, a label-selection criterion is proposed to evaluate the separability and balance level of the classes transformed from a label subset. Subsequently, by randomly selecting the first label or label subset, the remaining ones are iteratively chosen based on the proposed criterion. ACkEL can be realized in both the disjoint and overlapping modes, which adopt pool-based and stream-based frameworks, respectively. Experimental comparisons demonstrate the feasibility and effectiveness of the proposed methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303861",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Disjoint sets",
      "Ensemble learning",
      "Feature (linguistics)",
      "Feature selection",
      "Generalization",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Multi-label classification",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Random subspace method",
      "Selection (genetic algorithm)",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Ran"
      },
      {
        "surname": "Kwong",
        "given_name": "Sam"
      },
      {
        "surname": "Wang",
        "given_name": "Xu"
      },
      {
        "surname": "Jia",
        "given_name": "Yuheng"
      }
    ]
  },
  {
    "title": "Complex image processing with less data—Document image binarization by integrating multiple pre-trained U-Net modules",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107577",
    "abstract": "Artificial neural networks have been shown significant performance in various image-to-image conversion tasks. However, complex conversions often require a large number of images for model training. Therefore, we propose a convolutional model for image-to-image conversions using a pipeline of simpler image processing modules. To verify our proposed approach, we use a document image binarization as the task. Document image binarization is an important process that affects the accuracy of document analysis and recognition. In this paper, we propose a novel document binarization method called Cascading Modular U-Nets (CMU-Nets). CMU-Nets consist of pre-trained modular modules useful for overcoming the problem of a shortage of training images. We also propose a novel cascading scheme for improving overall cascading model performance. We verify the proposed model on all available Document Image Binarization Competition (DIBCO) and the Handwritten-DIBCO (H-DIBCO) datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303800",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Economic shortage",
      "Government (linguistics)",
      "Image (mathematics)",
      "Image processing",
      "Linguistics",
      "Modular design",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pipeline (software)",
      "Process (computing)",
      "Programming language"
    ],
    "authors": [
      {
        "surname": "Kang",
        "given_name": "Seokjun"
      },
      {
        "surname": "Iwana",
        "given_name": "Brian Kenji"
      },
      {
        "surname": "Uchida",
        "given_name": "Seiichi"
      }
    ]
  },
  {
    "title": "Iterative local re-ranking with attribute guided synthesis for face sketch recognition",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107579",
    "abstract": "Because of the large texture and spatial structure discrepancies between face sketches and photos, face sketch recognition becomes a challenging problem in face recognition community. For example, in law enforcement and security, the specific face sketch generation process could introduce some inevitable biases which results in poor face sketch recognition performance. In order to mimic the modality gap introduced by the biases during face sketch creation process, the novel iterative local re-ranking with attribute guided synthesis method is proposed for face sketch recognition, which does not require any extra manually annotation or human interaction. The clues of face attributes are utilized to generate images with varying local characteristic from probe sketches, which could help eliminate the unavoidable biases. Considering the special property of face sketches, the iterative local re-ranking algorithm is designed to encode the contextual information integrated with local invariant discriminative information for matching sketches with photos. Experimental results on multiple face sketch databases demonstrate that the proposed method achieves superior performances compared with state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303824",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Discriminative model",
      "Epistemology",
      "Face (sociological concept)",
      "Facial recognition system",
      "Gesture",
      "Gesture recognition",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Process (computing)",
      "Property (philosophy)",
      "Ranking (information retrieval)",
      "Sketch",
      "Sketch recognition",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Decheng"
      },
      {
        "surname": "Gao",
        "given_name": "Xinbo"
      },
      {
        "surname": "Wang",
        "given_name": "Nannan"
      },
      {
        "surname": "Peng",
        "given_name": "Chunlei"
      },
      {
        "surname": "Li",
        "given_name": "Jie"
      }
    ]
  },
  {
    "title": "Semi-automatic data annotation guided by feature space projection",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107612",
    "abstract": "Data annotation using visual inspection (supervision) of each training sample can be laborious. Interactive solutions alleviate this by helping experts propagate labels from a few supervised samples to unlabeled ones based solely on the visual analysis of their feature space projection (with no further sample supervision). We present a semi-automatic data annotation approach based on suitable feature space projection and semi-supervised label estimation. We validate our method on the popular MNIST dataset and on images of human intestinal parasites with and without fecal impurities, a large and diverse dataset that makes classification very hard. We evaluate two approaches for semi-supervised learning from the latent and projection spaces, to choose the one that best reduces user annotation effort and also increases classification accuracy on unseen data. Our results demonstrate the added-value of visual analytics tools that combine complementary abilities of humans and machines for more effective machine learning.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320304155",
    "keywords": [
      "Algorithm",
      "Analytics",
      "Annotation",
      "Artificial intelligence",
      "Artificial neural network",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Data mining",
      "Deep learning",
      "Feature (linguistics)",
      "Feature vector",
      "Linguistics",
      "MNIST database",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Projection (relational algebra)",
      "Sample (material)",
      "Semi-supervised learning",
      "Supervised learning",
      "Visual analytics",
      "Visualization"
    ],
    "authors": [
      {
        "surname": "Benato",
        "given_name": "Bárbara C."
      },
      {
        "surname": "Gomes",
        "given_name": "Jancarlo F."
      },
      {
        "surname": "Telea",
        "given_name": "Alexandru C."
      },
      {
        "surname": "Falcão",
        "given_name": "Alexandre X."
      }
    ]
  },
  {
    "title": "Multi-focus image fusion algorithm based on supervised learning for fully convolutional neural network",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2020.11.014",
    "abstract": "To improve the quality of multi-focus image fusion in photography applications, a multi-focus image fusion algorithm based on supervised learning for fully convolutional network is proposed. The aim of this algorithm is to make the neural network learn the complementary relationship between different focusing areas of source images, which is to select different focusing positions of the source images to synthesize a global clear image. In this algorithm, focusing images are constructed as training data. Dense connection and 1 × 1 convolution are used in the network to improve the understanding ability and efficiency of the network. The result of experiment shows that the proposed algorithm is superior to other contrast algorithms in both subjective visual evaluation and objective evaluation, and the quality of image fusion is significantly improved. Code is available at https://github.com/littlebaba/SF_MFIF.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520304256",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Code (set theory)",
      "Computer science",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Deep learning",
      "Focus (optics)",
      "Fusion",
      "Image (mathematics)",
      "Image fusion",
      "Image quality",
      "Linguistics",
      "Operating system",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Programming language",
      "Set (abstract data type)",
      "Source code"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Heng"
      },
      {
        "surname": "Zhang",
        "given_name": "Liming"
      },
      {
        "surname": "Jiang",
        "given_name": "Meirong"
      },
      {
        "surname": "Li",
        "given_name": "Yulong"
      }
    ]
  },
  {
    "title": "Standardization-refinement domain adaptation method for cross-subject EEG-based classification in imagined speech recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2020.11.013",
    "abstract": "Recent advances in imagined speech recognition from EEG signals have shown their capability of enabling a new natural form of communication, which is posed to improve the lives of subjects with motor disabilities. However, differences among subjects may be an obstacle to the applicability of a previously trained classifier to new users, since a significant amount of labeled samples must be acquired for each new user, making this process tedious and time-consuming. In this sense, unsupervised domain adaptation (UDA) methods, especially those based on deep learning (D-UDA), arise as a potential solution to address this issue by reducing the differences among feature distributions of subjects. It has been shown that the divergence in the marginal and conditional distributions must be reduced to encourage similar feature distributions. However, current D-UDA methods may become sensitive under adaptation scenarios where a low discriminative feature space among classes is given, reducing the accuracy performance of the classifier. To address this issue, we introduce a D-UDA method, named Standardization-Refinement Domain Adaptation (SRDA), which combines Adaptive Batch Normalization (AdaBN) with a novel loss function based on the variation of information (VOI), in order to build an adaptive classifier on EEG data corresponding to imagined speech. Our proposal, applied over two imagined speech datasets, resulted in SRDA outperforming standard classifiers for BCI and existing D-UDA methods, achieving accuracy performances of 61.02 ± 08.14% and 62.99 ± 04.78%, assessed using leave-one-out cross-validation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520304244",
    "keywords": [
      "Anthropology",
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Discriminative model",
      "Feature vector",
      "Machine learning",
      "Normalization (sociology)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Sociology",
      "Speech recognition",
      "Standardization"
    ],
    "authors": [
      {
        "surname": "Jiménez-Guarneros",
        "given_name": "Magdiel"
      },
      {
        "surname": "Gómez-Gil",
        "given_name": "Pilar"
      }
    ]
  },
  {
    "title": "Random walkers on morphological trees: A segmentation paradigm",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2020.11.001",
    "abstract": "The problem of image segmentation is often considered in the framework of graphs. In this context, two main paradigms exist: in the first, the vertices of a non-directed graph represent the pixels (leading e.g. to the watershed, the random walker or the graph cut approaches); in the second, the vertices of a directed graph represent the connected regions, leading to the so-called morphological trees (e.g. the component-trees or the trees of shapes). Various approaches have been proposed for carrying out segmentation from images modeled by such morphological trees, by computing cuts of these trees or by selecting relevant nodes from descriptive attributes. In this article, we propose a new way of carrying out segmentation from morphological trees. Our approach is dedicated to take advantage of the morphological tree of an image, enriched by multiple attributes in each node, by using maximally stable extremal regions and random walker paradigms for defining an optimal cut leading to a final segmentation. Experiments, carried out on multimodal medical images emphasize the potential relevance of this approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520304062",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Connected component",
      "Connected-component labeling",
      "Context (archaeology)",
      "Geography",
      "Graph",
      "Image segmentation",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pixel",
      "Random walker algorithm",
      "Scale-space segmentation",
      "Segmentation",
      "Theoretical computer science",
      "Tree (set theory)"
    ],
    "authors": [
      {
        "surname": "Alvarez Padilla",
        "given_name": "Francisco Javier"
      },
      {
        "surname": "Romaniuk",
        "given_name": "Barbara"
      },
      {
        "surname": "Naegel",
        "given_name": "Benoît"
      },
      {
        "surname": "Servagi-Vernat",
        "given_name": "Stephanie"
      },
      {
        "surname": "Morland",
        "given_name": "David"
      },
      {
        "surname": "Papathanassiou",
        "given_name": "Dimitri"
      },
      {
        "surname": "Passat",
        "given_name": "Nicolas"
      }
    ]
  },
  {
    "title": "An occlusion-resistant circle detector using inscribed triangles",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107588",
    "abstract": "Circle detection is a critical issue in pattern recognition and image analysis. Conventional geometry-based methods such as tangent or symmetry are sensitive to noise or occlusion. Area computation is more robust against noise, because it avoids differential calculations. Inspired by this characteristic, we present a novel method for fast circle detection using inscribed triangles. The proposed algorithm, which is robust to noise and resistant to occlusion, first extracts circular arcs by approximating line segments and identifying inflection points and sharp corners. To speed up the computation, irrelevant segments are filtered out through the triangle inequality. Arcs that belong to the same circle are then combined according to the position constraint and the inscribed triangle constraint. The circle parameters are further estimated by inscribed triangles based upon the Theil-Sen estimator and linear error refinement without the dependence of least-square fitting but still with the equivalent accuracy. Finally, candidate circles are verified to prune false positives through an inlier ratio rule, which jointly considers both distance and angle deviations. Extensive experiments are conducted on synthetic images including overlapping circles, and real images from four diverse datasets (three publicly available and one we built). Results are compared with those of representative state-of-the-art methods, and the proposed method is demonstrated to embraces several advantages: resistant to occlusion, more robust to noise, and better performance and efficiency.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303915",
    "keywords": [
      "Algorithm",
      "Arc (geometry)",
      "Arc length",
      "Artificial intelligence",
      "Computation",
      "Computer science",
      "Curvature",
      "Geometry",
      "Image (mathematics)",
      "Inscribed figure",
      "Mathematics",
      "Noise (video)",
      "Tangent"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Mingyang"
      },
      {
        "surname": "Jia",
        "given_name": "Xiaohong"
      },
      {
        "surname": "Yan",
        "given_name": "Dong-Ming"
      }
    ]
  },
  {
    "title": "Generalisations of stochastic supervision models",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107575",
    "abstract": "When the labelling information is not deterministic, traditional supervised learning algorithms cannot be applied. In this case, stochastic supervision models provide a valuable alternative to classification. However, these models are restricted in several aspects, which critically limits their applicability. In this paper, we provide four generalisations of stochastic supervision models, extending them to asymmetric assessments, multiple classes, feature-dependent assessments and multi-modal classes, respectively. Corresponding to these generalisations, we derive four new EM algorithms. We show the effectiveness of our generalisations through illustrative examples of simulated datasets, as well as real-world examples of three famous datasets, the MNIST dataset, the CIFAR-10 dataset and the EMNIST dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303782",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Chemistry",
      "Computer science",
      "Feature (linguistics)",
      "Linguistics",
      "MNIST database",
      "Machine learning",
      "Modal",
      "Philosophy",
      "Polymer chemistry"
    ],
    "authors": [
      {
        "surname": "Lu",
        "given_name": "Xiaoou"
      },
      {
        "surname": "Qiao",
        "given_name": "Yangqi"
      },
      {
        "surname": "Zhu",
        "given_name": "Rui"
      },
      {
        "surname": "Wang",
        "given_name": "Guijin"
      },
      {
        "surname": "Ma",
        "given_name": "Zhanyu"
      },
      {
        "surname": "Xue",
        "given_name": "Jing-Hao"
      }
    ]
  },
  {
    "title": "BLOCK-DBSCAN: Fast clustering for large scale data",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107624",
    "abstract": "We analyze the drawbacks of DBSCAN and its variants, and find the grid technique, which is used in Fast-DBSCAN and ρ-approximate DBSCAN, is almost useless in high dimensional data space. Because it usually yields considerable redundant distance computations. In order to tame these problems, two techniques are proposed: one is to use ϵ 2 -norm ball to identify Inner Core Blocks within which all points are core points, it has higher efficiency than grid technique for finding more core points at one time; the other is a fast approximate algorithm for judging whether two Inner Core Blocks are density-reachable from each other. Besides, cover tree is also used to accelerate the process of density computations. Based on the three techniques, an approximate approach, namely BLOCK-DBSCAN, is proposed for large scale data, which runs in about O(nlog (n)) expected time and obtains almost the same result as DBSCAN. BLOCK-DBSCAN has two versions, i.e., L 2 version can work well for relatively high dimensional data, and L ∞ version is suitable for high dimensional data. Experimental results show that BLOCK-DBSCAN is promising and outperforms NQDBSCAN, ρ-approximate DBSCAN and AnyDBC.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320304271",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Block (permutation group theory)",
      "CURE data clustering algorithm",
      "Cluster analysis",
      "Combinatorics",
      "Computation",
      "Computer science",
      "Correlation clustering",
      "DBSCAN",
      "Geometry",
      "Grid",
      "Mathematics",
      "Physics",
      "Quantum mechanics",
      "Scale (ratio)"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Yewang"
      },
      {
        "surname": "Zhou",
        "given_name": "Lida"
      },
      {
        "surname": "Bouguila",
        "given_name": "Nizar"
      },
      {
        "surname": "Wang",
        "given_name": "Cheng"
      },
      {
        "surname": "Chen",
        "given_name": "Yi"
      },
      {
        "surname": "Du",
        "given_name": "Jixiang"
      }
    ]
  },
  {
    "title": "Object-oriented remote sensing image information extraction method based on multi-classifier combination and deep learning algorithm",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2020.08.028",
    "abstract": "In recent years, high spatial resolution remote sensing technology has made significant progress. High-resolution remote sensing satellites provide great convenience for high-quality image acquisition. In order to adapt to changes in the appearance of the target, mainstream tracking algorithms often use pattern recognition methods to build a target appearance model with learning capabilities, and use the image frames acquired during the tracking process to update the appearance model. This paper mainly studies the object-oriented remote sensing image information extraction method based on multi-classifier combination and deep learning algorithm. In this paper, we use the splitting mechanism of the tree structure to retain the appearance model with diversity, and through the integrated learning integration strategy, the target position is collaboratively predicted. Through the comparative analysis on the OTB and VOT platforms, the algorithm works well when the requirements of the tracking standards are low (the accuracy threshold is greater than 20 pixels and the success threshold is less than 0.4 pixels). The experimental results in this paper show that compared with other advanced classification methods, the proposed method shows better generalization performance in accuracy, recall, f-measure, g-mean and AUC.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520303317",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Computer vision",
      "Pattern recognition (psychology)",
      "Pixel",
      "Precision and recall"
    ],
    "authors": [
      {
        "surname": "Tan",
        "given_name": "Qulin"
      },
      {
        "surname": "Guo",
        "given_name": "Bin"
      },
      {
        "surname": "Hu",
        "given_name": "Jun"
      },
      {
        "surname": "Dong",
        "given_name": "Xiaofeng"
      },
      {
        "surname": "Hu",
        "given_name": "Jiping"
      }
    ]
  },
  {
    "title": "Explainable skin lesion diagnosis using taxonomies",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107413",
    "abstract": "Deep neural networks have rapidly become an indispensable tool in many classification applications. However, the inclusion of deep learning methods in medical diagnostic systems has come at the cost of diminishing their explainability. This significantly reduces the safety of a diagnostic system, since the physician is unable to interpret and validate the output. Therefore, in this work we aim to address this major limitation and improve the explainability of a skin cancer diagnostic system. We propose to leverage two sources of information: (i) medical knowledge, in particular the taxonomic organization of skin lesions, which will be used to develop a hierarchical neural network; and (ii) recent advances in channel and spatial attention modules, which can identify interpretable features and regions in dermoscopy images. We demonstrate that the proposed approach achieves competitive results in two dermoscopy data sets (ISIC 2017 and 2018) and provides insightful information about its decisions, thus increasing the safety of the model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320302168",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Deep neural networks",
      "Leverage (statistics)",
      "Machine learning",
      "Medicine",
      "Pathology",
      "Pattern recognition (psychology)",
      "Skin lesion"
    ],
    "authors": [
      {
        "surname": "Barata",
        "given_name": "Catarina"
      },
      {
        "surname": "Celebi",
        "given_name": "M. Emre"
      },
      {
        "surname": "Marques",
        "given_name": "Jorge S."
      }
    ]
  },
  {
    "title": "Exploring DeshuffleGANs in Self-Supervised Generative Adversarial Networks",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108244",
    "abstract": "Generative Adversarial Networks (GANs) have become the most used networks towards solving the problem of image generation. Self-supervised GANs are later proposed to avoid the catastrophic forgetting of the discriminator and to improve the image generation quality without needing the class labels. However, the generalizability of the self-supervision tasks on different GAN architectures is not studied before. To that end, we extensively analyze the contribution of a previously proposed self-supervision task, deshuffling of the DeshuffleGANs in the generalizability context. We assign the deshuffling task to two different GAN discriminators and study the effects of the task on both architectures. We extend the evaluations compared to the previously proposed DeshuffleGANs on various datasets. We show that the DeshuffleGAN obtains the best FID results for several datasets compared to the other self-supervised GANs. Furthermore, we compare the deshuffling with the rotation prediction that is firstly deployed to the GAN training and demonstrate that its contribution exceeds the rotation prediction. We design the conditional DeshuffleGAN called cDeshuffleGAN to evaluate the quality of the learnt representations. Lastly, we show the contribution of the self-supervision tasks to the GAN training on the loss landscape and present that the effects of these tasks may not be cooperative to the adversarial training in some settings. Our code can be found at https://github.com/gulcinbaykal/DeshuffleGAN.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321004167",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Class (philosophy)",
      "Code (set theory)",
      "Computer science",
      "Context (archaeology)",
      "Detector",
      "Discriminator",
      "Economics",
      "Epistemology",
      "Generalizability theory",
      "Generative grammar",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Machine learning",
      "Management",
      "Mathematics",
      "Paleontology",
      "Philosophy",
      "Programming language",
      "Quality (philosophy)",
      "Rotation (mathematics)",
      "Set (abstract data type)",
      "Statistics",
      "Task (project management)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Baykal",
        "given_name": "Gulcin"
      },
      {
        "surname": "Ozcelik",
        "given_name": "Furkan"
      },
      {
        "surname": "Unal",
        "given_name": "Gozde"
      }
    ]
  },
  {
    "title": "Deep multi-task learning with relational attention for business success prediction",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107469",
    "abstract": "Multi-task learning is a promising machine learning branch, which aims to improve the generalization of the prediction models by sharing knowledge among tasks. Most of the existing multi-task learning methods rely on predefined task relationships and guide the learning process of models by linear regularization terms. On the one hand, improper setting of task relationships may result in negative knowledge transfer; on the other hand, these methods also suffer from the insufficiency of representation ability. To overcome these problems, this paper focuses on attention-based deep multi-task learning method, and provides a novel deep multi-task learning method, namely, Deep Multi-task Learning with Relational Attention (DMLRA). In particular, we first provide a task-specific attention module to specify features for different learning tasks, because different prediction tasks may rely on different parts of the shared feature set. Then, we design a relational attention module to learn relationships among multiple tasks automatically, and transfer positive and negative knowledge among multiple tasks accordingly. Moreover, we provide a joint deep multi-task learning framework to combine task-specific module and relational attention module. Finally, we apply our method on a multi-criteria business success assessment problem, both classical and the state-of-the-art multi-task learning methods are employed to provide baseline performance. The experiments are conducted on real-world datasets, results demonstrate the superiority of our method over the existing methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320302727",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Deep learning",
      "Economics",
      "Feature learning",
      "Generalization",
      "Machine learning",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Multi-task learning",
      "Programming language",
      "Regularization (linguistics)",
      "Set (abstract data type)",
      "Task (project management)",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Jiejie"
      },
      {
        "surname": "Du",
        "given_name": "Bowen"
      },
      {
        "surname": "Sun",
        "given_name": "Leilei"
      },
      {
        "surname": "Lv",
        "given_name": "Weifeng"
      },
      {
        "surname": "Liu",
        "given_name": "Yanchi"
      },
      {
        "surname": "Xiong",
        "given_name": "Hui"
      }
    ]
  },
  {
    "title": "Quantization based clustering: An iterative approach",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2020.12.007",
    "abstract": "In this paper we propose a simple new algorithm to perform clustering, based on the Alter algorithm proposed in [1] but lowering significantly the algorithmic complexity with respect to the number of clusters. An empirical study states the relevance of our iterative process and a confrontation on simulated multivariate and functional data shows the benefits of our algorithm.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520304384",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Epistemology",
      "Iterative and incremental development",
      "Iterative method",
      "Law",
      "Linde–Buzo–Gray algorithm",
      "Machine learning",
      "Multivariate statistics",
      "Operating system",
      "Philosophy",
      "Political science",
      "Process (computing)",
      "Quantization (signal processing)",
      "Relevance (law)",
      "Simple (philosophy)",
      "Software engineering"
    ],
    "authors": [
      {
        "surname": "Laloë",
        "given_name": "Thomas"
      }
    ]
  },
  {
    "title": "Multi-scale structural kernel representation for object detection",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107593",
    "abstract": "Existing high-performance object detection methods greatly benefit from the powerful representation ability of deep convolutional neural networks (CNNs). Recent researches show that integration of high-order statistics remarkably improves the representation ability of deep CNNs. However, high-order statistics for object detection lie in two challenges. Firstly, previous methods insert high-order statistics into deep CNNs as global representations, which lose spatial information of inputs, and so are not applicable to object detection. Furthermore, high-order statistics have special structures, which should be considered for proper use of high-order statistics. To overcome above challenges, this paper proposes a Multi-scale Structural Kernel Representation (MSKR) for improving performance of object detection. Our MSKR is developed based on the polynomial kernel approximation, which does not only draw into high-order statistics but also preserve the spatial information of input. To consider geometry structures of high-order representations, a feature power normalization method is introduced before computation of kernel representation. Comparing with the most commonly used first-order statistics in existing CNN-based detectors, our MSKR can generate more discriminative representations, and so be flexibly integrated into deep CNNs for improving performance of object detection. By adopting the proposed MSKR to existing object detection methods (i.e., Faster R-CNN, FPN, Mask R-CNN and RetinaNet), it achieves clear improvement on three widely used benchmarks, while obtaining very competitive performance with state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303964",
    "keywords": [
      "Anthropology",
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Discriminative model",
      "Kernel (algebra)",
      "Law",
      "Mathematics",
      "Normalization (sociology)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Hao"
      },
      {
        "surname": "Wang",
        "given_name": "Qilong"
      },
      {
        "surname": "Li",
        "given_name": "Peihua"
      },
      {
        "surname": "Zuo",
        "given_name": "Wangmeng"
      }
    ]
  },
  {
    "title": "Position-aware self-attention based neural sequence labeling",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107636",
    "abstract": "Sequence labeling is a fundamental task in natural language processing and has been widely studied. Recently, RNN-based sequence labeling models have increasingly gained attentions. Despite superior performance achieved by learning the long short-term (i.e., successive) dependencies, the way of sequentially processing inputs might limit the ability to capture the non-continuous relations over tokens within a sentence. To tackle the problem, we focus on how to effectively model successive and discrete dependencies of each token for enhancing the sequence labeling performance. Specifically, we propose an innovative attention-based model (called position-aware self-attention, i.e., PSA) as well as a well-designed self-attentional context fusion layer within a neural network architecture, to explore the positional information of an input sequence for capturing the latent relations among tokens. Extensive experiments on three classical tasks in sequence labeling domain, i.e., part-of-speech (POS) tagging, named entity recognition (NER) and phrase chunking, demonstrate our proposed model outperforms the state-of-the-arts without any external knowledge, in terms of various metrics.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320304398",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Chunking (psychology)",
      "Computer science",
      "Computer security",
      "Context (archaeology)",
      "Deep learning",
      "Economics",
      "Focus (optics)",
      "Genetics",
      "Management",
      "Natural language processing",
      "Optics",
      "Paleontology",
      "Physics",
      "Recurrent neural network",
      "Security token",
      "Sentence",
      "Sequence (biology)",
      "Sequence labeling",
      "Sequence learning",
      "Speech recognition",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Wei",
        "given_name": "Wei"
      },
      {
        "surname": "Wang",
        "given_name": "Zanbo"
      },
      {
        "surname": "Mao",
        "given_name": "Xianling"
      },
      {
        "surname": "Zhou",
        "given_name": "Guangyou"
      },
      {
        "surname": "Zhou",
        "given_name": "Pan"
      },
      {
        "surname": "Jiang",
        "given_name": "Sheng"
      }
    ]
  },
  {
    "title": "Virtual special issue on novel data-representation and classification techniques",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2020.12.002",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552030430X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Information retrieval",
      "Law",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Olvera-Lopez",
        "given_name": "J. Arturo"
      },
      {
        "surname": "Salas",
        "given_name": "Joaquin"
      },
      {
        "surname": "Carrasco-Ochoa",
        "given_name": "J. Ariel"
      },
      {
        "surname": "Martinez-Trinidad",
        "given_name": "José Fco."
      },
      {
        "surname": "Sarkar",
        "given_name": "Sudeep"
      }
    ]
  },
  {
    "title": "Exploiting textual queries for dynamically visual disambiguation",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107620",
    "abstract": "Due to the high cost of manual annotation, learning directly from the web has attracted broad attention. One issue that limits the performance of current webly supervised models is the problem of visual polysemy. In this work, we present a novel framework that resolves visual polysemy by dynamically matching candidate text queries with retrieved images. Specifically, our proposed framework includes three major steps: we first discover and then dynamically select the text queries according to the keyword-based image search results, we employ the proposed saliency-guided deep multi-instance learning (MIL) network to remove outliers and learn classification models for visual disambiguation. Compared to existing methods, our proposed approach can figure out the right visual senses, adapt to dynamic changes in the search results, remove outliers, and jointly learn the classification models. Extensive experiments and ablation studies on CMU-Poly-30 and MIT-ISD datasets demonstrate the effectiveness of our proposed approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320304234",
    "keywords": [
      "Annotation",
      "Artificial intelligence",
      "Computer science",
      "Information retrieval",
      "Machine learning",
      "Matching (statistics)",
      "Mathematics",
      "Outlier",
      "Pattern recognition (psychology)",
      "Polysemy",
      "Statistics",
      "Visualization"
    ],
    "authors": [
      {
        "surname": "Sun",
        "given_name": "Zeren"
      },
      {
        "surname": "Yao",
        "given_name": "Yazhou"
      },
      {
        "surname": "Xiao",
        "given_name": "Jimin"
      },
      {
        "surname": "Zhang",
        "given_name": "Lei"
      },
      {
        "surname": "Zhang",
        "given_name": "Jian"
      },
      {
        "surname": "Tang",
        "given_name": "Zhenmin"
      }
    ]
  },
  {
    "title": "Structured graph learning for clustering and semi-supervised classification",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107627",
    "abstract": "Graphs have become increasingly popular in modeling structures and interactions in a wide variety of problems during the last decade. Graph-based clustering and semi-supervised classification techniques have shown impressive performance. This paper proposes a graph learning framework to preserve both the local and global structure of data. Specifically, our method uses the self-expressiveness of samples to capture the global structure and adaptive neighbor approach to respect the local structure. Furthermore, most existing graph-based methods conduct clustering and semi-supervised classification on the graph learned from the original data matrix, which doesn’t have explicit cluster structure, thus they might not achieve the optimal performance. By considering rank constraint, the achieved graph will have exactly c connected components if there are c clusters or classes. As a byproduct of this, graph learning and label inference are jointly and iteratively implemented in a principled way. Theoretically, we show that our model is equivalent to a combination of kernel k-means and k-means methods under certain condition. Extensive experiments on clustering and semi-supervised classification demonstrate that the proposed method outperforms other state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320304301",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Graph",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Semi-supervised learning",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Kang",
        "given_name": "Zhao"
      },
      {
        "surname": "Peng",
        "given_name": "Chong"
      },
      {
        "surname": "Cheng",
        "given_name": "Qiang"
      },
      {
        "surname": "Liu",
        "given_name": "Xinwang"
      },
      {
        "surname": "Peng",
        "given_name": "Xi"
      },
      {
        "surname": "Xu",
        "given_name": "Zenglin"
      },
      {
        "surname": "Tian",
        "given_name": "Ling"
      }
    ]
  },
  {
    "title": "Cross-modality deep feature learning for brain tumor segmentation",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107562",
    "abstract": "Recent advances in machine learning and prevalence of digital medical images have opened up an opportunity to address the challenging brain tumor segmentation (BTS) task by using deep convolutional neural networks. However, different from the RGB image data that are very widespread, the medical image data used in brain tumor segmentation are relatively scarce in terms of the data scale but contain the richer information in terms of the modality property. To this end, this paper proposes a novel cross-modality deep feature learning framework to segment brain tumors from the multi-modality MRI data. The core idea is to mine rich patterns across the multi-modality data to make up for the insufficient data scale. The proposed cross-modality deep feature learning framework consists of two learning processes: the cross-modality feature transition (CMFT) process and the cross-modality feature fusion (CMFF) process, which aims at learning rich feature representations by transiting knowledge across different modality data and fusing knowledge from different modality data, respectively. Comprehensive experiments are conducted on the BraTS benchmarks, which show that the proposed cross-modality deep feature learning framework can effectively improve the brain tumor segmentation performance when compared with the baseline methods and state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303654",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Feature (linguistics)",
      "Feature learning",
      "Linguistics",
      "Machine learning",
      "Modality (human–computer interaction)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Process (computing)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Dingwen"
      },
      {
        "surname": "Huang",
        "given_name": "Guohai"
      },
      {
        "surname": "Zhang",
        "given_name": "Qiang"
      },
      {
        "surname": "Han",
        "given_name": "Jungong"
      },
      {
        "surname": "Han",
        "given_name": "Junwei"
      },
      {
        "surname": "Yu",
        "given_name": "Yizhou"
      }
    ]
  },
  {
    "title": "Lightweight dynamic conditional GAN with pyramid attention for text-to-image synthesis",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107384",
    "abstract": "The text-to-image synthesis task aims to generate photographic images conditioned on semantic text descriptions. To ensure the sharpness and fidelity of generated images, this task tends to generate high-resolution images (e.g., 1282 or 2562). However, as the resolution increases, the network parameters and complexity increases dramatically. Recent works introduce network structures with extensive parameters and heavy computations to guarantee the production of high-resolution images. As a result, these models come across problems of the unstable training process and high training cost. To tackle these issues, in this paper, we propose an effective information compensation based approach, namely Lightweight Dynamic Conditional GAN (LD-CGAN). LD-CGAN is a compact and structured single-stream network, and it consists of one generator and two independent discriminators to regularize and generate 642 and 1282 images in one feed-forward process. Specifically, the generator of LD-CGAN is composed of three major components: (1) Conditional Embedding (CE), which is an automatically unsupervised learning process aiming at disentangling integrated semantic attributes in the text space; (2) Conditional Manipulating Modular (CM-M) in Conditional Manipulating Block (CM-B), which is designed to continuously provide the image features with the compensation information (i.e., the disentangled attribute); and (3) Pyramid Attention Refine Block (PAR-B), which is used to enrich multi-scale features by capturing spatial importance between multi-scale context. Consequently, experiments conducted under two benchmark datasets, CUB and Oxford-102, indicate that our generated 1282 images can achieve comparable performance with 2562 images generated by the state-of-the-arts on two evaluation metrics: Inception Score (IS) and Visual-semantic Similarity (VS). Compared with the current state-of-the-art HDGAN, our LD-CGAN significantly decreases the number of parameters and computation time by 86.8% and 94.9%, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320301874",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Block (permutation group theory)",
      "Compensation (psychology)",
      "Computer science",
      "Context (archaeology)",
      "Economics",
      "Embedding",
      "Generator (circuit theory)",
      "Geodesy",
      "Geography",
      "Geometry",
      "Image (mathematics)",
      "Management",
      "Mathematics",
      "Modular design",
      "Operating system",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Physics",
      "Power (physics)",
      "Process (computing)",
      "Psychoanalysis",
      "Psychology",
      "Pyramid (geometry)",
      "Quantum mechanics",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Gao",
        "given_name": "Lianli"
      },
      {
        "surname": "Chen",
        "given_name": "Daiyuan"
      },
      {
        "surname": "Zhao",
        "given_name": "Zhou"
      },
      {
        "surname": "Shao",
        "given_name": "Jie"
      },
      {
        "surname": "Shen",
        "given_name": "Heng Tao"
      }
    ]
  },
  {
    "title": "Place perception from the fusion of different image representation",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107680",
    "abstract": "Inspired by the human way of place understanding, we present a novel indoor place perception network to overcome: 1). the simplicity of existing methods that only use the image features of object regions to recognize the indoor place, 2). insufficient consideration of the semantic information about object attributes and states. By utilizing multi-modal information containing the image and natural language, the proposed method can comprehensively express the attributes, state, and relationships of objects which are beneficial for indoor place understanding and recognition. Specifically, we first present a natural language generation framework based on a Convolution Neural Network (CNN) and Long Short-Term Memory (LSTM) to imitate the process of place understanding. Next, a Convolutional Auto-Encoder (CAE) and a mixed CNN-LSTM are proposed to extract image features and semantic features, respectively. Then, two different fusion strategies, namely feature-level fusion and object-level fusion, are designed to integrate different types of features and features from different objects. The category of the indoor place is finally recognized based on fused information. Comprehensive experiments are conducted on public datasets, and the results verify the effectiveness of the proposed place perception method based on linguistic cues.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320304830",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer science",
      "Computer vision",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Encoder",
      "Feature (linguistics)",
      "Image (mathematics)",
      "Law",
      "Linguistics",
      "Machine learning",
      "Natural language",
      "Natural language processing",
      "Neuroscience",
      "Object (grammar)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Perception",
      "Philosophy",
      "Political science",
      "Politics",
      "Process (computing)",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Pei"
      },
      {
        "surname": "Li",
        "given_name": "Xinde"
      },
      {
        "surname": "Li",
        "given_name": "Xianghui"
      },
      {
        "surname": "Pan",
        "given_name": "Hong"
      },
      {
        "surname": "Khyam",
        "given_name": "M.O."
      },
      {
        "surname": "Noor-A-Rahim",
        "given_name": "Md."
      },
      {
        "surname": "Ge",
        "given_name": "Shuzhi Sam"
      }
    ]
  },
  {
    "title": "Application of binocular disparity and receptive field dynamics: A biologically-inspired model for contour detection",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107657",
    "abstract": "Neurophysiological evidence demonstrates that classical receptive field responses in the primary visual cortex can be modulated by the non-classical receptive field. Although models based on the non-classical receptive field have been proposed, which has not employed the two following characteristics: dynamic regulation of the receptive field under external stimuli and depth determination by binocular cells. We propose a model that includes processing of gray-scale images and depth images. Luminance of grayscale image response was preprocessed using weighted least squares filtering; luminance of grayscale image and disparity information of depth image responses were obtained by V1 neuron responses. The receptive field size of each pixel used to calculate the luminance and disparity response is determined by the depth information. Luminance and disparity information responses were combined based on the optimal orientation, and robust contour maps were developed. Experimental results show that the proposed contour detection model outperforms biologically inspired models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032030460X",
    "keywords": [
      "Artificial intelligence",
      "Binocular disparity",
      "Binocular neurons",
      "Binocular vision",
      "Computer science",
      "Computer vision",
      "Geometry",
      "Grayscale",
      "Luminance",
      "Mathematics",
      "Neurophysiology",
      "Neuroscience",
      "Orientation (vector space)",
      "Pattern recognition (psychology)",
      "Pixel",
      "Psychology",
      "Receptive field"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Qing"
      },
      {
        "surname": "Lin",
        "given_name": "Chuan"
      },
      {
        "surname": "Li",
        "given_name": "Fuzhang"
      }
    ]
  },
  {
    "title": "Sparse motion fields for trajectory prediction",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107631",
    "abstract": "Trajectory prediction is a crucial element of many automated tasks, such as autonomous navigation or video surveillance. To automatically predict the motion of an agent (e.g., pedestrian or car), the model needs to efficiently represent human motion and “understand” the external stimuli that may influence human behavior. In this work we propose a methodology to model the motion of agents in a video scene. Our method is based on space-varying sparse motion fields, which simultaneously characterize diverse motion patterns in the scene and implicitly learn contextual cues about the static environment, namely obstacles and semantic constraints. The sparse motion fields are applied to the task of long-term trajectory prediction using a probabilistic generative approach. Several benchmark data sets are used to demonstrate the potential of the proposed approach and show that our method achieves competitive state-of-the-art performances.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320304349",
    "keywords": [
      "Artificial intelligence",
      "Astronomy",
      "Benchmark (surveying)",
      "Computer science",
      "Computer vision",
      "Economics",
      "Geodesy",
      "Geography",
      "Management",
      "Motion (physics)",
      "Physics",
      "Probabilistic logic",
      "Task (project management)",
      "Trajectory"
    ],
    "authors": [
      {
        "surname": "Barata",
        "given_name": "Catarina"
      },
      {
        "surname": "Nascimento",
        "given_name": "Jacinto C."
      },
      {
        "surname": "Lemos",
        "given_name": "João M."
      },
      {
        "surname": "Marques",
        "given_name": "Jorge S."
      }
    ]
  },
  {
    "title": "Robust kernelized graph-based learning",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107628",
    "abstract": "The studies of hidden complex structures in data have popularized the use of graph-based learning methods in semi-supervised and unsupervised learning tasks. Kernelized graph-based methods are proven to perform better, but these methods suffer from the issue of appropriate kernel selection. Instead of using multiple views, these methods generally use a single view. But multi-view methods need a proper weight assignment technique to each view in proportion to their contribution to the learning task. To solve this, a novel Self-weighted Multi-view Multiple Kernel Learning (SMVMKL) framework is proposed using multiple kernels on multiple views that automatically assigns appropriate weight to each kernel of each view without introducing an additional parameter. But the real-world data that is either noisy or corrupt with outliers which may effect the performance of the proposed SMVMKL method. To deal with this, a Robust Self-weighted Multi-view Multiple Kernel Learning (RSMVMKL) framework using the l 2,1-norm has also been proposed that reduces the effect of outliers present in the data set. Both the proposed methods have been evaluated on multiple benchmark data sets and result in a performance comparable with the other state-of-the-art multi-view methods considered in this paper.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320304313",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Combinatorics",
      "Computer science",
      "Geodesy",
      "Geography",
      "Graph",
      "Graph kernel",
      "Kernel (algebra)",
      "Kernel method",
      "Machine learning",
      "Mathematics",
      "Multiple kernel learning",
      "Outlier",
      "Pattern recognition (psychology)",
      "Polynomial kernel",
      "Semi-supervised learning",
      "Support vector machine",
      "Theoretical computer science",
      "Tree kernel"
    ],
    "authors": [
      {
        "surname": "Manna",
        "given_name": "Supratim"
      },
      {
        "surname": "Khonglah",
        "given_name": "Jessy Rimaya"
      },
      {
        "surname": "Mukherjee",
        "given_name": "Anirban"
      },
      {
        "surname": "Saha",
        "given_name": "Goutam"
      }
    ]
  },
  {
    "title": "Pool-based unsupervised active learning for regression using iterative representativeness-diversity maximization (iRDM)",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2020.11.019",
    "abstract": "Active learning (AL) selects the most beneficial unlabeled samples to label, and hence a better machine learning model can be trained from the same number of labeled samples. Most existing active learning for regression (ALR) approaches are supervised, which means the sampling process must use some label information, or an existing regression model. This paper considers completely unsupervised ALR, i.e., how to select the samples to label without knowing any true label information. We propose a novel unsupervised ALR approach, iterative representativeness-diversity maximization (iRDM), to optimally balance the representativeness and the diversity of the selected samples. Experiments on 60 datasets from various domains demonstrated its effectiveness. Our iRDM can be applied to both linear regression and kernel regression, and it even significantly outperforms supervised ALR when the number of labeled samples is small.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520304323",
    "keywords": [
      "Active learning (machine learning)",
      "Artificial intelligence",
      "Artificial neural network",
      "Combinatorics",
      "Computer science",
      "Kernel (algebra)",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Maximization",
      "Pattern recognition (psychology)",
      "Regression",
      "Regression analysis",
      "Representativeness heuristic",
      "Semi-supervised learning",
      "Statistics",
      "Supervised learning",
      "Unsupervised learning"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Ziang"
      },
      {
        "surname": "Jiang",
        "given_name": "Xue"
      },
      {
        "surname": "Luo",
        "given_name": "Hanbin"
      },
      {
        "surname": "Fang",
        "given_name": "Weili"
      },
      {
        "surname": "Liu",
        "given_name": "Jiajing"
      },
      {
        "surname": "Wu",
        "given_name": "Dongrui"
      }
    ]
  },
  {
    "title": "Multi-scale deep relational reasoning for facial kinship verification",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107541",
    "abstract": "In this paper, we propose a deep relational network which exploits multi-scale information of facial images for kinship verification. Unlike most existing deep learning based facial kinship verification methods which employ convolutional neural networks to extract holistic features, we present a deep model to exploit facial kinship relationship from local regions. For each given pair of face images, our method uses two convolutional neural networks which share parameters to extract different scales of features, which are expected to provide global contextual information of face images. We split a set of features at the same scale into multiple groups, where different groups capture information of different local regions. For each pair of local feature groups which are extracted from the same scale and position, we propose a relation network to reason their relationship, and use a verification network to infer the kin relation based on the results of local relations from different facial regions. We conduct experiments on two widely used facial kinship datasets: KinFaceW-I and KinFaceW-II, and our experimental results are presented to demonstrate the effectiveness of our approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303447",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Cartography",
      "Computer science",
      "Computer security",
      "Convolutional neural network",
      "Data mining",
      "Deep learning",
      "Exploit",
      "Face (sociological concept)",
      "Facial recognition system",
      "Feature (linguistics)",
      "Feature extraction",
      "Geography",
      "Kinship",
      "Law",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Programming language",
      "Relation (database)",
      "Scale (ratio)",
      "Set (abstract data type)",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Yan",
        "given_name": "Haibin"
      },
      {
        "surname": "Song",
        "given_name": "Chaohui"
      }
    ]
  },
  {
    "title": "Learning adaptive geometry for unsupervised domain adaptation",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107638",
    "abstract": "Unsupervised domain adaptation is an effective approach to solve the problem of dataset bias. However, most existing unsupervised domain adaptation methods assume that the geometry structures of data distributions are similar in the source and target domains. This assumption is invalid in many practical applications, because the training and test datasets usually differ in the variability modes and/or variation degrees. This paper handles the problem of inconsistent geometries by aligning both data representations and geometries. To overcome the lack of target labels in aligning geometries, this paper proposes learning the adaptive geometry that is derived from the domain-shared label space. Source and target geometries are aligned by constraining them with the unified criteria of the adaptive geometry. Combining the adaptive geometry learning and adversarial learning techniques, we develop a geometry-aware dual-stream network to learn the geometry-aligned representations. Experimental results show that our method achieves good performance on cross-dataset recognition tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320304416",
    "keywords": [
      "Adaptation (eye)",
      "Algorithm",
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Curvature",
      "Domain (mathematical analysis)",
      "Domain adaptation",
      "Geometry",
      "Information geometry",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Operating system",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Scalar curvature",
      "Space (punctuation)",
      "Unsupervised learning"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Baoyao"
      },
      {
        "surname": "Yuen",
        "given_name": "Pong C."
      }
    ]
  },
  {
    "title": "Violence detection and face recognition based on deep learning",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2020.11.018",
    "abstract": "With the emergence of the concept of “safe city”, security construction has gradually been valued by various cities, and video surveillance technology has also been continuously developed and applied. However, as the functional requirements of actual applications become more and more diverse, video surveillance systems also need to be more intelligent. The purpose of this article is to study methods of brute force detection and face recognition based on deep learning. Aiming at the problem of abnormal behavior detection, especially the low efficiency and low accuracy of brute force detection, a brute force detection method based on the combination of convolutional neural network and trajectory is proposed. This method uses artificial features and depth features to extract the spatiotemporal features of the video through a convolutional neural network and combines them with the trajectory features. In view of the problem that face images in surveillance video cannot be accurately recognized due to low resolution, two models are proposed: the multi-foot input CNN model and the SPP-based CNN model. By testing the performance of the brute force detection method proposed in this paper, the accuracy of the method on the Crow and Hockey datasets is as high as 92% and 97.6%, respectively. Experimental results show that the violence detection method proposed in this paper improves the accuracy of violence detection in video.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520304311",
    "keywords": [
      "Artificial intelligence",
      "Astronomy",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Deep learning",
      "Face (sociological concept)",
      "Face detection",
      "Facial recognition system",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Physics",
      "Social science",
      "Sociology",
      "Trajectory"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Pin"
      },
      {
        "surname": "Wang",
        "given_name": "Peng"
      },
      {
        "surname": "Fan",
        "given_name": "En"
      }
    ]
  },
  {
    "title": "Cross-view classification by joint adversarial learning and class-specificity distribution",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107633",
    "abstract": "Despite the promising preliminary results, none of existing deep learning based cross-view classification methods simultaneously takes into account both view consistency learning and class-specificity distribution of the extracted features, resulting in unstable classification performance. Moreover, most existing cross-view classification methods are sensitive to scale due to the scale issue of view representations, resulting in unstable view-consistent representations. In this paper, we propose a new deep adversarial network for cross-view classification that attempts to learn robust view-consistent representations by combing the thought of adversarial learning and metric learning in Fisher criterion. Meanwhile, a class-specificity distribution term, which is measured by ℓ12-norm, is employed to make the view-consistent representations with the same label to further have a common distribution in dimension space while view-representations with different labels have different distribution in the intrinsic dimension space. We formulate the aforementioned two concerns into a unified optimization framework. Extensive experiments on several real-world datasets indicate the effectiveness of our method over the other state-of-the-arts.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320304362",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Consistency (knowledge bases)",
      "Curse of dimensionality",
      "Dimension (graph theory)",
      "Economics",
      "Intrinsic dimension",
      "Law",
      "Machine learning",
      "Mathematics",
      "Metric (unit)",
      "Norm (philosophy)",
      "Operations management",
      "Pattern recognition (psychology)",
      "Political science",
      "Pure mathematics"
    ],
    "authors": [
      {
        "surname": "Deng",
        "given_name": "Siyang"
      },
      {
        "surname": "Xia",
        "given_name": "Wei"
      },
      {
        "surname": "Gao",
        "given_name": "Quanxue"
      },
      {
        "surname": "Gao",
        "given_name": "Xinbo"
      }
    ]
  },
  {
    "title": "Progressive sample mining and representation learning for one-shot person re-identification",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107614",
    "abstract": "In this paper, we aim to tackle the one-shot person re-identification problem where only one image is labelled for each person, while other images are unlabelled. This task is challenging due to the lack of sufficient labelled training data. To tackle this problem, we propose to iteratively guess pseudo labels for the unlabelled image samples, which are later used to update the re-identification model together with the labelled samples. A new sampling mechanism is designed to select unlabelled samples to pseudo labelled samples based on the distance matrix, and to form a training triplet batch including both labelled samples and pseudo labelled samples. We also design an HSoften-Triplet-Loss to soften the negative impact of the incorrect pseudo label, considering the unreliable nature of pseudo labelled samples. Finally, we deploy an adversarial learning method to expand the image samples to different camera views. Our experiments show that our framework achieves a new state-of-the-art one-shot Re-ID performance on Market-1501 (mAP 42.7%) and DukeMTMC-Reid dataset (mAP 40.3%). Code is available on https://github.com/detectiveli/PSMA.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320304179",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Chemistry",
      "Chromatography",
      "Code (set theory)",
      "Composite material",
      "Computer science",
      "Computer vision",
      "Economics",
      "Engineering",
      "Filter (signal processing)",
      "Identification (biology)",
      "Image (mathematics)",
      "Law",
      "Management",
      "Materials science",
      "Matrix (chemical analysis)",
      "Mechanical engineering",
      "One shot",
      "Optics",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Physics",
      "Political science",
      "Politics",
      "Programming language",
      "Representation (politics)",
      "Sample (material)",
      "Sampling (signal processing)",
      "Set (abstract data type)",
      "Shot (pellet)",
      "Single shot",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Hui"
      },
      {
        "surname": "Xiao",
        "given_name": "Jimin"
      },
      {
        "surname": "Sun",
        "given_name": "Mingjie"
      },
      {
        "surname": "Lim",
        "given_name": "Eng Gee"
      },
      {
        "surname": "Zhao",
        "given_name": "Yao"
      }
    ]
  },
  {
    "title": "Understanding adversarial attacks on deep learning based medical image analysis systems",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107332",
    "abstract": "Deep neural networks (DNNs) have become popular for medical image analysis tasks like cancer diagnosis and lesion detection. However, a recent study demonstrates that medical deep learning systems can be compromised by carefully-engineered adversarial examples/attacks with small imperceptible perturbations. This raises safety concerns about the deployment of these systems in clinical settings. In this paper, we provide a deeper understanding of adversarial examples in the context of medical images. We find that medical DNN models can be more vulnerable to adversarial attacks compared to models for natural images, according to two different viewpoints. Surprisingly, we also find that medical adversarial attacks can be easily detected, i.e., simple detectors can achieve over 98% detection AUC against state-of-the-art attacks, due to fundamental feature differences compared to normal examples. We believe these findings may be a useful basis to approach the design of more explainable and secure medical deep learning systems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320301357",
    "keywords": [
      "Adversarial system",
      "Art",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer security",
      "Context (archaeology)",
      "Deep learning",
      "Deep neural networks",
      "Image (mathematics)",
      "Machine learning",
      "Medical imaging",
      "Operating system",
      "Paleontology",
      "Software deployment",
      "Viewpoints",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Ma",
        "given_name": "Xingjun"
      },
      {
        "surname": "Niu",
        "given_name": "Yuhao"
      },
      {
        "surname": "Gu",
        "given_name": "Lin"
      },
      {
        "surname": "Wang",
        "given_name": "Yisen"
      },
      {
        "surname": "Zhao",
        "given_name": "Yitian"
      },
      {
        "surname": "Bailey",
        "given_name": "James"
      },
      {
        "surname": "Lu",
        "given_name": "Feng"
      }
    ]
  },
  {
    "title": "Towards Non-I.I.D. image classification: A dataset and baselines",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107383",
    "abstract": "I.I.D. 2 2 I.I.D.: Independent and Identically Distributed hypothesis between training and testing data is the basis of numerous image classification methods. Such property can hardly be guaranteed in practice where the Non-IIDness is common, causing instable performances of these models. In literature, however, the Non-I.I.D. 3 3 Non-I.I.D: Non-Independent and Identically Distributed image classification problem is largely understudied. A key reason is lacking of a well-designed dataset to support related research. In this paper, we construct and release a Non-I.I.D. image dataset called NICO 4 4 NICO: Non-I.I.D. Image dataset with Contexts , which uses contexts to create Non-IIDness consciously. Compared to other datasets, extended analyses prove NICO can support various Non-I.I.D. situations with sufficient flexibility. Meanwhile, we propose a baseline model with ConvNet structure for General Non-I.I.D. image classification, where distribution of testing data is unknown but different from training data. The experimental results demonstrate that NICO can well support the training of ConvNet model from scratch, and a batch balancing module can help ConvNets to perform better in Non-I.I.D. settings.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320301862",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Image (mathematics)",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "He",
        "given_name": "Yue"
      },
      {
        "surname": "Shen",
        "given_name": "Zheyan"
      },
      {
        "surname": "Cui",
        "given_name": "Peng"
      }
    ]
  },
  {
    "title": "Multiple attentional pyramid networks for Chinese herbal recognition",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107558",
    "abstract": "Chinese herbs play a critical role in Traditional Chinese Medicine. Due to different recognition granularity, they can be recognized accurately only by professionals with much experience. It is expected that they can be recognized automatically using new techniques like machine learning. However, there is no Chinese herbal image dataset available. Simultaneously, there is no machine learning method which can deal with Chinese herbal image recognition well. Therefore, this paper begins with building a new standard Chinese-Herbs dataset. Subsequently, a new Attentional Pyramid Networks (APN) for Chinese herbal recognition is proposed, where both novel competitive attention and spatial collaborative attention are proposed and then applied. APN can adaptively model Chinese herbal images with different feature scales. Finally, a new framework for Chinese herbal recognition is proposed as a new application of APN. Experiments are conducted on our constructed dataset and validate the effectiveness of our methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303617",
    "keywords": [
      "Alternative medicine",
      "Artificial intelligence",
      "Computer science",
      "Feature (linguistics)",
      "Geometry",
      "Granularity",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Medicine",
      "Operating system",
      "Pathology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pyramid (geometry)",
      "Traditional Chinese medicine"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Yingxue"
      },
      {
        "surname": "Wen",
        "given_name": "Guihua"
      },
      {
        "surname": "Hu",
        "given_name": "Yang"
      },
      {
        "surname": "Luo",
        "given_name": "Mingnan"
      },
      {
        "surname": "Dai",
        "given_name": "Dan"
      },
      {
        "surname": "Zhuang",
        "given_name": "Yishan"
      },
      {
        "surname": "Hall",
        "given_name": "Wendy"
      }
    ]
  },
  {
    "title": "DecomVQANet: Decomposing visual question answering deep network via tensor decomposition and regression",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107538",
    "abstract": "The model we developed is a novel comprehensive solution to compress and accelerate the Visual Question Answering systems. In our algorithm Convolutional Neural Network is compressed with Long Short Term Memory to accelerate processing simultaneously. We propose to conduct various decomposition methods and regression strategies on different layers, including Canonical Polyadic, Tucker, and Tensor Train to decompose Fully Connected layers in CNN and LSTM. The Flattening Layer and Fully Connected layer at the end of the model are replaced with Tensor Regression layers. In order to compress the parameter further, the feature flow between the layers is compressed by Tensor Contraction layer. The proposed tensor decomposition model was evaluated on VQA 2.0 dataset with Pythia as baseline model. Our proposed model achieved from 77% to 91% of compression ratio, and only from 1% to 5% accuracy drop.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303411",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Composite material",
      "Computer science",
      "Convolutional neural network",
      "Decomposition",
      "Ecology",
      "Flattening",
      "Geometry",
      "Materials science",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Regression",
      "Statistics",
      "Tensor (intrinsic definition)",
      "Tensor decomposition",
      "Tucker decomposition"
    ],
    "authors": [
      {
        "surname": "Bai",
        "given_name": "Zongwen"
      },
      {
        "surname": "Li",
        "given_name": "Ying"
      },
      {
        "surname": "Woźniak",
        "given_name": "Marcin"
      },
      {
        "surname": "Zhou",
        "given_name": "Meili"
      },
      {
        "surname": "Li",
        "given_name": "Di"
      }
    ]
  },
  {
    "title": "Auditory perception vs. face based systems for human age estimation in unsupervised environments: from countermeasure to multimodality",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2020.11.016",
    "abstract": "Face-based age estimation systems are commonly considered in biometric applications as well as in other fields such as forensics or healthcare. For security purposes, features extracted from the face can be used to verify or estimate the age of individuals in order to control their access to physical or logical resources. The main problem in using facial biometrics is its sensitivity, to acquisition (e.g. illumination, pose, occlusion, image quality, etc.), to face expression, and especially to potential attacks in unsupervised environments. In this work, we propose a robust modality using both random auditory stimulation and Deep-learning based age estimation, though individual perception (RaS-DeeP): (1) as a countermeasure to prevent attacks on face-based age estimation systems, but also (2) : as a complementary modality in a multimodal biometric system (i.e. face-sound perception) in order to improve the performances of face-based age estimation system. Used as countermeasure, we show that RaS-DeeP provides promising results with an EER value of 4.2%. On the other hand, when considering the multimodal system face-auditory perception, we show that, the performance of face age estimation system is enhanced with an EER of 3.3%. To evaluate the performance of multimodal system in real-time, 71 subjects from different age ranges achieving five repetitions, participated in our experiment.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552030427X",
    "keywords": [
      "Artificial intelligence",
      "Biometrics",
      "Computer science",
      "Computer vision",
      "Economics",
      "Estimation",
      "Face (sociological concept)",
      "Machine learning",
      "Management",
      "Modality (human–computer interaction)",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Perception",
      "Psychology",
      "Social science",
      "Sociology",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Ilyas",
        "given_name": "Muhammad"
      },
      {
        "surname": "Nait-ali",
        "given_name": "Amine"
      }
    ]
  },
  {
    "title": "Keyword weight optimization using gradient strategies in event focused web crawling",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2020.12.003",
    "abstract": "At present, a need for an integrated event focused crawling system for obtaining web data regarding key events is felt. At the time of a disaster or any other important event, several users attempt to find updated information regarding the event. The work has proposed a new and efficient method for such keyword set enhancement. Today, information has been growing rapidly, and it can be very challenging for any search engine to retrieve the necessary information properly. A web crawler is a primary unit of such search engines, and for this, their optimization could have been a major aspect of improving the efficiency of search. The large size and active nature of web information and continuous documentation and data updates are known as the web-based retrieval system. This focused crawling method concentrates on the automatic webpage classification which was used for determining the web page. Though various classifiers are used for determining the webpages, the identification of keywords plays an important role in improving the event focused web crawling. The proposed work has a novel and efficient method for such keyword set enhancement. Metaheuristic based optimized keyword weights are found to be efficient. The Term Frequency (TF) based feature extraction and a keyword weight optimization using the Stochastic Gradient Descent (SGD) algorithm is employed in an event focused web crawling. Gradient descent is a popular algorithm to achieve optimization, and the stochastic algorithm has the advantage of sub-differentiable and differentiable smoothness in the fitness function and is well suited for large data optimization. The algorithm is focused on making the keyword set optimal, and in case the keyword set is found to be better, the result documents returned can be even more relevant to users' queries. For this, Support Vector Machine (SVM) classifiers are employed. The experimental outcomes proved that the suggested technique outperformed the others, including the Particle Swarm Optimization (PSO) based weight-optimized solution. The proposed SGD weight optimization is better by 5.8% compared to PSO, showing its ability to examine high volumes of data.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520304335",
    "keywords": [
      "Anatomy",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer security",
      "Crawling",
      "Data mining",
      "Event (particle physics)",
      "Gradient descent",
      "Information retrieval",
      "Key (lock)",
      "Keyword density",
      "Medicine",
      "Physics",
      "Programming language",
      "Quantum mechanics",
      "Search engine",
      "Search engine optimization",
      "Set (abstract data type)",
      "Stochastic gradient descent",
      "Web crawler",
      "Web page",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Rajiv",
        "given_name": "S"
      },
      {
        "surname": "Navaneethan",
        "given_name": "C"
      }
    ]
  },
  {
    "title": "Video super-resolution based on a spatio-temporal matching network",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107619",
    "abstract": "Deep spatio-temporal neural networks have shown promising performance for video super-resolution (VSR) in recent years. However, most of them heavily rely on accuracy motion estimations. In this paper, we propose a novel spatio-temporal matching network (STMN) for video super-resolution, which works on the wavelet domain to reduce dependence on motion estimations. Specifically, our STMN consists of three major components: a temporal fusion wavelet network (TFWN), a non-local matching network (NLMN), and a global wavelet domain residual connection (GWDRC). TFWN adaptively extracts temporal fusion wavelet maps via three 3d convolutional layers and a discrete wavelet transform (DWT) decomposition layer. The extracted temporal fusion wavelet maps are rich in spatial information and knowledge of different frequencies from consecutive frames, which are feed to NLMN for learning deep wavelet representations. NLMN integrates super-resolution and denoising into a unified module by pyramidally stacking non-local matching residual blocks (NLMRB). At last, GWDRC reconstructs the super-resolved frames from the deep wavelet representations by using global wavelet domain residual information. Consequently, our STMN can efficiently enhance reconstruction quality by capturing different frequencies wavelet representations in consecutive frames, and does not require any motion compensation. Extensive experiments conducted on benchmark datasets demonstrate the effectiveness of our method compared with state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320304222",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Discrete wavelet transform",
      "Lifting scheme",
      "Motion compensation",
      "Pattern recognition (psychology)",
      "Residual",
      "Second-generation wavelet transform",
      "Stationary wavelet transform",
      "Wavelet",
      "Wavelet packet decomposition",
      "Wavelet transform"
    ],
    "authors": [
      {
        "surname": "Zhu",
        "given_name": "Xiaobin"
      },
      {
        "surname": "Li",
        "given_name": "Zhuangzi"
      },
      {
        "surname": "Lou",
        "given_name": "Jungang"
      },
      {
        "surname": "Shen",
        "given_name": "Qing"
      }
    ]
  },
  {
    "title": "Multi-objective adaptive differential evolution for SVM/SVR hyperparameters selection",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107649",
    "abstract": "Parameters Selection Problem (PSP) is a relevant and complex optimization issue in Support Vector Machine (SVM) and Support Vector Regression (SVR), looking for obtaining an optimal set of hyperparameters. In our case, the optimization problem is addressed to obtain models that minimize the number of support vectors and maximize generalization capacity. However, to obtain accurate and low complexity solutions, defining an adequate kernel function and the SVM/SVR’s hyperparameters are necessary, which currently represents a relevant research topic. To tackle this problem, this work proposes a multi-objective metaheuristic named Adaptive Parameter control with Mutant Tournament Multi-Objective Differential Evolution (APMT-MODE). Its performance is first tested in a series of benchmarks for classification and regression problems using simple kernels such as Gaussian and polynomial kernels. In both cases, the APMT-MODE is able to yield more precise and more straightforward solutions using simple kernels. Then, the approach is used on a real case study to create a welding bead depth and width SVR models for a Gas Metal Arc Welding (GMAW) process. Additionally, a study on kernel functions was developed in terms of computational effort, aiming to assess its performance for embedded systems applications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320304520",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Differential evolution",
      "Hyperparameter",
      "Hyperparameter optimization",
      "Kernel (algebra)",
      "Kernel method",
      "Least squares support vector machine",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Polynomial kernel",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Santos",
        "given_name": "Carlos Eduardo da Silva"
      },
      {
        "surname": "Sampaio",
        "given_name": "Renato Coral"
      },
      {
        "surname": "Coelho",
        "given_name": "Leandro dos Santos"
      },
      {
        "surname": "Bestard",
        "given_name": "Guillermo Alvarez"
      },
      {
        "surname": "Llanos",
        "given_name": "Carlos Humberto"
      }
    ]
  },
  {
    "title": "Multimodal subspace support vector data description",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107648",
    "abstract": "In this paper, we propose a novel method for projecting data from multiple modalities to a new subspace optimized for one-class classification. The proposed method iteratively transforms the data from the original feature space of each modality to a new common feature space along with finding a joint compact description of data coming from all the modalities. For data in each modality, we define a separate transformation to map the data from the corresponding feature space to the new optimized subspace by exploiting the available information from the class of interest only. We also propose different regularization strategies for the proposed method and provide both linear and non-linear formulations. The proposed Multimodal Subspace Support Vector Data Description outperforms all the competing methods using data from a single modality or fusing data from all modalities in four out of five datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320304519",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Data mining",
      "Feature (linguistics)",
      "Feature vector",
      "Linguistics",
      "Modalities",
      "Modality (human–computer interaction)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Regularization (linguistics)",
      "Social science",
      "Sociology",
      "Subspace topology"
    ],
    "authors": [
      {
        "surname": "Sohrab",
        "given_name": "Fahad"
      },
      {
        "surname": "Raitoharju",
        "given_name": "Jenni"
      },
      {
        "surname": "Iosifidis",
        "given_name": "Alexandros"
      },
      {
        "surname": "Gabbouj",
        "given_name": "Moncef"
      }
    ]
  },
  {
    "title": "Hypergraph convolution and hypergraph attention",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107637",
    "abstract": "Recently, graph neural networks have attracted great attention and achieved prominent performance in various research fields. Most of those algorithms have assumed pairwise relationships of objects of interest. However, in many real applications, the relationships between objects are in higher-order, beyond a pairwise formulation. To efficiently learn deep embeddings on the high-order graph-structured data, we introduce two end-to-end trainable operators to the family of graph neural networks, i.e., hypergraph convolution and hypergraph attention. Whilst hypergraph convolution defines the basic formulation of performing convolution on a hypergraph, hypergraph attention further enhances the capacity of representation learning by leveraging an attention module. With the two operators, a graph neural network is readily extended to a more flexible model and applied to diverse applications where non-pairwise relationships are observed. Extensive experimental results with semi-supervised node classification demonstrate the effectiveness of hypergraph convolution and hypergraph attention.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320304404",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Convolution (computer science)",
      "Discrete mathematics",
      "Engineering",
      "Graph",
      "Hypergraph",
      "Mathematics",
      "Node (physics)",
      "Pairwise comparison",
      "Structural engineering",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Bai",
        "given_name": "Song"
      },
      {
        "surname": "Zhang",
        "given_name": "Feihu"
      },
      {
        "surname": "Torr",
        "given_name": "Philip H.S."
      }
    ]
  },
  {
    "title": "Entropy based dictionary learning for image classification",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107634",
    "abstract": "In this paper, a new discriminative dictionary learning algorithm is introduced. An entropy based criterion is embedded into the objective function to enforce a proper structure for the dictionary items when decomposing signals of different classes. The proposed criterion influences the dictionary items to participate in the decomposition of a smaller number of classes as possible. Unlike the other methods, columns of the dictionary are not restricted to have pre-assigned labels and they are free to be representative of any class or to share features of several classes. The number of shared and discriminative items along with the number of dictionary items for each specific class is learned dynamically during the optimization process, depending on the complexity of the classification task and the distribution of different classes. The experimental results demonstrate that the proposed entropy based dictionary learning (EDL) algorithm outperforms other discriminative dictionary learning methods using several real-world image datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320304374",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Contextual image classification",
      "Dictionary learning",
      "Discriminative model",
      "Entropy (arrow of time)",
      "Image (mathematics)",
      "K-SVD",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics"
    ],
    "authors": [
      {
        "surname": "Abdi",
        "given_name": "Arash"
      },
      {
        "surname": "Rahmati",
        "given_name": "Mohammad"
      },
      {
        "surname": "Ebadzadeh",
        "given_name": "Mohammad M."
      }
    ]
  },
  {
    "title": "Tackling mode collapse in multi-generator GANs with orthogonal vectors",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107646",
    "abstract": "Generative Adversarial Networks (GANs) have been widely used to generate realistic-looking instances. However, training robust GAN is a non-trivial task due to the problem of mode collapse. Although many GAN variants are proposed to overcome this problem, they have limitations. Those existing studies either generate identical instances or result in negative gradients during training. In this paper, we propose a new approach to training GAN to overcome mode collapse by employing a set of generators, an encoder and a discriminator. A new minimax formula is proposed to simultaneously train all components in a similar spirit to vanilla GAN. The orthogonal vector strategy is employed to guide multiple generators to learn different information in a complementary manner. In this way, we term our approach Multi-Generator Orthogonal GAN (MGO-GAN). Specifically, the synthetic data produced by those generators are fed into the encoder to obtain feature vectors. The orthogonal value is calculated between any two feature vectors, which loyally reflects the correlation between vectors. Such a correlation indicates how different information has been learnt by generators. The lower the orthogonal value is, the more different information the generators learn. We minimize the orthogonal value along with minimizing the generator loss through back-propagation in the training of GAN. The orthogonal value is integrated with the original generator loss to jointly update the corresponding generator’s parameters. We conduct extensive experiments utilizing MNIST, CIFAR10 and CelebA datasets to demonstrate the significant performance improvement of MGO-GAN in terms of generated data quality and diversity at different resolutions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320304490",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Deep learning",
      "Detector",
      "Discriminator",
      "Encoder",
      "Feature (linguistics)",
      "Feature vector",
      "Generator (circuit theory)",
      "Linguistics",
      "MNIST database",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Minimax",
      "Mode (computer interface)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Power (physics)",
      "Quantum mechanics",
      "Telecommunications",
      "Value (mathematics)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Wei"
      },
      {
        "surname": "Fan",
        "given_name": "Li"
      },
      {
        "surname": "Wang",
        "given_name": "Zhenyu"
      },
      {
        "surname": "Ma",
        "given_name": "Chao"
      },
      {
        "surname": "Cui",
        "given_name": "Xiaohui"
      }
    ]
  },
  {
    "title": "Deep snippet selective network for weakly supervised temporal action localization",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107686",
    "abstract": "Temporal action localization has been a hot topic in video analyzation. In this paper, we propose a novel method called deep snippet selective network (DSSN) to address two key problems in weak supervision for temporal action localization, which are separability and integrality. Specifically, we employ two erasing branches to ensure the integrality, which can force the network to select other complementary snippets by erasing the most discriminative snippets. It is worth mentioning that a ternary mask is utilized to provide erasing branches with a background prior to enhance the separability of the model. Besides, we design a background suppression branch to further reduce the effect of background snippets. Extensive experiments on dataset THUMOS’14 and ActivityNet show the effectiveness of our method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320304891",
    "keywords": [
      "Action (physics)",
      "Artificial intelligence",
      "Computer science",
      "Information retrieval",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Snippet"
    ],
    "authors": [
      {
        "surname": "Ge",
        "given_name": "Yongxin"
      },
      {
        "surname": "Qin",
        "given_name": "Xiaolei"
      },
      {
        "surname": "Yang",
        "given_name": "Dan"
      },
      {
        "surname": "Jagersand",
        "given_name": "Martin"
      }
    ]
  },
  {
    "title": "TextMountain: Accurate scene text detection via instance segmentation",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107336",
    "abstract": "In this paper, we propose a novel scene text detection method named TextMountain. The key idea of TextMountain is making full use of border-center information. Different from previous works that treat center-border as a binary classification problem, we predict text center-border probability (TCBP) and text center-direction (TCD). The TCBP is just like a mountain whose top is text center and foot is text border. The mountaintop can separate text instances which cannot be easily achieved using semantic segmentation map and its rising direction can plan a road to top for each pixel on mountain foot at the group stage. The TCD helps TCBP learning better. Our label rules will not lead to the ambiguous problem with the transformation of angle, so the proposed method is robust to multi-oriented text and can also handle well curved text. In inference stage, each pixel at the mountain foot needs to search the path to the mountaintop and this process can be efficiently completed in parallel, yielding the efficiency of our method compared with others. The experiments on MLT, ICDAR2015, RCTW-17 and SCUT-CTW1500 datasets demonstrate that the proposed method achieves better or comparable performance in terms of both accuracy and efficiency. It is worth mentioning our method achieves an F-measure of 76.85% on MLT which outperforms the previous methods by a large margin. Code will be made available.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320301394",
    "keywords": [
      "Artificial intelligence",
      "Center (category theory)",
      "Chemistry",
      "Code (set theory)",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Crystallography",
      "Data mining",
      "Image (mathematics)",
      "Inference",
      "Key (lock)",
      "Machine learning",
      "Margin (machine learning)",
      "Measure (data warehouse)",
      "Operating system",
      "Path (computing)",
      "Pattern recognition (psychology)",
      "Pixel",
      "Process (computing)",
      "Programming language",
      "Segmentation",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Zhu",
        "given_name": "Yixing"
      },
      {
        "surname": "Du",
        "given_name": "Jun"
      }
    ]
  },
  {
    "title": "Learning representation from multiple media domains for enhanced event discovery",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107640",
    "abstract": "In this paper, we focus on event discovery by utilizing data distributed in multiple media domains, such as news media and social media. To this end, we propose an in-domain and cross-domain Laplacian regularization (ICLR) model, which can learn effective data representation for both textual news reports contributed by journalists in news media domain, and image posts shared by amateur users in social media domain. The achieved data representation can be used by classification and clustering strategies for existing and new event discovery, respectively. More specifically, ICLR constructs respective Laplacian regularization terms considering the property of inter-domain and intra-domain label consistency, which can be optimized by employing an alternating optimization strategy with theoretical guarantee for convergence. In particular, we collect and release a multi-domain and multimodal dataset for evaluations and public use.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032030443X",
    "keywords": [
      "Amateur",
      "Archaeology",
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Consistency (knowledge bases)",
      "Domain (mathematical analysis)",
      "Event (particle physics)",
      "Focus (optics)",
      "Geography",
      "Graph",
      "Information retrieval",
      "Laplacian matrix",
      "Law",
      "Mathematical analysis",
      "Mathematics",
      "Optics",
      "Physics",
      "Political science",
      "Politics",
      "Quantum mechanics",
      "Regularization (linguistics)",
      "Representation (politics)",
      "Social media",
      "Theoretical computer science",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Zhenguo"
      },
      {
        "surname": "Li",
        "given_name": "Qing"
      },
      {
        "surname": "Xie",
        "given_name": "Haoran"
      },
      {
        "surname": "Wang",
        "given_name": "Qi"
      },
      {
        "surname": "Liu",
        "given_name": "Wenyin"
      }
    ]
  },
  {
    "title": "Virtual sample-based deep metric learning using discriminant analysis",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107643",
    "abstract": "Deep metric learning (DML) has been designed to maximize the inter-class variance that is the distance between embedding features belonging to different classes. Since conventional DML techniques do not consider the statistical characteristics of the embedding space, or they calculate similarity using only a given feature, they make it difficult to adaptively reflect the characteristics of the feature distribution during the learning process. This paper proposes a virtual metric loss (VML) incorporating with embedding features by using virtual samples produced through linear discriminant analysis (LDA). This study is valuable in that it proposes a new metric that can learn inter-class variance of embedding features by integrating discriminant analysis and metric learning which have a common purpose of inter-class variance analysis. In addition, we theoretically analyze the eigenvalue equation problem and the degree of stabilization in the embedding space. We have verified the performance of the proposed VML through extensive experiments on large and few-shot retrieval datasets. For example, in the CUB200-2011 dataset, the VML showed a recall rate about 0.7% higher than a state-of-the-art method. We also explored a new similarity through virtual samples and adjusted the difficulty of embedding features, thereby confirming the possibility of expanding virtual samples into various fields of pattern recognition.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320304465",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Economics",
      "Embedding",
      "Feature (linguistics)",
      "Feature vector",
      "Image (mathematics)",
      "Linear discriminant analysis",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Metric (unit)",
      "Metric space",
      "Operations management",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Kim",
        "given_name": "Dae Ha"
      },
      {
        "surname": "Song",
        "given_name": "Byung Cheol"
      }
    ]
  },
  {
    "title": "Real-time Lexicon-free Scene Text Retrieval",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107656",
    "abstract": "In this work, we address the task of scene text retrieval: given a text query, the system returns all images containing the queried text. The proposed model uses a single shot CNN architecture that predicts bounding boxes and builds a compact representation of spotted words. In this way, this problem can be modeled as a nearest neighbor search of the textual representation of a query over the outputs of the CNN collected from the totality of an image database. Our experiments demonstrate that the proposed model outperforms previous state-of-the-art, while offering a significant increase in processing speed and unmatched expressiveness with samples never seen at training time. Several experiments to assess the generalization capability of the model are conducted in a multilingual dataset, as well as an application of real-time text spotting in videos.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320304593",
    "keywords": [
      "Artificial intelligence",
      "Bounding overwatch",
      "Computer science",
      "Economics",
      "Generalization",
      "Image (mathematics)",
      "Information retrieval",
      "Law",
      "Lexicon",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Minimum bounding box",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Spotting",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Mafla",
        "given_name": "Andrés"
      },
      {
        "surname": "Tito",
        "given_name": "Rubèn"
      },
      {
        "surname": "Dey",
        "given_name": "Sounak"
      },
      {
        "surname": "Gómez",
        "given_name": "Lluís"
      },
      {
        "surname": "Rusiñol",
        "given_name": "Marçal"
      },
      {
        "surname": "Valveny",
        "given_name": "Ernest"
      },
      {
        "surname": "Karatzas",
        "given_name": "Dimosthenis"
      }
    ]
  },
  {
    "title": "Goal driven network pruning for object recognition",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107468",
    "abstract": "Pruning studies up to date focused on uncovering a smaller network by removing redundant units, and fine-tuning to compensate accuracy drop as a result. In this study, unlike the others, we propose an approach to uncover a smaller network that is competent only in a specific task, similar to top-down attention mechanism in human visual system. This approach doesn’t require fine-tuning and is proposed as a fast and effective alternative of training from scratch when the network focuses on a specific task in the dataset. Pruning starts from the output and proceeds towards the input by computing neuron importance scores in each layer and propagating them to the preceding layer. In the meantime, neurons determined as worthless are pruned. We applied our approach on three benchmark datasets: MNIST, CIFAR-10 and ImageNet. The results demonstrate that the proposed pruning method typically reduces computational units and storage without harming accuracy significantly.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320302715",
    "keywords": [
      "Agronomy",
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Biology",
      "Chemistry",
      "Cognitive neuroscience of visual object recognition",
      "Computer science",
      "Economics",
      "Geodesy",
      "Geography",
      "Layer (electronics)",
      "MNIST database",
      "Machine learning",
      "Management",
      "Object (grammar)",
      "Operating system",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Pruning",
      "Scratch",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Kaplan",
        "given_name": "Cagri"
      },
      {
        "surname": "Bulbul",
        "given_name": "Abdullah"
      }
    ]
  },
  {
    "title": "A semi-supervised sparse K-Means algorithm",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2020.11.015",
    "abstract": "We consider the problem of data clustering with unidentified feature quality and when a small amount of labelled data is provided. An unsupervised sparse clustering method can be employed in order to detect the subgroup of features necessary for clustering and a semi-supervised method can use the labelled data to create constraints and enhance the clustering solution. In this paper we propose a K-Means variant that employs these techniques. We show that the algorithm maintains the high performance of other semi-supervised algorithms and in addition preserves the ability to identify informative from uninformative features. We examine the performance of the algorithm on synthetic and real world data sets. We use scenarios with a different amount and types of constraints as well as different clustering initialisation methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520304268",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "CURE data clustering algorithm",
      "Canopy clustering algorithm",
      "Cluster analysis",
      "Computer science",
      "Correlation clustering",
      "Data mining",
      "Data stream clustering",
      "Feature (linguistics)",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Vouros",
        "given_name": "Avgoustinos"
      },
      {
        "surname": "Vasilaki",
        "given_name": "Eleni"
      }
    ]
  },
  {
    "title": "Improving visible-thermal ReID with structural common space embedding and part models",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2020.11.020",
    "abstract": "With the emergence of large-scale datasets and deep learning systems, person re-identification(Re-ID) has made many significant breakthroughs. Meanwhile, Visible-Thermal person re-identification(V-T Re-ID) between visible and thermal images has also received ever-increasing attention. However, most of typical visible-visible person re-identification(V-V Re-ID) algorithms are difficult to be directly applied to the task of V-T Re-ID, due to the large cross-modality intra-class and inter-class variation. In this paper, we build an end-to-end dual-path spatial-structure-preserving common space network to transfer some V-V Re-ID methods to V-T Re-ID domain effectively. The framework mainly consists of two parts: a modility specific feature embedding network and a common feature space. Benefiting from the common space, our framework can abstract attentive common information by learning local feature representations for V-T Re-ID. We conduct extensive experiments on the publicly available RGB-IR re-ID benchmark datasets, SYSUMM01 and RegDB, for demonstration of the effectiveness of bridging the gap between V-V Re-ID and V-T Re-ID. Experimental results achieves the state-of-the-art performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520304347",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Botany",
      "Bridging (networking)",
      "Class (philosophy)",
      "Computer network",
      "Computer science",
      "Embedding",
      "Feature (linguistics)",
      "Feature learning",
      "Feature vector",
      "Geodesy",
      "Geography",
      "Identification (biology)",
      "Linguistics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "RGB color model",
      "Space (punctuation)",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Ran",
        "given_name": "Lingyan"
      },
      {
        "surname": "Hong",
        "given_name": "Yujun"
      },
      {
        "surname": "Zhang",
        "given_name": "Shizhou"
      },
      {
        "surname": "Yang",
        "given_name": "Yifei"
      },
      {
        "surname": "Zhang",
        "given_name": "Yanning"
      }
    ]
  },
  {
    "title": "Deep features for person re-identification on metric learning",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107424",
    "abstract": "Person re-identification, a branch of image retrieval, is an increasingly important public safety application. When monitoring larger areas, it is crucial to correctly match the same person in different camera views. With the emergence of deep learning and large-scale data, metric learning has significantly improved person re-identification performance, but the extent to which deep features affect metric learning performance is unknown. However, given the large number of approaches, datasets, evaluation indices, and experimental environments, comparing metric learning methods directly is difficult. To obtain a more comprehensive empirical evaluation of the person re-identification, here we summarize the different types of features and metric learning approaches from a label attributes perspective. Then, by combining advanced approaches to data enhancement and feature extraction, we conduct comprehensive experiments on metric learning methods with two datasets. For fairness, all methods use a unified code library that includes two data enhancement schemes, eight feature extraction algorithms, and eight metric learning methods. Our results show that, the relations of loss function with deep feature space and metric learning.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320302272",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Computer science",
      "Deep learning",
      "Economics",
      "Feature (linguistics)",
      "Feature extraction",
      "Feature vector",
      "Identification (biology)",
      "Linguistics",
      "Machine learning",
      "Metric (unit)",
      "Operations management",
      "Pattern recognition (psychology)",
      "Perspective (graphical)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Wanyin"
      },
      {
        "surname": "Tao",
        "given_name": "Dapeng"
      },
      {
        "surname": "Li",
        "given_name": "Hao"
      },
      {
        "surname": "Yang",
        "given_name": "Zhao"
      },
      {
        "surname": "Cheng",
        "given_name": "Jun"
      }
    ]
  },
  {
    "title": "Automatically discriminating and localizing COVID-19 from community-acquired pneumonia on chest X-rays",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107613",
    "abstract": "The COVID-19 outbreak continues to threaten the health and life of people worldwide. It is an immediate priority to develop and test a computer-aided detection (CAD) scheme based on deep learning (DL) to automatically localize and differentiate COVID-19 from community-acquired pneumonia (CAP) on chest X-rays. Therefore, this study aims to develop and test an efficient and accurate deep learning scheme that assists radiologists in automatically recognizing and localizing COVID-19. A retrospective chest X-ray image dataset was collected from open image data and the Xiangya Hospital, which was divided into a training group and a testing group. The proposed CAD framework is composed of two steps with DLs: the Discrimination-DL and the Localization-DL. The first DL was developed to extract lung features from chest X-ray radiographs for COVID-19 discrimination and trained using 3548 chest X-ray radiographs. The second DL was trained with 406-pixel patches and applied to the recognized X-ray radiographs to localize and assign them into the left lung, right lung or bipulmonary. X-ray radiographs of CAP and healthy controls were enrolled to evaluate the robustness of the model. Compared to the radiologists’ discrimination and localization results, the accuracy of COVID-19 discrimination using the Discrimination-DL yielded 98.71%, while the accuracy of localization using the Localization-DL was 93.03%. This work represents the feasibility of using a novel deep learning-based CAD scheme to efficiently and accurately distinguish COVID-19 from CAP and detect localization with high accuracy and agreement with radiologists.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320304167",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Biology",
      "CAD",
      "Computer science",
      "Computer-aided diagnosis",
      "Coronavirus disease 2019 (COVID-19)",
      "Deep learning",
      "Disease",
      "Gene",
      "Infectious disease (medical specialty)",
      "Internal medicine",
      "Medicine",
      "Pathology",
      "Pattern recognition (psychology)",
      "Pneumonia",
      "Radiography",
      "Radiology",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Zheng"
      },
      {
        "surname": "Xiao",
        "given_name": "Ying"
      },
      {
        "surname": "Li",
        "given_name": "Yong"
      },
      {
        "surname": "Zhang",
        "given_name": "Jie"
      },
      {
        "surname": "Lu",
        "given_name": "Fanggen"
      },
      {
        "surname": "Hou",
        "given_name": "Muzhou"
      },
      {
        "surname": "Liu",
        "given_name": "Xiaowei"
      }
    ]
  },
  {
    "title": "Efficient semantic segmentation with pyramidal fusion",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107611",
    "abstract": "Emergence of large datasets and resilience of convolutional models have enabled successful training of very large semantic segmentation models. However, high capacity implies high computational complexity and therefore hinders real-time operation. We therefore study compact architectures which aim at high accuracy in spite of modest capacity. We propose a novel semantic segmentation approach based on shared pyramidal representation and fusion of heterogeneous features along the upsampling path. The proposed pyramidal fusion approach is especially effective for dense inference in images with large scale variance due to strong regularization effects induced by feature sharing across the resolution pyramid. Interpretation of the decision process suggests that our approach succeeds by acting as a large ensemble of relatively simple models, as well as due to large receptive range and strong gradient flow towards early layers. Our best model achieves 76.4% mIoU on Cityscapes test and runs in real time on low-power embedded devices.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320304143",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Geometry",
      "Image (mathematics)",
      "Inference",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pyramid (geometry)",
      "Regularization (linguistics)",
      "Segmentation",
      "Upsampling"
    ],
    "authors": [
      {
        "surname": "Oršić",
        "given_name": "Marin"
      },
      {
        "surname": "Šegvić",
        "given_name": "Siniša"
      }
    ]
  },
  {
    "title": "Unified Cross-domain Classification via Geometric and Statistical Adaptations",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107658",
    "abstract": "Domain adaptation aims to learn an adaptive classifier for target data using the labelled source data from a different distribution. Most proposed works construct cross-domain classifier by exploring one-sided property of the input data, i.e., either geometric or statistical property. Therefore they may ignore the complementarity between the two properties. Moreover, many previous methods implement knowledge transfer with two separated steps: divergence minimization and classifier construction, which degrades the adaptation robustness. In order to address such problems, we propose a u nified c ross-domain classification method via g eometric and s tatistical adaptations (UCGS). UCGS models the divergence minimization and classifier construction in a unified way based on structural risk minimization principle and coupled adaptations theory. Specifically, UCGS constructs an adaptive model by simultaneously minimizing the structural risk on labelled source data, using Maximum Mean Discrepancy (MMD) criterion to implement statistical adaptation, and flexibly employing the Nyström method to explore the geometric connections between domains. A domain-invariant graph is successfully constructed to link the two domains geometrically. The standard supervised methods can be used to instantiate UCGS to handle inter-domain classification problems. Comprehensive experiments show the superiority of UCGS on several real-world datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320304611",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Domain adaptation",
      "Machine learning",
      "Mathematics",
      "Minification",
      "Pattern recognition (psychology)",
      "Probability distribution",
      "Programming language",
      "Statistics",
      "Structural risk minimization",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Weifeng"
      },
      {
        "surname": "Li",
        "given_name": "Jinfeng"
      },
      {
        "surname": "Liu",
        "given_name": "Baodi"
      },
      {
        "surname": "Guan",
        "given_name": "Weili"
      },
      {
        "surname": "Zhou",
        "given_name": "Yicong"
      },
      {
        "surname": "Xu",
        "given_name": "Changsheng"
      }
    ]
  },
  {
    "title": "Unsupervised text-to-image synthesis",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107573",
    "abstract": "Recently, text-to-image synthesis has achieved great progresses with the advancement of the Generative Adversarial Network (GAN). However, training the GAN models requires a large amount of pairwise image-text data, which is extremely labor-intensive to collect. In this paper, we make the first attempt to train a text-to-image synthesis model in an unsupervised manner, which does not require any human labeled image-text pair data. Specifically, we first rely on the visual concepts to bridge two independent image and sentence sets and thereby yield the pseudo image-text pair data, based on which one GAN model can thereby be initialized. One novel visual concept discrimination loss is proposed to train both generator and discriminator, which not only encourages the image expressing the true local visual concepts but also ensures the noisy visual concepts contained in the pseudo sentence being suppressed. Afterwards, one global semantic consistency regarding to the real sentence is used to adapt the pretrained GAN model to real sentences. Experimental results demonstrate that our proposed unsupervised training strategy is able to generate favorable images for given sentences, which even outperforms some existing models trained in the supervised manner. The code of this paper is available at https://github.com/dylls/Unsupervised_Text-to-Image_Synthesis.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303769",
    "keywords": [
      "Artificial intelligence",
      "Code (set theory)",
      "Computer science",
      "Consistency (knowledge bases)",
      "Detector",
      "Discriminator",
      "Generator (circuit theory)",
      "Image (mathematics)",
      "Natural language processing",
      "Pairwise comparison",
      "Pattern recognition (psychology)",
      "Physics",
      "Power (physics)",
      "Programming language",
      "Quantum mechanics",
      "Sentence",
      "Set (abstract data type)",
      "Telecommunications",
      "Unsupervised learning"
    ],
    "authors": [
      {
        "surname": "Dong",
        "given_name": "Yanlong"
      },
      {
        "surname": "Zhang",
        "given_name": "Ying"
      },
      {
        "surname": "Ma",
        "given_name": "Lin"
      },
      {
        "surname": "Wang",
        "given_name": "Zhi"
      },
      {
        "surname": "Luo",
        "given_name": "Jiebo"
      }
    ]
  },
  {
    "title": "AGUnet: Annotation-guided U-net for fast one-shot video object segmentation",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107580",
    "abstract": "The problem of semi-supervised video object segmentation has been popularly tackled by fine-tuning a general-purpose segmentation deep network on the annotated frame using hundreds of iterations of gradient descent. The time-consuming fine-tuning process, however, makes these methods difficult to use in practical applications. We propose a novel architecture called Annotation Guided U-net (AGUnet) for fast one-shot video object segmentation (VOS). AGUnet can quickly adapt a model trained on static images to segmenting the given target in a video by only several iterations of gradient descent. Our AGUnet is inspired by interactive image segmentation, where the interested target is segmented by using user annotated foreground. However, in AGUnet we use a fully-convolutional Siamese network to automatically annotate the foreground and background regions and fuse such annotation information into the skip connection of a U-net for VOS. Our AGUnet can be trained end-to-end effectively on static images instead of video sequences as required by many previous methods. The experiments show that AGUnet runs much faster than current state-of-the-art one-shot VOS algorithms while achieving competitive accuracy, and it has high generalization capability.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303836",
    "keywords": [
      "Annotation",
      "Artificial intelligence",
      "Artificial neural network",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Electrical engineering",
      "Engineering",
      "Frame (networking)",
      "Fuse (electrical)",
      "Gradient descent",
      "Image segmentation",
      "Object (grammar)",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Segmentation",
      "Shot (pellet)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Yin",
        "given_name": "Yingjie"
      },
      {
        "surname": "Xu",
        "given_name": "De"
      },
      {
        "surname": "Wang",
        "given_name": "Xingang"
      },
      {
        "surname": "Zhang",
        "given_name": "Lei"
      }
    ]
  },
  {
    "title": "Artificial intelligence for distributed smart systems",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2020.12.006",
    "abstract": "This paper is the editorial of the virtual special issue (VSI) “Artificial Intelligence for Distributed Smart Systems” (AI4DSS), of which the authors of this paper have been the guest editors. It aims to bring together the work of experts from the fields of artificial intelligence and that of smart sensing. Smart Sensing and, more generally, Smart Cyber Physical Systems are nowadays significantly impacting the everyday life of citizens and, in perspective, they will become pervasive in every aspect of human life from public health and well-being to home, infrastructures and environment management. Another important issue is related to the possibility of exploiting collaborative approaches through Distributed Architectures. In this kind of applications, smart sensors are spread into the environment of interest where some kind of “social intelligence” is generated. The papers included in this special issue allowed us to highlight the advances on this subject in three areas that are: Smart Cities, Smart Industries, Smart Healthcare.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520304372",
    "keywords": [
      "Ambient intelligence",
      "Artificial intelligence",
      "Building automation",
      "Computer network",
      "Computer science",
      "Computer security",
      "Data science",
      "Everyday life",
      "Human–computer interaction",
      "Intelligent decision support system",
      "Intelligent sensor",
      "Internet of Things",
      "Law",
      "Perspective (graphical)",
      "Physics",
      "Political science",
      "Smart environment",
      "Smart objects",
      "Smart system",
      "Thermodynamics",
      "Ubiquitous computing",
      "Wireless sensor network"
    ],
    "authors": [
      {
        "surname": "Molinara",
        "given_name": "M."
      },
      {
        "surname": "Bria",
        "given_name": "A."
      },
      {
        "surname": "De Vito",
        "given_name": "S."
      },
      {
        "surname": "Marrocco",
        "given_name": "C."
      }
    ]
  },
  {
    "title": "Convolution operations for relief-pattern retrieval, segmentation and classification on mesh manifolds",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2020.11.017",
    "abstract": "Relief patterns represent a surface characteristic that is well distinct from the 3D object shape. They can be seen as the 3D counterpart of the texture concept in the 2D images. A large part of texture analysis, in 2D image state-of-the-art, relies on some convolution-based filtering. Thus, the idea of extending such techniques to the mesh manifold domain is quite natural. Nevertheless, defining a convolution operator on a mesh manifold is not straightforward. In this paper, we propose two frameworks, namely, Mesh-Grid and Mesh-Convolution, to apply discrete and continuous filters directly on the mesh. We tested Mesh-Grid and Mesh-Convolution in the task of geometric texture retrieval, providing, to the best of our knowledge, the first results on the SHREC’18 dataset. Then, our convolution operator revealed to be effective also in the task of relief pattern classification on the SHREC’17 dataset, outperforming the state-of-the-art results. Finally, we propose a geometric texture segmentation approach to support manual annotation on large datasets, which revealed to be effective.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520304293",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer vision",
      "Convolution (computer science)",
      "Engineering",
      "Geometry",
      "Grid",
      "Image (mathematics)",
      "Manifold (fluid mechanics)",
      "Mathematics",
      "Mechanical engineering",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Segmentation",
      "Texture (cosmology)"
    ],
    "authors": [
      {
        "surname": "Tortorici",
        "given_name": "Claudio"
      },
      {
        "surname": "Berretti",
        "given_name": "Stefano"
      },
      {
        "surname": "Obeid",
        "given_name": "Ahmad"
      },
      {
        "surname": "Werghi",
        "given_name": "Naoufel"
      }
    ]
  },
  {
    "title": "Constructing multilayer locality-constrained matrix regression framework for noise robust face super-resolution",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107539",
    "abstract": "Representation learning methods have attracted considerable attention for learning-based face super-resolution in recent years. Conventional methods perform local models learning on low-resolution (LR) manifold and face reconstruction on high-resolution (HR) manifold respectively, leading to unsatisfactory reconstruction performance when the acquired LR face images are severely degraded (e.g., noisy, blurred). To tackle this issue, this paper proposes an efficient multilayer locality-constrained matrix regression (MLCMR) framework to learn the representation of the input LR patch and meanwhile preserve the manifold of the original HR space. Particularly, MLCMR uses nuclear norm regularization to capture the structural characteristic of the representation residual and applies an adaptive neighborhood selection scheme to find the HR patches that are compatible with its neighbors. Also, MLCMR iteratively applies the manifold structure of the desired HR space to induce the representation weights learning in the LR space, aims at reducing the inconsistency gap between different manifolds. Experimental results on widely used FEI database and real-world faces have demonstrated that compared with several state-of-the-art face super-resolution approaches, our proposed approach has the capability of obtaining better results both in objective metrics and visual quality.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303423",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Dimensionality reduction",
      "Eigenvalues and eigenvectors",
      "Engineering",
      "Face (sociological concept)",
      "Law",
      "Linguistics",
      "Locality",
      "Manifold (fluid mechanics)",
      "Mathematics",
      "Matrix norm",
      "Mechanical engineering",
      "Nonlinear dimensionality reduction",
      "Norm (philosophy)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Political science",
      "Politics",
      "Quantum mechanics",
      "Representation (politics)",
      "Residual",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Gao",
        "given_name": "Guangwei"
      },
      {
        "surname": "Yu",
        "given_name": "Yi"
      },
      {
        "surname": "Xie",
        "given_name": "Jin"
      },
      {
        "surname": "Yang",
        "given_name": "Jian"
      },
      {
        "surname": "Yang",
        "given_name": "Meng"
      },
      {
        "surname": "Zhang",
        "given_name": "Jian"
      }
    ]
  },
  {
    "title": "A prototype-based SPD matrix network for domain adaptation EEG emotion recognition",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107626",
    "abstract": "Emotion plays a vital role in human daily life, and EEG signals are widely used in emotion recognition. Due to individual variability, training a generic emotion recognition model across different subjects is difficult. The conventional method involves the collection of a large amount of calibration data to build subject-specific models. Recently, developing an effective brain-computer interface with a short calibration time has become a challenge. To solve this problem, we propose a domain adaptation SPD matrix network (daSPDnet) that can successfully capture an intrinsic emotional representation shared between different subjects. Our method jointly exploits feature adaptation with distribution confusion and sample adaptation with centroid alignment. We compute the SPD matrix based on the covariance as a feature and make a novel attempt to combine prototype learning with the Riemannian metric. Extensive experiments are conducted on the DREAMER and DEAP datasets, and the results show the superiority of our proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320304295",
    "keywords": [
      "Adaptation (eye)",
      "Artificial intelligence",
      "Computer science",
      "Confusion matrix",
      "Domain (mathematical analysis)",
      "Economics",
      "Feature (linguistics)",
      "Image (mathematics)",
      "Law",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Metric (unit)",
      "Neuroscience",
      "Operations management",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Psychology",
      "Representation (politics)",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Yixin"
      },
      {
        "surname": "Qiu",
        "given_name": "Shuang"
      },
      {
        "surname": "Ma",
        "given_name": "Xuelin"
      },
      {
        "surname": "He",
        "given_name": "Huiguang"
      }
    ]
  },
  {
    "title": "Universal adversarial perturbations against object detection",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107584",
    "abstract": "Despite the remarkable success of deep neural networks on many visual tasks, they have been proved to be vulnerable to adversarial examples. For visual tasks, adversarial examples are images added with visually imperceptible perturbations that result in failure for recognition. Previous works have demonstrated that adversarial perturbations can cause neural networks to fail on object detection. But these methods focus on generating an adversarial perturbation for a specific image, which is the image-specific perturbation. This paper tries to extend such image-level adversarial perturbations to detector-level, which are universal (image-agnostic) adversarial perturbations. Motivated by this, we propose a Universal Dense Object Suppression (U-DOS) algorithm to derive the universal adversarial perturbations against object detection and show that such perturbations with visual imperceptibility can lead the state-of-the-art detectors to fail in finding any objects in most images. Compared to image-specific perturbations, the results of image-agnostic perturbations are more interesting and also pose more challenges in AI security, because they are more convenient to be applied in the real physical world. We also analyze the generalization of such universal adversarial perturbations across different detectors and datasets under the black-box attack settings, showing it’s a simple but promising adversarial attack approach against object detection. Furthermore, we validate the class-specific universal perturbations, which can remove the detection results of the target class and keep others unchanged.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303873",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Artificial neural network",
      "Class (philosophy)",
      "Computer science",
      "Computer vision",
      "Deep neural networks",
      "Focus (optics)",
      "Generalization",
      "Image (mathematics)",
      "Mathematical analysis",
      "Mathematics",
      "Object (grammar)",
      "Object detection",
      "Optics",
      "Pattern recognition (psychology)",
      "Perturbation (astronomy)",
      "Physics",
      "Quantum mechanics"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Debang"
      },
      {
        "surname": "Zhang",
        "given_name": "Junge"
      },
      {
        "surname": "Huang",
        "given_name": "Kaiqi"
      }
    ]
  },
  {
    "title": "LOW: Training deep neural networks by learning optimal sample weights",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107585",
    "abstract": "The performance of deep learning (DL) models is highly dependent on the quality and size of the training data, whose annotations are often expensive and hard to obtain. This work proposes a new strategy to train DL models by Learning Optimal samples Weights (LOW), making better use of the available data. LOW determines how much each sample in a batch should contribute to the training process, by automatically estimating its weight in the loss function. This effectively forces the model to focus on more relevant samples. Consequently, the models exhibit a faster convergence and better generalization, specially on imbalanced data sets where class distribution is long-tailed. LOW can be easily integrated to train any DL model and can be combined with any loss function, while adding marginal computational burden to the training process. Additionally, the analysis of how sample weights change during training provides insights on what the model is learning and which samples or classes are more challenging. Results on popular computer vision benchmarks and on medical data sets show that DL models trained with LOW perform better than with other state-of-the-art strategies. 1 1 Code is available at https://github.com/cajosantiago/LOW.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303885",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Chemistry",
      "Chromatography",
      "Class (philosophy)",
      "Computer science",
      "Convergence (economics)",
      "Data mining",
      "Deep learning",
      "Economic growth",
      "Economics",
      "Evolutionary biology",
      "Focus (optics)",
      "Function (biology)",
      "Generalization",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Meteorology",
      "Operating system",
      "Optics",
      "Physics",
      "Process (computing)",
      "Sample (material)",
      "Training (meteorology)"
    ],
    "authors": [
      {
        "surname": "Santiago",
        "given_name": "Carlos"
      },
      {
        "surname": "Barata",
        "given_name": "Catarina"
      },
      {
        "surname": "Sasdelli",
        "given_name": "Michele"
      },
      {
        "surname": "Carneiro",
        "given_name": "Gustavo"
      },
      {
        "surname": "Nascimento",
        "given_name": "Jacinto C."
      }
    ]
  },
  {
    "title": "Virtual special issue on advanced deep learning methods for biomedical engineering",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2020.12.005",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520304360",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Deep learning"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Yu-Dong"
      },
      {
        "surname": "Dong",
        "given_name": "Zhengchao"
      },
      {
        "surname": "Li",
        "given_name": "Shuai"
      },
      {
        "surname": "Jain",
        "given_name": "Deepak Kumar"
      }
    ]
  },
  {
    "title": "Probabilistic framework for solving visual dialog",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107586",
    "abstract": "In this paper, we propose a probabilistic framework for solving the task of ‘Visual Dialog’. Solving this task requires reasoning and understanding of visual modality, language modality, and common sense knowledge to answer. Various architectures have been proposed to solve this task by variants of multi-modal deep learning techniques that combine visual and language representations. However, we believe that it is crucial to understand and analyze the sources of uncertainty for solving this task. Our approach allows for estimating uncertainty and also aids a diverse generation of answers. The proposed approach is obtained through a probabilistic representation module that provides us with representations for image, question and conversation history, a module that ensures that diverse latent representations for candidate answers are obtained given the probabilistic representations and an uncertainty representation module that chooses the appropriate answer that minimizes uncertainty. We thoroughly evaluate the model with a detailed ablation analysis, comparison with state of the art and visualization of the uncertainty that aids in the understanding of the method. Using the proposed probabilistic framework, we thus obtain an improved visual dialog system that is also more explainable.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303897",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Conversation",
      "Dialog box",
      "Economics",
      "Law",
      "Linguistics",
      "Machine learning",
      "Management",
      "Natural language processing",
      "Philosophy",
      "Political science",
      "Politics",
      "Probabilistic logic",
      "Representation (politics)",
      "Task (project management)",
      "Visualization",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Patro",
        "given_name": "Badri N."
      },
      {
        "surname": "Anupriy",
        "given_name": ""
      },
      {
        "surname": "Namboodiri",
        "given_name": "Vinay P."
      }
    ]
  },
  {
    "title": "A novel error-correcting output codes based on genetic programming and ternary digit operators",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107642",
    "abstract": "The key to the success of an Error-Correcting Output Code (ECOC) algorithm is the effective codematrix, which represents a set of class reassignment schemes for decomposing a multiclass problem into a set of binary class problems. This paper proposes a new method, which uses Ternary digit Operators based Genetic Programming (GP) to generate effective ECOC codematrix (TOGP-ECOC for short). In our GP, each terminal node stores a ternary digit string, representing a column and a related feature subset; each non-terminal node represents a ternary digit operator, which produces a new column based on its child nodes. In this way, each individual is interpreted as an ECOC codematrix along with a set of corresponding feature subsets, serving the solution for the multiclass classification task. When a new individual is produced, a legality checking process is carried out to verify whether the transformed codematrix follows the ECOC constraints. The illegal one is corrected according to different strategies. Besides, a local optimization algorithm is designed to prune redundant columns and improve the performance of each individual. Our experiments compared TOGP-ECOC with some well known ECOC algorithms on various data sets, and the results confirm the superiority of our algorithm. Our source code is available at: https://github.com/MLDMXM2017/TOGP-ECOC.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320304453",
    "keywords": [
      "Algorithm",
      "Arithmetic",
      "Artificial intelligence",
      "Binary number",
      "Biochemistry",
      "Bitwise operation",
      "Chemistry",
      "Class (philosophy)",
      "Code (set theory)",
      "Computer science",
      "Engineering",
      "Feature (linguistics)",
      "Gene",
      "Genetic programming",
      "Linguistics",
      "Mathematics",
      "Multiclass classification",
      "Node (physics)",
      "Operator (biology)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Repressor",
      "Set (abstract data type)",
      "Structural engineering",
      "Support vector machine",
      "Ternary operation",
      "Transcription factor"
    ],
    "authors": [
      {
        "surname": "Yi-Fan",
        "given_name": "Liang"
      },
      {
        "surname": "Chang",
        "given_name": "Liu"
      },
      {
        "surname": "Han-Rui",
        "given_name": "Wang"
      },
      {
        "surname": "Kun-Hong",
        "given_name": "Liu"
      },
      {
        "surname": "Jun-Feng",
        "given_name": "Yao"
      },
      {
        "surname": "Ying-Ying",
        "given_name": "She"
      },
      {
        "surname": "Gui-Ming",
        "given_name": "Dai"
      },
      {
        "surname": "Okina",
        "given_name": "Yuna"
      }
    ]
  },
  {
    "title": "Coupled adversarial learning for semi-supervised heterogeneous face recognition",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107618",
    "abstract": "Visible-near infrared (VIS-NIR) face matching is a challenging issue in heterogeneous face recognition due to the large spectrum domain discrepancy as well as the over-fitting on insufficient pairwise VIS and NIR images during training. This paper proposes a coupled adversarial learning (CAL) approach for the VIS-NIR face matching by performing adversarial learning on both image and feature levels. On the image level, we learn a transformation network from unpaired NIR-VIS images to transform a NIR image to VIS domain. Cycle loss, global intensity loss and local texture loss are employed to better capture the discrepancy between NIR and VIS domains. The synthesized NIR or VIS images can be further used to alleviate the over-fitting problem in a semi-supervised way. On the feature level, we seek a shared feature space in which the heterogeneous face matching problem can be approximately treated as a homogeneous face matching problem. An adversarial loss and an orthogonal constraint are employed to reduce the spectrum domain discrepancy and the over-fitting problem, respectively. Experimental results show that CAL not only synthesizes high-quality VIS or NIR images, but also obtains state-of-the-art recognition results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320304210",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Domain (mathematical analysis)",
      "Face (sociological concept)",
      "Facial recognition system",
      "Feature (linguistics)",
      "Gene",
      "Image (mathematics)",
      "Linguistics",
      "Matching (statistics)",
      "Mathematical analysis",
      "Mathematics",
      "Pairwise comparison",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Social science",
      "Sociology",
      "Statistics",
      "Transformation (genetics)"
    ],
    "authors": [
      {
        "surname": "He",
        "given_name": "Ran"
      },
      {
        "surname": "Li",
        "given_name": "Yi"
      },
      {
        "surname": "Wu",
        "given_name": "Xiang"
      },
      {
        "surname": "Song",
        "given_name": "Lingxiao"
      },
      {
        "surname": "Chai",
        "given_name": "Zhenhua"
      },
      {
        "surname": "Wei",
        "given_name": "Xiaolin"
      }
    ]
  },
  {
    "title": "CNAK: Cluster number assisted K-means",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107625",
    "abstract": "The K-means clustering algorithm is well-known for its easy computational approach. In this algorithm, essential cluster-level information is captured by the K cluster centroids. However, how many such centroids can reveal the structure of the underlying data depends upon the choice of K. In this paper, we propose a clustering algorithm in which the number of cluster K can be learned as well as it performs the clustering. Our work revolves around two observations: i) a large-sized random sampled dataset may have a similar distribution as the original data, and ii) for the true number of clusters their centroids, generated from a sampled datasets, approximate the cluster centroids generated from the original dataset. The first observation has paved the way to provide a scalable solution, and the second one forms the key aspect of building the proposed algorithm. We have tested our method on several real and synthetic datasets. Our method can solve a few pertinent issues of clustering a dataset: 1) detection of a single cluster in the absence of any other cluster in a dataset, 2) the presence of hierarchy, 3) clustering of a high dimensional dataset, 4) robustness over dataset having cluster imbalance, and 5) robustness to noise. We have observed significant improvement in speed and quality for predicting cluster numbers as well as the composition of clusters in a large dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320304283",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Cluster (spacecraft)",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Economics",
      "Gene",
      "Hierarchy",
      "Machine learning",
      "Market economy",
      "Mathematics",
      "Measure (data warehouse)",
      "Monte Carlo method",
      "Programming language",
      "Robustness (evolution)",
      "Stability (learning theory)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Saha",
        "given_name": "Jayasree"
      },
      {
        "surname": "Mukherjee",
        "given_name": "Jayanta"
      }
    ]
  },
  {
    "title": "Challenging tough samples in unsupervised domain adaptation",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107540",
    "abstract": "Existing domain adaptation approaches focus on taking advantage of easy samples, i.e, target samples which are easier for adaptation. In previous work, tough, or hard, target samples are generally regarded as outliers or just being left to chance. As a result, the adaptation of tough target samples remains as a challenging problem in the community. In this paper, we report three novel ideas for domain adaptation: 1) splitting target samples into easy and tough ones; 2) deploying different strategies for samples with different adaptation difficulties; 3) leveraging easy samples to facilitate tough ones. Furthermore, we present a novel approach, named challenging tough sample networks (CTSN), to practice the three ideas and tame tough samples. Specifically, in our approach, a CNN with domain adaptation layers is first used to rapidly handle the easy samples and identify the tough ones. Then, a GAN with two classifiers is tailored to adapt the tough samples. The GAN leverages classification discrepancy and easy samples to tame the tough ones. Extensive experiments on both classic and large-scale benchmarks verify that both easy and tough samples do exist in real-world datasets and our approach is able to handle them.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303435",
    "keywords": [
      "Adaptation (eye)",
      "Artificial intelligence",
      "Biology",
      "Chemistry",
      "Chromatography",
      "Classifier (UML)",
      "Computer science",
      "Domain (mathematical analysis)",
      "Domain adaptation",
      "Focus (optics)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Neuroscience",
      "Optics",
      "Outlier",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Sample (material)",
      "Scale (ratio)"
    ],
    "authors": [
      {
        "surname": "Zuo",
        "given_name": "Lin"
      },
      {
        "surname": "Jing",
        "given_name": "Mengmeng"
      },
      {
        "surname": "Li",
        "given_name": "Jingjing"
      },
      {
        "surname": "Zhu",
        "given_name": "Lei"
      },
      {
        "surname": "Lu",
        "given_name": "Ke"
      },
      {
        "surname": "Yang",
        "given_name": "Yang"
      }
    ]
  },
  {
    "title": "A BFS-Tree of ranking references for unsupervised manifold learning",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107666",
    "abstract": "Contextual information, defined in terms of the proximity of feature vectors in a feature space, has been successfully used in the construction of search services. These search systems aim to exploit such information to effectively improve ranking results, by taking into account the manifold distribution of features usually encoded. In this paper, a novel unsupervised manifold learning is proposed through a similarity representation based on ranking references. A breadth-first tree is used to represent similarity information given by ranking references and is exploited to discovery underlying similarity relationships. As a result, a more effective similarity measure is computed, which leads to more relevant objects in the returned ranked lists of search sessions. Several experiments conducted on eight public datasets, commonly used for image retrieval benchmarking, demonstrated that the proposed method achieves very high effectiveness results, which are comparable or superior to the ones produced by state-of-the-art approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320304696",
    "keywords": [
      "Artificial intelligence",
      "Benchmarking",
      "Business",
      "Computer science",
      "Computer security",
      "Data mining",
      "Exploit",
      "Feature (linguistics)",
      "Feature vector",
      "Image (mathematics)",
      "Information retrieval",
      "Linguistics",
      "Machine learning",
      "Marketing",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Ranking (information retrieval)",
      "Similarity (geometry)",
      "Similarity learning",
      "Tree (set theory)"
    ],
    "authors": [
      {
        "surname": "Pedronette",
        "given_name": "Daniel Carlos Guimarães"
      },
      {
        "surname": "Valem",
        "given_name": "Lucas Pascotti"
      },
      {
        "surname": "Torres",
        "given_name": "Ricardo da S."
      }
    ]
  },
  {
    "title": "Black-box attack against handwritten signature verification with region-restricted adversarial perturbations",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107689",
    "abstract": "Handwritten signature verification is used to verify the identity of individuals through recognizing their signatures. Adversarial examples can induce misclassification, hence posing a severe threat to signature verification. At present, a variety of adversarial example attacks have been developed for image classification, but they are not that useful for attacking signature verification due to two main reasons. First, adversarial perturbations are likely to be imposed on the background of signature images, making them perceptible to human eyes. Second, perfect knowledge about signature verification systems is actually unavailable to attackers. Therefore, how to generate effective and stealthy signature adversarial examples is still an open issue. To shed insights on this challenging problem, we propose the first black-box adversarial example attack against handwritten signature verification in this paper. Our method has two key designs. First, its perturbations are intentionally restricted to the foreground (i.e., strokes) of signature images, which reduces the risk of being recognized by humans. Second, a gradient-free method is developed to achieve the desired perturbations through iteratively updating their positions and optimizing their intensity. Extensive experiments confirm the three advantages of our method. First, the adversarial perturbations generated by our method are almost invisible, while those generated by existing methods are more well-marked. Second, our method defeats the state-of-the-art signature verification method with a surprisingly high success rate of 92.1%. Last, our method breaks through the defense of background cleaning, although this defense can deactivate almost all the existing adversarial example attacks towards signature verification.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320304921",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Black box",
      "Computer science",
      "Computer security",
      "Geometry",
      "Image (mathematics)",
      "Key (lock)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Signature (topology)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Haoyang"
      },
      {
        "surname": "Li",
        "given_name": "Heng"
      },
      {
        "surname": "Zhang",
        "given_name": "Hansong"
      },
      {
        "surname": "Yuan",
        "given_name": "Wei"
      }
    ]
  },
  {
    "title": "Exploring global diverse attention via pairwise temporal relation for video summarization",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107677",
    "abstract": "Video summarization is an effective way to facilitate video searching and browsing. Most of existing systems employ encoder-decoder based recurrent neural networks, which fail to explicitly diversify the system-generated summary frames while requiring intensive computations. In this paper, we propose an efficient convolutional neural network architecture for video SUMmarization via Global Diverse Attention called SUM-GDA, which adapts attention mechanism in a global perspective to consider pairwise temporal relations of video frames. Particularly, the GDA module has two advantages: (1) it models the relations within paired frames as well as the relations among all pairs, thus capturing the global attention across all frames of one video; (2) it reflects the importance of each frame to the whole video, leading to diverse attention on these frames. Thus, SUM-GDA is beneficial for generating diverse frames to form satisfactory video summary. Extensive experiments on three data sets, i.e., SumMe, TVSum, and VTW, have demonstrated that SUM-GDA and its extension outperform other competing state-of-the-art methods with remarkable improvements. In addition, the proposed models can be run in parallel with significantly less computational costs, which helps the deployment in highly demanding applications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320304805",
    "keywords": [
      "Artificial intelligence",
      "Automatic summarization",
      "Computer science",
      "Data mining",
      "Pairwise comparison",
      "Relation (database)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Ping"
      },
      {
        "surname": "Ye",
        "given_name": "Qinghao"
      },
      {
        "surname": "Zhang",
        "given_name": "Luming"
      },
      {
        "surname": "Yuan",
        "given_name": "Li"
      },
      {
        "surname": "Xu",
        "given_name": "Xianghua"
      },
      {
        "surname": "Shao",
        "given_name": "Ling"
      }
    ]
  },
  {
    "title": "Context-aware network for RGB-D salient object detection",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107630",
    "abstract": "Convolutional neural networks (CNNs) have shown unprecedented success in object representation and detection. Nevertheless, CNNs lack the capability to model context dependencies among objects, which are crucial for salient object detection. As the long short-term memory (LSTM) is advantageous in propagating information, in this paper, we propose two variant LSTM units for the exploration of contextual dependencies. By incorporating these units, we present a context-aware network (CAN) to detect salient objects in RGB-D images. The proposed model consists of three components: feature extraction, context fusion of multiple modalities and context-dependent deconvolution. The first component is responsible for extracting hierarchical features in color and depth images using CNNs, respectively. The second component fuses high-level features by a variant LSTM to model multi-modal spatial dependencies in contexts. The third component, embedded with another variant LSTM, models local hierarchical context dependencies of the fused features at multi-scales. Experimental results on two public benchmark datasets show that the proposed CAN can achieve state-of-the-art performance for RGB-D stereoscopic salient object detection.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320304337",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Component (thermodynamics)",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Context model",
      "Convolutional neural network",
      "Feature (linguistics)",
      "Feature extraction",
      "Geodesy",
      "Geography",
      "Linguistics",
      "Object (grammar)",
      "Object detection",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "RGB color model",
      "Salient",
      "Spatial contextual awareness",
      "Thermodynamics"
    ],
    "authors": [
      {
        "surname": "Liang",
        "given_name": "Fangfang"
      },
      {
        "surname": "Duan",
        "given_name": "Lijuan"
      },
      {
        "surname": "Ma",
        "given_name": "Wei"
      },
      {
        "surname": "Qiao",
        "given_name": "Yuanhua"
      },
      {
        "surname": "Miao",
        "given_name": "Jun"
      },
      {
        "surname": "Ye",
        "given_name": "Qixiang"
      }
    ]
  },
  {
    "title": "Semisupervised charting for spectral multimodal manifold learning and alignment",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107645",
    "abstract": "For one given scene, multimodal data are acquired from multiple sensors. They share some similarities across the sensor types (redundant part of the information, also called coupling part) and they also provide modality-specific information (dissimilarities across the sensors, also called decoupling part). Additional critical knowledge about the scene can hence be extracted, which is not extractable from each modality alone. For the processing of multimodal data, we propose in this paper a model to simultaneously learn the underlying low-dimensional manifold in each modality, and locally align these manifolds across different modalities. For each pair of modalities we first build a common manifold that represents the corresponding (redundant) part of information, ignoring non-corresponding (modality specific) parts. We propose a semi-supervised learning model, using a limited amount of prior knowledge about the coupling and decoupling components of the different modalities. We propose a localized version of Laplacian eigenmaps technique specifically designed to handle multimodal manifold learning, in which the ideas of local patching of the manifolds, also known as manifold charting, is combined with the joint spectral analysis of the graph Laplacians of the different modalities. The limited given supervised information is then extending on the manifold of each modality. The idea of functional mapping is finally used to align the different manifolds across modalities. The evaluation of the proposed model using synthetic and real-world multimodal problems shows promising results, compared to several related techniques.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320304489",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Control engineering",
      "Decoupling (probability)",
      "Dimensionality reduction",
      "Engineering",
      "Graph",
      "Machine learning",
      "Manifold (fluid mechanics)",
      "Manifold alignment",
      "Mechanical engineering",
      "Modalities",
      "Modality (human–computer interaction)",
      "Nonlinear dimensionality reduction",
      "Pattern recognition (psychology)",
      "Social science",
      "Sociology",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Pournemat",
        "given_name": "Ali"
      },
      {
        "surname": "Adibi",
        "given_name": "Peyman"
      },
      {
        "surname": "Chanussot",
        "given_name": "Jocelyn"
      }
    ]
  },
  {
    "title": "Temporal filtering networks for online action detection",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107695",
    "abstract": "Online action detection aims to detect a current action from an untrimmed, streaming video, where only current and past frames are available. Recent methods for online action detection have focused on how to model discriminative representations from temporally partial information. However, they overlook the fact that the input video contains background as well as actions. To overcome this problem, in this paper, we propose a novel approach, named Temporal Filtering Network, to distinguish between relevant and irrelevant information from a partially observed, untrimmed video. Specifically, we present a filtering module to learn relevance scores indicating how relevant the information is to a current action. Our filtering module emphasizes the relevant information to a current action, while it filters out the information of background and unrelated actions. We conduct extensive experiments on THUMOS-14 and TVSeries datasets. On these datasets, the proposed method outperforms state-of-the-art methods by a large margin. We also show the effectiveness of the filtering module through comprehensive ablation studies.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320304982",
    "keywords": [
      "Action (physics)",
      "Artificial intelligence",
      "Computer science",
      "Current (fluid)",
      "Data mining",
      "Discriminative model",
      "Electrical engineering",
      "Engineering",
      "Law",
      "Machine learning",
      "Margin (machine learning)",
      "Pattern recognition (psychology)",
      "Physics",
      "Political science",
      "Quantum mechanics",
      "Relevance (law)"
    ],
    "authors": [
      {
        "surname": "Eun",
        "given_name": "Hyunjun"
      },
      {
        "surname": "Moon",
        "given_name": "Jinyoung"
      },
      {
        "surname": "Park",
        "given_name": "Jongyoul"
      },
      {
        "surname": "Jung",
        "given_name": "Chanho"
      },
      {
        "surname": "Kim",
        "given_name": "Changick"
      }
    ]
  },
  {
    "title": "Robust sparse coding for one-class classification based on correntropy and logarithmic penalty function",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107685",
    "abstract": "Similar to binary and multi-class classifiers, one-class classifiers have to face the difficulty of ’curse of dimensionality’ when they are applied to deal with high-dimensional samples. As an efficient dimensionality reduction method, sparse coding tries to learn a set of over-complete bases to represent the given samples. It can effectively overcome the ’curse of dimensionality’ problem. However, the traditional sparse coding only fit for tackling Gaussian noise. When the noise within the given set of samples obey non-Gaussian distribution, the conventional sparse coding cannot obtain accurate coefficient vectors. To make sparse coding more fit for dealing with non-Gaussian noise and enhance the sparseness of the obtained coefficient vectors, correntropy is utilized to substitute its reconstruction error term and logarithmic penalty function is introduced as its regularization term. Furthermore, the obtained sparse coefficient vectors are used as the input vectors for one-class support vector machine (OCSVM). Experimental results on twenty UCI benchmark data sets and one handwritten digit data set demonstrate that the proposed method achieves better anti-noise and generalization abilities in comparison with its related approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032030488X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Curse of dimensionality",
      "Logarithm",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Neural coding",
      "Pattern recognition (psychology)",
      "Penalty method",
      "Sparse approximation",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Xing",
        "given_name": "Hong-Jie"
      },
      {
        "surname": "Liu",
        "given_name": "Ya-Jie"
      },
      {
        "surname": "He",
        "given_name": "Zi-Chuan"
      }
    ]
  },
  {
    "title": "A deep Kalman filter network for hand kinematics estimation using sEMG",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.01.001",
    "abstract": "In human-machine interfaces (HMI), deep learning (DL) techniques such as convolutional neural networks (CNN), long-short term memory networks (LSTM) and the hybrid CNN-LSTM framework have been exploited for hand kinematics estimation using surface electromyography (sEMG). However, these DL techniques only capture the relationship between sEMG and hand kinematics, but ignores the prior knowledge of the system. By contrast, Kalman filter (KF) can apply Kalman gain to combine the internal transition model and the observation model effectively. To this end, we propose a novel architecture named deep Kalman filter network (DKFN), in which we utilize CNN to extract high-level features from sEMG and employ a LSTM-based Kalman filter process (LSTM-KF) to conduct sequential regression. In particular, LSTM-KF adopts the computational graph of KF but estimates parameters of the transition/observation model and the Kalman gain from data using LSTM modules. With this process, the advantages of KF and LSTM can be exploited jointly. Experimental results demonstrate that the proposed DKFN can outperform CNN and CNN-LSTM in the sequential regression for wrist/fingers kinematics estimation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521000118",
    "keywords": [
      "Artificial intelligence",
      "Classical mechanics",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Deep learning",
      "Kalman filter",
      "Kinematics",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Physics"
    ],
    "authors": [
      {
        "surname": "Bao",
        "given_name": "Tianzhe"
      },
      {
        "surname": "Zhao",
        "given_name": "Yihui"
      },
      {
        "surname": "Zaidi",
        "given_name": "Syed Ali Raza"
      },
      {
        "surname": "Xie",
        "given_name": "Shengquan"
      },
      {
        "surname": "Yang",
        "given_name": "Pengfei"
      },
      {
        "surname": "Zhang",
        "given_name": "Zhiqiang"
      }
    ]
  },
  {
    "title": "Bounded manifold completion",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107661",
    "abstract": "Nonlinear dimensionality reduction is an active area of research. In this paper, we present a thematically different approach to detect a low-dimensional manifold that lies within a set of bounds derived from a given point cloud. A matrix representing distances on a low-dimensional manifold is low-rank, and our method is based on current low-rank Matrix Completion (MC) techniques for recovering a partially observed matrix from fully observed entries. MC methods are currently used to solve challenging real-world problems such as image inpainting and recommender systems. Our MC scheme utilizes efficient optimization techniques that employ a nuclear norm convex relaxation as a surrogate for non-convex and discontinuous rank minimization. The method theoretically guarantees on detection of low-dimensional embeddings and is robust to non-uniformity in the sampling of the manifold. We validate the performance of this approach using both a theoretical analysis as well as synthetic and real-world benchmark datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320304647",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Curse of dimensionality",
      "Diffusion map",
      "Dimensionality reduction",
      "Engineering",
      "Gaussian",
      "Hankel matrix",
      "Image (mathematics)",
      "Inpainting",
      "Intrinsic dimension",
      "Leverage (statistics)",
      "Low-rank approximation",
      "Manifold (fluid mechanics)",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Matrix completion",
      "Mechanical engineering",
      "Nonlinear dimensionality reduction",
      "Physics",
      "Quantum mechanics"
    ],
    "authors": [
      {
        "surname": "Gajamannage",
        "given_name": "Kelum"
      },
      {
        "surname": "Paffenroth",
        "given_name": "Randy"
      }
    ]
  },
  {
    "title": "Learning deep features for task-independent EEG-based biometric verification",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.01.004",
    "abstract": "Considerable interest has been recently devoted to the exploitation of brain activity as biometric identifier in automatic recognition systems, with a major focus on data acquired through electroencephalography (EEG). Several researches have in fact confirmed the presence of discriminative characteristics within brain signals recorded while performing specific mental tasks. Yet, to make EEG-based recognition appealing for practical applications, it would be highly advisable to investigate the existence and permanence of such distinctive traits while performing different mental tasks. In this regard, the present study evaluates the feasibility of performing task-independent EEG-based biometric recognition. A deep learning approach using siamese convolutional neural networks is employed to extract, from the considered EEG recordings, subject-specific template representations. An extensive set of experimental tests, performed on a multi-session database comprising EEG data acquired from 45 subjects while performing six different tasks, is employed to evaluate whether it is actually possible to verify the identity of a subject using brain signals, regardless the performed mental task.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521000143",
    "keywords": [
      "Artificial intelligence",
      "Biometrics",
      "Computer science",
      "Convolutional neural network",
      "Discriminative model",
      "Economics",
      "Electroencephalography",
      "Machine learning",
      "Management",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Programming language",
      "Psychology",
      "Session (web analytics)",
      "Set (abstract data type)",
      "Speech recognition",
      "Task (project management)",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Maiorana",
        "given_name": "Emanuele"
      }
    ]
  },
  {
    "title": "The trace kernel bandwidth criterion for support vector data description",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107662",
    "abstract": "Support vector data description (SVDD) is a popular anomaly detection technique. The computation of the SVDD classifier requires a kernel function, for which the Gaussian kernel is a common choice. The Gaussian kernel has a bandwidth parameter, and it is important to set the value of this parameter correctly to ensure good results. A small bandwidth leads to overfitting, and the resulting SVDD classifier overestimates the number of anomalies, whereas a large bandwidth leads to underfitting and an inability to detect many anomalies. In this paper, we present a new, unsupervised method for selecting the Gaussian kernel bandwidth. Our method exploits a low-rank representation of the kernel matrix to suggest a kernel bandwidth value. Our new technique is competitive with the current state of the art for low-dimensional data and performs extremely well for many classes of high-dimensional data. This method is also applicable to one-class support vector machines (OCSVM).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320304659",
    "keywords": [
      "Algorithm",
      "Anomaly detection",
      "Artificial intelligence",
      "Artificial neural network",
      "Bandwidth (computing)",
      "Combinatorics",
      "Computer network",
      "Computer science",
      "Gaussian",
      "Gaussian function",
      "Kernel (algebra)",
      "Kernel method",
      "Least squares support vector machine",
      "Mathematics",
      "Overfitting",
      "Pattern recognition (psychology)",
      "Physics",
      "Polynomial kernel",
      "Quantum mechanics",
      "Radial basis function kernel",
      "Support vector machine",
      "Variable kernel density estimation"
    ],
    "authors": [
      {
        "surname": "Chaudhuri",
        "given_name": "Arin"
      },
      {
        "surname": "Sadek",
        "given_name": "Carol"
      },
      {
        "surname": "Kakde",
        "given_name": "Deovrat"
      },
      {
        "surname": "Wang",
        "given_name": "Haoyu"
      },
      {
        "surname": "Hu",
        "given_name": "Wenhao"
      },
      {
        "surname": "Jiang",
        "given_name": "Hansi"
      },
      {
        "surname": "Kong",
        "given_name": "Seunghyun"
      },
      {
        "surname": "Liao",
        "given_name": "Yuwei"
      },
      {
        "surname": "Peredriy",
        "given_name": "Sergiy"
      }
    ]
  },
  {
    "title": "SSS-PR: A short survey of surveys in person re-identification",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2020.12.017",
    "abstract": "Person re-identification (re-id) addresses the problem of whether “a query image corresponds to an identity in the database” and is believed to play a fundamental role in security enforcement in the near future, particularly in crowded urban environments. Due to many possibilities in selecting appropriate model architectures, datasets, and settings, the performance reported by the state-of-the-art re-id methods oscillates significantly among the published surveys. Therefore, it is difficult to understand the mainstream trends and emerging research difficulties in person re-id. This paper proposes a multi-dimensional taxonomy to categorize the most relevant researches according to different perspectives and tries to unify the categorization of re-id methods and fill the gap between the recently published surveys. Furthermore, we discuss the open challenges with a focus on privacy concerns and the issues caused by the exponential increase in the number of re-id publications over the recent years. Finally, we discuss several challenging directions for future studies.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521000015",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Categorization",
      "Computer science",
      "Data science",
      "Focus (optics)",
      "Identification (biology)",
      "Information retrieval",
      "Law",
      "Mainstream",
      "Optics",
      "Physics",
      "Political science",
      "Taxonomy (biology)"
    ],
    "authors": [
      {
        "surname": "Yaghoubi",
        "given_name": "Ehsan"
      },
      {
        "surname": "Kumar",
        "given_name": "Aruna"
      },
      {
        "surname": "Proença",
        "given_name": "Hugo"
      }
    ]
  },
  {
    "title": "Robust line segment matching via reweighted random walks on the homography graph",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107693",
    "abstract": "This paper presents a novel method for matching line segments between stereo images. Given the fundamental matrix, the local homography can be over determined with pairwise line segment candidates. We exploit this constraint to initialize the candidate and construct the novel homography graph. Because the constraint between the node is based on the epipolar geometry, the homography graph is invariant to the local projective transformation. We employ the reweighted random walk on the graph to rank the candidate, then, we propose the constrained-greedy algorithm to obtain the reliable match. To the best of our knowledge, this is the first study to embed the epipolar geometry into the graph matching theory for the line segment matching. When evaluated on the 32 image patches, our method outperformed the state of the art methods, especially in the scenes of the wide baseline, steep viewpoint changes and dense line segments. The proposed algorithm is available at https://github.com/weidong-whu/line-match-RRW.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320304969",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Epipolar geometry",
      "Fundamental matrix (linear differential equation)",
      "Graph",
      "Homography",
      "Image (mathematics)",
      "Line segment",
      "Matching (statistics)",
      "Mathematical analysis",
      "Mathematics",
      "Pairwise comparison",
      "Pattern recognition (psychology)",
      "Projective space",
      "Projective test",
      "Statistics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Wei",
        "given_name": "Dong"
      },
      {
        "surname": "Zhang",
        "given_name": "Yongjun"
      },
      {
        "surname": "Li",
        "given_name": "Chang"
      }
    ]
  },
  {
    "title": "EKENet: Efficient knowledge enhanced network for real-time scene parsing",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107671",
    "abstract": "Scene parsing is essential for many high-level AI applications, such as intelligent vehicles and traffic surveillance. In this work, we propose a highly efficient and powerful deep convolutional neural network, namely Efficient Knowledge Enhanced Network (EKENet), for parsing scenes in real-time. Unlike most existing approaches that compromise efficiency for the sake of high accuracy, EKENet achieves an ideal trade-off between the two. Our EKENet is built upon a novel building block, namely Efficient Dual Abstraction (EDA) block, which employs an efficiently parallel convolution structure for extracting spatial features and modeling cross-channel correlations in a dual fashion. Additionally, a novel light-weight Encoding-Enhancing (EE) module is designed to enhance our EKENet, which can efficiently encode high-level knowledge extracted from top layers to guide the learning of low-level features from bottom layers. Extensive experiments on challenging benchmarks, Cityscapes and CamVid datasets, demonstrate that EKENet achieves the new state-of-the-art performance in terms of speed and accuracy tradeoff.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032030474X",
    "keywords": [
      "Abstraction",
      "Art",
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Block (permutation group theory)",
      "Chemistry",
      "Computer engineering",
      "Computer science",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Dual (grammatical number)",
      "ENCODE",
      "Encoding (memory)",
      "Epistemology",
      "Gene",
      "Geometry",
      "Literature",
      "Machine learning",
      "Mathematics",
      "Parsing",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Luo",
        "given_name": "Ao"
      },
      {
        "surname": "Yang",
        "given_name": "Fan"
      },
      {
        "surname": "Li",
        "given_name": "Xin"
      },
      {
        "surname": "Huang",
        "given_name": "Rui"
      },
      {
        "surname": "Cheng",
        "given_name": "Hong"
      }
    ]
  },
  {
    "title": "Biometric key generation based on generated intervals and two-layer error correcting technique",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107733",
    "abstract": "As for a biometric key, key management and biometric data security are both important. Existing bio-key generation methods are usually based on the biometric templates or features directly, it may expose user’s biometric data and will further make the biometric data permanently unusable for his secure identification recognitions. In this paper, a fingerprint bio-key generation approach using the feature distance is proposed. We utilize the relative distances among user’s fingerprint minutiae to generate a unique bio-key. Such bio-key is determinable and recoverable via the generation interval scheme. In addition, we use a two-layer error correcting technique to guarantee a better reliability during the data transmission. The experimental results positively show that our approach can ensure higher security of the bio-key and guarantee a good key regeneration rate. Besides, the storage of the original bio-key or any fingerprint template is unnecessary.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320305367",
    "keywords": [
      "Artificial intelligence",
      "Biometrics",
      "Computer science",
      "Computer security",
      "Cryptography",
      "Data mining",
      "Fingerprint (computing)",
      "Fingerprint recognition",
      "Key (lock)",
      "Key generation",
      "Minutiae",
      "Word error rate"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Peiyi"
      },
      {
        "surname": "You",
        "given_name": "Lin"
      },
      {
        "surname": "Hu",
        "given_name": "Gengran"
      },
      {
        "surname": "Hu",
        "given_name": "Liqin"
      },
      {
        "surname": "Jian",
        "given_name": "Zhihua"
      },
      {
        "surname": "Xing",
        "given_name": "Chaoping"
      }
    ]
  },
  {
    "title": "Online incremental hierarchical classification resonance network",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107672",
    "abstract": "Hierarchical classification is imperative in that almost all objects are described in hierarchical semantics. If a classification method enables incremental class learning to learn new objects online, it will be practically used for real-time applications. In this sense, we propose online incremental hierarchical classification resonance network (OIHCRN) that enables online incremental class learning in hierarchical classification. OIHCRN has a structure that grows horizontally and vertically online according to object classes, so that a newly added object can be classified. By the proposed process of scale-preserving projection and prior label appending, OIHCRN reflects the class dependency between class levels and simultaneously normalizes the input vector online. Additionally, to reduce the model complexity and improve performance, two auxiliary strategies, named OIHCRN with class END and OIHCRN with differentiated class labels, are introduced. To demonstrate the effectiveness of OIHCRNs, experiments are carried out for benchmark datasets and then for a multimedia recommendation system.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320304751",
    "keywords": [
      "Adaptive resonance theory",
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Class (philosophy)",
      "Computer science",
      "Data mining",
      "Dependency (UML)",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Object (grammar)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Physics",
      "Process (computing)",
      "Programming language",
      "Projection (relational algebra)",
      "Quantum mechanics",
      "Scale (ratio)",
      "Semantics (computer science)"
    ],
    "authors": [
      {
        "surname": "Park",
        "given_name": "Ju-Youn"
      },
      {
        "surname": "Kim",
        "given_name": "Jong-Hwan"
      }
    ]
  },
  {
    "title": "SI(FS) 2 : Fast simultaneous instance and feature selection for datasets with many features",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107723",
    "abstract": "Data reduction is becoming increasingly relevant due to the enormous amounts of data that are constantly being produced in many fields of research. Instance selection is one of the most widely used methods for this task. At the same time, most recent pattern recognition problems involve highly complex datasets with a large number of possible explanatory variables. For many reasons, this abundance of variables significantly hinders classification and recognition tasks. There are efficiency issues, too, because the speed of many classification algorithms is greatly improved when the complexity of the data is reduced. Thus, feature selection is also a widely used method for data reduction and for gaining an understanding of feature information. Although most methods address instance and feature selection separately, the two problems are interwoven, and benefits are expected from performing these two tasks jointly. However, few algorithms have been proposed for simultaneously addressing the tasks of instance and feature selection. Furthermore, most of those methods are based on complex heuristics that are very difficult to scale up even to moderately large datasets. This paper proposes a new algorithm for dealing with many instances and many features simultaneously by performing joint instance and feature selection using a simple heuristic search and several scaling-up mechanisms that can be successfully applied to datasets with millions of features and instances. In the proposed method, a forward selection search is performed in the feature space jointly with the application of standard instance selection in a constructive subspace built stepwise. Several simplifications are adopted in the search to obtain a scalable method. An extensive comparison using 95 large datasets shows the usefulness of our method and its ability to deal with millions of instances and features simultaneously. The method is able to obtain better classification performance results than state-of-the-art approaches while achieving considerable data reduction.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320305264",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Dimensionality reduction",
      "Feature (linguistics)",
      "Feature selection",
      "Heuristic",
      "Heuristics",
      "Linguistics",
      "Machine learning",
      "Operating system",
      "Philosophy",
      "Selection (genetic algorithm)"
    ],
    "authors": [
      {
        "surname": "García-Pedrajas",
        "given_name": "Nicolás"
      },
      {
        "surname": "del Castillo",
        "given_name": "Juan A. Romero"
      },
      {
        "surname": "Cerruela-García",
        "given_name": "Gonzalo"
      }
    ]
  },
  {
    "title": "Guided image filtering in shape-from-focus: A comparative analysis",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107670",
    "abstract": "Mostly, shape from focus (SFF) methods do not consider any prior to extend the accuracy of the depth map. Ultimately, even the improved depth map might lack the accurate structure of the object. While reviewing the guided filters, it has been observed that SFF has not been considered as an application. In this study, we not only suggest to apply guided filtering for depth enhancement but also provide a comparative analysis of recently proposed guided filters for SFF framework in a systematic way. In addition, a set of potential guidance maps has been suggested and the performance of these guidance maps has been evaluated. The improved performance of guided filters has been ranked against the depth maps of synthetic and real image sequences where the corresponding scenes have diverse range of geometrical complexities. It has been observed that guided image filtering is effective in improving the initial depth maps in SFF.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320304738",
    "keywords": [
      "Aerospace engineering",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Engineering",
      "Focus (optics)",
      "Image (mathematics)",
      "Mathematics",
      "Object (grammar)",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Programming language",
      "Range (aeronautics)",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Ali",
        "given_name": "Usman"
      },
      {
        "surname": "Lee",
        "given_name": "Ik Hyun"
      },
      {
        "surname": "Mahmood",
        "given_name": "Muhammad Tariq"
      }
    ]
  },
  {
    "title": "Image captioning with transformer and knowledge graph",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2020.12.020",
    "abstract": "The Transformer model has achieved very good results in machine translation tasks. In this paper, we adopt the Transformer model for the image captioning task. To promote the performance of image captioning, we improve the Transformer model from two aspects. First, we augment the maximum likelihood estimation (MLE) with an extra Kullback-Leibler (KL) divergence term to distinguish the difference between incorrect predictions. Second, we introduce a method to help the Transformer model generate captions by leveraging the knowledge graph. Experiments on benchmark datasets demonstrate the effectiveness of our method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521000040",
    "keywords": [
      "Artificial intelligence",
      "Closed captioning",
      "Computer science",
      "Electrical engineering",
      "Engineering",
      "Graph",
      "Image (mathematics)",
      "Language model",
      "Machine learning",
      "Machine translation",
      "Natural language processing",
      "Theoretical computer science",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Yu"
      },
      {
        "surname": "Shi",
        "given_name": "Xinyu"
      },
      {
        "surname": "Mi",
        "given_name": "Siya"
      },
      {
        "surname": "Yang",
        "given_name": "Xu"
      }
    ]
  },
  {
    "title": "A new localization method for epileptic seizure onset zones based on time-frequency and clustering analysis",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107687",
    "abstract": "High-frequency oscillations (HFOs) are spontaneous electroencephalogram patterns that have been regarded as potential biomarkers of epileptic seizure onset zones (SOZs). Accurately detected HFOs are used to localize SOZs, which is crucial for the presurgical assessment. Since the visual marking of HFOs is time-consuming, a method is desirable to automatically detect HFOs for localizing SOZs in clinical practice. However, the existing methods cannot obtain satisfactory performance, which are not suitable for clinical application. In order to solve this problem, we present a new localization method for epileptic SOZs in this study. Firstly, a threshold method is used to detect events of interest (EoIs). Secondly, a time-frequency analysis method is adopted to acquire channels of interest (CoIs) by calculating the average power of EoIs on each channel. Then, the k-medoids clustering method is employed to detect HFOs of CoIs. Finally, the concentrations of detected HFOs are used to localize SOZs. The superiority of our localization method is demonstrated by comparing its sensitivity and specificity with some existing methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320304908",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Computer vision",
      "Filter (signal processing)",
      "Pattern recognition (psychology)",
      "Time–frequency analysis"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Min"
      },
      {
        "surname": "Wan",
        "given_name": "Ting"
      },
      {
        "surname": "Wan",
        "given_name": "Xiongbo"
      },
      {
        "surname": "Fang",
        "given_name": "Zelin"
      },
      {
        "surname": "Du",
        "given_name": "Yuxiao"
      }
    ]
  },
  {
    "title": "Spectrum-aware discriminative deep feature learning for multi-spectral face recognition",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107632",
    "abstract": "One primary challenge of face recognition is that the performance is seriously affected by varying illumination. Multi-spectral imaging can capture face images in the visible spectrum and beyond, which is deemed to be an effective technology in response to this challenge. For current multi-spectral imaging-based face recognition methods, how to fully explore the discriminant and correlation features from both the intra-spectrum and inter-spectrum aspects with only a limited number of multi-spectral samples for model training has not been well studied. To address this problem, in this paper, we propose a novel face recognition approach named Spectrum-aware Discriminative Deep Learning (SDDL). To take full advantage of the multi-spectral training samples, we build a discriminative multi-spectral network (DMN) and take face sample pairs as the input of the network. By jointly considering the spectrum and the class label information, SDDL trains the network for projecting samples pairs into a discriminant feature subspace, on which the intrinsic relationship including the intra- and inter-spectrum discrimination and the inter-spectrum correlation among face samples is well discovered. The proposed approach is evaluated on three widely used datasets HK PolyU, CMU, and UWA. Extensive experimental results demonstrate the superiority of SDDL over state-of-the-art competing methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320304350",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Discriminant",
      "Discriminative model",
      "Face (sociological concept)",
      "Facial recognition system",
      "Feature (linguistics)",
      "Linear discriminant analysis",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Social science",
      "Sociology",
      "Speech recognition",
      "Subspace topology"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Fei"
      },
      {
        "surname": "Jing",
        "given_name": "Xiao-Yuan"
      },
      {
        "surname": "Feng",
        "given_name": "Yujian"
      },
      {
        "surname": "Ji",
        "given_name": "Yi-mu"
      },
      {
        "surname": "Wang",
        "given_name": "Ruchuan"
      }
    ]
  },
  {
    "title": "Ranking list preservation for feature matching",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107665",
    "abstract": "Feature matching plays a very important role in many computer vision and pattern recognition tasks. The spatial neighborhood relationship (representing the topological structures of some key feature points of an image scene) is generally well preserved between two feature points of an image pair. Several mismatch-removing methods that maintain the local neighborhood structures of potential true matches have been proposed. Defining local neighborhood structures is a crucial issue in the feature matching problem. In this paper, we propose a robust and efficient topological structure measurement called top K rank preservation (TopKRP) for mismatch removal from given putative point set. We transform feature points from the feature space to the ranking list space. Thus, the topological structure similarity of two feature points can be simply calculated by comparing their ranking lists, which are measured by the top K ranking similarity based on the spatial Euclidean distance as well as the angle correlation. TopKRP is validated on 10 public image pairs with typical scenes and 2 artificially established datasets, namely, MI52 and RS153. Experimental results demonstrate that the proposed approach outperforms several state-of-the-art feature matching methods, especially when the number of mismatches is large.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320304684",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Euclidean distance",
      "Feature (linguistics)",
      "Feature extraction",
      "Feature matching",
      "Feature vector",
      "Geometry",
      "Image (mathematics)",
      "Linguistics",
      "Matching (statistics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Point (geometry)",
      "Rank (graph theory)",
      "Ranking (information retrieval)",
      "Similarity (geometry)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Jiang",
        "given_name": "Junjun"
      },
      {
        "surname": "Ma",
        "given_name": "Qing"
      },
      {
        "surname": "Jiang",
        "given_name": "Xingyu"
      },
      {
        "surname": "Ma",
        "given_name": "Jiayi"
      }
    ]
  },
  {
    "title": "Mixed-precision quantized neural networks with progressively decreasing bitwidth",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107647",
    "abstract": "Efficient model inference is an important and practical issue in the deployment of deep neural networks on resource constraint platforms. Network quantization addresses this problem effectively by leveraging low-bit representation and arithmetic that could be conducted on dedicated embedded systems. In the previous works, the parameter bitwidth is set homogeneously and there is a trade-off between superior performance and aggressive compression. Actually, the stacked network layers, which are generally regarded as hierarchical feature extractors, contribute diversely to the overall performance. For a well-trained neural network, the feature distributions of different categories are organized gradually as the network propagates forward. Hence the capability requirement on the subsequent feature extractors is reduced. It indicates that the neurons in posterior layers could be assigned with lower bitwidth for quantized neural networks. Based on this observation, a simple yet effective mixed-precision quantized neural network with progressively decreasing bitwidth is proposed to improve the trade-off between accuracy and compression. Extensive experiments on typical network architectures and benchmark datasets demonstrate that the proposed method could achieve better or comparable results while reducing the memory space for quantized parameters by more than 25% in comparison with the homogeneous counterparts. In addition, the results also demonstrate that the higher-precision bottom layers could boost the 1-bit network performance appreciably due to a better preservation of the original image information while the lower-precision posterior layers contribute to the regularization of k − bit networks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320304507",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Computer science",
      "Feature (linguistics)",
      "Geodesy",
      "Geography",
      "Inference",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Quantization (signal processing)",
      "Regularization (linguistics)",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Chu",
        "given_name": "Tianshu"
      },
      {
        "surname": "Luo",
        "given_name": "Qin"
      },
      {
        "surname": "Yang",
        "given_name": "Jie"
      },
      {
        "surname": "Huang",
        "given_name": "Xiaolin"
      }
    ]
  },
  {
    "title": "Image denoising using complex-valued deep CNN",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107639",
    "abstract": "While complex-valued transforms have been widely used in image processing and have their deep connections to biological vision systems, complex-valued convolutional neural networks (CNNs) have not seen their applications in image recovery. This paper aims at investigating the potentials of complex-valued CNNs for image denoising. A CNN is developed for image denoising with its key mathematical operations defined in the complex number field to exploit the merits of complex-valued operations, including the compactness of convolution given by the tensor product of 1D complex-valued filters, the nonlinear activation on phase, and the noise robustness of residual blocks. The experimental results show that, the proposed complex-valued denoising CNN performs competitively against existing state-of-the-art real-valued denoising CNNs, with better robustness to possible inconsistencies of noise models between training samples and test images. The results also suggest that complex-valued CNNs provide another promising deep-learning-based approach to image denoising and other image recovery tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320304428",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Deep learning",
      "Gene",
      "Image (mathematics)",
      "Image denoising",
      "Image processing",
      "Noise reduction",
      "Non-local means",
      "Pattern recognition (psychology)",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Quan",
        "given_name": "Yuhui"
      },
      {
        "surname": "Chen",
        "given_name": "Yixin"
      },
      {
        "surname": "Shao",
        "given_name": "Yizhen"
      },
      {
        "surname": "Teng",
        "given_name": "Huan"
      },
      {
        "surname": "Xu",
        "given_name": "Yong"
      },
      {
        "surname": "Ji",
        "given_name": "Hui"
      }
    ]
  },
  {
    "title": "Alignment-free cancelable fingerprint templates with dual protection",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107735",
    "abstract": "Cancelable biometrics is an important biometric template protection technique. However, many existing cancelable fingerprint templates suffer post-transformation performance deterioration and the attacks via record multiplicity (ARM). In this paper, we design alignment-free cancelable fingerprint templates with dual protection, which is composed of the window-shift-XOR model and the partial discrete wavelet transform. The former defuses the ARM threat and is combined with the latter to provide dual protection and enhance matching performance. The designed cancelable templates meet the requirements of non-invertibility, diversity and revocability and demonstrate superior recognition accuracy, when evaluated over public databases; for example, the Equal Error Rate of the proposed method in the lost-key scenario under the 1vs1 protocol is 0% for both FVC2002 DB1 and DB2, 1.63% for FVC2002 DB3, 7.35% for FVC2004 DB1 and 4.69% for FVC2004DB2.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320305380",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Biochemistry",
      "Biometrics",
      "Cartridge",
      "Chemistry",
      "Computer science",
      "Dual (grammatical number)",
      "Engineering",
      "Fingerprint (computing)",
      "Gene",
      "Literature",
      "Mechanical engineering",
      "Pattern recognition (psychology)",
      "Programming language",
      "Template",
      "Transformation (genetics)"
    ],
    "authors": [
      {
        "surname": "Shahzad",
        "given_name": "Muhammad"
      },
      {
        "surname": "Wang",
        "given_name": "Song"
      },
      {
        "surname": "Deng",
        "given_name": "Guang"
      },
      {
        "surname": "Yang",
        "given_name": "Wencheng"
      }
    ]
  },
  {
    "title": "Deep Rényi entropy graph kernel",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107668",
    "abstract": "Graph kernels are applied heavily for the classification of structured data. In this paper, we propose a deep Rényi entropy graph kernel for this purpose. We gauge the deep information through a family of h-layer expansion subgraphs rooted at a vertex, and define a h-layer depth-based second-order Rényi entropy representation for each vertex. The second-order Rényi entropy representation is used together with Euclidean distance to build a deep second-order Rényi entropy graph kernel (SREGK). For graphs with n vertices, the time complexity for our kernel is O(n 3). This low-order polynomial complexity enables our subgraph kernels to easily scale up to graphs of reasonably large sizes and thus overcome the size limits arising in state-of-the-art graph kernels. Experimental results on fourteen real world graph datasets are shown to demonstrate the overall superior performance of our approach over a number of state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320304714",
    "keywords": [
      "Combinatorics",
      "Discrete mathematics",
      "Euclidean geometry",
      "Geometry",
      "Graph",
      "Mathematics",
      "Vertex (graph theory)"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Lixiang"
      },
      {
        "surname": "Bai",
        "given_name": "Lu"
      },
      {
        "surname": "Jiang",
        "given_name": "Xiaoyi"
      },
      {
        "surname": "Tan",
        "given_name": "Ming"
      },
      {
        "surname": "Zhang",
        "given_name": "Daoqiang"
      },
      {
        "surname": "Luo",
        "given_name": "Bin"
      }
    ]
  },
  {
    "title": "Fast high-precision ellipse detection method",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107741",
    "abstract": "Obtaining an optimal tradeoff between accuracy and efficiency in ellipse detection is a significant challenge. In this paper, we propose a fast, high-precision ellipse detection method that utilizes arc selection and grouping strategies to significantly reduce the computation amount. A fast corner detection algorithm is also proposed. In the proposed method, to generate ellipse candidates comprehensively, both grouped and ungrouped-salient arcs are fitted. Further, the salient ellipse candidates are selected as final detections that are subject to the selection strategy, which realizes both validation and de-redundancy (clustering) functions. A complexity analysis of the method revealed that the detection time is linearly related to the number of edge points. The results of extensive experiments conducted on three public datasets demonstrate that the proposed method is approximately 75% faster than state-of-the-art methods with comparable or higher precision, and its detection time is less than 30 ms in most cases.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320305446",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Cluster analysis",
      "Computation",
      "Computer science",
      "Ellipse",
      "Geometry",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Redundancy (engineering)",
      "Salient",
      "Selection (genetic algorithm)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Zepeng"
      },
      {
        "surname": "Chen",
        "given_name": "Derong"
      },
      {
        "surname": "Gong",
        "given_name": "Jiulu"
      },
      {
        "surname": "Wang",
        "given_name": "Changyuan"
      }
    ]
  },
  {
    "title": "Supervised learning for parameterized Koopmans–Beckmann’s graph matching",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2020.12.012",
    "abstract": "In this paper, we discuss a novel graph matching problem, namely the parameterized Koopmans–Beckmann’s graph matching (KBGMw). KBGMw is defined by a weighted linear combination of a series of Koopmans–Beckmann’s graph matching. First, we show that KBGMw can be taken as a special case of the parameterized Lawler’s graph matching, subject to certain conditions. Second, based on structured SVM, we propose a supervised learning method for automatically estimating the parameters of KBGMw. Experimental results on both synthetic and real image matching data sets show that the proposed method achieves relatively better performances, even superior to some deep learning methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520304438",
    "keywords": [
      "3-dimensional matching",
      "Algorithm",
      "Artificial intelligence",
      "Bipartite graph",
      "Computer science",
      "Graph",
      "Matching (statistics)",
      "Mathematics",
      "Parameterized complexity",
      "Pattern recognition (psychology)",
      "Statistics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Zeng",
        "given_name": "Shaofeng"
      },
      {
        "surname": "Liu",
        "given_name": "Zhiyong"
      },
      {
        "surname": "Yang",
        "given_name": "Xu"
      }
    ]
  },
  {
    "title": "Supervised quality evaluation of binary partition trees for object segmentation",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107667",
    "abstract": "The binary partition tree (BPT) allows for the hierarchical representation of images in a multiscale way, by providing a tree of nodes corresponding to image regions. In particular, cuts of a BPT can be interpreted as segmentations of the associated image. Building the BPT of an image then constitutes a relevant preliminary step for optimization-based segmentation methods. A wide literature has been devoted to the construction of BPTs, and their involvement in such segmentation tasks. Comparatively, there exist few works dedicated to evaluate the quality of BPTs, i.e. their ability to allow further segmentation methods to compute good results. We propose such a framework for evaluating the quality of a BPT with respect to the object segmentation problem, i.e. the segmentation of one or several objects from an image. This framework is supervised, since the notion of segmentation quality is not only depending on the application but also on the user’s objectives, expressed via the chosen ground-truth and quality metric. We develop two sides within this framework. First, we propose an intrinsic quality analysis, that relies on the structural coherence of the BPT with respect to ground-truth. More precisely, we evaluate to what extent the BPT structure is well-matching such examples, in a set / combinatorial fashion. Second, we propose an extrinsic analysis, by allowing the user to assess the quality of a BPT based on chosen metrics that correspond to the desired properties of the subsequent segmentation. In particular, we evaluate to what extent a BPT can provide good results with respect to such metrics whereas handling the trade-off with the cardinality of the cuts.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320304702",
    "keywords": [
      "Arithmetic",
      "Artificial intelligence",
      "Binary number",
      "Combinatorics",
      "Computer science",
      "Economics",
      "Ground truth",
      "Image segmentation",
      "Law",
      "Matching (statistics)",
      "Mathematical analysis",
      "Mathematics",
      "Metric (unit)",
      "Minimum spanning tree-based segmentation",
      "Operations management",
      "Partition (number theory)",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Scale-space segmentation",
      "Segmentation",
      "Segmentation-based object categorization",
      "Statistics",
      "Tree (set theory)"
    ],
    "authors": [
      {
        "surname": "Randrianasoa",
        "given_name": "Jimmy Francky"
      },
      {
        "surname": "Cettour-Janet",
        "given_name": "Pierre"
      },
      {
        "surname": "Kurtz",
        "given_name": "Camille"
      },
      {
        "surname": "Desjardin",
        "given_name": "Éric"
      },
      {
        "surname": "Gançarski",
        "given_name": "Pierre"
      },
      {
        "surname": "Bednarek",
        "given_name": "Nathalie"
      },
      {
        "surname": "Rousseau",
        "given_name": "François"
      },
      {
        "surname": "Passat",
        "given_name": "Nicolas"
      }
    ]
  },
  {
    "title": "Adaptive feature fusion for visual object tracking",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107679",
    "abstract": "Recent advanced trackers, consisting of discriminative classification component and dedicated bounding box estimation, have achieved improved performance in the visual tracking community. The most essential factor for the development is the utilization of different Convolutional Neural Networks (CNNs), which significantly improves the model capacity via offline trained deep feature representations. Though powerful deep structures emphasize more semantic appearance through high dimensional latent variables, how to achieve effective feature adaptation in the online tracking stage has not been sufficiently considered yet. To this end, we argue the necessity of exploring hierarchical and complementary appearance descriptors from different convolutional layers to achieve online tracking adaptation. Therefore, in this paper, we propose an adaptive feature fusion mechanism, which can balance the detection granularities from shallow to deep convolutional layers. To be specific, the correlation between template and instance is employed to generate adaptive weights to achieve advanced saliency and discrimination. In addition, considering temporal appearance variation, the projection matrix for the multi-channel inputs is jointly updated with the correlation classifier to further enhance the robustness. The experimental results on four recent benchmarks, i.e., OTB-2015, VOT2018, LaSOT and TrackingNet, demonstrate the effectiveness and robustness of the proposed method, with superior performance compared to the state-of-the-art approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320304829",
    "keywords": [
      "Active appearance model",
      "Artificial intelligence",
      "Biochemistry",
      "BitTorrent tracker",
      "Chemistry",
      "Classifier (UML)",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Discriminative model",
      "Eye tracking",
      "Gene",
      "Image (mathematics)",
      "Minimum bounding box",
      "Pattern recognition (psychology)",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Shaochuan"
      },
      {
        "surname": "Xu",
        "given_name": "Tianyang"
      },
      {
        "surname": "Wu",
        "given_name": "Xiao-Jun"
      },
      {
        "surname": "Zhu",
        "given_name": "Xue-Feng"
      }
    ]
  },
  {
    "title": "Adversarial co-distillation learning for image recognition",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107659",
    "abstract": "Knowledge distillation is an effective way to transfer the knowledge from a pre-trained teacher model to a student model. Co-distillation, as an online variant of distillation, further accelerates the training process and paves a new way to explore the “dark knowledge” by training n models in parallel. In this paper, we explore the “divergent examples”, which can make the classifiers have different predictions and thus induce the “dark knowledge”, and we propose a novel approach named Adversarial Co-distillation Networks (ACNs) to enhance the “dark knowledge” by generating extra divergent examples. Note that we do not involve any extra dataset, and we only utilize the standard training set to train the entire framework. ACNs are end-to-end frameworks composed of two parts: an adversarial phase consisting of Generative Adversarial Networks (GANs) to generate the divergent examples and a co-distillation phase consisting of multiple classifiers to learn the divergent examples. These two phases are learned in an iterative and adversarial way. To guarantee the quality of the divergent examples and the stability of ACNs, we further design “Weakly Residual Connection” module and “Restricted Adversarial Search” module to assist in the training process. Extensive experiments with various deep architectures on different datasets well demonstrate the effectiveness of our approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320304623",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Distillation",
      "Epistemology",
      "Generative grammar",
      "Image (mathematics)",
      "Machine learning",
      "Operating system",
      "Organic chemistry",
      "Philosophy",
      "Process (computing)",
      "Programming language",
      "Quality (philosophy)",
      "Set (abstract data type)",
      "Stability (learning theory)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Haoran"
      },
      {
        "surname": "Hu",
        "given_name": "Zhenzhen"
      },
      {
        "surname": "Qin",
        "given_name": "Wei"
      },
      {
        "surname": "Xu",
        "given_name": "Mingliang"
      },
      {
        "surname": "Wang",
        "given_name": "Meng"
      }
    ]
  },
  {
    "title": "Deep feature augmentation for occluded image classification",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107737",
    "abstract": "Due to the difficulty in acquiring massive task-specific occluded images, the classification of occluded images with deep convolutional neural networks (CNNs) remains highly challenging. To alleviate the dependency on large-scale occluded image datasets, we propose a novel approach to improve the classification accuracy of occluded images by fine-tuning the pre-trained models with a set of augmented deep feature vectors (DFVs). The set of augmented DFVs is composed of original DFVs and pseudo-DFVs. The pseudo-DFVs are generated by randomly adding difference vectors (DVs), extracted from a small set of clean and occluded image pairs, to the real DFVs. In the fine-tuning, the back-propagation is conducted on the DFV data flow to update the network parameters. The experiments on various datasets and network structures show that the deep feature augmentation significantly improves the classification accuracy of occluded images without a noticeable influence on the performance of clean images. Specifically, on the ILSVRC2012 dataset with synthetic occluded images, the proposed approach achieves 11.21% and 9.14% average increases in classification accuracy for the ResNet50 networks fine-tuned on the occlusion-exclusive and occlusion-inclusive training sets, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320305409",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Contextual image classification",
      "Convolutional neural network",
      "Deep learning",
      "Feature (linguistics)",
      "Feature extraction",
      "Feature vector",
      "Image (mathematics)",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Cen",
        "given_name": "Feng"
      },
      {
        "surname": "Zhao",
        "given_name": "Xiaoyu"
      },
      {
        "surname": "Li",
        "given_name": "Wuzhuang"
      },
      {
        "surname": "Wang",
        "given_name": "Guanghui"
      }
    ]
  },
  {
    "title": "Solving the same-different task with convolutional neural networks",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2020.12.019",
    "abstract": "Deep learning demonstrated major abilities in solving many kinds of different real-world problems in computer vision literature. However, they are still strained by simple reasoning tasks that humans consider easy to solve. In this work, we probe current state-of-the-art convolutional neural networks on a difficult set of tasks known as the same-different problems. All the problems require the same prerequisite to be solved correctly: understanding if two random shapes inside the same image are the same or not. With the experiments carried out in this work, we demonstrate that residual connections, and more generally the skip connections, seem to have only a marginal impact on the learning of the proposed problems. In particular, we experiment with DenseNets, and we examine the contribution of residual and recurrent connections in already tested architectures, ResNet-18, and CorNet-S respectively. Our experiments show that older feed-forward networks, AlexNet and VGG, are almost unable to learn the proposed problems, except in some specific scenarios. We show that recently introduced architectures can converge even in the cases where the important parts of their architecture are removed. We finally carry out some zero-shot generalization tests, and we discover that in these scenarios residual and recurrent connections can have a stronger impact on the overall test accuracy. On four difficult problems from the SVRT dataset, we can reach state-of-the-art results with respect to the previous approaches, obtaining super-human performances on three of the four problems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521000039",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Carry (investment)",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Economics",
      "Epistemology",
      "Finance",
      "Generalization",
      "Machine learning",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Residual",
      "Set (abstract data type)",
      "Simple (philosophy)",
      "Task (project management)",
      "Test set"
    ],
    "authors": [
      {
        "surname": "Messina",
        "given_name": "Nicola"
      },
      {
        "surname": "Amato",
        "given_name": "Giuseppe"
      },
      {
        "surname": "Carrara",
        "given_name": "Fabio"
      },
      {
        "surname": "Gennaro",
        "given_name": "Claudio"
      },
      {
        "surname": "Falchi",
        "given_name": "Fabrizio"
      }
    ]
  },
  {
    "title": "Customized VGG19 Architecture for Pneumonia Detection in Chest X-Rays",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2020.12.010",
    "abstract": "Pneumonia is one of the major illnesses in children and aged humans due to the Infection in the lungs. Early analysis of pneumonia is necessary to prepare for a possible treatment procedure to regulate and cure the disease. This research aspires to develop a Deep-Learning System (DLS) to diagnose the lung abnormality using chest X-ray (radiograph) images. The proposed work is implemented using; (i) Conventional chest radiographs and (ii) Chest radiograph treated with a threshold filter. The initial experimental evaluation is carried out using the traditional DLS, such as AlexNet, VGG16, VGG19 and ResNet50 with a SoftMax classifier. The results confirmed that, VGG19 provides better classification accuracy (86.97%) compared to other methods. Later, a customized VGG19 network is proposed using the Ensemble Feature Scheme (EFS), which combines the handcrafted features attained with CWT, DWT and GLCM with the Deep-Features (DF) achieved using Transfer-Learning (TL) practice. The performance of customized VGG19 is tested using different classifiers, such as SVM-linear, SVM-RBF, KNN classifier, Random-Forest (RF) and Decision-Tree (DT). The result confirms that VGG19 with RF classifier offers better accuracy (95.70%). When the similar experiment is repeated using threshold filter treated chest radiographs, the VGG19 with RF classifier offered superior classification accuracy (97.94%). This result confirms that, proposed DLS will work well on the benchmark images and in the future, it can be considered to diagnose clinical grade chest radiographs.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520304414",
    "keywords": [
      "Artificial intelligence",
      "Chest radiograph",
      "Classifier (UML)",
      "Computer science",
      "Deep learning",
      "Medicine",
      "Pattern recognition (psychology)",
      "Radiography",
      "Radiology",
      "Softmax function",
      "Support vector machine",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Dey",
        "given_name": "Nilanjan"
      },
      {
        "surname": "Zhang",
        "given_name": "Yu-Dong"
      },
      {
        "surname": "Rajinikanth",
        "given_name": "V."
      },
      {
        "surname": "Pugalenthi",
        "given_name": "R."
      },
      {
        "surname": "Raja",
        "given_name": "N. Sri Madhava"
      }
    ]
  },
  {
    "title": "ReLaText: Exploiting visual relationships for arbitrary-shaped scene text detection with graph convolutional networks",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107684",
    "abstract": "We introduce a new arbitrary-shaped text detection approach named ReLaText by formulating text detection as a visual relationship detection problem. To demonstrate the effectiveness of this new formulation, we start from using a “link” relationship to address the challenging text-line grouping problem firstly. The key idea is to decompose text detection into two subproblems, namely detection of text primitives and prediction of link relationships between nearby text primitive pairs. Specifically, an anchor-free region proposal network based text detector is first used to detect text primitives of different scales from different feature maps of a feature pyramid network, from which a text primitive graph is constructed by linking each pair of nearby text primitives detected from a same feature map with an edge. Then, a Graph Convolutional Network (GCN) based link relationship prediction module is used to prune wrongly-linked edges in the text primitive graph to generate a number of disjoint subgraphs, each representing a detected text instance. As GCN can effectively leverage context information to improve link prediction accuracy, our GCN based text-line grouping approach can achieve better text detection accuracy than previous text-line grouping methods, especially when dealing with text instances with large inter-character or very small inter-line spacing. Consequently, the proposed ReLaText achieves state-of-the-art performance on five public text detection benchmarks, namely RCTW-17, MSRA-TD500, Total-Text, CTW1500 and DAST1500.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320304878",
    "keywords": [
      "Artificial intelligence",
      "Automatic summarization",
      "Combinatorics",
      "Computer science",
      "Disjoint sets",
      "Graph",
      "Image (mathematics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Text detection",
      "Text graph",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Ma",
        "given_name": "Chixiang"
      },
      {
        "surname": "Sun",
        "given_name": "Lei"
      },
      {
        "surname": "Zhong",
        "given_name": "Zhuoyao"
      },
      {
        "surname": "Huo",
        "given_name": "Qiang"
      }
    ]
  },
  {
    "title": "Face illumination recovery for the deep learning feature under severe illumination variations",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107724",
    "abstract": "The deep learning feature is the best for face recognition nowadays, but its performance exhibits unsatisfactorily under severe illumination variations. The main reason is that the deep learning feature was trained by the internet face images with variations of large pose/expression and slight/moderate illumination, which cannot well tackle severe illumination variations. Inspired by the fact that the deep learning feature can cope well with slight/moderate varying illumination, this paper proposes an illumination recovery model to transform severe varying illumination to slight/moderate varying illumination. The illumination recovery model enables the illumination of the severe illumination variation image close to that of the reference image with slight/moderate varying illumination. The reference image generated from the severe illumination variation image is termed as the generated reference image (GRI), which is obtained by normalizing singular values of the logarithm version of the severe illumination variation image to have unit L2-norm. The gradient descent algorithm is employed to address the proposed illumination recovery model, to obtain the generated reference image based illumination recovery image (GRIR). GRIR preserves better face inherent information than GRI such as the face color. Experimental results indicate that the proposed GRIR can efficiently improve the performance of the deep learning feature under severe illumination variations.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320305276",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Face (sociological concept)",
      "Facial recognition system",
      "Feature (linguistics)",
      "Image (mathematics)",
      "Linguistics",
      "Logarithm",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Hu",
        "given_name": "Chang-Hui"
      },
      {
        "surname": "Yu",
        "given_name": "Jian"
      },
      {
        "surname": "Wu",
        "given_name": "Fei"
      },
      {
        "surname": "Zhang",
        "given_name": "Yang"
      },
      {
        "surname": "Jing",
        "given_name": "Xiao-Yuan"
      },
      {
        "surname": "Lu",
        "given_name": "Xiao-Bo"
      },
      {
        "surname": "Liu",
        "given_name": "Pan"
      }
    ]
  },
  {
    "title": "Self-attention binary neural tree for video summarization",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2020.12.016",
    "abstract": "In this paper, we address the problem of shot-level video summarization, which aims at selecting a subset of video shots as a summary to represent the original video contents compactly and completely. Most existing methods rely on various network architectures to learn a single score predictor for shot ranking and selection. Different from these methods, we plug network feature learning into a binary neural tree to consider multi-path predictions for each shot, thus enabling the shot evaluation from different aspects. Due to the hierarchical structure of the tree, video shots can be coarse-to-fine encoded by imposing self-attention on them along branches, leading to favorable predictions. Extensive experiments were conducted on two real-world datasets, and the results reveal that the proposed method achieves superior performance in comparison with previous state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520304475",
    "keywords": [
      "Agronomy",
      "Algorithm",
      "Arithmetic",
      "Artificial intelligence",
      "Artificial neural network",
      "Automatic summarization",
      "Binary number",
      "Binary tree",
      "Biology",
      "Chemistry",
      "Computer science",
      "Data mining",
      "Feature (linguistics)",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pruning",
      "Ranking (information retrieval)",
      "Selection (genetic algorithm)",
      "Shot (pellet)",
      "Tree (set theory)"
    ],
    "authors": [
      {
        "surname": "Fu",
        "given_name": "Hao"
      },
      {
        "surname": "Wang",
        "given_name": "Hongxing"
      }
    ]
  },
  {
    "title": "Discriminative subspace matrix factorization for multiview data clustering",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107676",
    "abstract": "In a real-world scenario, an object is easily considered as features combined by multiple views in reality. Thus, multiview features can be encoded into a unified and discriminative framework to achieve satisfactory clustering performance. An increasing number of algorithms have been proposed for multiview data clustering. However, existing multiview methods have several drawbacks. First, most multiview algorithms focus only on origin data in high dimension directly without the intrinsic structure in the relative low-dimensional subspace. Spectral and manifold-based methods ignore pseudo-information that can be extracted from the optimization process. Thus, we design an unsupervised nonnegative matrix factorization (NMF)-based method called discriminative multiview subspace matrix factorization (DMSMF) for clustering. We provide the following contributions. (1) We extend linear discriminant analysis and NMF to a multiview version and connect them to a unified framework to learn in the discriminant subspace. (2) We propose a multiview manifold regularization term and discriminant multiview manifold regularization term that instruct the regularization term to discriminate different classes and obtain the geometry st ructure from the low-dimensional subspace. (3) We design an effective optimization algorithm with proven convergence to obtain an optimal solution procedure for the complex model. Adequate experiments are conducted on multiple benchmark datasets. Finally, we demonstrate that our model is superior to other comparable multiview data clustering algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320304799",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Clustering high-dimensional data",
      "Computer science",
      "Discriminative model",
      "Eigenvalues and eigenvectors",
      "Linear discriminant analysis",
      "Mathematics",
      "Matrix decomposition",
      "Non-negative matrix factorization",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Regularization (linguistics)",
      "Subspace topology"
    ],
    "authors": [
      {
        "surname": "Ma",
        "given_name": "Jiaqi"
      },
      {
        "surname": "Zhang",
        "given_name": "Yipeng"
      },
      {
        "surname": "Zhang",
        "given_name": "Lefei"
      }
    ]
  },
  {
    "title": "Attributes based skin lesion detection and recognition: A mask RCNN and transfer learning-based deep learning framework",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2020.12.015",
    "abstract": "Malignant melanoma is considered to be one of the deadliest types of skin cancers which is responsible for the massive number of deaths worldwide. According to the American Cancer Society (ACS), more than a million Americans are living with this melanoma. Since 2019, 192,310 new cases of melanoma are registered, where 95,380 are noninvasive, and 96,480 are invasive. The numbers of deaths due to melanoma in 2019 alone are 7,230, comprising 4,740 men and 2,490 women. Melanoma may be curable if diagnosed at the earlier stages; however, the manual diagnosis is time-consuming and also dependent on the expert dermatologist. In this work, a fully automated computerized aided diagnosis (CAD) system is proposed based on the deep learning framework. In the proposed scheme, the original dermoscopic images are initially pre-processed using the decorrelation formulation technique, which later passes the resultant images to the MASK-RCNN for the lesion segmentation. In this step, the MASK RCNN model is trained using the segmented RGB images generated from the ground truth images of ISBI2016 and ISIC2017 datasets. The resultant segmented images are later passed to the DenseNet deep model for feature extraction. Two different layers, average pool and fully connected, are used for feature extraction, which are later combined, and the resultant vector is forwarded to the feature selection block for down - sampling using proposed entropy-controlled least square SVM (LS-SVM). Three datasets are utilized for validation - ISBI2016, ISBI2017, and HAM10000 to achieve an accuracy of 96.3%, 94.8%, and 88.5% respectively. Further, the performance of MASK-RCNN is also validated on ISBI2016 and ISBI2017 to attain an accuracy of 93.6% and 92.7%. To further increase our confidence in the proposed framework, a fair comparison with other state-of-the-art is also provided.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520304463",
    "keywords": [
      "Artificial intelligence",
      "Cancer",
      "Cancer research",
      "Computer science",
      "Deep learning",
      "Entropy (arrow of time)",
      "Feature extraction",
      "Feature selection",
      "Internal medicine",
      "Medicine",
      "Melanoma",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "RGB color model",
      "Segmentation",
      "Skin cancer",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Khan",
        "given_name": "Muhammad Attique"
      },
      {
        "surname": "Akram",
        "given_name": "Tallha"
      },
      {
        "surname": "Zhang",
        "given_name": "Yu-Dong"
      },
      {
        "surname": "Sharif",
        "given_name": "Muhammad"
      }
    ]
  },
  {
    "title": "Faster SVM training via conjugate SMO",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107644",
    "abstract": "We propose an improved version of the SMO algorithm for training classification and regression SVMs, based on a Conjugate Descent procedure. This new approach only involves a modest increase on the computational cost of each iteration but, in turn, usually results in a substantial decrease in the number of iterations required to converge to a given precision. Besides, we prove convergence of the iterates of this new Conjugate SMO as well as a linear rate when the kernel matrix is positive definite. We have implemented Conjugate SMO within the LIBSVM library and show experimentally that it is faster for many hyper-parameter configurations, being often a better option than second order SMO when performing a grid-search for SVM tuning.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320304477",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Combinatorics",
      "Composite material",
      "Computer science",
      "Conjugate",
      "Conjugate gradient method",
      "Convergence (economics)",
      "Economic growth",
      "Economics",
      "Iterated function",
      "Kernel (algebra)",
      "Materials science",
      "Mathematical analysis",
      "Mathematics",
      "Matrix (chemical analysis)",
      "Sequential minimal optimization",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Torres-Barrán",
        "given_name": "Alberto"
      },
      {
        "surname": "Alaíz",
        "given_name": "Carlos M."
      },
      {
        "surname": "Dorronsoro",
        "given_name": "José R."
      }
    ]
  },
  {
    "title": "Robust semi-supervised nonnegative matrix factorization for image clustering",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107683",
    "abstract": "Nonnegative matrix factorization (NMF) is a powerful dimension reduction method, and has received increasing attention in various practical applications. However, most traditional NMF based algorithms are sensitive to noisy data, or fail to fully utilize the limited supervised information. In this paper, a novel robust semi-supervised NMF method, namely correntropy based semi-supervised NMF (CSNMF), is proposed to solve these issues. Specifically, CSNMF adopts a correntropy based loss function instead of the squared Euclidean distance (SED) in constrained NMF to suppress the influence of non-Gaussian noise or outliers contaminated in real world data, and simultaneously uses two types of supervised information, i.e., the pointwise and pairwise constraints, to obtain the discriminative data representation. The proposed method is analyzed in terms of convergence, robustness and computational complexity. The relationships between CSNMF and several previous NMF based methods are also discussed. Extensive experimental results show the effectiveness and robustness of CSNMF in image clustering tasks, compared with several state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320304866",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Cluster analysis",
      "Computer science",
      "Dimensionality reduction",
      "Discriminative model",
      "Eigenvalues and eigenvectors",
      "Gene",
      "Mathematics",
      "Matrix decomposition",
      "Non-negative matrix factorization",
      "Outlier",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Peng",
        "given_name": "Siyuan"
      },
      {
        "surname": "Ser",
        "given_name": "Wee"
      },
      {
        "surname": "Chen",
        "given_name": "Badong"
      },
      {
        "surname": "Lin",
        "given_name": "Zhiping"
      }
    ]
  },
  {
    "title": "Network embedding from the line graph: Random walkers and boosted classification",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2020.12.018",
    "abstract": "In this paper, we propose to embed edges instead of nodes using state-of-the-art neural/factorization methods (DeepWalk, node2vec, NetMF). These methods produce latent representations based on co-ocurrence statistics by simulating fixed-length random walks and then taking bags-of-vectors as the input to the Skip Gram Learning with Negative Sampling (SGNS). We commence by expressing commute times embedding as matrix factorization, and thus relating this embedding to those of DeepWalk and node2vec. Recent results showing formal links between all these methods via the spectrum of graph Laplacian, are then extended to understand the results obtained by SGNS when we embed edges instead of nodes. Since embedding edges is equivalent to embedding nodes in the line graph, we proceed to combine both existing formal characterizations of the line graphs and empirical evidence in order to explain why this embedding dramatically outperforms its nodal counterpart in multi-label classification tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521000027",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Discrete mathematics",
      "Embedding",
      "Factorization",
      "Geometry",
      "Graph",
      "Graph embedding",
      "Line (geometry)",
      "Line graph",
      "Mathematics",
      "Random walk",
      "Statistics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Lozano",
        "given_name": "Miguel Angel"
      },
      {
        "surname": "Escolano",
        "given_name": "Francisco"
      },
      {
        "surname": "Curado",
        "given_name": "Manuel"
      },
      {
        "surname": "Hancock",
        "given_name": "Edwin R."
      }
    ]
  },
  {
    "title": "Facial micro-expressions as a soft biometric for person recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2020.12.021",
    "abstract": "Soft biometrics, although not discriminant enough for person recognition provides additional information that aids traditional person recognition. Initially, attempts were made to integrate appearance-based facial soft biometrics, such as facial marks, skin color, and hair color/style, but more recently behavior-based facial soft biometrics, such as head dynamics, visual speech, and facial expressions have also been studied. Facial expressions are further classified as macro and micro-expressions and most of the existing studies using facial expressions as a soft biometric have focused on macro-expressions. Therefore, in this study, we investigate the utility of micro-expressions as a soft biometric for person recognition. The proposed system is based on the fusion of traditional facial features that model the facial appearance with soft biometric features that model the micro-expressions in an image sequence. We tested a texture-based traditional feature extraction technique, two motion-based soft biometric techniques, and several fusion methods at feature, rank, and decision level. The experiments were conducted on three commonly used micro-expression databases and exhibit an improvement of around 5% identification rate when soft biometric traits are fused with traditional face recognition at decision level.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521000179",
    "keywords": [
      "Artificial intelligence",
      "Biometrics",
      "Computer science",
      "Computer vision",
      "Face detection",
      "Facial expression",
      "Facial recognition system",
      "Feature (linguistics)",
      "Feature extraction",
      "Linear discriminant analysis",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Speech recognition",
      "Three-dimensional face recognition"
    ],
    "authors": [
      {
        "surname": "Saeed",
        "given_name": "Usman"
      }
    ]
  },
  {
    "title": "Pairwise dependence-based unsupervised feature selection",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107663",
    "abstract": "Many research topics present very high dimensional data. Because of the heavy execution times and large memory requirements, many machine learning methods have difficulty in processing these data. In this paper, we propose a new unsupervised feature selection method considering the pairwise dependence of features (feature dependency-based unsupervised feature selection, or DUFS). To avoid selecting redundant features, the proposed method calculates the dependence among features and applies this information to a regression-based unsupervised feature selection process. We can select small feature set with the dependence among features by eliminating redundant features. To consider the dependence among features, we used mutual information widely used in supervised feature selection area. To our best knowledge, it is the first study to consider the pairwise dependence of features in the unsupervised feature selection method. Experimental results for six data sets demonstrate that the proposed method outperforms existing state-of-the-art unsupervised feature selection methods in most cases.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320304660",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Dependency (UML)",
      "Feature (linguistics)",
      "Feature learning",
      "Feature selection",
      "Linguistics",
      "Machine learning",
      "Minimum redundancy feature selection",
      "Mutual information",
      "Pairwise comparison",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Selection (genetic algorithm)",
      "Unsupervised learning"
    ],
    "authors": [
      {
        "surname": "Lim",
        "given_name": "Hyunki"
      },
      {
        "surname": "Kim",
        "given_name": "Dae-Won"
      }
    ]
  },
  {
    "title": "Expand globally, shrink locally: Discriminant multi-label learning with missing labels",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107675",
    "abstract": "In multi-label learning, the issue of missing labels brings a major challenge. Many methods attempt to recovery missing labels by exploiting low-rank structure of label matrix. However, these methods just utilize global low-rank label structure, ignore both local low-rank label structures and label discriminant information to some extent, leaving room for further performance improvement. In this paper, we develop a simple yet effective discriminant multi-label learning (DM2L) method for multi-label learning with missing labels. Specifically, we impose the low-rank structures on all the predictions of instances from the same labels (local shrinking of rank), and a maximally separated structure (high-rank structure) on the predictions of instances from different labels (global expanding of rank). In this way, these imposed low-rank structures can help modeling both local and global low-rank label structures, while the imposed high-rank structure can help providing more underlying discriminability. Our subsequent theoretical analysis also supports these intuitions. In addition, we provide a nonlinear extension via using kernel trick to enhance DM2L and establish a concave-convex objective to learn these models. Compared to the other methods, our method involves the fewest assumptions and only one hyper-parameter. Even so, extensive experiments show that our method still outperforms the state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320304787",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Discriminant",
      "Epistemology",
      "Gaussian",
      "Kernel (algebra)",
      "Learning to rank",
      "Linear discriminant analysis",
      "Machine learning",
      "Mathematics",
      "Matrix completion",
      "Missing data",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Rank (graph theory)",
      "Ranking (information retrieval)",
      "Simple (philosophy)"
    ],
    "authors": [
      {
        "surname": "Ma",
        "given_name": "Zhongchen"
      },
      {
        "surname": "Chen",
        "given_name": "Songcan"
      }
    ]
  },
  {
    "title": "A modified capsule network algorithm for oct corneal image segmentation",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.01.005",
    "abstract": "Cornea is the outmost layer of an eye helps to focuses the light rays towards the retinal layer of the eye. The irregular thickness of the corneal layer results in poor focus of light rays over the retinal layer and hence it results in blur vision. Lasik is a surgical procedure made for correcting the irregular thickness of the cornea to certain extent for making the light rays to fall exactly on the retinal layer. In order to undergo with a lasik procedure, an eye must have sufficient thickness of corneal layer to tolerate the medical procedure. Similarly, the corneal layers are can't be operated after certain extent. Therefore there is a need for pre-surgical planning by measuring the thickness availability of the corneal layer. The proposed work is engaged to identify the three major boundaries of the corneal layer using a capsule network based algorithm. The proposed work is segregated as preprocessing, classification and segmentation. A hybrid speckle noise reduction filter was employed in the preprocessing stage to minimize the noise presence in the corneal images. Then the images are moved further to train the ClassCaps algorithm to classify a better noiseless image from the group of test images. The modified SegCaps algorithm was utilized in this work to detect the three major boundaries of the cornea from the noiseless image output from the ClassCaps algorithm. The performance of the proposed algorithm was verified with the several classification and segmentation algorithm to prove its accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521000155",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Cornea",
      "LASIK",
      "Optics",
      "Physics",
      "Preprocessor",
      "Segmentation",
      "Speckle noise",
      "Speckle pattern"
    ],
    "authors": [
      {
        "surname": "Koresh",
        "given_name": "H. James Deva"
      },
      {
        "surname": "Chacko",
        "given_name": "Shanty"
      },
      {
        "surname": "Periyanayagi",
        "given_name": "M."
      }
    ]
  },
  {
    "title": "Fine-grained image inpainting with scale-enhanced generative adversarial network",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2020.12.008",
    "abstract": "With the emergence of Generative Adversarial Networks, great progress has been made in image inpainting. However, most existing methods can produce plausible results, but fail to generate finer textures and structures. This is mainly due to the fact that (1) the generation of finer content in the masked region of an image is not constrained enough during network training, and (2) many different alternative pixels are exist to fill in the masked regions, making it very difficult for the inpainting network to generate reasonable sharp edges. To address these issues, we propose a Scale Enhanced GAN (SE-GAN) model which combines the constraints of large- and small-scale receptive fields of our tailor-made discriminators to achieve fine-grained constraint on image details, a novel edge loss to further ensure the sharpness of the generated image. Experiments on multiple datasets including faces(CelebA-HQ), textures(DTD), buildings(Facade) and natural images(ImageNet, Places2) show that our approach can generate higher quality inpainting results with more details than previous methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520304396",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Constraint (computer-aided design)",
      "Enhanced Data Rates for GSM Evolution",
      "Generative grammar",
      "Geometry",
      "Image (mathematics)",
      "Image processing",
      "Image restoration",
      "Image texture",
      "Inpainting",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Pixel",
      "Quantum mechanics",
      "Scale (ratio)",
      "Texture synthesis"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Weirong"
      },
      {
        "surname": "Cao",
        "given_name": "Chengrui"
      },
      {
        "surname": "Liu",
        "given_name": "Jie"
      },
      {
        "surname": "Ren",
        "given_name": "Chenwen"
      },
      {
        "surname": "Wei",
        "given_name": "Yulin"
      },
      {
        "surname": "Guo",
        "given_name": "Honglin"
      }
    ]
  },
  {
    "title": "Joint discriminative feature learning for multimodal finger recognition",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107704",
    "abstract": "Recently, finger-based multimodal biometrics, due to its high security and stability, has received considerable attention compared with unimodal biometrics. However, existing multimodal finger feature extraction approaches separately extract the features of different modalities, at the same time ignoring correlations among these different modalities. Furthermore, most of the conventional finger feature representation approaches are hand-crafted by design, which require strong prior knowledge. It is therefore very important to explore and develop a suitable feature representation and fusion strategy for multimodal biometrics recognition. In this paper, we proposed a joint discriminative feature learning (JDFL) framework for multimodal finger recognition by combining finger vein (FV) and finger knuckle print (FKP) patterns. For the FV and FKP images, we first established the informative dominant direction vector by convoluting a bank of Gabor filters and the original finger image. Then, we developed a simple yet effective feature learning algorithm, which simultaneously maximized the distance of between-class samples and minimized the distance of within-class samples, as well as maximized the correlation among inter-modality samples of the within-class. Finally, we integrated the block-wise histograms of the learned feature maps together for multimodal finger fusion recognition. Experimental results demonstrated that the proposed approach has a better recognition performance than state-of-the-art finger recognition methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320305070",
    "keywords": [
      "Artificial intelligence",
      "Biometrics",
      "Computer science",
      "Discriminative model",
      "Feature (linguistics)",
      "Feature extraction",
      "Feature learning",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Shuyi"
      },
      {
        "surname": "Zhang",
        "given_name": "Bob"
      },
      {
        "surname": "Fei",
        "given_name": "Lunke"
      },
      {
        "surname": "Zhao",
        "given_name": "Shuping"
      }
    ]
  },
  {
    "title": "Representative null space LDA for discriminative dimensionality reduction",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107664",
    "abstract": "Null space Linear Discriminant Analysis (NLDA) was proposed twenty years ago to overcome the singularity problem of LDA in practical applications. With two decades of technique development, many Discriminative Dimensionality Reduction (DDR) methods that outperform NLDA have been proposed. This paper provides new insight into NLDA and illustrates that NLDA is much more powerful after solving its inherent problem. The main problem of NLDA is the intrinsic overfitting problem. An ideal NLDA model is proposed to analyze its overfitting problem. Based on the ideal NLDA model, a more reasonable Representative NLDA (RNLDA) method is proposed to prevent overfitting. Two simple but efficient RNLDA algorithms are proposed to implement the RNLDA method with a theoretical proof. This study theoretically analyzed and indicated that applying the classical but simple hold-out pretraining method can automatically set the only parameter to achieve high performance. Extensive experiments with eight databases demonstrate the superior performance of the RNLDA method over state-of-the-art DDR methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320304672",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Curse of dimensionality",
      "Data mining",
      "Dimensionality reduction",
      "Discriminative model",
      "Epistemology",
      "Geometry",
      "Ideal (ethics)",
      "Linear discriminant analysis",
      "Machine learning",
      "Mathematics",
      "Null (SQL)",
      "Overfitting",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Reduction (mathematics)",
      "Simple (philosophy)"
    ],
    "authors": [
      {
        "surname": "He",
        "given_name": "Zaixing"
      },
      {
        "surname": "Wu",
        "given_name": "Mengtian"
      },
      {
        "surname": "Zhao",
        "given_name": "Xinyue"
      },
      {
        "surname": "Zhang",
        "given_name": "Shuyou"
      },
      {
        "surname": "Tan",
        "given_name": "Jianrong"
      }
    ]
  },
  {
    "title": "KDD: A kernel density based descriptor for 3D point clouds",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107691",
    "abstract": "3D feature description is one of the central techniques that rely on point clouds since a lot of point cloud processing techniques apply the point-to-point correspondences that are achieved via feature descriptors as input data. The feature descriptor encodes the information of the underlying surface around the feature point so as to make a local surface distinguished from another. The focus of the existing descriptors is accumulating the geometric or topological measurements into histograms or encoding the 2D images that are acquired by rotationally projecting the 3D local surfaces onto 2D planes. Histograms can hardly deal with three or more dimensional information, and the rotational projection operation does bring much unnecessary intermediate computations. To overcome these limitations, in this article, a descriptor named Kernel Density Descriptor (KDD) has been presented. One core contribution of this method is to encode the information of the whole 3D space around the feature point via kernel density estimation, and another is providing the strategy for selecting different matching metrics for datasets with diverse levels of resolution qualities. We compare KDD against several representative descriptors on publicly available datasets, the experimental results demonstrate that the KDD descriptor achieves a satisfactory and balanced performance in terms of descriptiveness, robustness, and compactness, furthermore, the comparisons validate the overall superiority of our method. The benefits and applicability on object registration and recognition and 3D object reconstruction are demonstrated by the favorable results that are obtained for both public datastes and the real-world point clouds of Terracotta fragments.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320304945",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Estimator",
      "Feature (linguistics)",
      "Feature vector",
      "Gene",
      "Histogram",
      "Image (mathematics)",
      "Kernel (algebra)",
      "Kernel density estimation",
      "Kernel method",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Point cloud",
      "Projection (relational algebra)",
      "Robustness (evolution)",
      "Statistics",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Yuhe"
      },
      {
        "surname": "Li",
        "given_name": "Chunhui"
      },
      {
        "surname": "Guo",
        "given_name": "Bao"
      },
      {
        "surname": "Guo",
        "given_name": "Chenhao"
      },
      {
        "surname": "Zhang",
        "given_name": "Shunli"
      }
    ]
  },
  {
    "title": "Attribute and instance weighted naive Bayes",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107674",
    "abstract": "Naive Bayes (NB) continues to be one of the top 10 data mining algorithms, but its conditional independence assumption rarely holds true in real-world applications. Therefore, many different categories of improved approaches, including attribute weighting and instance weighting, have been proposed to alleviate this assumption. However, few of these approaches simultaneously pay attention to attribute weighting and instance weighting. In this study, we propose a new improved model called attribute and instance weighted naive Bayes (AIWNB), which combines attribute weighting with instance weighting into one uniform framework. In AIWNB, the attribute weights are incorporated into the naive Bayesian classification formula, and then the prior and conditional probabilities are estimated using instance weighted training data. To learn instance weights, we single out an eager approach and a lazy approach, and thus two different versions are created, which we denote as AIWNBE and AIWNBL, respectively. Extensive experimental results show that both AIWNBE and AIWNBL significantly outperform NB and all the other existing state-of-the-art competitors.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320304775",
    "keywords": [
      "Artificial intelligence",
      "Bayesian probability",
      "Computer science",
      "Conditional independence",
      "Data mining",
      "Independence (probability theory)",
      "Machine learning",
      "Mathematics",
      "Medicine",
      "Naive Bayes classifier",
      "Pattern recognition (psychology)",
      "Radiology",
      "Statistics",
      "Support vector machine",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Huan"
      },
      {
        "surname": "Jiang",
        "given_name": "Liangxiao"
      },
      {
        "surname": "Yu",
        "given_name": "Liangjun"
      }
    ]
  },
  {
    "title": "Cluster-wise unsupervised hashing for cross-modal similarity search",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107732",
    "abstract": "Cross-modal hashing similarity retrieval plays dual roles across various applications including search engines and autopilot systems. More generally, these methods also known to reduce the computation and memory storage in a training scheme. The key limitation of current methods are that: (i) they relax the discrete constrains to solve the optimization problem which may defeat the model purpose, (ii) projecting heterogenous data into a latent space may encourage to loss the diverse representations in such data, (iii) transforming real-valued data point to the binary codes always resulting in a loss of information and producing the suboptimal continuous latent space. In this paper, we propose a novel framework to project the original data points from different modalities into its own low-dimensional latent space and finds the cluster centroid points in its a low-dimensional space, using Cluster-wise Unsupervised Hashing (CUH). In particular, the proposed clustering scheme aims to jointly learns the compact hash codes and the corresponding linear hash functions. A discrete optimization framework is developed to learn the unified binary codes across modalities under of the guidance cluster-wise code-prototypes. Extensive experiments over multiple datasets demonstrate the effectiveness of our proposed model in comparison with the state-of-the-art in unsupervised cross-modal hashing tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320305355",
    "keywords": [
      "Arithmetic",
      "Artificial intelligence",
      "Binary code",
      "Binary number",
      "Cluster analysis",
      "Computer science",
      "Computer security",
      "Data mining",
      "Double hashing",
      "Dynamic perfect hashing",
      "Hash function",
      "Hash table",
      "Image (mathematics)",
      "K-independent hashing",
      "Locality-sensitive hashing",
      "Mathematics",
      "Nearest neighbor search",
      "Similarity (geometry)",
      "Theoretical computer science",
      "Universal hashing"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Lu"
      },
      {
        "surname": "Yang",
        "given_name": "Jie"
      },
      {
        "surname": "Zareapoor",
        "given_name": "Masoumeh"
      },
      {
        "surname": "Zheng",
        "given_name": "Zhonglong"
      }
    ]
  },
  {
    "title": "All-in-focus synthetic aperture imaging using generative adversarial network-based semantic inpainting",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107669",
    "abstract": "Occlusions handling poses a significant challenge to many computer vision and pattern recognition applications. Recently, Synthetic Aperture Imaging (SAI), which uses more than two cameras, is widely applied to reconstruct occluded objects in complex scenes. However, it usually fails in cases of heavy occlusions, in particular, when the occluded information is not captured by any of the camera views. Hence, it is a challenging task to generate a realistic all-in-focus synthetic aperture image which shows a completely occluded object. In this paper, semantic inpainting using a Generative Adversarial Network (GAN) is proposed to address the above-mentioned problem. The proposed method first computes a synthetic aperture image of the occluded objects using a labeling method, and an alpha matte of the partially occluded objects. Then, it uses energy minimization to reconstruct the background by focusing on the background depth of each camera. Finally, the occluded regions of the synthesized image are semantically inpainted using a GAN and the results are composited with the reconstructed background to generate a realistic all-in-focus image. The experimental results demonstrate that the proposed method can handle heavy occlusions and can produce better all-in-focus images than other state-of-the-art methods. Compared with traditional labeling methods, our method can quickly generate label for occlusion without introducing noise. To the best of our knowledge, our method is the first to address missing information caused by heavy occlusions in SAI using a GAN.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320304726",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Focus (optics)",
      "Generative adversarial network",
      "Image (mathematics)",
      "Inpainting",
      "Noise (video)",
      "Object (grammar)",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Synthetic aperture radar"
    ],
    "authors": [
      {
        "surname": "Pei",
        "given_name": "Zhao"
      },
      {
        "surname": "Jin",
        "given_name": "Min"
      },
      {
        "surname": "Zhang",
        "given_name": "Yanning"
      },
      {
        "surname": "Ma",
        "given_name": "Miao"
      },
      {
        "surname": "Yang",
        "given_name": "Yee-Hong"
      }
    ]
  },
  {
    "title": "A three-step classification framework to handle complex data distribution for radar UAV detection",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107709",
    "abstract": "Unmanned aerial vehicles (UAVs) have been used in a wide range of applications and become an increasingly important radar target. To better model radar data and to tackle the curse of dimensionality, a three-step classification framework is proposed for UAV detection. First we propose to utilize the greedy subspace clustering to handle potential outliers and the complex sample distribution of radar data. Parameters of the resulting multi-Gaussian model, especially the covariance matrices, could not be reliably estimated due to insufficient training samples and the high dimensionality. Thus, in the second step, a multi-Gaussian subspace reliability analysis is proposed to handle the unreliable feature dimensions of these covariance matrices. To address the challenges of classifying samples using the complex multi-Gaussian model and to fuse the distances of a sample to different clusters at different dimensionalities, a subspace-fusion scheme is proposed in the third step. The proposed approach is validated on a large benchmark dataset, which significantly outperforms the state-of-the-art approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320305124",
    "keywords": [
      "Aerospace engineering",
      "Anomaly detection",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Cluster analysis",
      "Computer science",
      "Covariance",
      "Curse of dimensionality",
      "Data mining",
      "Engineering",
      "Gaussian",
      "Geodesy",
      "Geography",
      "Mathematics",
      "Outlier",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Radar",
      "Range (aeronautics)",
      "Statistics",
      "Subspace topology",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Ren",
        "given_name": "Jianfeng"
      },
      {
        "surname": "Jiang",
        "given_name": "Xudong"
      }
    ]
  },
  {
    "title": "Dual subspace discriminative projection learning",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107581",
    "abstract": "In this paper, we propose a dual subspace discriminative projection learning (DSDPL) framework for multi-category image classification. Our approach reflects the notion that images are composed of class-shared information, class-specific information, and sparse noise. Unlike traditional subspace learning methods, DSDPL serves to decompose original high dimensional data, via learned projection matrices, into class-shared and class-specific subspaces. The learned projection matrices are jointly constrained with l 2,1 sparse norm and LDA terms while the reconstructive properties of DSDPL reduce information loss, leading to greater stability within low dimensional subspaces. Regression-based terms are also included to facilitate a more robust classification approach, using extracted class-specific features for better classification. Our approach is examined on five different datasets for face, object and scene classifications. Experimental results demonstrate not only the superiority and versatility of DSDPL over current benchmark approaches, but also a more robust classification approach with low sample size training data.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303848",
    "keywords": [
      "Algorithm",
      "Art",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biochemistry",
      "Chemistry",
      "Class (philosophy)",
      "Computer science",
      "Contextual image classification",
      "Discriminative model",
      "Dual (grammatical number)",
      "Gene",
      "Geodesy",
      "Geography",
      "Geometry",
      "Image (mathematics)",
      "Linear subspace",
      "Literature",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Projection (relational algebra)",
      "Robustness (evolution)",
      "Subspace topology"
    ],
    "authors": [
      {
        "surname": "Belous",
        "given_name": "Gregg"
      },
      {
        "surname": "Busch",
        "given_name": "Andrew"
      },
      {
        "surname": "Gao",
        "given_name": "Yongsheng"
      }
    ]
  },
  {
    "title": "Hyperspectral remote sensing image classification based on tighter random projection with minimal intra-class variance algorithm",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107635",
    "abstract": "Aiming at solving the problem of image size limiting in the traditional Random Projection (RP) algorithm, a novel Tighter Random Projection (TRP), which combines the scheme with Minimal Intra-class Variance (TRP-MIV) for hyperspectral remote sensing image classification is proposed. First, a new tighter dimensional boundary for expanding image size with the TRP-MIV matrix selected by multiple sampling for improving the class separability is defined to reduce dimension. Then the proposed algorithm is implemented, which integrates TRP-MIV for dimensionality reduction and Minimum Distance (MD) classifier for image classification. Finally, the image size and dimensionality reduction are evaluated by the number of spectral pixels under different theorems, and the spectral difference before and after dimensionality reduction, respectively. Classification performance is evaluated by kappa coefficient, Overall Accuracy (OA), Average Accuracy (AA), Average Precision Rate (APR) and running time. Classification results are obtained from the Airborne Visible/Infrared Imaging Spectrometer (AVIRIS) scanner and the Reflective Optics System Imaging Spectrometer (ROSIS) scanner, which indicate that the proposed algorithm is efficient and promising.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320304386",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Curse of dimensionality",
      "Dimensionality reduction",
      "Hyperspectral imaging",
      "Imaging spectrometer",
      "Mathematics",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Pixel",
      "Principal component analysis",
      "Spectrometer"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Quanhua"
      },
      {
        "surname": "Jia",
        "given_name": "Shuhan"
      },
      {
        "surname": "Li",
        "given_name": "Yu"
      }
    ]
  },
  {
    "title": "A review of lane detection methods based on deep learning",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107623",
    "abstract": "Lane detection is an application of environmental perception, which aims to detect lane areas or lane lines by camera or lidar. In recent years, gratifying progress has been made in detection accuracy. To the best of our knowledge, this paper is the first attempt to make a comprehensive review of vision-based lane detection methods. First, we introduce the background of lane detection, including traditional lane detection methods and related deep learning methods. Second, we group the existing lane detection methods into two categories: two-step and one-step methods. Around the above summary, we introduce lane detection methods from the following two perspectives: (1) network architectures, including classification and object detection-based methods, end-to-end image-segmentation based methods, and some optimization strategies; (2) related loss functions. For each method, its contributions and weaknesses are introduced. Then, a brief comparison of representative methods is presented. Finally, we conclude this survey with some current challenges, such as expensive computation and the lack of generalization. And we point out some directions to be further explored in the future, that is, semi-supervised learning, meta-learning and neural architecture search, etc.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032030426X",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Generalization",
      "Geography",
      "Geometry",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Object detection",
      "Pattern recognition (psychology)",
      "Pedestrian",
      "Pedestrian detection",
      "Point (geometry)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Tang",
        "given_name": "Jigang"
      },
      {
        "surname": "Li",
        "given_name": "Songbin"
      },
      {
        "surname": "Liu",
        "given_name": "Peng"
      }
    ]
  },
  {
    "title": "Robust control point estimation with an out-of-focus camera calibration pattern",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2020.12.014",
    "abstract": "The calibration of a zoom lens camera depends on the precision of the localization of control points. At a long focal length, the narrow depth-of-field (DOF) causes defocused blurring and an inevitable decrease in accuracy in control points localization. In particular, the camera calibration requires multiple control points defined on calibration patterns acquired at various camera angles. However, clear pattern images are difficult to obtain owing to the narrow DOF. We propose a robust and intuitive method to accurately estimate the control points in blurred images. To obtain control points that are less affected by blurring, we dynamically varied the circle size in the patterns and identified the local maximum point using the intensity gradient of accumulated concentric circles. This approach is robust to blurring and can be employed at all zoom levels. In our experiments, the error of the control point estimation was measured while varying the angles of the calibration patterns and the degree of blurring. Compared with the conventional checker pattern method, the performance of the proposed method in the estimation of the control points was better and its related camera parameters with severely defocused images were settled.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520304451",
    "keywords": [
      "Artificial intelligence",
      "Calibration",
      "Camera auto-calibration",
      "Camera lens",
      "Camera matrix",
      "Camera resectioning",
      "Computer science",
      "Computer vision",
      "Control point",
      "Focal length",
      "Focus (optics)",
      "Geometry",
      "Lens (geology)",
      "Mathematics",
      "Optics",
      "Physics",
      "Pinhole camera model",
      "Point (geometry)",
      "Statistics",
      "Zoom",
      "Zoom lens"
    ],
    "authors": [
      {
        "surname": "Choi",
        "given_name": "Hyunseok"
      },
      {
        "surname": "Ha",
        "given_name": "Ho-Gun"
      },
      {
        "surname": "Lee",
        "given_name": "Hyunki"
      },
      {
        "surname": "Hong",
        "given_name": "Jaesung"
      }
    ]
  },
  {
    "title": "Surrogate network-based sparseness hyper-parameter optimization for deep expression recognition",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107701",
    "abstract": "For facial expression recognition, the sparseness constraints of the features or weights can improve the generalization ability of a deep network. However, the optimization of the hyper-parameters in fusing different sparseness strategies demands much computation, when the traditional gradient-based algorithms are used. In this work, an iterative framework with surrogate network is proposed for the optimization of hyper-parameters in fusing different sparseness strategies. In each iteration, a network with significantly smaller model complexity is fitted to the original large network based on four Euclidean losses, where the hyper-parameters are optimized with heuristic optimizers. Since the surrogate network uses the same deep metrics and embeds the same hyper-parameters as the original network, the optimized hyper-parameters are then used for the training of the original deep network in the next iteration. While the performance of the proposed algorithm is justified with a tiny model, i.e. LeNet on the FER2013 database, our approach achieved competitive performances on six publicly available expression datasets, i.e., FER2013, CK+, Oulu-CASIA, MMI, AFEW and AffectNet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320305045",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computation",
      "Computer science",
      "Euclidean distance",
      "Generalization",
      "Heuristic",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Xie",
        "given_name": "Weicheng"
      },
      {
        "surname": "Chen",
        "given_name": "Wenting"
      },
      {
        "surname": "Shen",
        "given_name": "Linlin"
      },
      {
        "surname": "Duan",
        "given_name": "Jinming"
      },
      {
        "surname": "Yang",
        "given_name": "Meng"
      }
    ]
  },
  {
    "title": "Linear classifier combination via multiple potential functions",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107681",
    "abstract": "A vital aspect of the classification based model construction process is the calibration of the scoring function. One of the weaknesses of the calibration process is that it does not take into account the information about the relative positions of the recognized objects in the feature space. To alleviate this limitation, in this paper, we propose a novel concept of calculating a scoring function based on the distance of the object from the decision boundary and its distance to the class centroid. An important property is that the proposed score function has the same nature for all linear base classifiers, which means that outputs of these classifiers are equally represented and have the same meaning. The proposed approach is compared with other ensemble algorithms and experiments on multiple Keel datasets demonstrate the effectiveness of our method. To discuss the results of our experiments, we use multiple classification performance measures and statistical analysis.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320304842",
    "keywords": [
      "Artificial intelligence",
      "Calibration",
      "Centroid",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Decision boundary",
      "Epistemology",
      "Linear classifier",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Property (philosophy)",
      "Random subspace method",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Trajdos",
        "given_name": "Pawel"
      },
      {
        "surname": "Burduk",
        "given_name": "Robert"
      }
    ]
  },
  {
    "title": "Joint architecture and knowledge distillation in CNN for Chinese text recognition",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107722",
    "abstract": "The distillation technique helps transform cumbersome neural networks into compact networks so that models can be deployed on alternative hardware devices. The main advantage of distillation-based approaches include a simple training process, supported by most off-the-shelf deep learning software and no special hardware requirements. In this paper, we propose a guideline for distilling the architecture and knowledge of pretrained standard CNNs. The proposed algorithm is first verified on a large-scale task: offline handwritten Chinese text recognition (HCTR). Compared with the CNN in the state-of-the-art system, the reconstructed compact CNN can reduce the computational cost by > 10 × and the model size by > 8 × with negligible accuracy loss. Then, by conducting experiments on two additional classification task datasets: Chinese Text in the Wild (CTW) and MNIST, we demonstrate that the proposed approach can also be successfully applied on mainstream backbone networks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320305252",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Chemistry",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Distillation",
      "Economics",
      "MNIST database",
      "Machine learning",
      "Management",
      "Operating system",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Process (computing)",
      "Programming language",
      "Software",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Zi-Rui"
      },
      {
        "surname": "Du",
        "given_name": "Jun"
      }
    ]
  },
  {
    "title": "STAN: A sequential transformation attention-based network for scene text recognition",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107692",
    "abstract": "Scene text with an irregular layout is difficult to recognize. To this end, a Sequential Transformation Attention-based Network (STAN), which comprises a sequential transformation network and an attention-based recognition network, is proposed for general scene text recognition. The sequential transformation network rectifies irregular text by decomposing the task into a series of patch-wise basic transformations, followed by a grid projection submodule to smooth the junction between neighboring patches. The entire rectification process is able to be trained in an end-to-end weakly supervised manner, requiring only images and their corresponding groundtruth text. Based on the rectified images, an attention-based recognition network is employed to predict a character sequence. Experiments on several benchmarks demonstrate the state-of-the-art performance of STAN on both regular and irregular text.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320304957",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Biology",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Economics",
      "Gene",
      "Genetics",
      "Geometry",
      "Grid",
      "Management",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Physics",
      "Power (physics)",
      "Process (computing)",
      "Projection (relational algebra)",
      "Quantum mechanics",
      "Rectification",
      "Sequence (biology)",
      "State (computer science)",
      "Task (project management)",
      "Transformation (genetics)"
    ],
    "authors": [
      {
        "surname": "Lin",
        "given_name": "Qingxiang"
      },
      {
        "surname": "Luo",
        "given_name": "Canjie"
      },
      {
        "surname": "Jin",
        "given_name": "Lianwen"
      },
      {
        "surname": "Lai",
        "given_name": "Songxuan"
      }
    ]
  },
  {
    "title": "Local structured feature learning with dynamic maximum entropy graph",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107673",
    "abstract": "In recent years, Linear Discriminant Analysis (LDA) has seen huge adoption in data mining applications. Due to its globality, it is incompetent to handle multimodal data. Besides, most of LDA’s variants learn the projection matrix based on the pre-defined similarity matrix, which is easily affected by noisy and irrelevant features. To address above two issues, a novel local structured feature learning with Dynamic Maximum Entropy Graph (DMEG) method is developed which firstly develops a more discriminative LDA with whitening constraint that can minimize the within-class scatter while keeping the total samples scatter unchanged simultaneously. Second, for exploring the local structure of data, the ℓ0-norm constraint is imposed on similarity matrix to ensure the k connectivity on graph. More importantly, proposed model learns the similarity and projection matrix simultaneously to ensure that the neighborships can be found in the optimal subspace where the noise have been removed already. Moreover, a maximum entropy regularization is employed to reinforce the discriminability of graph and avoid the trivial solution. Last but not least, an efficient iterative optimization algorithm is provided to optimize proposed model with a NP-hard constraint. Extensive experiments conducted on synthetic and several real-world data sets demonstrate the efficiency in classification task and robustness to noise of proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320304763",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Discriminative model",
      "Entropy (arrow of time)",
      "Graph",
      "Linear discriminant analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Subspace topology",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Zheng"
      },
      {
        "surname": "Nie",
        "given_name": "Feiping"
      },
      {
        "surname": "Wang",
        "given_name": "Rong"
      },
      {
        "surname": "Yang",
        "given_name": "Hui"
      },
      {
        "surname": "Li",
        "given_name": "Xuelong"
      }
    ]
  },
  {
    "title": "graphkit-learn: A Python library for graph kernels based on linear patterns",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.01.003",
    "abstract": "This paper presents graphkit-learn, the first Python library for efficient computation of graph kernels based on linear patterns, able to address various types of graphs. Graph kernels based on linear patterns are thoroughly implemented, each with specific computing methods, as well as two well-known graph kernels based on non-linear patterns for comparative analysis. Since computational complexity is an Achilles’ heel of graph kernels, we provide several strategies to address this critical issue, including parallelization, the trie data structure, and the FCSP method that we extend to other kernels and edge comparison. All proposed strategies save orders of magnitudes of computing time and memory usage. Moreover, all the graph kernels can be simply computed with a single Python statement, thus are appealing to researchers and practitioners. For the convenience of use, an advanced model selection procedure is provided for both regression and classification problems. Experiments on synthesized datasets and 11 real-world benchmark datasets show the relevance of the proposed library.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521000131",
    "keywords": [
      "Algorithm",
      "Computation",
      "Computer science",
      "Data structure",
      "Graph",
      "Graph algorithms",
      "Programming language",
      "Python (programming language)",
      "Theoretical computer science",
      "Trie"
    ],
    "authors": [
      {
        "surname": "Jia",
        "given_name": "Linlin"
      },
      {
        "surname": "Gaüzère",
        "given_name": "Benoit"
      },
      {
        "surname": "Honeine",
        "given_name": "Paul"
      }
    ]
  },
  {
    "title": "FeatFlow: Learning geometric features for 3D motion estimation",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107574",
    "abstract": "3D motion estimation is an important prerequisite for the autonomous operation of vehicles and robots in dynamic environments. This work presents FeatFlow, a novel neural network architecture to estimate 3D motions from unstructured point clouds. Specifically, we learn deep geometric features to estimate the dense scene flow and the ego-motion of the platform. We build a scene flow estimation pipeline by an encoder-decoder architecture which comprises three novel modules: feature extractor, motion embedder, and flow decoder. By using a point-score layer to assign scores to the extracted features in a learning procedure, the feature extractor effectively extracts keypoints and features that are most significant for estimating the relative transformation between two consecutive point clouds. The whole model adaptively learns the required robust descriptors to represent a variety of point motions at the object or scene level. We evaluated our approach on synthetic data from FlyingThings3D, and real-world LiDAR scans from KITTI and Oxford RobotCar. Our network successfully generalizes to datasets with different patterns, outperforming various baselines and achieving state-of-the-art performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303770",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Encoder",
      "Feature (linguistics)",
      "Gene",
      "Geometry",
      "Image (mathematics)",
      "Linguistics",
      "Mathematics",
      "Motion (physics)",
      "Motion estimation",
      "Operating system",
      "Optical flow",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pipeline (software)",
      "Point (geometry)",
      "Point cloud",
      "Programming language",
      "Transformation (genetics)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Qing"
      },
      {
        "surname": "Wang",
        "given_name": "Cheng"
      },
      {
        "surname": "Li",
        "given_name": "Xin"
      },
      {
        "surname": "Wen",
        "given_name": "Chenglu"
      }
    ]
  },
  {
    "title": "DeepCADRME: A deep neural model for complex adverse drug reaction mentions extraction",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2020.12.013",
    "abstract": "Extracting mentions of Adverse Drug Reaction (ADR) from biomedical texts, aiming to support pharmacovigilance and drug safety surveillance, remains a challenging task as many ADR mentions are nested, discontinuous and overlapping. To solve these issues, in this paper, we propose a deep neural model for Complex Adverse Drug Reaction Mentions Extraction, called DeepCADRME. It first transforms the ADR mentions extraction problem as an N-level tagging sequence. Then, it feeds the sequences to an N-level model based on contextual embeddings where the output of the pre-trained model of the current level is used to build a new deep contextualized representation for the next level. This allows the DeepCADRME system to transfer knowledge between levels. Experimental results performed on the TAC 2017 ADR dataset, show the effectiveness of DeepCADRME which leads to a new state-of-the-art performance by reaching a F1 of 85.35% and 85.41% with and without mention types, respectively. The evaluation results also highlight the benefits of exploring language model to effectively extract different types of ADR mentions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552030444X",
    "keywords": [
      "Adverse drug reaction",
      "Artificial intelligence",
      "Artificial neural network",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Deep neural networks",
      "Drug",
      "Extraction (chemistry)",
      "Medicine",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Pharmacology"
    ],
    "authors": [
      {
        "surname": "El-allaly",
        "given_name": "Ed-drissiya"
      },
      {
        "surname": "Sarrouti",
        "given_name": "Mourad"
      },
      {
        "surname": "En-Nahnahi",
        "given_name": "Noureddine"
      },
      {
        "surname": "Ouatik El Alaoui",
        "given_name": "Said"
      }
    ]
  },
  {
    "title": "3D-GAT: 3D-Guided adversarial transform network for person re-identification in unseen domains",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107799",
    "abstract": "Person Re-identification (ReID) has witnessed remarkable improvements in the past couple of years. However, its applications in real-world scenarios are limited by the disparity among different cameras and datasets. In general, it remains challenging to generalize ReID algorithms from one domain to another, especially when the target domain is unknown. To solve this issue, we develop a 3D-guided adversarial transform (3D-GAT) network which explores the transfer ability of source training data to facilitate learning domain-independent knowledge. Being aware of a 3D model and human poses, 3D-GAT makes use of image-to-image translation to synthesize person images in different conditions whilst preserving features for identification as much as possible. With these augmented training data, it is easier for ReID approaches to perceive how a person can appear differently under varying viewpoints and poses, most of which are not seen in the training data, and thus achieve higher ReID accuracy especially in an unknown domain. Extensive experiments conducted on Market-1501, DukeMTMC-reID and CUHK03 demonstrate the effectiveness of our proposed approach, which is competitive to the baseline models in the original dataset and sets the new state-of-the-art in direct transfer to other datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320306026",
    "keywords": [
      "Adversarial system",
      "Art",
      "Artificial intelligence",
      "Baseline (sea)",
      "Biochemistry",
      "Biology",
      "Botany",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Domain (mathematical analysis)",
      "Gene",
      "Geology",
      "Identification (biology)",
      "Image (mathematics)",
      "Image translation",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Messenger RNA",
      "Oceanography",
      "Pattern recognition (psychology)",
      "Transfer of learning",
      "Translation (biology)",
      "Viewpoints",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Hengheng"
      },
      {
        "surname": "Li",
        "given_name": "Ying"
      },
      {
        "surname": "Zhuang",
        "given_name": "Zijie"
      },
      {
        "surname": "Xie",
        "given_name": "Lingxi"
      },
      {
        "surname": "Tian",
        "given_name": "Qi"
      }
    ]
  },
  {
    "title": "Semi-supervised kernel matrix learning using adaptive constraint-based seed propagation",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107750",
    "abstract": "In this paper, we propose semi-supervised kernel matrix learning (SS-KML) using adaptive constraint-based seed propagation (ACSP). Conventional SS-KML methods such as pairwise constraint propagation (PCP) and kernel propagation (KP) have achieved outstanding performance in data classification. However, they are likely to distort the global data structure because of using hard constraints in their semi-definite problems (SDPs) for constraint propagation. Moreover, given a large number of pairwise constraints and a large amount of samples, they tend to be incredibly complex, thus being hard to be applied to real-life complex problems such as internet-scale image categorization. To address this problem, we utilize adaptive constraints to effectively maintain the inherent coherence of samples and successfully propagate constraint information into all samples. Moreover, we adopt seed propagation to remarkably reduce the computational complexity of SS-KML. Experimental results demonstrate that ACSP achieves a significant improvement in performance over PCP and KP in terms of both effectiveness and efficiency.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320305537",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Coherence (philosophical gambling strategy)",
      "Combinatorics",
      "Composite material",
      "Computational complexity theory",
      "Computer science",
      "Constraint (computer-aided design)",
      "Constraint satisfaction problem",
      "Geometry",
      "Kernel (algebra)",
      "Kernel method",
      "Local consistency",
      "Machine learning",
      "Materials science",
      "Mathematical optimization",
      "Mathematics",
      "Matrix (chemical analysis)",
      "Multiple kernel learning",
      "Pairwise comparison",
      "Pattern recognition (psychology)",
      "Probabilistic logic",
      "Statistics",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Jian",
        "given_name": "Meng"
      },
      {
        "surname": "Jung",
        "given_name": "Cheolkon"
      }
    ]
  },
  {
    "title": "Stable feature selection using copula based mutual information",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107697",
    "abstract": "Feature selection is a key step in many machine learning tasks. A majority of the existing methods of feature selection address the problem by devising some scoring function while treating the features independently, thereby overlooking their interdependencies. We leverage the scale invariance property of copula to construct a greedy, supervised feature selection algorithm that maximizes the feature relevance while minimizing the redundant information content. Multivariate copula is used in the proposed copula Based Feature Selection (CBFS) to discover the dependence structure between features. The incorporation of copula-based multivariate dependency in the formulation of mutual information helps avoid averaging over multiple instances of bivariate dependencies, thus eliminating the average estimation error introduced when bivariate dependency is used between a pair of feature variables. Under a controlled setting, our algorithm outperformed the existing best practice methods in warding off the noise in data. On several real and synthetic datasets, the proposed algorithm performed competitively in maximizing classification accuracy. CBFS also outperforms the other methods in terms of its noise tolerance property.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320305008",
    "keywords": [
      "Artificial intelligence",
      "Bivariate analysis",
      "Computer science",
      "Copula (linguistics)",
      "Data mining",
      "Econometrics",
      "Feature selection",
      "Leverage (statistics)",
      "Machine learning",
      "Mathematics",
      "Mutual information",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Lall",
        "given_name": "Snehalika"
      },
      {
        "surname": "Sinha",
        "given_name": "Debajyoti"
      },
      {
        "surname": "Ghosh",
        "given_name": "Abhik"
      },
      {
        "surname": "Sengupta",
        "given_name": "Debarka"
      },
      {
        "surname": "Bandyopadhyay",
        "given_name": "Sanghamitra"
      }
    ]
  },
  {
    "title": "Enhancing in-tree-based clustering via distance ensemble and kernelization",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107731",
    "abstract": "Recently, we have proposed a novel physically-inspired method, called the Nearest Descent (ND), which plays the role of organizing all the samples into an effective Graph, called the in-tree. Due to its effective characteristics, this in-tree proves very suitable for data clustering. Nevertheless, this in-tree-based clustering still has some non-trivial limitations in terms of robustness, capability, etc. In this study, we first propose a distance-ensemble-based framework for the in-tree-based clustering, which proves a very convenient way to overcome the robustness limitation in our previous in-tree-based clustering. To enhance the capability of the in-tree-based clustering in handling extremely linearly-inseparable clusters, we kernelize the proposed ensemble-based clustering via the so-called kernel trick. As a result, the improved in-tree-based clustering method achieves high robustness and accuracy on diverse challenging synthetic and real-world datasets, showing a certain degree of practical value.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320305343",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Cluster analysis",
      "Combinatorics",
      "Computer science",
      "Data mining",
      "Kernelization",
      "Mathematics",
      "Parameterized complexity",
      "Pattern recognition (psychology)",
      "Tree (set theory)"
    ],
    "authors": [
      {
        "surname": "Qiu",
        "given_name": "Teng"
      },
      {
        "surname": "Li",
        "given_name": "Yongjie"
      }
    ]
  },
  {
    "title": "Learning to transfer focus of graph neural network for scene graph parsing",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107707",
    "abstract": "Scene graph parsing has become a new challenge in the field of image understanding and pattern recognition in recent years. It captures objects and their relationships, and provides a structured representation of the visual scene. Among the three types of high-level relationships of scene graphs, semantic relationships, which contain the global understanding of the scene, are the core and the most valuable, while geometric and possessive relationships contain local and limited information. However, semantic relationships have the characteristics of multiple types and fewer instances, leading to a low recognition rate of most semantic relationships by existing detectors. To address this issue, this paper proposes a new architecture, the graphical focal network, which uses a decision-level global detector to capture the dependencies between object and relationship local detectors. We construct a graphical focal loss, which overcomes the lack of semantic relationship instances by adjusting the proportion of relationship loss based on the degree of relationship rarity and learning difficulty, and improves the stability of key object recognition by adjusting the proportion of object loss based on the degree of node connectivity and the value of neighborhood relationships. The proposed relative depth encoding module and regional layout encoding module, respectively, introduce relative depth information and more effective geometric layout information between objects, thereby further improving the performance. Experiments using the Visual Genome benchmark show that our method outperforms the most advanced competitors in two types of performance metrics. For semantic types, the recognition rate of our method is 2.0 times that of the baseline.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320305100",
    "keywords": [
      "Artificial intelligence",
      "Cognitive neuroscience of visual object recognition",
      "Computer science",
      "Graph",
      "Machine learning",
      "Object (grammar)",
      "Parsing",
      "Pattern recognition (psychology)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Jiang",
        "given_name": "Junjie"
      },
      {
        "surname": "He",
        "given_name": "Zaixing"
      },
      {
        "surname": "Zhang",
        "given_name": "Shuyou"
      },
      {
        "surname": "Zhao",
        "given_name": "Xinyue"
      },
      {
        "surname": "Tan",
        "given_name": "Jianrong"
      }
    ]
  },
  {
    "title": "Connectivity-based convolutional neural network for classifying point clouds",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107708",
    "abstract": "The acquisition of point clouds with a 3D scanner often yields large-scale, irregular, and unordered raw data, which hinders the classification of objects from these data. Some studies have introduced a method of applying the point clouds to convolutional neural networks (CNNs). This is achieved after preprocessing the volume metrics or multi-view images. However, this method has a limited resolution and a low classification accuracy in comparison to heavy computation in object classification. In this paper, DenX-Conv is proposed to improve the accuracy of object classification while securing the connectivity of points from the raw point cloud. DenX-Conv can extract effective local geometric features by finding the neighbor connectivity based on the geometric topology information of the points. In addition, stable feature learning is made possible by applying a densely connected network to PointCNN's χ-Conv. Application of DenX-Conv to the ModelNet40 dataset resulted in a classification accuracy of 92.5%.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320305112",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Cognitive neuroscience of visual object recognition",
      "Computation",
      "Computer science",
      "Convolutional neural network",
      "Data mining",
      "Feature (linguistics)",
      "Linguistics",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Point cloud",
      "Preprocessor"
    ],
    "authors": [
      {
        "surname": "Lee",
        "given_name": "Jinwon"
      },
      {
        "surname": "Cheon",
        "given_name": "Sang-Uk"
      },
      {
        "surname": "Yang",
        "given_name": "Jeongsam"
      }
    ]
  },
  {
    "title": "Analyzing and visualizing scientific research collaboration network with core node evaluation and community detection based on network embedding",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.01.007",
    "abstract": "With the increasing complexity of scientific research, it has gradually turned to a collaborative approach, which can promote knowledge sharing, resource sharing and improve the efficiency of scientific research achievements. Therefore, It is of great significance to study the internal organizational structure and evolution mechanism of scientific research collaboration, which plays a crucial role in the management of scientific research work and the formulation of scientific and technological policies. This paper focuses on three aspects: core node evaluation, community detection and visual layout algorithm of scientific research collaboration network, which is constructed based on the network embedding of the scientific research achievements’ attributes. Considering network topology and node heterogeneity, a core node evaluation method is proposed, and a community detection algorithm and a visual layout algorithm is improved to display the community structure of scientific research collaboration network from many aspects. The experimental results show that the proposed method can more clearly show the internal structure of scientific research collaboration community.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521000180",
    "keywords": [
      "Artificial intelligence",
      "Computer network",
      "Computer science",
      "Core (optical fiber)",
      "Data mining",
      "Data science",
      "Embedding",
      "Engineering",
      "Knowledge management",
      "Node (physics)",
      "Resource (disambiguation)",
      "Shared resource",
      "Structural engineering",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Wenbin"
      },
      {
        "surname": "Luo",
        "given_name": "Jishuang"
      },
      {
        "surname": "Fan",
        "given_name": "Tongrang"
      },
      {
        "surname": "Ren",
        "given_name": "Yan"
      },
      {
        "surname": "Xia",
        "given_name": "Yukun"
      }
    ]
  },
  {
    "title": "A multi-scale descriptor for real time RGB-D hand gesture recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2020.11.011",
    "abstract": "The development of depth cameras, e.g., the Kinect sensor, provides new opportunities for human computer interaction (HCI). Although the Kinect sensor has been extensively applied for human tracking, human action recognition and hand gesture recognition, real time hand gesture recognition is still a challenging problem. In this paper, a new real time hand gesture recognition method is proposed. Since fingers are the most important clue for hand gesture classification, a finger-emphasized multi-scale descriptor is proposed. The proposed descriptor incorporates three types of parameters of multiple scales to make a discriminative representation of the hand shape. Furthermore, the features of fingers are emphasized for hand gesture analysis. Three solutions to hand gesture recognition are then investigated with DTW, SVM, and neural network. Extensive experiments are conducted and the results show that the proposed method is robust to noise, articulations and rigid transformations. The comparison with state-of-the-art methods verifies the accuracy and efficiency of our method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520304165",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Discriminative model",
      "Gesture",
      "Gesture recognition",
      "Image (mathematics)",
      "Law",
      "Noise (video)",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "RGB color model",
      "Representation (politics)",
      "Speech recognition",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Yao"
      },
      {
        "surname": "Yang",
        "given_name": "Jianyu"
      }
    ]
  },
  {
    "title": "EF-Net: A novel enhancement and fusion network for RGB-D saliency detection",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107740",
    "abstract": "Salient object detection (SOD) has gained tremendous attention in the field of computer vision. Multi-modal SOD based on the complementary information from RGB images and depth maps has shown remarkable success, making RGB-D saliency detection an active research topic. In this paper, we propose a novel multi-modal enhancement and fusion network (EF-Net) for effective RGB-D saliency detection. Specifically, we first utilize a color hint map module with RGB images to predict a hint map, which encodes the coarse information of salient objects. The resulting hint map is then utilized to enhance the depth map with our depth enhancement module, which suppresses the noise and sharpens the object boundary. Finally, we propose an effective layer-wise aggregation module to fuse the features extracted from the enhanced depth maps and RGB images for the accurate detection of salient objects. Our EF-Net utilizes an enhancement-and-fusion framework for saliency detection, which makes full use of the information from RGB images and depth maps. In addition, our depth enhancement module effectively resolves the low-quality issue of depth maps, which boosts the saliency detection performance remarkably. Extensive experiments on five widely-used benchmark datasets demonstrate that our method outperforms 12 state-of-the-art RGB-D saliency detection approaches in terms of five key evaluation metrics.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320305434",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Computer vision",
      "Depth map",
      "Electrical engineering",
      "Engineering",
      "Fuse (electrical)",
      "Fusion",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Linguistics",
      "Object detection",
      "Pattern recognition (psychology)",
      "Philosophy",
      "RGB color model",
      "Salient"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Qian"
      },
      {
        "surname": "Fu",
        "given_name": "Keren"
      },
      {
        "surname": "Liu",
        "given_name": "Ze"
      },
      {
        "surname": "Chen",
        "given_name": "Geng"
      },
      {
        "surname": "Du",
        "given_name": "Hongwei"
      },
      {
        "surname": "Qiu",
        "given_name": "Bensheng"
      },
      {
        "surname": "Shao",
        "given_name": "Ling"
      }
    ]
  },
  {
    "title": "Automated segmentation of the optic disc from fundus images using an asymmetric deep learning network",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107810",
    "abstract": "Accurate segmentation of the optic disc (OD) regions from colour fundus images is a critical procedure for computer-aided diagnosis of glaucoma. We present a novel deep learning network to automatically identify the OD regions. On the basis of the classical U-Net framework, we define a unique sub-network and a decoding convolutional block. The sub-network is used to preserve important textures and facilitate their detections, while the decoding block is used to improve the contrast of the regions-of-interest with their background. We integrate these two components into the classical U-Net framework to improve the accuracy and reliability of segmenting the OD regions depicted on colour fundus images. We train and evaluate the developed network using three publicly available datasets (i.e., MESSIDOR, ORIGA, and REFUGE). The results on an independent testing set (n = 1,970 images) show a segmentation performance with an average Dice similarity coefficient (DSC), intersection over union (IOU), and Matthew's correlation coefficient (MCC) of 0.9377, 0.8854, and 0.9383 when trained on the global field-of-view images, respectively, and 0.9735, 0.9494, and 0.9594 when trained on the local disc region images. When compared with the other three classical networks (i.e., the U-Net, M-Net, and Deeplabv3) on the same testing datasets, the developed network demonstrates a relatively higher performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320306130",
    "keywords": [
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Cartography",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Fundus (uterus)",
      "Geography",
      "Geometry",
      "Image (mathematics)",
      "Image segmentation",
      "Intersection (aeronautics)",
      "Mathematics",
      "Medicine",
      "Ophthalmology",
      "Pattern recognition (psychology)",
      "Segmentation",
      "Similarity (geometry)",
      "Sørensen–Dice coefficient"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Lei"
      },
      {
        "surname": "Gu",
        "given_name": "Juan"
      },
      {
        "surname": "Chen",
        "given_name": "Yize"
      },
      {
        "surname": "Liang",
        "given_name": "Yuanbo"
      },
      {
        "surname": "Zhang",
        "given_name": "Weijie"
      },
      {
        "surname": "Pu",
        "given_name": "Jiantao"
      },
      {
        "surname": "Chen",
        "given_name": "Hao"
      }
    ]
  },
  {
    "title": "A Master Key backdoor for universal impersonation attack against DNN-based face verification",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.01.009",
    "abstract": "We introduce a new attack against face verification systems based on Deep Neural Networks (DNN). The attack relies on the introduction into the network of a hidden backdoor, whose activation at test time induces a verification error allowing the attacker to impersonate any user. The new attack, named Master Key backdoor attack, operates by interfering with the training phase, so to instruct the DNN to always output a positive verification answer when the face of the attacker is presented at its input. With respect to existing attacks, the new backdoor attack offers much more flexibility, since the attacker does not need to know the identity of the victim beforehand. In this way, he can deploy a Universal Impersonation attack in an open-set framework, allowing him to impersonate any enrolled users, even those that were not yet enrolled in the system when the attack was conceived. We present a practical implementation of the attack targeting a Siamese-DNN face verification system, and show its effectiveness when the system is trained on VGGFace2 dataset and tested on LFW and YTF datasets. According to our experiments, the Master Key backdoor attack provides a high attack success rate even when the ratio of poisoned training data is as small as 0.01, thus raising a new alarm regarding the use of DNN-based face verification systems in security-critical applications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521000210",
    "keywords": [
      "Artificial intelligence",
      "Backdoor",
      "Computer science",
      "Computer security",
      "Face (sociological concept)",
      "Flexibility (engineering)",
      "Key (lock)",
      "Mathematics",
      "Social science",
      "Sociology",
      "Statistics",
      "Trojan"
    ],
    "authors": [
      {
        "surname": "Guo",
        "given_name": "Wei"
      },
      {
        "surname": "Tondi",
        "given_name": "Benedetta"
      },
      {
        "surname": "Barni",
        "given_name": "Mauro"
      }
    ]
  },
  {
    "title": "Infrared small target detection via adaptive M-estimator ring top-hat transformation",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107729",
    "abstract": "Top-Hat transformation is an essential technology in the field of infrared small target detection. Many modified Top-Hat transformation methods have been proposed based on the different structure of structural elements. However, these methods are still hard to handle the dim targets and complex background. It can be summarized as two reasons, one is that the structural elements cannot suppress the background adaptively due to the fixed value of structural elements in image. Another is that simple structural element cannot utilize the local feature for target enhancement. To overcome these two limitations, a special ring Top-Hat transformation based on M-estimator and local entropy is proposed in this paper. First, an adaptive ring structural element based on M-estimator is used to suppress the complex background. Second, a novel local entropy is proposed to weight structural element for capturing local feature and target enhancement. Finally, a comparison experiment based on massive infrared image data (more than 500 infrared target images) is done. And the results demonstrate that the proposed algorithm acquires better performance compared with some recent methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032030532X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Entropy (arrow of time)",
      "Estimator",
      "Feature (linguistics)",
      "Gene",
      "Infrared",
      "Linguistics",
      "Mathematics",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Statistics",
      "Transformation (genetics)"
    ],
    "authors": [
      {
        "surname": "Deng",
        "given_name": "Lizhen"
      },
      {
        "surname": "Zhang",
        "given_name": "Jieke"
      },
      {
        "surname": "Xu",
        "given_name": "Guoxia"
      },
      {
        "surname": "Zhu",
        "given_name": "Hu"
      }
    ]
  },
  {
    "title": "Temperature network for few-shot learning with distribution-aware large-margin metric",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107797",
    "abstract": "Few-shot learning learns to classify unseen data with few training samples in hand and has attracted increasing attentions recently. In this paper, we propose a novel Temperature Network to tackle few-shot learning tasks motivated by three crucial factors that are seldom considered in the existing literature. First, to encourage compact intra-class distribution, a general improvement for prototype-based methods is proposed to ensure compact intra-class distribution and the effectiveness is theoretically and experimentally validated. Second, the proposed Temperature Network can implicitly generate query-specific prototypes and thus enjoys a more effective distribution-aware metric. Third, to further strengthen the generalization ability of the proposed model, a novel and simple large-margin based method is developed by leveraging the temperature function and we gradually tune the learning temperature to stabilize the training process. Moreover, we note that the commonly used datasets in few-shot learning are actually contrived from large-scale datasets, and thus may not represent a real few-shot problem. We propose a real-life few shot problem, i.e., Dermnet skin disease, to comprehensively evaluate the performance of few-shot learning methods. Experiments conducted on conventional datasets as well as the proposed skin disease dataset demonstrate the superiority of the proposed method over other state-of-the-art methods. The source code of our method is available. 1 1 https://github.com/zwvews/TemperatureNetwork.git",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320306002",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Chemistry",
      "Class (philosophy)",
      "Computer science",
      "Data mining",
      "Economics",
      "Epistemology",
      "Evolutionary biology",
      "Function (biology)",
      "Generalization",
      "Machine learning",
      "Margin (machine learning)",
      "Mathematical analysis",
      "Mathematics",
      "Metric (unit)",
      "Operating system",
      "Operations management",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Process (computing)",
      "Shot (pellet)",
      "Simple (philosophy)"
    ],
    "authors": [
      {
        "surname": "Zhu",
        "given_name": "Wei"
      },
      {
        "surname": "Li",
        "given_name": "Wenbin"
      },
      {
        "surname": "Liao",
        "given_name": "Haofu"
      },
      {
        "surname": "Luo",
        "given_name": "Jiebo"
      }
    ]
  },
  {
    "title": "Siamese networks with distractor-reduction method for long-term visual object tracking",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107698",
    "abstract": "Many trackers which divide the tracking process into two stages have recently been proposed to solve the problem of long-term tracking. Their outstanding performance makes them become one of the mainstream algorithms of long-term tracking. To further improve the performance of two-stage tracking algorithms, some improvements are proposed in this paper. (a) A hard negative mining method is proposed. It can optimize the training process of the verification network and bridge the gap between the two sub-networks. (b) The architecture of the verification network is designed as a Siamese structure; therefore, the semantic ambiguity in classification can be alleviated. Extensive experiments performed on benchmarks demonstrate that the proposed approach significantly outperforms the state-of-the-art methods, yielding 7% relative gain in the VOT2018-LT dataset and 14.2% relative gain in the OxUvA dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032030501X",
    "keywords": [
      "Ambiguity",
      "Artificial intelligence",
      "BitTorrent tracker",
      "Bridge (graph theory)",
      "Computer science",
      "Computer vision",
      "Eye tracking",
      "Geometry",
      "Internal medicine",
      "Machine learning",
      "Mathematics",
      "Medicine",
      "Object (grammar)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Pedagogy",
      "Physics",
      "Process (computing)",
      "Programming language",
      "Psychology",
      "Quantum mechanics",
      "Reduction (mathematics)",
      "Term (time)",
      "Tracking (education)",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Xuan",
        "given_name": "Shiyu"
      },
      {
        "surname": "Li",
        "given_name": "Shengyang"
      },
      {
        "surname": "Zhao",
        "given_name": "Zifei"
      },
      {
        "surname": "Kou",
        "given_name": "Longxuan"
      },
      {
        "surname": "Zhou",
        "given_name": "Zhuang"
      },
      {
        "surname": "Xia",
        "given_name": "Gui-Song"
      }
    ]
  },
  {
    "title": "ROSNet: Robust one-stage network for CT lesion detection",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.01.011",
    "abstract": "Automatic lesion detection from computed tomography (CT) scans is an important task in medical diagnosis. However, three frequent properties of medical data make CT lesion detection a challenging task: (1) Scale variance: Large scale variation is across lesion instances. Especially, it is extremely difficult to detect small lesions; (2) Imbalanced data: The data distributions are highly imbalanced, where few classes account for the majority of data; (3) Prediction stability: Based on our observations, an input lesion image with slightly pixel shift or translation can lead to drastic output mispredictions and this is not allowed for medical applications. To address these challenges, this paper proposes a Robust One-Stage Network (ROSNet) for robust CT lesion detection. Specifically, a novel nested structure of neural networks is developed to generate a series of feature pyramids for detecting CT lesions in various scales, an effective data sensitive class-balanced loss as well as a shift-invariant downsampling strategy are also introduced to improve the detection performance. Experiments are conducted on a large-scale and diverse dataset, DeepLesion, showing that ROSNet outperforms the best performance in MICCAI 2019 by 3.95% (2-class detection task) and 25.41% (8-class detection task) in terms of mean average precision (mAP).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521000246",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Image (mathematics)",
      "Lesion",
      "Medicine",
      "Pattern recognition (psychology)",
      "Psychiatry",
      "Upsampling"
    ],
    "authors": [
      {
        "surname": "Lung",
        "given_name": "Kuan-Yu"
      },
      {
        "surname": "Chang",
        "given_name": "Chi-Rung"
      },
      {
        "surname": "Weng",
        "given_name": "Shao-En"
      },
      {
        "surname": "Lin",
        "given_name": "Hao-Siang"
      },
      {
        "surname": "Shuai",
        "given_name": "Hong-Han"
      },
      {
        "surname": "Cheng",
        "given_name": "Wen-Huang"
      }
    ]
  },
  {
    "title": "MobileGCN applied to low-dimensional node feature learning",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107788",
    "abstract": "The idea of the paper concentrates on an iterative learning process in Graph Convolution Networks (GCNs) involved in two vital steps: one is a message propagation (message passing) step to aggregate neighboring node features via aggregators performed, and another is an encoding output step to encode node feature representations by using updaters. In our model, we propose a novel affinity-aware encoding as an updater in GCNs, which aggregates the neighboring nodes of a node while updating this node’s features. By utilizing affinity values of our encoding, we order the neighboring nodes to determine the correspondence between encoding functions and the neighboring nodes. Furthermore, to explicitly reduce the model size, we propose a lightweight variant of our updater that integrates Depth-wise Separable Convolution (DSC) into it, namely Depth-wise Separable Graph Convolution (DSGC). Comprehensive experiments conducted on graph data demonstrate that our models’ accuracy improved significantly for graphs of low-dimensional node features. Also, performed in the low-dimensional node feature space we provide state-of-the-art results on two metrics (Macro-f1 and Matthews correlation coefficient (MCC)). Besides, our models are robust when taking different low-dimensional feature selection strategies.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320305914",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Convolution (computer science)",
      "ENCODE",
      "Encoding (memory)",
      "Engineering",
      "Feature (linguistics)",
      "Feature learning",
      "Feature vector",
      "Gene",
      "Graph",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Node (physics)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Separable space",
      "Structural engineering",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Dong",
        "given_name": "Wei"
      },
      {
        "surname": "Wu",
        "given_name": "Junsheng"
      },
      {
        "surname": "Bai",
        "given_name": "Zongwen"
      },
      {
        "surname": "Hu",
        "given_name": "Yaoqi"
      },
      {
        "surname": "Li",
        "given_name": "Weigang"
      },
      {
        "surname": "Qiao",
        "given_name": "Wei"
      },
      {
        "surname": "Woźniak",
        "given_name": "Marcin"
      }
    ]
  },
  {
    "title": "Variational posterior approximation using stochastic gradient ascent with adaptive stepsize",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107783",
    "abstract": "Scalable algorithms of variational posterior approximation allow Bayesian nonparametrics such as Dirichlet process mixture to scale up to larger dataset at fractional cost. Recent algorithms, notably the stochastic variational inference performs local learning from minibatch. The main problem with stochastic variational inference is that it relies on closed form solution. Stochastic gradient ascent is a modern approach to machine learning and is widely deployed in the training of deep neural networks. In this work, we explore using stochastic gradient ascent as a fast algorithm for the posterior approximation of Dirichlet process mixture. However, stochastic gradient ascent alone is not optimal for learning. In order to achieve both speed and performance, we turn our focus to stepsize optimization in stochastic gradient ascent. As as intermediate approach, we first optimize stepsize using the momentum method. Finally, we introduce Fisher information to allow adaptive stepsize in our posterior approximation. In the experiments, we justify that our approach using stochastic gradient ascent do not sacrifice performance for speed when compared to closed form coordinate ascent learning on these datasets. Lastly, our approach is also compatible with deep ConvNet features as well as scalable to large class datasets such as Caltech256 and SUN397.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320305860",
    "keywords": [
      "Algorithm",
      "Applied mathematics",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer security",
      "Database",
      "Deep learning",
      "Gradient descent",
      "Inference",
      "Key (lock)",
      "Mathematical optimization",
      "Mathematics",
      "Scalability",
      "Stochastic approximation",
      "Stochastic gradient descent",
      "Stochastic optimization"
    ],
    "authors": [
      {
        "surname": "Lim",
        "given_name": "Kart-Leong"
      },
      {
        "surname": "Jiang",
        "given_name": "Xudong"
      }
    ]
  },
  {
    "title": "Sparsely-labeled source assisted domain adaptation",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107803",
    "abstract": "Domain Adaptation (DA) aims to generalize the classifier learned from a well-labeled source domain to an unlabeled target domain. Existing DA methods usually assume that rich labels could be available in the source domain. However, we usually confront the source domain with a large number of unlabeled data but only a few labeled data, and thus, how to transfer knowledge from this sparsely-labeled source domain to the target domain is still a challenge, which greatly limits its application in the wild. This paper proposes a novel Sparsely-Labeled Source Assisted Domain Adaptation (SLSA-DA) algorithm to address the challenge with limited labeled source domain samples. Specifically, due to the label scarcity problem, the projected clustering is first conducted on both the source and target domains, so that the discriminative structures of data could be exploited elegantly. Then label propagation is adopted to propagate the labels from those limited labeled source samples to the whole unlabeled data progressively, so that the cluster labels are revealed correctly. Finally, we jointly align the marginal and conditional distributions to mitigate the cross-domain mismatching problem, and optimize those three procedures iteratively. However, it is nontrivial to incorporate the above three procedures into a unified optimization framework seamlessly since some variables to be optimized are implicitly involved in their formulas, thus they could not benefit to each other. Remarkably, we prove that the projected clustering and conditional distribution alignment could be reformulated into other formulations, thus the implicit variables are embedded in different optimization steps. As such, the variables related to those three quantities could be optimized in a unified optimization framework and benefit to each other, and improve the recognition performance obviously. Extensive experiments have verified that our approach could deal with the challenge in the SLSA-DA setting, and achieve the best performances across different real-world cross-domain visual recognition tasks. Our preliminary Matlab code is available at https://github.com/WWLoveTransfer/SLSA-DA/.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320306063",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Classifier (UML)",
      "Cluster analysis",
      "Computer science",
      "Conditional probability distribution",
      "Convex combination",
      "Convex optimization",
      "Data mining",
      "Discriminative model",
      "Domain (mathematical analysis)",
      "Domain adaptation",
      "Effective domain",
      "Geometry",
      "Labeled data",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Multi-source",
      "Operating system",
      "Pattern recognition (psychology)",
      "Regular polygon",
      "Source code",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Wei"
      },
      {
        "surname": "Chen",
        "given_name": "Shenglun"
      },
      {
        "surname": "Xiang",
        "given_name": "Yuankai"
      },
      {
        "surname": "Sun",
        "given_name": "Jing"
      },
      {
        "surname": "Li",
        "given_name": "Haojie"
      },
      {
        "surname": "Wang",
        "given_name": "Zhihui"
      },
      {
        "surname": "Sun",
        "given_name": "Fuming"
      },
      {
        "surname": "Ding",
        "given_name": "Zhengming"
      },
      {
        "surname": "Li",
        "given_name": "Baopu"
      }
    ]
  },
  {
    "title": "Multiple discriminant analysis for collaborative representation-based classification",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107819",
    "abstract": "Collaborative Representation-based Classifier (CRC) has shown its advantages and impressive results in face recognition. To further imporve the performance of CRC, we propose a novel dimensionality reduction method termed Multiple Discriminant Analysis for Collaborative Representation-based Classification (MDA-CRC). Considering the labeling criterion of CRC is class-specific, MDA-CRC solves a group of binary classification problems where specific feature subspaces are learned for each class. In each binary classification problem, an orthogonal discriminant analysis method based on collaborative representation is adopted. Hence, MDA-CRC can improve the discriminant ability of collaborative representation and be consistent with the labeling criterion of CRC simultaneously. Further, the convergence of MDA-CRC is proven. Extensive experiments on several benchmark datasets demonstrate the effectiveness of MDA-CRC.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321000066",
    "keywords": [
      "Arithmetic",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Binary classification",
      "Binary number",
      "Class (philosophy)",
      "Classifier (UML)",
      "Computer science",
      "Dimensionality reduction",
      "Discriminant",
      "Geodesy",
      "Geography",
      "Law",
      "Linear discriminant analysis",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Zheng",
        "given_name": "Zhichao"
      },
      {
        "surname": "Sun",
        "given_name": "Huaijiang"
      },
      {
        "surname": "Zhou",
        "given_name": "Ying"
      }
    ]
  },
  {
    "title": "Imprecise Gaussian discriminant classification",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107739",
    "abstract": "Gaussian discriminant analysis is a popular classification model, that in the precise case can produce unreliable predictions in case of high uncertainty (e.g., due to scarce or noisy data). While imprecise probability theory offers a nice theoretical framework to solve such issues, it has not been yet applied to Gaussian discriminant analysis. This work remedies this, by proposing a new Gaussian discriminant analysis based on robust Bayesian analysis and near-ignorance priors. The model delivers cautiouspredictions, in form of set-valued class, in case of limited or imperfect available information. We present and discuss results of experimentation on real and synthetic datasets, where for this latter we corrupt the test instance to see how our approach reacts to non i.i.d. samples. Experiments show that including an imprecise component in the Gaussian discriminant analysis produces reasonably cautious predictions, and that set-valued predictions correspond to instances for which the precise model performs poorly.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320305422",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Discriminant",
      "Gaussian",
      "Linear discriminant analysis",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics"
    ],
    "authors": [
      {
        "surname": "Carranza Alarcón",
        "given_name": "Yonatan Carlos"
      },
      {
        "surname": "Destercke",
        "given_name": "Sébastien"
      }
    ]
  },
  {
    "title": "Heterogeneous ensemble selection for evolving data streams",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107743",
    "abstract": "Ensemble learning has been widely applied to both batch data classification and streaming data classification. For the latter setting, most existing ensemble systems are homogenous, which means they are generated from only one type of learning model. In contrast, by combining several types of different learning models, a heterogeneous ensemble system can achieve greater diversity among its members, which helps to improve its performance. Although heterogeneous ensemble systems have achieved many successes in the batch classification setting, it is not trivial to extend them directly to the data stream setting. In this study, we propose a novel HEterogeneous Ensemble Selection (HEES) method, which dynamically selects an appropriate subset of base classifiers to predict data under the stream setting. We are inspired by the observation that a well-chosen subset of good base classifiers may outperform the whole ensemble system. Here, we define a good candidate as one that expresses not only high predictive performance but also high confidence in its prediction. Our selection process is thus divided into two sub-processes: accurate-candidate selection and confident-candidate selection. We define an accurate candidate in the stream context as a base classifier with high accuracy over the current concept, while a confident candidate as one with a confidence score higher than a certain threshold. In the first sub-process, we employ the prequential accuracy to estimate the performance of a base classifier at a specific time, while in the latter sub-process, we propose a new measure to quantify the predictive confidence and provide a method to learn the threshold incrementally. The final ensemble is formed by taking the intersection of the sets of confident classifiers and accurate classifiers. Experiments on a wide range of data streams show that the proposed method achieves competitive performance with lower running time in comparison to the state-of-the-art online ensemble methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032030546X",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Classifier (UML)",
      "Computer science",
      "Concept drift",
      "Context (archaeology)",
      "Data mining",
      "Data stream",
      "Data stream mining",
      "Ensemble learning",
      "Machine learning",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Selection (genetic algorithm)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Luong",
        "given_name": "Anh Vu"
      },
      {
        "surname": "Nguyen",
        "given_name": "Tien Thanh"
      },
      {
        "surname": "Liew",
        "given_name": "Alan Wee-Chung"
      },
      {
        "surname": "Wang",
        "given_name": "Shilin"
      }
    ]
  },
  {
    "title": "Exploiting structured high-level knowledge for domain-specific visual classification",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107806",
    "abstract": "In the last decade, deep learning models have yielded impressive performance on visual object recognition and image classification. However these methods still rely on learning visual data distributions and show difficulties in dealing with complex scenarios where visual appearance only is not enough to effectively tackle them. This is the case, for instance, of fine-grained image classification in domain-specific applications for which it is very complex to employ data-driven models because of the lack of large amounts of samples and that, instead, can be solved by resorting to specialized human knowledge. However, encoding this specialized knowledge and injecting it into deep models is not trivial. In this paper, we address this problem by: a) employing computational ontologies to model specialized knowledge in a structured representation and, b) building a hybrid visual-semantic classification framework. The classification method performs inference over a Bayesian Network graph, whose structure depends on the knowledge encoded in an ontology and evidences are built using the outputs of deep networks. We test our approach on a fine-grained classification task, employing an extremely complex dataset containing images from several fruit varieties as well as visual and semantic annotations. Since the classification is done at the variety level (e.g., discriminating between different cherry varieties), appearance changes slightly and expert domain knowledge — making using of contextual information — is required to perform classification accurately. Experimental results show that our approach significantly outperforms standard deep learning–based classification methods over the considered scenario as well as existing methods leveraging semantic information for classification. These results demonstrate, on one hand, the difficulty of purely-visual deep methods in tackling small and highly-specialized datasets and, on the other hard, the capabilities of our approach to effectively encode and use semantic knowledge for enhanced accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320306099",
    "keywords": [
      "Artificial intelligence",
      "Bayesian network",
      "Computer science",
      "Contextual image classification",
      "Deep learning",
      "Domain (mathematical analysis)",
      "Domain knowledge",
      "Economics",
      "Epistemology",
      "Image (mathematics)",
      "Inference",
      "Machine learning",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Multi-label classification",
      "Ontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Palazzo",
        "given_name": "S."
      },
      {
        "surname": "Murabito",
        "given_name": "F."
      },
      {
        "surname": "Pino",
        "given_name": "C."
      },
      {
        "surname": "Rundo",
        "given_name": "F."
      },
      {
        "surname": "Giordano",
        "given_name": "D."
      },
      {
        "surname": "Shah",
        "given_name": "M."
      },
      {
        "surname": "Spampinato",
        "given_name": "C."
      }
    ]
  },
  {
    "title": "Compact class-conditional domain invariant learning for multi-class domain adaptation",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107763",
    "abstract": "Neural network-based models have recently shown excellent performance in various kinds of tasks. However, a large amount of labeled data is required to train deep networks, and the cost of gathering labeled training data for every kind of domain is prohibitively expensive. Domain adaptation tries to solve this problem by transferring knowledge from labeled source domain data to unlabeled target domain data. Previous research tried to learn domain-invariant features of source and target domains to address this problem, and this approach has been used as a key concept in various methods. However, domain-invariant features do not mean that a classifier trained on source data can be directly applied to target data because it does not guarantee that data distribution of the same classes will be aligned across two domains. In this paper, we present novel generalization upper bounds for domain adaptation that motivates the need for class-conditional domain invariant learning. Based on this theoretical framework, we then propose a class-conditional domain invariant learning method that can learn a feature space in which features in the same class are expected to be mapped nearby. We empirically experimented that our model showed state-of-the-art performance on standard datasets and showed effectiveness by visualization of latent space.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320305665",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Class (philosophy)",
      "Classifier (UML)",
      "Computer science",
      "Conditional probability distribution",
      "Domain (mathematical analysis)",
      "Domain adaptation",
      "Invariant (physics)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematical physics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Lee",
        "given_name": "Woojin"
      },
      {
        "surname": "Kim",
        "given_name": "Hoki"
      },
      {
        "surname": "Lee",
        "given_name": "Jaewook"
      }
    ]
  },
  {
    "title": "Limitation of capsule networks",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.01.017",
    "abstract": "A recently proposed method in deep learning groups multiple neurons to capsules such that each capsule represents an object or part of an object. Routing algorithms route the output of capsules from lower-level layers to upper-level layers. In this paper, we prove that state-of-the-art routing procedures decrease the expressivity of capsule networks. More precisely, it is shown that EM-routing and routing-by-agreement prevent capsule networks from distinguishing inputs and their negative counterpart. Therefore, only symmetric functions can be expressed by capsule networks, and it can be concluded that they are not universal approximators. We also theoretically motivate and empirically show that this limitation affects the training of deep capsule networks negatively. Therefore, we present an incremental improvement for state-of-the-art routing algorithms that solves the aforementioned limitation and stabilizes the training of capsule networks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521000301",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Capsule",
      "Combinatorics",
      "Computer network",
      "Computer science",
      "Geology",
      "Mathematics",
      "Object (grammar)",
      "Paleontology",
      "Routing (electronic design automation)",
      "Routing algorithm",
      "Routing protocol",
      "State (computer science)",
      "Topology (electrical circuits)"
    ],
    "authors": [
      {
        "surname": "Peer",
        "given_name": "David"
      },
      {
        "surname": "Stabinger",
        "given_name": "Sebastian"
      },
      {
        "surname": "Rodríguez-Sánchez",
        "given_name": "Antonio"
      }
    ]
  },
  {
    "title": "Unsupervised deep hashing with node representation for image retrieval",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107785",
    "abstract": "Supervised graph convolution network (GCN) based hashing algorithms have achieved good results by recognizing images according to the relationships between objects, but they are hard to be applied to label-free scenarios. Besides, most existing unsupervised deep hashing algorithms neglect the relationships between different samples and thus fail to achieve high precision. To address this problem, we propose NRDH, an unsupervised D eep H ashing method with N ode R epresentation for image retrieval, which adopts unsupervised GCN to integrate the relationships between samples into image visual features. NRDH consists of node representation learning stage and hash function learning stage. In the first stage, we treat each image as a node of a graph and design GCN-based AutoEncoder, which can integrate the relationships between samples into node representation. In the second stage, we use above node representations to guide the network and help learn the hash function to fast achieve an end-to-end hash model to generate semantic hash codes. Extensive experiments on CIFAR-10, MS-COCO and FLICKR25K show NRDH can achieve higher performance and outperform the state-of-the-art unsupervised deep hashing methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320305884",
    "keywords": [
      "Artificial intelligence",
      "Autoencoder",
      "Computer science",
      "Computer security",
      "Convolutional neural network",
      "Deep learning",
      "Double hashing",
      "Engineering",
      "Feature hashing",
      "Feature learning",
      "Graph",
      "Hash function",
      "Hash table",
      "Image (mathematics)",
      "Image retrieval",
      "Law",
      "Node (physics)",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Structural engineering",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Yangtao"
      },
      {
        "surname": "Song",
        "given_name": "Jingkuan"
      },
      {
        "surname": "Zhou",
        "given_name": "Ke"
      },
      {
        "surname": "Liu",
        "given_name": "Yu"
      }
    ]
  },
  {
    "title": "Training deep retrieval models with noisy datasets: Bag exponential loss",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107811",
    "abstract": "Although the CNNs are a very powerful tool for image retrieval, the need of training datasets properly adapted to the application at hand hinders the usefulness of such networks, specially since the datasets need to be free of noise to avoid spoiling the learning process. An ad hoc preprocessing of the dataset to mitigate the noise is a possible solution, but it is usually non-trivial and requires significant human intervention. In this paper, we pave the road for training CNNs for image retrieval with noisy datasets. In particular, we propose a novel Bag Exponential Loss function that, inspired by the Multiple Instance Learning framework, works with bags of matching images instead of single pairs, and allows a dynamical weighting of the relevance of each sample as the training progresses. The formulation of the proposed model is general enough and may serve to other purposes than dealing with noise if parameters are chosen appropriately. Extensive experimental results show the superior performance of the proposed loss with respect to the current state-of-the-art as well as its ability to cope with noisy training sets. Pytorch code available in https://github.com/tmcortes/BELoss",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320306142",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Code (set theory)",
      "Computer science",
      "Data mining",
      "Deep learning",
      "Image (mathematics)",
      "Image retrieval",
      "Law",
      "Machine learning",
      "Medicine",
      "Noise (video)",
      "Operating system",
      "Overfitting",
      "Pattern recognition (psychology)",
      "Political science",
      "Preprocessor",
      "Process (computing)",
      "Programming language",
      "Radiology",
      "Relevance (law)",
      "Set (abstract data type)",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Martínez-Cortés",
        "given_name": "Tomás"
      },
      {
        "surname": "González-Díaz",
        "given_name": "Iván"
      },
      {
        "surname": "Díaz-de-María",
        "given_name": "Fernando"
      }
    ]
  },
  {
    "title": "RETRACTED: Deep learning for real-time semantic segmentation: Application in ultrasound imaging",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.01.010",
    "abstract": "This article has been retracted: please see Elsevier Policy on Article Withdrawal (https://www.elsevier.com/about/our-business/policies/article-withdrawal). This article has been retracted at the request of the Editors-in-Chief after an investigation. The authors have plagiarized part of a paper “Fully Convolutional Dense Dilated Net for Real-Time Semantic Segmentation of Thyroid Nodules” by Xuewei Li, Shuaijie Wang, Xi Wei, Mei Yu, Jian Yu, Jialin Zhu, Han Jiang, Zhiqiang Liu, Jie Gao and Ruiguo Yu that Abdeldjalil Ouahabi had received as a reviewer for another journal. One of the conditions of submission of a paper for publication is that authors declare explicitly that their work is original. Re-use of any data should be appropriately declared at the very outset. As such this article represents a severe abuse of the scientific publishing system. The scientific community takes a very strong view on this matter and apologies are offered to readers of the journal that this was not detected during the submission process.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521000234",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Filter (signal processing)",
      "Gene",
      "Image segmentation",
      "Pattern recognition (psychology)",
      "Robustness (evolution)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Ouahabi",
        "given_name": "Abdeldjalil"
      },
      {
        "surname": "Taleb-Ahmed",
        "given_name": "Abdelmalik"
      }
    ]
  },
  {
    "title": "Exploring touch-based behavioral authentication on smartphone email applications in IoT-enabled smart cities",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.01.019",
    "abstract": "The Internet of Things (IoT) allows various embedded devices and smart sensors to be connected with each other, which provides a basis for building smart cities. The IoT-enabled smart city can greatly benefit people’s daily lives, where smartphone is one of the most widely used IoT devices. For example, people can use the phone to check their financial account, store personal data and communicate with peers. Thus it is very important to safeguard the phones from unauthorized access. To complement traditional textual passwords, touch behavioral authentication has attracted much attention while it is still a challenge on how to build a robust scheme in practice. This is because users’ touch actions are often dynamic and hard to model. For this challenge, previous work has proved that touch actions could become consistent when users interact with social networking applications. Motivated by this observation, in this work, we perform a study to investigate users’ touch behavior within Email applications on smartphones (with Email being one of the most important and widely used means in connecting with others). The study results with 60 participants validate the former observation that users’ touch behavioral deviation can be greatly decreased when they play Email applications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521000325",
    "keywords": [
      "Authentication (law)",
      "Computer science",
      "Computer security",
      "Human–computer interaction",
      "Internet of Things",
      "Internet privacy",
      "Multimedia",
      "Smartphone app"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Wenjuan"
      },
      {
        "surname": "Meng",
        "given_name": "Weizhi"
      },
      {
        "surname": "Furnell",
        "given_name": "Steven"
      }
    ]
  },
  {
    "title": "Scale variance minimization for unsupervised domain adaptation in image segmentation",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107764",
    "abstract": "We focus on unsupervised domain adaptation (UDA) in image segmentation. Existing works address this challenge largely by aligning inter-domain representations, which may lead over-alignment that impairs the semantic structures of images and further target-domain segmentation performance. We design a scale variance minimization (SVMin) method by enforcing the intra-image semantic structure consistency in the target domain. Specifically, SVMin leverages an intrinsic property that simple scale transformation has little effect on the semantic structures of images. It thus introduces certain supervision in the target domain by imposing a scale-invariance constraint while learning to segment an image and its scale-transformation concurrently. Additionally, SVMin is complementary to most existing UDA techniques and can be easily incorporated with consistent performance boost but little extra parameters. Extensive experiments show that our method achieves superior domain adaptive segmentation performance as compared with the state-of-the-art. Preliminary studies show that SVMin can be easily adapted for UDA-based image classification.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320305677",
    "keywords": [
      "Accounting",
      "Artificial intelligence",
      "Biochemistry",
      "Business",
      "Chemistry",
      "Classifier (UML)",
      "Computer science",
      "Computer vision",
      "Consistency (knowledge bases)",
      "Constraint (computer-aided design)",
      "Domain (mathematical analysis)",
      "Domain adaptation",
      "Epistemology",
      "Gene",
      "Geometry",
      "Image (mathematics)",
      "Image segmentation",
      "Mathematical analysis",
      "Mathematics",
      "Minification",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Programming language",
      "Property (philosophy)",
      "Quantum mechanics",
      "Scale (ratio)",
      "Segmentation",
      "Transformation (genetics)",
      "Variance (accounting)"
    ],
    "authors": [
      {
        "surname": "Guan",
        "given_name": "Dayan"
      },
      {
        "surname": "Huang",
        "given_name": "Jiaxing"
      },
      {
        "surname": "Lu",
        "given_name": "Shijian"
      },
      {
        "surname": "Xiao",
        "given_name": "Aoran"
      }
    ]
  },
  {
    "title": "Cross-modal discriminant adversarial network",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107734",
    "abstract": "Cross-modal retrieval aims at retrieving relevant points across different modalities, such as retrieving images via texts. One key challenge of cross-modal retrieval is narrowing the heterogeneous gap across diverse modalities. To overcome this challenge, we propose a novel method termed as Cross-modal discriminant Adversarial Network (CAN). Taking bi-modal data as a showcase, CAN consists of two parallel modality-specific generators, two modality-specific discriminators, and a Cross-modal Discriminant Mechanism (CDM). To be specific, the generators project diverse modalities into a latent cross-modal discriminant space. Meanwhile, the discriminators compete against the generators to alleviate the heterogeneous discrepancy in this space, i.e., the generators try to generate unified features to confuse the discriminators, and the discriminators aim to classify the generated results. To further remove the redundancy and preserve the discrimination, we propose CDM to project the generated results into a single common space, accompanying with a novel eigenvalue-based loss. Thanks to the eigenvalue-based loss, CDM could push as much discriminative power as possible into all latent directions. To demonstrate the effectiveness of our CAN, comprehensive experiments are conducted on four multimedia datasets comparing with 15 state-of-the-art approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320305379",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Discriminant",
      "Linear discriminant analysis",
      "Mathematics",
      "Modal",
      "Pattern recognition (psychology)",
      "Polymer chemistry"
    ],
    "authors": [
      {
        "surname": "Hu",
        "given_name": "Peng"
      },
      {
        "surname": "Peng",
        "given_name": "Xi"
      },
      {
        "surname": "Zhu",
        "given_name": "Hongyuan"
      },
      {
        "surname": "Lin",
        "given_name": "Jie"
      },
      {
        "surname": "Zhen",
        "given_name": "Liangli"
      },
      {
        "surname": "Wang",
        "given_name": "Wei"
      },
      {
        "surname": "Peng",
        "given_name": "Dezhong"
      }
    ]
  },
  {
    "title": "OAENet: Oriented attention ensemble for accurate facial expression recognition",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107694",
    "abstract": "Facial Expression Recognition (FER) is a challenging yet important research topic owing to its significance with respect to its academic and commercial potentials. In this work, we propose an oriented attention pseudo-siamese network that takes advantage of global and local facial information for high accurate FER. Our network consists of two branches, a maintenance branch that consisted of several convolutional blocks to take advantage of high-level semantic features, and an attention branch that possesses a UNet-like architecture to obtain local highlight information. Specifically, we first input the face image into the maintenance branch. For the attention branch, we calculate the correlation coefficient between a face and its sub-regions. Next, we construct a weighted mask by correlating the facial landmarks and the correlation coefficients. Then, the weighted mask is sent to the attention branch. Finally, the two branches are fused to output the classification results. As such, a direction-dependent attention mechanism is established to remedy the limitation of insufficient utilization of local information. With the help of our attention mechanism, our network not only grabs a global picture but can also concentrate on important local areas. Experiments are carried out on 4 leading facial expression datasets. Our method has achieved a very appealing performance compared to other state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320304970",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Construct (python library)",
      "Convolutional neural network",
      "Correlation",
      "Epistemology",
      "Expression (computer science)",
      "Face (sociological concept)",
      "Facial expression",
      "Facial recognition system",
      "Geometry",
      "Image (mathematics)",
      "Mathematics",
      "Mechanism (biology)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Zhengning"
      },
      {
        "surname": "Zeng",
        "given_name": "Fanwei"
      },
      {
        "surname": "Liu",
        "given_name": "Shuaicheng"
      },
      {
        "surname": "Zeng",
        "given_name": "Bing"
      }
    ]
  },
  {
    "title": "Speech emotion recognition via learning analogies",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.01.018",
    "abstract": "This work introduces the few-shot learning paradigm in the speech emotion recognition domain. Emotional characterization of speech segments is carried out through analogies, i.e. by assessing similarities and dissimilarities between novel and known recordings. More specifically, we designed a Siamese Neural Network modeling such relationships on the combined log-Mel and temporal modulation spectrogram space. We present thorough experimentations assessing the performance of the proposed solution holistically, where it is demonstrated that it reaches state of the art rates when following the standard leave-one-speaker-out protocol, while at the same time being able to operate in non-stationary conditions, i.e. with limited knowledge of speakers and/or emotional classes. Finally, we investigated the activation maps in a layer-wise manner in order to interpret the predictions made by the model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521000313",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Domain (mathematical analysis)",
      "Emotion recognition",
      "Mathematical analysis",
      "Mathematics",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Spectrogram",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Ntalampiras",
        "given_name": "Stavros"
      }
    ]
  },
  {
    "title": "Neural random subspace",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107801",
    "abstract": "The random subspace method, also known as the pillar of random forests, is good at making precise and robust predictions. However, there is as yet no straightforward way to combine it with deep learning. In this paper, we therefore propose Neural Random Subspace (NRS), a novel deep learning based random subspace method. In contrast to previous forest methods, NRS enjoys the benefits of end-to-end, data-driven representation learning, as well as pervasive support from deep learning software and hardware platforms, hence achieving faster inference speed and higher accuracy. Furthermore, as a non-linear component to be encoded into Convolutional Neural Networks (CNNs), NRS learns non-linear feature representations in CNNs more efficiently than contemporary, higher-order pooling methods, producing excellent results with negligible increase in parameters, floating point operations (FLOPs) and real running time. Compared with random subspaces, random forests and gradient boosting decision trees (GBDTs), NRS demonstrates superior performance on 35 machine learning datasets. Moreover, on both 2D image and 3D point cloud recognition tasks, integration of NRS with CNN architectures achieves consistent improvements with only incremental cost.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032030604X",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Geometry",
      "Inference",
      "Linear subspace",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pooling",
      "Random forest",
      "Random subspace method",
      "Subspace topology"
    ],
    "authors": [
      {
        "surname": "Cao",
        "given_name": "Yun-Hao"
      },
      {
        "surname": "Wu",
        "given_name": "Jianxin"
      },
      {
        "surname": "Wang",
        "given_name": "Hanchen"
      },
      {
        "surname": "Lasenby",
        "given_name": "Joan"
      }
    ]
  },
  {
    "title": "Candidate fusion: Integrating language modelling into a sequence-to-sequence handwritten word recognition architecture",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107790",
    "abstract": "Sequence-to-sequence models have recently become very popular for tackling handwritten word recognition problems. However, how to effectively integrate an external language model into such recognizer is still a challenging problem. The main challenge while training a language model is to deal with the language model corpus which is usually different to the one used for training the handwritten word recognition system. Thus, the bias between both word corpora leads to incorrectness on the transcriptions, providing similar or even worse performances on the recognition task. In this work, we introduce Candidate Fusion, a novel way to integrate an external language model to a sequence-to-sequence architecture. Moreover, it provides suggestions from an external language knowledge, as a new input to the sequence-to-sequence recognizer. Hence, Candidate Fusion provides two improvements. On the one hand, the sequence-to-sequence recognizer has the flexibility to not only combine the information from itself and the language model, but also choose the importance of the information provided by the language model. On the other hand, the external language model has the ability to adapt itself to the training corpus and even learn the most common errors produced from the recognizer. Finally, by conducting comprehensive experiments, the Candidate Fusion proves to outperform the state-of-the-art language models for handwritten word recognition tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320305938",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Cache language model",
      "Comprehension approach",
      "Computer science",
      "Economics",
      "Genetics",
      "Language model",
      "Linguistics",
      "Management",
      "Natural language",
      "Natural language processing",
      "Philosophy",
      "Sequence (biology)",
      "Speech recognition",
      "Task (project management)",
      "Universal Networking Language",
      "Word (group theory)"
    ],
    "authors": [
      {
        "surname": "Kang",
        "given_name": "Lei"
      },
      {
        "surname": "Riba",
        "given_name": "Pau"
      },
      {
        "surname": "Villegas",
        "given_name": "Mauricio"
      },
      {
        "surname": "Fornés",
        "given_name": "Alicia"
      },
      {
        "surname": "Rusiñol",
        "given_name": "Marçal"
      }
    ]
  },
  {
    "title": "Robust visual tracking via spatio-temporal adaptive and channel selective correlation filters",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107738",
    "abstract": "In recent years, Discriminative Correlation Filter (DCF) based tracking methods have achieved impressive performance in visual tracking. However, their excellent performance usually comes at the cost of sacrificing the computational speed. Furthermore, training correlation filters using high dimensional raw features may introduce the risk of severe over-fitting. To address the above issues, we propose Spatio-Temporal adaptive and Channel selective Correlation Filters (STCCF) for robust tracking. Specifically, we first select a set of target-specific features from high dimensional features via an effective channel selective scheme based on the Taylor expansion. Then, we reformulate the filter learning problem from ridge regression to elastic net regression to adaptively select the discriminative features inside the target bounding box at the spatial level. Moreover, we constrain the filters to be adaptive across temporal frames by learning a transformation matrix from the initial filters to the previous filters. In particular, with a specific spatio-temporal-channel constraint, STCCF can not only alleviate the over-fitting problem and reduce the computational cost, but also enhance the discriminability and interpretability of the learned filters. The proposed STCCF can be optimized by using a few iterations of Alternating Direction Method of Multipliers (ADMM). Experiments on six challenging datasets show that STCCF can achieve promising performance with fast running speed.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320305410",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Channel (broadcasting)",
      "Chemistry",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Discriminative model",
      "Eye tracking",
      "Filter (signal processing)",
      "Gene",
      "Image (mathematics)",
      "Interpretability",
      "Minimum bounding box",
      "Pattern recognition (psychology)",
      "Transformation (genetics)"
    ],
    "authors": [
      {
        "surname": "Liang",
        "given_name": "Yanjie"
      },
      {
        "surname": "Liu",
        "given_name": "Yi"
      },
      {
        "surname": "Yan",
        "given_name": "Yan"
      },
      {
        "surname": "Zhang",
        "given_name": "Liming"
      },
      {
        "surname": "Wang",
        "given_name": "Hanzi"
      }
    ]
  },
  {
    "title": "A novel spatio-temporal Siamese network for 3D signature recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.01.012",
    "abstract": "Signature forgery is at the centre of several fraudulent activities and legal battles. The introduction of 3D signatures, the virtual signing of ones name in the air, has the potential to restrict forgers due to the absence of visual cues that can be easily copied. Existing 3D signature recognition approaches, however, have not leveraged the inherent spatial and temporal information, making it difficult to handle the diminished separability and reproducibility of these signatures. In this paper, we propose a novel spatio-temporal adaptation of the Siamese Neural Network, wherein one branch extracts spatial features using a 1D Convolutional Neural Network (CNN) while the other processes the input in the temporal domain using Long Short-Term Memory networks (LSTMs). Unlike conventional deep learning networks, Siamese networks are an application of One-Shot Learning so as to learn from a small amount of data as is often the case in real life problems. They employ a distance metric that is forced to be small for like samples (signatures from the same person), and large for different samples (from different persons). The proposed approach, termed ST-SNN, is compared to other baseline classification architectures, and demonstrated using a publicly available biometric 3D signature benchmark dataset, yielding True Positive Rate (TPR) of 94.63% with 4.1% False Acceptance Rate (FAR).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521000258",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biometrics",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Economics",
      "Geodesy",
      "Geography",
      "Geometry",
      "Mathematics",
      "Metric (unit)",
      "Operations management",
      "Pattern recognition (psychology)",
      "Signature (topology)"
    ],
    "authors": [
      {
        "surname": "Ghosh",
        "given_name": "Souvik"
      },
      {
        "surname": "Ghosh",
        "given_name": "Spandan"
      },
      {
        "surname": "Kumar",
        "given_name": "Pradeep"
      },
      {
        "surname": "Scheme",
        "given_name": "Erik"
      },
      {
        "surname": "Roy",
        "given_name": "Partha Pratim"
      }
    ]
  },
  {
    "title": "Averaging GPS segments competition 2019",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107730",
    "abstract": "Averaging GPS trajectories is needed in applications such as automatic generation of road network and finding representative movement patterns. We organized a challenge where participants submitted proposals to solve the averaging problem. In this paper, we review the proposals and evaluate their performance. We present a synthesis of the submitted methods and develop a new baseline composed of the well-performing components. The new baseline outperforms all existing averaging methods. All datasets, submissions and evaluations can be accessed on the competition webpage: http://cs.uef.fi/sipu/segments.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320305331",
    "keywords": [
      "Aesthetics",
      "Artificial intelligence",
      "Baseline (sea)",
      "Biology",
      "Competition (biology)",
      "Computer science",
      "Data mining",
      "Ecology",
      "Geology",
      "Global Positioning System",
      "Information retrieval",
      "Movement (music)",
      "Oceanography",
      "Philosophy",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Fränti",
        "given_name": "Pasi"
      },
      {
        "surname": "Mariescu-Istodor",
        "given_name": "Radu"
      }
    ]
  },
  {
    "title": "A CenterNet++ model for ship detection in SAR images",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107787",
    "abstract": "Ship detection in SAR images is a challenging task due to two difficulties. (1) Because of the long observation distance, ships in SAR images are small with low resolution, leading to high false negative. (2) Because of the complex onshore background, ships are easily confused with other objects with similar appearance. To solve these problems, we propose an effective and stable single-stage detector called CenterNet++. Our model mainly consists of three modules, i.e., feature refinement module, feature pyramids fusion module, and head enhancement module. Firstly, to address small objects detection problem, we design a feature refinement module for extracting multi-scale contextual information. Secondly, feature pyramids fusion module is developed for generating more powerful semantic information. Finally, to alleviate the impact of complex background, head enhancement module is proposed for a balance between foreground and background. To prove the effectiveness and robustness of the proposed method, we make extensive experiments on three popular SAR image datasets, i.e., AIR-SARShip, SSDD, SAR-Ship. The experimental results show that our CenterNet++ reaches state-of-the-art performance on all datasets. In addition, compared with the baseline CenterNet, the proposed method achieves a remarkable accuracy improvement with negligible increase in time cost.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320305902",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Detector",
      "Feature (linguistics)",
      "Gene",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Robustness (evolution)",
      "Synthetic aperture radar",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Guo",
        "given_name": "Haoyuan"
      },
      {
        "surname": "Yang",
        "given_name": "Xi"
      },
      {
        "surname": "Wang",
        "given_name": "Nannan"
      },
      {
        "surname": "Gao",
        "given_name": "Xinbo"
      }
    ]
  },
  {
    "title": "A novel dimension reduction and dictionary learning framework for high-dimensional data classification",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107793",
    "abstract": "High-dimensional problem poses significant challenges for dictionary learning based classification architecture. Joint Dimension Reduction and Dictionary Learning (JDRDL) framework shows great potential for overcoming the challenges caused by high dimensionality. However, most of the existing JDRDL approaches do not consider the complex nonlinear relationships within high-dimensional data, which limits their classification performance. To overcome this problem, a novel joint dimension reduction and dictionary learning framework is proposed in this paper for high-dimensional data classification. Firstly, at dimension reduction stage, an autoencoder is employed to learn a nonlinear mapping that reduces dimensionality and preserves nonlinear structure of the high-dimensional data. Then, at dictionary learning stage, the locality constraint with label embedding, which takes the locality and label information into account together, is incorporated into the learning process to preserve desirable nonlinear local structure and enhance class discrimination. Moreover, the mapping function and dictionary are optimized simultaneously to enhance the performance. Encouraging experimental results on multiple benchmark datasets confirm that the proposed framework is effective and efficient for high-dimensional data classification.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320305963",
    "keywords": [
      "Artificial intelligence",
      "Autoencoder",
      "Benchmark (surveying)",
      "Cluster analysis",
      "Clustering high-dimensional data",
      "Computer science",
      "Deep learning",
      "Dimension (graph theory)",
      "Dimensionality reduction",
      "Embedding",
      "Geodesy",
      "Geography",
      "Geometry",
      "Linguistics",
      "Locality",
      "Machine learning",
      "Mathematics",
      "Nonlinear dimensionality reduction",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Process (computing)",
      "Pure mathematics",
      "Reduction (mathematics)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Yanxia"
      },
      {
        "surname": "Chai",
        "given_name": "Yi"
      },
      {
        "surname": "Zhou",
        "given_name": "Han"
      },
      {
        "surname": "Yin",
        "given_name": "Hongpeng"
      }
    ]
  },
  {
    "title": "On parameterizing higher-order motion for behaviour recognition",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107710",
    "abstract": "Human behaviours consist different types of motion; we show how they can be disambiguated into their components in a richer way than that currently possible. Studies on optical flow have concentrated on motion alone without the higher order components: snap, jerk and acceleration. We are the first to show how the acceleration, jerk, snap and their constituent parts can be obtained from image sequences, and can be deployed for analysis, especially of behaviour. We demonstrate the estimation of acceleration in sport, human motion, traffic and in scenes of violent behaviour to demonstrate the wide potential for application of analysis of acceleration. Determining higher order components is suited to the analysis of scenes which contain them: higher order motion is innate to scenes containing acts of violent behaviour, but it is not just for behaviour which contains quickly changing movement: human gait contains acceleration though approaches have yet to consider radial and tangential acceleration, since they concentrate on motion alone. The analysis of synthetic and real-world images illustrates the ability of higher order motion to discriminate different objects under different motion. Then the new approaches are applied in heel strike detection in the analysis of human gait. These results demonstrate that the new approach is ready for developing new applications in behaviour recognition and provides a new basis for future research and applications of higher-order motion analysis.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320305136",
    "keywords": [
      "Acceleration",
      "Artificial intelligence",
      "Biology",
      "Classical mechanics",
      "Computer science",
      "Computer vision",
      "Gait",
      "Image (mathematics)",
      "Jerk",
      "Motion (physics)",
      "Motion analysis",
      "Motion estimation",
      "Optical flow",
      "Physics",
      "Physiology"
    ],
    "authors": [
      {
        "surname": "Sun",
        "given_name": "Yan"
      },
      {
        "surname": "Hare",
        "given_name": "Jonathon S."
      },
      {
        "surname": "Nixon",
        "given_name": "Mark S."
      }
    ]
  },
  {
    "title": "Real-time sufficient dimension reduction through principal least squares support vector machines",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107768",
    "abstract": "We propose a real-time approach for sufficient dimension reduction. Compared with popular sufficient dimension reduction methods including sliced inverse regression and principal support vector machines, the proposed principal least squares support vector machines approach enjoys better estimation of the central subspace. Furthermore, this new proposal can be used in the presence of streamed data for quick real-time updates. It is demonstrated through simulations and real data applications that our proposal performs better and faster than existing algorithms in the literature.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320305719",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Dimension (graph theory)",
      "Dimensionality reduction",
      "Estimator",
      "Geometry",
      "Least squares support vector machine",
      "Least-squares function approximation",
      "Mathematical optimization",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Principal (computer security)",
      "Principal component analysis",
      "Reduction (mathematics)",
      "Statistics",
      "Sufficient dimension reduction",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Artemiou",
        "given_name": "Andreas"
      },
      {
        "surname": "Dong",
        "given_name": "Yuexiao"
      },
      {
        "surname": "Shin",
        "given_name": "Seung Jun"
      }
    ]
  },
  {
    "title": "A hierarchical weighted low-rank representation for image clustering and classification",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107736",
    "abstract": "Low-rank representation (LRR), which is a powerful method to find the low-dimensional subspace structure embedded in high-dimensional data spaces, has been used in both unsupervised learning and semi-supervised classification. LRR aims at finding the lowest rank representation that can express each data sample as linear combination of other samples. However, this method doesn’t consider the geometrical structure of the data. Thus the similarity and local structure might be lost in the process of learning. Motivated by this, a novel hierarchical weighted low-rank representation (HWLRR) is proposed in this paper. In the new algorithm, a hierarchical weighted matrix is defined to find more samples that may belong to the same subspace using affinity propagation. By taking advantage of the affinity propagation, our proposed method can preserve both local and global structure of the whole dataset. The experimental results on both unsupervised learning and semi-supervised classification demonstrate the superiority of our proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320305392",
    "keywords": [
      "Affinity propagation",
      "Artificial intelligence",
      "Canopy clustering algorithm",
      "Cluster analysis",
      "Combinatorics",
      "Computer science",
      "Data mining",
      "Fuzzy clustering",
      "Image (mathematics)",
      "Law",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Rank (graph theory)",
      "Representation (politics)",
      "Similarity (geometry)",
      "Subspace topology"
    ],
    "authors": [
      {
        "surname": "Fu",
        "given_name": "Zhiqiang"
      },
      {
        "surname": "Zhao",
        "given_name": "Yao"
      },
      {
        "surname": "Chang",
        "given_name": "Dongxia"
      },
      {
        "surname": "Wang",
        "given_name": "Yiming"
      }
    ]
  },
  {
    "title": "Feature selection using bare-bones particle swarm optimization with mutual information",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107804",
    "abstract": "Feature selection (FS) is an important data processing method in pattern recognition and data mining. Due to not considering characteristics of the FS problem itself, traditional particle update mechanisms and swarm initialization strategies adopted in most particle swarm optimization (PSO) limit their performance on dealing with high-dimensional FS problems. Focused on it, this paper proposes a novel feature selection algorithm based on bare bones PSO (BBPSO) with mutual information. Firstly, an effective swarm initialization strategy based on label correlation is developed, making full use of the correlation between features and class labels to accelerate the convergence of swarm. Then, in order to enhance the exploitation performance of the algorithm, two local search operators, i.e., the supplementary operator and the deletion operator, are developed based on feature relevance-redundancy. Furthermore, an adaptive flip mutation operator is designed to help particles jump out of local optimal solutions. We apply the proposed algorithm to typical datasets based on the K-Nearest Neighbor classifier (K-NN), and compare it with eleven state-of-the-art algorithms, SFS, PTA, SGA, BPSO, PSO(4-2), HPSO-LS, Binary BPSO, NaFA, IBFA, KPLS-mRMR and SMBA-CSFS. The experimental results show that the proposed algorithm can achieve a feature subset with better performance, and is a highly competitive FS algorithm.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320306075",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Feature (linguistics)",
      "Feature selection",
      "Initialization",
      "Linguistics",
      "Local optimum",
      "Mutual information",
      "Operating system",
      "Particle swarm optimization",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Redundancy (engineering)",
      "Swarm behaviour"
    ],
    "authors": [
      {
        "surname": "Song",
        "given_name": "Xian-fang"
      },
      {
        "surname": "Zhang",
        "given_name": "Yong"
      },
      {
        "surname": "Gong",
        "given_name": "Dun-wei"
      },
      {
        "surname": "Sun",
        "given_name": "Xiao-yan"
      }
    ]
  },
  {
    "title": "Multi-view content-based mammogram retrieval using dynamic similarity and locality sensitive hashing",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107786",
    "abstract": "Content-Based Mammogram Retrieval (CBMR) methods using Multi-View Information Fusion (MVIF) have triggered a growing interest in the last years given their ability to help radiologists make the right breast-cancer related decision. To further improve the retrieval performance, this paper introduces an efficient MVIF-CBMR method based on late fusion that combines retrieval result-level of Medio-Lateral Oblique (MLO) and Cranio-Caudal (CC) views. The proposed method adopts a coupled multi-index with a dynamic distance to evaluate the similarity between mammograms, which allows to fully exert the discriminative power of the complementary of MLO-CC features. Furthermore, the ROI dataset signature indexing step uses a hashing technique to optimize the computational time for retrieving relevant images. Thus, the proposed method takes two query ROIs corresponding to two different views (MLO and CC) as input and displays the most similar ROIs to each view using a dynamic similarity assessment. The retrieved ROIs can therefore be analyzed according to their clinical cases for the final decision-making relative to the query ROIs. The experiments realized on the challenging Digital Database for Screening Mammography (DDSM) dataset have proved the effectiveness and the efficiency of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320305896",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Data mining",
      "Discriminative model",
      "Hash function",
      "Image (mathematics)",
      "Image retrieval",
      "Information retrieval",
      "Pattern recognition (psychology)",
      "Search engine indexing",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Jouirou",
        "given_name": "Amira"
      },
      {
        "surname": "Baâzaoui",
        "given_name": "Abir"
      },
      {
        "surname": "Barhoumi",
        "given_name": "Walid"
      }
    ]
  },
  {
    "title": "Plant leaf recognition by integrating shape and texture features",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107809",
    "abstract": "Plant leaf identification is a significant challenge in the fields of computer vision and pattern recognition. This article presents a new approach to plant leaf identification, one that integrates shape and texture characteristics. First, we introduce the shape and texture features used by the proposed plant leaf recognition method. The proposed multiscale triangle descriptor (MTD) is employed to characterize the shape information of a plant leaf, and the local binary pattern histogram Fourier (LBP-HF) is used as the texture feature. Then, the shape and texture features of a leaf image are combined by weighted distance measurement, where L 1 distance and chi-square distance are used for shape and texture features, respectively. The proposed approach provides a robust descriptor for the task of plant leaf recognition by combining the complementary MTD and LBP-HF features. The proposed approach has been thoroughly evaluated on three benchmark leaf datasets, including the Flavia, Swedish and MEW2012 leaf datasets. Our method achieves 77.6%, 85.7%, and 67.5% retrieval accuracy on the Flavia, Swedish and MEW2012 leaf datasets, respectively, while the corresponding classification accuracy is 99.1%, 98.4%, 95.6%. The recognition performance of our method is better or comparable to prior state-of-the-art plant leaf recognition method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320306129",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Botany",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Geodesy",
      "Geography",
      "Histogram",
      "Identification (biology)",
      "Image (mathematics)",
      "Linguistics",
      "Local binary patterns",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Plant identification",
      "Texture (cosmology)"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Chengzhuan"
      }
    ]
  },
  {
    "title": "A plug-in attribute correction module for generalized zero-shot learning",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107767",
    "abstract": "While Zero Shot Learning models can recognize new classes without training examples, they often fails to incorporate both seen and unseen classes together at the test time, which is known as the Generalized Zero-shot Learning (GZSL) problem. This paper identifies a bottleneck issue when attributes are not well-defined, reliable, inaccurate in quantitative representations, or suffering from the visual-semantic discrepancy. We propose a Generic Plug-in Attribute Correction (GPAC) module which can effectively accommodate conventional ZSL in GZSL tasks. Different from existing embedding-based approaches which often lose the favor of transparency in attributes, our key challenge is to fully preserve the original meaning of the attributes and make it complementary and interpretable to upgrade existing ZSL models. To this end, we propose a novel nonnegative constraint with iterative Stochastic Gradient Descent toolbox to effectively fit our GPAC module into previous ZSL models. Extensive experiments on five popular datasets show that our method can effectively correct attributes and make conventional ZSL can achieve state-of-the-art performance on GZSL tasks. It is also a good practice for future models when incorporating prior human knowledge.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320305707",
    "keywords": [
      "Artificial intelligence",
      "Bottleneck",
      "Computer science",
      "Computer security",
      "Constraint (computer-aided design)",
      "Embedded system",
      "Embedding",
      "Geometry",
      "Key (lock)",
      "Machine learning",
      "Mathematics",
      "Operating system",
      "Programming language",
      "Toolbox",
      "Upgrade"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Haofeng"
      },
      {
        "surname": "Bai",
        "given_name": "Haoyue"
      },
      {
        "surname": "Long",
        "given_name": "Yang"
      },
      {
        "surname": "Liu",
        "given_name": "Li"
      },
      {
        "surname": "Shao",
        "given_name": "Ling"
      }
    ]
  },
  {
    "title": "Adaptive hybrid attention network for hyperspectral image classification",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.01.015",
    "abstract": "The task of land cover classification from hyperspectral images (HSI) has recently witnessed sharp improvements with the applications of deep convolutional networks (CNN). This is mainly attributed to the hierarchical spectral-spatial feature learning capabilities of the CNN models. However, it is important to encode the short to long range spatial dependencies to predict the pixelwise labels without inducing additional redundancy at the feature level. The standard CNN based land cover classification models for HSI overlook this aspect in general. To overcome this, we propose a hybrid attention based 3D classification model for hyperspectral images. Our model comprises of 1D and 2D CNNs to individually generate the attention masks that respectively highlight the spectral and spatial characteristics of our input image. For optimal learning the spectral and spatial features in our model are combined adaptively with the original input and sent to the 3D classification module. To enhance the classification performance, we incorporate classwise Wasserstein loss alongwith the crossentropy loss. Our methods are evaluated on three widely used hyperspectral datasets: Houston datasets (DFC-2013 and DFC-2018) and Salinas dataset, and has satisfactorily outperformed all the prior benchmark methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521000283",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Cartography",
      "Civil engineering",
      "Computer science",
      "Convolutional neural network",
      "Engineering",
      "Feature (linguistics)",
      "Geography",
      "Hyperspectral imaging",
      "Land cover",
      "Land use",
      "Linguistics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Redundancy (engineering)"
    ],
    "authors": [
      {
        "surname": "Pande",
        "given_name": "Shivam"
      },
      {
        "surname": "Banerjee",
        "given_name": "Biplab"
      }
    ]
  },
  {
    "title": "Multicamera pedestrian detection using logic minimization",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107703",
    "abstract": "In this paper an algorithm for multicamera pedestrian detection is proposed. The first stage of this work is based on the probabilistic occupancy map framework, in which the ground plane is discretized into a grid and the likelihood of pedestrian presence at each location is estimated by comparing a rectangle, of the average size of the pedestrians standing there, with the foreground silhouettes in all camera views. In the second stage, where we borrowed the idea from the Quine-McCluskey method for logic function minimization, essential candidates are initially identified, each of which covers at least a significant part of the foreground that is not covered by the other candidates. Then non-essential candidates are selected to cover the remaining foregrounds by following an iterative process, which alternates between merging redundant candidates and finding emerging essential candidates. Experiments on benchmark video datasets have demonstrated the improved performance of this algorithm in comparison with some benchmark non-deep or deep multicamera/monocular algorithms for pedestrian detection.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320305069",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Computer vision",
      "Engineering",
      "Geodesy",
      "Geography",
      "Geometry",
      "Heuristic",
      "Lens (geology)",
      "Mathematics",
      "Minification",
      "Mobile robot",
      "Monocular",
      "Occupancy grid mapping",
      "Operating system",
      "Pedestrian",
      "Pedestrian detection",
      "Petroleum engineering",
      "Probabilistic logic",
      "Process (computing)",
      "Programming language",
      "Rectangle",
      "Robot",
      "Zoom"
    ],
    "authors": [
      {
        "surname": "Yan",
        "given_name": "Yuyao"
      },
      {
        "surname": "Xu",
        "given_name": "Ming"
      },
      {
        "surname": "Smith",
        "given_name": "Jeremy S."
      },
      {
        "surname": "Shen",
        "given_name": "Mo"
      },
      {
        "surname": "Xi",
        "given_name": "Jin"
      }
    ]
  },
  {
    "title": "On finite mixture modeling and model-based clustering of directed weighted multilayer networks",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107641",
    "abstract": "A novel approach relying on the notion of mixture models is proposed for modeling and clustering directed weighted networks. The developed methodology can be used in a variety of settings including multilayer networks. Computational issues associated with the developed procedure are effectively addressed by the use of MCMC techniques. The utility of the methodology is illustrated on a set of experiments as well as applications to real-life data containing export trade amounts for European countries.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320304441",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Data set",
      "Machine learning",
      "Mixture model",
      "Programming language",
      "Set (abstract data type)",
      "Variety (cybernetics)"
    ],
    "authors": [
      {
        "surname": "Melnykov",
        "given_name": "Volodymyr"
      },
      {
        "surname": "Sarkar",
        "given_name": "Shuchismita"
      },
      {
        "surname": "Melnykov",
        "given_name": "Yana"
      }
    ]
  },
  {
    "title": "Large group activity security risk assessment and risk early warning based on random forest algorithm",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.01.008",
    "abstract": "With the continuous development of artificial intelligence, machine learning, the necessary way to achieve artificial intelligence, is also constantly improving, of which deep learning is one of the contents. The purpose of this paper is to evaluate and warn the security risk of large-scale group activities based on the random forest algorithm. This paper uses the methods of calculating the importance of the random forest algorithm to variables and the calculation formula of the weight of the security risk index, and combining the model parameters of the random forest algorithm The optimization experiment and the random forest model training experiment are used for risk analysis, and the classification accuracy rate reaches a maximum of 0.86, which leads to the conclusion that the random forest algorithm has good predictive ability in the risk assessment of large-scale group activities. This article takes a certain international youth environmental protection festival as an example for analysis, and better verifies the feasibility and effectiveness of this article.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521000192",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Machine learning",
      "Physics",
      "Quantum mechanics",
      "Random forest",
      "Scale (ratio)",
      "Telecommunications",
      "Warning system"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Yanyu"
      },
      {
        "surname": "Zheng",
        "given_name": "Wenzhe"
      },
      {
        "surname": "Li",
        "given_name": "Wenbo"
      },
      {
        "surname": "Huang",
        "given_name": "Yimiao"
      }
    ]
  },
  {
    "title": "Computer vision techniques for Upper Aero-Digestive Tract tumor grading classification – Addressing pathological challenges",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.01.002",
    "abstract": "Oral cancer is one of the common cancer types which scales higher in death rate every year. The connectivity of two different cavities like oral cavity and nasal cavity is known as Upper Aero-Digestive Tract (UADT). Both oral and nasal cavities consist of thirteen connecting sites from mouth to upper stomach. The traditional pathological analysis like manual microscopic review brings out major intra and interobserver variability problem. A new automated system is proposed using computer vision techniques to focus and analyse major pathological problems like intra and interobserver variability problem and mis-classification of dysplasia type of tumours. The morphological behaviour of biopsy tissue samples are analysed digitally with different sites of UADT and different cancerous and non-cancerous stages. The proposed technique will play a major role in assisting the manual pathology procedure for analysing the morphology of dysplasia type of tumours and classification of tumour gradings. A method is proposed which integrates an alternate process to find the morphology of dysplasia type tumours using different image processing techniques. A state-of-the-art Force Reconstructed Particle Swarm Optimization Based SVM is proposed for UADT oral cancer classification for ten different oral cavity sites. The proposed classification technique achieved 94 % accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552100012X",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer vision",
      "Digestive tract",
      "Ecology",
      "Grading (engineering)",
      "Internal medicine",
      "Medicine",
      "Pathological",
      "Pathology"
    ],
    "authors": [
      {
        "surname": "Mathialagan",
        "given_name": "Prabhakaran"
      },
      {
        "surname": "Chidambaranathan",
        "given_name": "Malathy"
      }
    ]
  },
  {
    "title": "Single-shot 3D multi-person pose estimation in complex images",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107534",
    "abstract": "In this paper, we propose a new single shot method for multi-person 3D human pose estimation in complex images. The model jointly learns to locate the human joints in the image, to estimate their 3D coordinates and to group these predictions into full human skeletons. The proposed method deals with a variable number of people and does not need bounding boxes to estimate the 3D poses. It leverages and extends the Stacked Hourglass Network and its multi-scale feature learning to manage multi-person situations. Thus, we exploit a robust 3D human pose formulation to fully describe several 3D human poses even in case of strong occlusions or crops. Then, joint grouping and human pose estimation for an arbitrary number of people are performed using the associative embedding method. Our approach significantly outperforms the state of the art on the challenging CMU Panoptic and a previous single shot method on the MuPoTS-3D dataset. Furthermore, it leads to good results on the complex and synthetic images from the newly proposed JTA Dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032030337X",
    "keywords": [
      "3D pose estimation",
      "Archaeology",
      "Artificial intelligence",
      "Bounding overwatch",
      "Chemistry",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Embedding",
      "Exploit",
      "Feature (linguistics)",
      "History",
      "Hourglass",
      "Linguistics",
      "Matching (statistics)",
      "Mathematics",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pose",
      "Shot (pellet)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Benzine",
        "given_name": "Abdallah"
      },
      {
        "surname": "Luvison",
        "given_name": "Bertrand"
      },
      {
        "surname": "Pham",
        "given_name": "Quoc Cuong"
      },
      {
        "surname": "Achard",
        "given_name": "Catherine"
      }
    ]
  },
  {
    "title": "Reconstruction by inpainting for visual anomaly detection",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107706",
    "abstract": "Visual anomaly detection addresses the problem of classification or localization of regions in an image that deviate from their normal appearance. A popular approach trains an auto-encoder on anomaly-free images and performs anomaly detection by calculating the difference between the input and the reconstructed image. This approach assumes that the auto-encoder will be unable to accurately reconstruct anomalous regions. But in practice neural networks generalize well even to anomalies and reconstruct them sufficiently well, thus reducing the detection capabilities. Accurate reconstruction is far less likely if the anomaly pixels were not visible to the auto-encoder. We thus cast anomaly detection as a self-supervised reconstruction-by-inpainting problem. Our approach (RIAD) randomly removes partial image regions and reconstructs the image from partial inpaintings, thus addressing the drawbacks of auto-enocoding methods. RIAD is extensively evaluated on several benchmarks and sets a new state-of-the art on a recent highly challenging anomaly detection benchmark.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320305094",
    "keywords": [
      "Anomaly (physics)",
      "Anomaly detection",
      "Artificial intelligence",
      "Artificial neural network",
      "Autoencoder",
      "Benchmark (surveying)",
      "Cartography",
      "Computer science",
      "Computer vision",
      "Condensed matter physics",
      "Encoder",
      "Geography",
      "Image (mathematics)",
      "Inpainting",
      "Operating system",
      "Pattern recognition (psychology)",
      "Physics",
      "Pixel"
    ],
    "authors": [
      {
        "surname": "Zavrtanik",
        "given_name": "Vitjan"
      },
      {
        "surname": "Kristan",
        "given_name": "Matej"
      },
      {
        "surname": "Skočaj",
        "given_name": "Danijel"
      }
    ]
  },
  {
    "title": "Joint adaptive manifold and embedding learning for unsupervised feature selection",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107742",
    "abstract": "As data always lie on a lower-dimensional space, feature selection has become an important step in computer vision, machine learning and data mining. Due to the lack of class information, the performance of unsupervised feature selection depends on how to characterize and preserve the manifold structure among data. In this paper, we propose a novel unsupervised feature selection framework, named as joint adaptive manifold and embedding learning for unsupervised feature selection (JAMEL). It iteratively and adaptively learns lower-dimensional embeddings for data to preserve the manifold structure among data, regresses data to embeddings to measure the importance of features, and learns the manifold structure among data according to the data density in the intrinsic space, where the redundant and noisy features are eliminated. In addition, we present an efficient algorithm to solve the proposed problem, together with the convergence analysis. Finally, the evaluation results with the tasks of k -means, spectral clustering and nearest neighbor classification using the selected features on 12 datasets show the effectiveness and efficiency of our approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320305458",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Dimensionality reduction",
      "Embedding",
      "Engineering",
      "Feature (linguistics)",
      "Feature learning",
      "Feature selection",
      "Feature vector",
      "Linguistics",
      "Machine learning",
      "Manifold (fluid mechanics)",
      "Manifold alignment",
      "Mechanical engineering",
      "Nonlinear dimensionality reduction",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Unsupervised learning",
      "k-nearest neighbors algorithm"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Jian-Sheng"
      },
      {
        "surname": "Song",
        "given_name": "Meng-Xiao"
      },
      {
        "surname": "Min",
        "given_name": "Weidong"
      },
      {
        "surname": "Lai",
        "given_name": "Jian-Huang"
      },
      {
        "surname": "Zheng",
        "given_name": "Wei-Shi"
      }
    ]
  },
  {
    "title": "A simple graph embedding for anomaly detection in a stream of heterogeneous labeled graphs",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107746",
    "abstract": "In this work, we propose a new approach to detect anomalous graphs in a stream of directed and labeled heterogeneous edges. The stream consists of a sequence of edges derived from different graphs. Each of these dynamic graphs represents the evolution of a specific activity in a monitored system whose events are acquired in real-time. Our approach is based on graph clustering and uses a simple graph embedding based on substructures and graph edit distance. Our graph representation is flexible and updates incrementally the graph vectors as soon as a new edge arrives. This allows the detection of anomalies in real-time which is an important requirement for sensitive applications such as cyber-security. Our implementation results prove the effectiveness of our approach in terms of accuracy of detection and time processing.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320305495",
    "keywords": [
      "Algorithm",
      "Anomaly detection",
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Embedding",
      "Graph",
      "Graph embedding",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Kiouche",
        "given_name": "Abd Errahmane"
      },
      {
        "surname": "Lagraa",
        "given_name": "Sofiane"
      },
      {
        "surname": "Amrouche",
        "given_name": "Karima"
      },
      {
        "surname": "Seba",
        "given_name": "Hamida"
      }
    ]
  },
  {
    "title": "A simulated annealing algorithm with a dual perturbation method for clustering",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107713",
    "abstract": "Clustering is a powerful tool in exploratory data analysis that partitions a set of objects into clusters with the goal of maximizing the similarity of objects within each cluster. Due to the tendency of clustering algorithms to find suboptimal partitions of data, the approximation method Simulated Annealing (SA) has been used to search for near-optimal partitions. However, existing SA-based partitional clustering algorithms still settle to local optima. We propose a new SA-based clustering algorithm, the Simulated Annealing with Gaussian Mutation and Distortion Equalization algorithm (SAGMDE), which uses two perturbation methods to allow for both large and small perturbations in solutions. Our experiments on a diverse collection of data sets show that SAGMDE performs more consistently and yields better results than existing SA clustering algorithms in terms of cluster quality while maintaining a reasonable runtime. Finally, we use generative art as a visualization tool to compare various partitional clustering algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320305161",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "CURE data clustering algorithm",
      "Canopy clustering algorithm",
      "Cluster analysis",
      "Computer science",
      "Correlation clustering",
      "Data mining",
      "Determining the number of clusters in a data set",
      "Simulated annealing",
      "Visualization"
    ],
    "authors": [
      {
        "surname": "Lee",
        "given_name": "Julian"
      },
      {
        "surname": "Perkins",
        "given_name": "David"
      }
    ]
  },
  {
    "title": "Image super-resolution via channel attention and spatial graph convolutional network",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107798",
    "abstract": "Recently, deep convolutional neural networks (CNNs) have been widely explored in single image super-resolution (SISR) and obtained remarkable performance. However, most of the existing CNN-based SR methods mainly focus on wider or deeper architecture design, neglecting to discover the latent relationship of features, hence limiting the representational ability of networks. To address this issue, we propose a channel attention and spatial graph convolutional network (CASGCN) for more powerful feature obtaining and feature correlations modeling. The CASGCN is formed by several channel attention and spatial graph (CASG) blocks that incorporate global spatial and channel inter-dependencies for rendering features of each pixel. Inside the CASG block, channel branch and spatial branch are first arranged in a paralleled way, and then are concatenated to effectively learn the representation of each image pixel. Specifically, we use attention mechanism to extract informative features in channel branch while the spatial-aware graph is used in spatial branch to model the global self-similar information. Furthermore, the adjacency matrix in spatial-aware graph is dynamically generated via the Gram matrix to model global correlations between pixels and is shared across the whole network without auxiliary parameters. Extensive experiments on SISR with different degradation models show the effectiveness of our CASGCN in terms of quantitative and visual results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320306014",
    "keywords": [
      "Adjacency list",
      "Adjacency matrix",
      "Algorithm",
      "Artificial intelligence",
      "Channel (broadcasting)",
      "Computer network",
      "Computer science",
      "Convolutional neural network",
      "Feature (linguistics)",
      "Graph",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pixel",
      "Rendering (computer graphics)",
      "Spatial analysis",
      "Statistics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Yue"
      },
      {
        "surname": "Qi",
        "given_name": "Yong"
      }
    ]
  },
  {
    "title": "SVMs multi-class loss feedback based discriminative dictionary learning for image classification",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107690",
    "abstract": "The learning model has been popular recently due to its promising results in various image classification tasks. Many existing learning methods, especially the deep learning methods, need a large amount of training data to achieve a high accuracy of classification. Conversely, only provided with a small-size dataset, some dictionary learning (DL) methods can achieve a perfect performance on a image classification task and hence still get a lot of attention. Among these DL methods, DL based feature learning methods are the mainstream for image classification in recent years, however, most of these methods have trained a classifier independently from dictionary learning. Therefore, the features extracted by the learned dictionary may not be very proper to perform classification for the classifier. Inspired by the feedback mechanism in cybernetics, this paper proposes a novel discriminative DL framework, named support vector machines (SVMs) multi-class loss feedback based discriminative dictionary learning (SMLFDL) that learns a discriminative dictionary while training SVMs to make the features extracted by the learned dictionary and SVMs better matched with each other. Because of integrating dictionary learning and SVMs training into a unified learning framework and good exactness of the looped multi-class loss term formulated from the feedback viewpoint for the classification scheme, better classification performance can be achieved. Experimental results on several widely used image databases show that SMLFDL can achieve a competitive performance with other state-of-the-art dictionary learning methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320304933",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Contextual image classification",
      "Discriminative model",
      "Image (mathematics)",
      "Linear classifier",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Bao-Qing"
      },
      {
        "surname": "Guan",
        "given_name": "Xin-Ping"
      },
      {
        "surname": "Zhu",
        "given_name": "Jun-Wu"
      },
      {
        "surname": "Gu",
        "given_name": "Chao-Chen"
      },
      {
        "surname": "Wu",
        "given_name": "Kai-Jie"
      },
      {
        "surname": "Xu",
        "given_name": "Jia-Jie"
      }
    ]
  },
  {
    "title": "Behavior regularized prototypical networks for semi-supervised few-shot image classification",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107765",
    "abstract": "We propose a Behavior Regularized Prototypical Network (BR-ProtoNet) for few-shot image classification in semi-supervised scenarios. To learn a generalizable metric, we exploit readily-available unlabeled data and construct complementary constraints to regularize the model’s behavior. Specifically, we match the label spaces between each episode and the whole training set. The predictions on the unlabeled data over different episodes can be aggregated to capture more reliable category information. We further construct new instances via adversarial perturbation and interpolation. These instances regularize the model’s behavior over the neighborhoods of the original ones and along the interpolation paths among them. In addition, they ensure the learnt embedding space possesses the property of proximity preservation. The regularization of these aspects is incorporated into the optimization process of BR-ProtoNet on partially labeled data. We have conducted thorough experiments on multiple challenging benchmarks. The results suggest that the metric learning can significantly benefit from the proposed regularization, and thus leading to the state-of-the-art performance in semi-supervised few-shot image classification.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320305689",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Image (mathematics)",
      "Mathematics",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Shot (pellet)"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Shixin"
      },
      {
        "surname": "Zeng",
        "given_name": "Xiangping"
      },
      {
        "surname": "Wu",
        "given_name": "Si"
      },
      {
        "surname": "Yu",
        "given_name": "Zhiwen"
      },
      {
        "surname": "Azzam",
        "given_name": "Mohamed"
      },
      {
        "surname": "Wong",
        "given_name": "Hau-San"
      }
    ]
  },
  {
    "title": "Machine learning based feature selection and knowledge reasoning for CBR system under big data",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107805",
    "abstract": "Under big data, large number of features as well as their complex data types makes traditional feature selection and knowledge reasoning in CBR system not adapt to new condition. To solve these problems, first, this paper proposes Weighted Relative Probability Change of Solution Parameters (WRPCSP) algorithm to execute feature selection. Then, this paper integrates Bayesian network (BN) with CBR system for knowledge reasoning. Based on probability calculation and reasoning, WRPCSP algorithm together with BN allows the proposed CBR system to well work under big data. In addition, to overcome the efficiency problem caused by large number of features, this paper also proposes Group-Outside (GO) algorithm to assign the computing task of big data for parallel data processing. GO algorithm can make the computing capacity of Hadoop fully utilized to gain the least time costing for parallel data processing. Finally, lots of experiments are performed to validate the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320306087",
    "keywords": [
      "Artificial intelligence",
      "Bayesian network",
      "Big data",
      "Computer science",
      "Data mining",
      "Economics",
      "Feature (linguistics)",
      "Feature selection",
      "Linguistics",
      "Machine learning",
      "Management",
      "Philosophy",
      "Selection (genetic algorithm)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Guo",
        "given_name": "Yuan"
      },
      {
        "surname": "Zhang",
        "given_name": "Bing"
      },
      {
        "surname": "Sun",
        "given_name": "Y."
      },
      {
        "surname": "Jiang",
        "given_name": "K."
      },
      {
        "surname": "Wu",
        "given_name": "K."
      }
    ]
  },
  {
    "title": "IoU-uniform R-CNN: Breaking through the limitations of RPN",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107816",
    "abstract": "Region Proposal Network (RPN) is the cornerstone of two-stage object detectors. It generates a sparse set of object proposals and alleviates the extrem foreground-background class imbalance problem during training. However, we find that the potential of the detector has not been fully exploited due to the IoU distribution imbalance and inadequate quantity of the training samples generated by RPN. With the increasing intersection over union (IoU), the exponentially smaller numbers of positive samples would lead to the distribution skewed towards lower IoUs, which hinders the optimization of detector at high IoU levels. In this paper, to break through the limitations of RPN, we propose IoU-Uniform R-CNN, a simple but effective method that directly generates training samples with uniform IoU distribution for the regression branch as well as the IoU prediction branch. Besides, we improve the performance of IoU prediction branch by eliminating the feature offsets of RoIs at inference, which helps the NMS procedure by preserving accurately localized bounding box. Extensive experiments on the PASCAL VOC and MS COCO dataset show the effectiveness of our method, as well as its compatibility and adaptivity to many object detection architectures. The code is made publicly available at https://github.com/zl1994/IoU-Uniform-R-CNN.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321000030",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Bounding overwatch",
      "Computer science",
      "Detector",
      "Inference",
      "Pascal (unit)",
      "Pattern recognition (psychology)",
      "Programming language",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Zhu",
        "given_name": "Li"
      },
      {
        "surname": "Xie",
        "given_name": "Zihao"
      },
      {
        "surname": "Liu",
        "given_name": "Liman"
      },
      {
        "surname": "Tao",
        "given_name": "Bo"
      },
      {
        "surname": "Tao",
        "given_name": "Wenbing"
      }
    ]
  },
  {
    "title": "Fast hard negative mining for deep metric learning",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107795",
    "abstract": "Deep metric learning methods aim to measure similarity of data points (e.g. images) by calculating their distance in a high dimensional embedding space. These methods are usually trained by optimizing a ranking loss function, which is designed to bring together samples from the same class while separating them from samples from all other classes. The most challenging part of these methods is the selection of samples that contribute to effective network training. In this paper we present Bag of Negatives (BoN), a fast hard negative mining method, that provides a set, triplet or pair of potentially relevant training samples. BoN is an efficient method that selects a bag of hard negatives based on a novel online hashing strategy. We show the superiority of BoN against state-of-the-art hard negative mining methods in terms of accuracy and training time over three large datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320305987",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Data mining",
      "Deep learning",
      "Economics",
      "Embedding",
      "Image (mathematics)",
      "Machine learning",
      "Metric (unit)",
      "Operations management",
      "Pattern recognition (psychology)",
      "Programming language",
      "Ranking (information retrieval)",
      "Set (abstract data type)",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Gajić",
        "given_name": "Bojana"
      },
      {
        "surname": "Amato",
        "given_name": "Ariel"
      },
      {
        "surname": "Gatta",
        "given_name": "Carlo"
      }
    ]
  },
  {
    "title": "Dynamic spectral residual superpixels",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107705",
    "abstract": "We consider the problem of segmenting an image into superpixels in the context of k -means clustering, in which we wish to decompose an image into local, homogeneous regions corresponding to the underlying objects. Our novel approach builds upon the widely used Simple Linear Iterative Clustering (SLIC), and incorporate a measure of objects’ structure based on the spectral residual of an image. Based on this combination, we propose a modified initialisation scheme and search metric, which keeps fine-details. This combination leads to better adherence to object boundaries, while preventing unnecessary segmentation of large, uniform areas, and remaining computationally tractable in comparison to other methods. We demonstrate through numerical and visual experiments that our approach outperforms the state-of-the-art techniques.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320305082",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Residual"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Jianchao"
      },
      {
        "surname": "Aviles-Rivero",
        "given_name": "Angelica I."
      },
      {
        "surname": "Heydecker",
        "given_name": "Daniel"
      },
      {
        "surname": "Zhuang",
        "given_name": "Xiaosheng"
      },
      {
        "surname": "Chan",
        "given_name": "Raymond"
      },
      {
        "surname": "Schönlieb",
        "given_name": "Carola-Bibiane"
      }
    ]
  },
  {
    "title": "CPM: A general feature dependency pattern mining framework for contrast multivariate time series",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107711",
    "abstract": "With recent advances in sensor technology, multivariate time series data are becoming extremely large with sophisticated but insightful inter-variable dependency patterns. Mining contrast dependency patterns in controlled experiments can help quantify the differences between control and experimental time series, however, overwhelms practitioners’ capability. Existing methods suffer from determining whether the differences are caused by the intervention or by different states. We propose a novel Contrast Pattern Mining (CPM) framework to find the intervention-related differences by jointly determining and characterizing the dynamic states in both time series via multivariate Gaussian distributions. Under the CPM framework, we not only propose a new covariance-based contrast pattern model, but also integrate our previous proposed partial correlation-based model as a special case. An efficient generic algorithm is developed to optimize various CPM models by adjusting one of the sub-routines. Comprehensive experiments are conducted to analyze the effectiveness, scalability, utility, and interpretability of the proposed framework.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320305148",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Canonical correlation",
      "Computer science",
      "Contrast (vision)",
      "Covariance",
      "Data mining",
      "Database",
      "Dependency (UML)",
      "Feature (linguistics)",
      "Interpretability",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Multivariate statistics",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Scalability",
      "Series (stratigraphy)",
      "Statistics",
      "Time series"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Qingzhe"
      },
      {
        "surname": "Zhao",
        "given_name": "Liang"
      },
      {
        "surname": "Lee",
        "given_name": "Yi-Ching"
      },
      {
        "surname": "Sassan",
        "given_name": "Avesta"
      },
      {
        "surname": "Lin",
        "given_name": "Jessica"
      }
    ]
  },
  {
    "title": "Unsupervised learning framework for interest point detection and description via properties optimization",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107808",
    "abstract": "This paper presents an unsupervised interest point detection and description method named Properties Optimization Point (POP), which provides a unified objective to optimize different properties of interest point. First, the proposed objective formulates the interest point set as a latent variable, which is flexible to integrate different properties. With the latent variable, the probability formulations are designed for four traditional properties (sparsity, repeatability, invariability, discriminability). Second, a novel property termed as informativeness indicating the information complexity of a local area is designed to determine the areas containing high information, from which interest points are encouraged to be extracted. Third, an efficient approximate Expectation Maximization is proposed to optimize the non-differentiable objective which integrates the above five properties. Finally, POP is instantiated with fully convolutional networks. Experimental results demonstrate that POP outperforms state-of-the-art methods on a number of image matching benchmarks containing both planar and non-planar scenes.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320306117",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Differentiable function",
      "Edge detection",
      "Epistemology",
      "Geometry",
      "Image (mathematics)",
      "Image processing",
      "Interest point detection",
      "Latent variable",
      "Matching (statistics)",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Maximization",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Point (geometry)",
      "Point of interest",
      "Point set registration",
      "Programming language",
      "Property (philosophy)",
      "Set (abstract data type)",
      "Statistics",
      "Variable (mathematics)"
    ],
    "authors": [
      {
        "surname": "Yan",
        "given_name": "Pei"
      },
      {
        "surname": "Tan",
        "given_name": "Yihua"
      },
      {
        "surname": "Tai",
        "given_name": "Yuan"
      },
      {
        "surname": "Wu",
        "given_name": "Dongrui"
      },
      {
        "surname": "Luo",
        "given_name": "Hanbin"
      },
      {
        "surname": "Hao",
        "given_name": "Xiaolong"
      }
    ]
  },
  {
    "title": "GraphAIR: Graph representation learning with neighborhood aggregation and interaction",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107745",
    "abstract": "Graph representation learning is of paramount importance for a variety of graph analytical tasks, ranging from node classification to community detection. Recently, graph convolutional networks (GCNs) have been successfully applied for graph representation learning. These GCNs generate node representation by aggregating features from the neighborhoods, which follows the “neighborhood aggregation” scheme. In spite of having achieved promising performance on various tasks, existing GCN-based models have difficulty in well capturing complicated non-linearity of graph data. In this paper, we first theoretically prove that coefficients of the neighborhood interacting terms are relatively small in current models, which explains why GCNs barely outperforms linear models. Then, in order to better capture the complicated non-linearity of graph data, we present a novel GraphAIR framework which models the neighborhood interaction in addition to neighborhood aggregation. Comprehensive experiments conducted on benchmark tasks including node classification and link prediction using public datasets demonstrate the effectiveness of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320305483",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Data mining",
      "Engineering",
      "External Data Representation",
      "Feature learning",
      "Geodesy",
      "Geography",
      "Graph",
      "Law",
      "Linearity",
      "Machine learning",
      "Node (physics)",
      "Physics",
      "Political science",
      "Politics",
      "Quantum mechanics",
      "Ranging",
      "Representation (politics)",
      "Structural engineering",
      "Telecommunications",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Hu",
        "given_name": "Fenyu"
      },
      {
        "surname": "Zhu",
        "given_name": "Yanqiao"
      },
      {
        "surname": "Wu",
        "given_name": "Shu"
      },
      {
        "surname": "Huang",
        "given_name": "Weiran"
      },
      {
        "surname": "Wang",
        "given_name": "Liang"
      },
      {
        "surname": "Tan",
        "given_name": "Tieniu"
      }
    ]
  },
  {
    "title": "EACOFT: An energy-aware correlation filter for visual tracking",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107766",
    "abstract": "Correlation filter based trackers attribute to its calculation in the frequency domain can efficiently locate targets in a relatively fast speed. This characteristic however also limits its generalization in some specific scenarios. The reasons that they still fail to achieve superior performance to state-of-the-art (SOTA) trackers are possibly due to two main aspects. The first is that while tracking the objects whose energy is lower than the background, the tracker may occur drift or even lose the target. The second is that the biased samples may be inevitably selected for model training, which can easily lead to inaccurate tracking. To tackle these shortcomings, a novel energy-aware correlation filter (EACOFT) based tracking method is proposed, in our approach the energy between the foreground and the background is adaptively balanced, which enables the target of interest always having a higher energy than its background. The samples’ qualities are also evaluated in real time, which ensures that the samples used for template training are always helpful with tracking. In addition, we also propose an optimal bottom-up and top-down combined strategy for template training, which plays an important role in improving both the effectiveness and robustness of tracking. As a result, our approach achieves a great improvement on the basis of the baseline tracker, especially under the background clutter and fast motion challenges. Extensive experiments over multiple tracking benchmarks demonstrate the superior performance of our proposed methodology in comparison to a number of the SOTA trackers.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320305690",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "BitTorrent tracker",
      "Chemistry",
      "Clutter",
      "Computer science",
      "Computer vision",
      "Correlation",
      "Energy (signal processing)",
      "Eye tracking",
      "Filter (signal processing)",
      "Gene",
      "Geometry",
      "Mathematics",
      "Pedagogy",
      "Psychology",
      "Radar",
      "Robustness (evolution)",
      "Statistics",
      "Telecommunications",
      "Tracking (education)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Qiaoyuan"
      },
      {
        "surname": "Ren",
        "given_name": "Jinchang"
      },
      {
        "surname": "Wang",
        "given_name": "Yuru"
      },
      {
        "surname": "Wu",
        "given_name": "Yuanbo"
      },
      {
        "surname": "Sun",
        "given_name": "Haijiang"
      },
      {
        "surname": "Zhao",
        "given_name": "Huimin"
      }
    ]
  },
  {
    "title": "Extending the Beta divergence to complex values",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2020.11.005",
    "abstract": "Various information-theoretic divergences have been proposed for the cost function in tasks such as matrix factorization and clustering. One class of divergence is called the Beta divergence. By varying a real-valued parameter β , the Beta divergence connects several well-known divergences, such as the Euclidean distance, Kullback-Leibler divergence, and Itakura-Saito divergence. Unfortunately, the Beta divergence is properly defined only for positive real values, hindering its use for measuring distances between complex-valued data points. We define a new divergence, the Complex Beta divergence, that operates on complex values, and show that it coincides with the standard Beta divergence when the data is restricted to be in phase. Moreover, we show that different values of β place different penalties on errors in magnitude and phase.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520304104",
    "keywords": [
      "Algorithm",
      "Applied mathematics",
      "Biology",
      "Cluster analysis",
      "Divergence (linguistics)",
      "Euclidean distance",
      "Evolutionary biology",
      "Factorization",
      "Function (biology)",
      "Geometry",
      "Kullback–Leibler divergence",
      "Linguistics",
      "Mathematics",
      "Philosophy",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Vaz",
        "given_name": "Colin"
      },
      {
        "surname": "Narayanan",
        "given_name": "Shrikanth"
      }
    ]
  },
  {
    "title": "Type-reduced vague possibilistic fuzzy clustering for medical images",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107784",
    "abstract": "Soft computing provides the framework for dealing with the uncertainty and imprecision inherent in real-life applications. Soft computing has become a long-standing notable paradigm for medical image processing. A typical fuzzy clustering uses the fuzzy membership function. Nevertheless, there is an alternative membership representation, known as typicality or possibilistic membership. Unlike fuzzy membership that is probabilistic in nature, typicality represents an absolute membership and it is the degree of belonging of an object to a class that does not depend on its distances from the other classes. However, both fuzzy membership and typicality play important role in assigning membership to an object. This study proposes a novel clustering model that creates a vague environment enriched with the concept of fuzzy membership and typicality, while the use of type-reduction plays an essential role in capturing all the vagueness present in the data set. The proposed model is called type-reduced vague possibilistic fuzzy clustering (TVPFC), and we use MRI images to demonstrate its superior robustness over that of FCM (fuzzy c-means), PCM (possibilistic c-means), VCM (vague c-means) and IPFCM (interval-valued possibilistic fuzzy c-means).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320305872",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Defuzzification",
      "Fuzzy classification",
      "Fuzzy clustering",
      "Fuzzy logic",
      "Fuzzy number",
      "Fuzzy set",
      "Fuzzy set operations",
      "Machine learning",
      "Mathematics",
      "Membership function",
      "Pattern recognition (psychology)",
      "Type-2 fuzzy sets and systems",
      "Vagueness"
    ],
    "authors": [
      {
        "surname": "Bose",
        "given_name": "Ankita"
      },
      {
        "surname": "Mali",
        "given_name": "Kalyani"
      }
    ]
  },
  {
    "title": "NMF with feature relationship preservation penalty term for clustering problems",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107814",
    "abstract": "The method proposed in this paper belongs to the family of orthogonal non-negative matrix factorization (ONMF) methods designed to solve clustering problems. Unlike some existing ONMF methods that explicitly constrain the orthogonality of the coefficient matrix in the cost function to derive their clustering models, the proposed method integrates it implicitly, so that it results in a new optimization model with a penalty term. The latter is added to impose a scale relationship between the scatter of the cluster centroids and that of the data points. The solution of the new model involves deriving a new parametrized update scheme for the basis matrix, which makes it possible to improve the performance of the clustering by adjusting a parameter. The proposed clustering algorithm, which we call “pairwise Feature Relationship preservation-based NMF” (FR-NMF), is evaluated on several real-life and synthetic datasets and compared to eight existing NMF-based clustering models. The results obtained show the effectiveness of the proposed algorithm.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321000017",
    "keywords": [
      "Artificial intelligence",
      "Centroid",
      "Cluster analysis",
      "Computer science",
      "Correlation clustering",
      "Data mining",
      "Eigenvalues and eigenvectors",
      "Feature (linguistics)",
      "Geometry",
      "Linguistics",
      "Mathematics",
      "Matrix decomposition",
      "Non-negative matrix factorization",
      "Orthogonality",
      "Pairwise comparison",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Term (time)"
    ],
    "authors": [
      {
        "surname": "Hedjam",
        "given_name": "Rachid"
      },
      {
        "surname": "Abdesselam",
        "given_name": "Abdelhamid"
      },
      {
        "surname": "Melgani",
        "given_name": "Farid"
      }
    ]
  },
  {
    "title": "Topic-aware latent models for representation learning on networks",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.01.006",
    "abstract": "Network representation learning (NRL) methods have received significant attention over the last years thanks to their success in several graph analysis problems, including node classification, link prediction and clustering. Such methods aim to map each vertex of the network into a low dimensional space in a way that the structural information of the network is preserved. Of particular interest are methods based on random walks; such methods transform the network into a collection of node sequences, aiming to learn node representations by predicting the context of each node within the sequence. In this paper, we introduce TNE, a generic framework to enhance the embeddings of nodes acquired by means of random walk-based approaches with topic-based information. Similar to the concept of topical word embeddings in Natural Language Processing, the proposed model first assigns each node to a latent community with the favor of various statistical graph models and community detection methods, and then learns the enhanced topic-aware representations. We evaluate our methodology in two downstream tasks: node classification and link prediction. The experimental results demonstrate that by incorporating node and community embeddings, we are able to outperform widely-known baseline NRL models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521000167",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Cluster analysis",
      "Computer science",
      "Conditional random field",
      "Context (archaeology)",
      "Data mining",
      "Engineering",
      "Graph",
      "Law",
      "Machine learning",
      "Mathematics",
      "Node (physics)",
      "Paleontology",
      "Political science",
      "Politics",
      "Random walk",
      "Representation (politics)",
      "Statistics",
      "Structural engineering",
      "Theoretical computer science",
      "Topic model"
    ],
    "authors": [
      {
        "surname": "Çelikkanat",
        "given_name": "Abdulkadir"
      },
      {
        "surname": "Malliaros",
        "given_name": "Fragkiskos D."
      }
    ]
  },
  {
    "title": "Deep residual pooling network for texture recognition",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107817",
    "abstract": "Current deep learning-based texture recognition methods extract spatial orderless features from pre-trained deep learning models that are trained on large-scale image datasets. These methods either produce high dimensional features or have multiple steps like dictionary learning, feature encoding and dimension reduction. In this paper, we propose a novel end-to-end learning framework that not only overcomes these limitations, but also demonstrates faster learning. The proposed framework incorporates a residual pooling layer consisting of a residual encoding module and an aggregation module. The residual encoder preserves the spatial information for improved feature learning and the aggregation module generates orderless feature for classification through a simple averaging. The feature has the lowest dimension among previous deep texture recognition approaches, yet it achieves state-of-the-art performance on benchmark texture recognition datasets such as FMD, DTD, 4D Light and one industry dataset used for metal surface anomaly detection. Additionally, the proposed method obtains comparable results on the MIT-Indoor scene recognition dataset. Our codes are available at https://github.com/maoshangbo/DRP-Texture-Recognition.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321000042",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Deep learning",
      "Encoder",
      "Encoding (memory)",
      "Feature (linguistics)",
      "Feature extraction",
      "Feature learning",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Linguistics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pooling",
      "Residual",
      "Texture (cosmology)"
    ],
    "authors": [
      {
        "surname": "Mao",
        "given_name": "Shangbo"
      },
      {
        "surname": "Rajan",
        "given_name": "Deepu"
      },
      {
        "surname": "Chia",
        "given_name": "Liang Tien"
      }
    ]
  },
  {
    "title": "Semi-supervised blockwisely architecture search for efficient lightweight generative adversarial network",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107794",
    "abstract": "In the field of computer vision, methods that use fully supervised learning and fixed deep network structures need to be improved. Currently, many studies are devoted to designing neural architecture search methods to use neural networks in a more flexible way. However, most of these methods use fully supervised learning at the cost of extraordinary GPU training time. In view of the above problems, we propose a semi-supervised generative adversarial network and search network architecture based on block structure. Use real pictures and generated pictures with corresponding real tags and pseudo tags for training, to achieve the purpose of semi-supervised learning. By setting the layer’s hyperparameters to a variable and flexible stacking block structure, network architecture search is achieved. The proposed method realizes image generation and extends to image classification. In the experimental results in Section 4, the training time is greatly reduced and the model performance is improved, which illustrates the efficiency of our method. The code can be found in https://github.com/AICV-CUMT/STASGAN.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320305975",
    "keywords": [
      "Architecture",
      "Art",
      "Artificial intelligence",
      "Artificial neural network",
      "Block (permutation group theory)",
      "Code (set theory)",
      "Computer science",
      "Computer security",
      "Deep learning",
      "Generative grammar",
      "Geometry",
      "Hyperparameter",
      "Machine learning",
      "Mathematics",
      "Network architecture",
      "Pattern recognition (psychology)",
      "Programming language",
      "Set (abstract data type)",
      "Supervised learning",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Man"
      },
      {
        "surname": "Zhou",
        "given_name": "Yong"
      },
      {
        "surname": "Zhao",
        "given_name": "Jiaqi"
      },
      {
        "surname": "Xia",
        "given_name": "Shixiong"
      },
      {
        "surname": "Wang",
        "given_name": "Jiaqi"
      },
      {
        "surname": "Huang",
        "given_name": "Zizheng"
      }
    ]
  },
  {
    "title": "Facial expression recognition using distance and shape signature features",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2017.06.018",
    "abstract": "Distance and Shape signature features in human faces offer formidable significance in recognizing facial expressions. Identifying appropriate landmarks is a crucial as well as challenging issue in the field of expression recognition of human faces. Appearance model has been found useful to detect the salient landmarks on human faces. These salient landmarks induce a grid on the human face along with the formation of possible triangles joining the grid. Normalized distance and shape signatures are determined from the grid. Distance signature as well as shape signature find respective stability indices which play important role to recognize the facial expressions. Statistical measures such as range, moment, skewness, kurtosis and entropy are calculated from normalized distance and shape signature pair to supplement the feature set. This enhanced feature set is fed into a Multilayer Perceptron (MLP) to arrive at different expression categories encompassing anger, sadness, fear, disgust, surprise and happy. We investigated our proposed system on Cohn-Kanade (CK+), JAFFE, MMI and MUG databases to conduct and validate our experiment and establish its performance superiority over other existing competitors.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865517302246",
    "keywords": [
      "Anger",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Entropy (arrow of time)",
      "Facial expression",
      "Feature (linguistics)",
      "Kurtosis",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Psychiatry",
      "Psychology",
      "Quantum mechanics",
      "Sadness",
      "Salient",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Barman",
        "given_name": "Asit"
      },
      {
        "surname": "Dutta",
        "given_name": "Paramartha"
      }
    ]
  },
  {
    "title": "Self-attention-based conditional random fields latent variables model for sequence labeling",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.02.008",
    "abstract": "To process data like text and speech, Natural Language Processing (NLP) is a valuable tool. As on of NLP’s upstream tasks, sequence labeling is a vital part of NLP through techniques like text classification, machine translation, and sentiment analysis. In this paper, our focus is on sequence labeling where we assign semantic labels within input sequences. We present two novel frameworks, namely SA-CRFLV-I and SA-CRFLV-II, that use latent variables within random fields. These frameworks make use of an encoding schema in the form of a latent variable to be able to capture the latent structure in the observed data. SA-CRFLV-I shows the best performance at the sentence level whereas SA-CRFLV-II works best at the word level. In our in-depth experimental results, we compare our frameworks with 4 well-known sequence prediction methodologies which include NER, reference parsing, chunking as well as POS tagging. The proposed frameworks are shown to have better performance in terms of many well-known metrics.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521000635",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Chunking (psychology)",
      "Computer science",
      "Conditional random field",
      "Economics",
      "Genetics",
      "Latent semantic analysis",
      "Latent variable",
      "Linguistics",
      "Machine translation",
      "Management",
      "Natural language processing",
      "Parsing",
      "Philosophy",
      "Sentence",
      "Sequence (biology)",
      "Sequence labeling",
      "Task (project management)",
      "Word (group theory)"
    ],
    "authors": [
      {
        "surname": "Shao",
        "given_name": "Yinan"
      },
      {
        "surname": "Lin",
        "given_name": "Jerry Chun-Wei"
      },
      {
        "surname": "Srivastava",
        "given_name": "Gautam"
      },
      {
        "surname": "Jolfaei",
        "given_name": "Alireza"
      },
      {
        "surname": "Guo",
        "given_name": "Dongdong"
      },
      {
        "surname": "Hu",
        "given_name": "Yi"
      }
    ]
  },
  {
    "title": "Towards purchase prediction: A transaction-based setting and a graph-based method leveraging price information",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107824",
    "abstract": "Targeting at boosting business revenue, purchase prediction based on user behavior is crucial to e-commerce. However, it is not a well-explored topic due to a lack of relevant datasets. Specifically, no public dataset provides both price and discount information varying on time, which play an essential role in the user’s decision making. Besides, existing learn-to-rank methods cannot explicitly predict the purchase possibility for a specific user-item pair. In this paper, we propose a two-step graph-based model, where the graph model is applied in the first step to learn representations of both users and items over click-through data, and the second step is a classifier incorporating the price information of each transaction record. To evaluate the model performance, we propose a transaction-based framework focusing on the purchased items and their context clicks, which contain items that a user is interested in but fails to choose after comparison. Our experiments show that exploiting the price and discount information can significantly enhance prediction accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032100011X",
    "keywords": [
      "Accounting",
      "Artificial intelligence",
      "Boosting (machine learning)",
      "Business",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Database",
      "Database transaction",
      "Graph",
      "Machine learning",
      "Revenue",
      "Theoretical computer science",
      "Transaction data"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Zongxi"
      },
      {
        "surname": "Xie",
        "given_name": "Haoran"
      },
      {
        "surname": "Xu",
        "given_name": "Guandong"
      },
      {
        "surname": "Li",
        "given_name": "Qing"
      },
      {
        "surname": "Leng",
        "given_name": "Mingming"
      },
      {
        "surname": "Zhou",
        "given_name": "Chi"
      }
    ]
  },
  {
    "title": "Action recognition using kinematics posture feature on 3D skeleton joint locations",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.02.013",
    "abstract": "Action recognition is a very widely explored research area in computer vision and related fields. We propose Kinematics Posture Feature (KPF) extraction from 3D joint positions based on skeleton data for improving the performance of action recognition. In this approach, we consider the skeleton 3D joints as kinematics sensors. We propose Linear Joint Position Feature (LJPF) and Angular Joint Position Feature (AJPF) based on 3D linear joint positions and angles between bone segments. We then combine these two kinematics features for each video frame for each action to create the KPF feature sets. These feature sets encode the variation of motion in the temporal domain as if each body joint represents kinematics position and orientation sensors. In the next stage, we process the extracted KPF feature descriptor by using a low pass filter, and segment them by using sliding windows with optimized length. This concept resembles the approach of processing kinematics sensor data. From the segmented windows, we compute the Position-based Statistical Feature (PSF). These features consist of temporal domain statistical features (e.g., mean, standard deviation, variance, etc.). These statistical features encode the variation of postures (i.e., joint positions and angles) across the video frames. For performing classification, we explore Support Vector Machine (Linear), RNN, CNNRNN, and ConvRNN model. The proposed PSF feature sets demonstrate prominent performance in both statistical machine learning- and deep learning-based models. For evaluation, we explore five benchmark datasets namely UTKinect-Action3D, Kinect Activity Recognition Dataset (KARD), MSR 3D Action Pairs, Florence 3D, and Office Activity Dataset (OAD). To prevent overfitting, we consider the leave-one-subject-out framework as the experimental setup and perform 10-fold cross-validation. Our approach outperforms several existing methods in these benchmark datasets and achieves very promising classification performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521000751",
    "keywords": [
      "Architectural engineering",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Classical mechanics",
      "Computer science",
      "Computer vision",
      "Economics",
      "Engineering",
      "Feature (linguistics)",
      "Feature extraction",
      "Feature vector",
      "Finance",
      "Geodesy",
      "Geography",
      "Joint (building)",
      "Kinematics",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Position (finance)",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Ahad",
        "given_name": "Md Atiqur Rahman"
      },
      {
        "surname": "Ahmed",
        "given_name": "Masud"
      },
      {
        "surname": "Das Antar",
        "given_name": "Anindya"
      },
      {
        "surname": "Makihara",
        "given_name": "Yasushi"
      },
      {
        "surname": "Yagi",
        "given_name": "Yasushi"
      }
    ]
  },
  {
    "title": "UFOD: An AutoML framework for the construction, comparison, and combination of object detection models",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.01.022",
    "abstract": "Object detection models based on deep learning techniques have been successfully applied in several contexts; however, non-expert users might find challenging the use of these techniques due to several reasons, including the necessity of trying different algorithms implemented in heterogeneous libraries, the configuration of hyperparameters, the lack of support of many state-of-the-art algorithms for training them on custom datasets, or the variety of metrics employed to evaluate detection algorithms. These challenges have been tackled by the development of UFOD, an automated machine learning framework that trains several object detection algorithms (using different underlying frameworks and libraries), compares them, and finally selects the best model or ensembles them. Currently, the most well-known object detection algorithms have been included in our system, and new methods can be easily incorporated thanks to a high-level API. UFOD is available at https://github.com/ManuGar/UFOD/",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521000416",
    "keywords": [
      "Artificial intelligence",
      "Cartography",
      "Computer science",
      "Data mining",
      "Geography",
      "Hyperparameter",
      "Machine learning",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Train",
      "Variety (cybernetics)"
    ],
    "authors": [
      {
        "surname": "García-Domínguez",
        "given_name": "Manuel"
      },
      {
        "surname": "Domínguez",
        "given_name": "César"
      },
      {
        "surname": "Heras",
        "given_name": "Jónathan"
      },
      {
        "surname": "Mata",
        "given_name": "Eloy"
      },
      {
        "surname": "Pascual",
        "given_name": "Vico"
      }
    ]
  },
  {
    "title": "Low-rank tensor ring learning for multi-linear regression",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107753",
    "abstract": "The emergence of large-scale data demands new regression models with multi-dimensional coefficient arrays, known as tensor regression models. The recently proposed tensor ring decomposition has interesting properties of enhanced representation and compression capability, cyclic permutation invariance and balanced tensor ring rank, which may lead to efficient computation and fewer parameters in regression problems. In this paper, a generally multi-linear tensor-on-tensor regression model is proposed that the coefficient array has a low-rank tensor ring structure, which is termed tensor ring ridge regression (TRRR). Two optimization models are developed for the TRRR problem and solved by different algorithms: the tensor factorization based one is solved by alternating least squares algorithm, and accelerated by a fast network contraction, while the rank minimization based one is addressed by the alternating direction method of multipliers algorithm. Comparative experiments, including Spatio-temporal forecasting tasks and 3D reconstruction of human motion capture data from its temporally synchronized video sequences, demonstrate the enhanced performance of our algorithms over existing state-of-the-art ones, especially in terms of training time.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320305562",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Combinatorics",
      "Computation",
      "Computer science",
      "Geometry",
      "Linear least squares",
      "Linear regression",
      "Mathematical optimization",
      "Mathematics",
      "Rank (graph theory)",
      "Singular value decomposition",
      "Statistics",
      "Tensor (intrinsic definition)",
      "Tensor decomposition",
      "Tucker decomposition"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Jiani"
      },
      {
        "surname": "Zhu",
        "given_name": "Ce"
      },
      {
        "surname": "Long",
        "given_name": "Zhen"
      },
      {
        "surname": "Huang",
        "given_name": "Huyan"
      },
      {
        "surname": "Liu",
        "given_name": "Yipeng"
      }
    ]
  },
  {
    "title": "Reconstruction regularized low-rank subspace learning for cross-modal retrieval",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107813",
    "abstract": "With the rapid increase of multi-modal data through the internet, cross-modal matching or retrieval has received much attention recently. It aims to use one type of data as query and retrieve results from the database of another type. For this task, the most popular approach is the latent subspace learning, which learns a shared subspace for multi-modal data, so that we can efficiently measure cross-modal similarity. Instead of adopting traditional regularization terms, we hope that the latent representation could recover the multi-modal information, which works as a reconstruction regularization term. Besides, we assume that different view features for samples of the same category share the same representation in the latent space. Since the number of classes is generally smaller than the number of samples and the feature dimension, therefore the latent feature matrix of training instances should be low-rank. We try to learn the optimal latent representation, and propose a reconstruction based term to recover original multi-modal data and a low-rank term to regularize the learning of subspace. Our method can deal with both supervised and unsupervised cross-modal retrieval tasks. For those situations where the semantic labels are not easy to obtain, our proposed method can also work very well. We propose an efficient algorithm to optimize our framework. To evaluate the performance of our method, we conduct extensive experiments on various datasets. The experimental results show that our proposed method is very efficient and outperforms the state-of-the-art subspace learning approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320306166",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Combinatorics",
      "Computer science",
      "Data mining",
      "Feature learning",
      "Law",
      "Machine learning",
      "Mathematics",
      "Modal",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Polymer chemistry",
      "Rank (graph theory)",
      "Regularization (linguistics)",
      "Representation (politics)",
      "Subspace topology"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Jianlong"
      },
      {
        "surname": "Xie",
        "given_name": "Xingxu"
      },
      {
        "surname": "Nie",
        "given_name": "Liqiang"
      },
      {
        "surname": "Lin",
        "given_name": "Zhouchen"
      },
      {
        "surname": "Zha",
        "given_name": "Hongbin"
      }
    ]
  },
  {
    "title": "Weakly-supervised video object localization with attentive spatio-temporal correlation",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.02.015",
    "abstract": "Weakly-supervised video object localization is a challenging yet important task. The system should spatially localize the object of interest in videos, where only the descriptive sentences and their corresponding video segments are given in the training stage. Recent efforts propose to apply image-based Multiple Instance Learning (MIL) theory in this video task, and propagate the supervision from the video into frames by applying different frame-weighting strategies. Despite their promising progress, the spatio-temporal correlation between different object regions in videos has been largely ignored. To fill the research gap, in this work we introduce a simple but effective feature expression and aggregation framework, which utilizes the self-attention mechanism to capture the latent spatio-temporal correlation between multimodal object features and design a multimodal interaction module to model the similarity between the semantic query in sentences and the object regions in videos. We conduct extensive experimental evaluation on the YouCookII and ActivityNet-Entities datasets, which demonstrates significant improvements over multiple competitive baselines.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521000775",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Correlation",
      "Economics",
      "Feature (linguistics)",
      "Frame (networking)",
      "Geometry",
      "Image (mathematics)",
      "Linguistics",
      "Machine learning",
      "Management",
      "Mathematics",
      "Medicine",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Radiology",
      "Similarity (geometry)",
      "Task (project management)",
      "Telecommunications",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Mingui"
      },
      {
        "surname": "Cui",
        "given_name": "Di"
      },
      {
        "surname": "Wu",
        "given_name": "Lifang"
      },
      {
        "surname": "Jian",
        "given_name": "Meng"
      },
      {
        "surname": "Chen",
        "given_name": "Yukun"
      },
      {
        "surname": "Wang",
        "given_name": "Dong"
      },
      {
        "surname": "Liu",
        "given_name": "Xu"
      }
    ]
  },
  {
    "title": "End-to-end video text detection with online tracking",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107791",
    "abstract": "Text in videos usually acts as important semantic cues, which is helpful to video analysis. Video text detection is considered as one of the most difficult tasks in document analysis due to the following two challenges: 1) the difficulties caused by video scenes, i.e., motion blur, illumination changes, and occlusion; 2) the properties of text including variants of fonts, languages, orientations, and shapes. Most existing methods try to improve the video text detection through video text tracking, but treat these two tasks separately. This can significantly increase the amount of calculations and cannot take full advantage of the supervisory information of both tasks. In this work, we introduce explainable descriptor, combines appearance, geometry and PHOC features, to establish a bridge between detection and tracking and build an end-to-end video text detection model with online tracking to address these challenges together. By integrating these two branches into one trainable framework, they can promote each other and the computational cost is significantly reduced. Besides, the introduce explainable descriptor also make our end-to-end model have inherent interpretability. Experiments on existing video text benchmarks including ICDAR 2013 Video, DOST, Minetto and YVT verify the role of explainable descriptors in improving model expression ability and the proposed method significantly outperforms state-of-the-art methods. Our method improves F-score by more than 2 % on all datasets and achieves 81.52 % on the MOTA of the Minetto dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032030594X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "End-to-end principle",
      "Image (mathematics)",
      "Interpretability",
      "Motion (physics)",
      "Motion blur",
      "Pedagogy",
      "Psychology",
      "Text detection",
      "Tracking (education)",
      "Video processing",
      "Video retrieval",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Yu",
        "given_name": "Hongyuan"
      },
      {
        "surname": "Huang",
        "given_name": "Yan"
      },
      {
        "surname": "Pi",
        "given_name": "Lihong"
      },
      {
        "surname": "Zhang",
        "given_name": "Chengquan"
      },
      {
        "surname": "Li",
        "given_name": "Xuan"
      },
      {
        "surname": "Wang",
        "given_name": "Liang"
      }
    ]
  },
  {
    "title": "Exploring a unified low rank representation for multi-focus image fusion",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107752",
    "abstract": "Recent years have witnessed a trend that uses image representation models, including sparse representation (SR), low-rank representation (LRR) and their variants for multi-focus image fusion. Despite the thrilling preliminary results, existing methods conduct the fusion patch by patch, leading to insufficient consideration of the spatial consistency among the image patches within a local region or an object. As a result, not only the spatial artifacts are easily introduced to the fused image but also the “jagged” artifacts frequently arise on the boundaries between the focused regions and the de-focused regions, which is an inherent problem in these patch-based fusion methods.Aiming to address the above problems, we propose, in this paper,a new multi-focus image fusion method integrating super-pixel clustering and a unified LRR (ULRR) model. The entire algorithm is carried out in three steps. In the first step, the source image is segmented into a few super-pixels with irregular sizes, rather than patches with regular sizes, to diminish the “jagged” artifacts and meanwhile to preserve the boundaries of objects on the fused image. Secondly, a super-pixel clustering-based fusion strategy is employed to further reduce the spatial artifacts in the fused images. This is achieved by using a proposed ULRR model, which imposes the low-rank constraints onto each super-pixel cluster.Thisis apparently more reasonable for those images with complicated scenes. Moreover, a Laplacianregularization term is incorporated in the proposed ULRR model to ensure the spatial consistency among the super-pixels with the same cluster. Finally, a measure of focus for each super-pixel is defined to seek the focused as well as de-focused regions in thesource image via jointly using representation coefficients and sparse errors derived from the proposed ULRR model. Extensive experiments have been conducted and the results demonstrate the superiorities of the proposed fusion method in diminishing the spatial artifactsin the fused image and the “jagged” boundary artifacts between the focused and de-focused regions, compared to the state-of-the-art fusion algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320305550",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Consistency (knowledge bases)",
      "Focus (optics)",
      "Fusion",
      "Image (mathematics)",
      "Image fusion",
      "Law",
      "Linguistics",
      "Mathematics",
      "Object (grammar)",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Pixel",
      "Political science",
      "Politics",
      "Rank (graph theory)",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Qiang"
      },
      {
        "surname": "Wang",
        "given_name": "Fan"
      },
      {
        "surname": "Luo",
        "given_name": "Yongjiang"
      },
      {
        "surname": "Han",
        "given_name": "Jungong"
      }
    ]
  },
  {
    "title": "Gammadion binary pattern of Shearlet coefficients (GBPSC): An illumination-invariant heterogeneous face descriptor",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.01.028",
    "abstract": "This paper presents a novel face image descriptor called Gammadion Binary Pattern of Shearlet Coefficients (GBPSC) for illumination and noise invariant, homogeneous and heterogeneous face recognition. Exploiting the energy concentration property of the Digital Shearlet Transform, an efficient illumination and noise invariant feature extractor has been devised. Finally, inspired by the Gammadion structure, a robust multi-directional local binary pattern named Gammadion Binary Pattern (GBP) has been proposed. GBP is applied on the previously extracted illumination and noise invariant feature map to generate the GBPSC images. Recognition results on Extended Yale B and TUFTS dataset indicate the primacy of the proposed scheme in terms of common feature representation under varying illumination, and modality. Furthermore, the merger of the proposed GBPSC and Convolutional Neural Network (CNN) consistently outperforms other state-of-the art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521000477",
    "keywords": [
      "Arithmetic",
      "Artificial intelligence",
      "Binary number",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Facial recognition system",
      "Feature (linguistics)",
      "Feature extraction",
      "Histogram",
      "Image (mathematics)",
      "Invariant (physics)",
      "Linguistics",
      "Local binary patterns",
      "Mathematical physics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Shearlet"
    ],
    "authors": [
      {
        "surname": "Koley",
        "given_name": "Subhadeep"
      },
      {
        "surname": "Roy",
        "given_name": "Hiranmoy"
      },
      {
        "surname": "Bhattacharjee",
        "given_name": "Debotosh"
      }
    ]
  },
  {
    "title": "Multi-view discriminant analysis with sample diversity for ECG biometric recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.01.027",
    "abstract": "Currently, electrocardiogram (ECG) biometric recognition is a novel research trend, and many methods have been developed. Due to the influence of physical and psychological activities, there are heartbeats diversities of the same person. However, the existing ECG biometric recognition methods do not make use of sample diversity information. In this paper, we present a multi-view discriminant analysis approach in the consideration of sample diversity for ECG biometric recognition. Firstly, we propose a method of generating multiple views by using single lead ECG signal. Secondly, we present a multi-views learning framework, which takes sample diversity into account to generate a more discriminative subspace. Thirdly, to obtain a more robust solution, we introduce a denoising constraint to learn the relationships between different views, which can create a stable representation against ECG noise. At last, experimental results demonstrate that compared with the state-of-the-art methods on four databases, the proposed method can achieve competitive performance compared to state-of-the-art ECG biometric recognition methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521000465",
    "keywords": [
      "Artificial intelligence",
      "Biometrics",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Constraint (computer-aided design)",
      "Discriminant",
      "Discriminative model",
      "Geometry",
      "Law",
      "Linear discriminant analysis",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Sample (material)",
      "Speech recognition",
      "Subspace topology"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Yuwen"
      },
      {
        "surname": "Yang",
        "given_name": "Gongping"
      },
      {
        "surname": "Wang",
        "given_name": "Kuikui"
      },
      {
        "surname": "Yin",
        "given_name": "Yilong"
      }
    ]
  },
  {
    "title": "Scene-Graph-Guided message passing network for dense captioning",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.01.024",
    "abstract": "Dense captioning task aims to both localize and describe salient regions in images with natural languages. It can benefit from the rich visual concepts, including objects, pair-wise relationships and so on. However, due to the challenging combinatorial complexity of formulating < subject-predicate-object > triplets, very little work has been done to integrate them into the dense captioning task. Inspired by the recent success in scene graph generation for object and relationship detections, we propose a scene-graph-guided message passing network for dense caption generation. We first exploit message passing between objects and their relationships with a feature refining structure. Moreover, we formulate the message passing as the inter-connected visual concept generation problem while the objective function of scene graph generation is used to guide the region feature learning. Scene graph guide can propagate the structured knowledge of graph through the concept-region message passing mechanism (CR-MPM), which can improve the regional feature representation. Finally, the refined regional features are encoded by a LSTM-based decoder to generate dense captions. Our model can achieve competing performances on Visual Genome comparing against existing methods. Qualitative experiments also confirm the effect of our model in the dense captioning task.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552100043X",
    "keywords": [
      "Artificial intelligence",
      "Closed captioning",
      "Computer science",
      "Computer security",
      "Exploit",
      "Feature (linguistics)",
      "Graph",
      "Image (mathematics)",
      "Linguistics",
      "Message passing",
      "Philosophy",
      "Programming language",
      "Salient",
      "Theoretical computer science",
      "Visualization"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "An-An"
      },
      {
        "surname": "Wang",
        "given_name": "Yanhui"
      },
      {
        "surname": "Xu",
        "given_name": "Ning"
      },
      {
        "surname": "Liu",
        "given_name": "Shan"
      },
      {
        "surname": "Li",
        "given_name": "Xuanya"
      }
    ]
  },
  {
    "title": "Weakly-supervised action localization via embedding-modeling iterative optimization",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107831",
    "abstract": "Action recognition and localization in untrimmed videos in weakly supervised scenario is a challenging problem of great application prospects. Limited by the information available in video-level labels, it is a promising attempt to fully leverage the instructive knowledge learned on trimmed videos to facilitate analysis of untrimmed videos, considering that there are abundant trimmed videos which are publicly available and well segmented with semantic descriptions. In order to enforce effective trimmed-untrimmed augmentation, this paper presents a novel framework of embedding-modeling iterative optimization network, referred to as IONet. In the proposed method, action classification modeling and shared subspace embedding are learned jointly in an iterative way, so that robust cross-domain knowledge transfer is achieved. With a carefully designed two-stage self-attentive representation learning workflow for untrimmed videos, irrelevant backgrounds are eliminated and fine-grained temporal relevance can be robustly explored. Extensive experiments are conducted on two benchmark datasets, i.e., THUMOS14 and ActivityNet1.3, and experimental results clearly corroborate the efficacy of our method. Source code is available on GitHub . 2 2 https://github.com/HCShi/IONet .",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321000182",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Code (set theory)",
      "Computer science",
      "Database",
      "Domain (mathematical analysis)",
      "Embedding",
      "Feature learning",
      "Geodesy",
      "Geography",
      "Law",
      "Leverage (statistics)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Programming language",
      "Relevance (law)",
      "Representation (politics)",
      "Set (abstract data type)",
      "Source code",
      "Subspace topology",
      "Transfer of learning",
      "Workflow"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Xiao-Yu"
      },
      {
        "surname": "Shi",
        "given_name": "Haichao"
      },
      {
        "surname": "Li",
        "given_name": "Changsheng"
      },
      {
        "surname": "Li",
        "given_name": "Peng"
      },
      {
        "surname": "Li",
        "given_name": "Zekun"
      },
      {
        "surname": "Ren",
        "given_name": "Peng"
      }
    ]
  },
  {
    "title": "Towards a data-driven adaptive anomaly detection system for human activity",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.02.006",
    "abstract": "Research in the field of ambient intelligence allows for the utilisation of different computational models for human activity recognition and abnormality detection to promote independent living and to improve the quality of life for the increasing ageing population. The existing monitoring systems are not adaptive to the overly changing human behavioural routine leading to a high rate of false predictions. An adaptive system pipeline is proposed in this paper for adapting to changes in human behaviour based on data ageing and data dissimilarity forgetting factors. The forgetting factor feature allows adaptation of the model to the current routines of an individual while forgetting outdated behavioural patterns. The data ageing forgetting factor discard old behavioural routine based on the age of the activity data while in the data dissimilarity approach, this is achieved by measuring the similarity of the activity data. Behaviour modelling is achieved using an ensemble of novelty detection models termed as Consensus Novelty Detection Ensemble consisting of One-Class Support Vector Machine, Local Outlier Factor, Robust Covariance Estimation and Isolation Forest. The proposed approach is data-driven and environment-invariant, making it feasible for deployment in heterogeneous environments. A comparative analysis carried out with other abnormality detection models for human activities across two datasets shows that the proposed approach achieved better results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521000611",
    "keywords": [
      "Activity recognition",
      "Anomaly detection",
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Forgetting",
      "Linguistics",
      "Machine learning",
      "Novelty",
      "Novelty detection",
      "Outlier",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Theology"
    ],
    "authors": [
      {
        "surname": "Yahaya",
        "given_name": "Salisu Wada"
      },
      {
        "surname": "Lotfi",
        "given_name": "Ahmad"
      },
      {
        "surname": "Mahmud",
        "given_name": "Mufti"
      }
    ]
  },
  {
    "title": "Improving face recognition performance using TeCS 2 dictionary",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2020.12.022",
    "abstract": "Human mind processes the different primitive components of image signals such as color, shape, texture, and symmetry in a parallel and complex fashion. Deep neural networks aim to learn all these components from the image in an unsupervised manner. However, learning the primitive features is not formally assured in a deep learning formulation, and, adding these features explicitly would improve the performance. Especially in face recognition, humans intuitively and implicitly employ the usage of primitive features such as color, shape, texture, and symmetry of faces. Inspired by this observation, this paper presents a novel approach in building a learning based TeCS 2 space. This space consists of meta-level features obtained from dictionary learning and combining it with task specific deep learning classifiers (such as DenseNet) for face recognition. Confidence based fusion mechanism is presented to supplement the task specific deep learning classifier with the proposed TeCS 2 features. The effectiveness of the proposed framework is evaluated on four benchmark face recognition datasets: (i) Disguised Faces in the Wild (DFW), (ii) Labeled faces in the wild (LFW), (iii) IIITD Plastic Surgery dataset, and (iv) Point and Shoot Challenge (PaSC).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521000209",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Classifier (UML)",
      "Computer science",
      "Deep learning",
      "Face (sociological concept)",
      "Facial recognition system",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Suri",
        "given_name": "Saksham"
      },
      {
        "surname": "Sankaran",
        "given_name": "Anush"
      },
      {
        "surname": "Vatsa",
        "given_name": "Mayank"
      },
      {
        "surname": "Singh",
        "given_name": "Richa"
      }
    ]
  },
  {
    "title": "Momentum contrastive learning for few-shot COVID-19 diagnosis from chest CT images",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107826",
    "abstract": "The current pandemic, caused by the outbreak of a novel coronavirus (COVID-19) in December 2019, has led to a global emergency that has significantly impacted economies, healthcare systems and personal wellbeing all around the world. Controlling the rapidly evolving disease requires highly sensitive and specific diagnostics. While RT-PCR is the most commonly used, it can take up to eight hours, and requires significant effort from healthcare professionals. As such, there is a critical need for a quick and automatic diagnostic system. Diagnosis from chest CT images is a promising direction. However, current studies are limited by the lack of sufficient training samples, as acquiring annotated CT images is time-consuming. To this end, we propose a new deep learning algorithm for the automated diagnosis of COVID-19, which only requires a few samples for training. Specifically, we use contrastive learning to train an encoder which can capture expressive feature representations on large and publicly available lung datasets and adopt the prototypical network for classification. We validate the efficacy of the proposed model in comparison with other competing methods on two publicly available and annotated COVID-19 CT datasets. Our results demonstrate the superior performance of our model for the accurate diagnosis of COVID-19 based on chest CT images.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321000133",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Coronavirus disease 2019 (COVID-19)",
      "Disease",
      "Feature (linguistics)",
      "Infectious disease (medical specialty)",
      "Linguistics",
      "Machine learning",
      "Medicine",
      "Pathology",
      "Philosophy",
      "Radiology",
      "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Xiaocong"
      },
      {
        "surname": "Yao",
        "given_name": "Lina"
      },
      {
        "surname": "Zhou",
        "given_name": "Tao"
      },
      {
        "surname": "Dong",
        "given_name": "Jinming"
      },
      {
        "surname": "Zhang",
        "given_name": "Yu"
      }
    ]
  },
  {
    "title": "An evidential clustering algorithm by finding belief-peaks and disjoint neighborhoods",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107751",
    "abstract": "In this paper, we introduce a new evidential clustering algorithm based on finding the belief-peaks and disjoint neighborhoods, called BPDNEC. The basic idea of BPDNEC is that each cluster center has the highest possibility of becoming a cluster center among its neighborhood and neighborhoods of those cluster centers are disjoint in vector space. Such possibility is measured by the belief notion in framework of evidence theory. By solving an equation related to neighborhood size, the size of such disjoint neighborhoods is determined and those objects having highest belief among their neighborhoods are automatically detected as cluster centers. Finally, a credal partition is created by minimizing an objective function concerning dissimilarity matrix of data objects. Experimental results show that BPDNEC can automatically detect cluster centers and derive an appropriate credal partition for both object data and proximity data. Simulations on synthetic and real-world datasets validate the conclusions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320305549",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Cluster (spacecraft)",
      "Cluster analysis",
      "Combinatorics",
      "Computer science",
      "Disjoint sets",
      "Evolutionary biology",
      "Function (biology)",
      "Mathematics",
      "Partition (number theory)",
      "Programming language"
    ],
    "authors": [
      {
        "surname": "Gong",
        "given_name": "Chaoyu"
      },
      {
        "surname": "Su",
        "given_name": "Zhi-gang"
      },
      {
        "surname": "Wang",
        "given_name": "Pei-hong"
      },
      {
        "surname": "Wang",
        "given_name": "Qian"
      }
    ]
  },
  {
    "title": "Deep neural network oriented evolutionary parametric eye modeling",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107755",
    "abstract": "Comprehensive and accurate eye modeling is crucial to a variety of applications, including human-computer interaction, assistive technologies, and medical diagnosis. However, most studies focus on the localization of one or two components of eyes, such as pupil or iris, lacking a comprehensive eye model. We propose to model an eye image by a set of parametric curves. The set of curves are plotted on an eye image to form a Contour-Eye image. A deep neural network is trained to evaluate the fitness of the Contour-Eye image. Then an evolutionary process is conducted to search the best fitting curve set, guided by the trained deep neural network. Finally, an accurate eye model with optimized parametric curves is obtained. For the algorithm evaluation, a finely annotated eye dataset denoted as FAED-50 is established by us, which contains 2,498 eye images from 50 subjects. The experimental results on the FAED-50 and the relabeled CASIA datasets and comparison with the state-of-the-art methods demonstrate the effectiveness and accuracy of the proposed parametric model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320305586",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Focus (optics)",
      "Geometry",
      "Human eye",
      "Image (mathematics)",
      "Mathematics",
      "Neuroscience",
      "Operating system",
      "Optics",
      "Parametric equation",
      "Parametric model",
      "Parametric statistics",
      "Pattern recognition (psychology)",
      "Physics",
      "Process (computing)",
      "Programming language",
      "Pupil",
      "Set (abstract data type)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Zheng",
        "given_name": "Yang"
      },
      {
        "surname": "Fu",
        "given_name": "Hong"
      },
      {
        "surname": "Li",
        "given_name": "Ruimin"
      },
      {
        "surname": "Hsung",
        "given_name": "Tai-Chiu"
      },
      {
        "surname": "Song",
        "given_name": "Zongxi"
      },
      {
        "surname": "Wen",
        "given_name": "Desheng"
      }
    ]
  },
  {
    "title": "MetaCOVID: A Siamese neural network framework with contrastive loss for n-shot diagnosis of COVID-19 patients",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107700",
    "abstract": "Various AI functionalities such as pattern recognition and prediction can effectively be used to diagnose (recognize) and predict coronavirus disease 2019 (COVID-19) infections and propose timely response (remedial action) to minimize the spread and impact of the virus. Motivated by this, an AI system based on deep meta learning has been proposed in this research to accelerate analysis of chest X-ray (CXR) images in automatic detection of COVID-19 cases. We present a synergistic approach to integrate contrastive learning with a fine-tuned pre-trained ConvNet encoder to capture unbiased feature representations and leverage a Siamese network for final classification of COVID-19 cases. We validate the effectiveness of our proposed model using two publicly available datasets comprising images from normal, COVID-19 and other pneumonia infected categories. Our model achieves 95.6% accuracy and AUC of 0.97 in diagnosing COVID-19 from CXR images even with a limited number of training samples.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320305033",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Coronavirus disease 2019 (COVID-19)",
      "Deep learning",
      "Disease",
      "Encoder",
      "Infectious disease (medical specialty)",
      "Leverage (statistics)",
      "Machine learning",
      "Medicine",
      "Operating system",
      "Pathology",
      "Pattern recognition (psychology)",
      "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)"
    ],
    "authors": [
      {
        "surname": "Shorfuzzaman",
        "given_name": "Mohammad"
      },
      {
        "surname": "Hossain",
        "given_name": "M. Shamim"
      }
    ]
  },
  {
    "title": "Building crack identification and total quality management method based on deep learning",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.01.034",
    "abstract": "The existence of cracks will affect the stability of the building. It is very important to identify and deal with the cracks in time to ensure the safety and stability of the building. Based on the above background, the purpose of this paper is to study the method of building crack recognition and total quality management based on deep learning. This paper focuses on the computer vision technology in artificial intelligence, studies the image classification algorithm and semantic segmentation algorithm based on the deep learning method, and applies it to the field of building crack image analysis. In this paper, we use the deep convolution neural network to design the building image crack classification model and segmentation model, realize the identification and analysis of building cracks, and build a building crack analysis system, which can significantly improve the efficiency of building crack detection. Then, based on the image processing technology, the quantitative analysis of the fracture segmentation results is carried out. Through the basic morphological methods such as corrosion, expansion, opening and closing operations, the segmentation mark map, skeleton map and geometric parameter information of the fracture are obtained, which further provides the maintenance and judgment basis for professional engineers. The experimental results show that compared with FCN, the accuracy of rfcn-a is improved by 5.98%, the precision is improved by 6.07%, and the real and f'score are improved by 3.11% and 6.01%, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521000532",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Botany",
      "Closing (real estate)",
      "Computer science",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Deep learning",
      "Engineering",
      "Field (mathematics)",
      "Fracture (geology)",
      "Geotechnical engineering",
      "Identification (biology)",
      "Law",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Political science",
      "Pure mathematics",
      "Segmentation",
      "Stability (learning theory)"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Xinhua"
      },
      {
        "surname": "Liu",
        "given_name": "Xiujie"
      }
    ]
  },
  {
    "title": "Dynamic relationship identification for abnormality detection on financial time series",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.02.004",
    "abstract": "In this paper, we propose a novel strategy that identifies the dynamic relationship pattern for abnormality detection on financial time series. In particular, we select the basis indices that affect financial time series to discover the spurious relationships and construct a dynamic relationship matrix to model these. Then, we propose a graph embedding model by modifying the structural deep network embedding model to map these relationships into an embedding space. The abnormality is detected by using the outlier detection methods. To evaluate the proposed model, we have conducted the experiments by using the real financial time series (e.g., Apple, Amazon, Coke, Starbucks, and McDonald’s). The results showed that the proposed model achieved higher accuracy than the baselines by 4%.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521000593",
    "keywords": [
      "Abnormality",
      "Anomaly detection",
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Computer science",
      "Data mining",
      "Economics",
      "Embedding",
      "Finance",
      "Identification (biology)",
      "Machine learning",
      "Outlier",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Psychology",
      "Series (stratigraphy)",
      "Social psychology",
      "Spurious relationship",
      "Time series"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Gen"
      },
      {
        "surname": "Jung",
        "given_name": "Jason J."
      }
    ]
  },
  {
    "title": "Joint direct estimation of 3D geometry and 3D motion using spatio temporal gradients",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107759",
    "abstract": "Conventional image-motion based methods for structure from motion first compute optical flow, then solve for the 3D motion parameters based on the epipolar constraint, and finally recover the 3D geometry of the scene. However, errors in optical flow due to regularization can lead to large errors in 3D motion and structure. This paper investigates whether performance and consistency can be improved by avoiding optical flow estimation in the early stages of the structure-from-motion pipeline, and it proposes a new direct method based on image gradients (normal flow) only. Our main idea lies in a reformulation of the positive-depth constraint – the basis for estimating egomotion from normal flow – as a continuous piecewise differentiable function, which allows the use of well-known minimization techniques to solve for 3D motion. The 3D motion estimate is then refined and structure estimated adding a regularization based on depth. Experimental comparisons on standard synthetic datasets and the real-world driving benchmark dataset Kitti using three different optic flow algorithms show that the method achieves better accuracy in all but one case. Furthermore, it outperforms existing normal flow based 3D motion estimation techniques. Finally, the recovered 3D geometry is shown to be also very accurate.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320305628",
    "keywords": [],
    "authors": [
      {
        "surname": "Barranco",
        "given_name": "Francisco"
      },
      {
        "surname": "Fermüller",
        "given_name": "Cornelia"
      },
      {
        "surname": "Aloimonos",
        "given_name": "Yiannis"
      },
      {
        "surname": "Ros",
        "given_name": "Eduardo"
      }
    ]
  },
  {
    "title": "Learning efficient, explainable and discriminative representations for pulmonary nodules classification",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107825",
    "abstract": "Automatic pulmonary nodules classification is significant for early diagnosis of lung cancers. Recently, deep learning techniques have enabled remarkable progress in this field. However, these deep models are typically of high computational complexity and work in a black-box manner. To combat these challenges, in this work, we aim to build an efficient and (partially) explainable classification model. Specially, we use neural architecture search (NAS) to automatically search 3D network architectures with excellent accuracy/speed trade-off. Besides, we use the convolutional block attention module (CBAM) in the networks, which helps us understand the reasoning process. During training, we use A-Softmax loss to learn angularly discriminative representations. In the inference stage, we employ an ensemble of diverse neural networks to improve the prediction accuracy and robustness. We conduct extensive experiments on the LIDC-IDRI database. Compared with previous state-of-the-art, our model shows highly comparable performance by using less than 1/40 parameters. Besides, empirical study shows that the reasoning process of learned networks is in conformity with physicians’ diagnosis. Related code and results have been released at: https://github.com/fei-hdu/NAS-Lung.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321000121",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Discriminative model",
      "Gene",
      "Inference",
      "Machine learning",
      "Margin (machine learning)",
      "Pattern recognition (psychology)",
      "Pooling",
      "Robustness (evolution)",
      "Softmax function"
    ],
    "authors": [
      {
        "surname": "Jiang",
        "given_name": "Hanliang"
      },
      {
        "surname": "Shen",
        "given_name": "Fuhao"
      },
      {
        "surname": "Gao",
        "given_name": "Fei"
      },
      {
        "surname": "Han",
        "given_name": "Weidong"
      }
    ]
  },
  {
    "title": "Using dynamical quantization to perform split attempts in online tree regressors",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.01.033",
    "abstract": "A central aspect of online decision trees is evaluating the incoming data and performing model growth. For such, trees much deal with different kinds of input features. Numerical features are no exception, and they pose additional challenges compared to other kinds of features, as there is no trivial strategy to choose the best point to make a split decision. Regression tasks are even more challenging because both the features and the target are continuous. Typical online solutions evaluate and store all the points monitored between split attempts, which goes against the constraints posed in real-time applications. In this paper, we introduce the Quantization Observer (QO), a simple yet effective hashing-based algorithm to monitor and evaluate split candidates in numerical features for online tree regressors. QO can be easily integrated into incremental decision trees, such as Hoeffding Trees, and it has a monitoring cost of O ( 1 ) per instance and a sub-linear cost to evaluate split candidates. Previous solutions had a O ( log n ) cost per insertion (in the best case) and a linear cost to evaluate split candidates. Our extensive experimental setup highlights QO’s effectiveness in providing accurate split point suggestions while spending much less memory and processing time than its competitors.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521000520",
    "keywords": [],
    "authors": [
      {
        "surname": "Mastelini",
        "given_name": "Saulo Martiello"
      },
      {
        "surname": "de Carvalho",
        "given_name": "Andre Carlos Ponce de Leon Ferreira"
      }
    ]
  },
  {
    "title": "Machine learning based blind color image watermarking scheme for copyright protection",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.02.011",
    "abstract": "This work presents a blind and robust scheme using YCbCr color space, IWT (integer wavelet transform) and DCT (discrete cosine transform) for color image watermarking. During watermark insertion, Y channel is divided into blocks and Mersenne Twister random number generator is used to select the blocks for embedding. This randomized selection of blocks required a secret key, thus improving the security of the scheme. To reduce the computational complexity, the artificial neural network architecture is developed for watermark embedding. To check the robustness, several signal processing attacks such as JPEG compression, filtering attacks, noise attacks, cropping, resizing and other common attacks are applied on the watermarked images. The proposed work is tested on different images to verify the similarity in watermarking results. The scheme provides similar results (having little variation) for different test images. Experimental results demonstrate the superior performance in terms of imperceptibility and robustness. Further, the ANN framework provides faster embedding with approximately similar parametric results. The performance comparison with existing schemes demonstrates better performance for different attacks. The proposed work can be used in robust applications (i.e. copyright protection) for efficient results and less computational time.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521000660",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Digital watermarking",
      "Discrete cosine transform",
      "Embedding",
      "Gene",
      "Image (mathematics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Robustness (evolution)",
      "Watermark"
    ],
    "authors": [
      {
        "surname": "Sinhal",
        "given_name": "Rishi"
      },
      {
        "surname": "Jain",
        "given_name": "Deepak Kumar"
      },
      {
        "surname": "Ansari",
        "given_name": "Irshad Ahmad"
      }
    ]
  },
  {
    "title": "CHOP: An orthogonal hashing method for zero-shot cross-modal retrieval",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.02.016",
    "abstract": "Cross-modal retrieval has recently attracted much attention because it helps users retrieve data across different modalities. However, with the explosive growth of data, a large number of new emerging concepts (unseen classes) that have not been appeared in the training data (seen classes) bring great challenges to the traditional cross-modal retrieval. Nevertheless, most existing approaches mainly focus on improving cross-modal retrieval performance of seen classes, which may fail in the unseen classes. To address the challenge of zero-shot cross-modal retrieval, we propose an orthogonal method in this paper, i.e., Cross-modal Hashing with Orthogonal Projection (CHOP). It projects cross-modal features and class attributes onto a Hamming space, where each projection of cross-modal features is orthogonal to the mismatched class attributes. By so doing, the model can learn a discriminative and binary representation of each modality. In addition, the class attributes build a bridge to transfer knowledge from seen classes to unseen classes. Furthermore, the orthogonal constraint on binary codes can help to mitigate the hubness problem. Extensive experiments on three benchmark datasets show that the proposed CHOP is effective in handling zero-shot cross-modal retrieval.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521000787",
    "keywords": [
      "Algorithm",
      "Arithmetic",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Binary code",
      "Binary number",
      "Block code",
      "Chemistry",
      "Class (philosophy)",
      "Computer science",
      "Computer security",
      "Decoding methods",
      "Discriminative model",
      "Geodesy",
      "Geography",
      "Hamming code",
      "Hamming space",
      "Hash function",
      "Mathematics",
      "Modal",
      "Orthographic projection",
      "Pattern recognition (psychology)",
      "Polymer chemistry",
      "Projection (relational algebra)"
    ],
    "authors": [
      {
        "surname": "Yuan",
        "given_name": "Xu"
      },
      {
        "surname": "Wang",
        "given_name": "Guangze"
      },
      {
        "surname": "Chen",
        "given_name": "Zhikui"
      },
      {
        "surname": "Zhong",
        "given_name": "Fangming"
      }
    ]
  },
  {
    "title": "Interwoven texture-based description of interest points in images",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107821",
    "abstract": "Local feature description is to assign a unique signature to a key-point such that it becomes distinctive from the others regardless of changes in viewpoint, illumination, rotation, scale as well as distortions and noise. This paper proposes a novel approach to construct such a descriptor. For preserving both homogeneous and heterogeneous features of a given support region, we interweave the texture information so that the key-point is more likely to be assigned a distinctive signature and neighboring key-points will be less likely to share the same texture information. The main idea behind our descriptor is to increase the areas of our observations in the given scene while the length of the local support region is fixed. Gradient magnitude and divergence, as measurement parameters of texture information, are applied to a group of pixels instead of employing a pixel-wise strategy that make the descriptor more resistant to noise, distortions and illumination variation. The required storage of the proposed descriptor is just 72 floats and its computational complexity is much lower than those of existing ones. A comparative study between the proposed method and the selected state-of-the-art ones over multiple publicly accessible datasets with different characteristics shows its superiority, robustness and computational efficiency under various geometric changes, illumination variation, distortions and noise. The code and supplementary materials can be found at https://github.com/mogvision/InterTex-Feature-Descriptor.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032100008X",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Feature (linguistics)",
      "Gene",
      "Geometry",
      "Image (mathematics)",
      "Key (lock)",
      "Linguistics",
      "Mathematics",
      "Noise (video)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pixel",
      "Point (geometry)",
      "Robustness (evolution)",
      "Texture (cosmology)"
    ],
    "authors": [
      {
        "surname": "Ghahremani",
        "given_name": "Morteza"
      },
      {
        "surname": "Zhao",
        "given_name": "Yitian"
      },
      {
        "surname": "Tiddeman",
        "given_name": "Bernard"
      },
      {
        "surname": "Liu",
        "given_name": "Yonghuai"
      }
    ]
  },
  {
    "title": "A Less-constrained Sclera Recognition Method based on Stem-and-leaf Branches Network",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.01.025",
    "abstract": "Recognition based on the vasculature patterns observed on the sclera is a topic with great potential in biometrics. The emergence of neural network offers opportunities for more reliable identification. However, the annotation labels, which are extremely important for network learning, are often difficult to obtain, especially for sclera vessels. This paper proposes a robust sclera recognition method consisting of a non-learning-based segmentation step and a learning-based classification step, minimizing the reliance on supervision. The novelty of the approach lies in the designed stem-and-leaf branches network, named SLBNet, aggregating different levels of global and local features to obtain discriminative descriptors. Besides, a new dataset focusing on the sclera vasculature without iris, named ScleraVO, is built with the designed acquisition system. Experimental results demonstrate that the proposed recognition method achieves state-of-the-art performance and presents a good generalization ability.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521000441",
    "keywords": [
      "Annotation",
      "Artificial intelligence",
      "Artificial neural network",
      "Biometrics",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Discriminative model",
      "IRIS (biosensor)",
      "Image segmentation",
      "Iris recognition",
      "Machine learning",
      "Medicine",
      "Novelty",
      "Ophthalmology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Sclera",
      "Segmentation",
      "Theology"
    ],
    "authors": [
      {
        "surname": "Zhu",
        "given_name": "Dongchen"
      },
      {
        "surname": "Li",
        "given_name": "Jiamao"
      },
      {
        "surname": "Li",
        "given_name": "Hang"
      },
      {
        "surname": "Peng",
        "given_name": "Jingquan"
      },
      {
        "surname": "Wang",
        "given_name": "Xianshun"
      },
      {
        "surname": "Zhang",
        "given_name": "Xiaolin"
      }
    ]
  },
  {
    "title": "Hierarchical deep neural network for mental stress state detection using IoT based biomarkers",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.01.030",
    "abstract": "Affective state recognition at an early stage can help in mood stabilization, stress and depression management for mental well-being. Pro-active and remote mental healthcare warrants the use of various biomarkers to detect the affective mental state of the individual by evaluating the daily activities. With the easy accessibility of IoT-based sensors for healthcare, observable and quantifiable characteristics of our body, physiological changes in the body can be measured and tracked using various wearable devices. This work puts forward a model for mental stress state detection using sensor-based bio-signals. A multi-level deep neural network with hierarchical learning capabilities of convolution neural network is proposed. Multivariate time-series data consisting of both wrist-based and chest-based sensor bio-signals is trained using a hierarchy of networks to generate high-level features for each bio-signal feature. A model-level fusion strategy is proposed to combine the high-level features into one unified representation and classify the stress states into three categories as baseline, stress and amusement. The model is evaluated on the WESAD benchmark dataset for mental health and compares favourably to state-of-the-art approaches giving a superlative performance accuracy of 87.7%.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521000490",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Embedded system",
      "Feature (linguistics)",
      "Geodesy",
      "Geography",
      "Linguistics",
      "Machine learning",
      "Mental health",
      "Philosophy",
      "Psychiatry",
      "Psychology",
      "Wearable computer"
    ],
    "authors": [
      {
        "surname": "Kumar",
        "given_name": "Akshi"
      },
      {
        "surname": "Sharma",
        "given_name": "Kapil"
      },
      {
        "surname": "Sharma",
        "given_name": "Aditi"
      }
    ]
  },
  {
    "title": "Iterative weak/self-supervised classification framework for abnormal events detection",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.01.031",
    "abstract": "The detection of abnormal events in surveillance footage remains a challenge and has been the scope of various research works. Having observed that the state-of-the-art performance is still unsatisfactory, this paper provides a novel solution to the problem, with four-fold contributions: 1) upon the work of Sultani et al., we introduce one iterative learning framework composed of two experts working in the weak and self-supervised paradigms and providing additional amounts of learning data to each other, where the novel instances at each iteration are filtered by a Bayesian framework that supports the iterative data augmentation task; 2) we describe a novel term that is added to the baseline loss to spread the scores in the unit interval, which is crucial for the performance of the iterative framework; 3) we propose a Random Forest ensemble that fuses at the score level the top performing methods and reduces the EER values about 20% over the state-of-the-art; and 4) we announce the availability of the ”UBI-Fights” dataset, fully annotated at the frame level, that can be freely used by the research community. The code, details of the experimental protocols and the dataset are publicly available at http://github.com/DegardinBruno/.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521000507",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Baseline (sea)",
      "Bayesian probability",
      "Code (set theory)",
      "Combinatorics",
      "Computer science",
      "Data mining",
      "Economics",
      "Frame (networking)",
      "Geology",
      "Interval (graph theory)",
      "Iterative method",
      "Machine learning",
      "Management",
      "Mathematics",
      "Oceanography",
      "Physics",
      "Programming language",
      "Quantum mechanics",
      "Random forest",
      "Scope (computer science)",
      "Set (abstract data type)",
      "Task (project management)",
      "Telecommunications",
      "Term (time)"
    ],
    "authors": [
      {
        "surname": "Degardin",
        "given_name": "Bruno"
      },
      {
        "surname": "Proença",
        "given_name": "Hugo"
      }
    ]
  },
  {
    "title": "Visual SLAM for robot navigation in healthcare facility",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107822",
    "abstract": "The COVID-19 pandemic has affected many countries, posing a threat to human health and safety, and putting tremendous pressure on the medical system. This paper proposes a novel SLAM technology using RGB and depth images to improve hospital operation efficiency, reduce the risk of doctor-patient cross-infection, and curb the spread of the COVID-19. Most current visual SLAM researches assume that the environment is stationary, which makes handling real-world scenarios such as hospitals a challenge. This paper proposes a method that effectively deals with SLAM problems for scenarios with dynamic objects, e.g., people and movable objects, based on the semantic descriptor extracted from images with help of a knowledge graph. Specifically, our method leverages a knowledge graph to construct a priori movement relationship between entities and establishes high-level semantic information. Built upon this knowledge graph, a semantic descriptor is constructed to describe the semantic information around key points, which is rotation-invariant and robust to illumination. The seamless integration of the knowledge graph and semantic descriptor helps eliminate the dynamic objects and improves the accuracy of tracking and positioning of robots in dynamic environments. Experiments are conducted using data acquired from healthcare facilities, and semantic maps are established to meet the needs of robots for delivering medical services. In addition, to compare with the state-of-the-art methods, a publicly available dataset is used in our evaluation. Compared with the state-of-the-art methods, our proposed method demonstrated great improvement with respect to both accuracy and robustness in dynamic environments. The computational efficiency is also competitive.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321000091",
    "keywords": [
      "A priori and a posteriori",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Epistemology",
      "Gene",
      "Graph",
      "Mobile robot",
      "Philosophy",
      "Robot",
      "Robustness (evolution)",
      "Semantic mapping",
      "Simultaneous localization and mapping",
      "Theoretical computer science",
      "Visualization"
    ],
    "authors": [
      {
        "surname": "Fang",
        "given_name": "Baofu"
      },
      {
        "surname": "Mei",
        "given_name": "Gaofei"
      },
      {
        "surname": "Yuan",
        "given_name": "Xiaohui"
      },
      {
        "surname": "Wang",
        "given_name": "Le"
      },
      {
        "surname": "Wang",
        "given_name": "Zaijun"
      },
      {
        "surname": "Wang",
        "given_name": "Junyang"
      }
    ]
  },
  {
    "title": "BCMM: A novel post-based augmentation representation for early rumour detection on social media",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107818",
    "abstract": "Online social media (OSM) has become a hotbed for the rapid dissemination of disinformation or rumour. Therefore, rumour detection, especially early rumour detection (ERD), is very challenging given the limited, incomplete and noisy information. Although there are some researches on earlier rumour detection, most of their studies require a larger dataset or a longer detection time span, i.e., the rumour detection efficiency needs to be improved. In this paper, we focus on a shorter detection time span which also means fewer online posts to achieve the task of ERD. We proposed a novel post-based augmentation representation approach to process post content of rumour events in the early stages of their dissemination, i.e., backward compression mapping mechanism (BCMM). In addition, we combine BCMM with gated recurrent unit (GRU) to represent post content, topology network of posts and metadata extracted from post datasets. We apply a three-layers GRU to enhance the representation of dataset within one hour after the occurrence of a social media event, i.e., BCMM-GRU. The steps are as follows: (1) we input the first-hour data into the first layer; (2) the first 40 min of data are channelled into the second layer with the output of the first layer making a full mapping to the second layer simultaneously; (3) the first 20 min of data are sent to the third layer while the output of the second layer applies a full mapping to the third layer simultaneously. The evaluation of BCMM-GRU’s performance entails applying k-fold cross-validation (CV) set-up on four available real-life rumour event datasets. The experimental results are superior to the baselines and model variants and achieve a high accuracy of 80.09% and F1-score of 80.18%.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321000054",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Data mining",
      "Disinformation",
      "Engineering",
      "Event (particle physics)",
      "Focus (optics)",
      "Law",
      "Layer (electronics)",
      "Metadata",
      "Optics",
      "Organic chemistry",
      "Physics",
      "Political science",
      "Politics",
      "Quantum mechanics",
      "Representation (politics)",
      "Social media",
      "Systems engineering",
      "Task (project management)",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Luo",
        "given_name": "Yongcong"
      },
      {
        "surname": "Ma",
        "given_name": "Jing"
      },
      {
        "surname": "Yeo",
        "given_name": "Chai Kiat"
      }
    ]
  },
  {
    "title": "Person re-identification: Implicitly defining the receptive fields of deep learning classification frameworks",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.01.035",
    "abstract": "The receptive fields of deep learning models determine the most significant regions of the input data for providing correct decisions. Up to now, the primary way to learn such receptive fields is to train the models upon masked data, which helps the networks to ignore any unwanted regions, but also has two major drawbacks: (1) it yields edge-sensitive decision processes; and (2) it augments considerably the computational cost of the inference phase. Having theses weaknesses in mind, this paper describes a solution for implicitly enhancing the inference of the networks’ receptive fields, by creating synthetic learning data composed of interchanged segments considered apriori important or irrelevant for the network decision. In practice, we use a segmentation module to distinguish between the foreground (important) versus background (irrelevant) parts of each learning instance, and randomly swap segments between image pairs, while keeping the class label exclusively consistent with the label of the segments deemed important. This strategy typically drives the networks to interpret that the identity and clutter descriptions are not correlated. Moreover, the proposed solution has other interesting properties: (1) it is parameter-learning-free; (2) it fully preserves the label information; and (3) it is compatible with the data augmentation techniques typically used. In our empirical evaluation, we considered the person re-identification problem, and the well known RAP, Market1501 and MSMT-V2 datasets for two different settings (upper-body and full-body), having observed highly competitive results over the state-of-the-art. Under a reproducible research paradigm, both the code and the empirical evaluation protocol are available at https://github.com/Ehsan-Yaghoubi/reid-strong-baseline.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521000544",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Clutter",
      "Computer science",
      "Identification (biology)",
      "Inference",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Radar",
      "Segmentation",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Yaghoubi",
        "given_name": "Ehsan"
      },
      {
        "surname": "Borza",
        "given_name": "Diana"
      },
      {
        "surname": "Aruna Kumar",
        "given_name": "S.V."
      },
      {
        "surname": "Proença",
        "given_name": "Hugo"
      }
    ]
  },
  {
    "title": "Landmark guidance independent spatio-channel attention and complementary context information based facial expression recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.01.029",
    "abstract": "Attention based convolutional neural networks(CNNs) for facial expression recognition (FER) apply attention that is uniform across either spatial dimensions or channel dimensions or both spatial and channel dimensions. However, there are many issues viz. (i) in the presence of occlusions and pose variations, different channels respond differently, (ii) the response intensity of a channel differ across spatial locations, (iii) attention is defined based on external sources like landmark detectors and (iv) features used from pretrained face recognition (FR) model to complement the attention branch contain redundant information. To overcome these issues, an end-to-end architecture for FER is proposed in this work. This architecture obtains both local and global attention per channel per spatial location through a novel spatio-channel attention net (SCAN), without seeking any information from the landmark detectors. SCAN is complemented by a complementary context information (CCI) branch that builds expression representation from the pretrained FR features. Redundancies in FR features are eliminated by using efficient channel attention (ECA). The representation learnt by the proposed architecture is robust to occlusions and pose variations. This is demonstrated by the state-of-the-art performance of the proposed model on in-the-wild datasets including AffectNet, FERPlus, RAF-DB, SFEW and FED-RO. Further, the proposed architecture also reports superior performance on in-lab datasets (CK+, Oulu-CASIA and JAFFE) and a couple of constructed face masked datasets resembling masked faces in COVID-19 scenario. Codes are publicly available at https://github.com/1980x/SCAN-CCI-FER.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521000489",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Biochemistry",
      "Channel (broadcasting)",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Convolutional neural network",
      "Facial expression",
      "Gene",
      "Geography",
      "Landmark",
      "Law",
      "Pattern recognition (psychology)",
      "Political science",
      "Relevance (law)",
      "Remote sensing",
      "Robustness (evolution)",
      "Spatial analysis",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Gera",
        "given_name": "Darshan"
      },
      {
        "surname": "Balasubramanian",
        "given_name": "S"
      }
    ]
  },
  {
    "title": "A distance for belief functions of orderable set",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.02.010",
    "abstract": "This paper proposes a distance for measuring conflicts between ordered sets. A similarity coefficient is defined to quantify the distance between focal elements and it can characterize the differences in the distribution of sets in a continuous measurement space, so the distance would still vary with the physical distance even when the focal elements do not overlap. We prove that the proposed method satisfies the properties of distance, and discuss some other properties of the presented approach. An example of engineering budget indicates that the proposed distance can effectively measure the similarity of orderable set. By comparing with the existing methods, we show that the proposed metric is more robust and accurate in characterizing the aggrement of ordered sets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521000659",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Discrete mathematics",
      "Distance measurement",
      "Distance measures",
      "Earth mover's distance",
      "Economics",
      "Image (mathematics)",
      "Mathematics",
      "Measure (data warehouse)",
      "Metric (unit)",
      "Metric space",
      "Operating system",
      "Operations management",
      "Programming language",
      "Set (abstract data type)",
      "Similarity (geometry)",
      "Similarity measure",
      "Space (punctuation)"
    ],
    "authors": [
      {
        "surname": "Cheng",
        "given_name": "Cuiping"
      },
      {
        "surname": "Xiao",
        "given_name": "Fuyuan"
      }
    ]
  },
  {
    "title": "A rotation and scale invariant approach for multi-oriented floor plan image retrieval",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.01.020",
    "abstract": "An automatic system for analysis and retrieval of building floor plans images is helpful for the architects while designing new projects and providing recommendations to the buyers. For such systems, query by example is preferred over query by keyword, for which user’s requirements must be available in digital image form. Floor plans are converted to digital form by scanning and often get rotated slightly by a certain degree of angle during digitization. In this paper, we have proposed a geometric feature-based approach for floor plan image retrieval and our key contribution is to handle different kinds of rotation and scale while retrieving similar floor plan from the database. Our framework is divided into three phases, namely outer shape feature extraction, internal object feature extraction, followed by matching and retrieval. For our experimentation, we have rotated images of ROBIN dataset as currently no rotated floor plan dataset was available. Our experiment shows that the proposed methodology outperforms recent competing methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521000337",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Cartography",
      "Computer graphics (images)",
      "Computer science",
      "Computer vision",
      "Geography",
      "Invariant (physics)",
      "Mathematical physics",
      "Mathematics",
      "Plan (archaeology)",
      "Rotation (mathematics)",
      "Scale (ratio)",
      "Scale invariance",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Khade",
        "given_name": "Rasika"
      },
      {
        "surname": "Jariwala",
        "given_name": "Krupa"
      },
      {
        "surname": "Chattopadhyay",
        "given_name": "Chiranjoy"
      },
      {
        "surname": "Pal",
        "given_name": "Umapada"
      }
    ]
  },
  {
    "title": "Crossover-Net: Leveraging vertical-horizontal crossover relation for robust medical image segmentation",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107756",
    "abstract": "Accurate boundary segmentation in medical images is significant yet challenging due to large variation of shape, size and appearance within intra- and inter- samples. In this paper, we present a novel deep model termed as Crossover-Net for robust segmentation in medical images. The proposed model is inspired by an interesting observation – the features learned from horizontal and vertical directions can provide informative and complement contextual information to enhance discriminative ability between different tissues. Specifically, we first originally propose a cross-shaped patch, namely crossover-patch which consists of a pair of (orthogonal and overlapping) vertical and horizontal patches. Then, we develop our Crossover-Net to learn the vertical and horizontal crossover relation according to the proposed crossover-patches. To train our model end-to-end, we design a novel loss function to (1) impose the consistency on overlapping region of vertical and horizontal patches and (2) preserve the diversity on their non-overlapping regions. We have extensively evaluated our method on CT kidney tumor, MR cardiac, and X-ray breast mass segmentation tasks, showing promising results compared with the current state-of-the-art methods. The code is available at https://github.com/Qianyu1226/Crossover-Net.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320305598",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Boundary (topology)",
      "Computer science",
      "Computer vision",
      "Crossover",
      "Image segmentation",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Yu",
        "given_name": "Qian"
      },
      {
        "surname": "Gao",
        "given_name": "Yang"
      },
      {
        "surname": "Zheng",
        "given_name": "Yefeng"
      },
      {
        "surname": "Zhu",
        "given_name": "Jianbing"
      },
      {
        "surname": "Dai",
        "given_name": "Yakang"
      },
      {
        "surname": "Shi",
        "given_name": "Yinghuan"
      }
    ]
  },
  {
    "title": "Rethinking data collection for person re-identification: active redundancy reduction",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107827",
    "abstract": "Annotating a large-scale image dataset is very tedious, yet necessary for training person re-identification (re-ID) models. To alleviate such a problem, we present an active redundancy reduction (ARR) framework via training an effective re-ID model with the least labeling efforts. The proposed ARR framework actively selects informative and diverse samples for annotation by estimating their uncertainty and intra-diversity, thus it can significantly reduce the annotation workload. Moreover, we propose a computer-assisted identity recommendation module embedded in the ARR framework to help human annotators to rapidly and accurately label the selected samples. Extensive experiments were carried out on several public re-ID datasets to demonstrate the existence of data redundancy. Experimental results indicate that our method can reduce 57%, 63%, and 49% annotation efforts on the Market1501, MSMT17, and CUHK03, respectively, while maximizing the performance of the re-ID model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321000145",
    "keywords": [
      "Annotation",
      "Artificial intelligence",
      "Automatic image annotation",
      "Biology",
      "Botany",
      "Computer science",
      "Data collection",
      "Data mining",
      "Identification (biology)",
      "Image (mathematics)",
      "Image retrieval",
      "Machine learning",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Redundancy (engineering)",
      "Statistics",
      "Training set",
      "Workload"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Xin"
      },
      {
        "surname": "Liu",
        "given_name": "Lei"
      },
      {
        "surname": "Zhang",
        "given_name": "Xiaolong"
      },
      {
        "surname": "Guan",
        "given_name": "Weili"
      },
      {
        "surname": "Hu",
        "given_name": "Ruimin"
      }
    ]
  },
  {
    "title": "Change-point detection in hierarchical circadian models",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107820",
    "abstract": "This paper addresses the problem of change-point detection in sequences of high-dimensional and heterogeneous observations, which also possess a periodic temporal structure. Due to the dimensionality problem, when the time between change points is of the order of the dimension of the model parameters, drifts in the underlying distribution can be misidentified as changes. To overcome this limitation, we assume that the observations lie in a lower-dimensional manifold that admits a latent variable representation. In particular, we propose a hierarchical model that is computationally feasible, widely applicable to heterogeneous data and robust to missing instances. Additionally, the observations’ periodic dependencies are captured by non-stationary periodic covariance functions. The proposed technique is particularly well suited to (and motivated by) the problem of detecting changes in human behavior using smartphones and its application to relapse detection in psychiatric patients. Finally, we validate the technique on synthetic examples and we demonstrate its utility in the detection of behavioral changes using real data acquired by smartphones.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321000078",
    "keywords": [],
    "authors": [
      {
        "surname": "Moreno-Muñoz",
        "given_name": "Pablo"
      },
      {
        "surname": "Ramírez",
        "given_name": "David"
      },
      {
        "surname": "Artés-Rodríguez",
        "given_name": "Antonio"
      }
    ]
  },
  {
    "title": "Low-rank adaptive graph embedding for unsupervised feature extraction",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107758",
    "abstract": "Most of manifold learning based feature extraction methods are two-step methods, which first construct a weighted neighborhood graph and then use the pre-constructed graph to perform subspace learning. As a result, these methods fail to use the underlying correlation structure of data to learn an adaptive graph to preciously characterize the similarity relationship between samples. To address this problem, we propose a novel unsupervised feature extraction method called low-rank adaptive graph embedding (LRAGE), which can perform subspace learning and adaptive probabilistic neighborhood graph embedding simultaneously based on reconstruction error minimization. The proposed LRAGE is imposed with low-rank constraint for the sake of exploring the underlying correlation structure of data and learning more informative projection. Moreover, the L 2 , 1 -norm penalty is imposed on the regularization to further enhance the robustness of LRAGE. Since the resulting objective function has no closed-form solutions, an iterative optimization algorithm is elaborately designed. The convergence of the proposed algorithm is proved and the corresponding computational complexity analysis is also presented. In addition, we explore the potential properties of the proposed LRAGE by comparing it with several similar models on both synthetic and real-world data sets. Extensive experiments on five well-known face data sets and three non-face data sets demonstrate the superiority of the proposed LRAGE.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320305616",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Embedding",
      "Feature extraction",
      "Graph",
      "Graph embedding",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Subspace topology",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Lu",
        "given_name": "Jianglin"
      },
      {
        "surname": "Wang",
        "given_name": "Hailing"
      },
      {
        "surname": "Zhou",
        "given_name": "Jie"
      },
      {
        "surname": "Chen",
        "given_name": "Yudong"
      },
      {
        "surname": "Lai",
        "given_name": "Zhihui"
      },
      {
        "surname": "Hu",
        "given_name": "Qinghua"
      }
    ]
  },
  {
    "title": "Coupled-dynamic learning for vision and language: Exploring Interaction between different tasks",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107829",
    "abstract": "Intensive research interests have been paid for the vision and language communities. Especially, image captioning task aims to generate natural language descriptions from the image content. Oppositely, image synthesis task aims to generate realistic images from natural language descriptions. Moreover, both of them can achieve promising results by using Long Short-Term Memory (LSTM), which models the sequence dynamics at each time step as hidden state. Nevertheless, the research on dynamics is often limited in the individual task, while there is no progress exploring the mutual relationship between dynamics in different tasks. In this work, we present a novel coupled-dynamic formulation that can iteratively reduce the distance between task-dependent dynamics in the training process. To embed adverse information into individual network, we construct dual-loss architectures to interactively align dynamics. We evaluate the proposed framework on Flickr8k, Flickr30k and MSCOCO datasets. Experimental results show that our approach can boost dual tasks together and achieve competing performances against state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321000169",
    "keywords": [
      "Acoustics",
      "Art",
      "Artificial intelligence",
      "Biology",
      "Closed captioning",
      "Computer science",
      "Construct (python library)",
      "Dual (grammatical number)",
      "Dynamics (music)",
      "Economics",
      "Genetics",
      "Image (mathematics)",
      "Literature",
      "Machine learning",
      "Management",
      "Natural language",
      "Natural language processing",
      "Operating system",
      "Physics",
      "Process (computing)",
      "Programming language",
      "Sequence (biology)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Ning"
      },
      {
        "surname": "Tian",
        "given_name": "Hongshuo"
      },
      {
        "surname": "Wang",
        "given_name": "Yanhui"
      },
      {
        "surname": "Nie",
        "given_name": "Weizhi"
      },
      {
        "surname": "Song",
        "given_name": "Dan"
      },
      {
        "surname": "Liu",
        "given_name": "An-An"
      },
      {
        "surname": "Liu",
        "given_name": "Wu"
      }
    ]
  },
  {
    "title": "Deep video code for efficient face video retrieval",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107754",
    "abstract": "In this paper, we address one specific video retrieval problem in terms of human face. Given one query in forms of either a frame or a sequence from a person, we search the database and return the most relevant face videos, i.e., ones have the same class label with the query. Such problem is very challenging due to the large intra-class variations and the high request on the efficiency of video representations in terms of both time and space. To handle such challenges, this paper proposes a novel Deep Video Code (DVC) method which encodes video faces into compact binary codes. Specifically, we devise an end-to-end convolutional neural network (CNN) framework that takes face videos as training inputs, models each of them as a unified representation by temporal feature pooling operation, and finally projects the high-dimensional representations of both frames and videos into Hamming space to generate binary codes. In such Hamming space, distance of dissimilar pairs is larger than that of similar pairs by a margin. To this end, a novel bounded triplet hashing loss is elaborately designed, which takes all dissimilar pairs into consideration for each anchor point in a mini-batch, and the optimization of the loss function is smoother and more stable. Extensive experiments on challenging video face databases and general image/video datasets with comparison to the state-of-the-arts verify the effectiveness of our method in different kinds of retrieval scenarios.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320305574",
    "keywords": [
      "Algorithm",
      "Arithmetic",
      "Artificial intelligence",
      "Binary code",
      "Binary number",
      "Block code",
      "Code (set theory)",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Convolutional neural network",
      "Decoding methods",
      "Face (sociological concept)",
      "Hamming code",
      "Hamming distance",
      "Hamming space",
      "Hash function",
      "Machine learning",
      "Margin (machine learning)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pooling",
      "Programming language",
      "Set (abstract data type)",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Qiao",
        "given_name": "Shishi"
      },
      {
        "surname": "Wang",
        "given_name": "Ruiping"
      },
      {
        "surname": "Shan",
        "given_name": "Shiguang"
      },
      {
        "surname": "Chen",
        "given_name": "Xilin"
      }
    ]
  },
  {
    "title": "DCR: Disentangled component representation for sketch generation",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.01.016",
    "abstract": "We present a simple end-to-end model based on deep learning to automatically decompose sketched objects into components by disentangling the visual representation. The performance of visual representation learning based models degrades as categories increase. Rather than building a mapping from a static image to the whole sketch sequences, we propose an interpretable disentangled representation of sketch to understand component concepts and the relationship among such concepts. Our model takes the binary image of a sketched object and produces a component stroke sequence set corresponding to key components in the sketch. Experiments show that our method significantly outperforms all baselines quantitatively at the degree of disentanglement, and our method is more stable while training on tens of categories.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521000295",
    "keywords": [
      "Algorithm",
      "Arithmetic",
      "Artificial intelligence",
      "Binary number",
      "Component (thermodynamics)",
      "Computer science",
      "Computer security",
      "Gesture",
      "Gesture recognition",
      "Image (mathematics)",
      "Key (lock)",
      "Law",
      "Mathematics",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Physics",
      "Political science",
      "Politics",
      "Programming language",
      "Representation (politics)",
      "Set (abstract data type)",
      "Sketch",
      "Sketch recognition",
      "Thermodynamics"
    ],
    "authors": [
      {
        "surname": "Cao",
        "given_name": "Zhong"
      },
      {
        "surname": "Cui",
        "given_name": "Sen"
      },
      {
        "surname": "Zhang",
        "given_name": "Changshui"
      }
    ]
  },
  {
    "title": "Contrast-weighted dictionary learning based saliency detection for VHR optical remote sensing images",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107757",
    "abstract": "Object detection in very high resolution (VHR) optical remote sensing (RS) images is one of the most fundamental but challenging tasks in the field of RS image analysis. To reduce the computational complexity of redundant information and improve the efficiency of image processing, visual saliency models have been widely applied in this field. In this paper, a novel saliency detection model based on Contrast-weighted Dictionary Learning (CDL) is proposed for VHR optical RS images. Specifically, the proposed CDL learns salient and non-salient atoms from positive and negative samples to construct a discriminant dictionary, in which a contrast-weighted term is proposed to encourage the contrast-weighted patterns to be present in the learned salient dictionary while discouraging them from being present in the non-salient dictionary. Then, we measure the saliency by combining the coefficients of the sparse representation (SR) and reconstruction errors. Furthermore, by using the proposed joint saliency measure, a variety of saliency maps are generated based on the discriminant dictionary. Finally, a fusion method based on global gradient optimization is proposed to integrate multiple saliency maps. Experimental results on four datasets demonstrate that the proposed model outperforms other state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320305604",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Construct (python library)",
      "Contrast (vision)",
      "Data mining",
      "Discriminant",
      "Field (mathematics)",
      "Law",
      "Linear discriminant analysis",
      "Mathematics",
      "Measure (data warehouse)",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Programming language",
      "Pure mathematics",
      "Representation (politics)",
      "Salient",
      "Sparse approximation"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Zhou"
      },
      {
        "surname": "Chen",
        "given_name": "Huai-Xin"
      },
      {
        "surname": "Zhou",
        "given_name": "Tao"
      },
      {
        "surname": "Yang",
        "given_name": "Yun-Zhi"
      },
      {
        "surname": "Wang",
        "given_name": "Chang-Yin"
      },
      {
        "surname": "Liu",
        "given_name": "Bi-Yuan"
      }
    ]
  },
  {
    "title": "Collaborative representation with curriculum classifier boosting for unsupervised domain adaptation",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107802",
    "abstract": "Domain adaptation aims at leveraging rich knowledge in the source domain to build an accurate classifier in the different but related target domain. Most prior methods attempt to align features or reduce domain discrepancy by means of statistical properties yet ignore the differences among samples. In this paper, we put forward a novel solution based on collaborative representation for classifier adaptation. Similar to instance re-weighting, we aim to learn an adaptive classifier by multi-stage inference and instance rearranging. Specifically, a curriculum learning based sample selection scheme is proposed, then the chosen samples are integrated into training set iteratively. Due to the distribution mismatch of two domains, we propose distance-aware sparsity regularization to learn more flexible representations. Extensive experiments verify that the proposed method is comparable or superior to the state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320306051",
    "keywords": [
      "Artificial intelligence",
      "Boosting (machine learning)",
      "Classifier (UML)",
      "Computer science",
      "Domain adaptation",
      "Inference",
      "Machine learning",
      "Margin classifier",
      "Medicine",
      "Pattern recognition (psychology)",
      "Radiology",
      "Regularization (linguistics)",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Han",
        "given_name": "Chao"
      },
      {
        "surname": "Zhou",
        "given_name": "Deyun"
      },
      {
        "surname": "Xie",
        "given_name": "Yu"
      },
      {
        "surname": "Gong",
        "given_name": "Maoguo"
      },
      {
        "surname": "Lei",
        "given_name": "Yu"
      },
      {
        "surname": "Shi",
        "given_name": "Jiao"
      }
    ]
  },
  {
    "title": "Synergistic learning of lung lobe segmentation and hierarchical multi-instance classification for automated severity assessment of COVID-19 in CT images",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107828",
    "abstract": "Understanding chest CT imaging of the coronavirus disease 2019 (COVID-19) will help detect infections early and assess the disease progression. Especially, automated severity assessment of COVID-19 in CT images plays an essential role in identifying cases that are in great need of intensive clinical care. However, it is often challenging to accurately assess the severity of this disease in CT images, due to variable infection regions in the lungs, similar imaging biomarkers, and large inter-case variations. To this end, we propose a synergistic learning framework for automated severity assessment of COVID-19 in 3D CT images, by jointly performing lung lobe segmentation and multi-instance classification. Considering that only a few infection regions in a CT image are related to the severity assessment, we first represent each input image by a bag that contains a set of 2D image patches (with each cropped from a specific slice). A multi-task multi-instance deep network (called M 2 UNet) is then developed to assess the severity of COVID-19 patients and also segment the lung lobe simultaneously. Our M 2 UNet consists of a patch-level encoder, a segmentation sub-network for lung lobe segmentation, and a classification sub-network for severity assessment (with a unique hierarchical multi-instance learning strategy). Here, the context information provided by segmentation can be implicitly employed to improve the performance of severity assessment. Extensive experiments were performed on a real COVID-19 CT image dataset consisting of 666 chest CT images, with results suggesting the effectiveness of our proposed method compared to several state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321000157",
    "keywords": [
      "2019-20 coronavirus outbreak",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Coronavirus disease 2019 (COVID-19)",
      "Disease",
      "Infectious disease (medical specialty)",
      "Internal medicine",
      "Lobe",
      "Lung",
      "Medicine",
      "Outbreak",
      "Pathology",
      "Pattern recognition (psychology)",
      "Radiology",
      "Segmentation",
      "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)"
    ],
    "authors": [
      {
        "surname": "He",
        "given_name": "Kelei"
      },
      {
        "surname": "Zhao",
        "given_name": "Wei"
      },
      {
        "surname": "Xie",
        "given_name": "Xingzhi"
      },
      {
        "surname": "Ji",
        "given_name": "Wen"
      },
      {
        "surname": "Liu",
        "given_name": "Mingxia"
      },
      {
        "surname": "Tang",
        "given_name": "Zhenyu"
      },
      {
        "surname": "Shi",
        "given_name": "Yinghuan"
      },
      {
        "surname": "Shi",
        "given_name": "Feng"
      },
      {
        "surname": "Gao",
        "given_name": "Yang"
      },
      {
        "surname": "Liu",
        "given_name": "Jun"
      },
      {
        "surname": "Zhang",
        "given_name": "Junfeng"
      },
      {
        "surname": "Shen",
        "given_name": "Dinggang"
      }
    ]
  },
  {
    "title": "ACDnet: An action detection network for real-time edge computing based on flow-guided feature approximation and memory aggregation",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.02.001",
    "abstract": "Interpreting human actions requires understanding the spatial and temporal context of the scenes. State-of-the-art action detectors based on Convolutional Neural Network (CNN) have demonstrated remarkable results by adopting two-stream or 3D CNN architectures. However, these methods typically operate in a non-real-time, ofline fashion due to system complexity to reason spatio-temporal information. Consequently, their high computational cost is not compliant with emerging real-world scenarios such as service robots or public surveillance where detection needs to take place at resource-limited edge devices. In this paper, we propose ACDnet, a compact action detection network targeting real-time edge computing which addresses both efficiency and accuracy. It intelligently exploits the temporal coherence between successive video frames to approximate their CNN features rather than naively extracting them. It also integrates memory feature aggregation from past video frames to enhance current detection stability, implicitly modeling long temporal cues over time. Experiments conducted on the public benchmark datasets UCF-24 and JHMDB-21 demonstrate that ACDnet, when integrated with the SSD detector, can robustly achieve detection well above real-time (75 FPS). At the same time, it retains reasonable accuracy (70.92 and 49.53 frame mAP) compared to other top-performing methods using far heavier configurations. Codes will be available at https://github.com/dginhac/ACDnet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521000568",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Cloud computing",
      "Computer science",
      "Computer security",
      "Context (archaeology)",
      "Convolutional neural network",
      "Edge device",
      "Enhanced Data Rates for GSM Evolution",
      "Exploit",
      "Feature (linguistics)",
      "Frame (networking)",
      "Frame rate",
      "Geodesy",
      "Geography",
      "Linguistics",
      "Operating system",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Yu"
      },
      {
        "surname": "Yang",
        "given_name": "Fan"
      },
      {
        "surname": "Ginhac",
        "given_name": "Dominique"
      }
    ]
  },
  {
    "title": "Mixed pooling and richer attention feature fusion for crack detection",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.02.005",
    "abstract": "Automatic image crack detection is a critical task for ensuring the safety of various facilities. However, it remains a challenging topic due to the complex background from long and sharp crack topologies. Inspired by recent advances on computer vision applications in deep learning, we propose a novel network architecture with richer feature fusion and attention mechanism and mixed pooling module for crack detection. The proposed network uses the mixed pooling module to replace the conventional spatial pooling. Moreover, we first extract the richer convolutional features to better characterize cracks. Then, we use a spatial attention (SA) in low level feature maps to capture the spatial structure information of cracks. Besides, we use a channel-wise attention (CA) to capture the features of high-level context. Finally, we fuse them together for the final crack prediction. A large crack dataset is used for training and testing. We evaluate our method on a large scale crack dataset, and experimental results on the DeepCrack dataset have demonstrate the effectiveness of the proposed method against state-of-the-art crack detection methods, which achieves Precision(P) 87.3%, Recall(R) 88.5%, and F-score over 88%.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552100060X",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Convolutional neural network",
      "Deep learning",
      "Electrical engineering",
      "Engineering",
      "Feature (linguistics)",
      "Fuse (electrical)",
      "Linguistics",
      "Machine learning",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pooling",
      "Precision and recall",
      "Systems engineering",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Qiang"
      },
      {
        "surname": "Qu",
        "given_name": "Zhong"
      },
      {
        "surname": "Cao",
        "given_name": "Chong"
      }
    ]
  },
  {
    "title": "Socially-driven multi-interaction attentive group representation learning for group recommendation",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.02.007",
    "abstract": "Group recommendation has attracted much attention since group activities information has become increasing available in many online applications. A fundamental challenge in group recommendation is how to aggregate individuals’ preferences to infer the decision of a group. However, most existing group representation methods do not take into account the static and dynamic preferences of groups synchronously, leading to the suboptimal group recommendation performance. In this work, we propose a socially-driven multi-interaction group representation approach to learn static and dynamic group preference coherently. Specifically, we inject the social homophily and social influence into capturing static and dynamic preference of a group. Furthermore, we explore latent user-item and group-item multiple interactions with bipartite graphs for group representation. Extensive experimental results on two real-world datasets verify the effectiveness of our proposed approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521000623",
    "keywords": [
      "Aggregate (composite)",
      "Aggregation problem",
      "Artificial intelligence",
      "Bipartite graph",
      "Chemistry",
      "Composite material",
      "Computer science",
      "Graph",
      "Group (periodic table)",
      "Group decision-making",
      "Group work",
      "Homophily",
      "Information retrieval",
      "Law",
      "Machine learning",
      "Materials science",
      "Mathematical economics",
      "Mathematics",
      "Organic chemistry",
      "Pedagogy",
      "Political science",
      "Politics",
      "Preference",
      "Preference learning",
      "Psychology",
      "Recommender system",
      "Representation (politics)",
      "Social group",
      "Social psychology",
      "Statistics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Peipei"
      },
      {
        "surname": "Li",
        "given_name": "Lin"
      },
      {
        "surname": "Wang",
        "given_name": "Ru"
      },
      {
        "surname": "Xu",
        "given_name": "Guandong"
      },
      {
        "surname": "Zhang",
        "given_name": "Jianwei"
      }
    ]
  },
  {
    "title": "Compact learning for multi-label classification",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107833",
    "abstract": "Multi-label classification (MLC) studies the problem where each instance is associated with multiple relevant labels, which leads to the exponential growth of output space. It confronts with the great challenge for the exploration of the latent label relationship and the intrinsic correlation between feature and label spaces. MLC gave rise to a framework named label compression (LC) to obtain a compact space for efficient learning. Nevertheless, most existing LC methods failed to consider the influence of the feature space or misguided by original problematic features, which may result in performance degradation instead. In this paper, we present a compact learning (CL) framework to embed the features and labels simultaneously and with mutual guidance. The proposal is a versatile concept that does not rigidly adhere to some specific embedding methods, and is independent of the subsequent learning process. Following its spirit, a simple yet effective implementation called compact multi-label learning (CMLL) is proposed to learn a compact low-dimensional representation for both spaces. CMLL maximizes the dependence between the embedded spaces of the labels and features, and minimizes the loss of label space recovery concurrently. Theoretically, we provide a general analysis for different embedding methods. Practically, we conduct extensive experiments to validate the effectiveness of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321000200",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Embedding",
      "Feature (linguistics)",
      "Feature learning",
      "Feature vector",
      "Geometry",
      "Law",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Multi-label classification",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Process (computing)",
      "Reduction (mathematics)",
      "Representation (politics)",
      "Space (punctuation)"
    ],
    "authors": [
      {
        "surname": "Lv",
        "given_name": "Jiaqi"
      },
      {
        "surname": "Wu",
        "given_name": "Tianran"
      },
      {
        "surname": "Peng",
        "given_name": "Chenglun"
      },
      {
        "surname": "Liu",
        "given_name": "Yunpeng"
      },
      {
        "surname": "Xu",
        "given_name": "Ning"
      },
      {
        "surname": "Geng",
        "given_name": "Xin"
      }
    ]
  },
  {
    "title": "AG3line: Active grouping and geometry-gradient combined validation for fast line segment extraction",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107834",
    "abstract": "Line segment detectors based on local image domain passively fit a line segment from a set of pixels, but no constraint on line geometry is set in the grouping process. Therefore, unstable pixels, such as the pixels in grass, clouds, or weak gradient edges, may cause false positives and fractures. This paper proposes the detector named AG3line, which employs an efficient active grouping strategy. In AG3line, the pixel for the next grouping is calculated actively with the line geometry and it can even be accurate to one pixel. To reduce the fracture caused by unstable pixels, when the adjacent pixel cannot satisfy the grouping rules, the candidate pixels for the next grouping are expanded with the line geometry constraint. To furtherly control false positives, AG3line then validates and refines the line segments by exploiting both the line geometry and the alignment of gradient magnitude. When AG3line was evaluated utilizing the image dataset with the ground truth, it outperformed both the classical and the latest detectors.The implementation of AG3line is available at https://github.com/weidong-whu/AG3line.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321000212",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Constraint (computer-aided design)",
      "Detector",
      "False positive paradox",
      "Feature detection (computer vision)",
      "Geometry",
      "Ground truth",
      "Image (mathematics)",
      "Image processing",
      "Line (geometry)",
      "Line segment",
      "Mathematics",
      "Morphological gradient",
      "Operating system",
      "Pattern recognition (psychology)",
      "Pixel",
      "Process (computing)",
      "Programming language",
      "Set (abstract data type)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Yongjun"
      },
      {
        "surname": "Wei",
        "given_name": "Dong"
      },
      {
        "surname": "Li",
        "given_name": "Yansheng"
      }
    ]
  },
  {
    "title": "Projected fuzzy C-means clustering with locality preservation",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107748",
    "abstract": "Traditional partition-based clustering algorithms, hard or fuzzy version of C-means, could not deal with high-dimensional data sets effectively as redundant features may impact the computation of distances and local spatial structures among patterns are rarely considered. High dimensionality of space gives rise to so-called concentration effect that is detrimental. In this paper, a novel locality preserving based fuzzy C-means (LPFCM) clustering method and its optimization are presented. An orthogonally projected space, which preserves the locality of structural properties, can be generated in LPFCM, thus enhancing the capability of fuzzy C-means (FCM) for handling high-dimensional data. It is the first time to introduce projection techniques to the FCM optimization objective function directly, and the ideas of fuzzy clustering, geometric structure preservation and feature extraction are seamlessly integrated. LPFCM is also regarded as a unified model that combines two separate stages of spectral clustering. Experimental results on some benchmark data sets show the effectiveness of LPFCM in comparison with FCM and some state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320305513",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Cluster analysis",
      "Clustering high-dimensional data",
      "Combinatorics",
      "Computer science",
      "Curse of dimensionality",
      "Data mining",
      "Fuzzy clustering",
      "Fuzzy logic",
      "Geodesy",
      "Geography",
      "Linguistics",
      "Locality",
      "Mathematics",
      "Partition (number theory)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Projection (relational algebra)"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Jie"
      },
      {
        "surname": "Pedrycz",
        "given_name": "Witold"
      },
      {
        "surname": "Yue",
        "given_name": "Xiaodong"
      },
      {
        "surname": "Gao",
        "given_name": "Can"
      },
      {
        "surname": "Lai",
        "given_name": "Zhihui"
      },
      {
        "surname": "Wan",
        "given_name": "Jun"
      }
    ]
  },
  {
    "title": "Distance on the Cairo pattern",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.02.002",
    "abstract": "The Cairo pattern is a dual of a semiregular tiling of the plane; its name reflects the fact that various streets in Cairo are paved in this way. The cells of this grid are identical pentagons which are put in four different orientations. In this paper we present a coordinate system addressing the cells of the grid. Also a neighborhood relation and the concept of lanes are defined by the help of the coordinates. We prove a formula for the digital (path based) distance between any two pentagons where the paths are based on the steps to neighbor pentagons.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552100057X",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Data mining",
      "Dual (grammatical number)",
      "Geometry",
      "Grid",
      "Literature",
      "Mathematics",
      "Path (computing)",
      "Plane (geometry)",
      "Programming language",
      "Relation (database)",
      "Square tiling",
      "Topology (electrical circuits)"
    ],
    "authors": [
      {
        "surname": "Kovács",
        "given_name": "Gergely"
      },
      {
        "surname": "Nagy",
        "given_name": "Benedek"
      },
      {
        "surname": "Turgay",
        "given_name": "Neşet Deniz"
      }
    ]
  },
  {
    "title": "DAE-CNN: Exploiting and disentangling contrast agent effects for breast lesions classification in DCE-MRI",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.01.023",
    "abstract": "Convolutional Neural Networks (CNNs) are opening for unprecedented scenarios in fields where designing effective features is tedious even for domain experts. This is the case of medical imaging, i.e. procedures acquiring images of a human body interior for clinical proposes. Despite promising, we argue that CNNs naive use may not be effective since “medical images are more than pictures”. A notable example is breast Dynamic Contrast Enhanced-Magnetic Resonance Imaging (DCE-MRI), in which the kinetic of the injected Contrast Agent (CA) is crucial for lesion classification purposes. Therefore, in this work we introduce a new GAN like approach designed to simultaneously learn how to disentangle the CA effects from all the other image components while performing the lesion classification: the generator is an intrinsic Deforming Autoencoder (DAE), while the discriminator is a CNN. We compared the performance of the proposed approach against some literature proposals (both classical and CNN based) using patient-wise cross-validation. Finally, for the sake of completeness, we also analyzed the impact of variations in some key aspect of the proposed solution. Results not only show the effectiveness of our approach ( + 8 % AUC w.r.t. the runner-up) but also confirm that all the approach’s components effectively contribute to the solution.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521000428",
    "keywords": [
      "Artificial intelligence",
      "Autoencoder",
      "Completeness (order theory)",
      "Computer science",
      "Contrast (vision)",
      "Convolutional neural network",
      "Deep learning",
      "Detector",
      "Discriminator",
      "Dynamic contrast",
      "Generator (circuit theory)",
      "Image (mathematics)",
      "Magnetic resonance imaging",
      "Mathematical analysis",
      "Mathematics",
      "Medical imaging",
      "Medicine",
      "Pattern recognition (psychology)",
      "Physics",
      "Power (physics)",
      "Quantum mechanics",
      "Radiology",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Gravina",
        "given_name": "Michela"
      },
      {
        "surname": "Marrone",
        "given_name": "Stefano"
      },
      {
        "surname": "Sansone",
        "given_name": "Mario"
      },
      {
        "surname": "Sansone",
        "given_name": "Carlo"
      }
    ]
  },
  {
    "title": "Inferring spatial relations from textual descriptions of images",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107847",
    "abstract": "Generating an image from its textual description requires both a certain level of language understanding and common sense knowledge about the spatial relations of the physical entities being described. In this work, we focus on inferring the spatial relation between entities, a key step in the process of composing scenes based on text. More specifically, given a caption containing a mention to a subject and the location and size of the bounding box of that subject, our goal is to predict the location and size of an object mentioned in the caption. Previous work did not use the caption text information, but a manually provided relation holding between the subject and the object. In fact, the used evaluation datasets contain manually annotated ontological triplets but no captions, making the exercise unrealistic: a manual step was required; and systems did not leverage the richer information in captions. Here we present a system that uses the full caption, and Relations in Captions (REC-COCO), a dataset derived from MS-COCO which allows to evaluate spatial relation inference from captions directly. Our experiments show that: (1) it is possible to infer the size and location of an object with respect to a given subject directly from the caption; (2) the use of full text allows to place the object better than using a manually annotated relation. Our work paves the way for systems that, given a caption, decide which entities need to be depicted and their respective location and sizes, in order to then generate the final image.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321000340",
    "keywords": [
      "Artificial intelligence",
      "Bounding overwatch",
      "Computer science",
      "Data mining",
      "Focus (optics)",
      "Image (mathematics)",
      "Inference",
      "Information retrieval",
      "Leverage (statistics)",
      "Minimum bounding box",
      "Natural language processing",
      "Object (grammar)",
      "Operating system",
      "Optics",
      "Physics",
      "Process (computing)",
      "Relation (database)",
      "Spatial relation",
      "Subject (documents)",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Elu",
        "given_name": "Aitzol"
      },
      {
        "surname": "Azkune",
        "given_name": "Gorka"
      },
      {
        "surname": "de Lacalle",
        "given_name": "Oier Lopez"
      },
      {
        "surname": "Arganda-Carreras",
        "given_name": "Ignacio"
      },
      {
        "surname": "Soroa",
        "given_name": "Aitor"
      },
      {
        "surname": "Agirre",
        "given_name": "Eneko"
      }
    ]
  },
  {
    "title": "SmaAt-UNet: Precipitation nowcasting using a small attention-UNet architecture",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.01.036",
    "abstract": "Weather forecasting is dominated by numerical weather prediction that tries to model accurately the physical properties of the atmosphere. A downside of numerical weather prediction is that it is lacking the ability for short-term forecasts using the latest available information. By using a data-driven neural network approach we show that it is possible to produce an accurate precipitation nowcast. To this end, we propose SmaAt-UNet, an efficient convolutional neural networks-based on the well known UNet architecture equipped with attention modules and depthwise-separable convolutions. We evaluate our approaches on a real-life datasets using precipitation maps from the region of the Netherlands and binary images of cloud coverage of France. The experimental results show that in terms of prediction performance, the proposed model is comparable to other examined models while only using a quarter of the trainable parameters.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521000556",
    "keywords": [],
    "authors": [
      {
        "surname": "Trebing",
        "given_name": "Kevin"
      },
      {
        "surname": "Staǹczyk",
        "given_name": "Tomasz"
      },
      {
        "surname": "Mehrkanoon",
        "given_name": "Siamak"
      }
    ]
  },
  {
    "title": "Fragmentary label distribution learning via graph regularized maximum entropy criteria",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.01.026",
    "abstract": "Label distribution learning (LDL) is a new learning paradigm, which assumes that the labels are related to each instance to some degree. It has been successfully applied in many scenarios due to its ability to tackle label ambiguity. Nevertheless, LDL has also enlarged the labeling costs and difficulties. In many real application areas, such as pattern recognition and image classification, the labeling information is often incomplete, i.e., we cannot determine each label degree to each instance. The performance of traditional LDL algorithm will degrade since they often require complete supervised information. In this paper, we have proposed fragmentary LDL algorithm via Graph Regularized Maximum Entropy criteria (GRME) to solve this problem. It explores the relationship among the labels to recover missing label factors, together with a classifier for categorization. The integration of these two components facilitate the performance enhancement of GRME. Besides, compared with most of traditional methods which are transductive, another advantage of our method is the inductive nature to predict for new coming data directly. We have also extended ADMM algorithm to fit this particular nonconvex problem. The effectiveness of our method is verified by the experimental results on real-world data sets with different settings.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521000453",
    "keywords": [
      "Ambiguity",
      "Artificial intelligence",
      "Categorization",
      "Classifier (UML)",
      "Computer science",
      "Entropy (arrow of time)",
      "Graph",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Programming language",
      "Quantum mechanics",
      "Semi-supervised learning",
      "Text categorization",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Chao"
      },
      {
        "surname": "Gu",
        "given_name": "Shilin"
      },
      {
        "surname": "Tao",
        "given_name": "Hong"
      },
      {
        "surname": "Hou",
        "given_name": "Chenping"
      }
    ]
  },
  {
    "title": "View-invariant action recognition via Unsupervised AttentioN Transfer (UANT)",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107807",
    "abstract": "With wide applications in surveillance and human-robot interaction, view-invariant human action recognition is critical, however, challenging, due to the action occlusion and information loss caused by view change. Current methods mainly seek for a common feature space for different views. However, such solutions become invalid when there exist few common features, e.g. large view change. To tackle the problem, we propose an Unsupervised AttentioN Transfer (UANT) approach for view-invariant action recognition. Other than transferring feature knowledge, UANT transfers attention from one selected reference view to arbitrary views, which correctly emphasizes crucial body joints and their relations for view-invariant representation. In addition, the attention calculation method taking into account both recognition contribution and reliability of skeleton joints generates effective attention. Experiments showed its effectiveness for correctly locating crucial body joints in action sequences. We exhaustively evaluate our approach on the UESTC and the NTU dataset, performing unsupervised view-invariant evaluations, i.e. X-view and Arbitrary-view recognition. Experiment results demonstrate its superiority in view-invariant representation and recognition.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320306105",
    "keywords": [
      "Action (physics)",
      "Action recognition",
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Feature (linguistics)",
      "Invariant (physics)",
      "Law",
      "Linguistics",
      "Machine learning",
      "Mathematical physics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Political science",
      "Politics",
      "Quantum mechanics",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Ji",
        "given_name": "Yanli"
      },
      {
        "surname": "Yang",
        "given_name": "Yang"
      },
      {
        "surname": "Shen",
        "given_name": "Heng Tao"
      },
      {
        "surname": "Harada",
        "given_name": "Tatsuya"
      }
    ]
  },
  {
    "title": "Visual place recognition: A survey from deep learning perspective",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107760",
    "abstract": "Visual place recognition has attracted widespread research interest in multiple fields such as computer vision and robotics. Recently, researchers have employed advanced deep learning techniques to tackle this problem. While an increasing number of studies have proposed novel place recognition methods based on deep learning, few of them has provided a whole picture about how and to what extent deep learning has been utilized for this issue. In this paper, by delving into over 200 references, we present a comprehensive survey that covers various aspects of place recognition from deep learning perspective. We first present a brief introduction of deep learning and discuss its opportunities for recognizing places. After that, we focus on existing approaches built upon convolutional neural networks, including off-the-shelf and specifically designed models as well as novel image representations. We also discuss challenging problems in place recognition and present an extensive review of the corresponding datasets. To explore the future directions, we describe open issues and some new tools, for instance, generative adversarial networks, semantic scene understanding and multi-modality feature learning for this research topic. Finally, a conclusion is drawn for this paper.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032030563X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Data science",
      "Deep learning",
      "Feature (linguistics)",
      "Focus (optics)",
      "Generative grammar",
      "Linguistics",
      "Machine learning",
      "Modality (human–computer interaction)",
      "Open research",
      "Optics",
      "Perspective (graphical)",
      "Philosophy",
      "Physics",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Xiwu"
      },
      {
        "surname": "Wang",
        "given_name": "Lei"
      },
      {
        "surname": "Su",
        "given_name": "Yan"
      }
    ]
  },
  {
    "title": "Kernel two-dimensional ridge regression for subspace clustering",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107749",
    "abstract": "Subspace clustering methods have been extensively studied in recent years. For 2-dimensional (2D) data, existing subspace clustering methods usually convert 2D examples to vectors, which severely damages inherent structural information and relationships of the original data. In this paper, we propose a novel subspace clustering method, named KTRR, for 2D data. The KTRR provides us with a way to learn the most representative 2D features from 2D data in learning data representation. In particular, the KTRR performs 2D feature learning and low-dimensional representation construction simultaneously, which renders the two tasks to mutually enhance each other. 2D kernel is introduced to the KTRR, which renders the KTRR to have enhanced capability of capturing nonlinear relationships from data. An efficient algorithm is developed for its optimization with provable decreasing and convergent property in objective value. Extensive experimental results confirm the effectiveness and efficiency of our method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320305525",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Clustering high-dimensional data",
      "Combinatorics",
      "Computer science",
      "Data mining",
      "Feature (linguistics)",
      "Kernel (algebra)",
      "Kernel method",
      "Law",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Subspace topology",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Peng",
        "given_name": "Chong"
      },
      {
        "surname": "Zhang",
        "given_name": "Qian"
      },
      {
        "surname": "Kang",
        "given_name": "Zhao"
      },
      {
        "surname": "Chen",
        "given_name": "Chenglizhao"
      },
      {
        "surname": "Cheng",
        "given_name": "Qiang"
      }
    ]
  },
  {
    "title": "Face spoofing detection under super-realistic 3D wax face attacks",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.01.021",
    "abstract": "Face spoofing attacks based on 3D face images have posed a severe security risk to face recognition systems. Despite the great effort made by the technical community in recent years, existing 3D face spoofing databases, mostly based on 3D masks, still suffer from small sample size, low diversity, or poor authenticity due to the production difficulty and high cost. To fill in this gap, we introduce a new database in this paper with 4-000 single wax figure faces, named SWFFD (Single Wax Figure Face Database), as a type of super-realistic 3D face presentation attack. Collected from online resources, this database has high diversity in terms of subjects, lighting conditions, facial poses, and recording devices. We have also designed a new detection method, which combines attention-aware features from different face scales to generate discriminative representations for realistic face spoofing attack detection. Extensive experiments have been conducted on the SWFFD as well as the CelebA-HQ database (containing real faces from the online collection). Experimental results have demonstrated the effectiveness of the proposed method in both intra-database and cross-database testing scenarios.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521000404",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Database",
      "Discriminative model",
      "Face (sociological concept)",
      "Facial recognition system",
      "Pattern recognition (psychology)",
      "Social science",
      "Sociology",
      "Spoofing attack"
    ],
    "authors": [
      {
        "surname": "Jia",
        "given_name": "Shan"
      },
      {
        "surname": "Hu",
        "given_name": "Chuanbo"
      },
      {
        "surname": "Li",
        "given_name": "Xin"
      },
      {
        "surname": "Xu",
        "given_name": "Zhengquan"
      }
    ]
  },
  {
    "title": "Evolutionary multi-objective optimization based overlapping subspace clustering",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.02.012",
    "abstract": "Subspace clustering techniques divide the data set into various groups, where each group is represented by a subset of features known as subspace feature set, that are relevant to the objects in the group. The grouping is performed in such a way that similar objects are placed in the same group, whereas dissimilar objects are in different groups. Most of the previous subspace clustering methods have not considered an object to be a part of more than one cluster. However, in many real-life situations, an object may belong to more than one cluster. Moreover , subspace clustering algorithms developed in the past are based on single objective optimization framework which limits in optimizing only a particular shape or property of the clusters. To this end, we have developed an evolutionary-based overlapped subspace clustering method using multi-objective optimization framework. Various mutation operators have been used to explore the search space effectively. Multiple objectives that have been optimized simultaneously in this algorithm are ICC-index, MNR-index and PSM-index. The developed algorithm is evaluated with 7 real-life and 16 synthetic data sets. However, to check the efficiency of using multiple objectives, the proposed algorithm is also tested with 3 big data sets. An application of the proposed method is shown in bi-clustering the gene expression data. The results obtained using these 23 data sets and 3 big data sets are compared with many state-of-the-art algorithms. The comparative study illustrates the efficacy of the proposed algorithm over state-of-the-art algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521000672",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Mathematical optimization",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Subspace topology"
    ],
    "authors": [
      {
        "surname": "Paul",
        "given_name": "Dipanjyoti"
      },
      {
        "surname": "Saha",
        "given_name": "Sriparna"
      },
      {
        "surname": "Kumar",
        "given_name": "Abhishek"
      },
      {
        "surname": "mathew",
        "given_name": "Jimson"
      }
    ]
  },
  {
    "title": "Copycat CNN: Are random non-Labeled data enough to steal knowledge from black-box models?",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107830",
    "abstract": "Convolutional neural networks have been successful lately enabling companies to develop neural-based products, which demand an expensive process, involving data acquisition and annotation; and model generation, usually requiring experts. With all these costs, companies are concerned about the security of their models against copies and deliver them as black-boxes accessed by APIs. Nonetheless, we argue that even black-box models still have some vulnerabilities. In a preliminary work, we presented a simple, yet powerful, method to copy black-box models by querying them with natural random images. In this work, we consolidate and extend the copycat method: (i) some constraints are waived; (ii) an extensive evaluation with several problems is performed; (iii) models are copied between different architectures; and, (iv) a deeper analysis is performed by looking at the copycat behavior. Results show that natural random images are effective to generate copycats for several problems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321000170",
    "keywords": [
      "Artificial intelligence",
      "Black box",
      "Computer science",
      "Convolutional neural network",
      "Copycat",
      "Machine learning",
      "Process (computing)",
      "Programming language"
    ],
    "authors": [
      {
        "surname": "Correia-Silva",
        "given_name": "Jacson Rodrigues"
      },
      {
        "surname": "Berriel",
        "given_name": "Rodrigo F."
      },
      {
        "surname": "Badue",
        "given_name": "Claudine"
      },
      {
        "surname": "De Souza",
        "given_name": "Alberto F."
      },
      {
        "surname": "Oliveira-Santos",
        "given_name": "Thiago"
      }
    ]
  },
  {
    "title": "Experiencing with electronic image stabilization and PRNU through scene content image registration",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.01.014",
    "abstract": "This paper explores content-based image registration as a means of dealing with and understanding better Electronic Image Stabilization (EIS) in the context of Photo Response Non-Uniformity (PRNU) alignment. A novel and robust solution to extrapolate the transformation relating the different image output formats for a given device model is proposed. This general approach can be adapted to specifically extract the scale factor (and, when appropriate, the translation) so as to align native resolution images to video frames, with or without EIS on, and proceed to compare PRNU patterns. Comparative evaluations show that the proposed approach outperforms those based on brute-force and particle swarm optimization in terms of reliability, accuracy and speed. Furthermore, a tracking system able to revert back EIS in controlled environments is designed. This allows one to investigate the differences between the existing EIS implementations. The additional knowledge thus acquired can be exploited and integrated in order to design and implement better future PRNU pattern alignment methods, aware of EIS and suitable for video source identification in multimedia forensics applications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521000271",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Biology",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Gene",
      "Geometric transformation",
      "Image (mathematics)",
      "Machine learning",
      "Paleontology",
      "Particle swarm optimization",
      "Physics",
      "Power (physics)",
      "Quantum mechanics",
      "Reliability (semiconductor)",
      "Transformation (genetics)"
    ],
    "authors": [
      {
        "surname": "Bellavia",
        "given_name": "Fabio"
      },
      {
        "surname": "Fanfani",
        "given_name": "Marco"
      },
      {
        "surname": "Colombo",
        "given_name": "Carlo"
      },
      {
        "surname": "Piva",
        "given_name": "Alessandro"
      }
    ]
  },
  {
    "title": "Learning Robust Feature Transformation for Domain Adaptation",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107870",
    "abstract": "There is a growing importance of feature extraction in transferring valuable knowledge from a source domain to a different but related target domain. However, when the target data are contaminated by unpredictable and complex noises, the ability of most existing feature extraction methods would be limited. In this paper, we deeply investigate the robust property of Kernel Mean P-Power Error Loss (KMPE-Loss), and thus propose a novel Robust Transfer Feature Learning (RTFL) method to enhance the robustness of domain adaptation. The key idea of RTFL is to learn a shared transformation by: 1) detecting and neglecting the contaminated target points without any specific assumption on noises; 2) reconstructing the remaining clean target points using the corresponding source-domain neighborhood; 3) incorporating a relative entropy based regularization to reap theoretic advantages. Consequently, the distribution difference between two domains is accurately reduced for knowledge transfer. We propose an alternative procedure to optimize RTFL with explicitly guaranteed convergence. As an extension, the transformation based matrix in RTFL is restricted to a small dimension basis, admitting the highly reduced computation complexity. Extensive experiments in various domain adaptation tasks demonstrate the superiority of our methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321000571",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Classifier (UML)",
      "Computation",
      "Computer science",
      "Domain adaptation",
      "Entropy (arrow of time)",
      "Feature extraction",
      "Gene",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Robustness (evolution)",
      "Transformation (genetics)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Wei"
      },
      {
        "surname": "Wang",
        "given_name": "Hao"
      },
      {
        "surname": "Ran",
        "given_name": "Zhi-Yong"
      },
      {
        "surname": "He",
        "given_name": "Ran"
      }
    ]
  },
  {
    "title": "Adversarial robustness via attention transfer",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.03.011",
    "abstract": "Deep neural networks are known to be vulnerable to adversarial attacks. The empirical analysis in our study suggests that attacks tend to induce diverse network architectures to shift the attention to irrelevant regions. Motivated by this observation, we propose a regularization technique which enforces the attentions to be well aligned via the knowledge transfer mechanism, thereby encouraging the robustness. Resultant model exhibits unprecedented robustness, securing 63.81 % adversarial accuracy where the prior art is 51.59 % on CIFAR-10 dataset under PGD attacks. In addition, we go beyond performance to analytically investigate the proposed method as an effective defense. Significantly flattened loss landscape can be observed, demonstrating the promise of the proposed method for improving robustness and thus the deployment in security-sensitive settings.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521000982",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Deep neural networks",
      "Gene",
      "Machine learning",
      "Operating system",
      "Regularization (linguistics)",
      "Robustness (evolution)",
      "Software deployment"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Zhuorong"
      },
      {
        "surname": "Feng",
        "given_name": "Chao"
      },
      {
        "surname": "Wu",
        "given_name": "Minghui"
      },
      {
        "surname": "Yu",
        "given_name": "Hongchuan"
      },
      {
        "surname": "Zheng",
        "given_name": "Jianwei"
      },
      {
        "surname": "Zhu",
        "given_name": "Fanwei"
      }
    ]
  },
  {
    "title": "View-graph construction framework for robust and efficient structure-from-motion",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107712",
    "abstract": "A view-graph is vital for both the accuracy and robustness of structure-from-motion (SfM). Conventional matrix decomposition techniques treat all edges of view-graph equally; hence, many edge outliers are produced in matching pairs with fewer feature matches. To address this problem, we propose an incremental framework for view-graph construction, where the robustness of matched pairs that have a larger number of feature matches is propagated to their connected images. Given pairwise feature matches, a verified maximum spanning tree (VMST) is first constructed; for each edge in the VMST, we perform a local reconstruction and register its visible cameras. Based on the local reconstruction, pairwise relative geometries are computed and some new epipolar edges are produced. In this way, these newly computed edges inherit the robustness and accuracy of VMST, and by embedding them into VMST, our view-graph is constructed. We feed our view-graph into a standard SfM pipeline and compare this newly formed system with many of state-of-the-art SfM methods. The experimental results demonstrate that our view-graph provides a better foundation for conventional SfM systems, and enables them to reconstruct both general and ambiguous images.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032030515X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Epipolar geometry",
      "Gene",
      "Graph",
      "Image (mathematics)",
      "Mathematics",
      "Motion estimation",
      "Outlier",
      "Pairwise comparison",
      "Pattern recognition (psychology)",
      "Robustness (evolution)",
      "Spanning tree",
      "Structure from motion",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Cui",
        "given_name": "Hainan"
      },
      {
        "surname": "Shi",
        "given_name": "Tianxin"
      },
      {
        "surname": "Zhang",
        "given_name": "Jun"
      },
      {
        "surname": "Xu",
        "given_name": "Pengfei"
      },
      {
        "surname": "Meng",
        "given_name": "Yiping"
      },
      {
        "surname": "Shen",
        "given_name": "Shuhan"
      }
    ]
  },
  {
    "title": "Estimating the standard error of cross-Validation-Based estimators of classifier performance",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.02.022",
    "abstract": "First, we analyze the variance of the Cross Validation (CV)-based estimators used for estimating the performance of classification rules. Second, we propose a novel estimator to estimate this variance using the Influence Function (IF) approach that had been used previously very successfully to estimate the variance of the bootstrap-based estimators. The motivation for this research is that, as the best of our knowledge, the literature lacks a rigorous method for estimating the variance of the CV-based estimators. What is available is a set of ad-hoc procedures that have no mathematical foundation since they ignore the covariance structure among dependent random variables. The conducted experiments show that the IF proposed method has small RMS error with some bias. However, surprisingly, the ad-hoc methods still work better than the IF-based method. Unfortunately, this is due to the lack of enough smoothness if compared to the bootstrap estimator. This opens the research for three points: (1) more comprehensive simulation study to clarify when the IF method wins or looses; (2) more mathematical analysis to figure out why the ad-hoc methods work well; and (3) more mathematical treatment to figure out the connection between the appropriate amount of “smoothness” and decreasing the bias of the IF method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552100091X",
    "keywords": [
      "Accounting",
      "Algorithm",
      "Business",
      "Computer science",
      "Covariance",
      "Estimator",
      "Mathematical analysis",
      "Mathematics",
      "Smoothness",
      "Statistics",
      "Variance (accounting)"
    ],
    "authors": [
      {
        "surname": "Yousef",
        "given_name": "Waleed A."
      }
    ]
  },
  {
    "title": "Leveraging auxiliary image descriptions for dense video captioning",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.02.009",
    "abstract": "Collecting textual descriptions is an especially costly task for dense video captioning, since each event in the video needs to be annotated separately and a long descriptive paragraph needs to be provided. In this paper, we investigate a way to mitigate this heavy burden and propose to leverage captions of visually similar images as auxiliary context. Our model successfully fetches visually relevant images and combines noun and verb phrases from their captions to generating coherent descriptions. To this end, we use a generator and discriminator design, together with an attention-based fusion technique, to incorporate image captions as context in the video caption generation process. The experiments on the challenging ActivityNet Captions dataset demonstrate that our proposed approach achieves more accurate and more diverse video descriptions compared to the strong baseline using METEOR, BLEU and CIDEr-D metrics and qualitative evaluations.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521000647",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Closed captioning",
      "Computer science",
      "Context (archaeology)",
      "Detector",
      "Discriminator",
      "Economics",
      "Encoder",
      "Generator (circuit theory)",
      "Image (mathematics)",
      "Leverage (statistics)",
      "Management",
      "Natural language processing",
      "Noun",
      "Operating system",
      "Paleontology",
      "Paragraph",
      "Physics",
      "Power (physics)",
      "Quantum mechanics",
      "Speech recognition",
      "Task (project management)",
      "Telecommunications",
      "Verb",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Boran",
        "given_name": "Emre"
      },
      {
        "surname": "Erdem",
        "given_name": "Aykut"
      },
      {
        "surname": "Ikizler-Cinbis",
        "given_name": "Nazli"
      },
      {
        "surname": "Erdem",
        "given_name": "Erkut"
      },
      {
        "surname": "Madhyastha",
        "given_name": "Pranava"
      },
      {
        "surname": "Specia",
        "given_name": "Lucia"
      }
    ]
  },
  {
    "title": "Deep ancient Roman Republican coin classification via feature fusion and attention",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107871",
    "abstract": "We perform the classification of ancient Roman Republican coins via recognizing their reverse motifs where various objects, faces, scenes, animals, and buildings are minted along with legends. Most of these coins are eroded due to their age and varying degrees of preservation, thereby affecting their informative attributes for visual recognition. Changes in the positions of principal symbols on the reverse motifs also cause huge variations among the coin types. Lastly, in-plane orientations, uneven illumination, and a moderate background clutter further make the classification task non-trivial and challenging. To this end, we present a novel network model, CoinNet, that employs compact bilinear pooling, residual groups, and feature attention layers. Furthermore, we gathered the largest and most diverse image dataset of the Roman Republican coins that contains more than 18,000 images belonging to 228 different reverse motifs. On this dataset, our model achieves a classification accuracy of more than 98% and outperforms the conventional bag-of-visual-words based approaches and more recent state-of-the-art deep learning methods. We also provide a detailed ablation study of our network and its generalization capability.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321000583",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Deep learning",
      "Feature (linguistics)",
      "Generalization",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pooling"
    ],
    "authors": [
      {
        "surname": "Anwar",
        "given_name": "Hafeez"
      },
      {
        "surname": "Anwar",
        "given_name": "Saeed"
      },
      {
        "surname": "Zambanini",
        "given_name": "Sebastian"
      },
      {
        "surname": "Porikli",
        "given_name": "Fatih"
      }
    ]
  },
  {
    "title": "Adaptive super-resolution for person re-identification with low-resolution images",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107682",
    "abstract": "Person re-identification is challenging with low-resolution query and high-resolution gallery images. To address the resolution mismatch, many methods perform super-resolution (SR) on low-resolution queries with specifying a single scale factor. However, using a single SR module, whichever scale factor is specified, always brings both advantages and drawbacks in recovering and identifying identity information. A larger scale factor recovers more details but produces excessive artifacts, while a smaller one is on the contrary. To exploit their complementary property for more robust recovery and identification, we propose the Adaptive Person Super-Resolution (APSR) model. APSR jointly trains and fuses multiple SR modules based on their generated visual contents, to fully compensate and learn the complementary identity features in an end-to-end manner. To improve the robustness to artifacts during fusion, our model further learns informative features by online dividing and integrating the generated body regions. Extensive experiments verify the effectiveness of our method with state-of-the-art performances.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320304854",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Biology",
      "Botany",
      "Chemistry",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Epistemology",
      "Exploit",
      "Factor (programming language)",
      "Gene",
      "Geology",
      "High resolution",
      "Identification (biology)",
      "Image (mathematics)",
      "Low resolution",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Programming language",
      "Property (philosophy)",
      "Quantum mechanics",
      "Remote sensing",
      "Resolution (logic)",
      "Robustness (evolution)",
      "Scale (ratio)",
      "Superresolution"
    ],
    "authors": [
      {
        "surname": "Han",
        "given_name": "Ke"
      },
      {
        "surname": "Huang",
        "given_name": "Yan"
      },
      {
        "surname": "Song",
        "given_name": "Chunfeng"
      },
      {
        "surname": "Wang",
        "given_name": "Liang"
      },
      {
        "surname": "Tan",
        "given_name": "Tieniu"
      }
    ]
  },
  {
    "title": "A novel multi-loss-based deep adversarial network for handling challenging cases in semi-supervised image semantic segmentation",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.03.017",
    "abstract": "Image semantic segmentation is popular in computer vision and pattern recognition, since the high-level semantic understanding of images can be effectively realized. Based on whether and to what extent the training data should be labeled, most image semantic segmentation methods can be categorized into fully-supervised learning-based methods, weakly-supervised learning-based methods, and semi-supervised learning-based methods. Among them, semi-supervised image semantic segmentation receives increasing popularity recently, because of its flexibility and convenience in requiring partial training data to be labeled. Although semi-supervised image semantic segmentation is promising, its state-of-the-arts haven’t obtained satisfactory performance when handling challenging cases, including poor illumination, small-sized targets, multi-targets with the same semantics, etc. To tackle the above dilemmas, a novel multi-loss-based deep adversarial network is proposed in this paper. Technically, the more robust WGAN-GP model is utilized as the backbone of the novel network, instead of the conventional GAN model. Moreover, multiple losses including the cross entropy loss, the edge detection loss, the adversarial loss, and the semi-supervised loss, are all incorporated during the novel network’s training. Experimental analyses based on challenging cases shortlisted from the Pascal VOC 2012 dataset and the Cityscapes dataset suggest that, the novel network is capable to outperform state-of-the-arts in semi-supervised image semantic segmentation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521001045",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Deep learning",
      "Image segmentation",
      "Machine learning",
      "Pascal (unit)",
      "Pattern recognition (psychology)",
      "Programming language",
      "Segmentation",
      "Semantics (computer science)",
      "Supervised learning"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Wei"
      },
      {
        "surname": "Shao",
        "given_name": "Zhanfei"
      },
      {
        "surname": "Luo",
        "given_name": "Mingyuan"
      },
      {
        "surname": "Zhang",
        "given_name": "Peng"
      },
      {
        "surname": "Zha",
        "given_name": "Yufei"
      }
    ]
  },
  {
    "title": "An efficient computational algorithm for Hausdorff distance based on points-ruling-out and systematic random sampling",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107857",
    "abstract": "This paper proposes a novel algorithm for fast and accurate Hausdorff distance (HD) computation. The Hausdorff distance is used to measure the similarity between two point sets in various applications. However, it is hard to compute the HD algorithm efficiently between very large-scale point sets while ensuring the accuracy of the HD. The directed HD algorithm has two loops (called the outer loop and the inner loop) for calculating MAX-MIN distance, and the state-of-the-art algorithms, such as the Early break method and the Diffusion search method, focused on reducing the iterations of the inner loop. Our algorithm, however, concentrates on reducing the iterations of the outer loop. The proposed method simultaneously computes the temporary HD and temporary minimum distances of points corresponding to the outer loop using the opposite HD computation with very small systematic samples. Thereafter, a strategy of ruling out is employed to exclude non-contributing points. The new approach reduces the problems of different grid sizes and highly overlapping point sets as well as the very large-scale point sets. 3-D point clouds and real brain tumor segmentation (MRI 3-D volumes) are used for comparing the performance of the proposed algorithm and the state-of-the-art HD algorithms. In experimental results with 3-D point clouds, the proposed method is more than at least 1.5 times as faster as the compared algorithms. And, in experimental results with MRI 3-D volumes, the proposed method achieves a better performance than the compared algorithms over all pairs regardless of the grid size. Thus, as a whole, the proposed algorithm outperforms the compared algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321000443",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Combinatorics",
      "Computation",
      "Computer science",
      "Geometry",
      "Hausdorff distance",
      "Image (mathematics)",
      "Loop (graph theory)",
      "Mathematics",
      "Physics",
      "Point (geometry)",
      "Point cloud",
      "Quantum mechanics",
      "Scale (ratio)",
      "Segmentation",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Ryu",
        "given_name": "Jegoon"
      },
      {
        "surname": "Kamata",
        "given_name": "Sei-ichiro"
      }
    ]
  },
  {
    "title": "Three-dimensional choroid neovascularization growth prediction from longitudinal retinal OCT images based on a hybrid model",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.03.006",
    "abstract": "Choroid neovascularization (CNV) is a pathological manifestation of retinal-choroidal diseases such as age-related macular degeneration and pathological myopia, which can cause permanent loss of central vision. Prediction of its growth is important in treatment planning. In this paper, based on longitudinal optical coherence tomography (OCT) volumes, a three-dimensional CNV growth prediction framework is proposed. A hybrid model which combines the reaction-diffusion model and the hyperelastic biomechanical model through mass effect is adopted to characterize the growth of CNV region and its reaction with surround tissues. A treatment factor is also included so that the model can adjust to different treatment plan each patient receives. Tested on a dataset with 6 subjects, each with 12 longitudinal 3D images, the proposed method achieved average true positive rate (TPR), false positive rate (FPR) and Dice coefficient (DC) of 80.0 ± 7.62%, 23.4 ± 8.36% and 78.9 ± 7.54%, respectively, in predicting the future CNV regions, and outperforms those achieved by the single reaction-diffusion model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521000854",
    "keywords": [
      "Artificial intelligence",
      "Choroid",
      "Choroidal neovascularization",
      "Computer science",
      "Image (mathematics)",
      "Image segmentation",
      "Macular degeneration",
      "Medicine",
      "Neuroscience",
      "Ophthalmology",
      "Optical coherence tomography",
      "Pathological",
      "Pathology",
      "Psychology",
      "Retina",
      "Retinal",
      "Sørensen–Dice coefficient"
    ],
    "authors": [
      {
        "surname": "Meng",
        "given_name": "Qingquan"
      },
      {
        "surname": "Zuo",
        "given_name": "Chang"
      },
      {
        "surname": "Shi",
        "given_name": "Fei"
      },
      {
        "surname": "Zhu",
        "given_name": "Weifang"
      },
      {
        "surname": "Xiang",
        "given_name": "Dehui"
      },
      {
        "surname": "Chen",
        "given_name": "Haoyu"
      },
      {
        "surname": "Chen",
        "given_name": "Xinjian"
      }
    ]
  },
  {
    "title": "First person video summarization using different graph representations",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.03.013",
    "abstract": "First-person video summarization has emerged as an important research problem for computer vision and multimedia communities. In this paper, we show how different graph representations can be developed for accurately summarizing first-person (egocentric) videos in a computationally efficient manner. Each frame in a video is first represented as a weighted graph. A shot boundary detection method using graph based mutual information is developed. We next construct a weighted graph for each shot. A representative frame from each shot is selected using a graph centrality measure. A new way of characterizing egocentric video frames using a graph based center-surround model is shown next. Here, each representative frame is modeled as a union of a center region (graph) and a surround region (graph). By exploiting spectral measures of dissimilarity between the two (center and surround) graphs, optimal center and surround regions are determined. Optimal regions for all frames within a shot are kept the same as that of the representative frame. Center-surround differences in entropy and optical flow values along with PHOG (Pyramidal HOG) features are extracted from each frame. All frames in a video are finally represented by another weighted graph, termed as a Video Similarity Graph (VSG). The frames are clustered by applying a Minimum Spanning Tree (MST) based approach with a new measure for inadmissible edges. Frames closest to the centroid of each cluster are captured to build the summary. Experimental evaluation on two benchmark datasets indicate the advantage of the proposed formulation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521001008",
    "keywords": [
      "Artificial intelligence",
      "Automatic summarization",
      "Centroid",
      "Computer science",
      "Computer vision",
      "Graph",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Sahu",
        "given_name": "Abhimanyu"
      },
      {
        "surname": "Chowdhury",
        "given_name": "Ananda S."
      }
    ]
  },
  {
    "title": "Optical flow and scene flow estimation: A survey",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107861",
    "abstract": "Motion analysis is one of the most fundamental and challenging problems in the field of computer vision, which can be widely applied in many areas, such as autonomous driving, action recognition, scene understanding, and robotics. In general, the displacement field between subsequent frames can be divided into two types: optical flow and scene flow. The optical flow represents the pixel motion of adjacent frames. In contrast, the scene flow is a 3D motion field of the dynamic scene between two frames. Traditional approaches for the estimation of optical flow and scene flow usually leverage the variational technique, which can be solved as an energy minimization process. In recent years, deep learning has emerged as a powerful technique for learning feature representations directly from data. It has led to remarkable progress in the field of optical flow and scene flow estimation. In this paper, we provide a comprehensive survey of optical flow and scene flow estimation. First, we briefly review the pioneering approaches that use variational technique and then we delve in detail into the deep learning-based approaches. Furthermore, we present insightful observations on evaluation issues, specifically benchmark datasets, evaluation metrics, and state-of-the-art performance. Finally, we give the promising directions for future research. To the best of our knowledge, we are the first to review both optical flow and scene flow estimation, and the first to cover both traditional and deep learning-based approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321000480",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Field (mathematics)",
      "Flow (mathematics)",
      "Geometry",
      "Image (mathematics)",
      "Leverage (statistics)",
      "Machine learning",
      "Mathematics",
      "Motion estimation",
      "Optical flow",
      "Pure mathematics"
    ],
    "authors": [
      {
        "surname": "Zhai",
        "given_name": "Mingliang"
      },
      {
        "surname": "Xiang",
        "given_name": "Xuezhi"
      },
      {
        "surname": "Lv",
        "given_name": "Ning"
      },
      {
        "surname": "Kong",
        "given_name": "Xiangdong"
      }
    ]
  },
  {
    "title": "Building outlier detection ensembles by selective parameterization of heterogeneous methods",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.03.008",
    "abstract": "We address the problem of selecting members of ensembles for unsupervised outlier detection. The challenge here is to identify individually accurate but diverse members due to unsupervised nature of the problem. For this, we herein propose AnD-SELECT: Accurate-and-Diverse Selector, which considers a set of heterogeneous outlier detection methods at input and systematically selects accurate parameter variants i.e. parameterization of each type. Outlier detection methods in this input set are chosen such that they usually exhibit the characteristics of either progressive or regressive performance behavior with increasing parameter values. We then consider a wide range of parameter variants of each of these methods. From such homogeneous set of a method type, the objective is to select the more accurate parameterization-end, while avoiding selection of both the ends together due to above mentioned characteristics. Therefore, either a single accurate variant or a set of two variants showing explicit trade-off between accuracy and diversity, get selected. Evaluation on benchmark datasets shows notable performance improvement over existing selectors.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521000945",
    "keywords": [
      "Anomaly detection",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Combinatorics",
      "Composite material",
      "Computer science",
      "Data mining",
      "Geodesy",
      "Geography",
      "Homogeneous",
      "Machine learning",
      "Materials science",
      "Mathematics",
      "Outlier",
      "Pattern recognition (psychology)",
      "Programming language",
      "Range (aeronautics)",
      "Selection (genetic algorithm)",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Mukhriya",
        "given_name": "Akanksha"
      },
      {
        "surname": "Kumar",
        "given_name": "Rajeev"
      }
    ]
  },
  {
    "title": "Internet financing credit risk evaluation using multiple structural interacting elastic net feature selection",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107835",
    "abstract": "Internet financing is an important alternative to banks where individuals or SMEs borrow money using online trading platforms. A central problem for internet financing is how to identify the most influential factors that are closely related to the credit risks. This problem is inherently challenging because the raw data of internet financing is often associated with complex structural correlations and usually contains many irrelevant and redundant features. To effectively identify the most salient features for credit risk evaluation in internet financing, we develop a new multiple structural interacting elastic net model for feature selection (MSIEN). Our idea is based on converting the original vectorial features into structure-based feature graph representations to encapsulate structural relationship between pairwise samples, and defining two new information theoretic criteria. One criterion maximizes joint relevance of different pairwise feature combinations in relation to the target feature graph and the other minimizes the redundancy between pairwise features. Then two structural interaction matrices are obtained with the elements representing the proposed information theoretic measures. To identify the most informative features, we formulate a new optimization model which combines the interaction matrices and an elastic net regularization model for the feature subset selection problem. We exploit an efficient iterative optimization algorithm to solve the proposed problem and also provide the theoretical analyses on its convergence property and computational complexity. Finally, experimental results on datasets of internet financing demonstrate the effectiveness of the proposed MSIEN method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321000224",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Data mining",
      "Elastic net regularization",
      "Exploit",
      "Feature (linguistics)",
      "Feature selection",
      "Linguistics",
      "Machine learning",
      "Operating system",
      "Pairwise comparison",
      "Philosophy",
      "Redundancy (engineering)",
      "Salient",
      "The Internet",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Cui",
        "given_name": "Lixin"
      },
      {
        "surname": "Bai",
        "given_name": "Lu"
      },
      {
        "surname": "Wang",
        "given_name": "Yanchao"
      },
      {
        "surname": "Jin",
        "given_name": "Xin"
      },
      {
        "surname": "Hancock",
        "given_name": "Edwin R."
      }
    ]
  },
  {
    "title": "Spatial context-aware network for salient object detection",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107867",
    "abstract": "Salient Object Detection (SOD) is a fundamental problem in the field of computer vision. This paper presents a novel Spatial Context-Aware Network (SCA-Net) for SOD in images. Compared with other recent deep learning based SOD algorithms, SCA-Net can more effectively aggregate multi-level deep features. A Long-Path Context Module (LPCM) is employed to grant better discrimination ability to feature maps that incorporate coarse global information. Consequently, a more accurate initial saliency map can be obtained to facilitate subsequent predictions. SCA-Net also adopts a Short-Path Context Module (SPCM) to progressively enforce the interaction between local contextual cues and global features. Extensive experiments on five large-scale benchmarks demonstrate that SCA-Net achieves favorable performance against very recent state-of-the-art algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321000546",
    "keywords": [
      "Aggregate (composite)",
      "Archaeology",
      "Artificial intelligence",
      "Composite material",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Deep learning",
      "Feature (linguistics)",
      "Field (mathematics)",
      "Geography",
      "Geometry",
      "Linguistics",
      "Machine learning",
      "Materials science",
      "Mathematics",
      "Net (polyhedron)",
      "Path (computing)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Pure mathematics",
      "Salient",
      "Spatial contextual awareness"
    ],
    "authors": [
      {
        "surname": "Kong",
        "given_name": "Yuqiu"
      },
      {
        "surname": "Feng",
        "given_name": "Mengyang"
      },
      {
        "surname": "Li",
        "given_name": "Xin"
      },
      {
        "surname": "Lu",
        "given_name": "Huchuan"
      },
      {
        "surname": "Liu",
        "given_name": "Xiuping"
      },
      {
        "surname": "Yin",
        "given_name": "Baocai"
      }
    ]
  },
  {
    "title": "A Fully Residual Convolutional Neural Network for Background Subtraction",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.02.017",
    "abstract": "Background subtraction is an important step involved in solving computer vision problems. This paper proposes a novel background subtraction method with fully residual convolutional neural network (FR-CNN). This fully residual connection helps to fuse the fine scale and coarse scale feature information efficiently. The extracted non-handcrafted features are robust and promisingly efficient compared to the handcrafted features. Furthermore, the method uses temporal and spatial information for the background subtraction process. The optical flow image is used for extracting the temporal information. Additionally, a new background modeling technique is also proposed for the efficient background subtraction. The model is trained using the randomly selected 50 frames from each video sequence of the CDnet-2014 dataset and the FR-CNN model is evaluated by CDnet-2014 dataset. The results shown from the qualitative and quantitative analyses reveal that the FR-CNN model outperforms the state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521000817",
    "keywords": [
      "Algorithm",
      "Arithmetic",
      "Artificial intelligence",
      "Background subtraction",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Electrical engineering",
      "Engineering",
      "Feature (linguistics)",
      "Fuse (electrical)",
      "Image (mathematics)",
      "Linguistics",
      "Mathematics",
      "Optical flow",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Pixel",
      "Quantum mechanics",
      "Residual",
      "Scale (ratio)",
      "Subtraction"
    ],
    "authors": [
      {
        "surname": "Vijayan",
        "given_name": "Midhula"
      },
      {
        "surname": "Raguraman",
        "given_name": "Preeth"
      },
      {
        "surname": "Mohan",
        "given_name": "R"
      }
    ]
  },
  {
    "title": "Leveraging recent advances in deep learning for audio-Visual emotion recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.03.007",
    "abstract": "Emotional expressions are the behaviors that communicate our emotional state or attitude to others. They are expressed through verbal and non-verbal communication. Complex human behavior can be understood by studying physical features from multiple modalities; mainly facial, vocal and physical gestures. Recently, spontaneous multi-modal emotion recognition has been extensively studied for human behavior analysis. In this paper, we propose a new deep learning-based approach for audio-visual emotion recognition. Our approach leverages recent advances in deep learning like knowledge distillation and high-performing deep architectures. The deep feature representations of the audio and visual modalities are fused based on a model-level fusion strategy. A recurrent neural network is then used to capture the temporal dynamics. Our proposed approach substantially outperforms state-of-the-art approaches in predicting valence on the RECOLA dataset. Moreover, our proposed visual facial expression feature extraction network outperforms state-of-the-art results on the AffectNet and Google Facial Expression Comparison datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521000878",
    "keywords": [
      "Artificial intelligence",
      "Audio visual",
      "Computer science",
      "Deep learning",
      "Emotion recognition",
      "Facial expression",
      "Feature (linguistics)",
      "Feature extraction",
      "Feature learning",
      "Gesture",
      "Linguistics",
      "Modalities",
      "Multimedia",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Social science",
      "Sociology",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Schoneveld",
        "given_name": "Liam"
      },
      {
        "surname": "Othmani",
        "given_name": "Alice"
      },
      {
        "surname": "Abdelkawy",
        "given_name": "Hazem"
      }
    ]
  },
  {
    "title": "Enhancing the identification of web genres by combining internal and external structures",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.03.004",
    "abstract": "Automating the identification of the genre of web pages becomes a promising research area in web pages classification, as it can be used to improve the quality of the web search result and to reduce search time. Many studies have been proposed to identify the genre of web pages. These studies differ with respect to three main factors which are the features used, the classification algorithm and the list of genres used for the evaluation. The main idea of this paper is to combine the predictions produced by different classifiers using the internal and external structures of a web page. To combine the predictions of the different classifiers we used different OWA operators and the Dempster-Shafer (DS) combination rule. Moreover, we proposed an improved DS combination method based on the ranks of the predicted genres. The experiments conducted using the two known datasets (KI-04 and SANTINIS), show that our study achieves better results in comparison with other ensemble classifiers and genre identification works as well.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521000830",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Computer science",
      "Data mining",
      "Epistemology",
      "Identification (biology)",
      "Information retrieval",
      "Machine learning",
      "Philosophy",
      "Quality (philosophy)",
      "Web page",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Jebari",
        "given_name": "Chaker"
      }
    ]
  },
  {
    "title": "A nearest neighbor-based active learning method and its application to time series classification",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.03.016",
    "abstract": "Although the one nearest neighbor approach is widely used in time series classification, its successful performance requires enough labeled data, which is often difficult to obtain due to a high labeling cost. This article considers a practical classification scenario in which labeled data are scant but unlabeled data are plenty, and a limited budget for the annotating task is provided. For an effective classification with limited resources, we propose a nearest neighbor-based sampling strategy for active learning. The proposed approach uses highly local information to measure the uncertainty and utility of an unlabeled instance and is applicable to extremely sparse labeled data. Furthermore, we extend the proposed approach to batch mode active learning to select a batch of informative samples at each sampling iteration. Experimental results on the WAFER and ECG5000 data sets demonstrate the effectiveness of the proposed algorithm as compared with other nearest neighbor-based approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521001033",
    "keywords": [
      "Active learning (machine learning)",
      "Artificial intelligence",
      "Best bin first",
      "Biology",
      "Canopy clustering algorithm",
      "Cluster analysis",
      "Computer science",
      "Computer vision",
      "Correlation clustering",
      "Data mining",
      "Economics",
      "Filter (signal processing)",
      "Large margin nearest neighbor",
      "Machine learning",
      "Management",
      "Measure (data warehouse)",
      "Nearest neighbor search",
      "Nearest-neighbor chain algorithm",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Sampling (signal processing)",
      "Series (stratigraphy)",
      "Task (project management)",
      "Time series",
      "k-nearest neighbors algorithm"
    ],
    "authors": [
      {
        "surname": "Gweon",
        "given_name": "Hyukjun"
      },
      {
        "surname": "Yu",
        "given_name": "Hao"
      }
    ]
  },
  {
    "title": "Automatic COVID-19 lung infected region segmentation and measurement using CT-scans images",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107747",
    "abstract": "History shows that the infectious disease (COVID-19) can stun the world quickly, causing massive losses to health, resulting in a profound impact on the lives of billions of people, from both a safety and an economic perspective, for controlling the COVID-19 pandemic. The best strategy is to provide early intervention to stop the spread of the disease. In general, Computer Tomography (CT) is used to detect tumors in pneumonia, lungs, tuberculosis, emphysema, or other pleura (the membrane covering the lungs) diseases. Disadvantages of CT imaging system are: inferior soft tissue contrast compared to MRI as it is X-ray-based Radiation exposure. Lung CT image segmentation is a necessary initial step for lung image analysis. The main challenges of segmentation algorithms exaggerated due to intensity in-homogeneity, presence of artifacts, and closeness in the gray level of different soft tissue. The goal of this paper is to design and evaluate an automatic tool for automatic COVID-19 Lung Infection segmentation and measurement using chest CT images. The extensive computer simulations show better efficiency and flexibility of this end-to-end learning approach on CT image segmentation with image enhancement comparing to the state of the art segmentation approaches, namely GraphCut, Medical Image Segmentation (MIS), and Watershed. Experiments performed on COVID-CT-Dataset containing (275) CT scans that are positive for COVID-19 and new data acquired from the EL-BAYANE center for Radiology and Medical Imaging. The means of statistical measures obtained using the accuracy, sensitivity, F-measure, precision, MCC, Dice, Jacquard, and specificity are 0.98, 0.73, 0.71, 0.73, 0.71, 0.71, 0.57, 0.99 respectively; which is better than methods mentioned above. The achieved results prove that the proposed approach is more robust, accurate, and straightforward.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320305501",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Coronavirus disease 2019 (COVID-19)",
      "Disease",
      "Image segmentation",
      "Infectious disease (medical specialty)",
      "Medical imaging",
      "Medicine",
      "Pathology",
      "Radiology",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Oulefki",
        "given_name": "Adel"
      },
      {
        "surname": "Agaian",
        "given_name": "Sos"
      },
      {
        "surname": "Trongtirakul",
        "given_name": "Thaweesak"
      },
      {
        "surname": "Kassah Laouar",
        "given_name": "Azzeddine"
      }
    ]
  },
  {
    "title": "Deep multimodal learning for cross-modal retrieval: One model for all tasks",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.02.021",
    "abstract": "We investigate the effectiveness of a successful model in Visual-Question-Answering (VQA) problems as the core component in a cross-modal retrieval system that can accept images or text as queries, in order to retrieve relevant data from a multimodal document collection. To this end, we adapt the VQA model for deep multimodal learning to combine visual and textual representations for information search, and we call this model “Deep Multimodal Embeddings (DME)”. Instead of training the model to answer questions, we supervise DME to classify semantic topics/concepts previously identified in the document collection of interest. In contrast to previous approaches, we found that this model can handle any multimodal query with a single architecture while producing improved or competitive results in all retrieval tasks. We evaluate the model performance with 3 widely known databases for cross-modal retrieval tasks: Wikipedia Retrieval Database, Pascal Sentences, and MIR-Flickr-25k. The results show that the DME model learns effective multimodal representations, resulting in strongly improved retrieval performance, specifically in the top results of the ranked list, which are the most important to users in the most-common scenarios of information retrieval. Our work represents a new baseline for a wide set of different methodologies for cross-modal retrieval.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521000908",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Deep learning",
      "Machine learning",
      "Modal",
      "Polymer chemistry"
    ],
    "authors": [
      {
        "surname": "Beltrán",
        "given_name": "L. Viviana Beltrán"
      },
      {
        "surname": "Caicedo",
        "given_name": "Juan C."
      },
      {
        "surname": "Journet",
        "given_name": "Nicholas"
      },
      {
        "surname": "Coustaty",
        "given_name": "Mickaël"
      },
      {
        "surname": "Lecellier",
        "given_name": "François"
      },
      {
        "surname": "Doucet",
        "given_name": "Antoine"
      }
    ]
  },
  {
    "title": "Refining a k -nearest neighbor graph for a computationally efficient spectral clustering",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107869",
    "abstract": "Spectral clustering became a popular choice for data clustering for its ability of uncovering clusters of different shapes. However, it is not always preferable over other clustering methods due to its computational demands. One of the effective ways to bypass these computational demands is to perform spectral clustering on a subset of points (data representatives) then generalize the clustering outcome, this is known as approximate spectral clustering (ASC). ASC uses sampling or quantization to select data representatives. This makes it vulnerable to 1) performance inconsistency (since these methods have a random step either in initialization or training), 2) local statistics loss (because the pairwise similarities are extracted from data representatives instead of data points). We proposed a refined version of k -nearest neighbor graph, in which we keep data points and aggressively reduce number of edges for computational efficiency. Local statistics were exploited to keep the edges that do not violate the intra-cluster distances and nullify all other edges in the k -nearest neighbor graph. We also introduced an optional step to automatically select the number of clusters C . The proposed method was tested on synthetic and real datasets. Compared to ASC methods, the proposed method delivered a consistent performance despite significant reduction of edges.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032100056X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "CURE data clustering algorithm",
      "Canopy clustering algorithm",
      "Cluster analysis",
      "Computer science",
      "Correlation clustering",
      "Data mining",
      "Data point",
      "Data stream clustering",
      "Graph",
      "Initialization",
      "Mathematics",
      "Nearest neighbor search",
      "Nearest-neighbor chain algorithm",
      "Pairwise comparison",
      "Programming language",
      "Spectral clustering",
      "Theoretical computer science",
      "k-nearest neighbors algorithm"
    ],
    "authors": [
      {
        "surname": "Alshammari",
        "given_name": "Mashaan"
      },
      {
        "surname": "Stavrakakis",
        "given_name": "John"
      },
      {
        "surname": "Takatsuka",
        "given_name": "Masahiro"
      }
    ]
  },
  {
    "title": "GuessWhich? Visual dialog with attentive memory network",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107823",
    "abstract": "Visual dialog is a task that two agents: Question-BOT (Q-BOT) and Answer-BOT (A-BOT), which communicate in natural language on the situation of information asymmetry. Q-BOT generates questions based on an image caption and a historical dialog. A-BOT answers the questions grounded on the image. Moreover, we play a cooperative ‘image guessing’ game between Q-BOT and A-BOT, so that Q-BOT can select an unseen image from a set of images. However, as the valid information of the image caption and the historical dialog fades along the interaction, existing methods usually generate irrelevant and homogenous questions, which are worthless to the visual dialog system. To tackle this issue, we propose an Attentive Memory Network (AMN) to fully exploit the image caption and historical dialog information. Specifically, the attentive memory network mainly consists of a memory network and a fusion module. The memory network holds long term historical dialog information and gives each round of the dialog a different weight. Aside from the historical dialog information, the fusion module in Q-BOT and A-BOT further uses the image caption and the image feature, respectively. The caption information assists Q-BOT with the attentive generation of the questions, and the image feature helps A-BOT produce precise answers. With the AMN, the generated questions are diverse and concentrated, and the corresponding answers are accurate. The experimental results on VisDial v1.0 show the effectiveness of our proposed model, which outperforms the state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321000108",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Dialog box",
      "Economics",
      "Exploit",
      "Feature (linguistics)",
      "Image (mathematics)",
      "Linguistics",
      "Management",
      "Natural language processing",
      "Philosophy",
      "Programming language",
      "Set (abstract data type)",
      "Task (project management)",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Lei"
      },
      {
        "surname": "Lyu",
        "given_name": "Xinyu"
      },
      {
        "surname": "Song",
        "given_name": "Jingkuan"
      },
      {
        "surname": "Gao",
        "given_name": "Lianli"
      }
    ]
  },
  {
    "title": "Thermodynamic motif analysis for directed stock market networks",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107872",
    "abstract": "In this paper, we present a novel thermodynamically based analysis method for directed networks, and in particular for time-evolving networks in the finance domain. Based on an analogy with a dilute gas in statistical mechanics, we develop a partition function for a network composed of directed motifs. The method relies on the decomposition of directed networks into a series of frequently occurring graphlets, or motifs. According to the connection between a directed network and the dilute gas, the network motifs have the same topological structure as the low-order interactions between particles in the gas. This means that we can use the so-called cluster expansion from statistical mechanics to develop a partition function for the motif decomposition. In prior work, we have reported a detailed analysis of the cluster expansion for the case of undirected graphs, and showed how the resulting motif entropy can be used to analyse time evolving networks [1]. In this paper we extend this work to the case of directed graphs to compute thermodynamic quantities including energy, entropy and temperature for the directed network. The three thermodynamic quantities constitute the thermodynamic framework for the analysis of directed network evolution. We apply our thermodynamic framework to the financial and biological domains to represent real world complex systems as time-varying directed networks. Experimental results successfully demonstrate the effectiveness of the thermodynamic framework in representing the evolution of directed network structure and anomalous event detection.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321000595",
    "keywords": [
      "Cluster expansion",
      "Complex network",
      "Computer science",
      "Configuration entropy",
      "Entropy (arrow of time)",
      "Mathematics",
      "Network theory",
      "Physics",
      "Statistical mechanics",
      "Statistical physics",
      "Statistics",
      "Theoretical computer science",
      "Thermodynamic system",
      "Thermodynamics",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Dongdong"
      },
      {
        "surname": "Guo",
        "given_name": "Xingchen"
      },
      {
        "surname": "Wang",
        "given_name": "Jianjia"
      },
      {
        "surname": "Liu",
        "given_name": "Jiatong"
      },
      {
        "surname": "Zhang",
        "given_name": "Zhihong"
      },
      {
        "surname": "Hancock",
        "given_name": "Edwin R."
      }
    ]
  },
  {
    "title": "On-line learning the graph edit distance costs",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.02.019",
    "abstract": "This paper presents the first on-line learning method to automatically deduce the insertion, deletion and substitution edit costs of the graph edit distance. The learning method is based on embedding the substitution and deletion operations into a Euclidean space. The points in this space are classified into the ones that represent substitution edit operations and the ones that represent deletion edit operations. Thus, the learning strategy is based on deducing the hyper-plane in this space that best splits these two types of points. Any linear classifier can be used to deduce this hyper-plane, for instance LDA or SVM. The on-line method has the advantage that learning the edit costs and computing the graph edit distance with the new updated costs can be done simultaneously. Experimental validation shows that the matching accuracy is competitive with the off-line methods but without the need of the whole learning set.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552100088X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Edit distance",
      "Embedding",
      "Euclidean distance",
      "Graph",
      "Programming language",
      "Substitution (logic)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Rica",
        "given_name": "Elena"
      },
      {
        "surname": "Álvarez",
        "given_name": "Susana"
      },
      {
        "surname": "Serratosa",
        "given_name": "Francesc"
      }
    ]
  },
  {
    "title": "Bound estimation-based safe acceleration for maximum margin of twin spheres machine with pinball loss",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107860",
    "abstract": "Maximum margin of twin spheres support vector machine (MMTSSVM) is an efficient method for imbalanced data classification. As an extension to enhance noise insensitivity of MMTSSVM, MMTSSVM with pinball loss (Pin-MMTSM) has a good generalization performance. However, it is not efficient enough for large-scale data. Inspired by the sparse solution of SVMs, in this paper, we propose a safe accelerative approach to reduce the computational cost. Unlike the existing safe screening rules, where only one variable changes with the parameters. We utilize bound estimation-based to derive the upper and lower bounds of center and radius. With our approach, the inactive samples are discarded before solving the problem, thus it can reduce the computational cost. One important advantage of our approach is safety, i.e., we can obtain the same solution as solving original problem both in linear and non-linear cases. Moreover, it is obvious that our acceleration approach is independent of the solver. To further accelerate the computational speed, a decomposition method is employed. Experiments on three artificial datasets and twelve benchmark datasets clearly demonstrate the effectiveness of our approach. At last, we extend bound estimation-based method to ν -SVM, theoretical analysis and experimental results both verify its feasibility and effectiveness.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321000479",
    "keywords": [
      "Acceleration",
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Classical mechanics",
      "Computer science",
      "Generalization",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Machine learning",
      "Margin (machine learning)",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Noise (video)",
      "Physics",
      "Solver",
      "Support vector machine",
      "Upper and lower bounds",
      "Variable (mathematics)"
    ],
    "authors": [
      {
        "surname": "Yuan",
        "given_name": "Min"
      },
      {
        "surname": "Xu",
        "given_name": "Yitian"
      }
    ]
  },
  {
    "title": "Perception matters: Exploring imperceptible and transferable anti-forensics for GAN-generated fake face imagery detection",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.03.009",
    "abstract": "Recently, generative adversarial networks (GANs) can generate photo-realistic fake facial images which are perceptually indistinguishable from real face photos, promoting research on fake face detection. Though fake face forensics can achieve high detection accuracy, their anti-forensic counterparts are less investigated. Here we explore more imperceptible and transferable anti-forensics for fake face imagery detection based on adversarial attacks. Since facial and background regions are often smooth, even small perturbation could cause noticeable perceptual impairment in fake face images. Therefore it makes existing transfer-based adversarial attacks ineffective as an anti-forensic method. Our perturbation analysis reveals the intuitive reason of the perceptual degradation issue when directly applying such existing attacks. We then propose a novel adversarial attack method, better suitable for image anti-forensics, in the transformed color domain by considering visual perception. Conceptually simple yet effective, the proposed method can fool both deep learning and non-deep learning based forensic detectors, achieving higher adversarial transferability and significantly improved visual quality. Specially, when adversaries consider imperceptibility as a constraint, the proposed anti-forensic method achieves the state-of-the-art attacking performances in the transfer-based black-box setting (i.e. around 30% higher attack transferability than baseline attacks). More imperceptible and more transferable, the proposed method raises new security concerns to fake face imagery detection. We have released our code for public use, and hopefully the proposed method can be further explored in related forensic applications as an anti-forensic benchmark.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521000957",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Face (sociological concept)",
      "Face detection",
      "Facial recognition system",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Perception",
      "Psychology",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Yongwei"
      },
      {
        "surname": "Ding",
        "given_name": "Xin"
      },
      {
        "surname": "Yang",
        "given_name": "Yixin"
      },
      {
        "surname": "Ding",
        "given_name": "Li"
      },
      {
        "surname": "Ward",
        "given_name": "Rabab"
      },
      {
        "surname": "Wang",
        "given_name": "Z. Jane"
      }
    ]
  },
  {
    "title": "Multi-task face analyses through adversarial learning",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107837",
    "abstract": "The inherent relations among multiple face analysis tasks, such as landmark detection, head pose estimation, gender recognition and face attribute estimation are crucial to boost the performance of each task, but have not been thoroughly explored since typically these multiple face analysis tasks are handled as separate tasks. In this paper, we propose a novel deep multi-task adversarial learning method to localize facial landmark, estimate head pose and recognize gender jointly or estimate multiple face attributes simultaneously through exploring their dependencies from both image representation-level and label-level. Specifically, the proposed method consists of a deep recognition network R and a discriminator D . The deep recognition network is used to learn the shared middle-level image representation and conducts multiple face analysis tasks simultaneously. Through multi-task learning mechanism, the recognition network explores the dependencies among multiple face analysis tasks from image representation-level. The discriminator is introduced to enforce the distribution of the multiple face analysis tasks to converge to that inherent in the ground-truth labels. During training, the recognizer tries to confuse the discriminator, while the discriminator competes with the recognizer through distinguishing the predicted label combination from the ground-truth one. Though adversarial learning, we explore the dependencies among multiple face analysis tasks from label-level. Experimental results on benchmark databases demonstrate the effectiveness of the proposed method for multi-task face analyses.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321000248",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Computer vision",
      "Detector",
      "Discriminator",
      "Economics",
      "Face (sociological concept)",
      "Facial recognition system",
      "Geodesy",
      "Geography",
      "Ground truth",
      "Landmark",
      "Law",
      "Machine learning",
      "Management",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Social science",
      "Sociology",
      "Task (project management)",
      "Task analysis",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Shangfei"
      },
      {
        "surname": "Yin",
        "given_name": "Shi"
      },
      {
        "surname": "Hao",
        "given_name": "Longfei"
      },
      {
        "surname": "Liang",
        "given_name": "Guang"
      }
    ]
  },
  {
    "title": "An HEVC steganalytic approach against motion vector modification using local optimality in candidate list",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.02.018",
    "abstract": "Current research of video steganalysis is mainly oriented to H.264 or even earlier formats. As the H.265/HEVC standard is becoming more and more popular, there have been steganographic methods specifically designed for HEVC videos. Previous steganalytic methods are no longer effective due to the new features of HEVC. The advanced motion vector prediction technique employed by HEVC provides a new way for motion vector (MV) modification, utilizing the index of the candidate MV list. Such modification cannot be detected by previous steganalytic methods because the value of MV is not changed. To solve this problem, we proposed a new steganalytic strategy employing not only the local optimality of MV but also the local optimality in the candidate list. Combining the two types of local optimality, a 40-dimensional feature set is designed. Extensive experiments are carried out to evaluate the effectiveness of the proposed feature set. It is shown that compared with three current steganalyzers, our approach improves the performance against typical MV modification methods under various settings. In particular, our approach significantly boosts the detection accuracy of the index modification method that is exclusive to HEVC videos. To our best knowledge, this is the first work of video steganalytic method exploiting the new features of HEVC format.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521000866",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Feature vector",
      "Image (mathematics)",
      "Motion vector",
      "Pattern recognition (psychology)",
      "Steganalysis",
      "Steganography",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Shuowei"
      },
      {
        "surname": "Hu",
        "given_name": "Yongjian"
      },
      {
        "surname": "Liu",
        "given_name": "Beibei"
      },
      {
        "surname": "Li",
        "given_name": "Chang-Tsun"
      }
    ]
  },
  {
    "title": "Ordered Weighted Aggregation Networks for Video Face Recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.03.021",
    "abstract": "Video face recognition generally includes a step where all descriptors extracted for each frame are aggregated to generate a single video face representation. The most commonly used operator for aggregation is average, which gives the same relevance to each frame. Some adaptive aggregation algorithms have been developed, but most of them rely on the use of weighted mean as aggregation operator, thus disregarding many other types of aggregation operators. In this paper, we propose a novel adaptive aggregation scheme based on ordered weighted average (OWA) operators in contrast with the mainly used weighted mean scheme. Furthermore, besides presenting the theoretical aspects of our aggregation scheme, we develop two different concrete implementations to validate its suitability for video face recognition: Ordered weighted aggregation network (OWANet) and Weighted OWANet (WOWANet). Both algorithms are based on neural networks and are trainable through gradient descent in a classic supervised learning way. We conduct extensive experiments on YouTube Faces, COX Face and the IARPA Janus Benchmark A for evaluating recognition performance on verification and identification tasks. The experimentation process shows that both proposals achieve very competitive results in accuracy with respect to the existent state-of-the-art methods, while significantly reducing space and inference time.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521001082",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Face (sociological concept)",
      "Facial recognition system",
      "Frame (networking)",
      "Gene",
      "Geodesy",
      "Geography",
      "Gradient descent",
      "Inference",
      "Law",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Operating system",
      "Operator (biology)",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Process (computing)",
      "Relevance (law)",
      "Representation (politics)",
      "Repressor",
      "Scheme (mathematics)",
      "Social science",
      "Sociology",
      "Telecommunications",
      "Transcription factor"
    ],
    "authors": [
      {
        "surname": "Rivero-Hernández",
        "given_name": "Jacinto"
      },
      {
        "surname": "Morales-González",
        "given_name": "Annette"
      },
      {
        "surname": "Denis",
        "given_name": "Lester Guerra"
      },
      {
        "surname": "Méndez-Vázquez",
        "given_name": "Heydi"
      }
    ]
  },
  {
    "title": "ACN: Occlusion-tolerant face alignment by attentional combination of heterogeneous regression networks",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107761",
    "abstract": "This paper presents the Attentional Combination Network (ACN), which is a highly accurate face alignment method that is tolerant of occlusion. The method combines a coordinate regression network and a heatmap regression network with a spatial attention. The coordinate regression generates the coordinates of facial landmark points directly such that they are fitted to the input face on the whole. The heatmap regression generates the heatmap of facial landmark points such that each channel provides good localization of the detail of its facial landmark point. These independent regressions compensate for each other complementarily such that the overall fitting tendency of the coordinate regression compensates for the inaccurate alignment of the heatmap regression due to missing local information, and the detailed localization of the heatmap regression compensates for the relatively inaccurate alignment of the coordinate regression. The proposed ACN uses coordinate-to-heatmap and the heatmap-to-coordinate conversion networks to combine two heterogeneous regressions, and to generate the final coordinates of the facial landmark points. The ACN use the spatial attention mechanism to effectively reject impeditive local features that are caused by the occlusion. In experiments on several benchmarks, the proposed ACN achieved state-of-the-art accuracy",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320305641",
    "keywords": [
      "Artificial intelligence",
      "Cardiology",
      "Computer science",
      "Computer vision",
      "Coordinate system",
      "Face (sociological concept)",
      "Geometry",
      "Landmark",
      "Machine learning",
      "Mathematics",
      "Medicine",
      "Occlusion",
      "Pattern recognition (psychology)",
      "Point (geometry)",
      "Regression",
      "Regression analysis",
      "Social science",
      "Sociology",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Park",
        "given_name": "Hyunsung"
      },
      {
        "surname": "Kim",
        "given_name": "Daijin"
      }
    ]
  },
  {
    "title": "A generalized weighted distance k-Nearest Neighbor for multi-label problems",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2020.107526",
    "abstract": "In multi-label classification, each instance is associated with a set of pre-specified labels. One common approach is to use Binary Relevance (BR) paradigm to learn each label by a base classifier separately. Use of k-Nearest Neighbor (kNN) as the base classifier (denoted as BRkNN) is a simple, descriptive and powerful approach. In binary relevance a highly imbalanced view of dataset is used. However, kNN is known to perform poorly on imbalanced data. One approach to deal with this is to define the distance function in a parametric form and use the training data to adjust the parameters (i.e. adjusting boundaries between classes) by optimizing a performance measure customized for imbalanced data e.g. F -measure. Prototype Weighting (PW) scheme presented in the literature (Paredes & Vidal, 2006) uses gradient descent to specify the parameters by minimizing the classification error-rate on training data. This paper presents a generalized version of PW. First, instead of minimizing the error-rate proposed in PW, the generalized PW supports also other objective functions that use elements of confusion matrix (including F -measure). Second, PW originally presented for 1NN is extended to the general case of kNN (i.e., k > = 1 ). For problems having highly overlapped classes, it is expected to perform better since a value of k > 1 produces smoother decision boundaries which in turn can improve generalization. In multi-label problems with many labels or problems with highly overlapped classes, the proposed generalized PW is expected to significantly improve the performance as it involves many decision boundaries. The performance of the proposed method has been compared with state-of-the-art methods in multi-label classification containing 6 lazy classifiers based on kNN. Experiments show that the proposed method significantly outperforms other methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303290",
    "keywords": [
      "Algorithm",
      "Arithmetic",
      "Artificial intelligence",
      "Artificial neural network",
      "Binary decision diagram",
      "Binary number",
      "Classifier (UML)",
      "Computer science",
      "Confusion matrix",
      "Data mining",
      "Gradient descent",
      "Mathematics",
      "Measure (data warehouse)",
      "Medicine",
      "Parametric statistics",
      "Pattern recognition (psychology)",
      "Radiology",
      "Statistics",
      "Weighting",
      "k-nearest neighbors algorithm"
    ],
    "authors": [
      {
        "surname": "Rastin",
        "given_name": "Niloofar"
      },
      {
        "surname": "Jahromi",
        "given_name": "Mansoor Zolghadri"
      },
      {
        "surname": "Taheri",
        "given_name": "Mohammad"
      }
    ]
  },
  {
    "title": "A new EM algorithm for flexibly tied GMMs with large number of components",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107836",
    "abstract": "Gaussian mixture models (GMMs) are a family of generative models used extensively in many machine learning applications. The modeling power of GMMs is directly linked to the number of components. Memory, computational load and lack of enough data hinders using GMMs with large number of components. To tackle this problem, GMMs with a tying scheme that we call flexibly tied GMM was proposed in the literature of the speech recognition community. In the literature, a coordinate-descent EM algorithm was proposed for estimating the parameters of flexibly tied GMMs. In this paper, we aim at reintroducing flexibly tied GMMs to the pattern recognition community. We rigorously investigate various optimization methods and see none of the out-of-the-box optimization methods can solve the parameter estimation problem due to the complexity of the cost function. To this end, we develop a fast Newton EM algorithm that combined with the coordinate descent EM algorithm, it significantly outperforms pure coordinate descent EM and all other optimization algorithms. Furthermore, we propose a computation factorization technique to increase the speed and decrease memory requirement of both Newton and coordinate descent EM algorithms in the case of large number of components. Experimental results on many datasets verifies the efficacy of the proposed algorithm. It also verifies that flexibly tied GMM outperforms both basic GMM and other types of tied GMMs on the datasets in terms of the log-likelihood. We also evaluate the performance of flexibly tied GMM on a clustering problem, and show that it can outperform basic GMM and kmeans algorithm.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321000236",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Cluster analysis",
      "Computation",
      "Computer science",
      "Coordinate descent",
      "Eigenvalues and eigenvectors",
      "Expectation–maximization algorithm",
      "Mathematics",
      "Matrix decomposition",
      "Maximum likelihood",
      "Mixture model",
      "Non-negative matrix factorization",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Asheri",
        "given_name": "Hadi"
      },
      {
        "surname": "Hosseini",
        "given_name": "Reshad"
      },
      {
        "surname": "Araabi",
        "given_name": "Babak Nadjar"
      }
    ]
  },
  {
    "title": "Optical Flow based CNN for detection of unlearnt deepfake manipulations",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.03.005",
    "abstract": "A new phenomenon named Deepfakes constitutes a serious threat in video manipulation. AI-based technologies have provided easy-to-use methods to create extremely realistic videos. On the side of multimedia forensics, being able to individuate this kind of fake contents becomes ever more crucial. In this work, a new forensic technique able to detect fake and original video sequences is proposed; it is based on the use of CNNs trained to distinguish possible motion dissimilarities in the temporal structure of a video sequence by exploiting optical flow fields. The results obtained highlight comparable performances with the state-of-the-art methods which, in general, only resort to single video frames. Furthermore, the proposed optical flow based detection scheme also provides a superior robustness in the more realistic cross-forgery operative scenario and can even be combined with frame-based approaches to improve their global effectiveness.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521000842",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Frame (networking)",
      "Gene",
      "Image (mathematics)",
      "Optical flow",
      "Pattern recognition (psychology)",
      "Robustness (evolution)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Caldelli",
        "given_name": "Roberto"
      },
      {
        "surname": "Galteri",
        "given_name": "Leonardo"
      },
      {
        "surname": "Amerini",
        "given_name": "Irene"
      },
      {
        "surname": "Del Bimbo",
        "given_name": "Alberto"
      }
    ]
  },
  {
    "title": "Subspace clustering via stacked independent subspace analysis networks with sparse prior information",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.03.026",
    "abstract": "Sparse subspace clustering (SSC) method has gained considerable attention in recent decades owing to its advantages in the fields of clustering. In essence, SSC is to learn a sparse affinity matrix followed by striving for a low-dimensional representation of data. However, the SSC and its variants mainly focus on building high-quality affinity matrix while ignoring the importance of low-dimensional feature derived from the affinity matrix. Moreover, due to their intrinsic linearity of models, they cannot efficiently handle data with the nonlinear distribution. In this paper, we propose a stacked independent subspace analysis (ISA) with sparse prior information called stacked-ISASP to deal with these two issues. Powered by handling data with nonlinear structure, our method aims at seeking a low-dimensional feature from the image data. Concretely, the model can stack the modified independent subspace analysis networks by incorporating the prior subspace information from the original data. To validate the efficiency of the proposed method, we compare our proposed stacked-ISASP method with the state-of-the-art methods on real datasets. Experimental results show that our approach can not only learn a better low-dimensional structure from the data but also achieve better performance for the classification task.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521001136",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Clustering high-dimensional data",
      "Computer science",
      "Data mining",
      "Feature (linguistics)",
      "Focus (optics)",
      "Gaussian",
      "Law",
      "Linguistics",
      "Nonlinear system",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Political science",
      "Politics",
      "Quantum mechanics",
      "Representation (politics)",
      "Sparse approximation",
      "Sparse matrix",
      "Subspace topology"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Zongze"
      },
      {
        "surname": "Su",
        "given_name": "Chunchen"
      },
      {
        "surname": "Yin",
        "given_name": "Ming"
      },
      {
        "surname": "Ren",
        "given_name": "Zhigang"
      },
      {
        "surname": "Xie",
        "given_name": "Shengli"
      }
    ]
  },
  {
    "title": "Learning deep discriminative embeddings via joint rescaled features and log-probability centers",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107852",
    "abstract": "Recently softmax based loss functions have surged to advance image classification and face verification. Most efforts boost discrimination of the softmax loss by using novel angular margins in varying ways, but few analyze where the discrimination truly comes from whilst considering the power of relieving the overfitting to enhance the softmax loss. In this paper, we firstly delve into such mainstream of softmax based loss functions in theory, and recognize the importance of easing overfitting to the softmax loss. In terms of such analysis, this paper intends to bring the softmax loss up to the competitive level with current well-behaved loss functions. We do this in two ways: (1) regularizing the softmax to relieve the overfitting by learning the log-probability centers, and (2) rescaling deep embeddings of the softmax with a constant scale to further enhance inter-class separability in Euclidean space. We call the resulting loss function rLogCenter loss for short. Simple and interpretable as our loss is, it guides CNNs to induce performance gains in the experiments of both image classification and face verification.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032100039X",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Deep learning",
      "Discriminative model",
      "Face (sociological concept)",
      "Machine learning",
      "Mathematics",
      "Overfitting",
      "Pattern recognition (psychology)",
      "Social science",
      "Sociology",
      "Softmax function"
    ],
    "authors": [
      {
        "surname": "Cai",
        "given_name": "Huayue"
      },
      {
        "surname": "Zhang",
        "given_name": "Xiang"
      },
      {
        "surname": "Lan",
        "given_name": "Long"
      },
      {
        "surname": "Dong",
        "given_name": "Guohua"
      },
      {
        "surname": "Xu",
        "given_name": "Chuanfu"
      },
      {
        "surname": "Liu",
        "given_name": "Xinwang"
      },
      {
        "surname": "Luo",
        "given_name": "Zhigang"
      }
    ]
  },
  {
    "title": "Biometric recognition using wearable devices in real-life settings",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.03.020",
    "abstract": "The popularity of wearable devices, such as smart glasses, chestbands, and wristbands, is nowadays rapidly growing, thanks to the fact that they can be used to track physical activity and monitor users’ health. Recently, researchers have proposed to exploit their capability to collect physiological signals for enabling automatic user recognition. Wearable devices inherently provide the means for detecting their unauthorized usage, or for being used as front-end in biometric recognition systems controlling the access to either physical or virtual locations and services. The present work evaluates the feasibility of performing biometric recognition using signals captured by wearable devices, considering data collected through off-the-shelf commercial wristbands, and comparing recordings taken during two distinct sessions separated by an average time of 7 days. In more detail, recognition is performed leveraging on electrodermal activity (EDA) and blood volume pulse (BVP), considering measurements taken from 17 subjects performing natural activities such as attending or teaching lectures. Several tests have been carried out to determine the most effective representation of the considered EDA and BVP signals, as well as the most suitable classifier. The best recognition performance has been achieved exploiting convolutional neural networks to extract discriminative characteristics from the combined spectrograms of the employed EDA and BVP data, guaranteeing average correct identification rate of 98.58% for test samples lasting 30 seconds.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521001070",
    "keywords": [
      "Activity recognition",
      "Artificial intelligence",
      "Biology",
      "Biometrics",
      "Blood pressure",
      "Botany",
      "Classifier (UML)",
      "Computer science",
      "Computer security",
      "Convolutional neural network",
      "Discriminative model",
      "Embedded system",
      "Exploit",
      "Human–computer interaction",
      "Identification (biology)",
      "Medicine",
      "Pattern recognition (psychology)",
      "Pulse rate",
      "Radiology",
      "Spectrogram",
      "Speech recognition",
      "Wearable computer",
      "Wearable technology"
    ],
    "authors": [
      {
        "surname": "Piciucco",
        "given_name": "Emanuela"
      },
      {
        "surname": "Di Lascio",
        "given_name": "Elena"
      },
      {
        "surname": "Maiorana",
        "given_name": "Emanuele"
      },
      {
        "surname": "Santini",
        "given_name": "Silvia"
      },
      {
        "surname": "Campisi",
        "given_name": "Patrizio"
      }
    ]
  },
  {
    "title": "A distributed source autoencoder of local visual descriptors for 3D reconstruction",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.03.019",
    "abstract": "This paper presents a local descriptor coding scheme for multicamera surveillance and 3D reconstruction embedding an autoencoder into a traditional distributed source coding strategy. The proposed solution permits shifting most of the computational complexity at the decoder/receiver and exploiting the correlation among descriptors of different cameras (thus reducing the coded bit rate) without increasing the inter-device communication load. Experimental results show that the proposed scheme permits obtaining a satisfying accuracy with respect to the most recent solutions while generating a limited bit rate.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521001069",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Autoencoder",
      "Bit rate",
      "Coding (social sciences)",
      "Computational complexity theory",
      "Computer engineering",
      "Computer science",
      "Computer vision",
      "Decoding methods",
      "Deep learning",
      "Embedding",
      "Encoder",
      "Mathematical analysis",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Scheme (mathematics)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Milani",
        "given_name": "Simone"
      }
    ]
  },
  {
    "title": "Multi-task contrastive learning for automatic CT and X-ray diagnosis of COVID-19",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107848",
    "abstract": "Computed tomography (CT) and X-ray are effective methods for diagnosing COVID-19. Although several studies have demonstrated the potential of deep learning in the automatic diagnosis of COVID-19 using CT and X-ray, the generalization on unseen samples needs to be improved. To tackle this problem, we present the contrastive multi-task convolutional neural network (CMT-CNN), which is composed of two tasks. The main task is to diagnose COVID-19 from other pneumonia and normal control. The auxiliary task is to encourage local aggregation though a contrastive loss: first, each image is transformed by a series of augmentations (Poisson noise, rotation, etc.). Then, the model is optimized to embed representations of a same image similar while different images dissimilar in a latent space. In this way, CMT-CNN is capable of making transformation-invariant predictions and the spread-out properties of data are preserved. We demonstrate that the apparently simple auxiliary task provides powerful supervisions to enhance generalization. We conduct experiments on a CT dataset (4,758 samples) and an X-ray dataset (5,821 samples) assembled by open datasets and data collected in our hospital. Experimental results demonstrate that contrastive learning (as plugin module) brings solid accuracy improvement for deep learning models on both CT (5.49%-6.45%) and X-ray (0.96%-2.42%) without requiring additional annotations. Our codes are accessible online.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321000352",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Convolutional neural network",
      "Coronavirus disease 2019 (COVID-19)",
      "Deep learning",
      "Disease",
      "Economics",
      "Gene",
      "Generalization",
      "Infectious disease (medical specialty)",
      "Invariant (physics)",
      "Machine learning",
      "Management",
      "Mathematical analysis",
      "Mathematical physics",
      "Mathematics",
      "Medicine",
      "Multi-task learning",
      "Pathology",
      "Pattern recognition (psychology)",
      "Task (project management)",
      "Transformation (genetics)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Jinpeng"
      },
      {
        "surname": "Zhao",
        "given_name": "Gangming"
      },
      {
        "surname": "Tao",
        "given_name": "Yaling"
      },
      {
        "surname": "Zhai",
        "given_name": "Penghua"
      },
      {
        "surname": "Chen",
        "given_name": "Hao"
      },
      {
        "surname": "He",
        "given_name": "Huiguang"
      },
      {
        "surname": "Cai",
        "given_name": "Ting"
      }
    ]
  },
  {
    "title": "Common-covariance based person re-identification model",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.03.002",
    "abstract": "This paper develops a new covariance-matrix based person re-identification model. It takes each person as a Gaussian distribution with a common covariance matrix. The learning process is to find an optimal covariance matrix among all the distributions. This paper has two major contributions: (1) It proves the statistical meaning of Mahalanobis matrix as a common covariance matrix among all related classes; (2) The intra-class distance is measured using the determinant of the common covariance matrix. The proposed model largely reduces the computational cost. Moreover, it automatically avoids imbalanced computations between the distances for inter-class images and the distances for intra-class images. Experimental results demonstrate that the proposed model is more efficient than many state-of-the-art methods in both accuracy and time cost.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521000805",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "CMA-ES",
      "Composite material",
      "Computer science",
      "Covariance",
      "Covariance function",
      "Covariance intersection",
      "Covariance matrix",
      "Eigenvalues and eigenvectors",
      "Eight-point algorithm",
      "Estimation of covariance matrices",
      "Mahalanobis distance",
      "Materials science",
      "Mathematics",
      "Matrix (chemical analysis)",
      "Matérn covariance function",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Rational quadratic covariance function",
      "Scatter matrix",
      "State-transition matrix",
      "Statistics",
      "Symmetric matrix"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Hongyuan"
      },
      {
        "surname": "Wu",
        "given_name": "Linyu"
      },
      {
        "surname": "Chen",
        "given_name": "Fuhua"
      },
      {
        "surname": "Ding",
        "given_name": "Zongyuan"
      },
      {
        "surname": "Yin",
        "given_name": "Yuchang"
      },
      {
        "surname": "Dai",
        "given_name": "Chenchao"
      }
    ]
  },
  {
    "title": "Discriminative shared transform learning for sketch to image matching",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107815",
    "abstract": "Sketch to digital image matching refers to the problem of matching a sketch image (often drawn by hand or created by a software) against a gallery of digital images (captured via an acquisition device such as a digital camera). Automated sketch to digital image matching has applicability in several day to day tasks such as similar object image retrieval, forensic sketch matching in law enforcement scenarios, or profile linking using caricature face images on social media. As opposed to the digital images, sketch images are generally edge-drawings containing limited (or no) textural or colour based information. Further, there is no single technique for sketch generation, which often results in varying artistic or software styles, along with the interpretation bias of the individual creating the sketch. Beyond the variations observed across the two domains (sketch and digital image), automated sketch to digital image matching is further marred by the challenge of limited training data and wide intra-class variability. In order to address the above problems, this research proposes a novel Discriminative Shared Transform Learning (DSTL) algorithm for sketch to digital image matching. DSTL learns a shared transform for data belonging to the two domains, while modeling the class variations, resulting in discriminative feature learning. Two models have been presented under the proposed DSTL algorithm: (i) Contractive Model (C-Model) and (ii) Divergent Model (D-Model), which have been formulated with different supervision constraints. Experimental analysis on seven datasets for three case studies of sketch to digital image matching demonstrate the efficacy of the proposed approach, highlighting the importance of each component, its input-agnostic behavior, and improved matching performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321000029",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Digital image",
      "Discriminative model",
      "Gesture",
      "Gesture recognition",
      "Image (mathematics)",
      "Image processing",
      "Image retrieval",
      "Matching (statistics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Sketch",
      "Sketch recognition",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Nagpal",
        "given_name": "Shruti"
      },
      {
        "surname": "Singh",
        "given_name": "Maneet"
      },
      {
        "surname": "Singh",
        "given_name": "Richa"
      },
      {
        "surname": "Vatsa",
        "given_name": "Mayank"
      }
    ]
  },
  {
    "title": "Assessing partially ordered clustering in a multicriteria comparative context",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107850",
    "abstract": "This study considers the task of clustering for data characterized by peculiar quantitative features in that they express performance according to different indicators or criteria. Performance is supposed to be optimized in one way or the other, i.e. maximized or minimized. This peculiar type of data introduces a comparative context that is not generally taken into account in the field of pattern recognition, in general, and clustering, in particular. In the present study, we introduce different concepts and develop tools that facilitate the evaluation of data partitions in this comparative context leading to the consideration of asymmetric preference relationships between objects and between clusters. We show their usefulness on the basis of artificial data and also by analyzing the results produced on real data by means of clustering methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321000376",
    "keywords": [
      "Artificial intelligence",
      "Basis (linear algebra)",
      "Biology",
      "CURE data clustering algorithm",
      "Cluster analysis",
      "Computer science",
      "Consensus clustering",
      "Context (archaeology)",
      "Correlation clustering",
      "Data mining",
      "Engineering",
      "Field (mathematics)",
      "Geometry",
      "Machine learning",
      "Mathematics",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Pure mathematics",
      "Systems engineering",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Rosenfeld",
        "given_name": "Jean"
      },
      {
        "surname": "Smet",
        "given_name": "Yves De"
      },
      {
        "surname": "Debeir",
        "given_name": "Olivier"
      },
      {
        "surname": "Decaestecker",
        "given_name": "Christine"
      }
    ]
  },
  {
    "title": "On the detection-to-track association for online multi-object tracking",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.03.022",
    "abstract": "Driven by recent advances in object detection with deep neural networks, the tracking-by-detection paradigm has gained increasing prevalence in the research community of multi-object tracking (MOT). It has long been known that appearance information plays an essential role in the detection-to-track association, which lies at the core of the tracking-by-detection paradigm. While most existing works consider the appearance distances between the detections and the tracks, they ignore the statistical information implied by the historical appearance distance records in the tracks, which can be particularly useful when a detection has similar distances with two or more tracks. In this work, we propose a hybrid track association (HTA) algorithm that models the historical appearance distances of a track with an incremental Gaussian mixture model (IGMM) and incorporates the derived statistical information into the calculation of the detection-to-track association cost. Experimental results on three MOT benchmarks confirm that HTA effectively improves the target identification performance with a small compromise to the tracking speed. Additionally, compared to many state-of-the-art trackers, the DeepSORT tracker equipped with HTA achieves better or comparable performance in terms of the balance of tracking quality and speed.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521001094",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Association (psychology)",
      "BitTorrent tracker",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Epistemology",
      "Eye tracking",
      "Object (grammar)",
      "Object detection",
      "Operating system",
      "Pattern recognition (psychology)",
      "Pedagogy",
      "Philosophy",
      "Psychology",
      "Track (disk drive)",
      "Tracking (education)",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Lin",
        "given_name": "Xufeng"
      },
      {
        "surname": "Li",
        "given_name": "Chang-Tsun"
      },
      {
        "surname": "Sanchez",
        "given_name": "Victor"
      },
      {
        "surname": "Maple",
        "given_name": "Carsten"
      }
    ]
  },
  {
    "title": "Online anomaly detection in surveillance videos with asymptotic bound on false alarm rate",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107865",
    "abstract": "Anomaly detection in surveillance videos is attracting an increasing amount of attention. Despite the competitive performance of recent methods, they lack theoretical performance analysis, particularly due to the complex deep neural network architectures used in decision making. Additionally, online decision making is an important but mostly neglected factor in this domain. Much of the existing methods that claim to be online, depend on batch or offline processing in practice. Motivated by these research gaps, we propose an online anomaly detection method in surveillance videos with asymptotic bounds on the false alarm rate, which in turn provides a clear procedure for selecting a proper decision threshold that satisfies the desired false alarm rate. Our proposed algorithm consists of a multi-objective deep learning module along with a statistical anomaly detection module, and its effectiveness is demonstrated on several publicly available data sets where we outperform the state-of-the-art algorithms. All codes are available at https://github.com/kevaldoshi17/Prediction-based-Video-Anomaly-Detection-.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321000522",
    "keywords": [
      "Anomaly detection",
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Constant false alarm rate",
      "False alarm",
      "False positive rate",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Doshi",
        "given_name": "Keval"
      },
      {
        "surname": "Yilmaz",
        "given_name": "Yasin"
      }
    ]
  },
  {
    "title": "Super-resolving blurry face images with identity preservation",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.03.024",
    "abstract": "Face images captured in unconstrained settings may suffer from one or multiple degradations, which would degrade the visual aesthetics of images and the performance of face recognition methods. However, many current methods only focus on a specific degradation or restoring the images without considering face identity. To address these problems, an identity-preservation-based deep learning method is proposed for super-resolving blurry face images. First, an extra recognition module is designed and integrated with the restoration module to extract different levels of identity-related and semantic features. Second, an assemble loss function is developed to use the identity preservation information as regularization and prior to guide the restoration and recognition process. Finally, qualitative and quantitative evaluations are conducted to demonstrate the effectiveness of the proposed method for face recovery and face recognition. The results indicate that facial identity can serve as an effective prior to face image restoration.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521001112",
    "keywords": [
      "Aesthetics",
      "Art",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Face (sociological concept)",
      "Facial recognition system",
      "Focus (optics)",
      "Identity (music)",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Yong"
      },
      {
        "surname": "Zou",
        "given_name": "Haoyang"
      },
      {
        "surname": "Huang",
        "given_name": "Yan"
      },
      {
        "surname": "Jin",
        "given_name": "Lianwen"
      },
      {
        "surname": "Ling",
        "given_name": "Haibin"
      }
    ]
  },
  {
    "title": "Feature selection based on fuzzy-neighborhood relative decision entropy",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.03.001",
    "abstract": "Feature selection facilitates pattern recognition, and fuzzy neighborhood rough sets provide an effective tool. By fuzzy neighborhood rough sets, we propose a heuristic feature selection algorithm based on fuzzy-neighborhood relative decision entropy, called AFNRDE. At first, the fuzzy-neighborhood relative decision entropy is proposed by granulation extension and information fusion, and it acquires uncertainty measurement, integration computing, and granulation monotonicity; then, the corresponding feature selection and heuristic reduction algorithm are constructed; finally, the measure monotonicity and algorithm validity are verified by numerical example and data experiment. AFNRDE promotes initial algorithm FSMRDE based on relative decision entropy to numerical data processing, and it also outperforms two usual methods FNRS and FNGRS for classification performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521000799",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Entropy (arrow of time)",
      "Feature selection",
      "Fuzzy classification",
      "Fuzzy logic",
      "Fuzzy number",
      "Fuzzy set",
      "Mathematical optimization",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Xianyong"
      },
      {
        "surname": "Fan",
        "given_name": "Yunrui"
      },
      {
        "surname": "Yang",
        "given_name": "Jilin"
      }
    ]
  },
  {
    "title": "Joint stroke classification and text line grouping in online handwritten documents with edge pooling attention networks",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107859",
    "abstract": "Stroke classification and text line grouping are important tasks in online handwritten document segmentation. In the past, the two tasks were usually performed using different models which are trained independently and perform sequentially. This cannot optimize the integration of contextual information and the system may suffer from error accumulation in stroke classification. In this paper, we propose a method for joint text/non-text stroke classification and text line grouping in online handwritten documents using attention based graph neural network. In our framework, the stroke classification and text line grouping problems are formulated as node classification and node clustering problems in a relational graph, which is constructed based on the temporal and spatial relationship between strokes. We propose a new graph network architecture, called edge pooling attention network (EPAT) to efficiently aggregate information between the features of neighboring nodes and edges. The proposed model is trained by multi-task learning with cross entropy loss for node classification and distance metric loss for node clustering. In experiments on two online handwritten document datasets IAMOnDo and Kondate, the proposed method is demonstrated effective, yielding superior performance in both stroke classification and text line grouping.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321000467",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Convolutional neural network",
      "Enhanced Data Rates for GSM Evolution",
      "Graph",
      "Machine learning",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Pooling",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Ye",
        "given_name": "Jun-Yu"
      },
      {
        "surname": "Zhang",
        "given_name": "Yan-Ming"
      },
      {
        "surname": "Yang",
        "given_name": "Qing"
      },
      {
        "surname": "Liu",
        "given_name": "Cheng-Lin"
      }
    ]
  },
  {
    "title": "Attention fusion network for multi-spectral semantic segmentation",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.03.015",
    "abstract": "To improve the accuracy of multi-spectral semantic segmentation, an attention fusion network (AFNet) based on deep learning is proposed. Different from current methods, the AFNet uses a co-attention mechanism by designing an attention fusion module to calculate the spatial correlation between the red-green-blue (RGB) image and infrared (IR) image feature maps to guide the fusion of features from different spectra. This approach enhances the feature presentation and makes full use of the complementary characteristics of multi-spectral sources. The proposed network is tested on RGB-IR datasets and compared with relevant state-of-the-art networks. The experimental analyses prove that the proposed AFNet can improve multi-spectral semantic segmentation results with good visual definition and high accuracy in classification and localization.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521001021",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Fusion",
      "Image (mathematics)",
      "Image fusion",
      "Image segmentation",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "RGB color model",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Jiangtao"
      },
      {
        "surname": "Lu",
        "given_name": "Kaige"
      },
      {
        "surname": "Wang",
        "given_name": "Han"
      }
    ]
  },
  {
    "title": "Improving knowledge distillation using unified ensembles of specialized teachers",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.03.014",
    "abstract": "The increasing complexity of deep learning models led to the development of Knowledge Distillation (KD) approaches that enable us to transfer the knowledge between a very large network, called teacher and a smaller and faster one, called student. However, as recent evidence suggests, using powerful teachers often negatively impacts the effectiveness of the distillation process. In this paper, the reasons behind this apparent limitation are studied and an approach that transfers the knowledge to smaller models more efficiently is proposed. To this end, multiple highly specialized teachers are employed, each one for a small set of skills, overcoming the aforementioned limitation, while also achieving high distillation efficiency by diversifying the ensemble. At the same time, the employed ensemble is formulated in a unified structure, making it possible to simultaneously train multiple models. The effectiveness of the proposed method is demonstrated using three different image datasets, leading to improved distillation performance, even when compared with powerful state-of-the-art ensemble-based distillation methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552100101X",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Distillation",
      "Ensemble learning",
      "Machine learning",
      "Operating system",
      "Process (computing)",
      "Programming language",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Zaras",
        "given_name": "Adamantios"
      },
      {
        "surname": "Passalis",
        "given_name": "Nikolaos"
      },
      {
        "surname": "Tefas",
        "given_name": "Anastasios"
      }
    ]
  },
  {
    "title": "Geometric moment invariants to spatial transform and N-fold symmetric blur",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107887",
    "abstract": "In this paper, we focus on the derivation of blur moment invariants. Blur moment invariants are image moment-based features, which preserve their values when the image is convolved by a point-spread function (PSF). Suppose a PSF has N -fold rotational symmetry, we prove its geometric moments of the same order are linearly dependent. Depending on this property, a new approach is proposed to determine whether an existing similarity or affine moment invariant also has invariance to N -fold symmetric blur. Unlike earlier work, this method is not based on complicated operators and construction formulas. We use it to analyse classical moment-based features, and surprisingly find that five of Hu moment invariants are naturally invariant to N -fold symmetric blur. Meanwhile, we first prove the existence of moment invariants to both affine transform and N -fold symmetric blur. The experiments using synthetic and real blur image datasets are carried out to test these expectations. And the results show that five Hu moment invariants outperform some widely used blur moment invariants and non-moment image features in image retrieval, classification and template matching.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321000741",
    "keywords": [
      "Affine transformation",
      "Artificial intelligence",
      "Classical mechanics",
      "Computer science",
      "Computer vision",
      "Image (mathematics)",
      "Image moment",
      "Image processing",
      "Invariant (physics)",
      "Mathematical analysis",
      "Mathematical physics",
      "Mathematics",
      "Moment (physics)",
      "Physics",
      "Pure mathematics"
    ],
    "authors": [
      {
        "surname": "Mo",
        "given_name": "Hanlin"
      },
      {
        "surname": "Hao",
        "given_name": "Hongxiang"
      },
      {
        "surname": "Li",
        "given_name": "Hua"
      }
    ]
  },
  {
    "title": "Perceptual quality-preserving black-box attack against deep learning image classifiers",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.03.033",
    "abstract": "Deep neural networks provide unprecedented performance in all image classification problems, including biometric recognition systems, key elements in all smart city environments. Recent studies, however, have shown their vulnerability to adversarial attacks, spawning intense research in this field. To improve system security, new countermeasures and stronger attacks are proposed by the day. On the attacker’s side, there is growing interest for the realistic black-box scenario, in which the user has no access to the network parameters. The problem is to design efficient attacks which mislead the neural network without compromising image quality. In this work, we propose to perform the black-box attack along a high-saliency and low-distortion path, so as to improve both attack efficiency and image perceptual quality. Experiments on real-world systems prove the effectiveness of the proposed approach both on benchmark tasks and actual biometric applications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521001288",
    "keywords": [
      "Adversarial system",
      "Amplifier",
      "Artificial intelligence",
      "Artificial neural network",
      "Bandwidth (computing)",
      "Benchmark (surveying)",
      "Biology",
      "Biometrics",
      "Black box",
      "Computer network",
      "Computer science",
      "Computer security",
      "Deep learning",
      "Deep neural networks",
      "Distortion (music)",
      "Epistemology",
      "Field (mathematics)",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Image quality",
      "Key (lock)",
      "Machine learning",
      "Mathematics",
      "Neuroscience",
      "Perception",
      "Philosophy",
      "Pure mathematics",
      "Quality (philosophy)",
      "Vulnerability (computing)"
    ],
    "authors": [
      {
        "surname": "Gragnaniello",
        "given_name": "Diego"
      },
      {
        "surname": "Marra",
        "given_name": "Francesco"
      },
      {
        "surname": "Verdoliva",
        "given_name": "Luisa"
      },
      {
        "surname": "Poggi",
        "given_name": "Giovanni"
      }
    ]
  },
  {
    "title": "Divergent-convergent attention for image captioning",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107928",
    "abstract": "Attention mechanism has made great progress in image captioning, where semantic words or local regions are selectively embedded into the language model. However, current attention-based image captioning methods ignore the fine-grained semantic information and their interaction with visual regions. Inspired by the activity of human in describing an image: divergent observation and convergent attention, we propose a novel divergent-convergent attention (DCA) model to tackle the problems of the current attention model in image captioning. In our DCA model, divergent observation is mainly reflected in the multi-perspective inputs: a visual collection coming from object detection and three semantic components of scene graph made of objects, attributes and relations respectively. Then the convergent attention merges these multi-perspective inputs by adaptively deciding which perspective is crucial and which element in the focused perspective dominates in the attention process through a hierarchical structure. Our model also makes use of the interaction between visual objects and semantic components to achieve complementary advantages. Above all, owing to the interaction between divergent visual and semantic components, and the gradual convergence of attention, our model can attend to the corresponding local region more precisely under the guidance of semantic components. Besides, with the assistance of the visual components, the DCA model can effectively utilize the fine-grained semantic components to generate more descriptive sentences. Experiments on the MS COCO dataset demonstrate the superiority of our proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321001151",
    "keywords": [
      "Artificial intelligence",
      "Closed captioning",
      "Computer science",
      "Convergence (economics)",
      "Economic growth",
      "Economics",
      "Graph",
      "Image (mathematics)",
      "Natural language processing",
      "Object (grammar)",
      "Operating system",
      "Perspective (graphical)",
      "Process (computing)",
      "Rendering (computer graphics)",
      "Scene graph",
      "Theoretical computer science",
      "Visualization"
    ],
    "authors": [
      {
        "surname": "Ji",
        "given_name": "Junzhong"
      },
      {
        "surname": "Du",
        "given_name": "Zhuoran"
      },
      {
        "surname": "Zhang",
        "given_name": "Xiaodan"
      }
    ]
  },
  {
    "title": "Dyadic relational graph convolutional networks for skeleton-based human interaction recognition",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107920",
    "abstract": "Skeleton-based human interaction recognition is a challenging task requiring all abilities to recognize spatial, temporal, and interactive features. These abilities rarely co-exist in existing methods. Graph convolutional network (GCN) based methods fail to extract interactive features. Traditional interaction recognition methods cannot effectively capture spatial features from skeletons. Toward this end, we propose a novel Dyadic Relational Graph Convolutional Network (DR-GCN) for interaction recognition. Specifically, we make four contributions: (i) we design a Relational Adjacency Matrix (RAM) that represents dynamic relational graphs. These graphs are constructed combining both geometric features and relative attention from the two skeleton sequences; (ii) we propose a Dyadic Relational Graph Convolution Block (DR-GCB) that extracts spatial-temporal interactive features; (iii) we stack the proposed DR-GCBs to build DR-GCN and integrate our methods with an advanced model. (iv) Our models achieve state-of-the-art results on SBU and significant improvements on the mutual action sub-datasets of NTU-RGB+D and NTU-RGB+D 120.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321001072",
    "keywords": [
      "Action recognition",
      "Adjacency list",
      "Adjacency matrix",
      "Algorithm",
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Convolutional neural network",
      "Data mining",
      "Graph",
      "Pattern recognition (psychology)",
      "RGB color model",
      "Relational database",
      "Statistical relational learning",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Zhu",
        "given_name": "Liping"
      },
      {
        "surname": "Wan",
        "given_name": "Bohua"
      },
      {
        "surname": "Li",
        "given_name": "Chengyang"
      },
      {
        "surname": "Tian",
        "given_name": "Gangyi"
      },
      {
        "surname": "Hou",
        "given_name": "Yi"
      },
      {
        "surname": "Yuan",
        "given_name": "Kun"
      }
    ]
  },
  {
    "title": "Using recurrent neural networks for continuous authentication through gait analysis",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.03.010",
    "abstract": "This letter presents a novel framework for continuous user authentication of mobile devices based on gait analysis, exploiting inertial sensors and Recurrent Neural Network for deep-learning based classification. The proposed framework handles all the continuous authentication stages, starting from data collection to data preprocessing, classification, and policy enforcement. The letter will emphasize the data analysis aspects, discussing the methodologies used to improve the quality of classification, including data augmentation and a sliding window interval approach for improved training. Furthermore, will be discussed the enforcement, which is based on the Usage Control paradigm for continuous policy enforcement. A set of real experiments will demonstrate the effectiveness and efficiency of the proposed framework.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521000970",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Authentication (law)",
      "Computer science",
      "Computer security",
      "Data mining",
      "Data pre-processing",
      "Enforcement",
      "Law",
      "Machine learning",
      "Operating system",
      "Political science",
      "Preprocessor",
      "Programming language",
      "Set (abstract data type)",
      "Sliding window protocol",
      "Window (computing)"
    ],
    "authors": [
      {
        "surname": "Giorgi",
        "given_name": "Giacomo"
      },
      {
        "surname": "Saracino",
        "given_name": "Andrea"
      },
      {
        "surname": "Martinelli",
        "given_name": "Fabio"
      }
    ]
  },
  {
    "title": "STA3D: Spatiotemporally attentive 3D network for video saliency prediction",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.04.010",
    "abstract": "3D fully convolutional networks (FCN), which jointly leverage the spatial and temporal cues, have achieved great success in video saliency prediction. However, they still have limitations in some challenging cases, e.g. fixation shift. To address this issue, we propose a SpatioTemporally Attentive 3D Network (STA3D) to selectively propagate the significant temporal features and refine the spatial features in 3D FCN for video saliency prediction. Extensive experiments on three standard datasets demonstrate the superiority of the proposed model against the state-of-the-art.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521001409",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Demography",
      "Fixation (population genetics)",
      "Leverage (statistics)",
      "Pattern recognition (psychology)",
      "Population",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Zou",
        "given_name": "Wenbin"
      },
      {
        "surname": "Zhuo",
        "given_name": "Shengkai"
      },
      {
        "surname": "Tang",
        "given_name": "Yi"
      },
      {
        "surname": "Tian",
        "given_name": "Shishun"
      },
      {
        "surname": "Li",
        "given_name": "Xia"
      },
      {
        "surname": "Xu",
        "given_name": "Chen"
      }
    ]
  },
  {
    "title": "A novel consensus learning approach to incomplete multi-view clustering",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107890",
    "abstract": "Multi-view data may lose some instances in real applications. Most existing methods for clustering such incomplete multi-view data still have at least one of the following limitations: 1) The common relations among data points across all views are ignored. 2) The complementary multi-view information of original data representation is not well exploited. 3) Arbitrary incomplete scenarios or data with negative entries cannot be handled. To address these limitations, in this paper, we propose a novel Consensus Learning approach to Incomplete Multi-view Clustering (CLIMC). Specifically, a low-dimensional consensus representation is introduced to exploit complementary multi-view information from the original feature representation of available instances by integrating index matrices into matrix factorization. In addition, by combining self-representation, index matrices, and consensus term, a consensus similarity graph is leveraged to explore the underlying cross-view relations among data points. Further, the key of the proposed CLIMC is that the consensus representation is correlated with the similarity graph by a graph Laplacian regularization. Consequently, the compactness of the low-dimensional representation and the accuracy of similarity degree of the graph are reciprocally promoted. Extensive experiments on several multi-view datasets demonstrate the effectiveness of CLIMC over state-of-the-arts.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321000777",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Data point",
      "Feature learning",
      "Graph",
      "Image (mathematics)",
      "Laplacian matrix",
      "Law",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Similarity (geometry)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Jianlun"
      },
      {
        "surname": "Teng",
        "given_name": "Shaohua"
      },
      {
        "surname": "Fei",
        "given_name": "Lunke"
      },
      {
        "surname": "Zhang",
        "given_name": "Wei"
      },
      {
        "surname": "Fang",
        "given_name": "Xiaozhao"
      },
      {
        "surname": "Zhang",
        "given_name": "Zhuxiu"
      },
      {
        "surname": "Wu",
        "given_name": "Naiqi"
      }
    ]
  },
  {
    "title": "Brain segmentation based on multi-atlas and diffeomorphism guided 3D fully convolutional network ensembles",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107904",
    "abstract": "In this study, we proposed and validated a multi-atlas and diffeomorphism guided 3D fully convolutional network (FCN) ensemble model (M-FCN) for segmenting brain anatomical regions of interest (ROIs) from structural magnetic resonance images (MRIs). A novel multi-atlas and diffeomorphism based encoding block and ROI patches with adaptive sizes were used. In the multi-atlas and diffeomorphism based encoding block, both MRI intensity profiles and expert priors from deformed atlases were encoded and fed to the proposed FCN. Utilizing patches with adaptive sizes enabled more efficient network training and testing. To incorporate both local and global contextual information of a specific ROI, we employed a long skip connection between the layer of the encoding block and the layer of the encoding-decoding block. To relieve over-fitting of the proposed FCN model on the training data, we adopted an ensemble strategy in the learning procedure. Systematic evaluations were performed on two brain MRI datasets, aiming respectively at segmenting 14 subcortical and ventricular structures and 54 whole-brain ROIs. Compared with two state-of-the-art segmentation methods including a multi-atlas based segmentation method and an existing 3D FCN segmentation model, the proposed method exhibited superior segmentation performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321000911",
    "keywords": [
      "Anatomy",
      "Artificial intelligence",
      "Atlas (anatomy)",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Diffeomorphism",
      "Mathematical analysis",
      "Mathematics",
      "Medicine",
      "Pattern recognition (psychology)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Jiong"
      },
      {
        "surname": "Tang",
        "given_name": "Xiaoying"
      }
    ]
  },
  {
    "title": "GPNet: Gated pyramid network for semantic segmentation",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107940",
    "abstract": "Semantic segmentation is a challenging task which requires both solid unanimous global context and rich spatial information. Recent methods ignore adaptively capturing of valid feature. The lack of useful multi-scale information filtering hinders further explicit feature generation. In this paper, we develop a novel network named GPNet, which can densely capture and filter the multi-scale information in a gated and pair-wise manner. Specifically, a Gated Pyramid Module (GPM) is designed to incorporate dense and growing receptive fields from both low-level and high-level features. In GPM we build a gated path to select useful context among multi-scale information. Moreover, a Cross-Layer Attention Module (CLAM) is proposed to reuse the context information from shallow layers to guide the deep features. Comprehensive experimental evaluations are conducted on popular semantic segmentation benchmarks including Cityscapes and ADE20K. Our GPNet achieves the mIoU score of 82.5% and 45.81% on Cityscapes test set and ADE20K validation set, respectively, which are the new state-of-the-art results using ResNet-101 as the backbone.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321001278",
    "keywords": [
      "Artificial intelligence",
      "Attention network",
      "Backbone network",
      "Biology",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Ecology",
      "Economics",
      "Feature (linguistics)",
      "Filter (signal processing)",
      "Geometry",
      "Linguistics",
      "Management",
      "Mathematics",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Programming language",
      "Pyramid (geometry)",
      "Quantum mechanics",
      "Reuse",
      "Scale (ratio)",
      "Segmentation",
      "Set (abstract data type)",
      "Task (project management)",
      "Test set"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Yu"
      },
      {
        "surname": "Sun",
        "given_name": "Xin"
      },
      {
        "surname": "Dong",
        "given_name": "Junyu"
      },
      {
        "surname": "Chen",
        "given_name": "Changrui"
      },
      {
        "surname": "Lv",
        "given_name": "Qingxuan"
      }
    ]
  },
  {
    "title": "Are IoBT services accessible to everyone?",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.04.007",
    "abstract": "Biometric recognition aims at identifying a person by using their physiological or behavioral characteristics. When adopted for improving the security in the Internet of Things (IoT) field, it is commonly named Internet of Biometric Things (IoBT). However, despite its advantages there are further considerations on security and different ethical and legal issues, such as the possibility of exclusion of individuals due to pathologies, injuries, disabilities, or genetic defects. Indeed, these specific physical condition would lead to not satisfy the requirements commonly used for biometric recognition. As a consequence, the limitations of current biometric systems can exclude a person from the use of IoBT services. In this paper, we focus on the difficulty of iris recognition when it is affected by Coloboma, a congenital abnormality of membranes of the eye. We show how this pathological state impacts on the performance of the Daugman and Canny edge detection algorithms, which represent the most widespread methods used for the iris localization step in eye-based biometric. Results of an experimentation revealed that they correctly detected only 15.79% and 47.37% of Coloboma iris, respectively. In order to avoid the use of these inaccurate algorithms in case of Coloboma eye, we designed and experimented a Residual Neural Network classifier able to detect the presence of this disease with 99.79% of accuracy. This classifier may be a first step towards a more sophisticated “diversity-aware” biometric system which represents an alternative to actual IoBT authentication method for people with special physical condition.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521001379",
    "keywords": [
      "Artificial intelligence",
      "Biometrics",
      "Classifier (UML)",
      "Coloboma",
      "Computer science",
      "Computer security",
      "Iris recognition",
      "Machine learning",
      "Medicine",
      "Ophthalmology",
      "Password"
    ],
    "authors": [
      {
        "surname": "Francese",
        "given_name": "Rita"
      },
      {
        "surname": "Frasca",
        "given_name": "Maria"
      },
      {
        "surname": "Risi",
        "given_name": "Michele"
      }
    ]
  },
  {
    "title": "Reconstruction of frescoes by sequential layers of feature extraction",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.04.012",
    "abstract": "This work presents the development of a new method for solving puzzles, based on sequence of layers, called Retention Layers Method, using feature extraction algorithms with a new method to select the corresponding keypoints between puzzle pieces and reference image, called the Middle Triangle Method. Promising results, both in the correct positioning of the fragments and in the disposal of spurious fragments, can be seen by the high ratio of correctly positioned fragments, reaching up to 100%, surpassing the analyzed puzzle solvers.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521001422",
    "keywords": [
      "Algorithm",
      "Art",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Feature extraction",
      "Fresco",
      "Genetics",
      "Image (mathematics)",
      "Linguistics",
      "Machine learning",
      "Painting",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Sequence (biology)",
      "Spurious relationship",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "da Silva Teixeira",
        "given_name": "Thiago"
      },
      {
        "surname": "Sguario Coelho de Andrade",
        "given_name": "Mauren Louise"
      },
      {
        "surname": "Rodrigues da Luz",
        "given_name": "Mathias"
      }
    ]
  },
  {
    "title": "Boundary-induced and scene-aggregated network for monocular depth prediction",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107901",
    "abstract": "Monocular depth prediction is an important task in scene understanding. It aims to predict the dense depth of a single RGB image. With the development of deep learning, the performance of this task has made great improvements. However, two issues remain unresolved: (1) The deep feature encodes the wrong farthest region in a scene, which leads to a distorted 3D structure of the predicted depth; (2) The low-level features are insufficient utilized, which makes it even harder to estimate the depth near the edge with sudden depth change. To tackle these two issues, we propose the Boundary-induced and Scene-aggregated network (BS-Net). In this network, the Depth Correlation Encoder (DCE) is first designed to obtain the contextual correlations between the regions in an image, and perceive the farthest region by considering the correlations. Meanwhile, the Bottom-Up Boundary Fusion (BUBF) module is designed to extract accurate boundary that indicates depth change. Finally, the Stripe Refinement module (SRM) is designed to refine the dense depth induced by the boundary cue, which improves the boundary accuracy of the predicted depth. Several experimental results on the NYUD v2 dataset and the iBims-1 dataset illustrate the state-of-the-art performance of the proposed approach. And the SUN-RGBD dataset is employed to evaluate the generalization of our method. Code is available at https://github.com/XuefengBUPT/BS-Net.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321000881",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Boundary (topology)",
      "Code (set theory)",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Depth map",
      "Depth perception",
      "Economics",
      "Encoder",
      "Feature (linguistics)",
      "Generalization",
      "Image (mathematics)",
      "Linguistics",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Monocular",
      "Neuroscience",
      "Operating system",
      "Pattern recognition (psychology)",
      "Perception",
      "Philosophy",
      "Programming language",
      "RGB color model",
      "Set (abstract data type)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Xue",
        "given_name": "Feng"
      },
      {
        "surname": "Cao",
        "given_name": "Junfeng"
      },
      {
        "surname": "Zhou",
        "given_name": "Yu"
      },
      {
        "surname": "Sheng",
        "given_name": "Fei"
      },
      {
        "surname": "Wang",
        "given_name": "Yankai"
      },
      {
        "surname": "Ming",
        "given_name": "Anlong"
      }
    ]
  },
  {
    "title": "Pedestrian detection with super-resolution reconstruction for low-quality image",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107846",
    "abstract": "Pedestrian detection has emerged as a fundamental technology for autonomous cars, robotics, pedestrian search, and other applications. Although many excellent object detection algorithms can be used for pedestrian detection, it is still a challenging problem due to the complicated real-world scenarios, e.g., the detection of pedestrians in low-quality surveillance videos. In this paper, we aim to study the challenging topic of pedestrian detection in low-quality images. Low-quality images are interpreted as those taken with a low-resolution camera, heavy weather or a blurred scene, making it difficult to distinguish pedestrians from the background. To solve this problem, we first introduce a dataset called playground (PG) for low-quality image detection. Images from PG are shot using two different camera views, and pedestrian images are taken at different periods, including day and night. The dataset contains a total of 5,752 images with 31,041 annotations. The average size of the pedestrian is 87 × 41 and the image size is 480 × 640, indicating that these images are taken from very long distances. Then, we propose a super-resolution detection (SRD) network to enhance the resolution of low-quality images that can help distinguish pedestrians from the blurred background. Finally, based on these enhanced images, we adopt and improve the Faster R-CNN network to help relocate occluded pedestrians. Experimental results on this new dataset proved the efficiency and effectiveness of our algorithm on low-quality images.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321000339",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Epistemology",
      "Geography",
      "High resolution",
      "Image (mathematics)",
      "Image quality",
      "Image resolution",
      "Low resolution",
      "Object detection",
      "Pattern recognition (psychology)",
      "Pedestrian",
      "Pedestrian detection",
      "Philosophy",
      "Quality (philosophy)",
      "Remote sensing",
      "Robot",
      "Robotics",
      "Superresolution"
    ],
    "authors": [
      {
        "surname": "Jin",
        "given_name": "Yi"
      },
      {
        "surname": "Zhang",
        "given_name": "Yue"
      },
      {
        "surname": "Cen",
        "given_name": "Yigang"
      },
      {
        "surname": "Li",
        "given_name": "Yidong"
      },
      {
        "surname": "Mladenovic",
        "given_name": "Vladimir"
      },
      {
        "surname": "Voronin",
        "given_name": "Viacheslav"
      }
    ]
  },
  {
    "title": "Skeletonisation algorithms with theoretical guarantees for unorganised point clouds with high levels of noise",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107902",
    "abstract": "Data Science aims to extract meaningful knowledge from unorganised data. Real datasets usually come in the form of a cloud of points. It is a requirement of numerous applications to visualise an overall shape of a noisy cloud of points sampled from a non-linear object that is more complicated than a union of disjoint clusters. The skeletonisation problem in its hardest form is to find a 1-dimensional skeleton that correctly represents the shape of the cloud. This paper compares different algorithms that solve the above skeletonisation problem for any point cloud and guarantee a successful reconstruction. For example, given a highly noisy point sample of an unknown underlying graph, a reconstructed skeleton should be geometrically close and homotopy equivalent to (has the same number of independent cycles as) the underlying graph. One of these algorithms produces a Homologically Persistent Skeleton (HoPeS) for any cloud without extra parameters. This universal skeleton contains subgraphs that provably represent the 1-dimensional shape of the cloud at any scale. Other subgraphs of HoPeS reconstruct an unknown graph from its noisy point sample with a correct homotopy type and within a small offset of the sample. The extensive experiments on synthetic and real data reveal for the first time the maximum level of noise that allows successful graph reconstructions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321000893",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Discrete mathematics",
      "Disjoint sets",
      "Graph",
      "Homotopy",
      "Mathematics",
      "Offset (computer science)",
      "Point cloud",
      "Programming language",
      "Pure mathematics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Smith",
        "given_name": "P."
      },
      {
        "surname": "Kurlin",
        "given_name": "V."
      }
    ]
  },
  {
    "title": "A hierarchical sampling based triplet network for fine-grained image classification",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107889",
    "abstract": "Deep metric learning leverages well-designed distance measurement and a sample selection strategy to learn a discriminative feature space. Among the various deep metric learning formulations, triplet loss is built based on a 3-tuple that can simultaneously minimise the distance between the items in the positive pair and maximise the distance between those in the negative pair. However, this endeavour requires a critical selection of triplet samples to guide the training process. In this paper, we propose a layered Triplet loss to solve the fine-grained image classification problem. Unlike the existing triplet loss, which selects samples from only a single criterion, we construct the loss function with the ’coarse to fine’ scheme. This scheme can separate the coarse-level classes while clustering the fine-level samples within a certain margin. An ontology-based sampling method is proposed to enable the network to mine more reasonable hard triplets. Semantic knowledge is employed to assign the visually similar classes to the same learning task, from which hard triplets can be generated. Finally, the softmax tree classifier is used to classify the hierarchical features. The experimental results on multiple datasets demonstrate the effectiveness of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321000765",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Deep learning",
      "Discriminative model",
      "Economics",
      "Feature selection",
      "Machine learning",
      "Margin (machine learning)",
      "Metric (unit)",
      "Operations management",
      "Pattern recognition (psychology)",
      "Softmax function"
    ],
    "authors": [
      {
        "surname": "He",
        "given_name": "Guiqing"
      },
      {
        "surname": "Li",
        "given_name": "Feng"
      },
      {
        "surname": "Wang",
        "given_name": "Qiyao"
      },
      {
        "surname": "Bai",
        "given_name": "Zongwen"
      },
      {
        "surname": "Xu",
        "given_name": "Yuelei"
      }
    ]
  },
  {
    "title": "A survey on matching strategies for boundary image comparison and evaluation",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107883",
    "abstract": "Most of the strategies for boundary image evaluation involve the comparison of computer-generated images with ground truth solutions. While this can be done in different manners, recent years have seen a dominance of techniques based on the use of confusion matrices. That is, techniques that, at the evaluation stage, interpret boundary detection as a classification problem. These techniques require a correspondence between the boundary pixels in the candidate image and those in the ground truth; that correspondence is further used to create the confusion matrix, from which evaluation statistics can be computed. The correspondence between boundary images faces different challenges, mainly related to the matching of potentially displaced boundaries. Interestingly, boundary image comparison relates to many other fields of study in literature, from object tracking to biometrical identification. In this work, we survey all existing strategies for boundary matching, we propose a taxonomy to embrace them all, and perform a usability-driven quantitative analysis of their behaviour.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321000704",
    "keywords": [
      "Artificial intelligence",
      "Boundary (topology)",
      "Computer science",
      "Computer vision",
      "Confusion",
      "Confusion matrix",
      "Ground truth",
      "Human–computer interaction",
      "Image (mathematics)",
      "Matching (statistics)",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pixel",
      "Psychoanalysis",
      "Psychology",
      "Statistics",
      "Usability"
    ],
    "authors": [
      {
        "surname": "Lopez-Molina",
        "given_name": "C."
      },
      {
        "surname": "Marco-Detchart",
        "given_name": "C."
      },
      {
        "surname": "Bustince",
        "given_name": "H."
      },
      {
        "surname": "De Baets",
        "given_name": "B."
      }
    ]
  },
  {
    "title": "SmartCAMPP - Smartphone-based continuous authentication leveraging motion sensors with privacy preservation",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.04.013",
    "abstract": "Continuous Authentication (CA) approaches are attracting attention due to the explosion of available sensors from IoT devices such as smartphones. However, a critical privacy concern arises when CA data is outsourced. Data from motion sensors may reveal users’ private issues. Despite the need for CA in smartphones, no previous work has explored how to tackle this matter leveraging motion sensors in a privacy-preserving way. In this work, a mechanism dubbed SmartCAMPP is proposed to achieve CA based on gyroscope and accelerometer data. Format-preserving encryption techniques are applied to privately outsource them. Our results show the suitability of the proposed scheme, featuring 76.85 % of accuracy while taking 5.12 ms. of computation for authenticating each user. Interestingly, the use of cryptography does not lead to a significant impact as compared to a non-privacy-preserving mechanism.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521001434",
    "keywords": [
      "Accelerometer",
      "Aerospace engineering",
      "Artificial intelligence",
      "Authentication (law)",
      "Computer science",
      "Computer security",
      "Cryptography",
      "Encryption",
      "Engineering",
      "Gyroscope",
      "Mobile device",
      "Motion (physics)",
      "Motion sensors",
      "Operating system",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Hernández-Álvarez",
        "given_name": "Luis"
      },
      {
        "surname": "de Fuentes",
        "given_name": "José María"
      },
      {
        "surname": "González-Manzano",
        "given_name": "Lorena"
      },
      {
        "surname": "Hernández Encinas",
        "given_name": "Luis"
      }
    ]
  },
  {
    "title": "Time series cluster kernels to exploit informative missingness and incomplete label information",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107896",
    "abstract": "The time series cluster kernel (TCK) provides a powerful tool for analysing multivariate time series subject to missing data. TCK is designed using an ensemble learning approach in which Bayesian mixture models form the base models. Because of the Bayesian approach, TCK can naturally deal with missing values without resorting to imputation and the ensemble strategy ensures robustness to hyperparameters, making it particularly well suited for unsupervised learning. However, TCK assumes missing at random and that the underlying missingness mechanism is ignorable, i.e. uninformative, an assumption that does not hold in many real-world applications, such as e.g. medicine. To overcome this limitation, we present a kernel capable of exploiting the potentially rich information in the missing values and patterns, as well as the information from the observed data. In our approach, we create a representation of the missing pattern, which is incorporated into mixed mode mixture models in such a way that the information provided by the missing patterns is effectively exploited. Moreover, we also propose a semi-supervised kernel, capable of taking advantage of incomplete label information to learn more accurate similarities. Experiments on benchmark data, as well as a real-world case study of patients described by longitudinal electronic health record data who potentially suffer from hospital-acquired infections, demonstrate the effectiveness of the proposed methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321000832",
    "keywords": [
      "Artificial intelligence",
      "Bayesian probability",
      "Biochemistry",
      "Chemistry",
      "Combinatorics",
      "Computer science",
      "Data mining",
      "Ensemble learning",
      "Gene",
      "Hyperparameter",
      "Imputation (statistics)",
      "Kernel (algebra)",
      "Machine learning",
      "Mathematics",
      "Missing data",
      "Pattern recognition (psychology)",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Øyvind Mikalsen",
        "given_name": "Karl"
      },
      {
        "surname": "Soguero-Ruiz",
        "given_name": "Cristina"
      },
      {
        "surname": "Maria Bianchi",
        "given_name": "Filippo"
      },
      {
        "surname": "Revhaug",
        "given_name": "Arthur"
      },
      {
        "surname": "Jenssen",
        "given_name": "Robert"
      }
    ]
  },
  {
    "title": "MRP-GAN: Multi-resolution parallel generative adversarial networks for text-to-image synthesis",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.02.020",
    "abstract": "Synthesizing photographic images from given text descriptions is a challenging problem. Although current methods first synthesize an initial blurred image, then refine the initial image to a high-quality one, the most existing methods are difficult to refine the initial image to an image corresponding to the text description. In this paper, the Multi-resolution Parallel Generative Adversarial Networks for Text-to-Image Synthesis (MRP-GAN) is proposed to generate photographic images. MRP-GAN introduces a Multi-resolution Parallel structure to refine the initial images when the initial images are not synthesized well. The low-resolution semantics are maintained through the whole process by Multi-resolution Parallel structure. Response Gate is designed to fully explore the capability of Multi-resolution Parallel structure by aggregating the outputs of the multi-resolution parallel subnetworks. We also utilize an attention mechanism, named Residual Attention Network, to fine-tune more fine-grained details of the generated images. We evaluate our MRP-GAN model on the CUB and MS-COCO datasets. Extensive experiments demonstrate the state-of-the-art performance of MRP-GAN. Besides, we apply a Multi-resolution Parallel structure in the existing method to verify its transferability.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521000891",
    "keywords": [
      "Adversarial system",
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Generative adversarial network",
      "Generative grammar",
      "Image (mathematics)",
      "Image resolution",
      "Logit",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Process (computing)",
      "Programming language",
      "Resolution (logic)",
      "Transferability"
    ],
    "authors": [
      {
        "surname": "Qi",
        "given_name": "Zhongjian"
      },
      {
        "surname": "Fan",
        "given_name": "Chaogang"
      },
      {
        "surname": "Xu",
        "given_name": "Liangfeng"
      },
      {
        "surname": "Li",
        "given_name": "Xinke"
      },
      {
        "surname": "Zhan",
        "given_name": "Shu"
      }
    ]
  },
  {
    "title": "Hierarchical fusion convolutional neural networks for SAR image segmentation",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.04.005",
    "abstract": "Convolutional neural network (CNN) has achieved promising results in image segmentation recently. However, for the segmentation of synthetic aperture radar (SAR) images with complicated scene, the single receptive field in CNN has a limited ability to effectively capture structural and regional information at the same time. In this paper, we propose a hierarchical fusion CNN (HIFCNN) model for SAR image segmentation. At each convolutional layer, HIFCNN sets several different-sized receptive fields, and thus extracts hierarchical features. Concretely, the larger-sized receptive field captures regional information and is robust against speckle, while the smaller one preserves the structural information well. Then, based on the Dempster-Shafer evidential theory, the proposed hierarchical network, HIFCNN, implements a decision-level fusion to integrate these hierarchical features. In this way, the structural and regional information can be accurately captured by different receptive fields, which is beneficial for edge location, structure preservation and region homogeneity in SAR image segmentation. The effectiveness of HIFCNN model is demonstrated by the application to the segmentation of the simulated images and real SAR images.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521001355",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Image segmentation",
      "Pattern recognition (psychology)",
      "Receptive field",
      "Scale-space segmentation",
      "Segmentation",
      "Synthetic aperture radar"
    ],
    "authors": [
      {
        "surname": "Jiang",
        "given_name": "Yinyin"
      },
      {
        "surname": "Li",
        "given_name": "Ming"
      },
      {
        "surname": "Zhang",
        "given_name": "Peng"
      },
      {
        "surname": "Tan",
        "given_name": "Xiaofeng"
      },
      {
        "surname": "Song",
        "given_name": "Wanying"
      }
    ]
  },
  {
    "title": "SML: Semantic meta-learning for few-shot semantic segmentation☆",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.03.036",
    "abstract": "The significant amount of training data required for training Convolutional Neural Networks has become a bottleneck for applications like semantic segmentation. Few-shot semantic segmentation algorithms address this problem, with an aim to achieve good performance in the low-data regime, with few annotated training images. Recent approaches based on class-prototypes computed from available training data have achieved immense success for this task. In this work, we propose a novel meta-learning framework, Semantic Meta-Learning (SML), which incorporates class level semantic descriptions in the generated prototypes for this problem. In addition, we propose to use the well-established technique, ridge regression, to not only bring in the class-level semantic information, but also to effectively utilise the information available from multiple images present in the training data for prototype computation. This has a simple closed-form solution, and thus can be implemented easily and efficiently. Extensive experiments on the benchmark PASCAL-5i dataset under different experimental settings demonstrate the effectiveness of the proposed framework.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521001318",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Bottleneck",
      "Class (philosophy)",
      "Classifier (UML)",
      "Computer science",
      "Convolutional neural network",
      "Embedded system",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Pascal (unit)",
      "Programming language",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Pambala",
        "given_name": "Ayyappa Kumar"
      },
      {
        "surname": "Dutta",
        "given_name": "Titir"
      },
      {
        "surname": "Biswas",
        "given_name": "Soma"
      }
    ]
  },
  {
    "title": "Tripool: Graph triplet pooling for 3D skeleton-based action recognition",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107921",
    "abstract": "Graph Convolutional Network (GCN) has already been successfully applied to skeleton-based action recognition. However, current GCNs in this task are lack of pooling operations such that the architectures are inherently flat, which not only increases the computational complexity but also requires larger memory space to keep the entire graph embedding. More seriously, a flat architecture forces the high-level semantic feature representations to have the same physical structure of the low-level input skeletons, which we argue is unreasonable and harmful for the final performance. To address these issues, we propose Tripool, a novel graph pooling method for 3D action recognition from skeleton data. Tripool provides to optimize a triplet pooling loss, in which both graph topology and global graph context are taken into consideration, to learn a hierarchical graph representation. The training process of graph pooling is efficient since it optimizes the graph topology by minimizing an upper bound of the pooling loss. Besides, Tripool also automatically generates an embedding matrix since the graph is changed after pooling. On one hand, Tripool reduces the computational cost by removing the redundant nodes. On the other hand it overcomes the limitation of the topology constrain for the high-level semantic representations, thus improves the final performance. Tripool can be combined with various graph neural networks in an end-to-end fashion. Comprehensive experiments on two current largest scale 3D datasets are conducted to evaluate our method. With our Tripool, we consistently get the best results in terms of various performance measures.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321001084",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Embedding",
      "Graph",
      "Line graph",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pooling",
      "Theoretical computer science",
      "Topological graph theory",
      "Topology (electrical circuits)",
      "Voltage graph"
    ],
    "authors": [
      {
        "surname": "Peng",
        "given_name": "Wei"
      },
      {
        "surname": "Hong",
        "given_name": "Xiaopeng"
      },
      {
        "surname": "Zhao",
        "given_name": "Guoying"
      }
    ]
  },
  {
    "title": "Joint feature extraction and classification in a unified framework for cost-sensitive face recognition",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107927",
    "abstract": "Cost-sensitive face recognition is a challenging problem in pattern recognition. Due to the high-dimensional face features, cost-sensitive face recognition usually conducts feature extraction in advance, followed by the learning of classifier in reduced subspace. However, the pre-extracted face features are kept fixed and may suboptimal for subsequent classifier learning, which will degrade the final face recognition performance. Besides, most of face learners are cost insensitive. Even the cost-sensitive methods proposed for face recognition, they only incorporate the cost information in feature extraction or classification phase as an alternative. There is no doubt that some cost-sensitive information will be lost in their cost insensitive steps. To deal with these issues, this paper proposes to incorporate feature extraction and classification in a unified cost-sensitive framework for face recognition. The experimental results on three public face benchmarks, including Extended Yale B, CMU PIE and LFW datasets, demonstrate that the proposed method can significantly reduce the overall misclassification loss of face recognition system as well as the classification errors associated with high costs, when comparing with eleven state-of-the-art face learners and nine cost-sensitive methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032100114X",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Face (sociological concept)",
      "Face detection",
      "Facial recognition system",
      "Feature extraction",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Social science",
      "Sociology",
      "Subspace topology",
      "Three-dimensional face recognition"
    ],
    "authors": [
      {
        "surname": "Wan",
        "given_name": "Jianwu"
      },
      {
        "surname": "Chen",
        "given_name": "Yinjuan"
      },
      {
        "surname": "Bai",
        "given_name": "Bing"
      }
    ]
  },
  {
    "title": "Hybrid models for intraday stock price forecasting based on artificial neural networks and metaheuristic algorithms",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.03.030",
    "abstract": "Stock market prediction is one of the critical issues in fiscal market. It is important issue for the traders and investors. Artificial Neural Networks (ANNs) associated with nature inspired algorithms are playing an increasingly vital role in many areas including medical field, security systems and stock market. Several prediction models have been developed by researchers to forecast stock market trend. However, few studies have focused on improving stock market prediction accuracy especially when utilizing artificial neural networks to perform the analysis. This paper proposed nine new integrated models for forecasting intraday stock price based on the potential of three ANNs, Back Propagation Neural Network (BPNN), Radial Basis Function Neural Network (RBFNN), Time Delay Neural Network (TDNN) and nature inspired algorithms such as Genetic Algorithm (GA), Particle Swarm Optimization (PSO) and Artificial Bee Colony (ABC).The developed models were named as GA-BPNN, PSO-BPNN, ABC-BPNN, GA-RBFNN, PSO-RBFNN, ABC-RBFNN, GA-TDNN, PSO-TDNN and ABC-TDNN. Nature inspired algorithms are employed for optimizing the parameters of ANNs. Technical indicators calculated from historical data are fed as input to developed models. Proposed hybrid models validated on four datasets representing different sectors in NSE. Four statistical metrics, Root Mean Square Error (RMSE), Hit Rate (HR), Error Rate (ER) and prediction accuracy were utilized to gauge the performance of the developed models. Results proved that the PSO-BPNN model yielded the highest prediction accuracy in estimating intraday stock price. The other models, GA-BPNN, ABC-BPNN, GA-RBFNN, PSO-RBFNN, ABC-RBFNN, GA-TDNN, PSO-TDNN and ABC-TDNN produced lower performance with mean prediction accuracy of 97.24%, 98.37%, 84.01%, 85.15%, 84.01%, 83.87%, 89.95% and 78.61% respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521001239",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer science",
      "Data mining",
      "Horse",
      "Machine learning",
      "Mathematics",
      "Mean squared error",
      "Paleontology",
      "Particle swarm optimization",
      "Pattern recognition (psychology)",
      "Statistics",
      "Stock market",
      "Stock market prediction"
    ],
    "authors": [
      {
        "surname": "S",
        "given_name": "Kumar Chandar"
      }
    ]
  },
  {
    "title": "A new approach for optimal offline time-series segmentation with error bound guarantee",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107917",
    "abstract": "Piecewise Linear Approximation is one of the most commonly used strategies to represent time series effectively and approximately. This approximation divides the time series into non-overlapping segments and approximates each segment with a straight line. Many suboptimal methods were proposed for this purpose. This paper proposes a new optimal approach, called OSFS, based on feasible space (FS) Liu et al. (2008)[1], that minimizes the number of segments of the approximation and guarantees the error bound using the L ∞ -norm. On the other hand, a new performance measure combined with the OSFS method has been used to evaluate the performance of some suboptimal methods and that of the optimal method that minimizes the holistic approximation error ( L 2 -norm). The results have shown that the OSFS method is optimal and demonstrates the advantages of L ∞ -norm over L 2 -norm.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321001047",
    "keywords": [
      "Algorithm",
      "Applied mathematics",
      "Approximation algorithm",
      "Approximation error",
      "Biology",
      "Computer science",
      "Data mining",
      "Law",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Measure (data warehouse)",
      "Norm (philosophy)",
      "Paleontology",
      "Piecewise",
      "Piecewise linear function",
      "Political science",
      "Series (stratigraphy)",
      "Upper and lower bounds"
    ],
    "authors": [
      {
        "surname": "Carmona-Poyato",
        "given_name": "Ángel"
      },
      {
        "surname": "Fernández-Garcia",
        "given_name": "Nicolás Luis"
      },
      {
        "surname": "Madrid-Cuevas",
        "given_name": "Francisco José"
      },
      {
        "surname": "Durán-Rosal",
        "given_name": "Antonio Manuel"
      }
    ]
  },
  {
    "title": "Steganogram removal using multidirectional diffusion in fourier domain while preserving perceptual image quality",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.04.026",
    "abstract": "This paper proposes a novel and efficient method for destructing any secret information hidden (steganograms) inside an image by any steganographic technique while preserving the visual quality of the images as well. The method involves an iterative multi-directional diffusion process in the Fourier domain that disrupts the stego content present in the image until the visual quality of the image does not drop below the desired threshold. Most importantly, our presented method is universal and blind and does not entail any knowledge about the employed steganography methods, the hidden message or the cover image. Simulations ran on 12600 images created by eight different state-of-the-art steganography algorithms prove that our technique succeeded in erasing from 80% to 95% of stego content on average and is superior to other similar systems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521001653",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Cover (algebra)",
      "Domain (mathematical analysis)",
      "Engineering",
      "Fourier transform",
      "Frequency domain",
      "Human visual system model",
      "Image (mathematics)",
      "Image quality",
      "Information hiding",
      "Mathematical analysis",
      "Mathematics",
      "Mechanical engineering",
      "Operating system",
      "Pattern recognition (psychology)",
      "Process (computing)",
      "Steganography"
    ],
    "authors": [
      {
        "surname": "Geetha",
        "given_name": "S."
      },
      {
        "surname": "Subburam",
        "given_name": "S."
      },
      {
        "surname": "Selvakumar",
        "given_name": "S."
      },
      {
        "surname": "Kadry",
        "given_name": "Seifedine"
      },
      {
        "surname": "Damasevicius",
        "given_name": "Robertas"
      }
    ]
  },
  {
    "title": "Complex common spatial patterns on time-frequency decomposed EEG for brain-computer interface",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107918",
    "abstract": "Motor imagery brain-computer interface (MI-BCI) has many promising applications but there are problems such as poor classification accuracy and robustness which need to be addressed. We propose a novel approach called time-frequency common spatial patterns (TFCSP) to enhance the robustness and accuracy of the electroencephalogram (EEG) signal classification. The proposed approach decomposes the EEG signal into time stages and frequency components to find the most robust and discriminative features. Common spatial patterns (CSP) are extracted from every decomposed time-frequency cell and unreliable features are removed while remaining features are weighted and regularized for the classification. Comparison on three publicly available datasets from BCI competition III and IV shows that the proposed TFCSP outperforms state-of-the-art methods. This demonstrates that adopting subject reaction time paradigm is useful to enhance the classification performance. It also shows that the complex CSP in the frequency domain significantly effective than the commonly used bandpass-filters in time domain. Finally, this work proves that weighting and regularizing CSP features are better techniques than selecting the leading CSP features because the former alleviates information loss.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321001059",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Brain–computer interface",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Discriminative model",
      "Electroencephalography",
      "Filter (signal processing)",
      "Frequency domain",
      "Gene",
      "Medicine",
      "Motor imagery",
      "Pattern recognition (psychology)",
      "Psychiatry",
      "Psychology",
      "Radiology",
      "Robustness (evolution)",
      "Speech recognition",
      "Time domain",
      "Time–frequency analysis",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Mishuhina",
        "given_name": "Vasilisa"
      },
      {
        "surname": "Jiang",
        "given_name": "Xudong"
      }
    ]
  },
  {
    "title": "Temporally sorting images from real-world events",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.04.027",
    "abstract": "As smartphones become ubiquitous in modern life, every major event — from musical concerts to terrorist attempts — is massively captured by multiple devices and instantly uploaded to the Internet. Once shared through social media, the chronological order between available media pieces cannot be reliably recovered, hindering the understanding and reconstruction of that event. In this work, we propose data-driven methods for temporally sorting images originated from heterogeneous sources and captured from distinct angles, viewpoints, and moments. We model the chronological sorting task as an ensemble of binary classifiers whose answers are combined hierarchically to estimate an image’s temporal position within the duration of the event. We evaluate our method on images from the Notre-Dame Catedral fire and the Grenfell Tower fire events and discuss research challenges for analyzing data from real-world forensic events. Finally, we employ visualization techniques to understand what our models have learned, offering additional insights to the problem.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521001665",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Data science",
      "Event (particle physics)",
      "Physics",
      "Programming language",
      "Quantum mechanics",
      "Sorting",
      "Upload",
      "Viewpoints",
      "Visual arts",
      "Visualization",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Padilha",
        "given_name": "Rafael"
      },
      {
        "surname": "Andaló",
        "given_name": "Fernanda A."
      },
      {
        "surname": "Lavi",
        "given_name": "Bahram"
      },
      {
        "surname": "Pereira",
        "given_name": "Luís A.M."
      },
      {
        "surname": "Rocha",
        "given_name": "Anderson"
      }
    ]
  },
  {
    "title": "Static postural transition-based technique and efficient feature extraction for sensor-based activity recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.04.001",
    "abstract": "Smartphone sensor-based activity recognition seeks broad, high-level knowledge about human behaviors from multitudes of low-level sensor readings, and makes considerable headway in healthcare domain. Our primary contribution is to study the effective pre-processing technique and the extraction of robust features for the classification of sensor data for human activity recognition (HAR). In the pre-processing stages, we investigated multiple filtering parameters for reducing waveform delay, smartphone orientation constraint by introducing magnitude and jerk-based features, and optimum window length for analyzing the trade-off between model performance and latency. Besides, we proposed a feature named “Average Height” that summarizes the average peak to trough distance of the activity and encodes any change of motion for classification. We also proposed two feature selection techniques for offline and real-time faster activity recognition, and analyzed the impact of different feature sets on classifying different activities. Moreover, after performing the classification with optimized hyperparameters, we proposed a Static Postural Transition-based Post-Processing (SPTPP) technique. This post-processing approach analyzes the existence of postural transition from previous window activity to current window activity, and helps to improve the model output by analyzing the posture change. The impact of our proposed techniques are demonstrated on three benchmark datasets named HASC, HAR, and HAPT, where we obtained the state-of-the-art results. We used HASC dataset for optimizing model parameters in different stages, and explored HAR and HAPT datasets as test-beds to verify our optimizations and postprocessing technique.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521001252",
    "keywords": [
      "Activity recognition",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Feature (linguistics)",
      "Feature extraction",
      "Geodesy",
      "Geography",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Ahmed",
        "given_name": "Masud"
      },
      {
        "surname": "Antar",
        "given_name": "Anindya Das"
      },
      {
        "surname": "Ahad",
        "given_name": "Md Atiqur Rahman"
      }
    ]
  },
  {
    "title": "An end-to-end framework for unconstrained monocular 3D hand pose estimation",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107892",
    "abstract": "This work addresses the challenging problem of unconstrained 3D hand pose estimation using monocular RGB images. Most of the existing approaches assume some prior knowledge of hand (such as hand locations and side information) is available for 3D hand pose estimation. This restricts their use in unconstrained environments. Therefore, we present an end-to-end framework that robustly predicts hand prior information and accurately infers 3D hand pose by learning ConvNet models while only using keypoint annotations. To enhance the hand detector’s robustness, we propose a novel keypoint-based method to simultaneously predict hand regions and side labels, unlike existing methods that suffer from background color confusion caused by using segmentation or detection-based technology. Moreover, inspired by the human hand’s biological structure, we introduce two geometric constraints directly into the 3D coordinates prediction that further improves its performance. Experimental results show that our proposed framework outperforms the state-of-art methods on standard benchmark datasets while providing robust predictions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321000790",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Gene",
      "Geodesy",
      "Geography",
      "Monocular",
      "Pattern recognition (psychology)",
      "Pose",
      "RGB color model",
      "Robustness (evolution)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Sharma",
        "given_name": "Sanjeev"
      },
      {
        "surname": "Huang",
        "given_name": "Shaoli"
      }
    ]
  },
  {
    "title": "3D-CenterNet: 3D object detection network for point clouds with center estimation priority",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107884",
    "abstract": "In this paper, a single-stage 3D object detection framework, 3D-CenterNet, is proposed for accurate 3D object detection from point clouds. We find that the center position is more critical for accurate bounding box detection than the other two parameters, the size and the orientation. Motivated by this discovery, we propose the center regression module (CRM) to regress the centers’ location from the point-wise features. In CRM, the representative points belonging to objects are sampled to regress the center locations of the corresponding objects. The semantic and geometric information related to the estimated centers is aggregated for the following location refinement and other parameters’ estimation. The 3D-CenterNet stacks the CRMs to improve the accuracy of the estimated centers gradually. The size and orientation of the bounding boxes are decoded from the high dimensional center-wise features. The experiments on the KITTI benchmark and the SUN RGB-D datasets show that our proposed 3D-CenterNet achieves high-quality results in real time.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321000716",
    "keywords": [
      "Artificial intelligence",
      "Center (category theory)",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Crystallography",
      "Geometry",
      "Mathematics",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Point (geometry)",
      "Point cloud",
      "Pose",
      "Real-time computing"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Qi"
      },
      {
        "surname": "Chen",
        "given_name": "Jian"
      },
      {
        "surname": "Deng",
        "given_name": "Jianqiang"
      },
      {
        "surname": "Zhang",
        "given_name": "Xinfang"
      }
    ]
  },
  {
    "title": "Pruning by explaining: A novel criterion for deep neural network pruning",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107899",
    "abstract": "The success of convolutional neural networks (CNNs) in various applications is accompanied by a significant increase in computation and parameter storage costs. Recent efforts to reduce these overheads involve pruning and compressing the weights of various layers while at the same time aiming to not sacrifice performance. In this paper, we propose a novel criterion for CNN pruning inspired by neural network interpretability: The most relevant units, i.e. weights or filters, are automatically found using their relevance scores obtained from concepts of explainable AI (XAI). By exploring this idea, we connect the lines of interpretability and model compression research. We show that our proposed method can efficiently prune CNN models in transfer-learning setups in which networks pre-trained on large corpora are adapted to specialized tasks. The method is evaluated on a broad range of computer vision datasets. Notably, our novel criterion is not only competitive or better compared to state-of-the-art pruning criteria when successive retraining is performed, but clearly outperforms these previous criteria in the resource-constrained application scenario in which the data of the task to be transferred to is very scarce and one chooses to refrain from fine-tuning. Our method is able to compress the model iteratively while maintaining or even improving accuracy. At the same time, it has a computational cost in the order of gradient computation and is comparatively simple to apply without the need for tuning hyperparameters for pruning.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321000868",
    "keywords": [
      "Agronomy",
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Composite material",
      "Computation",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Economics",
      "Hyperparameter",
      "Interpretability",
      "Machine learning",
      "Management",
      "Materials science",
      "Pruning",
      "Range (aeronautics)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Yeom",
        "given_name": "Seul-Ki"
      },
      {
        "surname": "Seegerer",
        "given_name": "Philipp"
      },
      {
        "surname": "Lapuschkin",
        "given_name": "Sebastian"
      },
      {
        "surname": "Binder",
        "given_name": "Alexander"
      },
      {
        "surname": "Wiedemann",
        "given_name": "Simon"
      },
      {
        "surname": "Müller",
        "given_name": "Klaus-Robert"
      },
      {
        "surname": "Samek",
        "given_name": "Wojciech"
      }
    ]
  },
  {
    "title": "Stable topological signatures for metric trees through graph approximations",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.03.035",
    "abstract": "The rising field of Topological Data Analysis (TDA) provides a new approach to learning from data through persistence diagrams, which are topological signatures that quantify topological properties of data in a comparable manner. For point clouds, these diagrams are often derived from the Vietoris-Rips filtration—based on the metric equipped on the data—which allows one to deduce topological patterns such as components and cycles of the underlying space. In metric trees these diagrams often fail to capture other crucial topological properties, such as the present leaves and multifurcations. Prior methods and results for persistent homology attempting to overcome this issue mainly target Rips graphs, which are often unfavorable in case of non-uniform density across our point cloud. We therefore introduce a new theoretical foundation for learning a wider variety of topological patterns through any given graph. Given particular powerful functions defining persistence diagrams to summarize topological patterns, including the normalized centrality or eccentricity, we prove a new stability result, explicitly bounding the bottleneck distance between the true and empirical diagrams for metric trees. This bound is tight if the metric distortion obtained through the graph and its maximal edge-weight are small. Through a case study of gene expression data, we demonstrate that our newly introduced diagrams provide novel quality measures and insights into cell trajectory inference.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521001306",
    "keywords": [],
    "authors": [
      {
        "surname": "Vandaele",
        "given_name": "Robin"
      },
      {
        "surname": "Rieck",
        "given_name": "Bastian"
      },
      {
        "surname": "Saeys",
        "given_name": "Yvan"
      },
      {
        "surname": "De Bie",
        "given_name": "Tijl"
      }
    ]
  },
  {
    "title": "Tight lower bounds for dynamic time warping",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107895",
    "abstract": "Dynamic Time Warping ( DTW ) is a popular similarity measure for aligning and comparing time series. Due to DTW ’s high computation time, lower bounds are often employed to screen poor matches. Many alternative lower bounds have been proposed, providing a range of different trade-offs between tightness and computational efficiency. LB _ KEOGH provides a useful trade-off in many applications. Two recent lower bounds, LB _ IMPROVED and LB _ ENHANCED , are substantially tighter than LB _ KEOGH . All three have the same worst case computational complexity—linear with respect to series length and constant with respect to window size. We present four new DTW lower bounds in the same complexity class. LB _ PETITJEAN is substantially tighter than LB _ IMPROVED , with only modest additional computational overhead. LB _ WEBB is more efficient than LB _ IMPROVED , while often providing a tighter bound. LB _ WEBB is always tighter than LB _ KEOGH . The parameter free LB _ WEBB is usually tighter than LB _ ENHANCED . A parameterized variant, LB_Webb_Enhanced, is always tighter than LB _ ENHANCED . A further variant, LB _ WEBB * , is useful for some constrained distance functions. In extensive experiments, LB _ WEBB proves to be very effective for nearest neighbor search.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321000820",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Dynamic time warping",
      "Image warping",
      "Mathematics"
    ],
    "authors": [
      {
        "surname": "Webb",
        "given_name": "Geoffrey I."
      },
      {
        "surname": "Petitjean",
        "given_name": "François"
      }
    ]
  },
  {
    "title": "Deep learning-based apple detection using a suppression mask R-CNN",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.04.022",
    "abstract": "Robotic apple harvesting has received much research attention in the past few years due to growing shortage and rising cost in labor. One key enabling technology towards automated harvesting is accurate and robust apple detection, which poses great challenges as a result of the complex orchard environment that involves varying lighting conditions and foliage/branch occlusions. This letter reports on the development of a novel deep learning-based apple detection framework named Suppression Mask R-CNN. Specifically, we first collect a comprehensive apple orchard dataset for \"Gala\" and \"Blondee\" apples, using a color camera, under different lighting conditions (overcast and front lighting vs. back lighting). We then develop a novel suppression Mask R-CNN for apple detection, in which a suppression branch is added to the standard Mask R-CNN to suppress non-apple features generated by the original network. Comprehensive evaluations are performed, which show that the developed suppression Mask R-CNN network outperforms state-of-the-art models with a higher F1-score of 0.905 and a detection time of 0.25 second per frame on a standard desktop computer.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521001616",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Deep learning",
      "Economic shortage",
      "Frame rate",
      "Government (linguistics)",
      "Horticulture",
      "Key (lock)",
      "Linguistics",
      "Orchard",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Chu",
        "given_name": "Pengyu"
      },
      {
        "surname": "Li",
        "given_name": "Zhaojian"
      },
      {
        "surname": "Lammers",
        "given_name": "Kyle"
      },
      {
        "surname": "Lu",
        "given_name": "Renfu"
      },
      {
        "surname": "Liu",
        "given_name": "Xiaoming"
      }
    ]
  },
  {
    "title": "Scalable multi-label canonical correlation analysis for cross-modal retrieval",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107905",
    "abstract": "Multi-label canonical correlation analysis (ml-CCA) has been developed for cross-modal retrieval. However, the computation of ml-CCA involves dense matrices eigendecomposition, which can be computationally expensive. In addition, ml-CCA only takes semantic correlation into account which ignores the cross-modal feature correlation. In this paper, we propose a novel framework to simultaneously integrate the semantic correlation and feature correlation for cross-modal retrieval. By using the semantic transformation, we show that our model can avoid computing the covariance matrix explicitly which is a huge save of computational cost. Further analysis shows that our proposed method can be solved via singular value decomposition which has linear time complexity. Experimental results on three multi-label datasets have demonstrated the accuracy and efficiency of our proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321000923",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Canonical correlation",
      "Chemistry",
      "Computation",
      "Computational complexity theory",
      "Computer science",
      "Correlation",
      "Covariance",
      "Covariance matrix",
      "Data mining",
      "Database",
      "Eigendecomposition of a matrix",
      "Eigenvalues and eigenvectors",
      "Feature (linguistics)",
      "Gene",
      "Geometry",
      "Linguistics",
      "Mathematics",
      "Modal",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Polymer chemistry",
      "Quantum mechanics",
      "Scalability",
      "Singular value decomposition",
      "Statistics",
      "Transformation (genetics)"
    ],
    "authors": [
      {
        "surname": "Shu",
        "given_name": "Xin"
      },
      {
        "surname": "Zhao",
        "given_name": "Guoying"
      }
    ]
  },
  {
    "title": "Editorial for the special issue on the DAFNE project (DigitalAnastylosis of Frescoes challeNgE)",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.04.016",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521001550",
    "keywords": [
      "Art",
      "Computer science",
      "Fresco",
      "Painting",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Cantoni",
        "given_name": "Virginio"
      },
      {
        "surname": "Foresti",
        "given_name": "Gian Luca"
      },
      {
        "surname": "Sebe",
        "given_name": "Nicu"
      }
    ]
  },
  {
    "title": "Object recognition in performed basic daily activities with a handcrafted data glove prototype",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.04.017",
    "abstract": "In this paper, special attention is given to the hand. The literature provides solutions allowing the hand gestures recognition and/or object recognition for virtual reality, robotic applications, and so on. These solutions rely mainly on computer vision and data gloves. From this finding, we decided to develop our data glove prototype. The data glove is exploited to recognize common objects in the kitchen that the person can hold (e.g., hold a fork) while he/she performs basic daily activities such as drink a glass of water. The proposed approach is straightforward, cheap ( ∼ 260 $ in USD) and efficient ( ∼ 100%). Moreover, the designed data glove gives easy and direct access to the raw data provided by sensors. Besides, a comparison between classical machine learning algorithms (e.g., CART, Random Forest) and a deep neural network is given. Finally, the proposed prototype is described in a way that researchers can reproduce it for any applications involving the object recognition with the hand.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521001562",
    "keywords": [
      "Artificial intelligence",
      "Computer graphics (images)",
      "Computer science",
      "Computer vision",
      "Engineering",
      "Engineering drawing",
      "Object (grammar)",
      "Virtual reality",
      "Wired glove"
    ],
    "authors": [
      {
        "surname": "Maitre",
        "given_name": "Julien"
      },
      {
        "surname": "Rendu",
        "given_name": "Clément"
      },
      {
        "surname": "Bouchard",
        "given_name": "Kévin"
      },
      {
        "surname": "Bouchard",
        "given_name": "Bruno"
      },
      {
        "surname": "Gaboury",
        "given_name": "Sébastien"
      }
    ]
  },
  {
    "title": "Outfit compatibility prediction with multi-layered feature fusion network",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.04.009",
    "abstract": "Clothing plays a critical role in people’s daily lives, a perfect styling clothing is able to help people to avoid weaknesses and show their personal temperament, however, not everyone is good at styling. Compatibility is the core of styling, however, determining whether a pair of garments are compatible with each other is a challenging styling issue. Years of research have been devoted to fashion compatibility learning, whereas there are still several drawbacks in visual feature detection and compatibility calculation. In this paper, we propose an end-to-end framework to learn the compatibility among tops and bottoms. In order to improve the effects of visual feature extraction, a Multi-layer Non-local Feature Fusion framework (MNLFF) is developed. Feature fusion model is used to combine both high and low-level features, while non-local block is for global feature detection. We compare our technique with the prior state-of-the-art methods in the outfit compatibility prediction task and extensive experiments on existing datasets demonstrate its effectiveness.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521001392",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Chemical engineering",
      "Clothing",
      "Compatibility (geochemistry)",
      "Computer science",
      "Engineering",
      "History"
    ],
    "authors": [
      {
        "surname": "Lu",
        "given_name": "Shufang"
      },
      {
        "surname": "Zhu",
        "given_name": "Xiang"
      },
      {
        "surname": "Wu",
        "given_name": "Yingying"
      },
      {
        "surname": "Wan",
        "given_name": "Xianmei"
      },
      {
        "surname": "Gao",
        "given_name": "Fei"
      }
    ]
  },
  {
    "title": "Quaternionic extended local binary pattern with adaptive structural pyramid pooling for color image representation",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107891",
    "abstract": "This paper proposes a novel feature representation method for color images, namely quaternionic extended local binary pattern (QxLBP) with adaptive structural pyramid pooling (ASPP). First, we propose a QxLBP operator to encode local neighboring information and complementary modulus and phase information in the quaternion domain of color images. In QxLBP, an extended quaternionic representation (EQR) is proposed which introduces an information term into the real part of a quaternion. The resulting EQR enables us to flexibly encode discriminative features and handle multichannel image data. Second, we propose ASPP as a multiresolution pooling way to aggregate local features. Unlike the traditional spatial pyramid pooling which is sensitive to image rotation and spatial changes, ASPP is structure-oriented pooling which can adaptively aggregate the encoded features into multiresolution histogram representations. Experiments on four benchmark image datasets demonstrate that the proposed method achieves the state-of-the-art performance for color texture classification and scene categorization.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321000789",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Color image",
      "Computer science",
      "Computer vision",
      "Discriminative model",
      "ENCODE",
      "Feature (linguistics)",
      "Gene",
      "Geometry",
      "Histogram",
      "Image (mathematics)",
      "Image processing",
      "Law",
      "Linguistics",
      "Local binary patterns",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Pooling",
      "Pyramid (geometry)",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Song",
        "given_name": "Tiecheng"
      },
      {
        "surname": "Xin",
        "given_name": "Liangliang"
      },
      {
        "surname": "Gao",
        "given_name": "Chenqiang"
      },
      {
        "surname": "Zhang",
        "given_name": "Tianqi"
      },
      {
        "surname": "Huang",
        "given_name": "Yao"
      }
    ]
  },
  {
    "title": "Unified unsupervised and semi-supervised domain adaptation network for cross-scenario face anti-spoofing",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107888",
    "abstract": "Due to the environmental differences, many face anti-spoofing methods fail to generalize to unseen scenarios. In light of this, we propose a unified unsupervised and semi-supervised domain adaptation network (USDAN) for cross-scenario face anti-spoofing, aiming at minimizing the distribution discrepancy between the source and the target domains. Specifically, two modules, i.e., marginal distribution alignment module (MDA) and conditional distribution alignment module (CDA), are designed to seek a domain-invariant feature space via adversarial learning and make the features of the same class compact, respectively. By adding/removing the CDA module, the network can be easily switched for semi-supervised/unsupervised setting, in which sense our method is named with “unified”. Moreover, the adaptive cross-entropy loss and normalization techniques are further incorporated to improve the generalization. Extensive experimental results show that the proposed USDAN outperforms state-of-the-art methods on several public datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321000753",
    "keywords": [
      "Adaptation (eye)",
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Computer security",
      "Domain (mathematical analysis)",
      "Domain adaptation",
      "Face (sociological concept)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Psychology",
      "Social science",
      "Sociology",
      "Spoofing attack"
    ],
    "authors": [
      {
        "surname": "Jia",
        "given_name": "Yunpei"
      },
      {
        "surname": "Zhang",
        "given_name": "Jie"
      },
      {
        "surname": "Shan",
        "given_name": "Shiguang"
      },
      {
        "surname": "Chen",
        "given_name": "Xilin"
      }
    ]
  },
  {
    "title": "Multimodal fusion for indoor sound source localization",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107906",
    "abstract": "To identify the localization of indoor sound source, especially when attempted using only a single microphone, it is a challenging problem to machine learning. To address these issues, this paper presents a distinct novel solution based on fusing visual and acoustic models. Therefore, we propose two novel approaches. First, to estimate orientation of vocal object in a stable manner, we employ the visual approach as estimation model, where we develop a robust image feature representation method that adopts Fourier analysis to efficiently extract polar descriptors. Second the distance information is estimated by calculating the signal difference between transmit receive ends. To implement these, we use phoneme-level hidden Markov models (HMMs) extracted from clean speech sound, to estimate the acoustic transfer function (ATF), which can capture the speech signal as a network of phoneme HMMs. And using the separated frame sequences of the ATF, we can indicate the signal difference between two positions, which can be used to estimate the distance of sound source. Experimental results show that the proposed method can simultaneously extract the sound source parameters of direction and distance, and thus improves the verification task of sound source localization.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321000935",
    "keywords": [
      "Acoustic source localization",
      "Acoustics",
      "Artificial intelligence",
      "Audio signal",
      "Computer science",
      "Computer vision",
      "Frame (networking)",
      "Hidden Markov model",
      "Microphone",
      "Pattern recognition (psychology)",
      "Physics",
      "Programming language",
      "SIGNAL (programming language)",
      "Sound (geography)",
      "Sound pressure",
      "Speech coding",
      "Speech recognition",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Jinhui"
      },
      {
        "surname": "Takashima",
        "given_name": "Ryoichi"
      },
      {
        "surname": "Guo",
        "given_name": "Xingchen"
      },
      {
        "surname": "Zhang",
        "given_name": "Zhihong"
      },
      {
        "surname": "Xu",
        "given_name": "Xuexin"
      },
      {
        "surname": "Takiguchi",
        "given_name": "Tetsuya"
      },
      {
        "surname": "Hancock",
        "given_name": "Edwin R."
      }
    ]
  },
  {
    "title": "Graph characterisation using graphlet-based entropies",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.03.031",
    "abstract": "In this paper, we present a general framework to estimate the network entropy that is represented by means of an undirected graph and subsequently employ this framework for graph classification tasks. The proposed framework is based on local information functionals which are defined using induced connected subgraphs of different sizes. These induced subgraphs are termed graphlets. Specifically, we extract the set of all graphlets of a specific sizes and compute the graph entropy using our proposed framework. To classify the network into different categories, we construct a feature vector whose components are obtained by computing entropies of different graphlet sizes. We apply the proposed framework to two different tasks, namely view-based object recognition and biomedical datasets with binary outcomes classification. Finally, we report and compare the classification accuracies of the proposed method and compare against some of the state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521001240",
    "keywords": [
      "Arithmetic",
      "Artificial intelligence",
      "Binary classification",
      "Binary number",
      "Computer science",
      "Data mining",
      "Entropy (arrow of time)",
      "Graph",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Support vector machine",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Aziz",
        "given_name": "Furqan"
      },
      {
        "surname": "Akbar",
        "given_name": "Mian Saeed"
      },
      {
        "surname": "Jawad",
        "given_name": "Muhammad"
      },
      {
        "surname": "Malik",
        "given_name": "Abdul Haseeb"
      },
      {
        "surname": "Uddin",
        "given_name": "M. Irfan"
      },
      {
        "surname": "Gkoutos",
        "given_name": "Georgios V."
      }
    ]
  },
  {
    "title": "Mean-shift outlier detection and filtering",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107874",
    "abstract": "Traditional outlier detection methods create a model for data and then label as outliers for objects that deviate significantly from this model. However, when dat has many outliers, outliers also pollute the model. The model then becomes unreliable, thus rendering most outlier detectors to become ineffective. To solve this problem, we propose a mean-shift outlier detector. This detector employs a mean-shift technique to modify data and cancel the bias caused by the outliers. The mean-shift technique replaces every object by the mean of its k-nearest neighbors which essentially removes the effect of outliers before clustering without the need to know the outliers. In addition, it also detects outliers based on the distance shifted. Our experiments show that the proposed method works well regardless of the number of outliers in the data. This method outperforms all state-of-the-art methods tested, with both real-world numeric datasets as well as generated numeric and string datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321000613",
    "keywords": [
      "Anomaly detection",
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Detector",
      "Local outlier factor",
      "Mean-shift",
      "Outlier",
      "Pattern recognition (psychology)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Jiawei"
      },
      {
        "surname": "Rahardja",
        "given_name": "Susanto"
      },
      {
        "surname": "Fränti",
        "given_name": "Pasi"
      }
    ]
  },
  {
    "title": "Weakly-supervised semantic segmentation with saliency and incremental supervision updating",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107858",
    "abstract": "Weakly-supervised semantic segmentation aims at tackling the dense labeling task using weak supervision so as to reduce human annotation efforts. For weakly-supervised semantic segmentation using only image-level annotation, we propose a novel model of Learning with Saliency and Incremental Supervision Updating (LSISU), in which both the guidances of saliency prior and class information are jointly used and the segmentation supervision is dynamically updated. In the proposed LSISU, we present an image saliency objective complementary to classification loss, by which the trained weakly-supervised deep network can effectively deal with object co-occurrence problem. Meanwhile, we make full use of the class-wise pooling strategy to generate initial mask estimation of high quality. Given an initial annotation, a segmentation network is learned along with incremental supervision updating, which plays a role of region expansion and corrects the falsely estimated supervision for training images. The incremental supervision updating is performed on the fly and involves repeated usage of a fully connected conditional random field algorithm. LSISU achieves superior segmentation performance in terms of mIoU metric on benchmark datasets, which are 62.5% on the PASCAL VOC 2012 test set and 30.1% on the COCO val set.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321000455",
    "keywords": [
      "Annotation",
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Class (philosophy)",
      "Computer science",
      "Conditional random field",
      "Economics",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Metric (unit)",
      "Operations management",
      "Pascal (unit)",
      "Pattern recognition (psychology)",
      "Pooling",
      "Programming language",
      "Segmentation",
      "Set (abstract data type)",
      "Supervised learning",
      "Test set"
    ],
    "authors": [
      {
        "surname": "Luo",
        "given_name": "Wenfeng"
      },
      {
        "surname": "Yang",
        "given_name": "Meng"
      },
      {
        "surname": "Zheng",
        "given_name": "Weishi"
      }
    ]
  },
  {
    "title": "Jointly learning compact multi-view hash codes for few-shot FKP recognition",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107894",
    "abstract": "As a relatively new biometric trait, Finger-Knuckle-Print (FKP) plays a vital role in establishing a personal authentication system in modern society due to its rich discriminative features, low time cost in image capture and user-friendliness. However, most existing KFP descriptors are hand-crafted and fail to work well with limited training samples. In this paper, we propose a feature learning method for few-shot FKP recognition by jointly learning compact multi-view hash codes (JLCMHC) of a FKP image. We first form the multi-view data vectors (MVDV) to exploit the multiple feature-specific information from a FKP image. Then, we learn a feature projection to encode the MVDV into compact binary codes in an unsupervised manner, where 1) the variance of the learned feature codes on each view is maximized and 2) the difference of the inter-view binary codes is enlarged, so that the redundant information in MVDV is reduced and more informative features can be obtained. Lastly, we pool the binary codes into block-wise statistics features as the final descriptor for FKP representation and recognition. Experimental results on the existing benchmark FKP databases clearly show that the JLCMHC method outperforms the state-of-the-art FKP descriptors.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321000819",
    "keywords": [
      "Arithmetic",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Binary code",
      "Binary number",
      "Biometrics",
      "Block (permutation group theory)",
      "Computer science",
      "Computer security",
      "Discriminative model",
      "Feature (linguistics)",
      "Feature extraction",
      "Feature learning",
      "Geodesy",
      "Geography",
      "Geometry",
      "Hash function",
      "Histogram",
      "Image (mathematics)",
      "Linguistics",
      "Local binary patterns",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Fei",
        "given_name": "Lunke"
      },
      {
        "surname": "Zhang",
        "given_name": "Bob"
      },
      {
        "surname": "Wen",
        "given_name": "Jie"
      },
      {
        "surname": "Teng",
        "given_name": "Shaohua"
      },
      {
        "surname": "Li",
        "given_name": "Shuyi"
      },
      {
        "surname": "Zhang",
        "given_name": "David"
      }
    ]
  },
  {
    "title": "Robust and discriminative image representation: fractional-order Jacobi-Fourier moments",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107898",
    "abstract": "Robust and discriminative image representation is a long-lasting battle in the computer vision and pattern recognition. Moment-based image representation model is effective in satisfying the core conditions of semantic description, due to its geometric invariance and independence. However, moment-based descriptors suffer from a contradiction between the robustness and discriminability, which limits the further improvement of description quality. In this paper, a set of generic moments along with a novel representation framework are proposed to mitigate this troublesome contradiction. We first define a new set of orthogonal moments, named Fractional-order Jacobi-Fourier Moments (FJFM), which is characterized by the generic nature and time-frequency analysis capability. We then develop a new framework to improve both the robustness and discriminability of image representation, called Mixed Low-order Moment Feature (MLMF), by fully exploiting the time-frequency property of FJFM. Extensive experimental results and a real-world application are provided to demonstrate the superior performance of our FJFM-based MLMF, with respect to robustness and discriminability.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321000856",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Classical mechanics",
      "Computer science",
      "Discriminative model",
      "Fourier transform",
      "Gene",
      "Law",
      "Mathematical analysis",
      "Mathematics",
      "Moment (physics)",
      "Pattern recognition (psychology)",
      "Physics",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Hongying"
      },
      {
        "surname": "Qi",
        "given_name": "Shuren"
      },
      {
        "surname": "Tian",
        "given_name": "Jialin"
      },
      {
        "surname": "Niu",
        "given_name": "Panpan"
      },
      {
        "surname": "Wang",
        "given_name": "Xiangyang"
      }
    ]
  },
  {
    "title": "Probabilistic personalised cascade with abstention",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.03.029",
    "abstract": "Cascade learning with abstention and individualised feature selection is a class of models in high demand in personalised medical applications. The cascade consists of sequential classifiers and rejectors, where the classifiers estimate confidence of prediction, and the rejectors evaluate an expected cost-to-go of features not selected yet. The number of models is exponential in the number of features and, therefore, the challenge is to find efficient heuristics for the NP-hard problem. The state-of-the-art is based on complex deep neural networks. We introduce an efficient and robust approach based on a probabilistic graphical model representing a unified probabilistic classifier that can be applied at any stage of a multi-stage sequential model. As for the rejector, we build it on the probabilistic-based neural network that incorporates the very same probabilistic model to treat unobserved feature values. We illustrate the efficiency of the proposed method on several data sets, and compare our results to the state-of-the-art.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521001227",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Cascade",
      "Chemical engineering",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Engineering",
      "Feature selection",
      "Graphical model",
      "Heuristics",
      "Machine learning",
      "Naive Bayes classifier",
      "Operating system",
      "Probabilistic classification",
      "Probabilistic logic",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Shpakova",
        "given_name": "Tatiana"
      },
      {
        "surname": "Sokolovska",
        "given_name": "Nataliya"
      }
    ]
  },
  {
    "title": "FleBiC: Learning classifiers from high-dimensional biomedical data using discriminative biclusters with non-constant patterns",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107900",
    "abstract": "The discovery of discriminative patterns from high-dimensional data offers the possibility to learn from informative subspaces and pattern-centric features, paving the way to associative classifiers. Despite the success achieved by associative classifiers, such as random forests or XGBoost, they generally neglect discriminative subspaces with non-constant coherencies. Research on biclustering has for two decades highlighted the role of non-constant patterns in biomedical domains, including additive and order-preserving patterns. Still, their relevance for classification remains unexplored. This work assesses the impact of discriminative patterns with varying coherence and quality on associative classification. A novel classifier, FleBiC, is proposed as a result. FleBiC extends pattern-based biclustering with principles to match observations against non-constant and noise-tolerant patterns, address generalization difficulties, minimize scarcity of matches, support class disjunctions, and offer statistical guarantees. Results on biological and clinical data highlight the role of non-constant patterns, specially order-preserving patterns, for improving the performance of state-of-the-art classifiers.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032100087X",
    "keywords": [
      "Artificial intelligence",
      "Associative property",
      "Biclustering",
      "Boosting (machine learning)",
      "CURE data clustering algorithm",
      "Classifier (UML)",
      "Cluster analysis",
      "Computer science",
      "Correlation clustering",
      "Data mining",
      "Discriminative model",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pure mathematics"
    ],
    "authors": [
      {
        "surname": "Henriques",
        "given_name": "Rui"
      },
      {
        "surname": "Madeira",
        "given_name": "Sara C."
      }
    ]
  },
  {
    "title": "A comparative study on handcrafted features v/s deep features for open-set fingerprint liveness detection",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.03.032",
    "abstract": "A fingerprint liveness detector is a pattern classifier that is used to distinguish a live finger from a fake (spoof) one in the context of an automated fingerprint recognition system. As liveness detectors or presentation attack detectors are vulnerable to presentation attacks, the security and reliability of fingerprint recognition are compromised. Presentation attack detection mechanisms rely on handcrafted or deep features to classify an image as live or spoof. In addition, to strengthen the security, fingerprint liveness detectors should be robust to presentation attacks fabricated using unknown fabrication materials or fingerprint sensors. In this paper, we conduct a comprehensive study on the impact of handcrafted and deep features from fingerprint images on the classification error rate of the fingerprint liveness detection task. We use LBP, LPQ and BSIF as handcrafted features and VGG-19 and Residual CNN as deep feature extractors for this study. As the problem is targeted as an open-set problem, the emphasis is on achieving better robustness and generalization capability. In our observation, handcrafted features outperformed their deep counterparts in two of the three cases under the within-dataset environment. In the cross-sensor environment, deep features obtained a better accuracy, and in the cross-dataset environment, handcrafted features obtained a lower classification error rate.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521001276",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Classifier (UML)",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Discrete mathematics",
      "Fingerprint (computing)",
      "Fingerprint recognition",
      "Gene",
      "Liveness",
      "Mathematics",
      "Open set",
      "Pattern recognition (psychology)",
      "Programming language",
      "Robustness (evolution)",
      "Test set",
      "Word error rate"
    ],
    "authors": [
      {
        "surname": "Agarwal",
        "given_name": "Shivang"
      },
      {
        "surname": "Rattani",
        "given_name": "Ajita"
      },
      {
        "surname": "Chowdary",
        "given_name": "C. Ravindranath"
      }
    ]
  },
  {
    "title": "On the channel density of EEG signals for reliable biometric recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.04.003",
    "abstract": "Electroencephalography (EEG) provides appealing biometrics by encompassing unique attributes including robustness against forgery, privacy compliance, and aliveness detection. Among the main challenges in deploying EEG biometric systems in real-world applications, stability and usability are two important ones. They respectively reflect the capacity of the system to provide stable performance within and across different states, and the ease of use of the system. Previous studies indicate that the usability of an EEG biometric system is largely affected by the number of electrodes and reducing channel density is an effective way to enhance usability. However, it is still unclear what is the impact of channel density on recognition performance and stability. This study examines this issue for systems using different feature extraction and classification methods. Our results reveal a trade-off between channel density and stability. With low-density EEG, the recognition accuracy and stability are compromised to varying degrees. Based on the analysis, we propose a framework that integrates channel density augmentation, functional connectivity estimation and deep learning models for practical and stable EEG biometric systems. The framework helps to improve the stability of EEG biometric systems that use consumer-grade low channel density devices, while retaining the advantages of high usability.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552100132X",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Biometrics",
      "Channel (broadcasting)",
      "Chemistry",
      "Computer science",
      "Electroencephalography",
      "Feature extraction",
      "Gene",
      "Human–computer interaction",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Psychiatry",
      "Psychology",
      "Robustness (evolution)",
      "Speech recognition",
      "Stability (learning theory)",
      "Telecommunications",
      "Usability"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Min"
      },
      {
        "surname": "Kasmarik",
        "given_name": "Kathryn"
      },
      {
        "surname": "Bezerianos",
        "given_name": "Anastasios"
      },
      {
        "surname": "Tan",
        "given_name": "Kay Chen"
      },
      {
        "surname": "Abbass",
        "given_name": "Hussein"
      }
    ]
  },
  {
    "title": "Multivariate time series clustering based on complex network",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107919",
    "abstract": "Recent years have seen an increase in research on time series data mining (especially time-series clustering) owing to the widespread existence of time series in various fields. Techniques such as clustering can extract valuable information and potential patterns from time-series data. In this regard, the clustering analysis of multivariate time series is challenging because of the high dimensionality. Our study led us to develop a novel method based on complex networks for multivariate time series clustering (BCNC). BCNC includes a new method for mapping multivariate time series into complex networks and a new method to visualize multivariate time series. The solution is innovatively based on a relationship network and relies on the use of community detection technology to achieve complete multivariate time series clustering. The detailed algorithm and the simulation experiments of the proposed BCNC method are reported. The experimental results on various datasets show that BCNC is superior to traditional multivariate time series clustering methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321001060",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Cluster analysis",
      "Computer science",
      "Curse of dimensionality",
      "Data mining",
      "Hierarchical clustering",
      "Machine learning",
      "Multivariate statistics",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Series (stratigraphy)",
      "Time series"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Hailin"
      },
      {
        "surname": "Liu",
        "given_name": "Zechen"
      }
    ]
  },
  {
    "title": "Few-shot prototype alignment regularization network for document image layout segementation",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107882",
    "abstract": "Despite the great performance in layout analysis tasks made by semantic segmentation, they usually need a large number of annotated images for training and are difficult to learn a new category which is absent in the training categories. Meta-learning and few-shot segmentation have been developed to solve the above two difficulties. In this paper, we propose a novel method dubbed Few-Shot Prototype Alignment Regularization Network (FS-PARN). The FS-PARN method is inspired by recent studies in both metric learning and few-shot segmentation, which just need a few annotated images to solve the above two difficulties. Our FS-PARN method can make better use of the information of the support set by metric learning and have a better effect on image segmentation. It learns classification prototype within an embedding space and then completes pixel classification by matching each pixel on the query image with the learned prototype. In addition to obtaining high-quality prototypes through metric learning methods, our FS-PARN method also introduces prototype alignment regularization between support and query sets to make segmentation better. Notably, our FS-PARN model achieves the mean-IoU score of 28.8% and 31.7% on the practical document image datasets, i.e. PASCAL-5i, DSSE-200, and Layout Analysis Dataset, for 1-shot and 5-shot settings respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321000698",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Economics",
      "Embedding",
      "Metric (unit)",
      "Operations management",
      "Pascal (unit)",
      "Pattern recognition (psychology)",
      "Pixel",
      "Programming language",
      "Regularization (linguistics)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Yujie"
      },
      {
        "surname": "Zhang",
        "given_name": "Pengfei"
      },
      {
        "surname": "Xu",
        "given_name": "Xing"
      },
      {
        "surname": "Lai",
        "given_name": "Yi"
      },
      {
        "surname": "Shen",
        "given_name": "Fumin"
      },
      {
        "surname": "Chen",
        "given_name": "Lijiang"
      },
      {
        "surname": "Gao",
        "given_name": "Pengxiang"
      }
    ]
  },
  {
    "title": "Simplified Face Quality Assessment (SFQA)",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.03.037",
    "abstract": "Face quality assessment has grown into a necessary pre-requisite for better performance of the face recognition (FR) algorithm pipeline. In this work, we proposed a novel face quality assessment algorithm for face recognition algorithms. We named it simplified face quality assessment (SFQA) for its simple provision to be used in accompaniment with any FR algorithm. We first proposed a hashing based deep learning model for the prediction of face quality from the features of the corresponding FR algorithm. The last layer of the deep hash net gives binary bits as output which is then converted to the decimal value to get the face quality score (FQS) ranging between 0 and 100. The mutual quality of the probe and gallery images are mathematically clubbed to get the Face Quality Confidence Score (FQCS). We have experimentally shown the effects of FQCS on the recognition algorithm. We trained the prediction model using the FR algorithm for the quality estimation on the face images from CAS-PEAL, LFW, and QDF database. The performance of SFQA is found to outperform the state-of-the-art methods, as discussed in the paper.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521001331",
    "keywords": [
      "Algorithm",
      "Arithmetic",
      "Artificial intelligence",
      "Binary classification",
      "Binary number",
      "Computer science",
      "Computer security",
      "Epistemology",
      "Face (sociological concept)",
      "Facial recognition system",
      "Hash function",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pipeline (software)",
      "Programming language",
      "Quality (philosophy)",
      "Social science",
      "Sociology",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Bhattacharya",
        "given_name": "Shubhobrata"
      },
      {
        "surname": "Kyal",
        "given_name": "Chirag"
      },
      {
        "surname": "Routray",
        "given_name": "Aurobinda"
      }
    ]
  },
  {
    "title": "Adversarial attacks through architectures and spectra in face recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.04.004",
    "abstract": "The ability of Deep Neural Networks (DNNs) to make fast predictions with high accuracy made them very popular in real-time applications. DNNs are nowadays in use for secure access to services or mobile devices. However, as DNNs use increased, at the same time attack techniques are born to “break” them. This paper presents a particular way to fool DNNs by moving from one spectrum to another one. The application field we explore is face recognition. The attack is first built on a trained Face DNN on Visible, Near Infrared or Thermal images, then transposed to another spectrum to fool another DNN. The attacks performed are based on the Fast Gradient Sign Method with the aim to misclassify the subject knowing the DNN to attack (White-Box Attack) but without knowing the DNN on which the attack will be transposed (Black-Box Attack). Results show that this cross-spectral attack is able to fool the most popular DNN architectures. In worst cases the DNN becomes useless to perform face recognition after the attack.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521001343",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Deep neural networks",
      "Face (sociological concept)",
      "Facial recognition system",
      "Field (mathematics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pure mathematics",
      "Social science",
      "Sociology",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Bisogni",
        "given_name": "Carmen"
      },
      {
        "surname": "Cascone",
        "given_name": "Lucia"
      },
      {
        "surname": "Dugelay",
        "given_name": "Jean-Luc"
      },
      {
        "surname": "Pero",
        "given_name": "Chiara"
      }
    ]
  },
  {
    "title": "LCU-Net: A novel low-cost U-Net for environmental microorganism image segmentation",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107885",
    "abstract": "In this paper, we propose a novel Low-cost U-Net (LCU-Net) for the Environmental Microorganism (EM) image segmentation task to assist microbiologists in detecting and identifying EMs more effectively. The LCU-Net is an improved Convolutional Neural Network (CNN) based on U-Net, Inception, and concatenate operations. It addresses the limitation of single receptive field setting and the relatively high memory cost of U-Net. Experimental results show the effectiveness and potential of the proposed LCU-Net in the practical EM image segmentation field.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321000728",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Engineering",
      "Field (mathematics)",
      "Geometry",
      "Image (mathematics)",
      "Image segmentation",
      "Mathematics",
      "Net (polyhedron)",
      "Pattern recognition (psychology)",
      "Pure mathematics",
      "Segmentation",
      "Systems engineering",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Jinghua"
      },
      {
        "surname": "Li",
        "given_name": "Chen"
      },
      {
        "surname": "Kosov",
        "given_name": "Sergey"
      },
      {
        "surname": "Grzegorzek",
        "given_name": "Marcin"
      },
      {
        "surname": "Shirahama",
        "given_name": "Kimiaki"
      },
      {
        "surname": "Jiang",
        "given_name": "Tao"
      },
      {
        "surname": "Sun",
        "given_name": "Changhao"
      },
      {
        "surname": "Li",
        "given_name": "Zihan"
      },
      {
        "surname": "Li",
        "given_name": "Hong"
      }
    ]
  },
  {
    "title": "Capturing causality and bias in human action recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.04.008",
    "abstract": "Human action recognition using various sensors is a mandatory component of autonomous vehicles, humanoid robots, and ambient living environments. A particular interest is the detection and recognition of falls. In this paper, we propose the use of temporal convolution networks guided by knowledge distillation for detecting falls and recognizing types of falls using accelerometer data. Tri-axial accelerometers attached to the body measure the acceleration of the body joints when an action occurs. These data are used for pattern analysis and body action recognition. We demonstrate the existence of biases caused by soft biometrics when recognizing human body actions. We introduce a causal network to capture the influences of biases on system performance and illustrate how knowledge distillation can be applied to mitigate the bias effect.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521001380",
    "keywords": [
      "Acceleration",
      "Accelerometer",
      "Action (physics)",
      "Action recognition",
      "Artificial intelligence",
      "Artificial neural network",
      "Biometrics",
      "Causality (physics)",
      "Class (philosophy)",
      "Classical mechanics",
      "Computer science",
      "Computer vision",
      "Convolution (computer science)",
      "Humanoid robot",
      "Human–computer interaction",
      "Machine learning",
      "Operating system",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Robot"
    ],
    "authors": [
      {
        "surname": "Lai",
        "given_name": "Kenneth"
      },
      {
        "surname": "Yanushkevich",
        "given_name": "Svetlana N."
      },
      {
        "surname": "Shmerko",
        "given_name": "Vlad"
      },
      {
        "surname": "Hou",
        "given_name": "Ming"
      }
    ]
  },
  {
    "title": "Aberrance suppressed spatio-temporal correlation filters for visual object tracking",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107922",
    "abstract": "The objective of the present study is to design a correlation filter-based tracking method for robust visual object tracking. In the literature, numerous tracking methods have been proposed based on discriminative correlation filter (DCF) and obtained impressive performance. However, existing algorithms still face difficulties such as partial occlusion, clutter background, uncertainties, boundary effects (especially when the target search area is small) and other challenging visual factors. Furthermore, during the target detection process, the sudden changes in objects caused by illumination variations and partial/full occlusion degrade the performance. To tackle the drawbacks mentioned earlier, we propose a tracking algorithm concerning the aberrance suppressed correlation filters with spatio-temporal information for visual tracking. Specifically, we introduce a spatial regularization term into the correlation filter to suppresses the boundary effects. Following that, a temporal regularization is adopted into the DCF-based framework to achieve a more robust appearance model and further enhance the tracking performance. In addition, we introduce an approach to suppress the aberrance in response maps caused by the sudden changes. Technically, our proposed method can be directly solved by using the alternating direction method of multipliers (ADMM) technique with a low computational cost. Finally, extensive experimental results on OTB2013, OTB2015, TempleColor128 and UAV123 datasets demonstrate that the proposed method performs favorably against state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321001096",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Clutter",
      "Computer science",
      "Computer vision",
      "Correlation",
      "Discriminative model",
      "Eye tracking",
      "Filter (signal processing)",
      "Gene",
      "Geometry",
      "Mathematics",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Pedagogy",
      "Psychology",
      "Radar",
      "Regularization (linguistics)",
      "Robustness (evolution)",
      "Telecommunications",
      "Tracking (education)",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Elayaperumal",
        "given_name": "Dinesh"
      },
      {
        "surname": "Joo",
        "given_name": "Young Hoon"
      }
    ]
  },
  {
    "title": "Reading grid for feature selection relevance criteria in regression",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.04.031",
    "abstract": "Feature selection is an important preprocessing step in machine learning. It helps to better understand the importance of some features and to reduce the dimensionality of a dataset, which improves machine learning and information extraction. Among the different existing methods for selecting features, filters are popular because they are independent from the model, which will be learnt afterwards, and computationally efficient. The efficiency of filter methods relies on a strategic choice: the choice of the relevance criterion. Many criteria exist; they exhibit various properties, which in turn result in selecting different features. The choice of the criterion is thus important and should ideally be linked to the properties of the data and to users’ goals. This paper shows that six properties should be analysed when selecting a relevance criterion in the context of regression problems. It proposes a reading grid to analyse relevance criteria and to make a well-guided choice.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521001768",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Curse of dimensionality",
      "Data mining",
      "Feature (linguistics)",
      "Feature selection",
      "Filter (signal processing)",
      "Geometry",
      "Grid",
      "Law",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Preprocessor",
      "Regression",
      "Relevance (law)",
      "Selection (genetic algorithm)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Degeest",
        "given_name": "Alexandra"
      },
      {
        "surname": "Frénay",
        "given_name": "Benoît"
      },
      {
        "surname": "Verleysen",
        "given_name": "Michel"
      }
    ]
  },
  {
    "title": "Vector of Locally and Adaptively Aggregated Descriptors for Image Feature Representation",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107952",
    "abstract": "VLAD (Vector of Locally Aggregated Descriptors) has been widely adopted in image representation. However, the VLAD algorithm seeks for the algebraic sum of the residue vectors between the descriptors and the centroid of cluster they belong to, and this could decrease the discriminative power of feature representations. To this end, this paper originally proposes a VLAAD (Vector of Locally and Adaptively Aggregated Descriptors) framework to adaptively assign a weight to each residue vector. First, we compute the weights using the magnitude of each residue vector, and encapsulate the weighted VLAD block into ResNet to form an end-to-end Weighted NetVLAD method. To further enhance the discriminative power of the features, we subsequently replace the magnitude-based weight computation with a gating scheme to achieve automatic weight estimation. The enhanced version is named as Gated NetVLAD method. The experimental results on CIFAR-10, MNIST Digits, Pittsburgh Google street view and ImageNet-Dog datasets demonstrate the promotion in classification accuracy and retrieval mAP using VLAAD against several state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321001394",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Centroid",
      "Computation",
      "Computer science",
      "Discriminative model",
      "Feature vector",
      "Law",
      "MNIST database",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Jian"
      },
      {
        "surname": "Cao",
        "given_name": "Yunyin"
      },
      {
        "surname": "Wu",
        "given_name": "Qun"
      }
    ]
  },
  {
    "title": "Global motion estimation with iterative optimization-based independent univariate model for action recognition",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107925",
    "abstract": "Motion information used in the existed video action recognition schemes is mixing of global motion(GM) and local motion(LM). In fact, GM & LM have their respective semantic concepts. Thus, it is promising to decouple GM and LM from the mixed motions. Numerous efforts have been made on the design of global motion models for video encoding, video dejittering, video denoising, and so on. Nevertheless, some of the models are too basic to cover the camera motions in action recognition while others are over-complicated. In this paper, we focus on the characteristic of the action recognition and propose a novel independent univariate GM model. It ignores camera rotation, which appears rarely in action recognition videos, and represents the GM in x and y direction respectively. Furthermore, GM is position invariant because it is from the universal camera motion. Pixels with global motions are subjected to the same parametric model and pixels with mixed motion can be seen as outliers. Motivated by this, we develop an iterative optimization scheme for GM estimation which removes the outlier points step by step and estimates global motions in a coarse-to-fine manner. Finally, the LM is estimated through a Spatio-temporal threshold-based method. Experimental results demonstrate that the proposed GM model makes a better trade-off between the model complexity and the robustness. And the iterative optimization scheme is more effective than the existed algorithms. The compared experiments using four popular action recognition models on UCF-101 (for action recognition) and NCAA (for group activity recognition) demonstrate that local motions are more effective than the mixed motions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321001126",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Gene",
      "Machine learning",
      "Mathematics",
      "Motion estimation",
      "Multivariate statistics",
      "Outlier",
      "Parametric statistics",
      "Pattern recognition (psychology)",
      "Pixel",
      "Robustness (evolution)",
      "Statistics",
      "Univariate"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Lifang"
      },
      {
        "surname": "Yang",
        "given_name": "Zhou"
      },
      {
        "surname": "Jian",
        "given_name": "Meng"
      },
      {
        "surname": "Shen",
        "given_name": "Jialie"
      },
      {
        "surname": "Yang",
        "given_name": "Yuchen"
      },
      {
        "surname": "Lang",
        "given_name": "Xianglong"
      }
    ]
  },
  {
    "title": "Compensating over- and underexposure in optical target pose determination",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107930",
    "abstract": "Optical coded targets allow to determine the relative pose of a camera, on a metric scale, from one image only. Furthermore, they are easily and efficiently detected, opening to a wide range of applications in robotics and computer vision. In this work we describe the effect of pixel saturation and non-ideal lens Point Spread Function, causing the apparent position of the corners and the edges of the target to change as a function of the camera exposure time. This effect, which we call exposure bias, is frequent in over- or underexposed images and introduces a systematic error in the estimated camera pose. We propose an algorithm that is able to estimate and correct for the exposure bias exploiting specific geometric features of a common target design based on concentric circles. Through rigorous laboratory experiments carried out in a highly controlled environment, we demonstrate that the proposed algorithm is seven times more precise and three times more accurate in the target distance estimation than the algorithms available in the literature.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321001175",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Economics",
      "Finance",
      "Lens (geology)",
      "Metric (unit)",
      "Operations management",
      "Optics",
      "Physics",
      "Pixel",
      "Pose",
      "Position (finance)",
      "Robot",
      "Robotics"
    ],
    "authors": [
      {
        "surname": "Cledat",
        "given_name": "E."
      },
      {
        "surname": "Rufener",
        "given_name": "M."
      },
      {
        "surname": "Cucci",
        "given_name": "D.A."
      }
    ]
  },
  {
    "title": "Region-based dropout with attention prior for weakly supervised object localization",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107949",
    "abstract": "Weakly supervised object localization (WSOL) methods utilize the internal feature responses of a classifier trained only on image-level labels. Classifiers tend to focus on the most discriminative part of the target object, instead of considering its full extent. Adversarial erasing (AE) techniques have been proposed to ameliorate this problem. These techniques erase the most discriminative part during training, thereby encouraging the classifiers to learn the less discriminative parts of the object. Despite the success of AE-based methods, we have observed that the hyperparameters fail to generalize across model architectures and datasets. Therefore, new sets of hyperparameters must be determined for each architecture and dataset. The selection of hyperparameters frequently requires strong supervision (e.g., pixel-level annotations or human inspection). Because WSOL is premised on the assumption that such strong supervision is absent, the applicability of AE-based methods is limited. In this paper, we propose the region-based dropout with attention prior (RDAP) algorithm, which features hyperparameter transferability. We combined AE with regional dropout algorithms that provide greater stability against the selection of hyperparameters. We empirically confirmed that the RDAP method achieved state-of-the-art localization accuracy on four architectures, namely VGG-GAP, InceptionV3, ResNet-50 SE, and PreResNet-18, and two datasets, namely CUB-200-2011 and ImageNet-1k, with a single set of hyperparameters.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321001369",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Contextual image classification",
      "Discriminative model",
      "Dropout (neural networks)",
      "Hyperparameter",
      "Image (mathematics)",
      "Machine learning",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Choe",
        "given_name": "Junsuk"
      },
      {
        "surname": "Han",
        "given_name": "Dongyoon"
      },
      {
        "surname": "Yun",
        "given_name": "Sangdoo"
      },
      {
        "surname": "Ha",
        "given_name": "Jung-Woo"
      },
      {
        "surname": "Oh",
        "given_name": "Seong Joon"
      },
      {
        "surname": "Shim",
        "given_name": "Hyunjung"
      }
    ]
  },
  {
    "title": "A kernel path algorithm for general parametric quadratic programming problem",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107941",
    "abstract": "It is well known that the performance of a kernel method highly depends on the choice of kernel parameter. A kernel path provides a compact representation of all optimal solutions, which can be used to choose the optimal value of kernel parameter along with cross validation (CV) method. However, none of these existing kernel path algorithms provides a unified implementation to various learning problems. To fill this gap, in this paper, we first study a general parametric quadratic programming (PQP) problem that can be instantiated to an extensive number of learning problems. Then we provide a generalized kernel path (GKP) for the general PQP problem. Furthermore, we analyze the iteration complexity and computational complexity of GKP. Extensive experimental results on various benchmark datasets not only confirm the identity of GKP with several existing kernel path algorithms, but also show that our GKP is superior to the existing kernel path algorithms in terms of generalization and robustness.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032100128X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Discrete mathematics",
      "Gene",
      "Generalization",
      "Kernel (algebra)",
      "Kernel embedding of distributions",
      "Kernel method",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Parametric statistics",
      "Path (computing)",
      "Programming language",
      "Radial basis function kernel",
      "Robustness (evolution)",
      "Statistics",
      "Support vector machine",
      "Variable kernel density estimation"
    ],
    "authors": [
      {
        "surname": "Gu",
        "given_name": "Bin"
      },
      {
        "surname": "Xiong",
        "given_name": "Ziran"
      },
      {
        "surname": "Yu",
        "given_name": "Shuyang"
      },
      {
        "surname": "Zheng",
        "given_name": "Guansheng"
      }
    ]
  },
  {
    "title": "2D Wasserstein loss for robust facial landmark detection",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107945",
    "abstract": "The recent performance of facial landmark detection has been significantly improved by using deep Convolutional Neural Networks (CNNs), especially the Heatmap Regression Models (HRMs). Although their performance on common benchmark datasets has reached a high level, the robustness of these models still remains a challenging problem in the practical use under noisy conditions of realistic environments. Contrary to most existing work focusing on the design of new models, we argue that improving the robustness requires rethinking many other aspects, including the use of datasets, the format of landmark annotation, the evaluation metric as well as the training and detection algorithm itself. In this paper, we propose a novel method for robust facial landmark detection, using a loss function based on the 2D Wasserstein distance combined with a new landmark coordinate sampling relying on the barycenter of the individual probability distributions. Our method can be plugged-and-play on most state-of-the-art HRMs with neither additional complexity nor structural modifications of the models. Further, with the large performance increase, we found that current evaluation metrics can no longer fully reflect the robustness of these models. Therefore, we propose several improvements to the standard evaluation protocol. Extensive experimental results on both traditional evaluation metrics and our evaluation metrics demonstrate that our approach significantly improves the robustness of state-of-the-art facial landmark detection models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321001321",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Convolutional neural network",
      "Economics",
      "Gene",
      "Geodesy",
      "Geography",
      "Landmark",
      "Machine learning",
      "Mathematics",
      "Metric (unit)",
      "Operations management",
      "Pattern recognition (psychology)",
      "Regression",
      "Robustness (evolution)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Yan",
        "given_name": "Yongzhe"
      },
      {
        "surname": "Duffner",
        "given_name": "Stefan"
      },
      {
        "surname": "Phutane",
        "given_name": "Priyanka"
      },
      {
        "surname": "Berthelier",
        "given_name": "Anthony"
      },
      {
        "surname": "Blanc",
        "given_name": "Christophe"
      },
      {
        "surname": "Garcia",
        "given_name": "Christophe"
      },
      {
        "surname": "Chateau",
        "given_name": "Thierry"
      }
    ]
  },
  {
    "title": "A new DCT-PCM method for license plate number detection in drone images",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.05.002",
    "abstract": "License plate number detection in drone images is a complex problem because the images are generally captured at oblique angles and pose several challenges like perspective distortion, non-uniform illumination effect, degradations, blur, occlusion, loss of visibility etc. Unlike, most existing methods that focus on images captured by orthogonal direction (head-on), the proposed work focuses on drone text images. Inspired by the Phase Congruency Model (PCM), which is invariant to non-uniform illuminations, contrast variations, geometric transformation and to some extent to distortion, we explore the combination of DCT and PCM (DCT-PCM) for detecting license plate number text in drone images. Motivated by the strong discriminative power of deep learning models, the proposed method exploits fully connected neural networks for eliminating false positives to achieve better detection results. Furthermore, the proposed work constructs working model that fits for real environment. To evaluate the proposed method, we use our own dataset captured by drones and benchmark license plate datasets, namely, Medialab for experimentation. We also demonstrate the effectiveness of the proposed method on benchmark natural scene text detection datasets, namely, SVT, MSRA-TD-500, ICDAR 2017 MLT and Total-Text.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521001690",
    "keywords": [
      "Amplifier",
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Benchmark (surveying)",
      "Biochemistry",
      "Biology",
      "Chemistry",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Discrete cosine transform",
      "Discriminative model",
      "Distortion (music)",
      "Drone",
      "Gene",
      "Genetics",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Motion blur",
      "Pattern recognition (psychology)",
      "Transformation (genetics)"
    ],
    "authors": [
      {
        "surname": "Mokayed",
        "given_name": "Hamam"
      },
      {
        "surname": "Shivakumara",
        "given_name": "Palaiahnakote"
      },
      {
        "surname": "Woon",
        "given_name": "Hon Hock"
      },
      {
        "surname": "Kankanhalli",
        "given_name": "Mohan"
      },
      {
        "surname": "Lu",
        "given_name": "Tong"
      },
      {
        "surname": "Pal",
        "given_name": "Umapada"
      }
    ]
  },
  {
    "title": "Ring-Regularized Cosine Similarity Learning for Fine-Grained Face Verification",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.04.029",
    "abstract": "Face verification aims to determine whether a pair of face images belong to the same person. Different from the traditional face verification, the negative sample pairs in fine-grained face verification are composed of similar face images, e.g., facial images of twins, which makes it still very challenging. In this paper, we investigate the fine-grained face verification problem via metric learning techniques, and propose a ring-regularized cosine similarity learning (RRCSL) method to distinguish the negative face pairs. The proposed RRCSL method seeks a linear transformation to enlarge the cosine similarity of intra-class and reduce the cosine similarity of inter-class as much as possible, and adaptively learns the norm of samples to the scaled circle by exploiting the ring regularization term simultaneously. Experimental results on three face datasets demonstrate the effectiveness of RRCSL for fine-grained face verification.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521001719",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Cosine similarity",
      "Discrete cosine transform",
      "Economics",
      "Face (sociological concept)",
      "Facial recognition system",
      "Image (mathematics)",
      "Mathematics",
      "Metric (unit)",
      "Operations management",
      "Pattern recognition (psychology)",
      "Regularization (linguistics)",
      "Similarity (geometry)",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Jiawei"
      },
      {
        "surname": "Guo",
        "given_name": "Zhengwei"
      },
      {
        "surname": "Hu",
        "given_name": "Junlin"
      }
    ]
  },
  {
    "title": "Learning deep part-aware embedding for person retrieval",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107938",
    "abstract": "Person retrieval is an important vision task, aiming at matching the images of the same person under various camera views. The key challenge of person retrieval lies in the large intra-class variations among the person images. Therefore, how to learn discriminative feature representations becomes the core problem. In this paper, we propose a deep part-aware representation learning method for person retrieval. First, an improved triplet loss is introduced such that the global feature representations from the same identity are closely clustered. Meanwhile, a localization branch is proposed to automatically localize those discriminative person-wise parts or regions, only using identity labels in a weakly supervised manner. Via the learning simultaneously guided by the global branch and the localization branch, the proposed method can further improve the performance for person retrieval. Through an extensive set of ablation studies, we verify that the localization branch and the improved triplet loss each contributes to the performance boosts of the proposed method. Our model obtains superior (or comparable) performance compared to state-of-the-art methods for person retrieval on the four public person retrieval datasets. On the CUHK03-labeled dataset, for instance, the performance increases from 73.0% mAP and 77.9% rank-1 accuracy to 80.8% (+7.8%) mAP and 83.9% (+6.0%) rank-1 accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321001254",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Discriminative model",
      "Economics",
      "Embedding",
      "Feature (linguistics)",
      "Feature learning",
      "Image (mathematics)",
      "Image retrieval",
      "Linguistics",
      "Management",
      "Matching (statistics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Set (abstract data type)",
      "Statistics",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Yang"
      },
      {
        "surname": "Shen",
        "given_name": "Chunhua"
      },
      {
        "surname": "Yu",
        "given_name": "Xiaohan"
      },
      {
        "surname": "Chen",
        "given_name": "Hao"
      },
      {
        "surname": "Gao",
        "given_name": "Yongsheng"
      },
      {
        "surname": "Xiong",
        "given_name": "Shengwu"
      }
    ]
  },
  {
    "title": "A two-stage hybrid ant colony optimization for high-dimensional feature selection",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107933",
    "abstract": "Ant colony optimization (ACO) is widely used in feature selection owing to its excellent global/local search capabilities and flexible graph representation. However, the current ACO-based feature selection methods are mainly applied to low-dimensional datasets. For thousands of dimensional datasets, the search for the optimal feature subset (OFS) becomes extremely difficult due to the exponential increase of the search space. In this paper, we propose a two-stage hybrid ACO for high-dimensional feature selection (TSHFS-ACO). As an additional stage, it uses the interval strategy to determine the size of OFS for the following OFS search. Compared to the traditional one-stage methods that determine the size of OFS and search for OFS simultaneously, the stage of checking the performance of partial feature number endpoints in advance helps to reduce the complexity of the algorithm and alleviate the algorithm from getting into a local optimum. Moreover, the advanced ACO algorithm embeds the hybrid model, which uses the features’ inherent relevance attributes and the classification performance to guide OFS search. The test results on eleven high-dimensional public datasets show that TSHFS-ACO is suitable for high-dimensional feature selection. The obtained OFS has state-of-the-art performance on most datasets. And compared with other ACO-based feature selection methods, TSHFS-ACO has a shorter running time.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321001205",
    "keywords": [
      "ANT",
      "Ant colony optimization algorithms",
      "Artificial intelligence",
      "Biology",
      "Computer network",
      "Computer science",
      "Feature (linguistics)",
      "Feature selection",
      "Linguistics",
      "Mathematical optimization",
      "Mathematics",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Selection (genetic algorithm)",
      "Stage (stratigraphy)"
    ],
    "authors": [
      {
        "surname": "Ma",
        "given_name": "Wenping"
      },
      {
        "surname": "Zhou",
        "given_name": "Xiaobo"
      },
      {
        "surname": "Zhu",
        "given_name": "Hao"
      },
      {
        "surname": "Li",
        "given_name": "Longwei"
      },
      {
        "surname": "Jiao",
        "given_name": "Licheng"
      }
    ]
  },
  {
    "title": "NM-GAN: Noise-modulated generative adversarial network for video anomaly detection",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107969",
    "abstract": "As an important and challenging task for intelligent video surveillance systems, video anomaly detection is generally referred to as automatic recognition of video frames that contain abnormal targets, behavior or events. Although it has been widely applied in real scenes, anomaly detection remains a challenging task because of the vague definition of anomaly and the lack of the anomaly samples. Inspired by the widespread application of Generative Adversarial Network (GAN), we propose an end-to-end pipeline called NM-GAN which assembles an encode-decoder reconstruction network and a CNN-based discrimination network in a GAN-like architecture. The generalization ability of the reconstruction network is properly modulated via the adversarial learning around reconstruction error maps and noise maps. Meanwhile, the discrimination network is trained to distinguish anomaly samples from normal samples based on the reconstruction error maps. Finally, the output of the discrimination network is transferred to evaluate anomaly score of the input frame. The thorough proof-of-principle experiments and ablation tests on several popular datasets reveal that the proposed model enhance the generalization ability of the reconstruction network and the distinguishability of the discrimination network significantly. The comparison with the state-of-the-art shows that the proposed NM-GAN model outperforms most competing models in precision and stability.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321001564",
    "keywords": [
      "Algorithm",
      "Anomaly (physics)",
      "Anomaly detection",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Computer vision",
      "Condensed matter physics",
      "Decoding methods",
      "Economics",
      "Frame (networking)",
      "Generalization",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Machine learning",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Noise (video)",
      "Pattern recognition (psychology)",
      "Physics",
      "Pipeline (software)",
      "Programming language",
      "Stability (learning theory)",
      "Task (project management)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Dongyue"
      },
      {
        "surname": "Yue",
        "given_name": "Lingyi"
      },
      {
        "surname": "Chang",
        "given_name": "Xingya"
      },
      {
        "surname": "Xu",
        "given_name": "Ming"
      },
      {
        "surname": "Jia",
        "given_name": "Tong"
      }
    ]
  },
  {
    "title": "An ensemble-based semi-supervised feature ranking for multi-target regression problems",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.04.025",
    "abstract": "This study focuses on semi-supervised feature ranking (FR) applications for multi-target regression problems (MTR). As MTRs require prediction of several targets, we use a learning model that includes target interrelations via multi-objective trees. In processing the features for a semi-supervised learning model, transformation or scaling operations are usually required. To resolve this issue, we create a dissimilarity matrix via totally randomized trees to process the unsupervised information. Besides, we treat the split score function as a vector to make it suitable for considering each criterion regardless of their scales. We propose a semi-supervised FR scheme embedded to multi-objective trees that takes into account target and feature contributions simultaneously. Proposed FR score is compared with the state-of-the-art multi-target FR strategies via statistical analyses. Experimental studies show that proposed score significantly improves the performance of a recent tree-based and competitive multi-target learning model, i.e. predictive clustering trees. In addition, proposed approach outperforms its benchmarks when the available labelled data increase.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521001641",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Feature (linguistics)",
      "Gene",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Ranking (information retrieval)",
      "Regression",
      "Statistics",
      "Supervised learning",
      "Transformation (genetics)",
      "Tree (set theory)"
    ],
    "authors": [
      {
        "surname": "Adıyeke",
        "given_name": "Esra"
      },
      {
        "surname": "Baydoğan",
        "given_name": "Mustafa Gökçe"
      }
    ]
  },
  {
    "title": "PILS: Exploring high-order neighborhoods by pattern mining and injection",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107957",
    "abstract": "We introduce pattern injection local search (PILS), an optimization strategy that uses pattern mining to explore high-order local-search neighborhoods, and illustrate its application on the vehicle routing problem. PILS operates by storing a limited number of frequent patterns from elite solutions. During the local search, each pattern is used to define one move in which 1) incompatible edges are disconnected, 2) the edges defined by the pattern are reconnected, and 3) the remaining solution fragments are optimally reconnected. Each such move is accepted only in case of solution improvement. As visible in our experiments, this strategy results in a new paradigm of local search, which complements and enhances classical search approaches in a controllable amount of computational time. We demonstrate that PILS identifies useful high-order moves that would otherwise not be found by enumeration, and that it significantly improves the performance of state-of-the-art population-based and neighborhood-centered metaheuristics.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321001448",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Business",
      "Combinatorics",
      "Computer science",
      "Demography",
      "Enumeration",
      "Finance",
      "Local search (optimization)",
      "Mathematical optimization",
      "Mathematics",
      "Metaheuristic",
      "Order (exchange)",
      "Population",
      "Sociology",
      "State (computer science)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Arnold",
        "given_name": "Florian"
      },
      {
        "surname": "Santana",
        "given_name": "Ítalo"
      },
      {
        "surname": "Sörensen",
        "given_name": "Kenneth"
      },
      {
        "surname": "Vidal",
        "given_name": "Thibaut"
      }
    ]
  },
  {
    "title": "Signless-laplacian eigenvector centrality: A novel vital nodes identification method for complex networks",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.04.018",
    "abstract": "Identifying important and influential nodes in complex networks is crucial in understanding, controlling, accelerating or terminating spreading processes for information, diseases, innovations, behaviors, and so on. Many existing centrality methods evaluate a node’s importance (or centrality) according to its neighbors, but the effects of its incident edges are always ignored or treated equally. However, in reality, edges always play different roles, which are usually measured by the edge centrality. Note that the centrality of a vertex is affected by the centralities of its incident edges, and conversely the centrality of an edge is determined by the centralities of its two endpoints. In this paper, we present a novel way to evaluate the centrality for both nodes and edges simultaneously by constructing a mutually updated iterative framework. Furthermore, we will prove that the node centralities obtained by this framework are actually the principal eigenvector of the signless-laplacian matrix of the input network, thus we call this new node centrality method as signless-laplacian eigenvector centrality method. We test it on several classical data sets and all produce satisfying results. It is expected to have a promising applications in the future.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521001574",
    "keywords": [
      "Algorithm",
      "Betweenness centrality",
      "Biology",
      "Botany",
      "Centrality",
      "Combinatorics",
      "Computer science",
      "Data mining",
      "Eigenvalues and eigenvectors",
      "Graph",
      "Identification (biology)",
      "Laplacian matrix",
      "Mathematics",
      "Network theory",
      "Node (physics)",
      "Physics",
      "Quantum mechanics",
      "Theoretical computer science",
      "Topology (electrical circuits)",
      "Vertex (graph theory)"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Yan"
      },
      {
        "surname": "Feng",
        "given_name": "Zhidan"
      },
      {
        "surname": "Qi",
        "given_name": "Xingqin"
      }
    ]
  },
  {
    "title": "Graph-based neural network models with multiple self-supervised auxiliary tasks",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.04.021",
    "abstract": "Self-supervised learning is currently gaining a lot of attention, as it allows neural networks to learn robust representations from large quantities of unlabeled data. Additionally, multi-task learning can further improve representation learning by training networks simultaneously on related tasks, leading to significant performance improvements. In this paper, we propose three novel self-supervised auxiliary tasks to train graph-based neural network models in a multi-task fashion. Since Graph Convolutional Networks are among the most promising approaches for capturing relationships among structured data points, we use them as a building block to achieve competitive results on standard semi-supervised graph classification tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521001604",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Block (permutation group theory)",
      "Computer science",
      "Convolutional neural network",
      "Economics",
      "External Data Representation",
      "Geometry",
      "Graph",
      "Labeled data",
      "Law",
      "Machine learning",
      "Management",
      "Mathematics",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Semi-supervised learning",
      "Task (project management)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Manessi",
        "given_name": "Franco"
      },
      {
        "surname": "Rozza",
        "given_name": "Alessandro"
      }
    ]
  },
  {
    "title": "Star-based learning correlation clustering",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107966",
    "abstract": "Correlation clustering (CC) is a clustering method using a signed graph as input without specifying the number of clusters a priori. It had been widely used in real applications, such as social network and text mining. However, its exact optimization or approximate algorithms often give unsatisfactory results, especially for large-scale signed graphs. This paper tackles this problem and proposes a novel CC algorithm, termed star-based learning correlation clustering (SL-CC). The proposed SL-CC contains two phases. The first is a scale reduction for signed graphs. We propose a special motif, called a star structure, for reducing the scale of signed graphs. We assign the vertices within a star structure to have the same cluster label and then merge these vertices as a new vertex in the graph so we can shrink a large-scale graph to a much small-scale one. The second is to give a learning schema for the local search on the reduced graphs. It can discover some important stars as seeds of clusters according to the graph structure, and then justify whether the other stars need to be merged with seeds or not. We also construct a new integer linear programing (ILP) model based on cycle inequalities to perform the local search with final clustering results. The experiments and comparisons of the proposed SL-CC with some existing CC methods on synthetic and real data sets with variant scale structures of signed graphs demonstrate the efficiency and usefulness of the SL-CC algorithm.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321001539",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Cluster analysis",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Information retrieval",
      "Mathematics",
      "Merge (version control)",
      "Stars",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Hua",
        "given_name": "Jialin"
      },
      {
        "surname": "Yu",
        "given_name": "Jian"
      },
      {
        "surname": "Yang",
        "given_name": "Miin-Shen"
      }
    ]
  },
  {
    "title": "Single-Image super-resolution - When model adaptation matters",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107931",
    "abstract": "In recent years, impressive advances have been made in single-image super-resolution. Deep learning is behind much of this success. Deep(er) architecture design and external prior modeling are the key ingredients. The internal contents of the low-resolution input image are neglected with deep modeling, despite earlier works that show the power of using such internal priors. In this paper, we propose a variation of deep residual convolutional neural networks, which has been carefully designed for robustness and efficiency in both learning and testing. Moreover, we propose multiple strategies for model adaptation to the internal contents of the low-resolution input image and analyze their strong points and weaknesses. By trading runtime and using internal priors, we achieve improvements from 0.1 to 0.3 dB PSNR over the reported results on standard datasets. Our adaptation especially favors images with repetitive structures or high resolutions. It indicates a more practical usage when our adaption approach applies to sequences or videos in which adjacent frames are strongly correlated in their contents. Moreover, the approach can be combined with other simple techniques, such as back-projection and enhanced prediction, to realize further improvements.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321001187",
    "keywords": [
      "Adaptation (eye)",
      "Algorithm",
      "Artificial intelligence",
      "Bayesian probability",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Deep learning",
      "Gene",
      "Image (mathematics)",
      "Machine learning",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Prior probability",
      "Projection (relational algebra)",
      "Residual",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Liang",
        "given_name": "Yudong"
      },
      {
        "surname": "Timofte",
        "given_name": "Radu"
      },
      {
        "surname": "Wang",
        "given_name": "Jinjun"
      },
      {
        "surname": "Zhou",
        "given_name": "Sanping"
      },
      {
        "surname": "Gong",
        "given_name": "Yihong"
      },
      {
        "surname": "Zheng",
        "given_name": "Nanning"
      }
    ]
  },
  {
    "title": "Improved multi-scale dynamic feature encoding network for image demoiréing",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107970",
    "abstract": "The popularity of smartphones with digital cameras makes photographing using smartphones an important daily activity. Moiré patterns can easily appear when shooting objects with rich textures, such as computer screens, and will severely degrade the image quality. Image demoiréing is an important image restoration task that aims to remove moiré patterns and reveal the underlying clean image. Two key properties of moiré patterns—the widely distributed frequency spectrum and the dynamic nature of moiré textures—challenge the image demoiréing task. In this paper, we propose an improved Multi-scale convolutional network with Dynamic feature encoding for image DeMoiréing (MDDM+). We design two schemes in our network to respectively attack the broad frequency spectrum and the dynamic texture of moiré: a multi-scale structure to process images at different spatial resolutions and a dynamic feature encoding module to encode the texture dynamically. To capture more moiré and texture information from different frequencies, we further propose a novel L 1 wavelet loss used to train our model. Extensive experiments on two benchmarks show that our proposed image demoiréing network can outperform the state of the arts in terms of fidelity as well as perception.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321001576",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "ENCODE",
      "Encoding (memory)",
      "Feature (linguistics)",
      "Gene",
      "Image (mathematics)",
      "Image processing",
      "Image texture",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Cheng",
        "given_name": "Xi"
      },
      {
        "surname": "Fu",
        "given_name": "Zhenyong"
      },
      {
        "surname": "Yang",
        "given_name": "Jian"
      }
    ]
  },
  {
    "title": "Iterative sparse and deep learning for accurate diagnosis of Alzheimer’s disease",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107944",
    "abstract": "Deep learning techniques have been increasingly applied to the diagnosis of Alzheimer’s disease (AD) and the conversion from mild cognitive impairment (MCI) to AD. Despite their prevalence, existing methods usually suffer from using either irrelevant brain regions or less-accurate landmarks. In this paper, we propose the iterative sparse and deep learning (ISDL) model for joint deep feature extraction and critical cortical region identification to diagnose AD and MCI. We first design a deep feature extraction (DFE) module to capture the local-to-global structural information derived from 62 cortical regions. Then we design a sparse regression module to identify the critical cortical regions and integrate it into the DFE module to exclude irrelevant cortical regions from the diagnosis process. The parameters of the two modules are updated alternatively and iteratively in an end-to-end manner. Our experimental results suggest the ISDL model provides a state-of-the-art solution to both AD-CN classification and MCI-to-AD prediction.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032100131X",
    "keywords": [
      "Artificial intelligence",
      "Cognition",
      "Cognitive impairment",
      "Computer science",
      "Deep learning",
      "Feature (linguistics)",
      "Feature extraction",
      "Linguistics",
      "Machine learning",
      "Neuroscience",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Process (computing)",
      "Psychology"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Yuanyuan"
      },
      {
        "surname": "Xia",
        "given_name": "Yong"
      }
    ]
  },
  {
    "title": "Identity authentication based on keystroke dynamics for mobile device users",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.04.019",
    "abstract": "User authentication of mobile devices is an important means to protect data security and user privacy. Low overhead, high accuracy and continuous authentication are challenging problems in user authentication. Aiming at the disadvantages of current authentication methods based on keystroke dynamics such as low accuracy and one-time authentication, this paper proposes a new authentication method named UIKI (User Identity authentication method based on clusters of Keystroke time Intervals). UIKI consists of the valid user modeling phase and the runtime user authentication phase. In the valid user modeling phase, UIKI uses a clustering algorithm to find the stable centroids and the fluctuation range of a valid user's centroid positions. In the runtime user authentication phase, UIKI clusters the data to be authenticated with the stable centroids as the initial centroids, and compares the results of clustering with the stable centroids and the centroid fluctuation range of the valid user to determine whether the input data are from the valid user. Experimental results prove that UIKI has the average FAR (False Accept Rate) of 0.082 and the average FRR (False Reject Rate) of 0.052, which can effectively authenticate a user's identity. UIKI has the advantages of high accuracy, low overhead and continuous authentication, which is suitable for the user authentication of mobile devices.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521001586",
    "keywords": [
      "Acoustics",
      "Artificial intelligence",
      "Authentication (law)",
      "Authentication protocol",
      "Centroid",
      "Cluster analysis",
      "Computer science",
      "Computer security",
      "Identity (music)",
      "Keystroke dynamics",
      "Operating system",
      "Overhead (engineering)",
      "Password",
      "Physics",
      "S/KEY"
    ],
    "authors": [
      {
        "surname": "Gao",
        "given_name": "Zhigang"
      },
      {
        "surname": "Diao",
        "given_name": "Wenjie"
      },
      {
        "surname": "Huang",
        "given_name": "Yucai"
      },
      {
        "surname": "Xu",
        "given_name": "Ruichao"
      },
      {
        "surname": "Lu",
        "given_name": "Huijuan"
      },
      {
        "surname": "Zhang",
        "given_name": "Jianhui"
      }
    ]
  },
  {
    "title": "Single annotated pixel based weakly supervised semantic segmentation under driving scenes",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107979",
    "abstract": "Semantic segmentation tasks based on weakly supervised conditions have been put forward to achieve a lightweight labeling process. For simple images that only include a few categories, research based on image-level annotations has achieved acceptable performance. However, when facing complex scenes, since image contains a large number of classes, it becomes challenging to learn visual appearance based on image tags. In this case, image-level annotations are not useful in providing information. Therefore, we set up a new task in which a single annotated pixel is provided for each category in a whole dataset. Based on the more lightweight and informative condition, a three step process is built for pseudo labels generation, which progressively implements each class’ optimal feature representation, image inference, and context-location based refinement. In particular, since high-level semantics and low-level imaging features have different discriminative abilities for each class under driving scenes, we divide categories into “object” or “scene” and then provide different operations for the two types separately. Further, an alternate iterative structure is established to gradually improve segmentation performance, which combines CNN-based inter-image common semantic learning and imaging prior based intra-image modification process. Experiments on the Cityscapes dataset demonstrate that the proposed method provides a feasible way to solve weakly supervised semantic segmentation tasks under complex driving scenes.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321001667",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Class (philosophy)",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Discriminative model",
      "Feature (linguistics)",
      "Inference",
      "Law",
      "Linguistics",
      "Object (grammar)",
      "Operating system",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pixel",
      "Political science",
      "Politics",
      "Process (computing)",
      "Programming language",
      "Representation (politics)",
      "Segmentation",
      "Semantics (computer science)",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Xi"
      },
      {
        "surname": "Ma",
        "given_name": "Huimin"
      },
      {
        "surname": "Yi",
        "given_name": "Sheng"
      },
      {
        "surname": "Chen",
        "given_name": "Yanxian"
      },
      {
        "surname": "Ma",
        "given_name": "Hongbing"
      }
    ]
  },
  {
    "title": "MEMF: Multi-level-attention embedding and multi-layer-feature fusion model for person re-identification",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107937",
    "abstract": "Person re-identification (re-ID) methods need to extract representative, rich and discriminative features in order to deal with the effect of imperfect pedestrian detectors, illumination changes, occlusions, and background confusion. In this paper, a multi-level-attention embedding and multi-layer-feature fusion (MEMF) model is proposed for person re-ID. Specifically, a novel backbone network is designed, in which multi-level-attention blocks are embedded into a multi-layer-feature fusion architecture. Multi-level-attention blocks can highlight representative features and assist global feature expression, and multi-layer-feature fusion can increase the fine granularity of feature expression and obtain richer features. Besides, a new eigenvalue difference orthogonality (EDO) loss is designed to reduce the correlation between features. The final loss is defined as the combination of the cross-entropy loss and the EDO loss, which improves re-ID results. The proposed method is evaluated on four popular and challenging datasets. Detailed experiments demonstrate that the application of various elements of the MEMF model can help improve person re-ID performance. Compared with start-of-the-art methods, the MEMF model gets a promising result.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321001242",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Discriminative model",
      "Embedding",
      "Feature (linguistics)",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Sun",
        "given_name": "Jia"
      },
      {
        "surname": "Li",
        "given_name": "Yanfeng"
      },
      {
        "surname": "Chen",
        "given_name": "Houjin"
      },
      {
        "surname": "Zhang",
        "given_name": "Bin"
      },
      {
        "surname": "Zhu",
        "given_name": "Jinlei"
      }
    ]
  },
  {
    "title": "T-VLAD: Temporal vector of locally aggregated descriptor for multiview human action recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.04.023",
    "abstract": "Robust view-invariant human action recognition (HAR) requires effective representation of its temporal structure in multi-view videos. This study explores a view-invariant action representation based on convolutional features. Action representation over long video segments is computationally expensive, whereas features in short video segments limit the temporal coverage locally. Previous methods are based on complex multi-stream deep convolutional feature maps extracted over short segments. To cope with this issue, a novel framework is proposed based on a temporal vector of locally aggregated descriptors (T-VLAD). T-VLAD encodes long term temporal structure of the video employing single stream convolutional features over short segments. A standard VLAD vector size is a multiple of its feature codebook size (256 is normally recommended). VLAD is modified to incorporate time-order information of segments, where the T-VLAD vector size is a multiple of its smaller time-order codebook size. Previous methods have not been extensively validated for view-variation. Results are validated in a challenging setup, where one view is used for testing and the remaining views are used for training. State-of-the-art results have been obtained on three multi-view datasets with fixed cameras, IXMAS, MuHAVi and MCAD. Also, the proposed encoding approach T-VLAD works equally well on a dynamic background dataset, UCF101.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521001628",
    "keywords": [
      "Action recognition",
      "Artificial intelligence",
      "Class (philosophy)",
      "Codebook",
      "Computer science",
      "Convolutional neural network",
      "Feature (linguistics)",
      "Feature vector",
      "Invariant (physics)",
      "Law",
      "Linguistics",
      "Mathematical physics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Binte Naeem",
        "given_name": "Hajra"
      },
      {
        "surname": "Murtaza",
        "given_name": "Fiza"
      },
      {
        "surname": "Yousaf",
        "given_name": "Muhammad Haroon"
      },
      {
        "surname": "Velastin",
        "given_name": "Sergio A."
      }
    ]
  },
  {
    "title": "Editorial of special section on CIARP 2019",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.04.014",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521001537",
    "keywords": [
      "Artificial intelligence",
      "Computer graphics (images)",
      "Computer science",
      "Engineering",
      "Engineering physics",
      "Operating system",
      "Section (typography)",
      "Special section"
    ],
    "authors": [
      {
        "surname": "Nyström",
        "given_name": "Ingela"
      },
      {
        "surname": "Ruiz-Shulcloper",
        "given_name": "José"
      }
    ]
  },
  {
    "title": "New confocal hyperbola-based ellipse fitting with applications to estimating parameters of mechanical pipes from point clouds",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107948",
    "abstract": "This manuscript presents a new method for fitting ellipses to two-dimensional data using the confocal hyperbola approximation to the geometric distance of points to ellipses. The proposed method was evaluated and compared to established methods on simulated and real-world datasets. First, it was revealed that the confocal hyperbola distance considerably outperforms other distance approximations such as algebraic and Sampson. Next, the proposed ellipse fitting method was compared with five reliable and established methods proposed by Halir, Taubin, Kanatani, Ahn and Szpak. The performance of each method as a function of rotation, aspect ratio, noise, and arc-length were examined. It was observed that the proposed ellipse fitting method achieved almost identical results (and in some cases better) than the gold standard geometric method of Ahn and outperformed the remaining methods in all simulation experiments. Finally, the proposed method outperformed the considered ellipse fitting methods in estimating the geometric parameters of cylindrical mechanical pipes from point clouds. The results of the experiments show that the confocal hyperbola is an excellent approximation to the true geometric distance and produces reliable and accurate ellipse fitting in practical settings.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321001357",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Curve fitting",
      "Ellipse",
      "Geometry",
      "Hyperbola",
      "Mathematics",
      "Point (geometry)",
      "Point cloud",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Maalek",
        "given_name": "Reza"
      },
      {
        "surname": "Lichti",
        "given_name": "Derek D."
      }
    ]
  },
  {
    "title": "Scene image and human skeleton-based dual-stream human action recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.06.003",
    "abstract": "The dual stream-based human action recognition model offers the advantage of high recognition accuracy, but the algorithm is less robust in case of lighting changes. The human skeleton has a strong ability to express human behavior and actions; however, the scene information is ignored. Drawing on the idea of the dual-stream model, this paper proposes a human skeleton and scene image-based dual-stream model for human action recognition. The motion features are extracted through the spatio-temporal graph convolution of the human skeleton, and a scene recognition model is proposed based on the sparse frame sampling of video and video-level consensus strategy to process the scene video and gather the visual scene information. The proposed model exploits the advantages of skeleton information in motion expression and the superiority of the image in scene presentation. The scene information and spatio-temporal graph convolution-based human skeleton limbs are fused complementarily to achieve human action recognition. Compared to the conventional optical flow-based dual-stream action recognition method, this model is verified by experimenting under unstable light conditions, and the performance of human action recognition is robust and promising.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521001902",
    "keywords": [
      "Action recognition",
      "Art",
      "Artificial intelligence",
      "Artificial neural network",
      "Class (philosophy)",
      "Computer science",
      "Computer vision",
      "Convolution (computer science)",
      "Dual (grammatical number)",
      "Graph",
      "Human motion",
      "Human skeleton",
      "Image (mathematics)",
      "Literature",
      "Motion (physics)",
      "Optical flow",
      "Pattern recognition (psychology)",
      "Programming language",
      "Skeleton (computer programming)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Qingyang"
      },
      {
        "surname": "Zheng",
        "given_name": "Wanqiang"
      },
      {
        "surname": "Song",
        "given_name": "Yong"
      },
      {
        "surname": "Zhang",
        "given_name": "Chengjin"
      },
      {
        "surname": "Yuan",
        "given_name": "Xianfeng"
      },
      {
        "surname": "Li",
        "given_name": "Yibin"
      }
    ]
  },
  {
    "title": "A Novel Unsupervised domain adaptation method for inertia-Trajectory translation of in-air handwriting",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107939",
    "abstract": "As a new method of human-computer interaction, inertial sensor-based in-air handwriting can provide natural and unconstrained interaction to express more complex and rich information in 3D space. However, most of the existing literature is mainly focused on in-air handwriting recognition (IAHR), which makes these works suffer from the poor readability of inertial signals and the lack of labeled samples. To address these two problems, we use an unsupervised domain adaptation method to recover the trajectory of inertial signals and generate inertial samples using handwritten trajectories. In this paper, we propose an Air-Writing Translator model to learn the bi-directional translation between trajectory domain and inertial domain in the absence of paired inertial and trajectory samples. Through latent-level adversarial learning and latent classification loss, the proposed model learns to extract domain-invariant features between the inertial signal and the trajectory while preserving semantic consistency during the translation across the two domains. In addition, the proposed framework can accept inputs of arbitrary length and translate between different sampling rates. Experiments on two public datasets, 6DMG (in-air handwriting dataset) and CT (handwritten trajectory dataset), are conducted and the results demonstrate that the proposed model can achieve reliable translation between inertial domain and trajectory domain. Empirically, our method also yields the best results in comparison to the state-of-the-art methods for IAHR.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321001266",
    "keywords": [
      "Artificial intelligence",
      "Astronomy",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Domain (mathematical analysis)",
      "Gene",
      "Handwriting",
      "Inertial frame of reference",
      "Mathematical analysis",
      "Mathematics",
      "Messenger RNA",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Speech recognition",
      "Trajectory",
      "Translation (biology)"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Songbin"
      },
      {
        "surname": "Xue",
        "given_name": "Yang"
      },
      {
        "surname": "Zhang",
        "given_name": "Xin"
      },
      {
        "surname": "Jin",
        "given_name": "Lianwen"
      }
    ]
  },
  {
    "title": "Recognition of 3D emotional facial expression based on handcrafted and deep feature combination",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.04.030",
    "abstract": "Facial emotion recognition (FER) methods have been proposed mainly using 2D images. These methods suffer from many problems caused by the difficult conditions of unconstrained environments such as light conditions and view variations. In this paper, we aim to recognize the emotional facial expressions independently of their identity using the 3D data and 2D depth images. Since the 3D FER is a very fine-grained recognition task, mapping the 3D images into 2D depth images may lack some geometric characteristics of the expressive face and decay the FER performance. Convolutional Neural Networks (CNN), however, have been successfully applied to the 2D depth images and improved handcrafted-based methods in computer vision and pattern recognition applications. For this reason, we combine in this paper two types of features; handcrafted and deep learning features and prove their complementarity for 3D FER. Favorably, covariance descriptors have proven a very good ability to combine features from different types into a compact representation. Therefore, we propose to use the covariance matrices of features (handcrafted and deep ones), instead of the features independently. Since covariance matrices belong to one of the manifold space types, formed by SPD (Symmetric Positive Definite) matrices, we mainly focus on the generalization of the RBF kernel to the manifold space for 3D FER using a supervised SVM classification. The achieved performance of the proposed method on the Bosphorus and BU-3DFE datasets outperforms similar state-of-the-arts.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521001744",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Convolutional neural network",
      "Covariance",
      "Deep learning",
      "Face (sociological concept)",
      "Facial expression",
      "Feature (linguistics)",
      "Feature vector",
      "Generalization",
      "Kernel (algebra)",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Social science",
      "Sociology",
      "Statistics",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Hariri",
        "given_name": "Walid"
      },
      {
        "surname": "Farah",
        "given_name": "Nadir"
      }
    ]
  },
  {
    "title": "Unsupervised meta-learning for few-shot learning",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107951",
    "abstract": "Meta-learning is an effective tool to address the few-shot learning problem, which requires new data to be classified considering only a few training examples. However, when used for classification, it requires large labeled datasets, which are not always available in practice. In this paper, we propose an unsupervised meta-learning algorithm that learns from an unlabeled dataset and adapts to downstream human-specific tasks with few labeled data. The proposed algorithm constructs tasks using clustering embedding methods and data augmentation functions to satisfy two critical class distinction requirements. To alleviate the biases and the weak diversity problem introduced by data augmentation functions, the proposed algorithm uses two methods, which are shifting the feeding data between the inner-outer loops and a novel data augmentation function. We further provide theoretical analysis of the effect of augmentation data in the inner/outer loop. Experiments on the MiniImagenet and Omniglot datasets demonstrate that the proposed unsupervised meta-learning approach outperforms other tested unsupervised representation learning approaches and two recent unsupervised meta-learning baselines. Compared with supervised meta-learning approaches, certain results produced by our method are quite close to those produced by such methods trained on the human-designed labeled tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321001382",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Economics",
      "Embedding",
      "Feature learning",
      "Labeled data",
      "Law",
      "Machine learning",
      "Management",
      "Meta learning (computer science)",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Semi-supervised learning",
      "Task (project management)",
      "Unsupervised learning"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Hui"
      },
      {
        "surname": "Wang",
        "given_name": "Jiaxing"
      },
      {
        "surname": "Li",
        "given_name": "Hao"
      },
      {
        "surname": "Ouyang",
        "given_name": "Deqiang"
      },
      {
        "surname": "Shao",
        "given_name": "Jie"
      }
    ]
  },
  {
    "title": "Adaptive, Hybrid Feature Selection (AHFS)",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107932",
    "abstract": "This paper deals with the problem of integrating the most suitable feature selection methods for a given problem in order to achieve the best feature order. A new, adaptive and hybrid feature selection approach is proposed, which combines and utilizes multiple individual methods in order to achieve a more generalized solution. Various state-of-the-art feature selection methods are presented in detail with examples of their applications and an exhaustive evaluation is conducted to measure and compare the their performance with the proposed approach. Results prove that while the individual feature selection methods may perform with high variety on the test cases, the combined algorithm steadily provides noticeably better solution.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321001199",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Feature (linguistics)",
      "Feature selection",
      "Linguistics",
      "Machine learning",
      "Measure (data warehouse)",
      "Minimum redundancy feature selection",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Selection (genetic algorithm)",
      "Variety (cybernetics)"
    ],
    "authors": [
      {
        "surname": "Viharos",
        "given_name": "Zsolt János"
      },
      {
        "surname": "Kis",
        "given_name": "Krisztián Balázs"
      },
      {
        "surname": "Fodor",
        "given_name": "Ádám"
      },
      {
        "surname": "Büki",
        "given_name": "Máté István"
      }
    ]
  },
  {
    "title": "SNAP: Shaping neural architectures progressively via information density criterion",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107923",
    "abstract": "Excellent neural network architecture is built on the specific target task and device. As the target task or device is different, the neural architecture we need will be different, too. Rather than redesigning or searching a brand new one, adjusting the existing architecture automatically is an alternative yet efficient way. To this end, we propose a method to Shape the existing Neural Architectures Progressively (SNAP) to adapt the target task and device better. Inspired by the streamline of water drop shaped by air resistance, we define an information density criterion (play the role of resistance) to drive the network architecture reducing the size of the part with the lowest information density. Iteratively, a more adaptive architecture will be obtained progressively in a greedy way. Theoretically, we prove that the greedy strategy is reasonable and can shape a better architecture. Because of the small adjustment of architecture each time, new architecture can inherit the parameters in old architecture to avoid retraining it from scratch. So the proposed method is very efficient in no need of high computation cost. Experimental results show that proposed method can effectively improve the given network by adjusting its architecture. And it can generate different architectures for different tasks and devices to adapt them well. Compared with search-based auto-generated neural architectures, our approach can achieve comparable or even better performance in no need of tremendous computation resources.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321001102",
    "keywords": [
      "Algorithm",
      "Architecture",
      "Art",
      "Artificial intelligence",
      "Artificial neural network",
      "Computation",
      "Computer network",
      "Computer science",
      "Database-centric architecture",
      "Engineering",
      "Network architecture",
      "Operating system",
      "Programming language",
      "Reference architecture",
      "Scratch",
      "Software",
      "Software architecture",
      "Systems engineering",
      "Task (project management)",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Zhiqiang"
      },
      {
        "surname": "Xu",
        "given_name": "Ting-Bing"
      },
      {
        "surname": "Liao",
        "given_name": "Weijian"
      },
      {
        "surname": "Li",
        "given_name": "Zhengcheng"
      },
      {
        "surname": "Li",
        "given_name": "Jinpeng"
      },
      {
        "surname": "Liu",
        "given_name": "Cheng-Lin"
      },
      {
        "surname": "He",
        "given_name": "Huiguang"
      }
    ]
  },
  {
    "title": "Nonlocal graph theory based transductive learning for hyperspectral image classification",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107967",
    "abstract": "Hyperspectral Image classification plays an important role in the maintenance of remote image analysis, which has been attracting a lot of research interest. Although various approaches, including unsupervised and supervised methods, have been proposed, obtaining a satisfactory classification result is still a challenge. In this paper, an efficient transductive learning method using variational nonlocal graph theory for hyperspectral image classification is proposed. First, the nonlocal vector neighborhood similarity is employed to build sparse graph representation. Then the variational segmentation framework is extended to label space, and the vectorization nonlocal energy function is constructed. Next, a fast comprehensive alternating minimization iteration algorithm is designed to implement labels transductive learning. At the same time, the labeled sample constraints are doubled ensured with simplex projection. Finally, experiments on six widely used hyperspectral image datasets are implemented, compared with other state-of-the-art classification methods, the classification results demonstrate that the proposed method has higher classification performance. Benefiting from graph theory and transductive idea, the proposed classification method can propagate labels and overcome the very high dimensionality and limited labeling problem to some extent.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321001540",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Curse of dimensionality",
      "Dimensionality reduction",
      "Graph",
      "Hyperspectral imaging",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Baoxiang"
      },
      {
        "surname": "Ge",
        "given_name": "Linyao"
      },
      {
        "surname": "Chen",
        "given_name": "Ge"
      },
      {
        "surname": "Radenkovic",
        "given_name": "Milena"
      },
      {
        "surname": "Wang",
        "given_name": "Xiaopeng"
      },
      {
        "surname": "Duan",
        "given_name": "Jinming"
      },
      {
        "surname": "Pan",
        "given_name": "Zhenkuan"
      }
    ]
  },
  {
    "title": "Recognition of human locomotion on various transportations fusing smartphone sensors",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.04.015",
    "abstract": "Recognition of daily human activities in various locomotion and transportation modes has numerous applications like coaching users for behavior modification and maintaining a healthy lifestyle. Besides, applications and user interfaces aware of user mobility through their smartphones can also aid in urban transportation planning, smart parking, and vehicular traffic monitoring. In this paper, we explored smartphone sensor-based two benchmark datasets (Sussex Huawei Locomotion (SHL) and Transportation Mode Detection (TMD)). Firstly, we demonstrated preprocesssing of sensor data, window length optimization based on Akaike Information Criteria (AIC), and introduced smartphone orientation independent features. We also provided an in-depth analysis of different smartphone sensors’ importance for classifying daily activities and transportation modes. We justified the sensor relevance by showing the variation of performances with the number of sensors explored. For refining classifier predictions, we also proposed a post-processing approach named “Mode technique”. This method primarily concentrates on the statistical analysis of transportation modes and improves the activity recognition rate in statistical classifiers: Decision Tree, K-Nearest Neighbors, Linear Discriminant Analysis, Logistic Regression, Support Vectors Machine with RBF kernel, Random Forest, and deep learning-based methods: Artificial Neural Network and Recurrent Neural Network by smoothing the outputs of these classifiers. Besides, we showed the use of magnitude and jerk-based features to overcome the overfitting problem due to smartphone orientation. We obtained 97.2% accuracy in the SHL dataset and 99.13% accuracy in the TMD dataset. These results demonstrate that our approach can profoundly recognize various activities in advanced locomotion and transportation modes compared to existing methods in two large-scale datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521001549",
    "keywords": [
      "Activity recognition",
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Classifier (UML)",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Decision tree",
      "Geodesy",
      "Geography",
      "Linear discriminant analysis",
      "Machine learning",
      "Overfitting",
      "Pattern recognition (psychology)",
      "Random forest",
      "Smoothing"
    ],
    "authors": [
      {
        "surname": "Antar",
        "given_name": "Anindya Das"
      },
      {
        "surname": "Ahmed",
        "given_name": "Masud"
      },
      {
        "surname": "Ahad",
        "given_name": "Md Atiqur Rahman"
      }
    ]
  },
  {
    "title": "Temporally smooth online action detection using cycle-consistent future anticipation",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107954",
    "abstract": "Many video understanding tasks work in the offline setting by assuming that the input video is given from the start to the end. However, many real-world problems require the online setting, making a decision immediately using only the current and the past frames of videos such as in autonomous driving and surveillance systems. In this paper, we present a novel solution for online action detection by using a simple yet effective RNN-based networks called the Future Anticipation and Temporally Smoothing network (FATSnet). The proposed network consists of a module for anticipating the future that can be trained in an unsupervised manner with the cycle-consistency loss, and another component for aggregating the past and the future for temporally smooth frame-by-frame predictions. We also propose a solution to relieve the performance loss when running RNN-based models on very long sequences. Evaluations on TVSeries, THUMOS’14, and BBDB show that our method achieve the state-of-the-art performances compared to the previous works on online action detection.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321001412",
    "keywords": [
      "Action (physics)",
      "Anticipation (artificial intelligence)",
      "Artificial intelligence",
      "Component (thermodynamics)",
      "Computer science",
      "Computer vision",
      "Consistency (knowledge bases)",
      "Frame (networking)",
      "Machine learning",
      "Mathematics",
      "Online model",
      "Physics",
      "Quantum mechanics",
      "Smoothing",
      "Statistics",
      "Telecommunications",
      "Thermodynamics"
    ],
    "authors": [
      {
        "surname": "Kim",
        "given_name": "Young Hwi"
      },
      {
        "surname": "Nam",
        "given_name": "Seonghyeon"
      },
      {
        "surname": "Kim",
        "given_name": "Seon Joo"
      }
    ]
  },
  {
    "title": "Local descriptor-based multi-prototype network for few-shot Learning",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107935",
    "abstract": "Prototype-based few-shot learning methods are promising in that they are simple yet effective to handle any-shot problems, and many prototype associated works are raised since then. However, these traditional prototype-based methods generally use only one single prototype to represent a class, which essentially cannot effectively estimate the complicated distribution of a class. To tackle this problem, we propose a novel Local descriptor-based Multi-Prototype Network (LMPNet) in this paper, a well-designed framework that generates an embedding space with multiple prototypes. Specifically, the proposed LMPNet employs local descriptors to represent each image, which can capture more informative and subtler cues of an image than the normally adopted image-level features. Moreover, to alleviate the uncertainty introduced by the fixed construction (averaging over samples) of prototypes, we introduce a channel squeeze and spatial excitation (sSE) attention module to learn multiple local descriptor-based prototypes for each class through end-to-end learning. Extensive experiments on both few-shot and fine-grained few-shot image classification tasks have been conducted on various benchmark datasets, including miniImageNet, tieredImageNet, Stanford Dogs, Stanford Cars, and CUB-200-2010. The experimental results of our LMPNet on above datasets show tangibly learning performance improvements and distinguishable outcomes over the baseline models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321001229",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Chemistry",
      "Class (philosophy)",
      "Computer science",
      "Computer vision",
      "Embedding",
      "Engineering",
      "Epistemology",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Machine learning",
      "Mechanical engineering",
      "One shot",
      "Optics",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Shot (pellet)",
      "Simple (philosophy)",
      "Single shot"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Hongwei"
      },
      {
        "surname": "Wu",
        "given_name": "Zhangkai"
      },
      {
        "surname": "Li",
        "given_name": "Wenbin"
      },
      {
        "surname": "Huo",
        "given_name": "Jing"
      },
      {
        "surname": "Gao",
        "given_name": "Yang"
      }
    ]
  },
  {
    "title": "Robust domain-adaptive discriminant analysis",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.05.005",
    "abstract": "Consider a domain-adaptive supervised learning setting, where a classifier learns from labeled data in a source domain and unlabeled data in a target domain to predict the corresponding target labels. If the classifier’s assumption on the relationship between domains (e.g. covariate shift, common subspace, etc.) is valid, then it will usually outperform a non-adaptive source classifier. If its assumption is invalid, it can perform substantially worse. Validating assumptions on domain relationships is not possible without target labels. We argue that, in order to make domain-adaptive classifiers more practical, it is necessary to focus on robustness; robust in the sense that an adaptive classifier will still perform at least as well as a non-adaptive classifier without having to rely on the validity of strong assumptions. With this objective in mind, we derive a conservative parameter estimation technique, which is transductive in the sense of Vapnik and Chervonenkis, and show for discriminant analysis that the new estimator is guaranteed to achieve a lower risk on the given target samples compared to the source classifier. Experiments on problems with geographical sampling bias indicate that our parameter estimator performs well.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521001732",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Discriminant",
      "Domain (mathematical analysis)",
      "Linear discriminant analysis",
      "Mathematical analysis",
      "Mathematics",
      "Multiple discriminant analysis",
      "Optimal discriminant analysis",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Kouw",
        "given_name": "Wouter M."
      },
      {
        "surname": "Loog",
        "given_name": "Marco"
      }
    ]
  },
  {
    "title": "A Self-Supervised Deep Learning Framework for Unsupervised Few-Shot Learning and Clustering",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.05.004",
    "abstract": "The need to learn a good representation is a core problem central to AI. We present a self-supervised representation learning framework and demonstrate its use for few-shot classification and clustering. Our framework can be interpreted as repeatedly discovering new categories from learned embeddings and training a new embedding function with self-supervised signals to differentiate the discovered categories. In our framework, we first discover categories from unlabeled data. Next we post-process the previous partition results to remove outliers and derive prototypes of each category. We then construct few-shot learning tasks with previously selected data and augmented virtual data. Lastly, we iterative train the network through previous steps to learn the final representation. Our framework can considerably outperform previous baselines in unsupervised few-shot classification tasks on miniImageNet and Omniglot data sets. We also validate our learned representation on clustering tasks and demonstrate that our framework further improves upon the recent deep clustering methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521001720",
    "keywords": [
      "Artificial intelligence",
      "Canopy clustering algorithm",
      "Cluster analysis",
      "Computer science",
      "Conceptual clustering",
      "Embedding",
      "Feature learning",
      "Fuzzy clustering",
      "Law",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Unsupervised learning"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Hongjing"
      },
      {
        "surname": "Zhan",
        "given_name": "Tianyang"
      },
      {
        "surname": "Davidson",
        "given_name": "Ian"
      }
    ]
  },
  {
    "title": "Collaborative learning in bounding box regression for object detection",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.05.007",
    "abstract": "Object detection has attracted growing attention due to its extensive application prospect, in which bounding box regression is an essential component. Dedicated to collaborative learning in bounding box regression, we explore the unified framework of smooth ℓ 1 and intersection over union, named SLIoU. On the basis of that, we propose a SLIoU loss as localization loss, which focuses on the geometric relationships of pairs of rectangular bounding boxes in overlapping degree, central position and structural shape. Furthermore, we propose a SLIoU-NMS for suppressing redundant detection boxes, which adaptively maps the evaluation value of detection boxes to meet the evaluation metric using nonlinear representation. By incorporating SLIoU loss and SLIoU-NMS into the state-of-the-art one-stage detectors, the detection performance is considerably improved.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552100177X",
    "keywords": [
      "Aerospace engineering",
      "Algorithm",
      "Artificial intelligence",
      "Basis (linear algebra)",
      "Bounding overwatch",
      "Computer science",
      "Computer vision",
      "Engineering",
      "Geometry",
      "Image (mathematics)",
      "Intersection (aeronautics)",
      "Law",
      "Machine learning",
      "Mathematics",
      "Metric (unit)",
      "Minimum bounding box",
      "Object (grammar)",
      "Object detection",
      "Operations management",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Regression",
      "Representation (politics)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Fang",
        "given_name": "Xian"
      },
      {
        "surname": "Kuang",
        "given_name": "Zengsheng"
      },
      {
        "surname": "Zhang",
        "given_name": "Ruixun"
      },
      {
        "surname": "Shao",
        "given_name": "Xiuli"
      },
      {
        "surname": "Wang",
        "given_name": "Hongpeng"
      }
    ]
  },
  {
    "title": "Ventral & Dorsal Stream Theory based Zero-Shot Action Recognition",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107953",
    "abstract": "Most Zero-Shot Action Recognition (ZSAR) methods establish visual-semantic joint embedding space, which is based on commonly used visual features and semantic embeddings, to learn the correlation between actions. Nevertheless, extracting visual features without structural guidance would lead to sparse video features, which reflect the correlation of actions, fall into oblivion. Based on the Ventral & Dorsal Stream Theory (VD), we propose a VD-ZSAR method to extract irredundant visual feature, which can relieve relation ambiguity caused by redundant visual feature. And a visual-semantic joint embedding space is learned by combining nonredundant visual space with semantic space. Specifically, visual space is constructed by the motion cues perceived by Dorsal Stream, and the object cues perceived by Ventral Stream. Semantic space is constructed by sentence-to-vector generator. The visual-semantic joint embedding space is built by a nonlinear similarity metric learning mechanism, which can better implicitly reflect the correlation between actions. Extensive experiments on the Olympic, HDMB51 and UCF101 datasets validate the favorable performance of our proposed approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321001400",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Economics",
      "Embedding",
      "Feature (linguistics)",
      "Feature vector",
      "Image (mathematics)",
      "Linguistics",
      "Metric (unit)",
      "Neuroscience",
      "Object (grammar)",
      "Operating system",
      "Operations management",
      "Pattern recognition (psychology)",
      "Perception",
      "Philosophy",
      "Psychology",
      "Similarity (geometry)",
      "Space (punctuation)",
      "Visual space"
    ],
    "authors": [
      {
        "surname": "Xing",
        "given_name": "Meng"
      },
      {
        "surname": "Feng",
        "given_name": "Zhiyong"
      },
      {
        "surname": "Su",
        "given_name": "Yong"
      },
      {
        "surname": "Peng",
        "given_name": "Weilong"
      },
      {
        "surname": "Zhang",
        "given_name": "Jianhai"
      }
    ]
  },
  {
    "title": "Pattern Recognition and Remote Sensing techniques applied to Land Use and Land Cover mapping in the Brazilian Savannah",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.04.028",
    "abstract": "The Brazilian Savannah, or Cerrado, has gained vital importance in the discussions about sustainable land development after the conversion of half of its natural vegetation. For the last two decades, most of the agricultural expansion in Brazil has occurred in this biome. This is related to technological improvements in agriculture as well as to environmental compliance policies that have effectively reduced soybean expansion in the Brazilian Amazon biome. Therefore, remotely sensed imagery, pattern recognition and image processing techniques have been employed to analyze and monitor the land dynamics over Cerrado. In this work, we present a brief review on Land Use and Land Cover mapping (LULC) in the Cerrado biome from an application perspective: natural vegetation, pastureland, agriculture, and deforestation. In this review we selected some studies whose results could contribute to the development of more detailed and accurate LULC maps for the Cerrado biome.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521001677",
    "keywords": [
      "Agriculture",
      "Agroforestry",
      "Amazon rainforest",
      "Archaeology",
      "Biology",
      "Biome",
      "Computer science",
      "Deforestation (computer science)",
      "Ecology",
      "Ecosystem",
      "Environmental science",
      "Geography",
      "Land cover",
      "Land use",
      "Medicine",
      "Pathology",
      "Programming language",
      "Remote sensing",
      "Vegetation (pathology)"
    ],
    "authors": [
      {
        "surname": "Fonseca",
        "given_name": "Leila M.G."
      },
      {
        "surname": "Körting",
        "given_name": "Thales S."
      },
      {
        "surname": "Bendini",
        "given_name": "Hugo do N."
      },
      {
        "surname": "Girolamo-Neto",
        "given_name": "Cesare D."
      },
      {
        "surname": "Neves",
        "given_name": "Alana K."
      },
      {
        "surname": "Soares",
        "given_name": "Anderson R."
      },
      {
        "surname": "Taquary",
        "given_name": "Evandro C."
      },
      {
        "surname": "Maretto",
        "given_name": "Raian V."
      }
    ]
  },
  {
    "title": "Introduction to the special issue on “Biometrics in Smart Cities: Techniques and Applications (BI_SCI)”",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.05.001",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521001689",
    "keywords": [
      "Artificial intelligence",
      "Biometrics",
      "Computer science"
    ],
    "authors": [
      {
        "surname": "Nappi",
        "given_name": "Michele"
      },
      {
        "surname": "Barra",
        "given_name": "Silvio"
      },
      {
        "surname": "Castiglione",
        "given_name": "Aniello"
      },
      {
        "surname": "Narducci",
        "given_name": "Fabio"
      },
      {
        "surname": "Vijayakumar",
        "given_name": "Pandi"
      }
    ]
  },
  {
    "title": "Discriminative feature alignment: Improving transferability of unsupervised domain adaptation by Gaussian-guided latent alignment",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107943",
    "abstract": "In this paper, we focus on the unsupervised domain adaptation problem where an approximate inference model is to be learned from a labeled data domain and expected to generalize well to an unlabeled domain. The success of unsupervised domain adaptation largely relies on the cross-domain feature alignment. Previous work has attempted to directly align features by classifier-induced discrepancies. Nevertheless, a common feature space cannot always be learned via this direct feature alignment especially when large domain gaps exist. To solve this problem, we introduce a Gaussian-guided latent alignment approach to align the latent feature distributions of the two domains under the guidance of a prior. In such an indirect way, the distributions over the samples from the two domains will be constructed on a common feature space, i.e., the space of the prior, which promotes better feature alignment. To effectively align the target latent distribution with this prior distribution, we also propose a novel unpaired L1-distance by taking advantage of the formulation of the encoder-decoder. The extensive evaluations on nine benchmark datasets validate the superior knowledge transferability through outperforming state-of-the-art methods and the versatility of the proposed method by improving the existing work significantly.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321001308",
    "keywords": [],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Jing"
      },
      {
        "surname": "Chen",
        "given_name": "Jiahong"
      },
      {
        "surname": "Lin",
        "given_name": "Jianzhe"
      },
      {
        "surname": "Sigal",
        "given_name": "Leonid"
      },
      {
        "surname": "de Silva",
        "given_name": "Clarence W."
      }
    ]
  },
  {
    "title": "Variational DNN embeddings for text-independent speaker verification",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.05.003",
    "abstract": "In state-of-the-art text-independent speaker verification systems, a discriminative deep neural network (DNN) model learns speaker-discriminative representations (x-vectors) for utterances using labeled data. For the verification task, a Probabilistic Linear Discriminant Analysis (PLDA) model is used to decide whether two x-vectors come from the same speaker. The PLDA scoring model assumes Gaussian priors and conditional distributions across speakers, which is not the case for x-vectors. This work introduces a variational-based regularization term that encourages the network to generate embeddings that follow a desired prior distribution. The regularization function performs a non-parametric match between the embeddings generated in the mini-batch training and a sample from the desired distribution. Unlike Variational Auto Encoders (VAEs), no distribution parameter is necessary to be learned, and no sampling schema is employed, which makes the proposed method flexible for different desired distributions. Our experiments compared the proposed method with the standard x-vectors system jointly with other approaches recently proposed to generate Gaussianized representations. We assessed the verification performance of the systems using the Fisher English Training Speech - Part II database in eight test conditions based on the gender of the speakers and the duration of the speech segments. Besides using the standard Gaussian distribution as prior, we also applied a less strict distribution (uniform). The proposed method outperformed others in all test conditions for both distributions, with gains of performance between 7.68% and 20.49% when compared to the standard x-vectors. To understand the effect of the regularizations into the embeddings space, we also conducted a 2-dimensional visual comparison, which showed clusters with better quality when the regularizations were applied.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521001707",
    "keywords": [
      "Algorithm",
      "Anthropology",
      "Artificial intelligence",
      "Bayesian probability",
      "Computer science",
      "Discriminative model",
      "Gaussian",
      "Mathematics",
      "Normalization (sociology)",
      "Parametric statistics",
      "Pattern recognition (psychology)",
      "Physics",
      "Prior probability",
      "Quantum mechanics",
      "Regularization (linguistics)",
      "Sociology",
      "Speech recognition",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Pinheiro",
        "given_name": "Hector N.B."
      },
      {
        "surname": "Ren",
        "given_name": "Tsang Ing"
      },
      {
        "surname": "Adami",
        "given_name": "André G."
      },
      {
        "surname": "Cavalcanti",
        "given_name": "George D.C."
      }
    ]
  },
  {
    "title": "Recognition of visual-related non-driving activities using a dual-camera monitoring system",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107955",
    "abstract": "For a Level 3 automated vehicle, according to the SAE International Automation Levels definition (J3016), the identification of non-driving activities (NDAs) that the driver is engaging with is of great importance in the design of an intelligent take-over interface. Much of the existing literature focuses on the driver take-over strategy with associated Human-Machine Interaction design. This paper proposes a dual-camera based framework to identify and track NDAs that require visual attention. This is achieved by mapping the driver's gaze using a nonlinear system identification approach, on the object scene, recognised by a deep learning algorithm. A novel gaze-based region of interest (ROI) selection module is introduced and contributes about a 30% improvement in average success rate and about a 60% reduction in average processing time compared to the results without this module. This framework has been successfully demonstrated to identify five types of NDA required visual attention with an average success rate of 86.18%. The outcome of this research could be applicable to the identification of other NDAs and the tracking of NDAs within a certain time window could potentially be used to evaluate the driver's attention level for both automated and human-driving vehicles.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321001424",
    "keywords": [
      "Advanced driver assistance systems",
      "Art",
      "Artificial intelligence",
      "Automation",
      "Biology",
      "Botany",
      "Computer science",
      "Computer vision",
      "Dual (grammatical number)",
      "Engineering",
      "Eye tracking",
      "Identification (biology)",
      "Literature",
      "Mechanical engineering",
      "Object detection",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Lichao"
      },
      {
        "surname": "Dong",
        "given_name": "Kuo"
      },
      {
        "surname": "Ding",
        "given_name": "Yan"
      },
      {
        "surname": "Brighton",
        "given_name": "James"
      },
      {
        "surname": "Zhan",
        "given_name": "Zhenfei"
      },
      {
        "surname": "Zhao",
        "given_name": "Yifan"
      }
    ]
  },
  {
    "title": "PRRNet: Pixel-Region relation network for face forgery detection",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107950",
    "abstract": "As advanced facial manipulation technologies develop rapidly, one can easily modify an image by changing the identity or the facial expression of the target person, which threatens social security. To address this problem, face forgery detection becomes an important and challenging task. In this paper, we propose a novel network, called Pixel-Region Relation Network (PRRNet), to capture pixel-wise and region-wise relations respectively for face forgery detection. The main motivation is that a facial manipulated image is composed of two parts from different sources, and the inconsistencies between the two parts is a significant kind of evidence for manipulation detection. Specifically, PRRNet contains two serial relation modules, i.e. the Pixel-Wise Relation (PR) module and the Region-Wise Relation (RR) module. For each pixel in the feature map, the PR module captures its similarities with other pixels to exploit the local relations information. Then, the PR module employs a spatial attention mechanism to represent the manipulated region and the original region separately. With the representations of the two regions, the RR module compares them with multiple metrics to measure the inconsistency between these two regions. In particular, the final predictions are obtained totally based on whether the inconsistencies exist. PRRNet achieves the state-of-the-art detection performance on three recent proposed face forgery detection datasets. Besides, our PRRNet shows the robustness when trained and tested on different image qualities.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321001370",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Data mining",
      "Exploit",
      "Face (sociological concept)",
      "Face detection",
      "Facial recognition system",
      "Feature (linguistics)",
      "Gene",
      "Image (mathematics)",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pixel",
      "Relation (database)",
      "Robustness (evolution)",
      "Social science",
      "Sociology",
      "Spatial relation"
    ],
    "authors": [
      {
        "surname": "Shang",
        "given_name": "Zhihua"
      },
      {
        "surname": "Xie",
        "given_name": "Hongtao"
      },
      {
        "surname": "Zha",
        "given_name": "Zhengjun"
      },
      {
        "surname": "Yu",
        "given_name": "Lingyun"
      },
      {
        "surname": "Li",
        "given_name": "Yan"
      },
      {
        "surname": "Zhang",
        "given_name": "Yongdong"
      }
    ]
  },
  {
    "title": "TrSeg: Transformer for semantic segmentation",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.04.024",
    "abstract": "Recent efforts in semantic segmentation using deep learning frameworks have made notable advances. However, capturing the existence of objects in an image at multiple scales still remains a challenge. In this paper, we address the semantic segmentation task based on transformer architecture. Unlike existing methods that capture multi-scale contextual information through infusing every single-scale piece of information from parallel paths, we propose a novel semantic segmentation network incorporating a transformer (TrSeg) to adaptively capture multi-scale information with the dependencies on original contextual information. Given the original contextual information as keys and values, the multi-scale contextual information from the multi-scale pooling module as queries is transformed by the transformer decoder. The experimental results show that TrSeg outperforms the other methods of capturing multi-scale information by large margins.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552100163X",
    "keywords": [
      "Architecture",
      "Art",
      "Artificial intelligence",
      "Computer science",
      "Information retrieval",
      "Machine learning",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Physics",
      "Pooling",
      "Quantum mechanics",
      "Segmentation",
      "Transformer",
      "Visual arts",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Jin",
        "given_name": "Youngsaeng"
      },
      {
        "surname": "Han",
        "given_name": "David"
      },
      {
        "surname": "Ko",
        "given_name": "Hanseok"
      }
    ]
  },
  {
    "title": "Beyond visual semantics: Exploring the role of scene text in image understanding",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.06.011",
    "abstract": "Images with visual and scene text content are ubiquitous in everyday life. However, current image interpretation systems are mostly limited to using only the visual features, neglecting to leverage the scene text content. In this paper, we propose to jointly use scene text and visual channels for robust semantic interpretation of images. We not only extract and encode visual and scene text cues but also model their interplay to generate a contextual joint embedding with richer semantics. The contextual embedding thus generated is applied to retrieval and classification tasks on multimedia images with scene text content to demonstrate its effectiveness. In the retrieval framework, we augment the contextual semantic representation with scene text cues to mitigate vocabulary misses that may have occurred during the semantic embedding. To deal with irrelevant or erroneous scene text recognition, we also apply query-based attention to the text channel. We show that our multi-channel approach, involving contextual semantics and scene text, improves upon the absolute accuracy of the current state-of-the-art methods on Advertisement Images Dataset by 8.9% in the relevant statement retrieval task and by 5% in the topic classification task.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521002178",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Embedding",
      "Image (mathematics)",
      "Image retrieval",
      "Information retrieval",
      "Law",
      "Leverage (statistics)",
      "Linguistics",
      "Natural language processing",
      "Philosophy",
      "Political science",
      "Politics",
      "Programming language",
      "Representation (politics)",
      "Semantics (computer science)",
      "Visual Word",
      "Visualization",
      "Vocabulary"
    ],
    "authors": [
      {
        "surname": "Dey",
        "given_name": "Arka Ujjal"
      },
      {
        "surname": "Ghosh",
        "given_name": "Suman K."
      },
      {
        "surname": "Valveny",
        "given_name": "Ernest"
      },
      {
        "surname": "Harit",
        "given_name": "Gaurav"
      }
    ]
  },
  {
    "title": "Scale-balanced loss for object detection",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107997",
    "abstract": "Object detection is an important field in computer vision. Nevertheless, a research area that has so far not received much attention is the study into the effectiveness of anchor matching strategy and imbalance in anchor-based object detection, in particular small object detection. It is clear that the objects with larger sizes tend to match more anchors than smaller ones. This matching imbalance may result in poor performance in detecting small objects. It can be alleviated by paying more attention to the objects that match with fewer anchors. We propose an innovative flexible loss function for object detection, which is compatible with popular anchor-based detection methods. The proposed method, called the scale-balanced loss, does not add any extra computational cost to the original pipelines. By re-weighting strategy, the proposed method significantly improves the accuracy of multi-scale object detection, especially for small objects. Comprehensive experiments indicate that the scale-balanced loss achieved excellent generalization performance when separately applied to some popular detection methods. The scale-balanced loss attained up to 15% improvements on recall rates of small and medium objects in both the PASCAL VOC and MS COCO dataset. It is also beneficial to the AP result on MS COCO with an improvement of more than 1.5%.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321001849",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Matching (statistics)",
      "Mathematics",
      "Medicine",
      "Object (grammar)",
      "Object detection",
      "Pascal (unit)",
      "Pattern recognition (psychology)",
      "Physics",
      "Programming language",
      "Quantum mechanics",
      "Radiology",
      "Scale (ratio)",
      "Statistics",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Shuang",
        "given_name": "Kai"
      },
      {
        "surname": "Lyu",
        "given_name": "Zhiheng"
      },
      {
        "surname": "Loo",
        "given_name": "Jonathan"
      },
      {
        "surname": "Zhang",
        "given_name": "Wentao"
      }
    ]
  },
  {
    "title": "Deformed contour segment matching for multi-source images",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107968",
    "abstract": "Robust and accurate multi-source matching is a difficult task due to significant nonlinear radiometric differences, background clutter, and geometric deformation in corresponding regions. Motivated by these existing problems, a discriminating yet robust combined descriptor for multi-source image matching, called deformed contour segment similarity (DCSS), is proposed in this work. First, the proposed DCSS, which is constructed by histogram of the combined contour features rather than the commonly used corner point and gradient, presents the accurate correspondence between image pairs and improves the descriptive ability to radiometric differences. Second, the deformed curve is presented via a finite-dimensional matrix Lie group to determine the similarity metric with an explicit geodesic solution. The geodesic distance, which indicates the nearest distance between curves in fluid space, is defined as the weight coefficient of the constructed histogram to enhance the robustness of the descriptor. The proposed algorithm utilizes the holistic contour information for the scoring and ranking of the shape similarity hypothesis, which can effectively reduce the influence of partially missing contours. Finally, a precise bilateral matching rule is used to perform the matching between the corresponding contour segments. Some experiments are carried out on various infrared-visible image data sets. The results demonstrate that the proposed DCSS achieves more robust and accurate matching performance than many popular multi-source image matching methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321001552",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Clutter",
      "Computer science",
      "Computer vision",
      "Gene",
      "Geodesic",
      "Geometry",
      "Histogram",
      "Image (mathematics)",
      "Matching (statistics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Radar",
      "Robustness (evolution)",
      "Similarity (geometry)",
      "Statistics",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Quan"
      },
      {
        "surname": "Xu",
        "given_name": "Guili"
      },
      {
        "surname": "Cheng",
        "given_name": "Yuehua"
      },
      {
        "surname": "Wang",
        "given_name": "Zhengsheng"
      },
      {
        "surname": "Li",
        "given_name": "Zhenhua"
      }
    ]
  },
  {
    "title": "Some aspects of fractional-order circular moments for image analysis",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.06.006",
    "abstract": "In this paper, we briefly review the fractional-order circular moments, such as fractional-order Zernike moments, fractional-order Fourier–Mellin moments, fractional-order Legendre–Fourier moments, and fractional-order Chebyshev–Fourier moments, which can characterize, analyze, and manipulate the information contained in an image with minimal redundancy. Also, they depend on an α parameter for better feature extraction. Therefore, we propose a procedure to find the optimal α in terms of image reconstruction error and classification. We validate the search for the best rotation-invariant features using the MNIST and MNIST-R datasets. Finally, we present the study results and conclusions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521001975",
    "keywords": [
      "Algorithm",
      "Applied mathematics",
      "Artificial intelligence",
      "Artificial neural network",
      "Chebyshev filter",
      "Computer science",
      "Economics",
      "Finance",
      "Fourier analysis",
      "Fourier transform",
      "Fractional Fourier transform",
      "Invariant (physics)",
      "Legendre polynomials",
      "MNIST database",
      "Mathematical analysis",
      "Mathematical physics",
      "Mathematics",
      "Operating system",
      "Optics",
      "Order (exchange)",
      "Physics",
      "Redundancy (engineering)",
      "Velocity Moments",
      "Wavefront",
      "Zernike polynomials"
    ],
    "authors": [
      {
        "surname": "Vargas-Vargas",
        "given_name": "Horlando"
      },
      {
        "surname": "Camacho-Bello",
        "given_name": "César"
      },
      {
        "surname": "Rivera-López",
        "given_name": "José S."
      },
      {
        "surname": "Noriega-Escamilla",
        "given_name": "Alicia"
      }
    ]
  },
  {
    "title": "Stability of three-way concepts and its application to natural language generation",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.06.005",
    "abstract": "Three-way concept analysis (3WCA) has been an emerging and important methodology for knowledge discovery and data analysis. Particularly, 3WCA can efficiently characterize the information of “jointly possessed” and “jointly not possessed” compared to the classical formal concept only can describe common attributes owned by objects. This property, typical of 3WCA has a huge potential in the field of Natural Language Generation (NLG). However, the construction of a three-way concept lattice is proved as an NP-complete problem and even harder than the construction of conventional concept lattice. This could negatively affect the use of 3WCA for NLG in real contexts. Hence, it is necessary to prune the three-way concept lattice and extract more interesting three-way concepts for knowledge acquisition. To this end, this paper defines the stability of a three-way concept and analyzes the relevant properties. An efficient computational algorithm for calculating the stability of three-way concepts is developed and evaluated by an experiment. In addition, a case study on NLG is conducted for demonstrating the applicability of the proposed technique.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521001951",
    "keywords": [
      "Archaeology",
      "Arithmetic",
      "Artificial intelligence",
      "Computer science",
      "Discrete mathematics",
      "History",
      "Machine learning",
      "Mathematics",
      "Natural (archaeology)",
      "Natural language",
      "Natural language processing",
      "Natural number",
      "Programming language",
      "Stability (learning theory)"
    ],
    "authors": [
      {
        "surname": "Hao",
        "given_name": "Fei"
      },
      {
        "surname": "Gao",
        "given_name": "Jie"
      },
      {
        "surname": "Bisogni",
        "given_name": "Carmen"
      },
      {
        "surname": "Min",
        "given_name": "Geyong"
      },
      {
        "surname": "Loia",
        "given_name": "Vincenzo"
      },
      {
        "surname": "De Maio",
        "given_name": "Carmen"
      }
    ]
  },
  {
    "title": "Regularizer based on Euler characteristic for retinal blood vessel segmentation",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.05.023",
    "abstract": "Segmentation of retinal blood vessels is important for the analysis of diabetic retinopathy (DR). Existing methods do not prioritize the small and disconnected vessels for DR. With the aim of paying attention to the small and disconnected vessel regions, this study introduced Euler characteristics (EC) from topology to calculate the number of isolated objects on segmented vessel regions, which is the key contribution of this study. In addition, we utilized the number of isolated objects in a U-Net-like deep convolutional neural network (CNN) architecture as a regularizer to train the network for improving the connectivity between the pixels of the vessel regions. The proposed network performance of the regularizer based on EC in reconstructing vessel regions is compared over the network without our regularizer. Furthermore, the capacity of the proposed regularizer approach in enhancing the smoothness and pixel connectivity of the vessels is compared with graph-based smoothing (GS) and combined GS with isolated objects (GISO) regularizers for delineating blood vessel regions. The proposed approach achieved the area under the curve value of 0.982, which is much higher than the state-of-the-arts, and thus it is suggested that the proposed system could support accuracy and reliability in decision-making for DR detection.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521002075",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Convolutional neural network",
      "Cut",
      "Graph",
      "Image segmentation",
      "Key (lock)",
      "Pattern recognition (psychology)",
      "Pixel",
      "Segmentation",
      "Smoothing",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Hakim",
        "given_name": "Lukman"
      },
      {
        "surname": "Kavitha",
        "given_name": "Muthu Subash"
      },
      {
        "surname": "Yudistira",
        "given_name": "Novanto"
      },
      {
        "surname": "Kurita",
        "given_name": "Takio"
      }
    ]
  },
  {
    "title": "Two-stage adaptive random Fourier sampling method for image reconstruction",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107990",
    "abstract": "We propose a random Fourier sampling scheme to enhance the accuracy of the high frequency pattern estimation for image reconstruction. This method is designed to work in a constrained ℓ 1 minimization based on the Fourier-Haar interplay revealing a column-wise maximum coherent structure that we provide. Essential in the scheme is to generate a data-driven density function by a small percentage of Fourier samples. The density function governs a random sampling procedure to acquire high frequency information, resulting in better reconstruction of the Haar wavelet coefficients. We also discuss a few examples of exact recovery of the Haar wavelet coefficients from which the proposed sampling scheme has emerged. Numerical experiments confirm superiority of the proposed sampling scheme to other conventional sampling schemes in the ℓ 1 framework.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321001771",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Discrete wavelet transform",
      "Filter (signal processing)",
      "Fourier analysis",
      "Fourier transform",
      "Haar wavelet",
      "Iterative reconstruction",
      "Mathematical analysis",
      "Mathematics",
      "Sampling (signal processing)",
      "Wavelet",
      "Wavelet transform"
    ],
    "authors": [
      {
        "surname": "Yun",
        "given_name": "Joo Dong"
      },
      {
        "surname": "Kim",
        "given_name": "Yunho"
      }
    ]
  },
  {
    "title": "Rough-Bayesian approach to select class-pair specific descriptors for HEp-2 cell staining pattern recognition",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107982",
    "abstract": "One of the important problems in computer-aided diagnosis of connective tissue disease is automatic recognition of staining patterns present in HEp-2 cells. In this regard, the paper introduces a novel approach for the recognition of staining patterns by HEp-2 cell indirect immunofluorescence image analysis. The proposed method assumes that a fixed set of local texture descriptors or scales may not be effective for classifying staining patterns into multiple classes. A particular set of descriptors or scales may be significant for classifying a pair of classes, but may not be relevant for other pairs of classes. The proposed approach, therefore, first selects a set of local texture descriptors under appropriate scales for each class-pair, and then forms the final feature set for multiple classes from the relevant descriptors of all possible pairs of classes. A novel framework, termed as Rough-Bayesian model, is introduced to evaluate the relevance of a descriptor and/or a scale. It is based on the merits of rough sets and Bayes decision theory. During the selection of relevant descriptor and/or scale, the proposed method takes care of the presence of both noisy pixels in an HEp-2 cell image and noisy HEp-2 cell images in a staining pattern class. The support vector machine is used to predict the staining patterns present in HEp-2 cell images. The performance of the proposed method, along with a comparison with state-of-the-art methods, is demonstrated on several HEp-2 cell image databases. An important finding is that the accuracy for classifying HEp-2 cell images is significantly increased if class-pair specific descriptors under appropriate scales are considered, instead of selecting a uniform set of descriptors and scales for multiple classes.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321001692",
    "keywords": [
      "Artificial intelligence",
      "Bayes' theorem",
      "Bayesian probability",
      "Class (philosophy)",
      "Computer science",
      "Feature (linguistics)",
      "Image (mathematics)",
      "Linguistics",
      "Mathematics",
      "Naive Bayes classifier",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Set (abstract data type)",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Kumar",
        "given_name": "Debamita"
      },
      {
        "surname": "Maji",
        "given_name": "Pradipta"
      }
    ]
  },
  {
    "title": "PASTLE: Pivot-aided space transformation for local explanations",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.05.018",
    "abstract": "During the last decade, more and more Artificial Intelligence systems have been designed using complex and sophisticated architectures to reach unprecedented predictive performance. The side effect is an increase in opacity of their inner workings which is inadmissible when such systems are applied in critical domains (healthcare, finance and so on). The eXplainable AI (XAI) research field aims to overcome this limitation thus helping humans to understand black-box decisions. In this paper we propose a novel model-agnostic XAI technique, named Pivot-Aided Space Transformation for Local Explanations (PASTLE), which exploits an instance-space transformation to explain any model’s predictions, aiming to enhance human trust towards the AI decisions. We experimentally evaluate the effects of the introduced space transformation on various real-world data sets and our user study reveals promising results in terms of effective explainability.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521002014",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Black box",
      "Chemistry",
      "Computer science",
      "Computer security",
      "Data science",
      "Exploit",
      "Field (mathematics)",
      "Gene",
      "Machine learning",
      "Mathematics",
      "Operating system",
      "Pure mathematics",
      "Space (punctuation)",
      "Transformation (genetics)"
    ],
    "authors": [
      {
        "surname": "La Gatta",
        "given_name": "Valerio"
      },
      {
        "surname": "Moscato",
        "given_name": "Vincenzo"
      },
      {
        "surname": "Postiglione",
        "given_name": "Marco"
      },
      {
        "surname": "Sperlì",
        "given_name": "Giancarlo"
      }
    ]
  },
  {
    "title": "Pedestrian instance segmentation with prior structure of semantic parts",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.05.012",
    "abstract": "Existing pedestrian segmentation and detection methods often show a significant drop in performance when heavy occlusion and deformation happen because most approaches rely on holistic modeling. Unlike many previous deep models that directly learn a holistic detector, in this paper, we introduce a pedestrian instance segmentation method with a prior structure of semantic parts named Part Mask R-CNN. Based on pedestrian parts’ proportion structure, process the original dataset annotations and then generate parts annotations as prior. By combining the semantic part branch with other classic detection and segmentation branches, the network learns more about pedestrian instances. Besides, we get such a more accurate pedestrian instance segmentation model without any artificial annotations. By extensive evaluations on the Cityscapes dataset, the results demonstrate that the proposed method can improve approaches such as Mask R-CNN, inaccuracy on pedestrian single class instance segmentation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521001926",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Engineering",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Pedestrian",
      "Segmentation",
      "Transport engineering"
    ],
    "authors": [
      {
        "surname": "Chu",
        "given_name": "Huazhen"
      },
      {
        "surname": "Ma",
        "given_name": "Huimin"
      },
      {
        "surname": "Li",
        "given_name": "Xi"
      }
    ]
  },
  {
    "title": "Learn to abstract via concept graph for weakly-supervised few-shot learning",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107946",
    "abstract": "In recent years, a large number of meta-learning methods have been proposed to address few-shot learning problems and have shown superior performance. However, the explicit prior knowledge (e.g., concept graph) and weakly-supervised information are rarely explored in existing methods, which are usually free or cheap to collect. In this paper, we introduce a concept graph for the weakly-supervised few-shot learning, and propose a novel meta-learning framework, namely, MetaConcept. Our key idea is to learn a universal meta-learner inferring any-level classifier, so as to boost the classification performance of meta-learning on the novel classes. Specifically, we firstly propose a novel regularization with multi-level conceptual abstraction to train a universal meta-learner to infer not only an entity classifier but also a concept classifier at different levels via the concept graph (i.e., learn to abstract). Then, we propose a meta concept inference network as the universal meta-learner for the base learner, aiming to quickly adapt to a novel task by the joint inference of the abstract concepts and a few annotated samples. We have conducted extensive experiments on two weakly-supervised few-shot learning benchmarks, namely, WS-ImageNet-Pure and WS-ImageNet-Mix. Our experimental results show that (1) the proposed MetaConcept outperforms state-of-the-art methods with an improvement of 2% to 6% in classification accuracy; (2) the proposed MetaConcept is able to yield a good performance though merely training with weakly-labeled datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321001333",
    "keywords": [
      "Abstraction",
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Epistemology",
      "Graph",
      "Inference",
      "Machine learning",
      "Philosophy",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Baoquan"
      },
      {
        "surname": "Leung",
        "given_name": "Ka-Cheong"
      },
      {
        "surname": "Li",
        "given_name": "Xutao"
      },
      {
        "surname": "Ye",
        "given_name": "Yunming"
      }
    ]
  },
  {
    "title": "Robust generalised quadratic discriminant analysis",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107981",
    "abstract": "Quadratic discriminant analysis (QDA) is a widely used statistical tool to classify observations from different multivariate Normal populations. The generalized quadratic discriminant analysis (GQDA) classification rule/classifier, which generalizes the QDA and the minimum Mahalanobis distance (MMD) classifiers to discriminate between populations with underlying elliptically symmetric distributions competes quite favorably with the QDA classifier when it is optimal and performs much better when QDA fails under non-Normal underlying distributions with heavy tail, e.g. Cauchy distribution. However, the classification rule in GQDA is still based on the sample mean vector and the sample dispersion matrix of a training set, which are extremely non-robust under data contamination. In real world, however, it is quite common to face data which are highly vulnerable to outliers and so the lack of robustness of the classical estimators of the mean vector and the dispersion matrix reduces the efficiency of the GQDA classifier significantly, increasing the misclassification errors. The present paper investigates the performance of the GQDA classifier when the classical estimators of the mean vector and the dispersion matrix used therein are replaced by various robust counterparts. Applications to various real data sets as well as simulation studies reveal far better performance of the proposed robust versions of the GQDA classifier. A comparative study has been made to advocate the appropriate choice of the robust estimators to be used in a specific situation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321001680",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Discriminant",
      "Estimator",
      "Geometry",
      "Linear discriminant analysis",
      "Mahalanobis distance",
      "Margin classifier",
      "Mathematics",
      "Multivariate normal distribution",
      "Multivariate statistics",
      "Outlier",
      "Pattern recognition (psychology)",
      "Quadratic classifier",
      "Quadratic equation",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Ghosh",
        "given_name": "Abhik"
      },
      {
        "surname": "SahaRay",
        "given_name": "Rita"
      },
      {
        "surname": "Chakrabarty",
        "given_name": "Sayan"
      },
      {
        "surname": "Bhadra",
        "given_name": "Sayan"
      }
    ]
  },
  {
    "title": "TCD-CF: Triple cross-domain collaborative filtering recommendation",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.06.016",
    "abstract": "Recently, data sparsity is still one of the critical problems faced by recommendation systems. Although many existing methods based on cross-domain can alleviate it to a certain extent, these methods only use the information of single-domain (e.g., user-side, item-side and rating-side) or dual-domain (e.g., user-rating-side, user-item-side and item-rating-side) to make recommendations, which results in performance degradation. In this paper, we propose a triple cross-domain collaborative filtering method to alleviate data sparsity, named TCD-CF. In TCD-CF method, the triple-side intrinsic characteristics are first obtained by using the joint nonnegative matrix factorization to integrate the user-side, item-side and rating-side domain knowledge. Then the extended codebook (as knowledge to transfer) based on these intrinsic characteristics is constructed by using the orthogonal nonnegative matrix tri-factorization. Finally, the codebook-based transfer method for cross-system CF is applied into the source domain and target domain to predict the missing ratings and perform recommendation in the target domain. Extensive experiments on two real-world datasets demonstrate that the proposed method outperforms the state-of-the-art methods for the cross-domain recommendation task.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521002233",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Codebook",
      "Collaborative filtering",
      "Computer science",
      "Data mining",
      "Domain (mathematical analysis)",
      "Economics",
      "Eigenvalues and eigenvectors",
      "Machine learning",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Matrix decomposition",
      "Pattern recognition (psychology)",
      "Physics",
      "Programming language",
      "Quantum mechanics",
      "Recommender system",
      "Side effect (computer science)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Taiheng"
      },
      {
        "surname": "Deng",
        "given_name": "Xiuqin"
      },
      {
        "surname": "He",
        "given_name": "Zhaoshui"
      },
      {
        "surname": "Long",
        "given_name": "Yonghong"
      }
    ]
  },
  {
    "title": "CDF Transform-and-Shift: An effective way to deal with datasets of inhomogeneous cluster densities",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107977",
    "abstract": "The problem of inhomogeneous cluster densities has been a long-standing issue for distance-based and density-based algorithms in clustering and anomaly detection. These algorithms implicitly assume that all clusters have approximately the same density. As a result, they often exhibit a bias towards dense clusters in the presence of sparse clusters. Many remedies have been suggested; yet, we show that they are partial solutions which do not address the issue satisfactorily. To match the implicit assumption, we propose to transform a given dataset such that the transformed clusters have approximately the same density while all regions of locally low density become globally low density—homogenising cluster density while preserving the cluster structure of the dataset. We show that this can be achieved by using a new multi-dimensional Cumulative Distribution Function in a transform-and-shift method. The method can be applied to every dataset, before the dataset is used in many existing algorithms to match their implicit assumption without algorithmic modification. We show that the proposed method performs better than existing remedies.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321001643",
    "keywords": [
      "Cluster (spacecraft)",
      "Computer science",
      "Operating system"
    ],
    "authors": [
      {
        "surname": "Zhu",
        "given_name": "Ye"
      },
      {
        "surname": "Ting",
        "given_name": "Kai Ming"
      },
      {
        "surname": "Carman",
        "given_name": "Mark J."
      },
      {
        "surname": "Angelova",
        "given_name": "Maia"
      }
    ]
  },
  {
    "title": "Robust subspace clustering network with dual-domain regularization",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.06.009",
    "abstract": "The field of deep subspace clustering has advanced rapidly in recent years. Ideas such as self-expression and self-supervision have led to innovative network design and improved clustering performance. However, it is observed that the nonlinear low-dimensional manifold constraint is valid in not only the ambient but also the latent feature spaces. Meanwhile, the issue of robustness has been largely overlooked in the literature of deep subspace clustering despite previous studies on robust model-based clustering. Based on these two observations, we present a robust subspace clustering network (RSCN) based on a novel hybrid loss function with dual-domain regularization. On the one hand, we propose to replace the existing L 2 loss by a robust hybrid function inspired by half-quadratic minimization; on the other hand, we come up with a novel strategy of sparsity regularization in the dual domain (both ambient and feature space). To the best of our knowledge, this is the first attempt to incorporate dual manifold constraints into deep subspace clustering. Experimental results show that our new network outperforms the existing state-of-the-art on several widely-studied datasets such as Extended Yale B, COIL20, and COIL100. The performance gain of our RSCN over several other competing approaches improves dramatically in the presence of noise contamination.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521002099",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Cluster analysis",
      "Computer science",
      "Gene",
      "Pattern recognition (psychology)",
      "Regularization (linguistics)",
      "Robustness (evolution)",
      "Subspace topology"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Fangfang"
      },
      {
        "surname": "Yuan",
        "given_name": "Peng"
      },
      {
        "surname": "Shi",
        "given_name": "Guangming"
      },
      {
        "surname": "Li",
        "given_name": "Xin"
      },
      {
        "surname": "Dong",
        "given_name": "Weisheng"
      },
      {
        "surname": "Wu",
        "given_name": "Jinjian"
      }
    ]
  },
  {
    "title": "Generate classical Chinese poems with theme-style from images",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.05.016",
    "abstract": "The automatic generation of poems from images is a classic task in natural language processing. Recently, it has gained tremendous research interest due to the help of neural sequence-to-sequence networks. The current methods are not qualified to generate poems under the joint control of the theme and style conditions, while these two key factors are critical to the quality of Chinese poetry. This paper proposes an image-based Chinese poem generation network (ICPGN), which can obtain the holistic content and sentiments in images and transform them into theme and style representations. A poem theme and style control module (PTSC) is designed to ensure that the generated poems meet the theme and style requirements simultaneously. A new dataset CQC2020 is constructed for training and testing of the proposed network and other related methods. Extensive manual and machine experiment results demonstrate the effectiveness and competitiveness of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521001999",
    "keywords": [
      "Archaeology",
      "Art",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer science",
      "Computer security",
      "Control (management)",
      "Engineering",
      "Epistemology",
      "Genetics",
      "History",
      "Image (mathematics)",
      "Key (lock)",
      "Literature",
      "Natural (archaeology)",
      "Natural language processing",
      "Philosophy",
      "Poetry",
      "Quality (philosophy)",
      "Sequence (biology)",
      "Style (visual arts)",
      "Systems engineering",
      "Task (project management)",
      "Theme (computing)",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Chunlei"
      },
      {
        "surname": "Wang",
        "given_name": "Jiangnan"
      },
      {
        "surname": "Yuan",
        "given_name": "Shaozu"
      },
      {
        "surname": "Wang",
        "given_name": "Leiquan"
      },
      {
        "surname": "Zhang",
        "given_name": "Weishan"
      }
    ]
  },
  {
    "title": "Detection of COVID-19 from speech signal using bio-inspired based cepstral features",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107999",
    "abstract": "The early detection of COVID-19 is a challenging task due to its deadly spreading nature and existing fear in minds of people. Speech-based detection can be one of the safest tools for this purpose as the voice of the suspected can be easily recorded. The Mel Frequency Cepstral Coefficient (MFCC) analysis of speech signal is one of the oldest but potential analysis tools. The performance of this analysis mainly depends on the use of conversion between normal frequency scale to perceptual frequency scale and the frequency range of the filters used. Traditionally, in speech recognition, these values are fixed. But the characteristics of speech signals vary from disease to disease. In the case of detection of COVID-19, mainly the coughing sounds are used whose bandwidth and properties are quite different from the complete speech signal. By exploiting these properties the efficiency of the COVID-19 detection can be improved. To achieve this objective the frequency range and the conversion scale of frequencies have been suitably optimized. Further to enhance the accuracy of detection performance, speech enhancement has been carried out before extraction of features. By implementing these two concepts a new feature called COVID-19 Coefficient (C-19CC) is developed in this paper. Finally, the performance of these features has been compared.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321001862",
    "keywords": [
      "Aerospace engineering",
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Cepstrum",
      "Computer science",
      "Engineering",
      "Feature extraction",
      "Mel-frequency cepstrum",
      "Pattern recognition (psychology)",
      "Physics",
      "Programming language",
      "Quantum mechanics",
      "Range (aeronautics)",
      "SIGNAL (programming language)",
      "Scale (ratio)",
      "Speech processing",
      "Speech recognition",
      "Telecommunications",
      "Voice activity detection"
    ],
    "authors": [
      {
        "surname": "Dash",
        "given_name": "Tusar Kanti"
      },
      {
        "surname": "Mishra",
        "given_name": "Soumya"
      },
      {
        "surname": "Panda",
        "given_name": "Ganapati"
      },
      {
        "surname": "Satapathy",
        "given_name": "Suresh Chandra"
      }
    ]
  },
  {
    "title": "MetAdapt: Meta-learned task-adaptive architecture for few-shot classification",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.05.010",
    "abstract": "Recently, great progress has been made in the field of Few-Shot Learning (FSL). While many different methods have been proposed, one of the key factors leading to higher FSL performance is surprisingly simple. It is the backbone network architecture used to embed the images of the few-shot tasks. While first works on FSL resorted to small architectures with just a few convolution layers, recent works show that large architectures pre-trained on the training portion of FSL datasets produce strong features that are more easily transferable to novel few-shot tasks, thus attaining significant gains to methods using them. Despite these observations, little to no work has been done towards finding the right backbone for FSL. In this paper we propose MetAdapt that not only meta-searches for an optimized architecture for FSL using Network Architecture Search (NAS), but also results in a model that can adaptively ‘re-wire’ itself predicting the better architecture for a given novel few-shot task. Using the proposed approach we observe strong results on two popular few-shot benchmarks: miniImageNet and FC100.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521001884",
    "keywords": [],
    "authors": [
      {
        "surname": "Doveh",
        "given_name": "Sivan"
      },
      {
        "surname": "Schwartz",
        "given_name": "Eli"
      },
      {
        "surname": "Xue",
        "given_name": "Chao"
      },
      {
        "surname": "Feris",
        "given_name": "Rogerio"
      },
      {
        "surname": "Bronstein",
        "given_name": "Alex"
      },
      {
        "surname": "Giryes",
        "given_name": "Raja"
      },
      {
        "surname": "Karlinsky",
        "given_name": "Leonid"
      }
    ]
  },
  {
    "title": "Digital hair removal by deep learning for skin lesion segmentation",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107994",
    "abstract": "Occlusion due to hair in dermoscopic images affects the diagnostic operation and the accuracy of its analysis of a skin lesion. Also, dermis hair has the following different characteristics: thin; overlapping; faded; of similar contrast or colour to the underlying skin; and obscuring/covering textured lesions. These make digital hair removal (DHR), which involves hair segmentation and hair gap inpainting, a challenging task. Thus, traditional hard-coded threshold-based hair removal methods are not effective, resulting in over-removal which loses important information of the skin lesion, or under-removal which cannot remove the hair effectively. In this paper, we propose a deep learning approach to DHR based on U-Net and a free-form image inpainting architecture. In hair segmentation, a well-labelled dataset is created and used to train U-Net in order to obtain accurate hair masks. In inpainting, a free-form image inpainting architecture (i.e., Gated convolution and SN-PatchGAN) which has been trained on millions of images is used to inpaint any hair gaps. We also propose an evaluation method to analyze the effect of hair removal based on a single dermoscopic image, named intra structural similarity (Intra-SSIM). The process of DHR is repeated until there is no change in the average value of Intra-SSIM. Using the ISIC 2018 dataset, the performance of the proposed method is shown to be better than other state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321001813",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer vision",
      "Convolution (computer science)",
      "Deep learning",
      "Image (mathematics)",
      "Image segmentation",
      "Inpainting",
      "Pattern recognition (psychology)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Wei"
      },
      {
        "surname": "Joseph Raj",
        "given_name": "Alex Noel"
      },
      {
        "surname": "Tjahjadi",
        "given_name": "Tardi"
      },
      {
        "surname": "Zhuang",
        "given_name": "Zhemin"
      }
    ]
  },
  {
    "title": "Dual self-attention with co-attention networks for visual question answering",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107956",
    "abstract": "Visual Question Answering (VQA) as an important task in understanding vision and language has been proposed and aroused wide interests. In previous VQA methods, Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN) are generally used to extract visual and textual features respectively, and then the correlation between these two features is explored to infer the answer. However, CNN mainly focuses on extracting local spatial information and RNN pays more attention on exploiting sequential architecture and long-range dependencies. It is difficult for them to integrate the local features with their global dependencies to learn more effective representations of the image and question. To address this problem, we propose a novel model, i.e., Dual Self-Attention with Co-Attention networks (DSACA), for VQA. It aims to model the internal dependencies of both the spatial and sequential structure respectively by using the newly proposed self-attention mechanism. Specifically, DSACA mainly contains three submodules. The visual self-attention module selectively aggregates the visual features at each region by a weighted sum of the features at all positions. The textual self-attention module automatically emphasizes the interdependent word features by integrating associated features among the sentence words. Besides, the visual-textual co-attention module explores the close correlation between visual and textual features learned from self-attention modules. The three modules are integrated into an end-to-end framework to infer the answer. Extensive experiments performed on three generally used VQA datasets confirm the favorable performance of DSACA compared with state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321001436",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Attention network",
      "Computer science",
      "Convolutional neural network",
      "Dual (grammatical number)",
      "Economics",
      "Feature (linguistics)",
      "Linguistics",
      "Literature",
      "Machine learning",
      "Management",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Question answering",
      "Sentence",
      "Task (project management)",
      "Word (group theory)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Yun"
      },
      {
        "surname": "Zhang",
        "given_name": "Xiaoming"
      },
      {
        "surname": "Zhang",
        "given_name": "Qianyun"
      },
      {
        "surname": "Li",
        "given_name": "Chaozhuo"
      },
      {
        "surname": "Huang",
        "given_name": "Feiran"
      },
      {
        "surname": "Tang",
        "given_name": "Xianghong"
      },
      {
        "surname": "Li",
        "given_name": "Zhoujun"
      }
    ]
  },
  {
    "title": "Boundarymix: Generating pseudo-training images for improving segmentation with scribble annotations",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107924",
    "abstract": "Weakly-supervised semantic segmentation, as a promising solution to alleviate the burden of collecting per-pixel annotations, aims to train a segmentation model from partial weak annotations. Scribble on the object is one of the commonly used weak annotations and has shown to be sufficient for learning a decent segmentation model. Despite being effective, scribble-based weakly-supervised learning methods often lead to imprecise segmentation on object boundaries. This is mainly because the scribble annotations usually locate inside the objects and the dataset lacks annotations close to the semantic boundaries. To alleviate this issue, this paper proposes a simple-but-effective solution, i.e., BoundaryMix, which generates pseudo training image-annotation pairs from the original images to supplement the missing semantic boundaries. Specifically, given a prediction of segmentation, we cut off the regions around the estimated boundaries, which are error-prone and replace them with the contents from another image, which in effect creates new samples with less ambiguity around semantic boundaries. With training on scribbles and the on-the-fly generated pseudo annotations, the network acquires better prediction capability around the boundary region and thus improves the overall segmentation performance. By conducting experiments on PASCAL VOC 2012 dataset and POTSDAM dataset with only scribble annotations, we demonstrate the excellent performance of the proposed method and the almost closed gap between scribble-supervised and fully-supervised image segmentation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321001114",
    "keywords": [
      "Ambiguity",
      "Annotation",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Image segmentation",
      "Pascal (unit)",
      "Pattern recognition (psychology)",
      "Programming language",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Lu",
        "given_name": "Wanxuan"
      },
      {
        "surname": "Gong",
        "given_name": "Dong"
      },
      {
        "surname": "Fu",
        "given_name": "Kun"
      },
      {
        "surname": "Sun",
        "given_name": "Xian"
      },
      {
        "surname": "Diao",
        "given_name": "Wenhui"
      },
      {
        "surname": "Liu",
        "given_name": "Lingqiao"
      }
    ]
  },
  {
    "title": "Mixed-order spectral clustering for complex networks",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107964",
    "abstract": "Spectral clustering (SC) is a popular approach for gaining insights from complex networks. Conventional SC focuses on second-order structures (e.g. edges) without direct consideration of higher-order structures (e.g. triangles). This has motivated SC extensions that directly consider higher-order structures. However, both approaches are limited to considering a single order. To address this issue, this paper proposes a novel Mixed-Order Spectral Clustering (MOSC) framework to model both second-order and third-order structures simultaneously. To model mixed-order structures, we propose two new methods based on Graph Laplacian (GL) and Random Walks (RW). MOSC-GL combines edge and triangle adjacency matrices, with theoretical performance guarantee. MOSC-RW combines first-order and second-order random walks for a probabilistic interpretation. Moreover, we design mixed-order cut criteria to enable existing SC methods to preserve mixed-order structures, and develop new mixed-order evaluation metrics for structure-level evaluation. Experiments on community detection and superpixel segmentation show (1) the superior performance of the MOSC methods over existing SC methods, (2) enhanced performance of conventional SC due to mixed-order cut criteria, and (3) new insights of output clusters offered by the mixed-order evaluation metrics.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321001515",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Economics",
      "Enhanced Data Rates for GSM Evolution",
      "Finance",
      "Graph",
      "Laplacian matrix",
      "Mathematics",
      "Order (exchange)",
      "Probabilistic logic",
      "Random walk",
      "Spectral clustering",
      "Statistics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Ge",
        "given_name": "Yan"
      },
      {
        "surname": "Peng",
        "given_name": "Pan"
      },
      {
        "surname": "Lu",
        "given_name": "Haiping"
      }
    ]
  },
  {
    "title": "Optic disc segmentation by U-net and probability bubble in abnormal fundus images",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107971",
    "abstract": "Segmenting optic disc (OD) in abnormal fundus images is a challenge task because of many distractions such as illumination variations, blurry boundary, occlusion of retinal vessels and big bright lesions. Data-driven deep learning is effective and robust to illumination variations, blurry boundary and occlusion in the normal fundus images but sensitive to big bright lesions in abnormal images. In this paper, an automatic OD segmentation method fusing U-net with model-driven probability bubble approach is proposed in abnormal fundus images. The probability bubble is conceived according to the position relationship between retinal vessels and OD, and the localization result is fused into the output layer of U-net through calculating the joint probability. The proposed method takes the advantage of the deep learning architecture and improves the architecture’s performance by including the model-driven position constraint when lack of sufficient training data. Experiments show that the proposed method successfully removes the distraction of bright lesions in abnormal fundus images and obtains a satisfying OD segmentation on three public databases: Kaggle, MESSIDOR and NIVE, and it outperforms existing methods with a very high accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321001588",
    "keywords": [
      "Artificial intelligence",
      "Boundary (topology)",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Fundus (uterus)",
      "Mathematical analysis",
      "Mathematics",
      "Medicine",
      "Ophthalmology",
      "Optic disc",
      "Pattern recognition (psychology)",
      "Retinal",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Fu",
        "given_name": "Yinghua"
      },
      {
        "surname": "Chen",
        "given_name": "Jie"
      },
      {
        "surname": "Li",
        "given_name": "Jiang"
      },
      {
        "surname": "Pan",
        "given_name": "Dongyan"
      },
      {
        "surname": "Yue",
        "given_name": "Xuezheng"
      },
      {
        "surname": "Zhu",
        "given_name": "Yiming"
      }
    ]
  },
  {
    "title": "Image stitching based on angle-consistent warping",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107993",
    "abstract": "Many warping methods for image stitching have been proposed to construct panoramic image mosaics free of artifacts. Existing methods heavily rely on coordinate correspondences between keypoints in stitching, which may not provide adequate constraints for alignment. In this paper, we discover and employ a new constraint — angle correspondences to address the above problem. The angle of a feature point represents the local directional structure of the point, which is an extension to its position and customarily ignored in image stitching. We propose to jointly consider the coordinates as well as the angles in keypoint correspondences. Such a strategy helps to generate a correct warping in the overlapping regions of the stitched image. In addition, we propose a novel constraint — mesh angle preservation to prevent undesired distortion in non-overlapping areas. Experiments in several challenging cases demonstrate that our method yields more accurate results with significantly less artifacts in comparison with state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321001801",
    "keywords": [
      "Amplifier",
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Constraint (computer-aided design)",
      "Distortion (music)",
      "Economics",
      "Feature (linguistics)",
      "Finance",
      "Geometry",
      "Image (mathematics)",
      "Image stitching",
      "Image warping",
      "Linguistics",
      "Mathematics",
      "Philosophy",
      "Point (geometry)",
      "Position (finance)"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Yinqi"
      },
      {
        "surname": "Zheng",
        "given_name": "Huicheng"
      },
      {
        "surname": "Ma",
        "given_name": "Yiyan"
      },
      {
        "surname": "Yan",
        "given_name": "Zhiwei"
      }
    ]
  },
  {
    "title": "Random vector functional link neural network based ensemble deep learning",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107978",
    "abstract": "In this paper, we propose deep learning frameworks based on the randomized neural network. Inspired by the principles of Random Vector Functional Link (RVFL) network, we present a deep RVFL network (dRVFL) with stacked layers. The parameters of the hidden layers of the dRVFL are randomly generated within a suitable range and kept fixed while the output weights are computed using the closed-form solution as in a standard RVFL network. We also propose an ensemble deep network (edRVFL) that can be regarded as a marriage of ensemble learning with deep learning. Unlike traditional ensembling approaches that require training several models independently from scratch, edRVFL is obtained by training a single dRVFL network once. Both dRVFL and edRVFL frameworks are generic and can be used with any RVFL variant. To illustrate this, we integrate the deep learning RVFL networks with a recently proposed sparse pre-trained RVFL (SP-RVFL). Experiments on 46 tabular UCI classification datasets and 12 sparse datasets demonstrate that the proposed deep RVFL networks outperform state-of-the-art deep feed-forward neural networks (FNNs).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321001655",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Deep learning",
      "Deep neural networks",
      "Ensemble learning",
      "Machine learning",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Shi",
        "given_name": "Qiushi"
      },
      {
        "surname": "Katuwal",
        "given_name": "Rakesh"
      },
      {
        "surname": "Suganthan",
        "given_name": "P.N."
      },
      {
        "surname": "Tanveer",
        "given_name": "M."
      }
    ]
  },
  {
    "title": "Graph matching as a graph convolution operator for graph neural networks",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.06.008",
    "abstract": "Convolutional neural networks (CNNs), in a few decades, have outperformed the existing state of the art methods in classification context. However, in the way they were formalised, CNNs are bound to operate on euclidean spaces. Indeed, convolution is a signal operation that are defined on euclidean spaces. This has restricted deep learning main use to euclidean-defined data such as sound or image. And yet, numerous computer application fields (among which network analysis, computational social science, chemo-informatics or computer graphics) induce non-euclideanly defined data such as graphs, networks or manifolds. In this paper we propose a new convolution neural network architecture, defined directly into graph space. Convolution and pooling operators are defined in graph domain thanks to a graph matching procedure between the input signal and a filter. We show its usability in a back-propagation context. Experimental results show that our model performance is at state of the art level on simple tasks. It shows robustness with respect to graph domain changes and improvement with respect to other euclidean and non-euclidean convolutional architectures.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521002087",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Graph",
      "Pattern recognition (psychology)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Martineau",
        "given_name": "Chloé"
      },
      {
        "surname": "Raveaux",
        "given_name": "Romain"
      },
      {
        "surname": "Conte",
        "given_name": "Donatello"
      },
      {
        "surname": "Venturini",
        "given_name": "Gilles"
      }
    ]
  },
  {
    "title": "Level set framework with transcendental constraint for robust and fast image segmentation",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107985",
    "abstract": "Though image segmentation models are plentiful and have many applications nowadays, it can be difficult to segment images with complex boundaries and serious intensity inhomogeneity. To some extent, the region-scalable fitting energy model can segment images suffering from intensity inhomogeneity since it considers image intensity as a function, but it relies on initial conditions dramatically. Nowadays, prior knowledge has been widely applied in image segmentation models, which can integrate automatic method and experts experience in one robust and fast segmentation model. In this paper we present a new model that can segment various images accurately by taking the advantages of the region-scalable fitting energy model and the advanced transcendental constraint from artificial experience. The proposed energy functional consists of a smooth length term, a target image data term and a transcendental constraint term. The transcendental constraint term plays a key role in the proposed model, which not only gives the accurate segmentation results but also provides us the chance to carry out the parallel computation. In the proposed-parallel model, the efficiency is improved a lot and the results become more precise compared with other methods. The split Bregman method is applied to minimize the energy functional. Furthermore, we present the convergence analysis and the time complexity analysis of our algorithm. Multiple experimental results and comparisons including parameters sensitivity discussion are shown to demonstrate the superiority of the proposed model such as high accuracy, robustness and efficiency.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321001722",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computation",
      "Computer science",
      "Computer vision",
      "Constraint (computer-aided design)",
      "Database",
      "Energy functional",
      "Gene",
      "Geometry",
      "Image (mathematics)",
      "Image segmentation",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Physics",
      "Quantum mechanics",
      "Robustness (evolution)",
      "Scalability",
      "Scale-space segmentation",
      "Segmentation",
      "Segmentation-based object categorization",
      "Term (time)"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Yunyun"
      },
      {
        "surname": "Wang",
        "given_name": "Ruofan"
      },
      {
        "surname": "Shu",
        "given_name": "Xiu"
      },
      {
        "surname": "Feng",
        "given_name": "Chong"
      },
      {
        "surname": "Xie",
        "given_name": "Ruicheng"
      },
      {
        "surname": "Jia",
        "given_name": "Wenjing"
      },
      {
        "surname": "Li",
        "given_name": "Chunming"
      }
    ]
  },
  {
    "title": "Robust feature matching using guided local outlier factor",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107986",
    "abstract": "Matching local features on two or more images is fundamental for many applications in the field of computer vision and pattern recognition. Identifying and rejecting mismatches is an important part in the framework of feature matching, due to the putative correspondences always contaminated by mismatches with the error-prone local feature detectors. In this paper, we introduce a novel method, namely Guided Local Outlier Factor (GLOF) for feature matching with gross mismatches under multi-granularity neighborhood structure-preserving. We first construct a tentative correspondence set by matching multi-features. Then, we identify and remove mismatches. Inspired by the anomaly detection technique, putative correspondences are assigned to a particular score, so abnormal instances, i.e., mismatches can be classified by a user-defined threshold. More specially, the neighborhood preserving guides the local searching procedure. Moreover, to eliminate the fluctuation of the matching results with different sizes of local neighbors, we use the multi-granularity algorithm to average out the deviation. Experimental results demonstrate that the introduced approach is superior to several state-of-the-art methods in terms of mismatch rejection on publicly available datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321001734",
    "keywords": [
      "Anomaly detection",
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Feature (linguistics)",
      "Feature extraction",
      "Granularity",
      "Linguistics",
      "Local outlier factor",
      "Matching (statistics)",
      "Mathematics",
      "Operating system",
      "Outlier",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Gang"
      },
      {
        "surname": "Chen",
        "given_name": "Yufei"
      }
    ]
  },
  {
    "title": "Multi-type relational clustering for enterprise cyber-security networks",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.05.021",
    "abstract": "Several cyber-security data sources are collected in enterprise networks providing relational information between different types of nodes in the network, namely computers, users and ports. This relational data can be expressed as adjacency matrices detailing inter-type relationships corresponding to relations between nodes of different types and intra-type relationships showing relationships between nodes of the same type. In this paper, we propose an extension of Non-Negative Matrix Tri-Factorisation (NMTF) to simultaneously cluster nodes based on their intra and inter-type relationships. Existing NMTF based clustering methods suffer from long computational times due to large matrix multiplications. In our approach, we enforce stricter cluster indicator constraints on the factor matrices to circumvent these issues. Additionally, to make our proposed approach less susceptible to variation in results due to random initialisation, we propose a novel initialisation procedure based on Non-Negative Double Singular Value Decomposition for multi-type relational clustering. Finally, a new performance measure suitable for assessing clustering performance on unlabelled multi-type relational data sets is presented. Our algorithm is assessed on both a simulated and real computer network against standard approaches showing its strong performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521002051",
    "keywords": [
      "Adjacency matrix",
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Cluster (spacecraft)",
      "Cluster analysis",
      "Computer network",
      "Computer science",
      "Data mining",
      "Ecology",
      "Eigenvalues and eigenvectors",
      "Graph",
      "Matrix decomposition",
      "Non-negative matrix factorization",
      "Physics",
      "Quantum mechanics",
      "Relational database",
      "Relational database management system",
      "Singular value decomposition",
      "Theoretical computer science",
      "Type (biology)"
    ],
    "authors": [
      {
        "surname": "Riddle-Workman",
        "given_name": "Elizabeth"
      },
      {
        "surname": "Evangelou",
        "given_name": "Marina"
      },
      {
        "surname": "Adams",
        "given_name": "Niall M."
      }
    ]
  },
  {
    "title": "Unconstrained end-to-end text reading with feature rectification",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.05.017",
    "abstract": "We propose an end-to-end trainable network that can simultaneously localize and recognize irregular text from images. Specifically, we find the feature incompatibility problem, which arises from the contradiction between detection and recognition tasks for feature extraction of the convolutional neural network, and propose to introduce the larger-scale features for the recognition part to improve the accuracy of recognition instead of using the same feature with the detection. To extract effective text features for perspective and curved text recognition, we propose a position-sensitive network to rectify the text proposal features in the recognition branch. The position-sensitive network, which is trained in a weak supervision way, takes the proposal detection feature as input and outputs the feature rectification information. Experiments demonstrate that the proposed method can achieve state-of-the-art or highly competitive performance compared with baselines on a number of benchmarks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521002002",
    "keywords": [
      "Algorithm",
      "Arithmetic",
      "Artificial intelligence",
      "Computer science",
      "Electrical engineering",
      "End-to-end principle",
      "Engineering",
      "Feature (linguistics)",
      "Linguistics",
      "Mathematics",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Reading (process)",
      "Rectification",
      "Speech recognition",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Du",
        "given_name": "Chen"
      },
      {
        "surname": "Wang",
        "given_name": "Yanna"
      },
      {
        "surname": "Wang",
        "given_name": "Chunheng"
      },
      {
        "surname": "Xiao",
        "given_name": "Baihua"
      },
      {
        "surname": "Shi",
        "given_name": "Cunzhao"
      }
    ]
  },
  {
    "title": "Penalized -regression-based bicluster localization",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107984",
    "abstract": "Biclustering (co-clustering, two-mode clustering), as one of the classical unsupervised learning methods, has been applied in many different fields in recent years. Different types of biclustering methods have been developed such as probabilistic methods, two-way clustering methods, variance minimization methods, and so on. However, few regression-based methods have been proposed to the best of our knowledge. Such methods have been applied in traditional clustering, which can improve both the computational efficiency and the clustering accuracy. In this paper, we present a penalized regression-based method for localizing the biclusters (PRbiclust). By imposing Truncated LASSO Penalty (TLP) and group TLP terms to penalize the column vectors and the row vectors in the regression model, the structure of biclusters in the data matrix is recovered. The model is formulated as an optimization problem with nonconvex penalties, and a computationally efficient algorithm is proposed to solve it. Convergence of the algorithm is proved. To extract the biclusters from the recovered data matrix, we propose a graph-based localization method. An evaluation criterion is also proposed to measure the efficiency of bicluster localization when noise entries exist. We apply the proposed method to both simulated datasets with different setups and a real dataset. Experiments show that this method can well capture the bicluster structure, and performs better than the existing works.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321001710",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biclustering",
      "CURE data clustering algorithm",
      "Cluster analysis",
      "Computer science",
      "Correlation clustering",
      "Data mining",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Probabilistic logic",
      "Regression",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Gao",
        "given_name": "Hanjia"
      },
      {
        "surname": "Bai",
        "given_name": "Zhengjian"
      },
      {
        "surname": "Gao",
        "given_name": "Weiguo"
      },
      {
        "surname": "Zhang",
        "given_name": "Shuqin"
      }
    ]
  },
  {
    "title": "Robust deep k -means: An effective and simple method for data clustering",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107996",
    "abstract": "Clustering aims to partition an input dataset into distinct groups according to some distance or similarity measurements. One of the most widely used clustering method nowadays is the k -means algorithm because of its simplicity and efficiency. In the last few decades, k -means and its various extensions have been formulated to solve the practical clustering problems. However, existing clustering methods are often presented in a single-layer formulation (i.e., shallow formulation). As a result, the mapping between the obtained low-level representation and the original input data may contain rather complex hierarchical information. To overcome the drawbacks of low-level features, deep learning techniques are adopted to extract deep representations and improve the clustering performance. In this paper, we propose a robust deep k -means model to learn the hidden representations associate with different implicit lower-level attributes. By using the deep structure to hierarchically perform k -means, the hierarchical semantics of data can be exploited in a layerwise way. Data samples from the same class are forced to be closer layer by layer, which is beneficial for clustering task. The objective function of our model is derived to a more trackable form such that the optimization problem can be tackled more easily and the final robust results can be obtained. Experimental results over 12 benchmark data sets substantiate that the proposed model achieves a breakthrough in clustering performance, compared with both classical and state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321001837",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Epistemology",
      "Geodesy",
      "Geography",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Simple (philosophy)"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Shudong"
      },
      {
        "surname": "Kang",
        "given_name": "Zhao"
      },
      {
        "surname": "Xu",
        "given_name": "Zenglin"
      },
      {
        "surname": "Liu",
        "given_name": "Quanhui"
      }
    ]
  },
  {
    "title": "MASTER: Multi-aspect non-local network for scene text recognition",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107980",
    "abstract": "Attention-based scene text recognizers have gained huge success, which leverages a more compact intermediate representation to learn 1d- or 2d- attention by a RNN-based encoder-decoder architecture. However, such methods suffer from attention-driftproblem because high similarity among encoded features leads to attention confusion under the RNN-based local attention mechanism. Moreover, RNN-based methods have low efficiency due to poor parallelization. To overcome these problems, we propose the MASTER, a self-attention based scene text recognizer that (1) not only encodes the input-output attention but also learns self-attention which encodes feature-feature and target-target relationships inside the encoder and decoder and (2) learns a more powerful and robust intermediate representation to spatial distortion, and (3) owns a great training efficiency because of high training parallelization and a high-speed inference because of an efficient memory-cache mechanism. Extensive experiments on various benchmarks demonstrate the superior performance of our MASTER on both regular and irregular scene text. Pytorch code can be found at https://github.com/wenwenyu/MASTER-pytorch, and Tensorflow code can be found at https://github.com/jiangxiluning/MASTER-TF.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321001679",
    "keywords": [
      "Algorithm",
      "Amplifier",
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Cache",
      "Code (set theory)",
      "Computer network",
      "Computer science",
      "Decoding methods",
      "Deep learning",
      "Distortion (music)",
      "Encoder",
      "Encoding (memory)",
      "Feature (linguistics)",
      "Feature learning",
      "Image (mathematics)",
      "Inference",
      "Law",
      "Linguistics",
      "Operating system",
      "Parallel computing",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Programming language",
      "Representation (politics)",
      "Set (abstract data type)",
      "Similarity (geometry)",
      "Source code"
    ],
    "authors": [
      {
        "surname": "Lu",
        "given_name": "Ning"
      },
      {
        "surname": "Yu",
        "given_name": "Wenwen"
      },
      {
        "surname": "Qi",
        "given_name": "Xianbiao"
      },
      {
        "surname": "Chen",
        "given_name": "Yihao"
      },
      {
        "surname": "Gong",
        "given_name": "Ping"
      },
      {
        "surname": "Xiao",
        "given_name": "Rong"
      },
      {
        "surname": "Bai",
        "given_name": "Xiang"
      }
    ]
  },
  {
    "title": "Low-resolution face recognition in resource-constrained environments",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.05.009",
    "abstract": "Although Deep Neural Networks (DNNs) have achieved tremendous success in the face recognition task, utilizing them in resource-constrained environments with limited networking and computing is challenging. Such environments often demand a small model capable of being effectively trained on a small number of labeled training data, with low training complexity, and low-resolution input images. To address these challenges, we adopt an emerging machine learning methodology called Successive Subspace Learning (SSL) to propose LRFRHop, a high-performance data-efficient low-resolution face recognition model for resource-constrained environments. SSL offers an explainable non-parametric feature extraction submodel that flexibly trades the model size for the verification performance. Its training complexity is significantly lower than DNN-based models since it is trained in a one-pass feedforward manner without backpropagation. Furthermore, active learning can be conveniently incorporated to reduce the labeling cost. We demonstrate the effectiveness of LRFRHop by conducting experiments on the LFW and the CMU Multi-PIE datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521001872",
    "keywords": [],
    "authors": [
      {
        "surname": "Rouhsedaghat",
        "given_name": "Mozhdeh"
      },
      {
        "surname": "Wang",
        "given_name": "Yifan"
      },
      {
        "surname": "Hu",
        "given_name": "Shuowen"
      },
      {
        "surname": "You",
        "given_name": "Suya"
      },
      {
        "surname": "Kuo",
        "given_name": "C.-C. Jay"
      }
    ]
  },
  {
    "title": "SRAGL-AWCL: A two-step multi-view clustering via sparse representation and adaptive weighted cooperative learning",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107987",
    "abstract": "Sparse representation and cooperative learning are two representative technologies in the field of multi-view spectral clustering. The former can effectively extract features of multiple views by the removal of redundant information contained in each view. The latter can incorporate the diversity of each view. However, traditional sparse representation and cooperative learning algorithms are inadequate in preserving the internal geometric features of data by manifold regularization. In fact, general approaches rarely consider the similarities between the internal graph structures of individual views. Moreover, to achieve the optimal global feature learning, we present a novel two-step multi-view spectral clustering strategy, which combines the proposed sparse representation by adaptive graph learning with adaptive weighted cooperative learning. In the first step, the proposed matrix factorization by manifold regularization can strengthen the sparse features clustering discrimination of samples of each view. Specifically, the synchronization optimization method by introducing adaptive graph learning can better retain its internal complete structure of each view. This ensures the structure correlation of views through the usage of the sparse matrix and the optimal graph similarity matrix. In the second step, the adaptive weighted cooperative learning is performed on each view to get a global optimized matrix. In order to ensure that the global matrix is associated with various view features, graph learning is also performed on the global matrix. Experiment results on several multi-view datasets and single-view datasets show that the proposed method significantly outperformed the state-of-the-art algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321001746",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Dimensionality reduction",
      "Eigenvalues and eigenvectors",
      "Feature learning",
      "Gaussian",
      "Graph",
      "Machine learning",
      "Mathematics",
      "Matrix decomposition",
      "Nonlinear dimensionality reduction",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Regularization (linguistics)",
      "Semi-supervised learning",
      "Sparse approximation",
      "Sparse matrix",
      "Spectral clustering",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Tan",
        "given_name": "Junpeng"
      },
      {
        "surname": "Yang",
        "given_name": "Zhijing"
      },
      {
        "surname": "Cheng",
        "given_name": "Yongqiang"
      },
      {
        "surname": "Ye",
        "given_name": "Jielin"
      },
      {
        "surname": "Wang",
        "given_name": "Bing"
      },
      {
        "surname": "Dai",
        "given_name": "Qingyun"
      }
    ]
  },
  {
    "title": "Analysis of a parallel MCMC algorithm for graph coloring with nearly uniform balancing",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.05.014",
    "abstract": "We propose the analysis of a scalable parallel MCMC algorithm for graph coloring aimed at balancing the color class sizes, provided that a suitable number of colors is made available. Firstly, it is shown that the Markov chain converges to the target distribution by repeatedly sampling from suitable proposed distributions over the neighboring colors of each node, independently and hence in parallel manner. We prove that the number of conflicts in the improper colorings genereted thoughout the iterations of the algorithm rapidly converges in probability to 0. As for the balancing, given to the complexity of the distributions involved, we propose a qualitative analysis about the balancing level achieved. Based on a collection of multinoulli distributions arising from the color occurrences within every node neighborhood, we provide some evidence about the character of the final color balancing, which results to be nearly uniform over the color classes. Some numerical simulations on big social graphs confirm the fast convergence and the balancing trend, which is validated through a statistical hypothesis test eventually.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521001963",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Bayesian probability",
      "Computer science",
      "Convergence (economics)",
      "Database",
      "Economic growth",
      "Economics",
      "Geometry",
      "Graph",
      "Graph coloring",
      "Grid",
      "Load balancing (electrical power)",
      "Machine learning",
      "Markov chain",
      "Markov chain Monte Carlo",
      "Mathematics",
      "Scalability",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Conte",
        "given_name": "Donatello"
      },
      {
        "surname": "Grossi",
        "given_name": "Giuliano"
      },
      {
        "surname": "Lanzarotti",
        "given_name": "Raffaella"
      },
      {
        "surname": "Lin",
        "given_name": "Jianyi"
      },
      {
        "surname": "Petrini",
        "given_name": "Alessandro"
      }
    ]
  },
  {
    "title": "An efficient and locality-oriented Hausdorff distance algorithm: Proposal and analysis of paradigms and implementations",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107989",
    "abstract": "Hausdorff distance (HD) is a popular similarity metric used in the comparison of images or 3D volumes. Although popular, its main weakness is computing power consumption, being one of the slowest set distances. In this work, a novel, parallel and locality-oriented Hausdorff distance implementation is proposed. Novel as it is the first time in the literature that an actual algorithmic implementation using morphological dilations is proposed and thoroughly evaluated. Parallel, as it is more robust in terms of parallelization than the state-of-the-art algorithm and local as it has an intrinsic sensitivity to voxels that are closer in space. This proposal can be faster than the state-of-the-art in several practical cases such as in medical imaging registrations (up to 8 times faster on average in one of the CPU experiments) and is faster in the worst-case (up to 22337 times faster in one of the CPU experiments). Worst-case scenarios and high resolution volumes also favor the proposed approach. Throughout the work, several sequential and parallel CPU and GPU implementations are evaluated and compared.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032100176X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Hausdorff distance",
      "Implementation",
      "Linguistics",
      "Locality",
      "Philosophy",
      "Programming language",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Rodrigues",
        "given_name": "Érick Oliveira"
      }
    ]
  },
  {
    "title": "Integrating information theory and adversarial learning for cross-modal retrieval",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107983",
    "abstract": "Accurately matching visual and textual data in cross-modal retrieval has been widely studied in the multimedia community. To address these challenges posited by the heterogeneity gap and the semantic gap, we propose integrating Shannon information theory and adversarial learning. In terms of the heterogeneity gap, we integrate modality classification and information entropy maximization adversarially. For this purpose, a modality classifier (as a discriminator) is built to distinguish the text and image modalities according to their different statistical properties. This discriminator uses its output probabilities to compute Shannon information entropy, which measures the uncertainty of the modality classification it performs. Moreover, feature encoders (as a generator) project uni-modal features into a commonly shared space and attempt to fool the discriminator by maximizing its output information entropy. Thus, maximizing information entropy gradually reduces the distribution discrepancy of cross-modal features, thereby achieving a domain confusion state where the discriminator cannot classify two modalities confidently. To reduce the semantic gap, Kullback-Leibler (KL) divergence and bi-directional triplet loss are used to associate the intra- and inter-modality similarity between features in the shared space. Furthermore, a regularization term based on KL-divergence with temperature scaling is used to calibrate the biased label classifier caused by the data imbalance issue. Extensive experiments with four deep models on four benchmarks are conducted to demonstrate the effectiveness of the proposed approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321001709",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Detector",
      "Discriminator",
      "Entropy (arrow of time)",
      "Information theory",
      "Joint entropy",
      "Kullback–Leibler divergence",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Maximization",
      "Mutual information",
      "Pattern recognition (psychology)",
      "Physics",
      "Principle of maximum entropy",
      "Quantum mechanics",
      "Statistics",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Wei"
      },
      {
        "surname": "Liu",
        "given_name": "Yu"
      },
      {
        "surname": "Bakker",
        "given_name": "Erwin M."
      },
      {
        "surname": "Lew",
        "given_name": "Michael S."
      }
    ]
  },
  {
    "title": "GR-RNN: Global-context residual recurrent neural networks for writer identification",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107975",
    "abstract": "This paper presents an end-to-end neural network system to identify writers through handwritten word images, which jointly integrates global-context information and a sequence of local fragment-based features. The global-context information is extracted from the tail of the neural network by a global average pooling step. The sequence of local and fragment-based features is extracted from a low-level deep feature map which contains subtle information about the handwriting style. The spatial relationship between the sequence of fragments is modeled by the recurrent neural network (RNN) to strengthen the discriminative ability of the local fragment features. We leverage the complementary information between the global-context and local fragments, resulting in the proposed global-context residual recurrent neural network (GR-RNN) method. The proposed method is evaluated on four public data sets and experimental results demonstrate that it can provide state-of-the-art performance. In addition, the neural networks trained on gray-scale images provide better results than neural networks trained on binarized and contour images, indicating that texture information plays an important role for writer identification. The source code is available: https://github.com/shengfly/writer-identification.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032100162X",
    "keywords": [
      "Algorithm",
      "Archaeology",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Context (archaeology)",
      "Deep learning",
      "Discriminative model",
      "Geography",
      "Leverage (statistics)",
      "Pattern recognition (psychology)",
      "Pooling",
      "Recurrent neural network",
      "Residual"
    ],
    "authors": [
      {
        "surname": "He",
        "given_name": "Sheng"
      },
      {
        "surname": "Schomaker",
        "given_name": "Lambert"
      }
    ]
  },
  {
    "title": "Cost-sensitive design of quadratic discriminant analysis for imbalanced data",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.06.002",
    "abstract": "Learning from imbalanced training data represents a major challenge that has triggered recent interest from both academia and industry. As far as classification is concerned, it has been observed that several algorithms provide low accuracy when designed out of imbalanced data sets, among which regularized quadratic discriminant analysis (R-QDA) is the most illustrative example. Based on recent asymptotic findings, the study in [2] has brought a better understanding of the reasons behind the excessive sensitivity of R-QDA to data imbalance, which allowed for the development of a novel quadratic based classifier that presents higher robustness to such scenarios. However, the selection of the parameters for this classifier relied on the minimization of the overall classification error rate, which is not considered as a relevant performance metric in extremely imbalanced training data. In this work, we follow a multi-model selection approach for the selection of the parameters of the classifier proposed in [2]. Such an approach involves solving a multi-objective optimization problem, but, contrary to related works, we do not resort to evolutionary algorithms to solve this problem but rather to a solely training data dependent technique based on asymptotic approximations for the classification performances. This allows us to transform the multi-objective optimization problem into a scalar optimization problem. Our proposed approach presents the main advantages of being more accurate and less complex, avoiding the need for computationally expensive cross-validation procedures. Its interest goes beyond the quadratic discriminant analysis, paving the way towards a principled method for the design of classification algorithms in imbalanced data scenarios.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521001896",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Discriminant",
      "Geometry",
      "Linear discriminant analysis",
      "Machine learning",
      "Mathematics",
      "Optimal discriminant analysis",
      "Pattern recognition (psychology)",
      "Quadratic classifier",
      "Quadratic equation",
      "Statistics",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Bejaoui",
        "given_name": "Amine"
      },
      {
        "surname": "Elkhalil",
        "given_name": "Khalil"
      },
      {
        "surname": "Kammoun",
        "given_name": "Abla"
      },
      {
        "surname": "Alouini",
        "given_name": "Mohamed-Slim"
      },
      {
        "surname": "Al-Naffouri",
        "given_name": "Tareq"
      }
    ]
  },
  {
    "title": "Deep center-based dual-constrained hashing for discriminative face image retrieval",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107976",
    "abstract": "With the advantages of low storage cost and extremely fast retrieval speed, deep hashing methods have attracted much attention for image retrieval recently. However, large-scale face image retrieval with significant intra-class variations is still challenging. Neither existing pairwise/triplet labels-based nor softmax classification loss-based deep hashing works can generate compact and discriminative binary codes. Considering these issues, we propose a center-based framework integrating end-to-end hashing learning and class centers learning simultaneously. The framework minimizes the intra-class variance by clustering intra-class samples into a learnable class center. To strengthen inter-class separability, it additionally imposes a novel regularization term to enlarge the Hamming distance between pairwise class centers. Moreover, a simple yet effective regression matrix is introduced to encourage intra-class samples to generate the same binary codes, which further enhances the hashing codes compactness. Experiments on four large-scale datasets show the proposed method outperforms state-of-the-art baselines under various code lengths and commonly-used evaluation metrics.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321001631",
    "keywords": [
      "Algorithm",
      "Arithmetic",
      "Artificial intelligence",
      "Binary code",
      "Binary number",
      "Block code",
      "Computer science",
      "Computer security",
      "Decoding methods",
      "Discriminative model",
      "Hamming code",
      "Hamming distance",
      "Hamming space",
      "Hash function",
      "Image (mathematics)",
      "Image retrieval",
      "Mathematics",
      "Pairwise comparison",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Ming"
      },
      {
        "surname": "Zhe",
        "given_name": "Xuefei"
      },
      {
        "surname": "Chen",
        "given_name": "Shifeng"
      },
      {
        "surname": "Yan",
        "given_name": "Hong"
      }
    ]
  },
  {
    "title": "Diverse part attentive network for video-based person re-identification",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.05.020",
    "abstract": "Attention mechanisms have achieved success in video-based person re-identification (re-ID). However, current global attentions tend to focus on the most salient parts, e.g., clothes, and ignore other subtle but valuable cues, e.g., hair, bag, and shoes. They still do not make full use of valuable information from diverse parts of human bodies. To tackle this issue, we propose a Diverse Part Attentive Network (DPAN) to exploit discriminative and diverse body cues. The framework consists of two modules: spatial diverse part attention and temporal diverse part attention. The spatial module utilizes channel grouping to exploit diverse parts of human bodies including salient and subtle parts. The temporal module aims to learn diverse weights for fusing learned features. Besides, this framework is lightweight, which introduces marginal parameters and computational complexities. Extensive experiments were conducted on three popular benchmarks, i.e. iLIDS-VID, PRID2011 and MARS. Our method achieves competitive performance on these datasets compared with state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552100204X",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Computer science",
      "Computer security",
      "Discriminative model",
      "Engineering",
      "Exploit",
      "Focus (optics)",
      "Identification (biology)",
      "Optics",
      "Physics",
      "Salient",
      "Systems engineering",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Shu",
        "given_name": "Xiujun"
      },
      {
        "surname": "Li",
        "given_name": "Ge"
      },
      {
        "surname": "Wei",
        "given_name": "Longhui"
      },
      {
        "surname": "Zhong",
        "given_name": "Jia-Xing"
      },
      {
        "surname": "Zang",
        "given_name": "Xianghao"
      },
      {
        "surname": "Zhang",
        "given_name": "Shiliang"
      },
      {
        "surname": "Wang",
        "given_name": "Yaowei"
      },
      {
        "surname": "Liang",
        "given_name": "Yongsheng"
      },
      {
        "surname": "Tian",
        "given_name": "Qi"
      }
    ]
  },
  {
    "title": "Discriminative deep semi-nonnegative matrix factorization network with similarity maximization for unsupervised feature learning",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.06.013",
    "abstract": "Deep Semi-NMF (DSN), which learns hierarchical representations by stacking multiple layers Semi-NMF, shows competitive performance in unsupervised data analysis. However, the features learned from DSN always lack of representativity and discriminativity. In this paper, we build a novel Deep Semi-NMF network (DSNnet) to address the issues of DSN. Specifically, DSNnet contains multiple fully-connected layers, in which the activation function of each layer adopts Smoothly Clipped Absolute Deviation (SCAD). The non-negative hidden features are computed forwardly, while the network parameters are updated by the stochastic gradient descent method. Moreover, to enhance the discriminativity of features, we suggest simultaneously minimizing the reconstruction error of input and output, and maximizing the similarity between input and learned features. The proposed similarity measurement, which consists of global geometric similarity and local pointwise similarity, encourages the compactness between similar points and separateness between dissimilar points in the feature space, and is beneficial to preserve intrinsic information of original data. Extensive experiments conducted on several datasets illustrate the superiority of the proposed approach in comparison with state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521002191",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Discriminative model",
      "Eigenvalues and eigenvectors",
      "Feature (linguistics)",
      "Feature learning",
      "Image (mathematics)",
      "Linguistics",
      "Mathematical optimization",
      "Mathematics",
      "Matrix decomposition",
      "Maximization",
      "Non-negative matrix factorization",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Similarity (geometry)",
      "Similarity learning",
      "Unsupervised learning"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Wei"
      },
      {
        "surname": "Chen",
        "given_name": "Feiyu"
      },
      {
        "surname": "Ge",
        "given_name": "Yongxin"
      },
      {
        "surname": "Huang",
        "given_name": "Sheng"
      },
      {
        "surname": "Zhang",
        "given_name": "Xiaohong"
      },
      {
        "surname": "Yang",
        "given_name": "Dan"
      }
    ]
  },
  {
    "title": "Heuristics-based learning approach for choquistic regression models",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.05.013",
    "abstract": "Recently predictive models based on the Choquet integral have been applied successfully in machine learning and multi criteria decision making context. The ability of the Choquet integral to capture non-linear dependencies and its comprehensibility make it a very appealing tool. Yet, its complexity is often a barrier to estimate model parameters. In fact, the number of monotonicity constraints grows exponentially as the number of feature increases. This study addresses a heuristic approach to learn parameters underlying the choquistic regression model. In this regard, this study compares the gain of the proposed approach versus the original formalism of the choquistic regression. In addition, the run-time comparison in the experimental study is presented.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552100194X",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Choquet integral",
      "Computer science",
      "Formalism (music)",
      "Fuzzy logic",
      "Heuristic",
      "Heuristics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Monotonic function",
      "Musical",
      "Regression",
      "Statistics",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Fallah Tehrani",
        "given_name": "Ali"
      }
    ]
  },
  {
    "title": "Two metrics for attributed hypergraphs",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.06.007",
    "abstract": "Modern quantitative challenges require to tackle problems on increasingly complex systems in which the relationships between the comprised entities cannot be modelled in a simple pairwise fashion, as graphs do. Such approximation of higher-order relations may lead to a substantial loss of information, hence the need to use more general models than graphs. The most natural choice is to use hypergraphs, discrete structures able to capture k -adic relationships among the entities participating in the problem, modelled as vertices, by grouping them in non-empty sets which constitute the hyperedges of the hypergraph. Since one of the most desirable abilities in this context is to quantify the difference between two such high-order systems, devising distance metrics between hypergraphs becomes of the utmost importance. In this paper, we aim at tackling precisely this problem. Motivated by our previous work on graphs, we propose two distance measures between attributed hypergraphs and we prove that they satisfy the properties of a metric. Both metrics are based on the notion of the maximal common subhypergraph.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521002026",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Mathematics"
    ],
    "authors": [
      {
        "surname": "Smaniotto",
        "given_name": "Sebastiano"
      },
      {
        "surname": "Pelillo",
        "given_name": "Marcello"
      }
    ]
  },
  {
    "title": "Feature selection with kernelized multi-class support vector machine",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107988",
    "abstract": "Feature selection is an important procedure in machine learning because it can reduce the complexity of the final learning model and simplify the interpretation. In this paper, we propose a novel non-linear feature selection method that targets multi-class classification problems in the framework of support vector machines. The proposed method is achieved using a kernelized multi-class support vector machine with a fast version of recursive feature elimination. The proposed method selects features that work well for all classes, as the involved classifier simultaneously constructs multiple decision functions that separates each class from the others. We formulate the classifier as a large optimisation problem, and iteratively solve one decision function at a time, leading to a lower computational time complexity than when solving the large optimisation problem directly. The coefficients of the classifier are then used as a ranking criterion in the accelerated recursive feature elimination by adding batch elimination and a rechecking process. Experimental results on several datasets demonstrate the superior performance of the proposed feature selection method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321001758",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Feature selection",
      "Feature vector",
      "Linear classifier",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Ranking (information retrieval)",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Guo",
        "given_name": "Yinan"
      },
      {
        "surname": "Zhang",
        "given_name": "Zirui"
      },
      {
        "surname": "Tang",
        "given_name": "Fengzhen"
      }
    ]
  },
  {
    "title": "Document-level relation extraction via graph transformer networks and temporal convolutional networks",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.06.012",
    "abstract": "Relation Extraction (RE) aims at extracting meaningful relation facts between entities in texts. It is an important semantic processing task in the field of natural language processing (NLP) and has many applications. Traditional RE focuses on extracting entity relationships from a single input sentence. Recently, the research scope has been extended from sentence level to document level. However, compared with sentence-level RE, document-level RE, which needs to identify the inter-sentence relations from entities scattered in different sentences, is more complex and still lacks of solutions. To solve this problem, we propose a novel document-level RE method based on Heterogeneous Graph Neural Networks in this paper. Concretely, to obtain token embeddings containing long-distance dependency signals well, we encode the document with Temporal Convolutional Networks, whose dilated convolution and residual structure allow the effective and efficient preservation of historical information. To better describe the interaction between different elements, we construct the input documents as heterogeneous graphs with different node and edge types and utilize Graph Transformer Networks to generate semantic paths. Numerical experiments on two document-level biomedical datasets demonstrate the effectiveness of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552100218X",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer security",
      "Convolutional neural network",
      "ENCODE",
      "Gene",
      "Graph",
      "Information extraction",
      "Natural language",
      "Natural language processing",
      "Natural language understanding",
      "Physics",
      "Quantum mechanics",
      "Relationship extraction",
      "Security token",
      "Semantic role labeling",
      "Sentence",
      "Theoretical computer science",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Shi",
        "given_name": "Yong"
      },
      {
        "surname": "Xiao",
        "given_name": "Yang"
      },
      {
        "surname": "Quan",
        "given_name": "Pei"
      },
      {
        "surname": "Lei",
        "given_name": "MingLong"
      },
      {
        "surname": "Niu",
        "given_name": "Lingfeng"
      }
    ]
  },
  {
    "title": "Cross-scene foreground segmentation with supervised and unsupervised model communication",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107995",
    "abstract": "In this paper 1 1 Dong Liang, Bin Kang and Xinyu Liu contributed equally to this work. , we investigate cross-scene video foreground segmentation via supervised and unsupervised model communication. Traditional unsupervised background subtraction methods often face the challenging problem of updating the statistical background model online. In contrast, supervised foreground segmentation methods, such as those that are based on deep learning, rely on large amounts of training data, thereby limiting their cross-scene performance. Our method leverages segmented masks from a cross-scene trained deep model (spatio-temporal attention model (STAM), pyramid scene parsing network (PSPNet), or DeepLabV3+) to seed online updates for the statistical background model (CPB), thereby refining the foreground segmentation. More flexible than methods that require scene-specific training and more data-efficient than unsupervised models, our method outperforms state-of-the-art approaches on CDNet2014, WallFlower, and LIMU according to our experimental results. The proposed framework can be integrated into a video surveillance system in a plug-and-play form to realize cross-scene foreground segmentation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321001825",
    "keywords": [
      "Artificial intelligence",
      "Background subtraction",
      "Computer science",
      "Computer vision",
      "Contrast (vision)",
      "Deep learning",
      "Geometry",
      "Image segmentation",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pixel",
      "Pyramid (geometry)",
      "Segmentation",
      "Statistical model"
    ],
    "authors": [
      {
        "surname": "Liang",
        "given_name": "Dong"
      },
      {
        "surname": "Kang",
        "given_name": "Bin"
      },
      {
        "surname": "Liu",
        "given_name": "Xinyu"
      },
      {
        "surname": "Gao",
        "given_name": "Pan"
      },
      {
        "surname": "Tan",
        "given_name": "Xiaoyang"
      },
      {
        "surname": "Kaneko",
        "given_name": "Shun’ichi"
      }
    ]
  },
  {
    "title": "Visual question answering in the medical domain based on deep learning approaches: A comprehensive study",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.07.002",
    "abstract": "Visual Question Answering (VQA) in the medical domain has attracted more attention from research communities in the last few years due to its various applications. This paper investigates several deep learning approaches in building a medical VQA system based on ImageCLEF’s VQA-Med dataset, which consists of about 4K images with about 15K question-answer pairs. Due to the wide variety of the images and questions included in this dataset, the proposed model is a hierarchical one consisting of many sub-models, each tailored to handle certain questions. For that, a special model is built to classify the questions into four categories, where each category is handled by a separate sub-model. At their core, all of these models consist of pre-trained Convolution Neural Networks (CNN). In order to get the best results, extensive experiments are performed and various techniques are employed including Data Augmentation (DA), Multi-Task Learning (MTL), Global Average Pooling (GAP), Ensembling, and Sequence to Sequence (Seq2Seq) models. Overall, the final model achieves 60.8 accuracy and 63.4 BLEU score, which are competitive with the state-of-the-art results despite using less demanding and simpler sub-models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521002348",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer science",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Deep learning",
      "Domain (mathematical analysis)",
      "Economics",
      "Genetics",
      "Information retrieval",
      "Machine learning",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Natural language processing",
      "Pooling",
      "Question answering",
      "Sequence (biology)",
      "Task (project management)",
      "Variety (cybernetics)"
    ],
    "authors": [
      {
        "surname": "Al-Sadi",
        "given_name": "Aisha"
      },
      {
        "surname": "Al-Ayyoub",
        "given_name": "Mahmoud"
      },
      {
        "surname": "Jararweh",
        "given_name": "Yaser"
      },
      {
        "surname": "Costen",
        "given_name": "Fumie"
      }
    ]
  },
  {
    "title": "PCLoss: Fashion Landmark Estimation with Position Constraint Loss",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108028",
    "abstract": "Fashion landmark estimation aims at locating functional key points of clothes, which has wide potential applications in electronic commerce. However, due to the occlusion and weak outline information, landmark estimation occurs outliers and duplicate detection problems. To alleviate these issues, we propose Position Constraint Loss (PCLoss) to constrain error landmark locations by utilizing the position relationship of landmarks. Specifically, PCLoss adds a regularization term for each landmark to regularize their relative positions, and it can be easily applied to both regression and heatmap based methods without extra computation during inference. Unlike existing approaches that propagate landmark information between feature layers by specific network structures, PCLoss introduces position relations of landmarks in the label space without modifying the network structure. In addition, we leverage the skeleton-like relation of clothing to further strengthen position constraints between landmarks. Extensive experimental results on DeepFashion, FLD and FashionAI demonstrate that our methods can effectively increase the performance of mainstream frameworks by a large margin. We also explore the effectiveness of PCLoss on human pose estimation task, and the experimental results on COCO 2017 prove the generality of our methods on other key point estimation tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321002156",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Constraint (computer-aided design)",
      "Data mining",
      "Economics",
      "Finance",
      "Geometry",
      "Inference",
      "Landmark",
      "Leverage (statistics)",
      "Mathematics",
      "Outlier",
      "Pattern recognition (psychology)",
      "Pose",
      "Position (finance)"
    ],
    "authors": [
      {
        "surname": "Song",
        "given_name": "Meijia"
      },
      {
        "surname": "Liu",
        "given_name": "Hong"
      },
      {
        "surname": "Shi",
        "given_name": "Wei"
      },
      {
        "surname": "Li",
        "given_name": "Xia"
      }
    ]
  },
  {
    "title": "Polarization image fusion with self-learned fusion strategy",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108045",
    "abstract": "Polarization image fusion aims to integrate intensity and degree of linear polarization images into one with more details, which is beneficial to improve the ability of targets detection under complex background. The fusion strategies in conventional methods are designed in a hand-crafted way and not robust to different fusion tasks. In this paper, we propose a novel and deep network to address the polarization image fusion issue with self-learned strategy. The network consists of Encoder, Fusion, and Decoder layers. Feature maps extracted by Encoder are fused, then fed into Decoder to generate fused images. Besides, a novel loss function is adopted to train the network in an unsupervised way, without ground truth of fused images. To verify the advantage, the network trained on polarization images is also used to infrared and visible images fusion, and multi-focus image fusion. Experimental results showed that our method outperforms several state-of-the-art methods in terms of visual quality and quantitative measurement. The proposed fused method can be applied in the military and civilian fields such as camouflage and hidden targets detection, medical diagnosis, and environmental monitoring.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321002326",
    "keywords": [
      "Artificial intelligence",
      "Camouflage",
      "Computer science",
      "Computer vision",
      "Encoder",
      "Fusion",
      "Fusion rules",
      "Ground truth",
      "Image (mathematics)",
      "Image fusion",
      "Linguistics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Junchao"
      },
      {
        "surname": "Shao",
        "given_name": "Jianbo"
      },
      {
        "surname": "Chen",
        "given_name": "Jianlai"
      },
      {
        "surname": "Yang",
        "given_name": "Degui"
      },
      {
        "surname": "Liang",
        "given_name": "Buge"
      }
    ]
  },
  {
    "title": "Bayesian damage recognition in document images based on a joint global and local homogeneity model",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108034",
    "abstract": "Physical damages (such as torn-offs and scratches) are commonly seen in historical documents. Recognition of such damages is currently absent in digitization-and-information-extraction (DIE) systems but crucial for automatic document comprehension and exploitation. In this paper we propose a generic damage recognition (DR) method based on a joint global and local modeling of the text homogeneity (TH) pattern exhibited in document images. More specifically, a connected component (CC) based formulation is developed as a global homogeneity measure, where TH is characterized using a probabilistic graph model for a coarse recognition of damaged regions. A multi-resolution analysis (MRA) of TH is further developed for a granular within-CC recognition of damage pixels, where the disparity between damage and text pixels is characterized by exploiting neighborhood transitions. This enables the formulation of a local homogeneity measure, where the neighborhood transition around an individual pixel is modeled using the propagation of the approximation coefficients of a stationary wavelet transform (SWT). The proposed global and local homogeneity measures are integrated as a joint likelihood in a Bayesian model with a Markov random field (MRF) prior, where DR is formulated as a maximum a posterior (MAP) inference which is addressed using Markov Chain Monte Carlo (MCMC) sampling. The resulting algorithm is tested on a set of real-life historical newspaper images containing damages of varying size and shape. The performance of the algorithm is evaluated using both F-measures and the Intersection-over-Union (IoU) metric, where test results demonstrate the promising potential of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321002211",
    "keywords": [
      "Artificial intelligence",
      "Bayesian probability",
      "Computer science",
      "Homogeneity (statistics)",
      "Image segmentation",
      "Machine learning",
      "Markov chain",
      "Markov chain Monte Carlo",
      "Markov random field",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pixel",
      "Probabilistic logic",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Lu",
        "given_name": "Tan"
      },
      {
        "surname": "Dooms",
        "given_name": "Ann"
      }
    ]
  },
  {
    "title": "EFNet: Enhancement-Fusion Network for Semantic Segmentation",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108023",
    "abstract": "Semantic segmentation is a challenging and important task in computer vision. Convolutional neural networks (CNNs) have demonstrated their outstanding performances on such dense classification tasks. Most recent segmentation networks mainly focus on feature extraction for one single input image, while paying little attention to facilitating the segmentation by image manipulation or enhancement. In this paper, we design an enhancement-fusion network (EFNet), which aims at enhancing an input image for more diversified features to boost the following task of pixel-wise labeling. Specifically, the enhancement modules are trained to produce multiple enhanced images. Afterwards, the fusion module selectively attends on such images and fuses them to yield one new image. The proposed EFNet can be directly and flexibly integrated as an auxiliary network with state-of-the-art semantic segmentation networks, while maintaining the end-to-end training manner. Extensive results on benchmark datasets corroborate that the combination of the EFNet and the CNN-based semantic segmentation networks significantly improves the segmentation performance compared with the original segmentation networks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321002107",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Economics",
      "Feature (linguistics)",
      "Focus (optics)",
      "Geodesy",
      "Geography",
      "Image segmentation",
      "Linguistics",
      "Management",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Scale-space segmentation",
      "Segmentation",
      "Segmentation-based object categorization",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Zhijie"
      },
      {
        "surname": "Song",
        "given_name": "Ran"
      },
      {
        "surname": "Duan",
        "given_name": "Peng"
      },
      {
        "surname": "Li",
        "given_name": "Xiaolei"
      }
    ]
  },
  {
    "title": "Average Localised Proximity: A new data descriptor with good default one-class classification performance",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107991",
    "abstract": "One-class classification is a challenging subfield of machine learning in which so-called data descriptors are used to predict membership of a class based solely on positive examples of that class, and no counter-examples. A number of data descriptors that have been shown to perform well in previous studies of one-class classification, like the Support Vector Machine (SVM), require setting one or more hyperparameters. There has been no systematic attempt to date to determine optimal default values for these hyperparameters, which limits their ease of use, especially in comparison with hyperparameter-free proposals like the Isolation Forest (IF). We address this issue by determining optimal default hyperparameter values across a collection of 246 one-class classification problems derived from 50 different real-world datasets. In addition, we propose a new data descriptor, Average Localised Proximity (ALP) to address certain issues with existing approaches based on nearest neighbour distances. Finally, we evaluate classification performance using a leave-one-dataset-out procedure, and find strong evidence that ALP outperforms IF and a number of other data descriptors, as well as weak evidence that it outperforms SVM, making ALP a good default choice.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321001783",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Data mining",
      "Hyperparameter",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Lenz",
        "given_name": "Oliver Urs"
      },
      {
        "surname": "Peralta",
        "given_name": "Daniel"
      },
      {
        "surname": "Cornelis",
        "given_name": "Chris"
      }
    ]
  },
  {
    "title": "Laryngoscope8: Laryngeal image dataset and classification of laryngeal disease based on attention mechanism",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.06.034",
    "abstract": "Laryngeal disease is a common disease worldwide. However, currently there are no public laryngeal image datasets, which hinders the development of automatic classification of laryngeal disease. In this work, we build a new laryngeal image dataset called Laryngoscope8, which comprises 3057 images of 1950 unique individuals, and the images have been labeled with one of eight labels (including seven pathological labels and one normal label) by professional otolaryngologists. We also propose a laryngeal disease classification method, which uses attention mechanism to obtain the critical area under the supervision of image labels for laryngeal disease classification. That is, we first train a CNN model to classify the laryngeal images. If the classification result is correct, the region with strong response is most likely a critical area. The regions with strong responses are used as training data to train an object localization model that can automatically locate the critical area. Given an image for classification, the trained object localization model is employed to locate the critical area. Then, the located critical area is employed for image classification. The entire process only requires image-level labels and does not require manual labeling of the critical area. Experiment results show that the proposed method achieves promising performance in laryngeal disease classification.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521002646",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Contextual image classification",
      "Epistemology",
      "Image (mathematics)",
      "Mechanism (biology)",
      "Object (grammar)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Process (computing)"
    ],
    "authors": [
      {
        "surname": "Yin",
        "given_name": "Li"
      },
      {
        "surname": "Liu",
        "given_name": "Yang"
      },
      {
        "surname": "Pei",
        "given_name": "Mingtao"
      },
      {
        "surname": "Li",
        "given_name": "Jinrang"
      },
      {
        "surname": "Wu",
        "given_name": "Mukun"
      },
      {
        "surname": "Jia",
        "given_name": "Yuanyuan"
      }
    ]
  },
  {
    "title": "Visual Attention Dehazing Network with Multi-level Features Refinement and Fusion",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108021",
    "abstract": "Image dehazing is very important for many computer vision tasks. However, typical CNN-based methods learn a direct mapping from a hazy image to a clear image, ignoring relevant haze priors and multi-level features. In this paper, a new Visual Attention Dehazing Network (VADN) with multi-level refinement and fusion is proposed, which leverages a haze attention map as a haze relevant prior and learns complementary haze information among multi-level features. The VADN contains a feature extraction network, a recurrent refinement network and an encoder-decoder network. The feature extraction network captures the multi-level features. The recurrent refinement network generates and refines the haze attention map by taking low-level features and high-level features as inputs alternatively. Then, the haze attention map is injected into the encoder-decoder network to obtain the clear image with the help of complementary information learned from informative multi-level features. The experimental results demonstrate that the average PSNR of VADN is 32.50 dB which outperforms most state-of-the-art methods by up to 5.14 dB. Besides, the run time of VADN is 0.067 s, only 55 % of the run time spent by the recent enhanced pix2pix dehazing network.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321002089",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Encoder",
      "Feature (linguistics)",
      "Feature extraction",
      "Haze",
      "Image (mathematics)",
      "Linguistics",
      "Meteorology",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics"
    ],
    "authors": [
      {
        "surname": "Yin",
        "given_name": "Shibai"
      },
      {
        "surname": "Yang",
        "given_name": "Xiaolong"
      },
      {
        "surname": "Wang",
        "given_name": "Yibin"
      },
      {
        "surname": "Yang",
        "given_name": "Yee-Hong"
      }
    ]
  },
  {
    "title": "Soft-Boundary Label Relaxation with class placement constraints for semantic segmentation of the railway environment",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.07.014",
    "abstract": "In this paper, we focus on the challenging task of the semantic segmentation of train front-view images. Managing trackside facilities can be done by using detailed and precise information about the surrounding railway environment. Semantic segmentation enables us to understand the 2D environment, but there is no adequate large-scale dataset available for training a CNN for this purpose. Some attempts have been made to generate pseudo-data from unlabeled sequential frames to compensate for the lack of volume in training data, but the moving speed of trains makes it difficult to apply them directly. We aim to solve this problem by proposing the Soft Boundary Label Relaxation (Soft-BLR) method, which considers label boundaries extending over multiple pixels to cope with more severely distorted pseudo-data and to better train the CNN in the initial training stage. Furthermore, we modify the loss function to penalize inference results based on the distance from the label boundary to solve the misalignment problems of border pixels. Through experimental evaluation, we report that the proposed method outperforms previous methods on not only the semantic segmentation of challenging railway images, but also that of general street-view images.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521002592",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Boundary (topology)",
      "Cartography",
      "Class (philosophy)",
      "Computer science",
      "Computer vision",
      "Database",
      "Economics",
      "Evolutionary biology",
      "Focus (optics)",
      "Function (biology)",
      "Geography",
      "Image segmentation",
      "Inference",
      "Machine learning",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Optics",
      "Physics",
      "Pixel",
      "Psychology",
      "Relaxation (psychology)",
      "Scalability",
      "Segmentation",
      "Social psychology",
      "Task (project management)",
      "Train"
    ],
    "authors": [
      {
        "surname": "Furitsu",
        "given_name": "Yuki"
      },
      {
        "surname": "Deguchi",
        "given_name": "Daisuke"
      },
      {
        "surname": "Kawanishi",
        "given_name": "Yasutomo"
      },
      {
        "surname": "Ide",
        "given_name": "Ichiro"
      },
      {
        "surname": "Murase",
        "given_name": "Hiroshi"
      },
      {
        "surname": "Mukojima",
        "given_name": "Hiroki"
      },
      {
        "surname": "Nagamine",
        "given_name": "Nozomi"
      }
    ]
  },
  {
    "title": "Evolution of ICTs-empowered-identification: A general re-ranking method for person re-identification",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.06.031",
    "abstract": "Re-ranking is becoming a critical part of retrieved based Person re-identification algorithms. Existing re-ranking methods always require lots of queries and memory to go through the k -nearest neighbors. To solve this problem, we introduce a Feature Relation Map (FRM) to mine the latent relation between the k -neighbors through convolution neural network, and propose a metric learning based Similarity Evaluation (SE) model to obtain the re-ranking distance from the FRM. The dilated convolution is then introduced by concatenating the dilated convolution kernels and normal convolution kernels along the channel dimension in the improvement of the SE model (named SE-d model), to allow the SE model to efficiently compare more samples pairs for obtaining the final re-ranking distance. Further, we embedding out FRM-SE model to the existing re-ranking methods to prove the effectiveness of our re-ranking model. The experiments on Market1501, CUHK03, DukeMTMC and MSMT17 benchmarks illustrates the superiority of proposed method to the state-of-the-art re-ranking methods. Although the SE model make a great improve performance, the SE-d model gains an increase of 0.31%, 0.63% in top-1 respectably compared with the SE model in the DukeMTMC and MSMT17 datasets. Meanwhile, the SE-d model speed up the model convergence and shorten the training time. Furthermore, in the transfer learning setting, the model trained on either Market1501, CUHK03 or DukeMTMC can achieve a comparable accuracy improvement on the MSMT17 dataset, which validates the generalization of our SE model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521002427",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Botany",
      "Computer science",
      "Convergence (economics)",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Data mining",
      "Economic growth",
      "Economics",
      "Feature (linguistics)",
      "Generalization",
      "Identification (biology)",
      "Image (mathematics)",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Metric (unit)",
      "Operations management",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Ranking (information retrieval)",
      "Relation (database)",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Zhu",
        "given_name": "Bin"
      },
      {
        "surname": "Xu",
        "given_name": "Tongkun"
      },
      {
        "surname": "Zheng",
        "given_name": "Bolun"
      },
      {
        "surname": "Zhang",
        "given_name": "Quan"
      },
      {
        "surname": "Sun",
        "given_name": "Yaoqi"
      },
      {
        "surname": "Liu",
        "given_name": "Anan"
      },
      {
        "surname": "Mao",
        "given_name": "Zhendong"
      },
      {
        "surname": "Yan",
        "given_name": "Chenggang"
      }
    ]
  },
  {
    "title": "Pitch-robust acoustic feature using single frequency filtering for children’s KWS",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.07.015",
    "abstract": "The pitch and speaking rate are the two significant factors that cause the acoustic mismatch in children’s keyword spotting (KWS) system. This paper proposes a pitch-robust acoustic feature based on single frequency filtering (SFF) for the development of children’s KWS system. In the proposed approach using SFF, the amplitude envelopes (AEs) of the speech data are computed at D -number of selected frequencies separated in Mel scale. The AEs are then averaged over short-time overlapping analysis frames and logarithmically compressed to represent the D -dimensional feature set per analysis frame, here termed as Mel spaced single frequency average log envelope (MSSF-ALE). By using the proposed MSSF-ALE feature, improved performance is observed for the deep neural network-hidden Markov model-based KWS system over the standard Mel-frequency cepstral coefficients (MFCC) and MFCC extracted from the smoothed spectra. The relative improvement of 104.44% in term-weighted value ( T W V ) for children’s KWS is observed over the MFCC by using MSSF-ALE. The performance of the KWS system is then evaluated with data-augmented training through explicit speaking rate modification of the training data set. The MSSF-ALE provides a relative improvement of 195.94% in T W V over MFCC with the data-augmented training. The MSSF-ALE also results in improved performance than the explored features in noisy test cases.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521002610",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Data set",
      "Feature (linguistics)",
      "Feature extraction",
      "Frame (networking)",
      "Hidden Markov model",
      "Keyword spotting",
      "Linguistics",
      "Mel-frequency cepstrum",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Speech recognition",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Pattanayak",
        "given_name": "Biswaranjan"
      },
      {
        "surname": "Pradhan",
        "given_name": "Gayadhar"
      }
    ]
  },
  {
    "title": "Representing point clouds with generative conditional invertible flow networks",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.07.001",
    "abstract": "In this paper, we propose a simple yet effective method to represent point clouds as sets of samples drawn from a cloud-specific probability distribution. This interpretation matches intrinsic characteristics of point clouds: the number of points and their ordering within a cloud is not important as all points are drawn from the proximity of the object boundary. We postulate to represent each cloud as a parameterized probability distribution of points in space, which is defined by a generative neural network. The network operates by composing several spatial transformations of point locations. Once trained, it provides a natural framework for point cloud manipulation. For instance we can decouple cloud shape from its orientation and provide routines for aligning a new cloud into a default spatial orientation. To exploit similarities between same-class objects and to improve model performance, we turn to weight sharing: networks that model densities of points belonging to objects in the same family share all parameters with the exception of a small, object-specific embedding vector. We show that these embedding vectors capture semantic relationships between objects. Our method leverages generative invertible flow networks to learn embeddings as well as to generate point clouds. Thanks to this formulation and contrary to similar approaches, we are able to train our model in an end-to-end fashion. As a result, our model offers competitive or superior quantitative results on benchmark datasets, while enabling unprecedented capabilities to perform cloud manipulation tasks, such as point cloud registration and regeneration, by a generative network.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521002294",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Cloud computing",
      "Computer science",
      "Embedding",
      "Generative grammar",
      "Generative model",
      "Geodesy",
      "Geography",
      "Object (grammar)",
      "Operating system",
      "Parameterized complexity",
      "Point cloud",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Stypułkowski",
        "given_name": "Michał"
      },
      {
        "surname": "Kania",
        "given_name": "Kacper"
      },
      {
        "surname": "Zamorski",
        "given_name": "Maciej"
      },
      {
        "surname": "Zięba",
        "given_name": "Maciej"
      },
      {
        "surname": "Trzciński",
        "given_name": "Tomasz"
      },
      {
        "surname": "Chorowski",
        "given_name": "Jan"
      }
    ]
  },
  {
    "title": "Bangla Sign alphabet recognition with zero-shot and transfer learning",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.06.020",
    "abstract": "Bangla, being the fifth most spoken language in the world has its own distinct sign language with two methods (one-handed and two-handed) of representation. However, a standard automatic recognition system of Bangla sign language (BdSL) is still to be achieved. Though widely studied and explored by researchers in the past years, certain unaddressed issues like identifying unseen signs and both types of BdSL or lack of evaluation of the models in versatile environmental conditions demarcate the real-world implementation of the automatic recognition of BdSL. To find a probable solution to the shortcomings in the existing works, this paper proposes two approaches based on conventional transfer learning and contemporary Zero-shot learning (ZSL) for automatic BdSL alphabet recognition of both seen and unseen data. The performance of the proposed system is evaluated for both types of Bangla sign representations as well as on a large dataset with 35,149 images from over 350 subjects, varying in terms of backgrounds, camera angle, light contrast, skin tone, hand size, and orientation. For the ZSL approach, a new semantic descriptor dedicated to BdSL is created and a split of the dataset into seen and unseen classes is proposed. Our model achieved 68.21%, 91.57%, and 54.34% of harmonic mean accuracy, seen accuracy, and zero-shot accuracy with six unseen classes respectively. For the transfer learning-based approach, we found pre-trained DenseNet201 architecture to be the best performing feature extractor and Linear Discriminant Analysis as the best classifier with an overall accuracy of 93.68% on the large dataset after conducting quantitative experimentation on 18 CNN architectures and 21 classifiers. The satisfactory result from our models supports its very probative potential to serve extensively for the hearing and speaking impaired community.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521002269",
    "keywords": [
      "Artificial intelligence",
      "Bengali",
      "Classifier (UML)",
      "Computer science",
      "Feature (linguistics)",
      "Linear discriminant analysis",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Sign (mathematics)",
      "Sign language",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Nihal",
        "given_name": "Ragib Amin"
      },
      {
        "surname": "Rahman",
        "given_name": "Sejuti"
      },
      {
        "surname": "Broti",
        "given_name": "Nawara Mahmood"
      },
      {
        "surname": "Ahmed Deowan",
        "given_name": "Shamim"
      }
    ]
  },
  {
    "title": "Inlier clustering based on the residuals of random hypotheses",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.07.007",
    "abstract": "We present an approach for motion clustering based on a novel observation that a signature for putative pixel correspondences can be generated by collecting their residuals with respect to model hypotheses drawn randomly from the data. Inliers of the same motion cluster should have strongly correlated residuals, which are low when a hypothesis is consistent with the data in the cluster and high otherwise. After evaluating a number of hypotheses, members of the same cluster can be identified based on these correlations. Due to this property, we named our approach Inlier Clustering based on the Residuals of Random Hypotheses (ICR). An important advantage of ICR is that it does not require an inlier-outlier threshold or parameter tuning. In addition, we propose a supervised recursive formulation of ICR (r-ICR) that, unlike many motion clustering methods, does not require the number of clusters to be known a priori, as long as annotated data are available for training. We validate ICR and r-ICR on several publicly available datasets for robust geometric model fitting.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521002439",
    "keywords": [
      "A priori and a posteriori",
      "Artificial intelligence",
      "Cluster (spacecraft)",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Epistemology",
      "Mathematics",
      "Outlier",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Kutbi",
        "given_name": "Mohammed"
      },
      {
        "surname": "Chang",
        "given_name": "Yizhe"
      },
      {
        "surname": "Mordohai",
        "given_name": "Philippos"
      }
    ]
  },
  {
    "title": "Channel Attention in LiDAR-camera Fusion for Lane Line Segmentation",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108020",
    "abstract": "To assess the contributions of the different feature channels of sensors, we introduce a novel multimodal fusion method and demonstrate its practical utility using LiDAR-camera fusion networks. Specifically, a channel attention module that can be easily added to a fusion segmentation network is proposed. In this module, we use the channel attention mechanism to obtain the cross-channel local interaction information, and the weights of feature channels are assigned to represent the contributions of different feature channels. To verify the effectiveness of the proposed method, we conduct experiments on two types of feature fusion with the KITTI benchmark and A2D2 dataset. Our model achieves precise edge segmentation, with a 5.59% gain in precision and a 2.12% gain in F2-score compared to the values of the original fusion method. We believe that we have introduced a new optimization idea for multimodal fusion.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321002077",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Channel (broadcasting)",
      "Computer science",
      "Computer vision",
      "Enhanced Data Rates for GSM Evolution",
      "Feature (linguistics)",
      "Fusion",
      "Geodesy",
      "Geography",
      "Geometry",
      "Lidar",
      "Line (geometry)",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Remote sensing",
      "Segmentation",
      "Sensor fusion",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Xinyu"
      },
      {
        "surname": "Li",
        "given_name": "Zhiwei"
      },
      {
        "surname": "Gao",
        "given_name": "Xin"
      },
      {
        "surname": "Jin",
        "given_name": "Dafeng"
      },
      {
        "surname": "Li",
        "given_name": "Jun"
      }
    ]
  },
  {
    "title": "Dual feature extraction network for hyperspectral image analysis",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107992",
    "abstract": "Hyperspectral anomaly detection (HAD) is a research endeavor of high practical relevance within remote sensing scene interpretation. In this work, we propose an unsupervised approach, dual feature extraction network (DFEN) for HAD, to gradually build up ever-greater discrimination between the original data and background. In particular, we impose an end-to-end discriminative learning loss on two networks. Among them, adversarial learning aims to keep the original spectrum while Gaussian constrained learning intends to learn the background distribution in the potential space. To extract the anomaly, we calculate spatial and spectral anomaly scores based on mean squared error (MSE) spatial distance and orthogonal projection divergence (OPD) spectral distance between two latent feature matrices. Finally, the comprehensive detection result is obtained by a simple dot product between two domains to further reduce the false alarm rate. Experiments have been conducted on eight real hyperspectral data sets captured by different sensors over different scenes, which show that the proposed DFEN method is superior to other compared methods in detection accuracy or false alarm rate.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321001795",
    "keywords": [
      "Algorithm",
      "Anomaly detection",
      "Artificial intelligence",
      "Computer science",
      "Constant false alarm rate",
      "Discriminative model",
      "Divergence (linguistics)",
      "False alarm",
      "Feature (linguistics)",
      "Feature extraction",
      "Feature vector",
      "Hyperspectral imaging",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Projection (relational algebra)"
    ],
    "authors": [
      {
        "surname": "Xie",
        "given_name": "Weiying"
      },
      {
        "surname": "Lei",
        "given_name": "Jie"
      },
      {
        "surname": "Fang",
        "given_name": "Shuo"
      },
      {
        "surname": "Li",
        "given_name": "Yunsong"
      },
      {
        "surname": "Jia",
        "given_name": "Xiuping"
      },
      {
        "surname": "Li",
        "given_name": "Mingsuo"
      }
    ]
  },
  {
    "title": "R-SigNet: Reduced space writer-independent feature learning for offline writer-dependent signature verification",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.06.033",
    "abstract": "Handwritten signatures are a widespread biometric trait for person identification and verification. Reliable authentication and authorization of individuals are, however, challenging tasks due to possible skilled forgeries; especially when a forger has access to a given signature and deliberately tries to imitate it. This problem is even more emphasised in offline signature verification, where dynamic signature information is lost, resulting, as a consequence, in an increased difficulty discerning between genuine and forged signatures. To address this issue, solutions based on convolutional neural networks (CNN) are currently being devised to automatically extract features from a signature. Although highly performing, these methods require a high number of learnable parameters to produce meaningful signature representations, ultimately leading to long training times. In this paper, the R-SigNet architecture, a multi-task approach exploiting a relaxed loss to learn a reduced feature space for writer-independent (WI) signature verification, is presented. Compact generic features are automatically extracted by this network, so that a support vector machine (SVM) can be trained and tested in offline writer-dependent (WD) mode. By leveraging a small generic feature space, the proposed system achieves improved performances and reduced training times with respect to the current literature, as shown by the results obtained on several benchmark datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521002634",
    "keywords": [
      "Artificial intelligence",
      "Authentication (law)",
      "Benchmark (surveying)",
      "Biology",
      "Biometrics",
      "Botany",
      "Computer science",
      "Computer security",
      "Convolutional neural network",
      "Economics",
      "Feature (linguistics)",
      "Feature vector",
      "Geodesy",
      "Geography",
      "Geometry",
      "Identification (biology)",
      "Linguistics",
      "Machine learning",
      "Management",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Signature (topology)",
      "Support vector machine",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Avola",
        "given_name": "Danilo"
      },
      {
        "surname": "Bigdello",
        "given_name": "Manoochehr Joodi"
      },
      {
        "surname": "Cinque",
        "given_name": "Luigi"
      },
      {
        "surname": "Fagioli",
        "given_name": "Alessio"
      },
      {
        "surname": "Marini",
        "given_name": "Marco Raoul"
      }
    ]
  },
  {
    "title": "Offline signature verification using a region based deep metric learning network",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108009",
    "abstract": "Handwritten signature verification is a widely used biometric for person identity authentication in document forensics. Despite the tremendous efforts in past research, offline signature verification still remains a challenge, particularly in discriminating between genuine signatures and skilled forgeries, because the difference of appearance between genuine and skilled forgery may be smaller than that between genuine ones. This challenge is even more critical in writer-independent scenario, where each writer has very few samples for training. This paper proposes a region based Deep Convolutional Siamese Network using metric learning method, which is applicable to both writer-dependent (WD) and writer-independent (WI) scenario. For representing minute but discriminative details, a Mutual Signature DenseNet (MSDN) is designed to extract features and learn the similarity measure from local regions instead of whole signature images. Based on local regions comparison, the similarity scores of multiple regions are fused for final decision of verification. In experiments on public datasets CEDAR and GPDS, the proposed method achieved state-of-the-art performance of 6.74% EER and 8.24% EER in WI scenario, respectively, and 1.67% EER and 1.65% EER in WD scenario, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321001965",
    "keywords": [
      "Acoustics",
      "Artificial intelligence",
      "Authentication (law)",
      "Biometrics",
      "Computer science",
      "Computer security",
      "Convolutional neural network",
      "Discriminative model",
      "Economics",
      "Geometry",
      "Identity (music)",
      "Image (mathematics)",
      "Machine learning",
      "Mathematics",
      "Metric (unit)",
      "Operations management",
      "Pattern recognition (psychology)",
      "Physics",
      "Signature (topology)",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Li"
      },
      {
        "surname": "Huang",
        "given_name": "Linlin"
      },
      {
        "surname": "Yin",
        "given_name": "Fei"
      },
      {
        "surname": "Chen",
        "given_name": "Youbin"
      }
    ]
  },
  {
    "title": "Arbitrary-view human action recognition via novel-view action generation",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108043",
    "abstract": "Arbitrary-view human action recognition is still a big challenge due to the view changes. A possible solution is to enlarge the view range of action samples in the training set. Therefore, we propose a Two-Branch Novel-View action Generation approach based on auxiliary conditional GAN, which generates a novel-view action sample for arbitrary-view human action recognition. The generated sample enlarge the view range of action samples for training. Furthermore, to narrow the representation of actions in different views, we propose a view-domain generalization model that improves the recognition performance of arbitrary-view human action recognition. Our approach is evaluated on three large-scale RGB+D skeleton datasets including UESTC varying-view RGB+D dataset, NTU RGB+D 60, and NTU RGB+D 120 datasets, with two types of view-invariant evaluations, i.e., the cross-view, and arbitrary-view recognition. The proposed approach achieves outstanding performance in human action recognition.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321002302",
    "keywords": [
      "Action (physics)",
      "Action recognition",
      "Artificial intelligence",
      "Chemistry",
      "Chromatography",
      "Class (philosophy)",
      "Computer science",
      "Computer vision",
      "Generalization",
      "Invariant (physics)",
      "Law",
      "Mathematical analysis",
      "Mathematical physics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Political science",
      "Politics",
      "Programming language",
      "Quantum mechanics",
      "RGB color model",
      "Representation (politics)",
      "Sample (material)",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Gedamu",
        "given_name": "Kumie"
      },
      {
        "surname": "Ji",
        "given_name": "Yanli"
      },
      {
        "surname": "Yang",
        "given_name": "Yang"
      },
      {
        "surname": "Gao",
        "given_name": "LingLing"
      },
      {
        "surname": "Shen",
        "given_name": "Heng Tao"
      }
    ]
  },
  {
    "title": "Deep leaf: Mask R-CNN based leaf detection and segmentation from digitized herbarium specimen images",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.07.003",
    "abstract": "The generation of morphological traits of plants such as the leaf length, width, perimeter, area, and petiole length are fundamental features of herbarium specimens, thus providing high-quality data to investigate plant responses to ongoing climatic change and plant history evolution. However, the existing measurement methods are primarily associated with manual analysis, which is labor-intensive and inefficient. This paper proposes a deep learning-based approach, called Deep Leaf, for detecting and pixel-wise segmentation of leaves based on the improved state-of-the-art instance segmentation approach, Mask Region Convolutional Neural Network (Mask R-CNN). Deep Leaf can accurately detect each leaf in the herbarium specimen and measure the associated morphological traits. The experimental results indicate that our automated approach can segment the leaves of different families. Compared to manual measurement done by ecologist and botanist experts, the average relative error of leaf length is 4.6%, while the average relative error of leaf width is 5.7%.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521002361",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Cartography",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Geography",
      "Herbarium",
      "Hymenoptera",
      "Pattern recognition (psychology)",
      "Petiole (insect anatomy)",
      "Pixel",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Triki",
        "given_name": "Abdelaziz"
      },
      {
        "surname": "Bouaziz",
        "given_name": "Bassem"
      },
      {
        "surname": "Gaikwad",
        "given_name": "Jitendra"
      },
      {
        "surname": "Mahdi",
        "given_name": "Walid"
      }
    ]
  },
  {
    "title": "Controllable Image Generation with Semi-supervised Deep Learning and Deformable-Mean-Template Based Geometry-Appearance Disentanglement",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108001",
    "abstract": "Typical deep-neural-network (DNN) based generative image models often (i) show limited ability to learn a disentangled latent representation, (ii) show limited controllability leading to undesirable side effects when manipulating selected attributes during image generation, and (iii) require large attribute-annotated training sets. We propose a generative DNN model for face images by explicitly disentangling geometry and appearance modeling to achieve selective controllability of the desired attributes with less side effects. To learn geometric variability, we leverage grayscale sketch representations to learn (i) a deformable mean template representing the population-mean face geometry and (ii) a generative model of deformations to model individual face-geometry variations, using dense image registration. We learn the appearance variability in a (color-image) space that we explicitly design by factoring out the geometric variability. We propose a variational formulation to enable semi-supervised learning when manually-annotated attributes are severely limited in the training set. Results on large datasets show that, compared to schemes using deformation models or variational learning, our method significantly improves face-image model fits and facial-feature controllability even with semi-supervised learning.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321001886",
    "keywords": [
      "Applied mathematics",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer vision",
      "Controllability",
      "Convolutional neural network",
      "Face (sociological concept)",
      "Generative grammar",
      "Generative model",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Wadhwani",
        "given_name": "Krishna"
      },
      {
        "surname": "Awate",
        "given_name": "Suyash P."
      }
    ]
  },
  {
    "title": "Siamese network for object tracking with multi-granularity appearance representations",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108003",
    "abstract": "A reliable tracker has the ability to adapt to change of objects over time, and is robust and accurate. We build such a tracker by extracting semantic features using robust Siamese networks and multi-granularity color features. It incorporates a semantic model that can capture high quality semantic features and an appearance model that can describe object at pixel, local and global levels effectively. Furthermore, we propose a novel selective traverse algorithm to allocate weights to semantic models and appearance models dynamically for better tracking performance. During tracking, our tracker updates appearance representations for objects based on the recent tracking results. The proposed tracker operates at speeds that exceed the real-time requirement, and outperforms nearly all other state-of-the-art trackers on OTB-2013/2015 and VOT-2016/2017 benchmarks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321001904",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Granularity",
      "Object (grammar)",
      "Pedagogy",
      "Programming language",
      "Psychology",
      "Tracking (education)",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Zhuoyi"
      },
      {
        "surname": "Zhang",
        "given_name": "Yifeng"
      },
      {
        "surname": "Cheng",
        "given_name": "Xu"
      },
      {
        "surname": "Lu",
        "given_name": "Guojun"
      }
    ]
  },
  {
    "title": "Robust multi-view continuous subspace clustering",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2018.12.004",
    "abstract": "This paper proposed a novel Robust Multi-View Continuous Subspace Clustering (RMVCSC) algorithm, which can untangle heavily mixed clusters by optimizing a single continuous objective. The proposed objective uses robust estimators to automatically clip specious inter-cluster connections while maintaining convincing intra-cluster correspondences in the common representation subspace learned from multiple views. The common representation subspace can reveal the underlying cluster structure in data. RMVCSC is optimized in an alternating minimization scheme, in which the clustering result and the common representation subspace are simultaneously optimized. Since different views can describe distinct perspectives of input data, the proposed algorithm has more accurate clustering performance than conventional algorithms by exploring information among multi-view data. In other words, the proposed algorithm optimizes a novel continuous objective in the simultaneously learned common representation subspace across multiple views. By using robust redescending estimators, the proposed algorithm is not prone to stick into bad local minima even with outliers in data. This kind of robust continuous clustering methods has never been used for multi-view clustering before. Moreover, the convergence of the proposed algorithm is theoretically proved, and the experimental results show that the proposed RMVCSC can outperform several very recent proposed algorithms in terms of clustering accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865518309152",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "CURE data clustering algorithm",
      "Cluster analysis",
      "Computer science",
      "Correlation clustering",
      "Data mining",
      "Estimator",
      "Law",
      "Mathematical analysis",
      "Mathematics",
      "Maxima and minima",
      "Outlier",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Statistics",
      "Subspace topology"
    ],
    "authors": [
      {
        "surname": "Ma",
        "given_name": "Junbo"
      },
      {
        "surname": "Wang",
        "given_name": "Ruili"
      },
      {
        "surname": "Ji",
        "given_name": "Wanting"
      },
      {
        "surname": "Zhao",
        "given_name": "Jiawei"
      },
      {
        "surname": "Zong",
        "given_name": "Ming"
      },
      {
        "surname": "Gilman",
        "given_name": "Andrew"
      }
    ]
  },
  {
    "title": "Color edge detection by learning classification network with anisotropic directional derivative matrices",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108004",
    "abstract": "In this paper, a neural network-based color edge detector is constructed by learning a classifier using anisotropic directional derivative (ANDD) matrices of a color image as input. The training stage on a color edge dataset with ground truth (GT) edges includes calculation of ANDD matrices, generation of feature matrices, and training a classifier. For each training image, a set of ANDD matrices are calculated from the ANDDs with different parameter setups for training and from which a set of the color edge strength maps (CESMs) are extracted by the singular vector decomposition. The CESMs and the GTs on edges of the image are combined into a feature matrix for training. Using the feature matrices of all the training images as input, a classification neural network is trained and it outputs the probability of a pixel to be an edge pixel. In the detection stage, for a color image, its ANDD matrices, CESMs, and the color edge direction maps (CEDMs) are first computed and then the CESMs are input into the classification neural network to obtain the edge probability map (EPM) of the image. Finally, the non-maximum suppression and hysteresis thresholding are applied to the EPM and CEDMs to generate the binary edge map. The proposed detector attains better performance than the existing gradient-based detectors and is competitive with learning-based detectors on three commonly-used color image datasets for edge and contour detection.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321001916",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer vision",
      "Edge detection",
      "Feature vector",
      "Image (mathematics)",
      "Image gradient",
      "Image processing",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pixel",
      "Thresholding"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Ou"
      },
      {
        "surname": "Shui",
        "given_name": "Peng-Lang"
      }
    ]
  },
  {
    "title": "Edge-guided Composition Network for Image Stitching",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108019",
    "abstract": "Panorama creation is still challenging in consumer-level photography because of varying conditions of image capturing. A long-standing problem is the presence of artifacts caused by structure inconsistent image transitions. Since it is difficult to achieve perfect alignment in unconstrained shooting environment especially with parallax and object movements, image composition becomes a crucial step to produce artifact-free stitching results. Current energy-based seam-cutting image composition approaches are limited by the hand-crafted features, which are not discriminative and adaptive enough to robustly create structure consistent image transitions. In this paper, we present the first end-to-end deep learning framework named Edge Guided Composition Network (EGCNet) for the composition stage in image stitching. We cast the whole composition stage as an image blending problem, and aims to regress the blending weights to seamlessly produce the stitched image. To better preserve the structure consistency, we exploit perceptual edges to guide the network with additional geometric prior. Specifically, we introduce a perceptual edge branch to integrate edge features into the model and propose two edge-aware losses for edge guidance. Meanwhile, we gathered a general-purpose dataset for image stitching training and evaluation (namely, RISD). Extensive experiments demonstrate that our EGCNet produces plausible results with less running time, and outperforms traditional methods especially under the circumstances of parallax and object motions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321002065",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Enhanced Data Rates for GSM Evolution",
      "Image (mathematics)",
      "Image stitching",
      "Image translation",
      "Panorama",
      "Parallax"
    ],
    "authors": [
      {
        "surname": "Dai",
        "given_name": "Qinyan"
      },
      {
        "surname": "Fang",
        "given_name": "Faming"
      },
      {
        "surname": "Li",
        "given_name": "Juncheng"
      },
      {
        "surname": "Zhang",
        "given_name": "Guixu"
      },
      {
        "surname": "Zhou",
        "given_name": "Aimin"
      }
    ]
  },
  {
    "title": "Deep self-representative subspace clustering network",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108041",
    "abstract": "Deep learning based subspace clustering networks have been a significant technique for motion segmentation, unsupervised image segmentation, image representation and compression, and face clustering by separating the high-dimensional data points into their representative low-dimensional linear subspaces. Effective feature selection is critical to remove redundant samples and select the representative feature subset from high-dimensional data space; hence deriving the number of subspaces, their dimensions, data segmentation, and a basis for each subspace. The effective self-representative feature selection and emphasis by scaling the feature map in the learned embedded space is required for deep learning based subspace clustering to reduce the number of parameters and dimension of the self-representative layer. In this paper, we propose a self-representative feature extraction deep neural network for unsupervised subspace clustering to improve representativeness and clustering ability. The extensive relevant results on various data demonstrate that deep subspace clustering employing self-representative features from high-dimensional data can effectively reduce the dimension of the self-representative layer while improving performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321002284",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Clustering high-dimensional data",
      "Computer science",
      "Correlation clustering",
      "Feature extraction",
      "Geometry",
      "Linear subspace",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Subspace topology"
    ],
    "authors": [
      {
        "surname": "Baek",
        "given_name": "Sangwon"
      },
      {
        "surname": "Yoon",
        "given_name": "Gangjoon"
      },
      {
        "surname": "Song",
        "given_name": "Jinjoo"
      },
      {
        "surname": "Yoon",
        "given_name": "Sang Min"
      }
    ]
  },
  {
    "title": "MHFP: Multi-view based hierarchical fusion pooling method for 3D shape recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.07.010",
    "abstract": "3D shape recognition has received widespread attention in the field of computer vision. Since the 3D model contains much geometric information, which is difficult to extract but important to feature learning, effective description of 3D shape is still facing great challenges. With the rapid development of deep learning, a large number of methods have been proposed. However, these approaches always focus on the learning of view features but ignore the multi-view information protection in the process of feature fusion. In this work, we propose a novel Multi-view based Hierarchical Fusion Pooling Method (MHFP) for 3D Model Recognition, which hierarchically fuses the features of multi-view into a compact descriptor. Our approach considers the correlation between views, it can powerfully remove redundant information and retain a large amount of essential information. Meanwhile, we design a 3D attention module to dig out the correlation between the views, which prepares for the graph construction. To verify the effectiveness of our MHFP, we conduct experiments on the ModelNet40 dataset and compare it with some state-of-the-art methods. The final results demonstrate the superiority of our proposed approach in 3D shape recognition.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521002555",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Feature (linguistics)",
      "Field (mathematics)",
      "Focus (optics)",
      "Graph",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Operating system",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Pooling",
      "Process (computing)",
      "Pure mathematics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Liang",
        "given_name": "Qi"
      },
      {
        "surname": "Li",
        "given_name": "Qiang"
      },
      {
        "surname": "Zhang",
        "given_name": "Lihu"
      },
      {
        "surname": "Mi",
        "given_name": "Haixiao"
      },
      {
        "surname": "Nie",
        "given_name": "Weizhi"
      },
      {
        "surname": "Li",
        "given_name": "Xuanya"
      }
    ]
  },
  {
    "title": "ACAE-REMIND for online continual learning with compressed feature replay",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.06.025",
    "abstract": "Online continual learning aims to learn from a non-IID stream of data from a number of different tasks, where the learner is only allowed to consider data once. Methods are typically allowed to use a limited buffer to store some of the images in the stream. Recently, it was found that feature replay, where an intermediate layer representation of the image is stored (or generated) leads to superior results than image replay, while requiring less memory. Quantized exemplars can further reduce the memory usage. However, a drawback of these methods is that they use a fixed (or very intransigent) backbone network. This significantly limits the learning of representations that can discriminate between all tasks. To address this problem, we propose an auxiliary classifier auto-encoder (ACAE) module for feature replay at intermediate layers with high compression rates. The reduced memory footprint per image allows us to save more exemplars for replay. In our experiments, we conduct task-agnostic evaluation under online continual learning setting and get state-of-the-art performance on ImageNet-Subset, CIFAR100 and CIFAR10 dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521002312",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Encoder",
      "Feature (linguistics)",
      "Feature learning",
      "Image (mathematics)",
      "Law",
      "Linguistics",
      "Machine learning",
      "Memory footprint",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Kai"
      },
      {
        "surname": "van de Weijer",
        "given_name": "Joost"
      },
      {
        "surname": "Herranz",
        "given_name": "Luis"
      }
    ]
  },
  {
    "title": "MIDCAN: A multiple input deep convolutional attention network for Covid-19 diagnosis based on chest CT and chest X-ray",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.06.021",
    "abstract": "Background COVID-19 has caused 3.34m deaths till 13/May/2021. It is now still causing confirmed cases and ongoing deaths every day. Method This study investigated whether fusing chest CT with chest X-ray can help improve the AI's diagnosis performance. Data harmonization is employed to make a homogeneous dataset. We create an end-to-end multiple-input deep convolutional attention network (MIDCAN) by using the convolutional block attention module (CBAM). One input of our model receives 3D chest CT image, and other input receives 2D X-ray image. Besides, multiple-way data augmentation is used to generate fake data on training set. Grad-CAM is used to give explainable heatmap. Results The proposed MIDCAN achieves a sensitivity of 98.10±1.88%, a specificity of 97.95±2.26%, and an accuracy of 98.02±1.35%. Conclusion Our MIDCAN method provides better results than 8 state-of-the-art approaches. We demonstrate the using multiple modalities can achieve better results than individual modality. Also, we demonstrate that CBAM can help improve the diagnosis performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521002270",
    "keywords": [
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Combinatorics",
      "Computer science",
      "Convolutional neural network",
      "Coronavirus disease 2019 (COVID-19)",
      "Data set",
      "Deep learning",
      "Disease",
      "Electronic engineering",
      "Engineering",
      "Geometry",
      "Homogeneous",
      "Image (mathematics)",
      "Infectious disease (medical specialty)",
      "Mathematics",
      "Medicine",
      "Modality (human–computer interaction)",
      "Pathology",
      "Pattern recognition (psychology)",
      "Programming language",
      "Radiology",
      "Sensitivity (control systems)",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Yu-Dong"
      },
      {
        "surname": "Zhang",
        "given_name": "Zheng"
      },
      {
        "surname": "Zhang",
        "given_name": "Xin"
      },
      {
        "surname": "Wang",
        "given_name": "Shui-Hua"
      }
    ]
  },
  {
    "title": "Unsupervised domain-adaptive scene-specific pedestrian detection for static video surveillance",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108038",
    "abstract": "Objects from one category may be drawn from different distributions due to diverse illuminations, backgrounds, and camera viewpoints. Traditional object detection methods generally perform poorly due to the domain shift. To address this problem, we propose to train a domain-adaptive scene-specific pedestrian detector in an unsupervised manner. A generic detector is transferred to different target domains from one labeled source domain dataset without human-annotated target samples. Specifically, we first extend the generic detector to a dual-boundary classifier and collect hard samples as unlabeled target samples according to the detection confidence. Then, we propose a cycle semantic transfer network to align the instance-level and class-level distributions between the source domain and target domain and automatically label the hard samples. The initial generic detector is then re-trained by these labeled hard samples and specialized to a target scene. This process can be conveniently extended to different surveillance scenarios and generate specific detectors under various static camera viewpoints. Moreover, to reduce the impact of mislabeled hard samples on the generic detector, an online gradual optimization algorithm is proposed to iteratively update the generic model, thereby obtaining an optimized process that is insensitive to individual mislabeled target samples. Extensive experiments show that even if the target domain is not manually annotated, the proposed self-learning method demonstrates the effectiveness of pedestrian detection in various domain shift scenarios, and it outperforms existing scene-specific pedestrian detection methods and some classic supervised methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321002259",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Computer vision",
      "Detector",
      "Domain (mathematical analysis)",
      "Engineering",
      "Mathematical analysis",
      "Mathematics",
      "Object detection",
      "Operating system",
      "Pattern recognition (psychology)",
      "Pedestrian",
      "Pedestrian detection",
      "Process (computing)",
      "Telecommunications",
      "Transport engineering",
      "Viewpoints",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Mou",
        "given_name": "Quanzheng"
      },
      {
        "surname": "Wei",
        "given_name": "Longsheng"
      },
      {
        "surname": "Wang",
        "given_name": "Conghao"
      },
      {
        "surname": "Luo",
        "given_name": "Dapeng"
      },
      {
        "surname": "He",
        "given_name": "Songze"
      },
      {
        "surname": "Zhang",
        "given_name": "Jing"
      },
      {
        "surname": "Xu",
        "given_name": "Huimin"
      },
      {
        "surname": "Luo",
        "given_name": "Chen"
      },
      {
        "surname": "Gao",
        "given_name": "Changxin"
      }
    ]
  },
  {
    "title": "Multiscale permutation entropy for two-dimensional patterns",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.06.028",
    "abstract": "Complexity measures are important to understand and analyze systems with one dimensional data. However, extension of these methods to images (two dimensional data) are much less usual. Bidimensional multiscale sample entropy ( M S E 2 D ) has recently been proposed as a new complexity measure for texture evaluation. However, M S E 2 D leads to undefined or unreliable values for small-sized textures and requires a long computation time. This is why we herein propose the bidimensional multiscale permutation entropy ( M P E 2 D ) to evaluate the complexity of 2D patterns. M P E 2 D is applied to different synthesized textures, to softwood samples, and to study the texture of breast histopathology images. The results show that M P E 2 D is a valuable tool for texture analysis and that it is computationally noticeably faster than M S E 2 D .",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521002373",
    "keywords": [
      "Acoustics",
      "Algorithm",
      "Artificial intelligence",
      "Computation",
      "Computer science",
      "Entropy (arrow of time)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Permutation (music)",
      "Physics",
      "Quantum mechanics",
      "Sample entropy"
    ],
    "authors": [
      {
        "surname": "Morel",
        "given_name": "Cristina"
      },
      {
        "surname": "Humeau-Heurtier",
        "given_name": "Anne"
      }
    ]
  },
  {
    "title": "Scientific papers citation analysis using textual features and SMOTE resampling techniques",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.07.009",
    "abstract": "Ascertaining the impact of research is significant for the research community and academia of all disciplines. The only prevalent measure associated with the quantification of research quality is the citation-count. Although a number of citations play a significant role in academic research, sometimes citations can be biased or made to discuss only the weaknesses and shortcomings of the research. By considering the sentiment of citations and recognizing patterns in text can aid in understanding the opinion of the peer research community and will also help in quantifying the quality of research articles. Efficient feature representation combined with machine learning classifiers has yielded significant improvement in text classification. However, the effectiveness of such combinations has not been analyzed for citation sentiment analysis. This study aims to investigate pattern recognition using machine learning models in combination with frequency-based and prediction-based feature representation techniques with and without using Synthetic Minority Oversampling Technique (SMOTE) on publicly available citation sentiment dataset. Sentiment of citation instances are classified into positive, negative or neutral. Results indicate that the Extra tree classifier in combination with Term Frequency-Inverse Document Frequency achieved 98.26% accuracy on the SMOTE-balanced dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552100252X",
    "keywords": [
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Citation",
      "Classifier (UML)",
      "Computer network",
      "Computer science",
      "Data mining",
      "Feature (linguistics)",
      "Information retrieval",
      "Linguistics",
      "Machine learning",
      "Oversampling",
      "Philosophy",
      "Sentiment analysis",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Umer",
        "given_name": "Muhammad"
      },
      {
        "surname": "Sadiq",
        "given_name": "Saima"
      },
      {
        "surname": "Missen",
        "given_name": "Malik Muhammad Saad"
      },
      {
        "surname": "Hameed",
        "given_name": "Zahid"
      },
      {
        "surname": "Aslam",
        "given_name": "Zahid"
      },
      {
        "surname": "Siddique",
        "given_name": "Muhammad Abubakar"
      },
      {
        "surname": "NAPPI",
        "given_name": "Michele"
      }
    ]
  },
  {
    "title": "An automatic clustering algorithm based on the density-peak framework and Chameleon method",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.06.017",
    "abstract": "The density-peak clustering (DPC) method (Rodriguez and Laio, 2014) clusters the data efficiently by fast searching density peaks. Recently, an improved DPC algorithm named 3DC method (Liang and Chen, 2016) was proposed for automatically detecting the correct structure of the clusters. However, it is difficult to select correct parameters for the DPC and 3DC methods and the local property of data set can’t be revealed due to their global density assumption in some scenarios. To overcome this drawback, the K-nearest neighbor (KNN) framework is adapted for defining the density of the DPC method. Nevertheless, such KNN-based methods can’t automatically detect the number of the clusters compared with the 3DC method. In this paper, an automatic clustering method is proposed, which needs only a discrete input parameter. Meanwhile, by utilizing the cluster stability for the Chameleon framework, the proposed method can automatically detect the correct structure of the clusters. The experimental results on the synthetic and real world data demonstrate that the proposed method has a more robust performance. Besides, the proposed method is robust to the choices of the input parameter.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521002221",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "CURE data clustering algorithm",
      "Cluster (spacecraft)",
      "Cluster analysis",
      "Computer science",
      "Correlation clustering",
      "Data mining",
      "Data set",
      "Determining the number of clusters in a data set",
      "Epistemology",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Property (philosophy)",
      "Set (abstract data type)",
      "Stability (learning theory)",
      "k-nearest neighbors algorithm"
    ],
    "authors": [
      {
        "surname": "Liang",
        "given_name": "Zhou"
      },
      {
        "surname": "Chen",
        "given_name": "Pei"
      }
    ]
  },
  {
    "title": "Motion estimation in hazy videos",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.06.029",
    "abstract": "Motion estimation is the basic need for the success of many video analysis algorithms such as moving object detection, human activity recognition, etc. Most of the motion estimation algorithms are prone to weather conditions and thus, they fail to estimate the motion in degraded weather. Severe weather situations like snow, rain, haze, smog, etc., degrades the performance and reliability of video analysis algorithms. In this paper, we have analyzed the effect of the haze on motion estimation in hazy videos. We propose a cascaded architecture i.e. haze removal followed by optical flow for motion estimation in hazy videos. The proposed image de-hazing network is build upon the Residual and Inception module concepts and named as ResINet. Further, an optical flow is utilized to estimate the motion information. We have carried out the visual analysis to validate the proposed approach for motion estimation in hazy videos. Also, to validate the proposed ResINet for de-hazing, we carried out the quantitative analysis on two benchmark image de-hazing datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521002385",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Computer vision",
      "Geodesy",
      "Geography",
      "Haze",
      "Image (mathematics)",
      "Meteorology",
      "Motion (physics)",
      "Motion estimation",
      "Optical flow",
      "Physics",
      "Power (physics)",
      "Quantum mechanics",
      "Quarter-pixel motion",
      "Reliability (semiconductor)",
      "Residual"
    ],
    "authors": [
      {
        "surname": "Chaudhary",
        "given_name": "Sachin"
      },
      {
        "surname": "Dudhane",
        "given_name": "Akshay"
      },
      {
        "surname": "Patil",
        "given_name": "Prashant W."
      },
      {
        "surname": "Murala",
        "given_name": "Subrahmanyam"
      },
      {
        "surname": "Talbar",
        "given_name": "Sanjay"
      }
    ]
  },
  {
    "title": "Identification of source social network of digital images using deep neural network",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.06.019",
    "abstract": "Identification of image provenance is one of the crucial tasks to be tackled in the field of image forensics. Due to the rapid growth in technology, a huge amount of digital images are shared among users through social networks which facilitates many malicious activities. Establishing the provenance of downloaded images is an important task that could help focus investigations in a specific direction. Such a task is based on the identification of unique fingerprints that are imprinted on images by the social network during the process of upload and download. Based on this, we propose a deep learning based approach that learns the unique traces from the images transformed to the discrete cosine and wavelet domains, to investigate whether the image under test originates directly from a camera or from a specific social network site. The proposed method is able to efficiently identify the specific social network of provenance of the downloaded images and outperforms the state-of-the-art techniques. The encouraging results on images represented in the wavelet domain obtained for the well-known datasets, namely, VISION, IPLAB, and FODB sheds the light into a new way of identifying the image provenance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521002257",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Botany",
      "Computer science",
      "Computer vision",
      "Digital image",
      "Domain (mathematical analysis)",
      "Economics",
      "Field (mathematics)",
      "Focus (optics)",
      "Identification (biology)",
      "Image (mathematics)",
      "Image processing",
      "Machine learning",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Operating system",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Process (computing)",
      "Pure mathematics",
      "Task (project management)",
      "Upload",
      "Wavelet",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Manisha",
        "given_name": ""
      },
      {
        "surname": "Karunakar",
        "given_name": "A.K."
      },
      {
        "surname": "Li",
        "given_name": "Chang-Tsun"
      }
    ]
  },
  {
    "title": "CT-Net: Cascade T-shape deep fusion networks for document binarization",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108010",
    "abstract": "Document binarization is a key step in most document analysis tasks. However, historical-document images usually suffer from various degradations, making this a very challenging processing stage. The performance of document image binarization has improved dramatically in recent years by the use of Convolutional Neural Networks (CNNs). In this paper, a dual-task, T-shaped neural network is proposed that has the main task of binarization and an auxiliary task of image enhancement. The neural network for enhancement learns the degradations in document images and the specific CNN-kernel features can be adapted towards the binarization task in the training process. In addition, the enhancement image can be considered as an improved version of the input image, which can be fed into the network for fine-tuning, making it possible to design a chained-cascade network (CT-Net). Experimental results on document binarization competition datasets (DIBCO datasets) and MCS dataset show that our proposed method outperforms competing state-of-the-art methods in most cases.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321001977",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Cascade",
      "Chemistry",
      "Chromatography",
      "Combinatorics",
      "Computer science",
      "Computer security",
      "Convolutional neural network",
      "Economics",
      "Image (mathematics)",
      "Kernel (algebra)",
      "Key (lock)",
      "Management",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Process (computing)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "He",
        "given_name": "Sheng"
      },
      {
        "surname": "Schomaker",
        "given_name": "Lambert"
      }
    ]
  },
  {
    "title": "Learning the micro deformations by max-pooling for offline signature verification",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108008",
    "abstract": "For signature verification systems, micro deformations can be defined as the small differences in the same strokes of signatures or special writing habits of different signers. These micro deformations can reveal the core distinction between the genuine signatures and skilled forgeries. In this paper, we prove that Convolutional Neural Networks (CNNs) have the potential to extract those micro deformations by max-pooling. More specifically, the micro deformations can be determined by watching the location coordinates of the maximum values in pooling windows of max-pooling. Extensive analysis and experiments demonstrate that it is possible to achieve state-of-the-art performance by using this location information as a new feature for capturing micro deformations, along with convolutional features. The proposed method outperforms the state-of-the-art systems on four publicly available datasets of different languages, i.e., English (GPDSsynthetic, CEDAR), Persian (UTSig), and Hindi (BHSig260).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321001953",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Feature (linguistics)",
      "Geometry",
      "Hindi",
      "Linguistics",
      "Mathematics",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pooling",
      "Signature (topology)"
    ],
    "authors": [
      {
        "surname": "Zheng",
        "given_name": "Yuchen"
      },
      {
        "surname": "Iwana",
        "given_name": "Brian Kenji"
      },
      {
        "surname": "Malik",
        "given_name": "Muhammad Imran"
      },
      {
        "surname": "Ahmed",
        "given_name": "Sheraz"
      },
      {
        "surname": "Ohyama",
        "given_name": "Wataru"
      },
      {
        "surname": "Uchida",
        "given_name": "Seiichi"
      }
    ]
  },
  {
    "title": "Relation-based Discriminative Cooperation Network for Zero-Shot Classification",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108024",
    "abstract": "Zero-shot learning (ZSL) aims to assign the category corresponding to the relevant semantic as the label of the unseen sample based on the relationship between the learned visual and semantic features. However, most typical ZSL models faced with the domain bias problem, which leads to unseen or test samples being easily misclassified into seen or training categories. To handle this problem, we propose a relation-based discriminative cooperation network (RDCN) model for ZSL in this work. The proposed model effectively utilize the robust metric space spanned by the cooperated semantics with the help of a set of relations. On the other hand, we devise the relation network to measure the relationship between the visual features and embedded semantics, and the validation information will guide the embedding module to learn more discriminative information. At last, the proposed RDCN model is validated on six benchmarks, and extensive experiments demonstrate the superiority of proposed method over most existing ZSL models on the traditional zero-shot setting and the more realistic generalized zero-shot setting.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321002119",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Discriminative model",
      "Domain (mathematical analysis)",
      "Economics",
      "Embedding",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Measure (data warehouse)",
      "Metric (unit)",
      "Natural language processing",
      "Operations management",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Relation (database)",
      "Semantics (computer science)",
      "Set (abstract data type)",
      "Zero (linguistics)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Yang"
      },
      {
        "surname": "Gao",
        "given_name": "Xinbo"
      },
      {
        "surname": "Gao",
        "given_name": "Quanxue"
      },
      {
        "surname": "Han",
        "given_name": "Jungong"
      },
      {
        "surname": "Shao",
        "given_name": "Ling"
      }
    ]
  },
  {
    "title": "Dynamic sampling for deep metric learning",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.06.027",
    "abstract": "Deep metric learning maps visually similar images onto nearby locations and visually dissimilar images apart from each other in an embedding manifold. The learning process is mainly based on the supplied image negative and positive training pairs. In this paper, a dynamic sampling strategy is proposed to organize the training pairs in an easy-to-hard order to feed into the network. It allows the network to learn general boundaries between categories from the easy training pairs at its early stages and finalize the details of the model mainly relying on the hard training samples in the later. Compared to the existing training sample mining approaches, the hard samples are mined with little harm to the learned general model. This dynamic sampling strategy is formulated as two simple terms that are compatible with various loss functions. Consistent performance boost is observed when it is integrated with several popular loss functions on fashion search and fine-grained image search.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552100235X",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Computer vision",
      "Economics",
      "Embedding",
      "Epistemology",
      "Filter (signal processing)",
      "Image (mathematics)",
      "Machine learning",
      "Metric (unit)",
      "Operating system",
      "Operations management",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Process (computing)",
      "Sample (material)",
      "Sampling (signal processing)",
      "Simple (philosophy)"
    ],
    "authors": [
      {
        "surname": "Liang",
        "given_name": "Chang-Hui"
      },
      {
        "surname": "Zhao",
        "given_name": "Wan-Lei"
      },
      {
        "surname": "Chen",
        "given_name": "Run-Qing"
      }
    ]
  },
  {
    "title": "Node2vec with weak supervision on community structures",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.06.024",
    "abstract": "Detecting communities or the modular structure of real-life networks (e.g. a social network or a product purchase network) is an important task because the way a network functions is often determined by its communities. Traditional approaches to community detection involve modularity-based algorithms, which generally speaking, construct partitions based on heuristics that seek to maximize the ratio of the edges within the partitions to those between them. On the other hand, node embedding approaches represent each node in a graph as a real-valued vector and is thereby able to transform the problem of community detection in a graph to that of clustering a set of vectors. Existing node embedding approaches are primarily based on, first, initiating random walks from each node to construct a context of a node, and then make the vector representation of a node close to its context. However, standard node embedding approaches do not directly take into account the community structure of a network while constructing the context around each node. To alleviate this, we propose a community structure aware node embedding approach, where we incorporate an initial combinatorial approach-based partition information into the objective function of node embedding. We demonstrate that our proposed combination of the combinatorial and the embedding approaches for community detection outperforms a number of combinatorial-based baselines on a wide range of real-life and synthetic networks of different sizes and densities.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521002324",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Combinatorics",
      "Community structure",
      "Computer science",
      "Context (archaeology)",
      "Embedding",
      "Engineering",
      "Graph",
      "Heuristics",
      "Mathematics",
      "Node (physics)",
      "Operating system",
      "Paleontology",
      "Partition (number theory)",
      "Structural engineering",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Chattopadhyay",
        "given_name": "Swarup"
      },
      {
        "surname": "Ganguly",
        "given_name": "Debasis"
      }
    ]
  },
  {
    "title": "Sliced Wasserstein based Canonical Correlation Analysis for Cross-Domain Recommendation",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.06.015",
    "abstract": "To solve the problem of data sparsity and cold start, the cross-domain recommendation is a promising research direction in the recommender system. The goal of cross-domain recommendation is to transfer learned knowledge from the source domain to the target domain by different means to improve the performance of the recommendation. But most approaches face the distribution misalignment. In this paper, we propose a joint learning cross-domain recommendation model that can extract domain-specific and common features simultaneously, and only use the implicit feedback data of users without additional auxiliary information. To the best of our knowledge, it is the first attempt to combine the sliced Wasserstein distance and canonical correlation analysis for the cross-domain recommendation scenario. Our one intuition is to reduce the reconstruction error caused by the variational inference based autoencoder model by the optimal transportation theory. Another attempt is to improve the correlation between domains by combining the idea of the canonical correlation analysis. With rigorous experiments, we empirically demonstrated that our model can achieve better performance compared with the state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552100221X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Autoencoder",
      "Canonical correlation",
      "Computer science",
      "Correlation",
      "Data mining",
      "Deep learning",
      "Domain (mathematical analysis)",
      "Epistemology",
      "Geometry",
      "Inference",
      "Intuition",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Recommender system"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Zian"
      },
      {
        "surname": "Nie",
        "given_name": "Jie"
      },
      {
        "surname": "Wang",
        "given_name": "Chenglong"
      },
      {
        "surname": "Huang",
        "given_name": "Lei"
      }
    ]
  },
  {
    "title": "Low-complexity arrays of contour signatures for exact shape retrieval",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108000",
    "abstract": "We propose a framework for a fast exact shape retrieval called Low-complexity Arrays of Contour Signatures. The purposes are to match a shape against a database in constant time and to retrieve correct shapes very close to the query, while the latter may have undergone rigid transformations and noise. We present a shape signature based on prior works as well as a compact characterization of such signatures, a system of associative arrays allowing a short search time for retrieval and a technique of pairwise alignment. This method shows a good resilience to perturbations and is performed in constant computational time.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321001874",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Associative property",
      "Computational complexity theory",
      "Computer science",
      "Constant (computer programming)",
      "Geometry",
      "Image (mathematics)",
      "Mathematics",
      "Noise (video)",
      "Pairwise comparison",
      "Pattern recognition (psychology)",
      "Programming language",
      "Pure mathematics",
      "Signature (topology)"
    ],
    "authors": [
      {
        "surname": "Lardeux",
        "given_name": "Florian"
      },
      {
        "surname": "Marchand",
        "given_name": "Sylvain"
      },
      {
        "surname": "Gomez-Krämer",
        "given_name": "Petra"
      }
    ]
  },
  {
    "title": "ScieNet: Deep learning with spike-assisted contextual information extraction",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108002",
    "abstract": "Spiking neural network (SNN) is a type of artificial neural network that uses biologically inspired neuron models and learning rules to develop artificial intelligence with capability parallel to human brain. Deep neural networks (DNNs), on the other hand, uses less biologically plausible neurons and training methods such as gradient descent, and has shown good accuracy in computer vision tasks. However, human brain can still outperform DNN in certain scenarios. For example, DNN experiences significant performance degradation when perturbation from various sources is present in the input, which makes DNN less reliable for systems interacting with physical world. In this paper, we present a hybrid deep network architecture with spike-assisted contextual information extraction (ScieNet) as a solution to the problem. ScieNet integrates a front-end SNN with a novel stochastic spike-timing-dependent plasticity (STDP) algorithm that extracts visual context from images. The back-end DNN is trained for classification given the contextual information. The integrated network demonstrates high resilience to input perturbations without relying on pre-training on perturbed inputs. We demonstrate ScieNet with various back-end DNNs for image classification using different datasets and considering both stochastic and structured input perturbations. Experimental results demonstrate significant improvement in accuracy on perturbed images, while maintaining state-of-the-art accuracy on clean images.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321001898",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Biology",
      "Chemistry",
      "Computer science",
      "Context (archaeology)",
      "Deep neural networks",
      "Gradient descent",
      "Machine learning",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Receptor",
      "Software engineering",
      "Spike (software development)",
      "Spike-timing-dependent plasticity",
      "Spiking neural network",
      "Stochastic gradient descent",
      "Synaptic plasticity"
    ],
    "authors": [
      {
        "surname": "She",
        "given_name": "Xueyuan"
      },
      {
        "surname": "Long",
        "given_name": "Yun"
      },
      {
        "surname": "Kim",
        "given_name": "Daehyun"
      },
      {
        "surname": "Mukhopadhyay",
        "given_name": "Saibal"
      }
    ]
  },
  {
    "title": "RGB-IR cross-modality person ReID based on teacher-student GAN model",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.07.006",
    "abstract": "RGB-Infrared (RGB-IR) person re-identification (ReID) is a technology where the system can automatically identify the same person appearing at different parts of a video when light is unavailable. The critical challenge of this task is the cross-modality gap of features under different modalities. To solve this challenge, we proposed a Teacher-Student GAN model (TS-GAN) to adopt different domains and guide the ReID backbone. (1) In order to get corresponding RGB-IR image pairs, the RGB-IR Generative Adversarial Network (GAN) was used to generate IR images. (2) To kick-start the training of identities, a ReID Teacher module was trained under IR modality person images, which is then used to guide its Student counterpart in training. (3) Likewise, to better adapt different domain features and enhance model ReID performance, three Teacher-Student loss functions were used. Unlike other GAN based models, the proposed model only needs the backbone module at the test stage, making it more efficient and resource-saving. To showcase our model’s capability, we did extensive experiments on the newly-released SYSU-MM01 and RegDB RGB-IR Re-ID benchmark and achieved superior performance to the state-of-the-art with 47.4% mAP and 69.4% mAP respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521002415",
    "keywords": [
      "Artificial intelligence",
      "Backbone network",
      "Benchmark (surveying)",
      "Computer science",
      "Computer vision",
      "Domain (mathematical analysis)",
      "Engineering",
      "Geodesy",
      "Geography",
      "Mathematical analysis",
      "Mathematics",
      "Modalities",
      "Modality (human–computer interaction)",
      "RGB color model",
      "Social science",
      "Sociology",
      "Systems engineering",
      "Task (project management)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Ziyue"
      },
      {
        "surname": "Jiang",
        "given_name": "Shuai"
      },
      {
        "surname": "Huang",
        "given_name": "Congzhentao"
      },
      {
        "surname": "Li",
        "given_name": "Yang"
      },
      {
        "surname": "Xu",
        "given_name": "Richard Yi Da"
      }
    ]
  },
  {
    "title": "Periphery-aware COVID-19 diagnosis with contrastive representation enhancement",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108005",
    "abstract": "Computer-aided diagnosis has been extensively investigated for more rapid and accurate screening during the outbreak of COVID-19 epidemic. However, the challenge remains to distinguish COVID-19 in the complex scenario of multi-type pneumonia classification and improve the overall diagnostic performance. In this paper, we propose a novel periphery-aware COVID-19 diagnosis approach with contrastive representation enhancement to identify COVID-19 from influenza-A (H1N1) viral pneumonia, community acquired pneumonia (CAP), and healthy subjects using chest CT images. Our key contributions include: 1) an unsupervised Periphery-aware Spatial Prediction (PSP) task which is designed to introduce important spatial patterns into deep networks; 2) an adaptive Contrastive Representation Enhancement (CRE) mechanism which can effectively capture the intra-class similarity and inter-class difference of various types of pneumonia. We integrate PSP and CRE to obtain the representations which are highly discriminative in COVID-19 screening. We evaluate our approach comprehensively on our constructed large-scale dataset and two public datasets. Extensive experiments on both volume-level and slice-level CT images demonstrate the effectiveness of our proposed approach with PSP and CRE for COVID-19 diagnosis.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321001928",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Coronavirus disease 2019 (COVID-19)",
      "Discriminative model",
      "Disease",
      "Image (mathematics)",
      "Infectious disease (medical specialty)",
      "Internal medicine",
      "Law",
      "Medicine",
      "Pathology",
      "Pattern recognition (psychology)",
      "Pneumonia",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Hou",
        "given_name": "Junlin"
      },
      {
        "surname": "Xu",
        "given_name": "Jilan"
      },
      {
        "surname": "Jiang",
        "given_name": "Longquan"
      },
      {
        "surname": "Du",
        "given_name": "Shanshan"
      },
      {
        "surname": "Feng",
        "given_name": "Rui"
      },
      {
        "surname": "Zhang",
        "given_name": "Yuejie"
      },
      {
        "surname": "Shan",
        "given_name": "Fei"
      },
      {
        "surname": "Xue",
        "given_name": "Xiangyang"
      }
    ]
  },
  {
    "title": "Multimodal grid features and cell pointers for scene text visual question answering",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.06.026",
    "abstract": "This paper presents a new model for the task of scene text visual question answering. In this task questions about a given image can only be answered by reading and understanding scene text. Current state of the art models for this task make use of a dual attention mechanism in which one attention module attends to visual features while the other attends to textual features. A possible issue with this is that it makes difficult for the model to reason jointly about both modalities. To fix this problem we propose a new model that is based on an single attention mechanism that attends to multi-modal features conditioned to the question. The output weights of this attention module over a grid of multi-modal spatial features are interpreted as the probability that a certain spatial location of the image contains the answer text to the given question. Our experiments demonstrate competitive performance in two standard datasets with a model that is × 5 faster than previous methods at inference time. Furthermore, we also provide a novel analysis of the ST-VQA dataset based on a human performance study. Supplementary material, code, and data is made available through this link.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521002336",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Economics",
      "Geometry",
      "Grid",
      "Grid cell",
      "Image (mathematics)",
      "Information retrieval",
      "Linguistics",
      "Management",
      "Mathematics",
      "Modal",
      "Modalities",
      "Natural language processing",
      "Philosophy",
      "Polymer chemistry",
      "Question answering",
      "Reading (process)",
      "Social science",
      "Sociology",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Gómez",
        "given_name": "Lluís"
      },
      {
        "surname": "Biten",
        "given_name": "Ali Furkan"
      },
      {
        "surname": "Tito",
        "given_name": "Rubén"
      },
      {
        "surname": "Mafla",
        "given_name": "Andrés"
      },
      {
        "surname": "Rusiñol",
        "given_name": "Marçal"
      },
      {
        "surname": "Valveny",
        "given_name": "Ernest"
      },
      {
        "surname": "Karatzas",
        "given_name": "Dimosthenis"
      }
    ]
  },
  {
    "title": "Saliency for free: Saliency prediction as a side-effect of object recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.05.015",
    "abstract": "Saliency is the perceptual capacity of our visual system to focus our attention (i.e. gaze) on relevant objects instead of the background. So far, computational methods for saliency estimation required the explicit generation of a saliency map, process which is usually achieved via eyetracking experiments on still images. This is a tedious process that needs to be repeated for each new dataset. In the current paper, we demonstrate that is possible to automatically generate saliency maps without ground-truth. In our approach, saliency maps are learned as a side effect of object recognition. Extensive experiments carried out on both real and synthetic datasets demonstrated that our approach is able to generate accurate saliency maps, achieving competitive results when compared with supervised methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521001987",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Focus (optics)",
      "Gaze",
      "Ground truth",
      "Image (mathematics)",
      "Kadir–Brady saliency detector",
      "Neuroscience",
      "Object (grammar)",
      "Operating system",
      "Optics",
      "Pattern recognition (psychology)",
      "Perception",
      "Physics",
      "Process (computing)",
      "Psychology",
      "Saliency map"
    ],
    "authors": [
      {
        "surname": "Figueroa-Flores",
        "given_name": "Carola"
      },
      {
        "surname": "Berga",
        "given_name": "David"
      },
      {
        "surname": "van de Weijer",
        "given_name": "Joost"
      },
      {
        "surname": "Raducanu",
        "given_name": "Bogdan"
      }
    ]
  },
  {
    "title": "Network edge entropy decomposition with spin statistics",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108040",
    "abstract": "In a previous study, we have explored how to decompose the global entropy of a network into edge components using a graph-spectral decomposition technique. Here, we develop this work in more depth to understand the role of edge entropy as an efficient and effective tool in analysing network structure. We use the edge entropy distribution as a network feature or characterisation and combine it with linear discriminant analysis to distinguish different types of network model and structure. Interpreting the normalised Laplacian matrix as the network Hamiltonian (or energy) operator, the network is assumed to be in thermodynamic equilibrium with a heat bath where the energy states correspond to the normalised Laplacian eigenvalues. To model the way in which particles occupy the energy states, we explore the use of three different spin-dependent statistical models to determine the thermodynamic entropy of the network. These are a) the classical spinless Maxwell-Boltzmann distribution, and two models based on quantum mechanical spin-statistics, namely b) the Bose-Einstein model for particles with integer spin, and c) the Fermi-Dirac model for particles with half-integer spin. By using the spectral decomposition of the Laplacian, we illustrate how to project out the edge-entropy components from the global network entropy. In this way, the detailed distribution of entropy across the edges of a network can be constructed. Compared to our previous study of the von Neumann edge entropy, where the edge entropy just depends on the degrees of the nodes forming an edge, in the case of the new statistical mechanical model, there is a more subtle dependence of the edge entropy on the structure of a network. We illustrate how this new edge entropy distribution can be used to more effectively identify variations in network structure, in particular for edges incorporating nodes of large degree. Numerical experiments on synthetic and real-world data-sets are presented to evaluate the qualitative and quantitative differences in performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321002272",
    "keywords": [
      "Entropy (arrow of time)",
      "Joint quantum entropy",
      "Laplace operator",
      "Mathematics",
      "Physics",
      "Principle of maximum entropy",
      "Quantum",
      "Quantum discord",
      "Quantum entanglement",
      "Quantum mechanics",
      "Quantum relative entropy",
      "Statistical physics",
      "Statistics",
      "Von Neumann entropy"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Jianjia"
      },
      {
        "surname": "Wilson",
        "given_name": "Richard C."
      },
      {
        "surname": "Hancock",
        "given_name": "Edwin R."
      }
    ]
  },
  {
    "title": "Deep graph learning for semi-supervised classification",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108039",
    "abstract": "Graph learning (GL) can dynamically capture the distribution structure (graph structure) of data based on graph convolutional networks (GCN), and the learning quality of the graph structure directly influences GCN for semi-supervised classification. Most existing methods combine the computational layer and the related losses into GCN for exploring the global graph (measuring graph structure from all data samples) or local graph (measuring graph structure from local data samples). The global graph emphasizes the whole structure description of the inter-class data, while the local graph tends to the neighborhood structure representation of the intra-class data. However, it is difficult to simultaneously balance these learning process graphs for semi-supervised classification because of the interdependence of these graphs. To simulate the interdependence, deep graph learning (DGL) is proposed to find a better graph representation for semi-supervised classification. DGL can not only learn the global structure by the previous layer metric computation updating, but also mine the local structure by next layer local weight reassignment. Furthermore, DGL can fuse the different structures by dynamically encoding the interdependence of these structures, and deeply mine the relationship of the different structures by hierarchical progressive learning to improve the performance of semi-supervised classification. Experiments demonstrate that the DGL outperforms state-of-the-art methods on three benchmark datasets (Citeseer, Cora, and Pubmed) for citation networks and two benchmark datasets (MNIST and Cifar10) for images.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321002260",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Deep learning",
      "Graph",
      "MNIST database",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Semi-supervised learning",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Lin",
        "given_name": "Guangfeng"
      },
      {
        "surname": "Kang",
        "given_name": "Xiaobing"
      },
      {
        "surname": "Liao",
        "given_name": "Kaiyang"
      },
      {
        "surname": "Zhao",
        "given_name": "Fan"
      },
      {
        "surname": "Chen",
        "given_name": "Yajun"
      }
    ]
  },
  {
    "title": "Blind decision making: Reinforcement learning with delayed observations",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.06.022",
    "abstract": "Many applications, such as robotics, are increasingly utilizing Reinforcement Learning (RL). The current implementation of RL typically assumes that the state update from the previous actions happens instantaneously, and thus can be used for making future decisions. However, this may not always be true. When the state update is not available, the decision is taken partly in the blind since it cannot rely on the current state information. This paper proposes an approach, where the delay in the knowledge of the state can be used, and the decisions are made based on the available information which may not include the current state information. One approach could be to include the actions after the last-known state as a part of the state information, however, that leads to an increased state-space making the problem complex and slower in convergence. The proposed algorithm gives an alternate approach where the state space is not enlarged, as compared to the case when there is no delay in the state update. Evaluations on the control environments further illustrate the improved performance of the proposed algorithm.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521002282",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Convergence (economics)",
      "Current (fluid)",
      "Economic growth",
      "Economics",
      "Electrical engineering",
      "Engineering",
      "Machine learning",
      "Mathematics",
      "Reinforcement learning",
      "Robot",
      "Robotics",
      "State (computer science)",
      "State information",
      "State space",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Agarwal",
        "given_name": "Mridul"
      },
      {
        "surname": "Aggarwal",
        "given_name": "Vaneet"
      }
    ]
  },
  {
    "title": "On assigning probabilities to new hypotheses",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.07.011",
    "abstract": "The paper proposes the way how to assign a proper prior probability to a new, generally compound, hypothesis. To this purpose, it uses the minimum relative-entropy principle and a forecaster-based knowledge transfer. Methodologically, it opens a way towards enriching the standard Bayesian framework by the possibility to extend the set of models during learning without the need to restart. The presented use scenarios concern: (a) creating new hypotheses, (b) learning problems with an insufficient amount of data, and (c) sequential Monte Carlo estimation. They indicate a strong application potential of the proposed technique. Related interesting open research problems are listed.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521002567",
    "keywords": [
      "Artificial intelligence",
      "Bayesian probability",
      "Computer science",
      "Data mining",
      "Entropy (arrow of time)",
      "Kullback–Leibler divergence",
      "Machine learning",
      "Mathematics",
      "Monte Carlo method",
      "Physics",
      "Principle of maximum entropy",
      "Programming language",
      "Quantum mechanics",
      "Set (abstract data type)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Kárný",
        "given_name": "Miroslav"
      }
    ]
  },
  {
    "title": "Normalized edge convolutional networks for skeleton-based hand gesture recognition",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108044",
    "abstract": "Dynamic hand skeletons consisting of discrete spatial-temporal finger joint clouds effectively convey the intentions of communicators. Previous graph convolutional networks (GCNs) relying on human hand-crafted inductive biases have been quickly promoted for skeleton-based hand gesture recognition (SHGR). However, most existing graph constructions for GCN-based solutions are set manually, only considering the physical topology of the hand skeleton, and the fixed dependencies among hand joints may lead to suboptimal models. To enrich the local dependencies, we emphasize that hand skeletons can be seen from two views: explicit joint clouds and implicit skeleton topology. Starting from those two views of hand gestures, we attempt to introduce dynamics and diversities into the local neighborhood of the graph by dividing it into sets of physical neighbors, temporal neighbors and varying neighbors. Next, we systematically proceed with three innovations, including the novel edge-varying graph, normalized edge convolution operation, and zig-zag sampling strategy, to alleviate the challenges resulting from engineering practices. Finally, spatial-based GCNs called normalized edge convolutional networks are constructed for hand gesture recognition. Experiments on publicly available hand datasets show that our work is stable for performing state-of-the-art gesture recognition, and ablation experiments are also provided to validate each contribution.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321002314",
    "keywords": [
      "Architectural engineering",
      "Artificial intelligence",
      "Artificial neural network",
      "Combinatorics",
      "Computer science",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Engineering",
      "Enhanced Data Rates for GSM Evolution",
      "Gesture",
      "Gesture recognition",
      "Graph",
      "Human skeleton",
      "Joint (building)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Programming language",
      "Set (abstract data type)",
      "Skeleton (computer programming)",
      "Theoretical computer science",
      "Topology (electrical circuits)"
    ],
    "authors": [
      {
        "surname": "Guo",
        "given_name": "Fangtai"
      },
      {
        "surname": "He",
        "given_name": "Zaixing"
      },
      {
        "surname": "Zhang",
        "given_name": "Shuyou"
      },
      {
        "surname": "Zhao",
        "given_name": "Xinyue"
      },
      {
        "surname": "Fang",
        "given_name": "Jinhui"
      },
      {
        "surname": "Tan",
        "given_name": "Jianrong"
      }
    ]
  },
  {
    "title": "A review of methods for imbalanced multi-label classification",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107965",
    "abstract": "Multi-Label Classification (MLC) is an extension of the standard single-label classification where each data instance is associated with several labels simultaneously. MLC has gained much importance in recent years due to its wide range of application domains. However, the class imbalance problem has become an inherent characteristic of many multi-label datasets, where the samples and their corresponding labels are non-uniformly distributed over the data space. The imbalanced problem in MLC imposes challenges to multi-label data analytics which can be viewed from three perspectives: imbalance within labels, among labels, and label-sets. In this paper, we provide a review of the approaches for handling the imbalance problem in multi-label data by collecting the existing research work. As the first systematic study of approaches addressing an imbalanced problem in MLC, this paper provides a comprehensive survey of the state-of-the-art methods for imbalanced MLC, including the characteristics of imbalanced multi-label datasets, evaluation measures and comparative analysis of the proposed methods. The study also discusses important results reported so far in the literature and highlights some of their strengths and limitations to guide future research.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321001527",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Composite material",
      "Computer science",
      "Data mining",
      "Data science",
      "Machine learning",
      "Materials science",
      "Multi-label classification",
      "Range (aeronautics)"
    ],
    "authors": [
      {
        "surname": "Tarekegn",
        "given_name": "Adane Nega"
      },
      {
        "surname": "Giacobini",
        "given_name": "Mario"
      },
      {
        "surname": "Michalak",
        "given_name": "Krzysztof"
      }
    ]
  },
  {
    "title": "FM-based: Algorithm research on rural tourism recommendation combining seasonal and distribution features",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2018.12.022",
    "abstract": "Recommended technologies for the tourism system, such as the travelocity.com and visiteurope.com, have gained tremendous popularity in the past few years. Although many research works have been dedicated to improving recommendation services and overcoming recommendation challenges, little attention has been attached to the application of recommended technology in Rural Tourism. In order to solve the difficult ‘what to choose’ problem caused by information overload, this paper combines the features of Rural Tourism, giving a first attempt in this field. First, an effective method of seasonal feature extraction is proposed. Next, a detailed method of geographical distribution feature extraction is described. In particular, a block-number basing on the geographical distribution feature to the Attractions is proposed. Furthermore, the FM-based (Factorization Machines) algorithm is presented as a recommended solution for Rural Tourism. Finally, the comparative experiments are provided to prove the effectiveness of this solution.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786551830936X",
    "keywords": [
      "Algorithm",
      "Archaeology",
      "Computer science",
      "Data mining",
      "Distribution (mathematics)",
      "Feature (linguistics)",
      "Field (mathematics)",
      "Geography",
      "Information overload",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Philosophy",
      "Popularity",
      "Psychology",
      "Pure mathematics",
      "Rural tourism",
      "Social psychology",
      "Tourism",
      "Tourism geography",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Xiaojian"
      },
      {
        "surname": "Yu",
        "given_name": "Limin"
      },
      {
        "surname": "Wang",
        "given_name": "Minjuan"
      },
      {
        "surname": "Gao",
        "given_name": "Wanlin"
      }
    ]
  },
  {
    "title": "Density-aware and background-aware network for crowd counting via multi-task learning",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.07.013",
    "abstract": "In this paper, we propose a density-aware and background-aware network via multi-task learning (MTL-DB) for crowd counting. It aims to enable the model to capture the high-level semantic information of density and background via multi-task joint training, which may jointly optimize the generation of density maps. Initially, MTL-DB utilizes the first ten layers of VGG-16 with Batch Normalization as the front-end to extract primary features which will be shared by all tasks. Then, a multi-task back-end is constructed by integrating the main task of density map estimation with two auxiliary tasks, i.e., density classification and background segmentation. The density classification auxiliary task captures the density-related information with a fully connected classifier, while the background segmentation auxiliary task applies dilated convolutional network to distinguish the head area of pedestrians and background. With high-level semantic awareness, the main task generates estimated density maps utilizing normal convolutional layers. Furthermore, a multi-task joint loss is proposed to improve the quality of estimated density maps. Extensive experiments on three challenging crowd datasets (ShanghaiTech Part A & B, UCF_CC_50, and UCF_QNRF) verified the effectiveness of this multi-task learning model. MTL-DB outperformed other multi-task learning methods on the ShanghaiTech dataset, both Part A and Part B.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521002580",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Density estimation",
      "Economics",
      "Estimator",
      "Machine learning",
      "Management",
      "Mathematics",
      "Multi-task learning",
      "Pattern recognition (psychology)",
      "Segmentation",
      "Statistics",
      "Task (project management)",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Xinyue"
      },
      {
        "surname": "Sang",
        "given_name": "Jun"
      },
      {
        "surname": "Wu",
        "given_name": "Weiqun"
      },
      {
        "surname": "Liu",
        "given_name": "Kai"
      },
      {
        "surname": "Liu",
        "given_name": "Qi"
      },
      {
        "surname": "Xia",
        "given_name": "Xiaofeng"
      }
    ]
  },
  {
    "title": "Improving One-Shot NAS with Shrinking-and-Expanding Supernet",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108025",
    "abstract": "Training a supernet using a copy of shared weights has become a popular approach to speed up neural architecture search (NAS). However, it is difficult for supernet to accurately evaluate on a large-scale search space due to high weight coupling in weight-sharing setting. To address this, we present a shrinking-and-expanding supernet that decouples the shared parameters by reducing the degree of weight sharing, avoiding unstable and inaccurate performance estimation as in previous methods. Specifically, we propose a new shrinking strategy that progressively simplifies the original search space by discarding unpromising operators in a smart way. Based on this, we further present an expanding strategy by appropriately increasing parameters of the shrunk supernet. We provide comprehensive evidences showing that, in weight-sharing supernet, the proposed method SE-NAS brings more accurate and more stable performance estimation. Experimental results on ImageNet dataset indicate that SE-NAS achieves higher Top-1 accuracy than its counterparts under the same complexity constraint and search space. The ablation study is presented to further understand SE-NAS.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321002120",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Business",
      "Chemistry",
      "Computer science",
      "Constraint (computer-aided design)",
      "Coupling (piping)",
      "Economic policy",
      "Engineering",
      "European union",
      "Geometry",
      "Machine learning",
      "Mathematics",
      "Mechanical engineering",
      "Operating system",
      "Organic chemistry",
      "Physics",
      "Quantum mechanics",
      "Resizing",
      "Scale (ratio)",
      "Shot (pellet)",
      "Space (punctuation)"
    ],
    "authors": [
      {
        "surname": "Hu",
        "given_name": "Yiming"
      },
      {
        "surname": "Wang",
        "given_name": "Xingang"
      },
      {
        "surname": "Li",
        "given_name": "Lujun"
      },
      {
        "surname": "Gu",
        "given_name": "Qingyi"
      }
    ]
  },
  {
    "title": "Robust gait recognition using hybrid descriptors based on Skeleton Gait Energy Image",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2019.05.012",
    "abstract": "Gait features have been widely applied in human identification. The commonly-used representations for gait recognition can be roughly classified into two categories: model-free features and model-based features. However, due to the view variances and clothes changes, model-free features are sensitive to the appearance changes. For model-based features, there is great difficulty in extracting the underlying models from gait sequences. Based on the confidence maps and the part affinity fields produced by a two-branch multi-stage CNN network, a new model-based representation, Skeleton Gait Energy Image (SGEI), has been proposed in this paper. Another contribution is that a hybrid representation has been produced, which uses SGEI to remedy the deficiency of model-free features, Gait Energy Image (GEI) for instance. The experimental performances indicate that our proposed methods are more robust to the cloth changes, and contribute to increasing the robustness of gait recognition in the unconstrained environments with view variances and clothes changes.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519301618",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Gait",
      "Gait analysis",
      "Medicine",
      "Pattern recognition (psychology)",
      "Physical medicine and rehabilitation",
      "Programming language",
      "Skeleton (computer programming)"
    ],
    "authors": [
      {
        "surname": "Yao",
        "given_name": "Lingxiang"
      },
      {
        "surname": "Kusakunniran",
        "given_name": "Worapan"
      },
      {
        "surname": "Wu",
        "given_name": "Qiang"
      },
      {
        "surname": "Zhang",
        "given_name": "Jian"
      },
      {
        "surname": "Tang",
        "given_name": "Zhenmin"
      },
      {
        "surname": "Yang",
        "given_name": "Wankou"
      }
    ]
  },
  {
    "title": "Modeling local and global behavior for trajectory classification using graph based algorithm",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2019.05.014",
    "abstract": "Understanding motion patterns is of great importance to analyze the behavior of objects in the vigilance area. Grouping the motion patterns into clusters in such a way that similar motion patterns lie in same cluster and the inter-cluster variance is maximized in a challenging work. Variation in the duration of trajectory patterns in terms of time or number of points in them (even in the trajectories from same cluster) makes it more difficult to correctly classify in respective clusters while using full length trajectories, local clue can be used along with the global information. Trajectories can be segmented into distinctive parts and local contribution of these parts can be used to improve the performance of the system. In this work, we have formulated the trajectory classification problem into graph based similarity problem using Douglas–Peucker (DP) algorithm, Complete Bipartite Graphs (CBG), and Minimum Spanning Tree (MST). Local behavior of objects has been analyzed using their motion segments and Dynamic Time Warping (DTW) has been used for finding similarity among motion trajectories. Class-wise global and local costs have been computed using DTW, CBG, and MST and their fusion has been done using Particle Swarm Optimization (PSO) to improve the classification rate. Trajectory datasets, namely T15, LabOmni, and CROSS have been used in experiments. The proposed method yields encouraging results and outperforms the state of the art techniques.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519301631",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Astronomy",
      "Bipartite graph",
      "Computer science",
      "Dynamic time warping",
      "Graph",
      "Image (mathematics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Similarity (geometry)",
      "Theoretical computer science",
      "Trajectory"
    ],
    "authors": [
      {
        "surname": "Saini",
        "given_name": "Rajkumar"
      },
      {
        "surname": "Kumar",
        "given_name": "Pradeep"
      },
      {
        "surname": "Roy",
        "given_name": "Partha Pratim"
      },
      {
        "surname": "Pal",
        "given_name": "Umapada"
      }
    ]
  },
  {
    "title": "Perturbation-based methods for explaining deep neural networks: A survey",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.06.030",
    "abstract": "Deep neural networks (DNNs) have achieved state-of-the-art results in a broad range of tasks, in particular the ones dealing with the perceptual data. However, full-scale application of DNNs in safety-critical areas is hindered by their black box-like nature, which makes their inner workings nontransparent. As a response to the black box problem, the field of explainable artificial intelligence (XAI) has recently emerged and is currently rapidly growing. The present survey is concerned with perturbation-based XAI methods, which allow to explore DNN models by perturbing their input and observing changes in the output. We present an overview of the most recent research focusing on the differences and similarities in the applications of perturbation-based methods to different data types, from extensively studied perturbations of images to the just emerging research on perturbations of video, natural language, software code, and reinforcement learning entities.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521002440",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Black box",
      "Computer science",
      "Deep neural networks",
      "Machine learning",
      "Neuroscience",
      "Perception",
      "Perturbation (astronomy)",
      "Physics",
      "Programming language",
      "Quantum mechanics",
      "Reinforcement learning",
      "Software"
    ],
    "authors": [
      {
        "surname": "Ivanovs",
        "given_name": "Maksims"
      },
      {
        "surname": "Kadikis",
        "given_name": "Roberts"
      },
      {
        "surname": "Ozols",
        "given_name": "Kaspars"
      }
    ]
  },
  {
    "title": "Monocular multi-person pose estimation: A survey",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108046",
    "abstract": "Multi-person pose estimation in unconstrained scenarios, with an unknown number of individuals, is a main step towards scene understanding and action recognition. Due to the recent advancements on the architecture of convolutional networks, body part detectors are now accurate and estimate poses in real-time for both single- and multi-person scenes. In contrast, assigning detected body parts to coherent human poses when there are multiple persons interacting is an arduous task. To name a few of the challenges that arise in such scenes: person-to-person occlusion, truncated body parts, and more sources for double counting. Recently, the community contributed towards solving most of them. Hence, it would be interesting to analyze and compile successful approaches from current literature into research trends, and identify possible gaps for future works. To the best of our knowledge, there is no up-to-date review on the main advancements in the field that target this particular set of challenges. This survey fills this gap by reviewing the main breakthroughs on multi-person pose estimation over the last decade and summarizing their impact on the state-of-the-art. Regarding scientific contributions, we propose a novel taxonomy that categorizes the reviewed methods according to their main contributions to the pose estimation pipeline, lists the main datasets and evaluation metrics to train new models, and provides insights on the best entries of publicly available benchmarks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321002338",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data science",
      "Economics",
      "Estimation",
      "Field (mathematics)",
      "Machine learning",
      "Management",
      "Mathematics",
      "Pipeline (software)",
      "Pose",
      "Programming language",
      "Pure mathematics",
      "Set (abstract data type)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Souza dos Reis",
        "given_name": "Eduardo"
      },
      {
        "surname": "Seewald",
        "given_name": "Lucas Adams"
      },
      {
        "surname": "Antunes",
        "given_name": "Rodolfo Stoffel"
      },
      {
        "surname": "Rodrigues",
        "given_name": "Vinicius Facco"
      },
      {
        "surname": "da Rosa Righi",
        "given_name": "Rodrigo"
      },
      {
        "surname": "da Costa",
        "given_name": "Cristiano André"
      },
      {
        "surname": "da Silveira Jr.",
        "given_name": "Luiz Gonzaga"
      },
      {
        "surname": "Eskofier",
        "given_name": "Bjoern"
      },
      {
        "surname": "Maier",
        "given_name": "Andreas"
      },
      {
        "surname": "Horz",
        "given_name": "Tim"
      },
      {
        "surname": "Fahrig",
        "given_name": "Rebecca"
      }
    ]
  },
  {
    "title": "Sequential fusion of facial appearance and dynamics for depression recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.07.005",
    "abstract": "In mental health assessment, it is validated that nonverbal cues like facial expressions can be indicative of depressive disorders. Recently, the multimodal fusion of facial appearance and dynamics based on convolutional neural networks has demonstrated encouraging performance in depression analysis. However, correlation and complementarity between different visual modalities have not been well studied in prior methods. In this paper, we propose a sequential fusion method for facial depression recognition. For mining the correlated and complementary depression patterns in multimodal learning, a chained-fusion mechanism is introduced to jointly learn facial appearance and dynamics in a unified framework. We show that such sequential fusion can provide a probabilistic perspective of the model correlation and complementarity between two different data modalities for improved depression recognition. Results on a benchmark dataset show the superiority of our method against several state-of-the-art alternatives.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521002397",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Complementarity (molecular biology)",
      "Computer science",
      "Convolutional neural network",
      "Correlation",
      "Facial expression",
      "Fusion",
      "Genetics",
      "Geometry",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Modalities",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Qian"
      },
      {
        "surname": "Chaturvedi",
        "given_name": "Iti"
      },
      {
        "surname": "Ji",
        "given_name": "Shaoxiong"
      },
      {
        "surname": "Cambria",
        "given_name": "Erik"
      }
    ]
  },
  {
    "title": "Human motion reconstruction using deep transformer networks",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.06.018",
    "abstract": "Establishing a human motion reconstruction system from very few constraints imposed on the body has been an interesting and important research topic because it significantly reduces the degrees of freedom to be managed. However, it is a well-known mathematically ill-posed problem as the dimension of constraints is much lower than that of the human pose to be determined. Therefore, it is challenging to directly reconstruct the whole body joint information from very few constraints due to many possible solutions. To address this issue, we present a novel deep learning framework with an attention mechanism using large-scale motion capture (mocap) data for mapping very few user-defined constraints into the human motion as realistically as possible. Our system is built upon the attention networks for looking back further to achieve better results. Experimental results show that our network model is capable of producing more accurate results compared with previous approaches. We also conducted several experiments to test all possible combinations of the features extracted from the mocap data, and found the best feature combination to generate high-quality poses.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521002245",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Dimension (graph theory)",
      "Electrical engineering",
      "Engineering",
      "Feature (linguistics)",
      "Human motion",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Motion (physics)",
      "Motion capture",
      "Philosophy",
      "Pure mathematics",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Kim",
        "given_name": "Seong Uk"
      },
      {
        "surname": "Jang",
        "given_name": "Hanyoung"
      },
      {
        "surname": "Im",
        "given_name": "Hyeonseung"
      },
      {
        "surname": "Kim",
        "given_name": "Jongmin"
      }
    ]
  },
  {
    "title": "Online deep transferable dictionary learning",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108007",
    "abstract": "In real-world applications, large-scale unlabeled data usually becomes available gradually over time. Online learning is important to update models while preserving their historical knowledge. However, a time-varying distribution shift exists in incoming sequential data in online learning, resulting in a data cluster discrepancy between the incoming unlabeled data and older labeled data, which is a challenging situation for online learning. To address this issue, we propose an online deep transferable dictionary learning (ODTDL) method that simultaneously mitigates the data cluster discrepancy for incoming unlabeled data while preserving historical knowledge of older data in the dictionary. By forming a locally linear representation and association of incoming unlabeled data over a small amount of labeled data in a deep feature space, the proposed ODTDL method can reveal data cluster discrepancies. To implement this approach, we propose a two-level affiliation regularizer that both comprehensively reveals the local instance-level and global cluster-level affiliations and enables an off-the-shelf dictionary reconstruction error method to establish a knowledge transfer pipeline between the labeled and unlabeled data. For online learning, this approach further decomposes the knowledge transfer pipeline into batchwise transfer pipelines, thereby establishing batchwise transfer pipelines between labeled and unlabeled data. Finally, the proposed method is confirmed to be feasible in online semi-supervised learning (SSL) and online unsupervised domain adaptation (UDA) scenarios and demonstrates its superiority in the online setting.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321001941",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Deep learning",
      "Domain adaptation",
      "Domain knowledge",
      "Feature (linguistics)",
      "Feature learning",
      "Knowledge management",
      "Knowledge transfer",
      "Labeled data",
      "Linguistics",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pipeline (software)",
      "Programming language",
      "Semi-supervised learning",
      "Transfer of learning",
      "Unsupervised learning"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Sheng"
      },
      {
        "surname": "Wu",
        "given_name": "Ancong"
      },
      {
        "surname": "Zheng",
        "given_name": "Wei-Shi"
      }
    ]
  },
  {
    "title": "Semantic segmentation on Swiss3DCities: A benchmark study on aerial photogrammetric 3D pointcloud dataset",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.06.004",
    "abstract": "We introduce a new outdoor urban 3D pointcloud dataset, covering a total area of 2.7 km 2 , sampled from three Swiss cities with different characteristics. The dataset is manually annotated for semantic segmentation with per-point labels, and is built using photogrammetry from images acquired by multirotors equipped with high-resolution cameras. In contrast to datasets acquired with ground LiDAR sensors, the resulting point clouds are uniformly dense and complete, and are useful to disparate applications, including autonomous driving, gaming and smart city planning. As a benchmark, we report quantitative results of PointNet++, an established point-based deep 3D semantic segmentation model; on this model, we additionally study the impact of using different cities for model generalization.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521001938",
    "keywords": [
      "Aerial imagery",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Cartography",
      "Computer science",
      "Computer vision",
      "Generalization",
      "Geography",
      "Geometry",
      "Lidar",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Photogrammetry",
      "Point (geometry)",
      "Point cloud",
      "Remote sensing",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Can",
        "given_name": "Gülcan"
      },
      {
        "surname": "Mantegazza",
        "given_name": "Dario"
      },
      {
        "surname": "Abbate",
        "given_name": "Gabriele"
      },
      {
        "surname": "Chappuis",
        "given_name": "Sébastien"
      },
      {
        "surname": "Giusti",
        "given_name": "Alessandro"
      }
    ]
  },
  {
    "title": "Weak segmentation supervised deep neural networks for pedestrian detection",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108063",
    "abstract": "Semantic segmentation has been used successfully as a complementary information source in pedestrian detection. However, it requires accurate pixel-level semantic segmentation annotations for training, but it is extremely time-consuming to obtain these. In this work, we solve this problem by using weak segmentation masks automatically generated by depth images. This enables joint semantic segmentation and pedestrian detection with only ground truth bounding boxes for training. We show that this joint training boosts the performance of the pedestrian detector. Moreover, we show that fusing the outputs of the classification network and the generated segmentation masks leads to a further detection performance improvement. Extensive experiments have been conducted on three RGBD pedestrian datasets to demonstrate the effectiveness of our proposed method. As a byproduct, we also obtain pedestrian segmentation results of good quality, without using pixel-level segmentation annotations during training.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321002508",
    "keywords": [
      "Architectural engineering",
      "Artificial intelligence",
      "Artificial neural network",
      "Bounding overwatch",
      "Computer science",
      "Computer vision",
      "Engineering",
      "Ground truth",
      "Image segmentation",
      "Joint (building)",
      "Pattern recognition (psychology)",
      "Pedestrian",
      "Pedestrian detection",
      "Pixel",
      "Scale-space segmentation",
      "Segmentation",
      "Transport engineering"
    ],
    "authors": [
      {
        "surname": "Guo",
        "given_name": "Zhixin"
      },
      {
        "surname": "Liao",
        "given_name": "Wenzhi"
      },
      {
        "surname": "Xiao",
        "given_name": "Yifan"
      },
      {
        "surname": "Veelaert",
        "given_name": "Peter"
      },
      {
        "surname": "Philips",
        "given_name": "Wilfried"
      }
    ]
  },
  {
    "title": "Minimum variance-embedded kernelized extension of extreme learning machine for imbalance learning",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108069",
    "abstract": "In classification problems, detecting a skew class has extensively been studied in the machine learning community. Traditional extreme learning machine (ELM) algorithm becomes biased towards the majority class due to imbalance learning. To handle this problem, several extensions of ELM have been proposed such as variances-constrained weighted ELM (VW-ELM) and class-specific kernelized ELM (CSKELM). Kernelized ELM (KELM) has a better generalization capability than traditional ELM. This work proposes novel minimum variance embedded-kernelized weighted extreme learning machine (MVKWELM) and minimum variance-embedded class-specific kernelized extreme learning machine (MVCSKELM) methods for handling the imbalanced classification problems more effectively. These methods constitute novel extensions of the VW-ELM and CSKELM classifiers respectively. This minimum variance-embedding enhances the generalization capability of the algorithm by minimizing the intra-class variance. MVCSKELM uses the advantages of both the minimum variance-embedding framework and the class-specific regularization parameters. The proposed MVCSKELM also has comparable computational complexity compared to kernelized weighted ELM (KWELM). The proposed MVCSKELM adopted class-specific regularization parameters, which are determined by using class distribution. The proposed works are evaluated using benchmark real-world imbalanced datasets downloaded from the KEEL dataset repository. The experimental results demonstrate that MVKWELM and MVCSKELM achieve superior performance in contrast to KELM, KWELM, CCR-KELM, CSKELM, RUSBoost, WKSMOTE, VW-ELM, and EasyEnsemble for imbalance learning.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321002569",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Embedding",
      "Extreme learning machine",
      "Generalization",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Regularization (linguistics)"
    ],
    "authors": [
      {
        "surname": "Raghuwanshi",
        "given_name": "Bhagat Singh"
      },
      {
        "surname": "Shukla",
        "given_name": "Sanyam"
      }
    ]
  },
  {
    "title": "Ensemble selection with joint spectral clustering and structural sparsity",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108061",
    "abstract": "Generally, ensemble selection techniques are split into two categories: dynamic and static. Static ensemble selection selects a fixed subset of the original ensemble which improves the space complexity but is not flexible to each test instance. Dynamic ensemble selection selects base learners on-the-fly according to each test instance but it does not significantly improve the complexity. Currently, there is no ensemble selection technique that is robust to the test instances as well as improves space complexity. To narrow this gap, we propose a novel static ensemble selection method, called Ensemble Selection with Joint Spectral Clustering and Structural Sparsity. This method integrates spectral clustering and structural sparsity into a joint framework whose ensemble selection result is robust to test instances and consumes less space. Using 25 datasets from KEEL and UCI, we demonstrate the effectiveness of our proposed algorithm and its promising performance compared to that of other state-of-the-art algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032100248X",
    "keywords": [
      "Algorithm",
      "Architectural engineering",
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Engineering",
      "Ensemble learning",
      "Joint (building)",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Selection (genetic algorithm)",
      "Spectral clustering"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Zhenlei"
      },
      {
        "surname": "Zhao",
        "given_name": "Suyun"
      },
      {
        "surname": "Li",
        "given_name": "Zheng"
      },
      {
        "surname": "Chen",
        "given_name": "Hong"
      },
      {
        "surname": "Li",
        "given_name": "Cuiping"
      },
      {
        "surname": "Shen",
        "given_name": "Yufeng"
      }
    ]
  },
  {
    "title": "Deep multi-scale and multi-modal fusion for 3D object detection",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.08.028",
    "abstract": "The perception of 3D objects in the scene is the basis of autonomous driving. Most autonomous driving cars are equipped with cameras and Lidar to obtain 3D spatial information. RGB images taken from the camera and point cloud produced by Lidar both have their own advantages for 3D object detection. In order to make better use of the advantages of image data and point cloud data, a 3D object detection method based on Deep Multi-scale and Multi-modal Fusion (DMMF) is proposed. Firstly, point cloud is projected to the Bird’s Eye View (BEV) and extract BEV map and RGB image feature with feature extractor, respectively. Then, fuse the multi-modal feature with the deep multi-scale fusion method and finally input to position regression and classification network for object classification and accurate positioning. The experimental results on the benchmark KITTI dataset show that the method reaches state-of-the-art in both car and pedestrian classes, especially for hard level data, the detection AP is significantly improved.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521003305",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Cartography",
      "Computer science",
      "Computer vision",
      "Economics",
      "Electrical engineering",
      "Engineering",
      "Feature (linguistics)",
      "Feature extraction",
      "Finance",
      "Fuse (electrical)",
      "Geodesy",
      "Geography",
      "Lidar",
      "Linguistics",
      "Object detection",
      "Pattern recognition (psychology)",
      "Pedestrian",
      "Pedestrian detection",
      "Philosophy",
      "Point cloud",
      "Position (finance)",
      "RGB color model",
      "Remote sensing",
      "Scale (ratio)",
      "Sensor fusion"
    ],
    "authors": [
      {
        "surname": "Guo",
        "given_name": "Rui"
      },
      {
        "surname": "Li",
        "given_name": "Deng"
      },
      {
        "surname": "Han",
        "given_name": "Yahong"
      }
    ]
  },
  {
    "title": "MaskCOV: A random mask covariance network for ultra-fine-grained visual categorization",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108067",
    "abstract": "Ultra-fine-grained visual categorization (ultra-FGVC) categorizes objects with more similar patterns between classes than those in fine-grained visual categorization (FGVC), e.g., where the spectrum of granularity significantly moves down from classifying species to classifying cultivars within the same species. It is considered as an open research problem mainly due to the following challenges. First, the inter-class differences among images are much smaller by level of orders (e.g., cultivars in the same species) than those in current FGVC tasks (e.g., species). Second, there is only a few samples per category, which is beyond the ability of most large training data favored convolutional neural network methods. To address these problems, we propose a novel random mask covariance network (MaskCOV), which integrates an auxiliary self-supervised learning module with a powerful in-image data augmentation scheme for the ultra-FGVC. Specifically, we first uniformly partition input images into patches and then augment data by randomly shuffling and masking these patches. On top of that, we introduce an auxiliary self-supervised learning module of predicting the spatial covariance context of these patches to increase discriminability of our network for classification. Very encouraging experimental results of the proposed method in comparison with the state-of-the-art benchmarks demonstrate its superiority and potential of MaskCOV concept, which pushes research boundary forward from the fine-grained to the ultra-fine-grained visual categorization.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321002545",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Biology",
      "Categorization",
      "Computer science",
      "Context (archaeology)",
      "Convolutional neural network",
      "Covariance",
      "Machine learning",
      "Masking (illustration)",
      "Mathematics",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Spatial contextual awareness",
      "Statistics",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Yu",
        "given_name": "Xiaohan"
      },
      {
        "surname": "Zhao",
        "given_name": "Yang"
      },
      {
        "surname": "Gao",
        "given_name": "Yongsheng"
      },
      {
        "surname": "Xiong",
        "given_name": "Shengwu"
      }
    ]
  },
  {
    "title": "A novel eye center localization method for multiview faces",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108078",
    "abstract": "Existing studies on eye center localization have mostly applied localization methods on faces viewed from frontal angles or faces with small yaw-rotation angles. This study proposed a novel eye center localization method that can effectively localize the eye centers under various situations on multiview faces with large yaw-rotation angles (from +67.5° to −67.5°). First, this study developed a multiview face detector that can be applied to large yaw angles and can flexibly detect and precisely capture the face region. The face detector can facilitate the generation of satisfactory results by a complete representation generative adversarial network (CR-GAN). Because a large yaw-rotation angle can substantially reduce the completeness of eye representation in an image or even cause the eyes to disappear within the face image, this study used a CR-GAN to produce frontal face images to solve the problem of incomplete representation in multiview face images. Furthermore, this study proposed a new iris-ripple filter to increase the accuracy and robustness of gradient localization. Finally, a new depth corresponding-points conversion method was proposed to automatically estimate the rotation variable between two faces and the conversion relationship between the eye centers. This method can effectively solve the problem of instability resulting from a CR-GAN during eye generation and can ensure localization accuracy for eyeballs with subtle changes. According to the experimental results, compared with other advanced methods, the proposed method exhibited higher accuracy and robustness in eye center localization when applied to images obtained from four databases that involved various challenging situations, including large yaw-rotation angle, illumination change, head pose change, gaze interaction, and complete occlusion of eyes.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032100265X",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Detector",
      "Eye movement",
      "Face (sociological concept)",
      "Gene",
      "Pattern recognition (psychology)",
      "Robustness (evolution)",
      "Rotation (mathematics)",
      "Social science",
      "Sociology",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Hsu",
        "given_name": "Wei-Yen"
      },
      {
        "surname": "Chung",
        "given_name": "Chi-Jui"
      }
    ]
  },
  {
    "title": "MSAR-Net: Multi-scale attention based light-weight image super-resolution",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.08.011",
    "abstract": "Recently, single image super-resolution (SISR), aiming to preserve the lost structural and textural information from the input low resolution image, has witnessed huge demand from the videos and graphics industries. The exceptional success of convolution neural networks (CNNs), has absolutely revolutionized the field of SISR. However, for most of the CNN-based SISR methods, excessive memory consumption in terms of parameters and flops, hinders their application in low-computing power devices. Moreover, different state-of-the-art SR methods collect different features, by treating all the pixels contributing equally to the performance of the network. In this paper, we take into consideration both the performance and the reconstruction efficiency, and propose a Light-weight multi-scale attention residual network (MSAR-Net) for SISR. The proposed MSAR-Net consists of stack of multi-scale attention residual (MSAR) blocks for feature refinement, and an up and down-sampling projection (UDP) block for edge refinement of the extracted multi-scale features. These blocks are capable of effectively exploiting the multi-scale edge information, without increasing the number of parameters. Specially, we design our network in progressive fashion, for substituting the large scale factors ( × 4) combinations, with small scale factor ( × 2) combinations, and thus gradually exploit the hierarchical information. In parallel, for modulation of multi-scale features in global and local manners, channel and spatial attention in MSAR block is being used. Visual results and quantitative metrics of PSNR and SSIM exhibit the accuracy of the proposed approach on synthetic benchmark super-resolution datasets. The experimental analysis shows that the proposed approach outperforms the other existing methods for SISR in terms of memory footprint, inference time and visual quality.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521003020",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Block (permutation group theory)",
      "Computer science",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Enhanced Data Rates for GSM Evolution",
      "Geodesy",
      "Geography",
      "Geometry",
      "Image resolution",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Projection (relational algebra)",
      "Quantum mechanics",
      "Residual",
      "Scale (ratio)"
    ],
    "authors": [
      {
        "surname": "Mehta",
        "given_name": "Nancy"
      },
      {
        "surname": "Murala",
        "given_name": "Subrahmanyam"
      }
    ]
  },
  {
    "title": "Understanding crowd flow patterns using active-Langevin model",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108037",
    "abstract": "Crowd flow describes the elementary group behavior. Dynamics behind group behavior can help to identify abnormalities in flows. Quantifying flow dynamics can be challenging. In this paper, an algorithm has been proposed to describe groups’ movements in crowded scenarios by analyzing videos. A force model has been proposed based on the active Langevin equation, where the motion points are assumed to behave similarly to active colloidal particles in fluids. The force model is further augmented with computer-vision techniques to segment linear and non-linear flows. The evaluation of the proposed spatio-temporal flow segmentation scheme has been carried out with public datasets. Experiments reveal that the proposed system can segment the flows with lesser errors than existing methods. The segmentation accuracy and Normalized Mutual Information (NMI) have improved by 10 % as compared to existing flow segmentation algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321002247",
    "keywords": [
      "Acoustics",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Dynamics (music)",
      "Flow (mathematics)",
      "Geometry",
      "Langevin dynamics",
      "Langevin equation",
      "Mathematics",
      "Motion (physics)",
      "Physics",
      "Segmentation",
      "Statistical physics",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Behera",
        "given_name": "Shreetam"
      },
      {
        "surname": "Dogra",
        "given_name": "Debi Prosad"
      },
      {
        "surname": "Bandyopadhyay",
        "given_name": "Malay Kumar"
      },
      {
        "surname": "Roy",
        "given_name": "Partha Pratim"
      }
    ]
  },
  {
    "title": "MVFFNet: Multi-view feature fusion network for imbalanced ship classification",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.07.024",
    "abstract": "The accurate classification of moving ships is of fundamental importance to maritime authorities for ensuring the safety and security of shipping operations. With the wide use of automatic identification systems (AISs), which allow ships to receive identification/location information from nearby ships, it is feasible to classify the types of ships by analysing ship behaviour from AIS-based trajectories. However, the imbalanced features of AIS data make it difficult to achieve satisfactory classification results in the presence of several different types of ships. To overcome these potential limitations, we propose a multi-view feature fusion network (MVFFNet) to achieve accurate ship classification with imbalanced data. To guarantee the powerful representation and generalization abilities of MVFFNet, we first extract several multi-view features (i.e., motion features and morphological features) from AIS-based ship trajectories. Several kinematic variables related to ship behaviour are empirically adopted as motion features. The morphological features are automatically extracted via convolutional auto-encoder (CAE) networks. CAE networks are capable of optimally learning the features from informative trajectory images, which are strictly related to the original ship trajectories. The bidirectional gated recurrent unit (BiGRU) network is then proposed to combine multi-view features to generate the ship classification results. In addition, a hybrid loss function is presented to handle the imbalance problem of ship types, potentially leading to enhanced robustness and accuracy of ship classification. Comprehensive experiments on two realistic datasets have demonstrated that our proposed MVFFNet consistently outperforms other competing methods in terms of classification accuracy and robustness.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521002737",
    "keywords": [
      "Artificial intelligence",
      "Automatic Identification System",
      "Biochemistry",
      "Biology",
      "Botany",
      "Chemistry",
      "Computer science",
      "Data mining",
      "Feature (linguistics)",
      "Gene",
      "Identification (biology)",
      "Linguistics",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Liang",
        "given_name": "Maohan"
      },
      {
        "surname": "Zhan",
        "given_name": "Yang"
      },
      {
        "surname": "Liu",
        "given_name": "Ryan Wen"
      }
    ]
  },
  {
    "title": "Design of cancelable MCC-based fingerprint templates using Dyno-key model",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108074",
    "abstract": "Minutia Cylinder Code (MCC) is an effective, high-quality representation of local minutia structures. MCC templates demonstrate fast and excellent fingerprint matching performance, but if compromised, they can be reverse-engineered to retrieve minutia information. In this paper, we propose alignment-free cancelable MCC-based templates by exploiting the MCC feature extraction and representation. The core component of our design is a dynamic random key model, called Dyno-key model. The Dyno-key model dynamically extracts elements from MCC’s binary feature vectors based on randomly generated keys. Those extracted elements are discarded after the block-based logic operations so as to increase security. Leveling with the performance of the unprotected, reproduced MCC templates, the proposed method exhibits competitive performance in comparison with state-of-the-art cancelable fingerprint templates, as evaluated over seven public databases, FVC2002 DB1-DB3, FVC2004 DB1 and DB2, and FVC2006 DB2 and DB3. The proposed cancelable MCC-based templates satisfy all the requirements of biometric template protection.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321002612",
    "keywords": [
      "Arithmetic",
      "Artificial intelligence",
      "Binary number",
      "Computer science",
      "Computer security",
      "Feature (linguistics)",
      "Fingerprint (computing)",
      "Fingerprint recognition",
      "Key (lock)",
      "Law",
      "Linguistics",
      "Mathematics",
      "Minutiae",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Programming language",
      "Representation (politics)",
      "Template"
    ],
    "authors": [
      {
        "surname": "Bedari",
        "given_name": "Aseel"
      },
      {
        "surname": "Wang",
        "given_name": "Song"
      },
      {
        "surname": "Yang",
        "given_name": "Wencheng"
      }
    ]
  },
  {
    "title": "Feature learning and patch matching for diverse image inpainting",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108036",
    "abstract": "We present an image inpainting approach to generate diverse high-quality inpainting results. Recent advances in deep adversarial networks have led to significant improvements in the challenging task of filling large holes in natural images. Although deep generative models can generate visually plausible structures and textures, most of them are not interpretable, making it difficult to control the inpainting output. In addition, deep generative models do not have capacity to produce diverse results for each input. To address such limitations, we design a novel free-form image inpainting framework with two sequential steps: the first step formulates the inpainting process as a regression problem and utilizes a U-Net-like convolutional neural network to map an input to a coarse inpainting output, and the second step utilizes the nearest neighbor based pixel-wise matching to map the coarse output to diverse high-quality outputs. The second step allows our approach to compose novel high-quality content by copy-pasting high-frequency missing information from different training exemplars. Experiments on multiple datasets, i.e., CelebA-HQ, AFHQ, and Paris StreetView, show that our approach is able to offer multiple natural outputs with higher diversity in a controllable manner.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321002235",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Deep learning",
      "Feature (linguistics)",
      "Image (mathematics)",
      "Inpainting",
      "Linguistics",
      "Matching (statistics)",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pixel",
      "Process (computing)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Zeng",
        "given_name": "Yuan"
      },
      {
        "surname": "Gong",
        "given_name": "Yi"
      },
      {
        "surname": "Zhang",
        "given_name": "Jin"
      }
    ]
  },
  {
    "title": "Detecting pulmonary diseases using deep features in X-ray images",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108081",
    "abstract": "COVID-19 leads to radiological evidence of lower respiratory tract lesions, which support analysis to screen this disease using chest X-ray. In this scenario, deep learning techniques are applied to detect COVID-19 pneumonia in X-ray images, aiding a fast and precise diagnosis. Here, we investigate seven deep learning architectures associated with data augmentation and transfer learning techniques to detect different pneumonia types. We also propose an image resizing method with the maximum window function that preserves anatomical structures of the chest. The results are promising, reaching an accuracy of 99.8% considering COVID-19, normal, and viral and bacterial pneumonia classes. The differentiation between viral pneumonia and COVID-19 achieved an accuracy of 99.8%, and 99.9% of accuracy between COVID-19 and bacterial pneumonia. We also evaluated the impact of the proposed image resizing method on classification performance comparing with the bilinear interpolation; this pre-processing increased the classification rate regardless of the deep learning architectures used. We c ompared our results with ten related works in the state-of-the-art using eight sets of experiments, which showed that the proposed method outperformed them in most cases. Therefore, we demonstrate that deep learning models trained with pre-processed X-ray images could precisely assist the specialist in COVID-19 detection.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321002685",
    "keywords": [
      "Artificial intelligence",
      "Bilinear interpolation",
      "Computer science",
      "Computer vision",
      "Coronavirus disease 2019 (COVID-19)",
      "Deep learning",
      "Disease",
      "Image (mathematics)",
      "Infectious disease (medical specialty)",
      "Internal medicine",
      "Medicine",
      "Pathology",
      "Pattern recognition (psychology)",
      "Pneumonia",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Vieira",
        "given_name": "Pablo"
      },
      {
        "surname": "Sousa",
        "given_name": "Orrana"
      },
      {
        "surname": "Magalhães",
        "given_name": "Deborah"
      },
      {
        "surname": "Rabêlo",
        "given_name": "Ricardo"
      },
      {
        "surname": "Silva",
        "given_name": "Romuere"
      }
    ]
  },
  {
    "title": "Multi-style learning for adaptation of perception intelligence in home service robots",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.08.026",
    "abstract": "Robots need more intelligence to complete perception tasks in uncertain and unstructured environments. This paper presents a new self-evolving home service robot framework that learns new perception skills by using manually labeled data obtained in a new home environment. In this framework, a global model is trained which serves as the starting point of the robots’ local model, and an adaptation mechanism is developed in the robot to adapt the initial local model to the new home environment. First, three different data sampling styles are proposed to carry out the adaptation process, and theoretical analysis is given to explain the difference between the proposed three data sampling styles. Second, the most suitable data sampling style for incremental learning for a home service robot is determined. Third, we present a case study of multi-style learning, and the experimental results validate our analysis. The theoretical analysis and experimental results lead us to propose a guideline of data collection and labeling for robots to adapt their perception intelligence in home environments.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521003238",
    "keywords": [
      "Adaptation (eye)",
      "Archaeology",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Economics",
      "Economy",
      "Filter (signal processing)",
      "Geography",
      "Geometry",
      "Human–computer interaction",
      "Machine learning",
      "Mathematics",
      "Neuroscience",
      "Operating system",
      "Perception",
      "Point (geometry)",
      "Process (computing)",
      "Psychology",
      "Robot",
      "Sampling (signal processing)",
      "Service (business)",
      "Service robot",
      "Style (visual arts)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Qi"
      },
      {
        "surname": "Zhang",
        "given_name": "Senlin"
      },
      {
        "surname": "Sheng",
        "given_name": "Weihua"
      },
      {
        "surname": "Chen",
        "given_name": "Badong"
      },
      {
        "surname": "Liu",
        "given_name": "Meiqin"
      }
    ]
  },
  {
    "title": "A comparative study of shallow learning and deep transfer learning techniques for accurate fingerprints vitality detection",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.07.025",
    "abstract": "This work inspects deep learning architectures and shallow learning techniques to determine whether the image of a fingerprint is real (Live) or not (Fake). It is known that Deep Learning techniques deliver, in general, good accuracies being able to automatically extract relevant patterns, at the same time, it is also known that these algorithms require large amounts of data. For this reason, transfer learning aims to transfer the knowledge learnt over a huge dataset to a new, smaller dataset. In this work, because of the limited size of the LivDet2019 dataset, three well known deep learning architectures such as Inception V3, ResNet50 and NASNet Large have been modified to perform transfer learning from the huge imagenet dataset to the smaller LivDet2019. The hypothesis at the very basis of this work is that the deep learning architectures trained on the huge imagenet dataset would learn to extract relevant patterns like lines, shapes, curves, jump between curves, etc… Later, the extracted knowledge, is fine-tuned on the LivDet2019 dataset to recognize fingerprint minuities as a non-linear combination of the previously learned patterns. For sake of completeness, state of art shallow learning image descriptors, finetuned for fingerprint recognition, such as Binarized Statistical Image Features (BSIF), Local Phase Quantization (LPQ) and Weber Local Descriptor (WLD) are used for extracting features from the LivDet2019 dataset. The classification on each of these extracted features is performed both with a linear and non-linear (gaussian) support vector machine. Accuracies suggest that both shallow learning and deep learning techniques are on par with the accuracies of reviewed works and thus transfer learning in fingerprint liveness detection is a feasible strategy that deserve attention and future research with the aim of increasing fingerprint detection accuracies.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521002749",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Deep learning",
      "Fingerprint (computing)",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Support vector machine",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Impedovo",
        "given_name": "Donato"
      },
      {
        "surname": "Dentamaro",
        "given_name": "Vincenzo"
      },
      {
        "surname": "Abbattista",
        "given_name": "Giacomo"
      },
      {
        "surname": "Gattulli",
        "given_name": "Vincenzo"
      },
      {
        "surname": "Pirlo",
        "given_name": "Giuseppe"
      }
    ]
  },
  {
    "title": "Learning features from covariance matrix of gabor wavelet for face recognition under adverse conditions",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108085",
    "abstract": "Face recognition under adverse conditions, such as low-resolution, difficult illumination, blur and noise remains a challenging task. Among existing face recognition methods, Gabor wavelet plays a significant role and has robust performance under adverse conditions since it models the visual cortices of mammalian brain. It has been demonstrated the subbands of Gabor Wavelet (GW) can be efficiently represented by a covariance matrix. However, because covariance matrix does not belong to Euclidean space, Euclidean-based measure such as 2-norm cannot be directly applied to covariance matrix, and more importantly, it is difficult to incorporate learning techniques for the covariance matrix to promote the performance of face recognition. To address this issue, we propose two promising methods by learning the Covariance Matrix of Gabor Wavelet (LCMoG). The first method, called LCMoG-CNN, uses a shallow Convolutional Neural Network (CNN) to project the covariance matrices of GW into a feature vector of Euclidean space; the second method, called LCMoG-LWPZ, uses matrix-logarithm to embed the covariance matrix in the linear space and then uses Whitening Principal Component Analysis (WPCA) to learn the face features from the embedded covariance matrix. The proposed methods are effective to extract the fine features from the face image and have better performance than Deep CNN (DCNN) for small-varying face pose . For the large-varying face pose , LCMoG features combining with DCNN feature can enhance the performance of face recognition. In the experiments, the proposed methods yield promising recognition & verification accuracies under adverse conditions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321002727",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Covariance",
      "Covariance matrix",
      "Discrete wavelet transform",
      "Euclidean distance",
      "Facial recognition system",
      "Feature (linguistics)",
      "Feature vector",
      "Gabor wavelet",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Statistics",
      "Wavelet",
      "Wavelet transform"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Chaorong"
      },
      {
        "surname": "Huang",
        "given_name": "Yuanyuan"
      },
      {
        "surname": "Huang",
        "given_name": "Wei"
      },
      {
        "surname": "Qin",
        "given_name": "Fengqing"
      }
    ]
  },
  {
    "title": "PNU Spoofing: a menace for biometrics authentication systems?",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.07.008",
    "abstract": "The Pixel Non-Uniformity noise (PNU noise, for short) is a characteristic noise of digital camera sensors that has been originally used a mean to perform Source Camera Identification (SCI), that is, to identify the digital camera that has been used to take an image under scrutiny. Actually, its usage has been extended to other relevant application domains, such as carrying out sensor identification in iris biometrics, to resolve the integrity of a biometric authentication system, in health monitoring systems based on bio-signal processing, and to prevent health fraud scams. As a consequence of this popularity, several counter-forensics techniques have been proposed in the scientific literature to deceive this identification approach and, thus, putting at risk its effectiveness. The goal of this paper is to experiment with some of these counter-forensics techniques and to assess whether they are able or not to deceive the PNU-based identification process. We focus our attention on the particular case where a target image is modified so as to appear as been taken by a different device than the original one (i.e., spoofing). The results of our experiments show that this kind of spoofing is apparently successful in deceiving the PNU-based identification process. However, we also show that the spoofed images retain traces of their originating camera. By leveraging on this information, it is still possible to trace back the device used to take that image. This finding may have a deep impact on the usage of Pixel Non Uniformity (PNU) noise for identification tasks, such as the ones carried out by biometric recognition and monitoring systems, and on the possibility to really cheat them by means of counter-forensics techniques.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521002518",
    "keywords": [
      "Artificial intelligence",
      "Authentication (law)",
      "Biology",
      "Biometrics",
      "Botany",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Identification (biology)",
      "Image (mathematics)",
      "Noise (video)",
      "Operating system",
      "Pixel",
      "Process (computing)",
      "Spoofing attack"
    ],
    "authors": [
      {
        "surname": "Bruno",
        "given_name": "Andrea"
      },
      {
        "surname": "Cattaneo",
        "given_name": "Giuseppe"
      },
      {
        "surname": "Ferraro Petrillo",
        "given_name": "Umberto"
      },
      {
        "surname": "Capasso",
        "given_name": "Paola"
      }
    ]
  },
  {
    "title": "ERINet: Enhanced rotation-invariant network for point cloud classification",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.08.010",
    "abstract": "Point cloud classification has attracted increasing attention due to the outstanding performance of elaborated networks on synthetic datasets. However, rotation invariance has been seldom investigated. In this paper, we propose a straightforward rotation-invariant network called ERINet with a novel enhanced rotation-invariant module for point cloud classification. The enhanced rotation-invariant module is composed of a representation conversion component and a feature aggregation layer. It first takes 12 well-designed rotation-invariant features as the representation of point cloud and leverages the feature aggregation layer to aggregate the features of neighbor points into a discriminative rotation-invariant representation. The enhanced rotation-invariant module is further combined with the multi-layer perceptron and the fully connected layers to form an efficient ERINet. The proposed ERINet demonstrated its advantages with a small model size and high speed. The enhanced rotation-invariant module of our ERINet is also extensible and can be easily integrated with mainstream networks to improve rotation robustness. The experimental results on rotation-augmented datasets demonstrate that our ERINet outperforms other state-of-the-art methods in rotation robustness for point cloud classification.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521002877",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Discriminative model",
      "Gene",
      "Invariant (physics)",
      "Mathematical physics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Perceptron",
      "Point cloud",
      "Robustness (evolution)",
      "Rotation (mathematics)"
    ],
    "authors": [
      {
        "surname": "Gu",
        "given_name": "Ruibin"
      },
      {
        "surname": "Wu",
        "given_name": "Qiuxia"
      },
      {
        "surname": "Ng",
        "given_name": "Wing W.Y."
      },
      {
        "surname": "Xu",
        "given_name": "Hongbin"
      },
      {
        "surname": "Wang",
        "given_name": "Zhiyong"
      }
    ]
  },
  {
    "title": "A new similarity-based classifier with Dombi aggregative operators",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.08.024",
    "abstract": "In this paper we extend the similarity classifier to cover Dombi aggregation operators. The similarity classifier was earlier studied with the ordered weighted averaging (OWA) operator, the generalized mean, and other operators. We concentrate on the use of Dombi operators during aggregation of similarities within the classifier. Four Dombi aggregation operators applied here include: conjunctive, disjunctive, weighted conjunctive, and the product operator. From each of these operators, we form a variant of the similarity classifier. The proposed methods were tested on four real world medical datasets which include: fertility, lung cancer, Haberman’s survival, and liver disorder. The new classifiers achieved improved classification accuracies compared with earlier methods. Compared with the classifier using the generalized mean, the proposed method achieved an improvement of 17.80 % on fertility, 5.88 % on lung cancer, 0.65 % on Haberman’s survival, and 0.29 % on liver disorder datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521003214",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Classifier (UML)",
      "Computer science",
      "Gene",
      "Mathematics",
      "Operator (biology)",
      "Pattern recognition (psychology)",
      "Repressor",
      "Transcription factor"
    ],
    "authors": [
      {
        "surname": "Kurama",
        "given_name": "Onesfole"
      }
    ]
  },
  {
    "title": "Metaheuristic Search Based Feature Selection Methods for Classification of Cancer",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108079",
    "abstract": "Cancer is a cluster of diseases caused due to unusual cell growth. This paper aims to discover cancer prediction from the microarray gene expression data using the selected features. The metaheuristic search algorithms select the global and local optimal features using population and neighbourhood based algorithms. Although the ant colony optimization and genetic algorithm search for the global optimal features from the dataset entails enhanced classification, sometimes there occur some challenges in the selection of neighbourhood features. Against this background, two feature selection algorithms are proposed to hybridize tabu search, a neighbourhood based search algorithm with global optimal feature selection algorithm. Those are (1) Ant Colony Optimization and Tabu search with Fuzzy Rough set for Optimal feature selection (ACTFRO) algorithm, (2) Genetic algorithm and Tabu search with Fuzzy Rough set for Optimal feature selection (GATFRO) algorithm. The performance of proposed feature selection algorithms is assessed through a fuzzy rough nearest neighbour classifier using ten-fold cross validation. Four cancer medical datasets and one non-medical dataset are used to analyse the performance of the proposed algorithms in terms of classification accuracy, computation time, sensitivity, specificity, f-measure, receiver operation characteristics and positive predicted value. Results derived from the different performance metrics confirm that the proposed algorithms evidence effective global and local feature selection hybridization with improved results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321002661",
    "keywords": [
      "Algorithm",
      "Ant colony optimization algorithms",
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Feature selection",
      "Genetic algorithm",
      "Machine learning",
      "Metaheuristic",
      "Pattern recognition (psychology)",
      "Tabu search"
    ],
    "authors": [
      {
        "surname": "Meenachi",
        "given_name": "L."
      },
      {
        "surname": "Ramakrishnan",
        "given_name": "S."
      }
    ]
  },
  {
    "title": "Holistic word descriptor for lexicon reduction in handwritten arabic documents",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108072",
    "abstract": "Most of word recognition systems rely on a pre-defined lexicon in aims to achieve high performance. Recently, the availability of training /testing data allows to include a huge number of words in the lexicon to recognize. However, this leads to high computation cost as the lexicon is grown. In addition, including more and more word-classes may lead to increase the burden on classification methods and degrade the recognition rate. In this work, we propose a holistic word descriptor for word lexicon reduction in Arabic handwritten documents. The proposed descriptor represents geometrical features of word shape through three main feature sets, defined from multi-scale convexity concavity analysis. The first two sets are dedicated to defined the number of peaks and their intensity levels of convexity/concavity peaks, respectively. In contrast, the last set is dedicated to define a region codes of the peaks by analyzing their regions according to their spatial information. Given a query word and lexicon(reference dataset), the lexicon reduction system is applied by first defining the holistic word descriptor for both query word and each word in the lexicon. The lexicon is then indexed according to its distances to the query word descriptor. Finally, the reduced lexicon is formulated from the first k th entries of the indexed lexicon. The proposed system has been evaluated under two well-known Arabic datasets, namely Ibn Sina and IFN/ENIT. Reported results show superior performance compared to prior art, with 93.7 % and 91.2 % reduction efficacy for Ibn Sina and IFN/ENIT, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321002594",
    "keywords": [
      "Arabic",
      "Artificial intelligence",
      "Computer science",
      "Geometry",
      "Lexicon",
      "Linguistics",
      "Mathematics",
      "Natural language processing",
      "Philosophy",
      "Reduction (mathematics)",
      "Speech recognition",
      "Word (group theory)"
    ],
    "authors": [
      {
        "surname": "Elaiwat",
        "given_name": "Said"
      }
    ]
  },
  {
    "title": "Simultaneous positive sequential vectors modeling and unsupervised feature selection via continuous hidden Markov models",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108073",
    "abstract": "Since positive data vectors are often naturally generated in various real-life applications, positive vectors modeling has become an important research topic. In this article, we tackle the problem of modeling positive sequential vectors through continuous hidden Markov models (HMMs). Motivated by several recent studies in which the generalized inverted Dirichlet (GID) distribution has provided better performance than the Gaussian distribution for modeling positive data, instead of adopting Gaussian mixture models (GMM) as the emission density for conventional continuous HMMs, we theoretically propose a novel HMM by considering the mixture of GID distributions as the emission density. Moreover, to cope with high-dimensional data which may contain irrelevant features, an unsupervised localized feature selection method is incorporated with our model, which results in a unified framework that can simultaneously perform positive sequential data modeling and feature selection. To learn the proposed model, we develop a convergence-guaranteed algorithm based on variational Bayes. The advantages of our model are demonstrated through both simulated data sets and a real-life application about human action recognition.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321002600",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Bayesian probability",
      "Boundary value problem",
      "Computer science",
      "Dirichlet distribution",
      "Feature (linguistics)",
      "Feature selection",
      "Feature vector",
      "Hidden Markov model",
      "Hyperparameter",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Mixture model",
      "Model selection",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Prior probability"
    ],
    "authors": [
      {
        "surname": "Fan",
        "given_name": "Wentao"
      },
      {
        "surname": "Wang",
        "given_name": "Ru"
      },
      {
        "surname": "Bouguila",
        "given_name": "Nizar"
      }
    ]
  },
  {
    "title": "Robust Multichannel EEG Signal Reconstruction Method",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.08.014",
    "abstract": "Reducing noise is an urgent problem that solved in practical application, so designing robust signal recovery approach based on compressive sensing (CS) theory is critical for the multichannel electroencephalogram (EEG) signals. However, almost all the algorithms seldom regard noise or just take Gaussian noise emerged in transmission into account. When the original signal is affected by noise before compression and transmission, all these CS approaches will be out of effect. In order to abase the impact of impulsive noise, a new robust CS approach based on cosparse and low-rank priors is proposed to recover the clean original signal near perfectly. Specially, our model employs Welsch estimator to retrain the impulsive noise, and weighted schatten- p norm to efficiently utilize low-rank prior knowledge. Moreover, we present an iterative optimization strategy to reconstruct the signals. Experimental results have demonstrated that the proposed algorithm can exactly reconstruct the multichannel EEG signals under the impact of impulsive noise.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521003056",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Bayesian probability",
      "Biochemistry",
      "Chemistry",
      "Compressed sensing",
      "Computer science",
      "Electroencephalography",
      "Estimator",
      "Gaussian noise",
      "Gene",
      "Image (mathematics)",
      "Mathematics",
      "Noise (video)",
      "Pattern recognition (psychology)",
      "Prior probability",
      "Programming language",
      "Psychiatry",
      "Psychology",
      "Radar",
      "Robustness (evolution)",
      "SIGNAL (programming language)",
      "Signal processing",
      "Signal reconstruction",
      "Speech recognition",
      "Statistics",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Zhu",
        "given_name": "Jun"
      },
      {
        "surname": "Feng",
        "given_name": "Lei"
      },
      {
        "surname": "Mo",
        "given_name": "Xiaohui"
      }
    ]
  },
  {
    "title": "A method for user-customized compensation of metamorphopsia through video see-through enabled head mounted display",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.09.010",
    "abstract": "Advances in Augmented Reality technologies and, particularly, the availability of video see-through enabled head mounted displays (HMD), are allowing to devise new strategies to help individuals with visual impairments in daily life. In this work, an approach is proposed to compensate a serious visual impairment, known as metamorphopsia, a vision disorder characterized by deformed images. The goal is to provide patients with a digitally restored visual field, through real-time processing of video see-through streams captured from the HMD. To this regard, we present two contributions, respectively, an interactive discrete modeling of patient’s eye-specific vision distortion and a compensation of the latter by means of corresponding real-time counter-distortion of incoming frames. Our approach, indeed, maps each of the video streams acquired by the stereoscopic video see-through cameras aboard the headset on a 2D polygonal mesh which is then counter-warped by moving its vertices based on the previously built distortion model and then displayed, restored, on the HMD’s screen. First user evaluations report promising results along with usability issues related to HMD technology.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521003354",
    "keywords": [
      "Artificial intelligence",
      "Compensation (psychology)",
      "Computer graphics (images)",
      "Computer science",
      "Computer vision",
      "Geology",
      "Geomorphology",
      "Head (geology)",
      "Human–computer interaction",
      "Metamorphopsia",
      "Optical head-mounted display",
      "Optics",
      "Physics",
      "Psychoanalysis",
      "Psychology",
      "Visual acuity"
    ],
    "authors": [
      {
        "surname": "Cimmino",
        "given_name": "Lucia"
      },
      {
        "surname": "Pero",
        "given_name": "Chiara"
      },
      {
        "surname": "Ricciardi",
        "given_name": "Stefano"
      },
      {
        "surname": "Wan",
        "given_name": "Shaohua"
      }
    ]
  },
  {
    "title": "Lung segmentation and automatic detection of COVID-19 using radiomic features from chest CT images",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108071",
    "abstract": "This paper aims to develop an automatic method to segment pulmonary parenchyma in chest CT images and analyze texture features from the segmented pulmonary parenchyma regions to assist radiologists in COVID-19 diagnosis. A new segmentation method, which integrates a three-dimensional (3D) V-Net with a shape deformation module implemented using a spatial transform network (STN), was proposed to segment pulmonary parenchyma in chest CT images. The 3D V-Net was adopted to perform an end-to-end lung extraction while the deformation module was utilized to refine the V-Net output according to the prior shape knowledge. The proposed segmentation method was validated against the manual annotation generated by experienced operators. The radiomic features measured from our segmentation results were further analyzed by sophisticated statistical models with high interpretability to discover significant independent features and detect COVID-19 infection. Experimental results demonstrated that compared with the manual annotation, the proposed segmentation method achieved a Dice similarity coefficient of 0.9796, a sensitivity of 0.9840, a specificity of 0.9954, and a mean surface distance error of 0.0318 mm. Furthermore, our COVID-19 classification model achieved an area under curve (AUC) of 0.9470, a sensitivity of 0.9670, and a specificity of 0.9270 when discriminating lung infection with COVID-19 from community-acquired pneumonia and healthy controls using statistically significant radiomic features. The significant features measured from our segmentation results agreed well with those from the manual annotation. Our approach has great promise for clinical use in facilitating automatic diagnosis of COVID-19 infection on chest CT images.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321002582",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Coronavirus disease 2019 (COVID-19)",
      "Disease",
      "Image (mathematics)",
      "Image segmentation",
      "Infectious disease (medical specialty)",
      "Medicine",
      "Pathology",
      "Pattern recognition (psychology)",
      "Segmentation",
      "Similarity (geometry)",
      "Sørensen–Dice coefficient"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Chen"
      },
      {
        "surname": "Xu",
        "given_name": "Yan"
      },
      {
        "surname": "He",
        "given_name": "Zhuo"
      },
      {
        "surname": "Tang",
        "given_name": "Jinshan"
      },
      {
        "surname": "Zhang",
        "given_name": "Yijun"
      },
      {
        "surname": "Han",
        "given_name": "Jungang"
      },
      {
        "surname": "Shi",
        "given_name": "Yuxin"
      },
      {
        "surname": "Zhou",
        "given_name": "Weihua"
      }
    ]
  },
  {
    "title": "Deep learning approaches for workout repetition counting and validation",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.09.006",
    "abstract": "The study of the human body motion throughout different activities is one of the most challenging and long-standing problems in Computer Vision. With the recent advances in Deep Learning algorithms, the information acquired from conventional frame sensors can be used to infer the human body pose for further analysis. Specifically, one can contemplate the performance of body movements throughout physical exercises to provide feedback to the individual. Therefore, we propose a system for workout repetition counting and validation based on a set of skeleton-based and deep semantic features that are obtained from a 2D human pose estimation network. To this end, we have acquired over 130 participants performing five popular Cross-Fit exercises in order to train a Convolutional Neural Network to predict the exercises’ moments at a frame level. Hence, this underlying idea of inferring the moment of the exercise is two-fold: (i) to provide information about the exercise execution with a fine-level of detail; (ii) the ability to detect invalid repetitions promptly. Finally, a repetition counting and validation module receives the predicted moment and outputs the current number of valid repetitions that one has been performing with over 92 % precision scores for 4 out of 5 considered exercises.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552100324X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Deep learning",
      "Linguistics",
      "Machine learning",
      "Philosophy",
      "Repetition (rhetorical device)"
    ],
    "authors": [
      {
        "surname": "Ferreira",
        "given_name": "Bruno"
      },
      {
        "surname": "Ferreira",
        "given_name": "Pedro M."
      },
      {
        "surname": "Pinheiro",
        "given_name": "Gil"
      },
      {
        "surname": "Figueiredo",
        "given_name": "Nelson"
      },
      {
        "surname": "Carvalho",
        "given_name": "Filipe"
      },
      {
        "surname": "Menezes",
        "given_name": "Paulo"
      },
      {
        "surname": "Batista",
        "given_name": "Jorge"
      }
    ]
  },
  {
    "title": "IsGAN: Identity-sensitive generative adversarial network for face photo-sketch synthesis",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108077",
    "abstract": "Face photo-sketch synthesis aims to generate face sketches from real photos and vice versa. It can be abstracted as a constrained quantization problem. Although many efforts have been dedicated to this problem, it is still a challenging task to synthesize detail-preserving photos or sketches due to the significant differences between face sketch (drawn by people) and photo (taken by cameras) domains. In this paper, we propose a novel Identity-sensitive Generative Adversarial Network (IsGAN) to address it. Our key insight is to formalize face photo-sketch synthesis as a special case of image-to-image translation and propose to embed identity information through adversarial learning. In particular, an adversarial architecture is used to capture the differences between the two domains, and a new network loss, namely, identity recognition loss is introduced to preserve the detailed identifiable information, which is crucial for photo-sketch synthesis. In addition, to enforce structural consistency during generation, a cyclic-synthesized loss is applied between the generated image of one domain and cycled image of another. The experiments on the CUFS and CUFSF datasets suggest that our model achieves state-of-the-art performance in both qualitative and quantitative measures.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321002648",
    "keywords": [
      "Acoustics",
      "Adversarial system",
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Consistency (knowledge bases)",
      "Domain (mathematical analysis)",
      "Face (sociological concept)",
      "Facial recognition system",
      "Generative adversarial network",
      "Generative grammar",
      "Identity (music)",
      "Image (mathematics)",
      "Image editing",
      "Image translation",
      "Key (lock)",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantization (signal processing)",
      "Rendering (computer graphics)",
      "Sketch",
      "Social science",
      "Sociology",
      "View synthesis"
    ],
    "authors": [
      {
        "surname": "Yan",
        "given_name": "Lan"
      },
      {
        "surname": "Zheng",
        "given_name": "Wenbo"
      },
      {
        "surname": "Gou",
        "given_name": "Chao"
      },
      {
        "surname": "Wang",
        "given_name": "Fei-Yue"
      }
    ]
  },
  {
    "title": "Mining consistent correspondences using co-occurrence statistics",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108062",
    "abstract": "In this paper, we propose a mismatch removal method, which mines consistent image feature correspondences using co-occurrence statistics. The proposed method relies on a co-occurrence matrix that counts the number of pixel value pairs co-occurring within the images. Specifically, we propose to integrate the co-occurrence statistics with local spatial information, to preserve the consensus of neighborhood elements. Then, a new measure based on co-occurrence statistics is defined for correspondence similarity, to preserve the consensus of neighborhood topology. After that, with the consensus of neighborhood elements and neighborhood topology, the mismatch removal problem is formulated into a mathematical model, which has a closed-form solution. Extensive experiments show that the proposed method is able to achieve superior or competitive performance on matching accuracy over several state-of-the-art competing methods. In addition, we further exploit the consensus of neighborhood elements and neighborhood topology to propose a novel guided sampling method, which can significantly improve the quality of sampling minimal subsets over state-of-the-arts for two-view geometric model fitting.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321002491",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Feature (linguistics)",
      "Filter (signal processing)",
      "Image (mathematics)",
      "Linguistics",
      "Matching (statistics)",
      "Mathematics",
      "Measure (data warehouse)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pixel",
      "Sampling (signal processing)",
      "Similarity (geometry)",
      "Statistics",
      "Topology (electrical circuits)"
    ],
    "authors": [
      {
        "surname": "Xiao",
        "given_name": "Guobao"
      },
      {
        "surname": "Wang",
        "given_name": "Shiping"
      },
      {
        "surname": "Wang",
        "given_name": "Han"
      },
      {
        "surname": "Ma",
        "given_name": "Jiayi"
      }
    ]
  },
  {
    "title": "A survey on text generation using generative adversarial networks",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108098",
    "abstract": "This work presents a thorough review concerning recent studies and text generation advancements using Generative Adversarial Networks. The usage of adversarial learning for text generation is promising as it provides alternatives to generate the so-called “natural” language. Nevertheless, adversarial text generation is not a simple task as its foremost architecture, the Generative Adversarial Networks, were designed to cope with continuous information (image) instead of discrete data (text). Thus, most works are based on three possible options, i.e., Gumbel-Softmax differentiation, Reinforcement Learning, and modified training objectives. All alternatives are reviewed in this survey as they present the most recent approaches for generating text using adversarial-based techniques. The selected works were taken from renowned databases, such as Science Direct, IEEEXplore, Springer, Association for Computing Machinery, and arXiv, whereas each selected work has been critically analyzed and assessed to present its objective, methodology, and experimental results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321002855",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Computer science",
      "Data science",
      "Extreme value theory",
      "Generative grammar",
      "Gumbel distribution",
      "Machine learning",
      "Mathematics",
      "Natural language",
      "Natural language generation",
      "Statistics",
      "Text generation"
    ],
    "authors": [
      {
        "surname": "de Rosa",
        "given_name": "Gustavo H."
      },
      {
        "surname": "Papa",
        "given_name": "João P."
      }
    ]
  },
  {
    "title": "SCOAT-Net: A novel network for segmenting COVID-19 lung opacification from CT images",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108109",
    "abstract": "Automatic segmentation of lung opacification from computed tomography (CT) images shows excellent potential for quickly and accurately quantifying the infection of Coronavirus disease 2019 (COVID-19) and judging the disease development and treatment response. However, some challenges still exist, including the complexity and variability features of the opacity regions, the small difference between the infected and healthy tissues, and the noise of CT images. Due to limited medical resources, it is impractical to obtain a large amount of data in a short time, which further hinders the training of deep learning models. To answer these challenges, we proposed a novel spatial- and channel-wise coarse-to-fine attention network (SCOAT-Net), inspired by the biological vision mechanism, for the segmentation of COVID-19 lung opacification from CT images. With the UNet++ as basic structure, our SCOAT-Net introduces the specially designed spatial-wise and channel-wise attention modules, which serve to collaboratively boost the attention learning of the network and extract the efficient features of the infected opacification regions at the pixel and channel levels. Experiments show that our proposed SCOAT-Net achieves better results compared to several state-of-the-art image segmentation networks and has acceptable generalization ability.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032100296X",
    "keywords": [
      "Artificial intelligence",
      "Channel (broadcasting)",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Coronavirus disease 2019 (COVID-19)",
      "Deep learning",
      "Disease",
      "Generalization",
      "Geometry",
      "Image (mathematics)",
      "Image segmentation",
      "Infectious disease (medical specialty)",
      "Mathematical analysis",
      "Mathematics",
      "Medicine",
      "Net (polyhedron)",
      "Noise (video)",
      "Pathology",
      "Pattern recognition (psychology)",
      "Pixel",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Shixuan"
      },
      {
        "surname": "Li",
        "given_name": "Zhidan"
      },
      {
        "surname": "Chen",
        "given_name": "Yang"
      },
      {
        "surname": "Zhao",
        "given_name": "Wei"
      },
      {
        "surname": "Xie",
        "given_name": "Xingzhi"
      },
      {
        "surname": "Liu",
        "given_name": "Jun"
      },
      {
        "surname": "Zhao",
        "given_name": "Di"
      },
      {
        "surname": "Li",
        "given_name": "Yongjie"
      }
    ]
  },
  {
    "title": "Automated hyperparameter selection for the PC algorithm",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.09.009",
    "abstract": "The PC algorithm infers causal relations using conditional independence tests that require a pre-specified Type I α level. PC is however unsupervised, so we cannot tune α using traditional cross-validation. We therefore propose AutoPC, a fast procedure that optimizes α directly for a user chosen metric. We in particular force PC to double check its output by executing a second run on the recovered graph. We choose the final output as the one which maximizes stability between the two runs. AutoPC consistently outperforms the state of the art across multiple metrics.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521003329",
    "keywords": [
      "Algorithm",
      "Alpha (finance)",
      "Artificial intelligence",
      "Computer science",
      "Conditional independence",
      "Construct validity",
      "Economics",
      "Graph",
      "Hyperparameter",
      "Independence (probability theory)",
      "Machine learning",
      "Mathematics",
      "Metric (unit)",
      "Operations management",
      "Psychometrics",
      "Selection (genetic algorithm)",
      "Stability (learning theory)",
      "Statistics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Strobl",
        "given_name": "Eric V."
      }
    ]
  },
  {
    "title": "Non-linear and selective fusion of cross-modal images",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108042",
    "abstract": "Existing image fusion methods pay little research attention to human visual characteristics. However, human visual characteristics play an important role in visual processing tasks. To solve this problem, we propose a cross-modal image fusion method that combines illuminance factors and attention mechanisms. Human visual characteristics are studied and simulated in cross-modal image fusion task. Firstly, in order to reject high and low-frequency mixing and reduce the halo effect, we perform cross-modal image multi-scale decomposition. Secondly, in order to remove highlights, the visual saliency map and the deep feature map are combined with the illuminance fusion factor to perform high-low frequency non-linear fusion. Thirdly, the feature maps are selected through a channel attention network to obtain the final fusion map. Finally, we validate our image fusion method on public datasets of infrared and visible images. The experimental results demonstrate the superiority of our fusion method under the complex illumination environment. In addition, the experimental results also demonstrate the effectiveness of our simulation of human visual characteristics.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321002296",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Fusion",
      "Linguistics",
      "Mathematics",
      "Modal",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Polymer chemistry"
    ],
    "authors": [
      {
        "surname": "Fang",
        "given_name": "Aiqing"
      },
      {
        "surname": "Zhao",
        "given_name": "Xinbo"
      },
      {
        "surname": "Yang",
        "given_name": "Jiaqi"
      },
      {
        "surname": "Zhang",
        "given_name": "Yanning"
      },
      {
        "surname": "Zheng",
        "given_name": "Xiang"
      }
    ]
  },
  {
    "title": "Unsupervised Deep Learning based Variational Autoencoder Model for COVID-19 Diagnosis and Classification",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.08.018",
    "abstract": "At present times, COVID-19 has become a global illness and infected people has increased exponentially and it is difficult to control due to the non-availability of large quantity of testing kits. Artificial intelligence (AI) techniques including machine learning (ML), deep learning (DL), and computer vision (CV) approaches find useful for the recognition, analysis, and prediction of COVID-19. Several ML and DL techniques are trained to resolve the supervised learning issue. At the same time, the potential measure of the unsupervised learning technique is quite high. Therefore, unsupervised learning techniques can be designed in the existing DL models for proficient COVID-19 prediction. In this view, this paper introduces a novel unsupervised DL based variational autoencoder (UDL-VAE) model for COVID-19 detection and classification. The UDL-VAE model involved adaptive Wiener filtering (AWF) based preprocessing technique to enhance the image quality. Besides, Inception v4 with Adagrad technique is employed as a feature extractor and unsupervised VAE model is applied for the classification process. In order to verify the superior diagnostic performance of the UDL-VAE model, a set of experimentation was carried out to highlight the effective outcome of the UDL-VAE model. The obtained experimental values showcased the effectual results of the UDL-VAE model with the higher accuracy of 0.987 and 0.992 on the binary and multiple classes respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552100310X",
    "keywords": [
      "Artificial intelligence",
      "Autoencoder",
      "Computer science",
      "Deep learning",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Preprocessor",
      "Unsupervised learning"
    ],
    "authors": [
      {
        "surname": "Mansour",
        "given_name": "Romany F."
      },
      {
        "surname": "Escorcia-Gutierrez",
        "given_name": "José"
      },
      {
        "surname": "Gamarra",
        "given_name": "Margarita"
      },
      {
        "surname": "Gupta",
        "given_name": "Deepak"
      },
      {
        "surname": "Castillo",
        "given_name": "Oscar"
      },
      {
        "surname": "Kumar",
        "given_name": "Sachin"
      }
    ]
  },
  {
    "title": "MISS GAN: A Multi-IlluStrator style generative adversarial network for image to illustration translation",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.08.006",
    "abstract": "Unsupervised style transfer that supports diverse input styles using only one trained generator is a challenging and interesting task in computer vision. This paper proposes a Multi-IlluStrator Style Generative Adversarial Network (MISS GAN) that is a multi-style framework for unsupervised image-to-illustration translation, which can generate styled yet content preserving images. The illustrations dataset is a challenging one since it is comprised of illustrations of seven different illustrators, hence contains diverse styles. Existing methods require to train several generators (as the number of illustrators) to handle the different illustrators’ styles, which limits their practical usage, or require to train an image specific network, which ignores the style information provided in other images of the illustrator. MISS GAN is both input image specific and uses the information of other images using only one trained model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552100283X",
    "keywords": [
      "Adversarial system",
      "Art",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Economics",
      "Gene",
      "Generative adversarial network",
      "Generative grammar",
      "Generator (circuit theory)",
      "Image (mathematics)",
      "Image translation",
      "Management",
      "Messenger RNA",
      "Natural language processing",
      "Physics",
      "Power (physics)",
      "Quantum mechanics",
      "Style (visual arts)",
      "Task (project management)",
      "Translation (biology)",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Barzilay",
        "given_name": "Noa"
      },
      {
        "surname": "Shalev",
        "given_name": "Tal Berkovitz"
      },
      {
        "surname": "Giryes",
        "given_name": "Raja"
      }
    ]
  },
  {
    "title": "A confidence prior for image dehazing",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108076",
    "abstract": "By sorting channel-minimized values in an ascending order, we individually put the values of several existing image dehazing priors on the curve of sorted values to propose a framework for unifying and understanding these priors. Then we propose a confidence ratio to specify the probability of each channel-minimized value within a range, and thus we can intuitively find a suitable point from the curve, which is actually defined as a novel prior. Although our novel prior and existing ones are perfectly unified under the same framework, our prior has an important advantage that it can freely control the suppression degree of outliers by directly adjusting the confidence ratio of channel-minimized values. In this way, we can remove influence of outliers in a controllable manner. To solve the problems caused by heterogeneity of pixel values and abrupt jumps of scene depths in hazy images, we adopt a regression method to adaptively learn the relationship between patch appearance and confidence ratios for all pixels. To further improve robustness, we use a Gaussian kernel to smooth the estimated confidence ratios for local consistency. Extensive experiments on both natural and synthetic images show that our confidence prior achieves significantly better performance than existing state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321002636",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Bayesian probability",
      "Biochemistry",
      "Channel (broadcasting)",
      "Chemistry",
      "Combinatorics",
      "Computer network",
      "Computer science",
      "Confidence interval",
      "Confidence region",
      "Consistency (knowledge bases)",
      "Gaussian",
      "Gene",
      "Kernel (algebra)",
      "Mathematics",
      "Outlier",
      "Pattern recognition (psychology)",
      "Physics",
      "Pixel",
      "Prior probability",
      "Quantum mechanics",
      "Robustness (evolution)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Yuan",
        "given_name": "Feiniu"
      },
      {
        "surname": "Zhou",
        "given_name": "Yu"
      },
      {
        "surname": "Xia",
        "given_name": "Xue"
      },
      {
        "surname": "Qian",
        "given_name": "Xueming"
      },
      {
        "surname": "Huang",
        "given_name": "Jian"
      }
    ]
  },
  {
    "title": "Convex covariate clustering for classification",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.08.012",
    "abstract": "Clustering, like covariate selection for classification, is an important step to compress and interpret the data. However, clustering of covariates is often performed independently of the classification step, which can lead to undesirable clustering results that harm interpretability and compression rate. Therefore, we propose a method that can cluster covariates while taking into account class label information of samples. We formulate the problem as a convex optimization problem which uses both, a-priori similarity information between covariates, and information from class-labeled samples. Like ordinary convex clustering [1], the proposed method offers a unique global minima making it insensitive to initialization. In order to solve the convex problem, we propose a specialized alternating direction method of multipliers (ADMM), which scales up to several thousands of variables. Furthermore, in order to circumvent computationally expensive cross-validation, we propose a model selection criterion based on approximating the marginal likelihood. Experiments on synthetic and real data confirm the usefulness of the proposed clustering method and the selection criterion.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521003044",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Covariate",
      "Data mining",
      "Feature selection",
      "Fuzzy clustering",
      "Initialization",
      "Interpretability",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Programming language",
      "Selection (genetic algorithm)"
    ],
    "authors": [
      {
        "surname": "Andrade",
        "given_name": "Daniel"
      },
      {
        "surname": "Fukumizu",
        "given_name": "Kenji"
      },
      {
        "surname": "Okajima",
        "given_name": "Yuzuru"
      }
    ]
  },
  {
    "title": "Deep neural network compression through interpretability-based filter pruning",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108056",
    "abstract": "This paper proposes a method to compress deep neural networks (DNNs) based on interpretability. For a trained DNN model, the activation maximization technique is first used to visualize every filter of the DNN model. Then, a single-layer filter pruning approach is introduced from what is learned by visualization. The entire DNN model is compressed layer by layer by using the single-layer filter pruning method in which the compression of the current layer is based on the compression of the preceding layers. Importantly, in addition to effective compression, the proposed method renders a better interpretation of the deep learning process. With a 60 % compression rate of the VGG-16, our method achieves 0.8429 Top-1 accuracy under CIFAR-10, with a slight accuracy drop of only 0.0322, and the storage space of the model can be compressed to 9.42 Mb. For a modern DNN model such as ResNet50, our visualization-based filter pruning method is significantly better than other pruning strategies in different convolutional layers under different compression rates and the larger ImageNet dataset. After pruning, the computation cost and storage requirement of the DNN can be significantly reduced, which means that complex DNN models can be easily implemented in small mobile devices, thus enabling the efficient use of DNNs in the Internet of Things technologies.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321002430",
    "keywords": [
      "Agronomy",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Data compression",
      "Data compression ratio",
      "Filter (signal processing)",
      "Image (mathematics)",
      "Image compression",
      "Image processing",
      "Interpretability",
      "Pattern recognition (psychology)",
      "Pruning"
    ],
    "authors": [
      {
        "surname": "Yao",
        "given_name": "Kaixuan"
      },
      {
        "surname": "Cao",
        "given_name": "Feilong"
      },
      {
        "surname": "Leung",
        "given_name": "Yee"
      },
      {
        "surname": "Liang",
        "given_name": "Jiye"
      }
    ]
  },
  {
    "title": "Fused lasso for feature selection using structural information",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108058",
    "abstract": "Most state-of-the-art feature selection methods tend to overlook the structural relationship between a pair of samples associated with each feature dimension, which may encapsulate useful information for refining the performance of feature selection. Moreover, they usually consider candidate feature relevancy equivalent to selected feature relevancy, and therefore, some less relevant features may be misinterpreted as salient features. To overcome these issues, we propose a new feature selection method based on graph-based feature representations and the Fused Lasso framework in this paper. Unlike state-of-the-art feature selection approaches, our method has two main advantages. First, it can accommodate structural relationship between a pair of samples through a graph-based feature representation. Second, our method can enhance the trade-off between the relevancy of each individual feature on the one hand and its redundancy between pairwise features on the other. This is achieved through the use of a Fused Lasso framework applied to features reordered on the basis of their relevance with respect to the target feature. To effectively solve the optimization problem, an iterative algorithm is developed to identify the most discriminative features. Experiments demonstrate that our proposed approach can outperform its competitors on benchmark datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321002454",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Data mining",
      "Dimensionality reduction",
      "Discriminative model",
      "Feature (linguistics)",
      "Feature selection",
      "Geodesy",
      "Geography",
      "Graph",
      "Linguistics",
      "Machine learning",
      "Minimum redundancy feature selection",
      "Operating system",
      "Pairwise comparison",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Redundancy (engineering)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Cui",
        "given_name": "Lixin"
      },
      {
        "surname": "Bai",
        "given_name": "Lu"
      },
      {
        "surname": "Wang",
        "given_name": "Yue"
      },
      {
        "surname": "Yu",
        "given_name": "Philip S."
      },
      {
        "surname": "Hancock",
        "given_name": "Edwin R."
      }
    ]
  },
  {
    "title": "Visual question answering: Which investigated applications?",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.09.008",
    "abstract": "Visual Question Answering (VQA) is an extremely stimulating and challenging research area where Computer Vision (CV) and Natural Language Processig (NLP) have recently met. In image captioning and video summarization, the semantic information is completely contained in still images or video dynamics, and it has only to be mined and expressed in a human-consistent way. Differently from this, in VQA semantic information in the same media must be compared with the semantics implied by a question expressed in natural language, doubling the artificial intelligence-related effort. Some recent surveys about VQA approaches have focused on methods underlying either the image-related processing or the verbal-related one, or on the way to consistently fuse the conveyed information. Possible applications are only suggested, and, in fact, most cited works rely on general-purpose datasets that are used to assess the building blocks of a VQA system. This paper rather considers the proposals that focus on real-world applications, possibly using as benchmarks suitable data bound to the application domain. The paper also reports about some recent challenges in VQA research.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521003147",
    "keywords": [
      "Artificial intelligence",
      "Automatic summarization",
      "Closed captioning",
      "Computer science",
      "Domain (mathematical analysis)",
      "Focus (optics)",
      "Image (mathematics)",
      "Information retrieval",
      "Mathematical analysis",
      "Mathematics",
      "Natural language",
      "Natural language processing",
      "Natural language understanding",
      "Optics",
      "Physics",
      "Programming language",
      "Question answering",
      "Semantics (computer science)"
    ],
    "authors": [
      {
        "surname": "Barra",
        "given_name": "Silvio"
      },
      {
        "surname": "Bisogni",
        "given_name": "Carmen"
      },
      {
        "surname": "De Marsico",
        "given_name": "Maria"
      },
      {
        "surname": "Ricciardi",
        "given_name": "Stefano"
      }
    ]
  },
  {
    "title": "Futuristic person re-identification over internet of biometrics things (IoBT): Technical potential versus practical reality",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.08.007",
    "abstract": "This article presents an overview of how person re-identification can be achieved over the Internet of Biometric Things (IoBT) architecture by enabling technologies and protocols for multimodal biometric authentication leveraging futuristic cues. The Internet of Things (IoT), as a new era of technology, extends the power of the internet to a whole range of devices, thus reshaping our daily lives in the best possible way. IoT-enabled intelligent surveillance devices are the most indispensable part of public safety and security in smart cities. These IoT devices generate a vast amount of surveillance traffic that is practically impossible for humans to continuously monitor and/or analyze. Person re-identification (PRId), which aims to track and recognize a person in a multi-camera scene is an important feature of visual surveillance systems in IoT infrastructures and can utilize the aforementioned traffic. This is where the concept of IoBT, which is a cloud-centric biometric authentication architecture composed of these IoT-enabled devices for the PRId system, comes into play. This article conceptualizes an overview of interpreting various futuristic cues on the IoT platform for achieving PRId. We highlight some opportunities and key challenges of implementing this futuristic PRId system on IoBT. The article is a proof of concept of the technical potential of such implementation in the near future.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521002841",
    "keywords": [
      "Architecture",
      "Art",
      "Authentication (law)",
      "Biology",
      "Biometrics",
      "Botany",
      "Cloud computing",
      "Computer science",
      "Computer security",
      "Human–computer interaction",
      "Identification (biology)",
      "Internet of Things",
      "Key (lock)",
      "Operating system",
      "The Internet",
      "Visual arts",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Behera",
        "given_name": "Nayan Kumar Subhashis"
      },
      {
        "surname": "Behera",
        "given_name": "Tanmay Kumar"
      },
      {
        "surname": "Nappi",
        "given_name": "Michele"
      },
      {
        "surname": "Bakshi",
        "given_name": "Sambit"
      },
      {
        "surname": "Sa",
        "given_name": "Pankaj Kumar"
      }
    ]
  },
  {
    "title": "Adoption of improved neural network blade pattern recognition in prevention and control of corona virus disease-19 pandemic",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.08.033",
    "abstract": "To explore the adoption effect of improved neural network blade pattern in corona virus disease (COVID)-19, comparative analysis is implemented. First, the following hypotheses are proposed. I: in addition to the confirmed cases and deaths, people suspected of being infected are also involved in the spread of the epidemic. II: patients who have been cured may also develop secondary infections, so it is considered that there is still a link between cured cases and the spread of the epidemic. III: only the relevant data of the previous day is used to predict the epidemic prevention and control of the next day. Then, the epidemic data from February 1st to February 15th in X province were selected as the control. The combined neural network model is used for prevention and control prediction, and the prediction results of the traditional neural network model are compared. The results show that the predictions of the daily new cases by the five neural network models have little difference with the actual value, and the trend is basically consistent. However, there are still differences in some time nodes. The errors of neural network 1 on the 6th and network 3 on the 13th are large. The accuracy of the combined neural network prediction model is high, and there is little difference between the result and the actual value at each time node. The prediction of the cumulative number of diagnoses per day of the five neural network models is also analyzed, and the results are relatively ideal. In addition, the accuracy of the combined neural network prediction model is high, and the difference between the result and the actual value at each time node is relatively small. It is found that the standard deviations of neural networks 2 and 3 are relatively high through the comparison of the deviations. The deviation means of the five models were all relatively low, and the mean deviation and standard deviation of the combined neural network model are the lowest. It is found that the accuracy of prediction on the epidemic spread in this province is good by comparing the performance of each neural network model. Regarding various indicators, the prediction accuracy of the combined neural network model is higher than that of the other four models, and its performance is also the best. Finally, the MSE of the improved neural network model is lower compared with the traditional neural network model. Moreover, with the change of learning times, the change trend of MSE is constant (P < 0.05 for all). In short, the improved neural network blade model has better performance compared with that of the traditional neural network blade model. The prediction results of the epidemic situation are accurate, and the application effect is remarkable, so the proposed model is worthy of further promotion and application in the medical field.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521003330",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Control (management)",
      "Coronavirus disease 2019 (COVID-19)",
      "Disease",
      "Infectious disease (medical specialty)",
      "Mathematics",
      "Medicine",
      "Pandemic",
      "Pathology",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Ma",
        "given_name": "Yanli"
      },
      {
        "surname": "Li",
        "given_name": "Zhonghua"
      },
      {
        "surname": "Gou",
        "given_name": "Jixiang"
      },
      {
        "surname": "Ding",
        "given_name": "Lihua"
      },
      {
        "surname": "Yang",
        "given_name": "Dong"
      },
      {
        "surname": "Feng",
        "given_name": "Guiliang"
      }
    ]
  },
  {
    "title": "Learning event guided network for salient object detection",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.08.034",
    "abstract": "Salient object detection (SOD) focuses on mimicking the attention mechanism in human vision system. Due to the limited information provided from the traditional image data, the image-based SOD advances are very challenging in some complex scenes. Inspired by the emerging event cameras that provide asynchronous measurements of local temporal contrast over a large dynamic range, we propose a new idea to extract more effective information from the combination of event flow and RGB images. In this paper, we construct an end-to-end joint network for salient object detection (ERSOD-Net), which simultaneously supervise the RGB image and the event data within the corresponding image exposure time. To fully exploit temporal information of event data, Long Short-Term Memory module is utilized to effectively process event and learn salient object event surfaces. Moreover, multi-level feature interaction is designed to fuse two complementary branches of image and event and to predict significance mapping. Finally, to demonstrate the effectiveness of our model, a real Event-RGB SOD dataset (ERSOD) is built by DAVIS camera. Experiments on both benchmark and ERSOD datasets show that the proposed event guided network greatly improves the SOD performance in different evaluation metrics. The code and datasets will be released later at https://github.com/jxr326/ERSOD-Net.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521003317",
    "keywords": [
      "Artificial intelligence",
      "Asynchronous communication",
      "Benchmark (surveying)",
      "Code (set theory)",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Event (particle physics)",
      "Feature (linguistics)",
      "Feature extraction",
      "Geodesy",
      "Geography",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Programming language",
      "Quantum mechanics",
      "RGB color model",
      "Salient",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Jiang",
        "given_name": "Xiurong"
      },
      {
        "surname": "Zhu",
        "given_name": "Lin"
      },
      {
        "surname": "Tian",
        "given_name": "Hui"
      }
    ]
  },
  {
    "title": "Reinforced SVM method and memorization mechanisms",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108018",
    "abstract": "The paper is devoted to two problems: (1) reinforcement of SVM algorithms, and (2) justification of memorization mechanisms for generalization. (1) Current SVM algorithm was designed for the case when the risk for the set of nonnegative slack variables is defined by l 1 norm. In this paper, along with that classical l 1 norm, we consider risks defined by l 2 norm and l ∞ norm. Using these norms, we formulate several modifications of the existing SVM algorithm and show that the resulting modified SVM algorithms can improve (sometimes significantly) the classification performance. (2) Generalization ability of existing learning algorithms is usually explained by arguments involving uniform convergence of empirical losses to the corresponding expected losses over a given set of functions. However, along with bounds for uniform convergence of empirical losses to the expected losses, the VC theory also provides bounds for relative uniform convergence. These bounds lead to a more accurate estimate of the expected loss. Advanced methods of estimating of expected risk of error have to leverage these bounds, which also support mechanisms of training data memorization, which, as the paper demonstrates, can improve classification performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321002053",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Convergence (economics)",
      "Economic growth",
      "Economics",
      "Generalization",
      "Generalization error",
      "Law",
      "Leverage (statistics)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Mathematics education",
      "Memorization",
      "Norm (philosophy)",
      "Political science",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Vapnik",
        "given_name": "Vladimir"
      },
      {
        "surname": "Izmailov",
        "given_name": "Rauf"
      }
    ]
  },
  {
    "title": "Predicting soluble solids content in “Fuji” apples of different ripening stages based on multiple information fusion",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.08.003",
    "abstract": "Soluble solids content (SSC) is one of the important components for the assessment of fruit quality. Near-infrared (NIR) spectroscopy, multispectral and hyperspectral imaging techniques are widely used methods for the SSC estimation. However, the high prices are unaffordable for small commercial orchards. The objective of this work is to present a computer vision system for the SSC estimation of “Fuji” apples in different ripening stages. Multiple information of the color channels (channel a* and channel b*) in L*a*b* color space translated from RGB images were applied to extract color features. Stacked autoencoder (SAE) algorithm was used to extract color features in pixel-level, and the trained parameters were used as the initial parameters of the prediction model by using back propagation neural network (BPNN) algorithm. The results show that the prediction accuracy of the SAE-BPNN model based on color features in pixel-level is higher than that of the BPNN model based on pure color features in feature-level. Moreover, the model with fusion information of both color channels as inputs can get better prediction results than that of single channel. The SAE-BPNN model with SAE structure of 270-120-30 yielded the best result with R p 2 = 0.5953 and RMSEP = 0.8856%. This research indicates that RGB imaging technology based on multiple information has potential for the SSC prediction, which provides reference values for fruit harvesting and non-destructive evaluation of fruit quality.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521002804",
    "keywords": [
      "Artificial intelligence",
      "Channel (broadcasting)",
      "Chemistry",
      "Color space",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Food science",
      "Fusion",
      "Hyperspectral imaging",
      "Image (mathematics)",
      "Linguistics",
      "Multispectral image",
      "Near-infrared spectroscopy",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Pixel",
      "RGB color model",
      "Ripening"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Yirui"
      },
      {
        "surname": "Wang",
        "given_name": "Juan"
      },
      {
        "surname": "Li",
        "given_name": "Na"
      },
      {
        "surname": "Yang",
        "given_name": "Jing"
      },
      {
        "surname": "Ren",
        "given_name": "Zhenhui"
      }
    ]
  },
  {
    "title": "Efficient approximate approach for graph edit distance problem",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.08.027",
    "abstract": "Graph Edit Distance (GED) problem is a well-known tool used to measure the similarity/dissimilarity between two graphs. It searches for the best set of edit operations (in terms of cost) that transforms one graph into another. Due to the NP-hardness nature of the problem, the search space increases exponentially making exact approaches impossible to use for large graphs. In this context, there is a huge need for approaches that give near-optimal results in reasonable time. In this paper, we propose a tree-based approximate approach for dealing with GED problem. It operates on a search tree that models all possible solutions of the problem. Since exploring the whole tree is impractical; this approach keeps only the best k nodes at each level of the tree for further exploration. This reduces enormously the execution time without scarifying the solution quality. Experiments using small and medium size data-sets show the low deviation of our results as compared to the optimal results of a Depth First Search algorithm. Moreover, our approach show a strong scalability potential by dealing with large data-sets in low execution time.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521003251",
    "keywords": [
      "Algorithm",
      "Combinatorics",
      "Computer science",
      "Database",
      "Edit distance",
      "Graph",
      "Mathematics",
      "Programming language",
      "Scalability",
      "Set (abstract data type)",
      "Theoretical computer science",
      "Tree (set theory)"
    ],
    "authors": [
      {
        "surname": "Dabah",
        "given_name": "Adel"
      },
      {
        "surname": "Chegrane",
        "given_name": "Ibrahim"
      },
      {
        "surname": "Yahiaoui",
        "given_name": "Saïd"
      }
    ]
  },
  {
    "title": "Machine learning in precision medicine to preserve privacy via encryption",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.07.004",
    "abstract": "Precision medicine is an emerging approach for disease treatment and prevention that delivers personalized care to individual patients by considering their genetic makeups, medical histories, environments, and lifestyles. Despite the rapid advancement of precision medicine and its considerable promise, several underlying technological challenges remain unsolved. One such challenge of great importance is the security and privacy of precision health-related data, such as genomic data and electronic health records, which stifle collaboration and hamper the full potential of machine-learning (ML) algorithms. To preserve data privacy while providing ML solutions, this article makes three contributions. First, we propose a generic machine learning with encryption (MLE) framework, which we used to build an ML model that predicts cancer from one of the most recent comprehensive genomics datasets in the field. Second, our framework’s prediction accuracy is slightly higher than that of the most recent studies conducted on the same dataset, yet it maintains the privacy of the patients’ genomic data. Third, to facilitate the validation, reproduction, and extension of this work, we provide an open-source repository that contains the design and implementation of the framework, all the ML experiments and code, and the final predictive model deployed to a free cloud service.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521002403",
    "keywords": [],
    "authors": [
      {
        "surname": "Briguglio",
        "given_name": "William"
      },
      {
        "surname": "Moghaddam",
        "given_name": "Parisa"
      },
      {
        "surname": "Yousef",
        "given_name": "Waleed A."
      },
      {
        "surname": "Traoré",
        "given_name": "Issa"
      },
      {
        "surname": "Mamun",
        "given_name": "Mohammad"
      }
    ]
  },
  {
    "title": "Image deep clustering based on local-topology embedding",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.08.004",
    "abstract": "Reasonable feature representation plays an important role in improving the performance of clustering algorithms. However, recent deep clustering studies only focusing on feature representation at the pixel level leads to feature representation with low discrimination. Our key insight is that considering local-topology information between images would help to get a highly discriminative representation, and therefore we design a replacement strategy to find local-topology representation of data, and propose a two-stage image deep clustering algorithm based on local-topology embedding called ITEC. Specifically, we take advantage of data augmentation technique to improve the generalization performance of the learning models; then local-topology representation of data is embedded into the representation of data itself, so as to better complete tasks of image clustering. Extensive experiments demonstrate that local-topology information effectively promotes the performance of deep clustering significantly.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521002816",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Combinatorics",
      "Computer science",
      "Discriminative model",
      "Embedding",
      "Feature (linguistics)",
      "Feature learning",
      "Generalization",
      "Law",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Topology (electrical circuits)"
    ],
    "authors": [
      {
        "surname": "Pan",
        "given_name": "Jing"
      },
      {
        "surname": "Qian",
        "given_name": "Yuhua"
      },
      {
        "surname": "Li",
        "given_name": "Feijiang"
      },
      {
        "surname": "Guo",
        "given_name": "Qian"
      }
    ]
  },
  {
    "title": "A fast and efficient image watermarking scheme based on Deep Neural Network",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.08.015",
    "abstract": "In this work, a robust image watermarking method is proposed based on the LWT (lifting wavelet transform) and DNN (Deep Neural Network). Watermark embedding uses wavelet transforms that help in maintaining a high value of imperceptibility and robustness. Different frequency bands are tested to find the optimum balance between robustness and imperceptibility. To check the robustness of proposed work, various attacks like compression attacks, noise attacks and filtering attacks are used. Deep Neural Network is trained to identify the changes made by these attacks on the different frequency bands. The use of high-frequency sub-band (LH (low-high)/LH1/HL (high-low) 2) for watermark insertion enhances the invisibility of scheme. On the contrary, various sub-bands perform differently (in terms o/f robustness) against the different types of attacks. The proposed scheme is tested on 600 images and the average PSNR (peak signal to noise ratio) is found to be 44.1148 dB with the variance of 0.5146 and standard deviation of 0.7173. Also, time analysis shows it to suitable for real-time applications. Comparative analysis suggests that the proposed method depicts improved performance over state-of-the-art techniques in almost all the parameters for most of the cases.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521003068",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Digital watermarking",
      "Embedding",
      "Gene",
      "Image (mathematics)",
      "Invisibility",
      "Pattern recognition (psychology)",
      "Robustness (evolution)",
      "Watermark",
      "Wavelet",
      "Wavelet transform"
    ],
    "authors": [
      {
        "surname": "Mellimi",
        "given_name": "Sandeep"
      },
      {
        "surname": "Rajput",
        "given_name": "Vishal"
      },
      {
        "surname": "Ansari",
        "given_name": "Irshad Ahmad"
      },
      {
        "surname": "Ahn",
        "given_name": "Chang Wook"
      }
    ]
  },
  {
    "title": "Triple-Input-Unsupervised neural Networks for deformable image registration",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.08.032",
    "abstract": "Deep-learning frameworks have been widely used in deformable image registration(DIR), which is a fundamental step in medical image analysis. In traditional deep-learning-based DIR methods, pairwise registration is conducted for each image, cooperated with a dual-input structure. They only considered that the registered images need to be as similar as possible to the target image, while ignored that all registered images to the same target image should have similar appearance too. Therefore, in this paper we add the deformation constraints between different registered images to the registration network in order to enhance the appearance consistency of the registered images and improve the registration accuracy. Specifically, a Triple-Input-Unsupervised neural Network (TIUNet) is proposed to register two source images to a common target image at the same time. Then, a novel deformation-strengthened loss is designed to simulate the deformation constraint between the two warped source images. Finally, the deformation constraints between the source images and the target image are jointly optimized to obtain the deformation field model. The experimental results on the available tissue segmentation database demonstrate that our proposed TIUNet method shows a superior performance of deformation field model over several other advanced deformable image registration algorithms. Moreover, benefited from the triple-input structure, our proposed TIUNet method could expand the available training sets naturally and is suitable for the registration task with fewer training data.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521003287",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer vision",
      "Consistency (knowledge bases)",
      "Constraint (computer-aided design)",
      "Deep learning",
      "Deformation (meteorology)",
      "Field (mathematics)",
      "Geometry",
      "Image (mathematics)",
      "Image registration",
      "Mathematics",
      "Meteorology",
      "Pairwise comparison",
      "Pattern recognition (psychology)",
      "Physics",
      "Pure mathematics",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Qu",
        "given_name": "Lei"
      },
      {
        "surname": "Wan",
        "given_name": "Wan"
      },
      {
        "surname": "Guo",
        "given_name": "Kaixuan"
      },
      {
        "surname": "Liu",
        "given_name": "Yu"
      },
      {
        "surname": "Tang",
        "given_name": "Jun"
      },
      {
        "surname": "Li",
        "given_name": "Xiaolei"
      },
      {
        "surname": "Wu",
        "given_name": "Jun"
      }
    ]
  },
  {
    "title": "Gaussian process latent variable model factorization for context-aware recommender systems",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.09.007",
    "abstract": "Context-aware recommender systems (CARS) have gained increasing attention due to their ability to utilize contextual information. Compared to traditional recommender systems, CARS are, in general, able to generate more accurate recommendations. The latent factors approach accounts for a large proportion of CARS. Recently, a non-linear Gaussian Process (GP) based factorization method was proven to outperform the state-of-the-art methods in CARS. Despite its effectiveness, standard GP model-based methods can suffer from over-fitting and may not be able to determine the impact of each context automatically. In order to address such shortcomings, we propose a Gaussian Process Latent Variable Model Factorization (GPLVMF) method, where we apply an appropriate prior to the original GP model. Our work is primarily inspired by the Gaussian Process Latent Variable Model (GPLVM), which is a non-linear dimensionality reduction method. Since the traditional maximum likelihood approach for standard GP cannot be applied to our model, we adopt a variational inference method to solve the GPLVMF model. As a result, we improve the results on the real datasets significantly as well as capturing the importance of each context. In addition to the general advantages, our method flexibly provides two contributions regarding recommender system settings: (1) addressing the influence of bias by setting a non-zero mean function, and (2) utilizing real-valued contexts by fixing the latent space with real values.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521003263",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Combinatorics",
      "Computer science",
      "Context (archaeology)",
      "Data mining",
      "Factorization",
      "Gaussian",
      "Gaussian process",
      "Inference",
      "Latent variable",
      "Latent variable model",
      "Lexicographical order",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Operating system",
      "Paleontology",
      "Physics",
      "Process (computing)",
      "Quantum mechanics",
      "Recommender system",
      "Variable (mathematics)"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Wei"
      },
      {
        "surname": "Xu",
        "given_name": "Richard Yi Da"
      }
    ]
  },
  {
    "title": "Assessment of dispersion patterns for negative stress detection from electroencephalographic signals",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108094",
    "abstract": "Negative stress, or distress, represents a serious problem in advanced societies given its adverse consequences for health. Many studies have focused on the detection of distress from physiological signals such as the electroencephalogram (EEG). To this respect, the combination of regularity-based quadratic sample entropy (QSampEn) and symbolic amplitude-aware permutation entropy (AAPE) has reported valuable outcomes in distress recognition. In the present work, the recently introduced symbolic metric called dispersion entropy (DispEn) is applied for the first time to the same problem. Statistically significant results reported by the single metric have demonstrated its capability for calm and distress detection. Furthermore, relevant differences have been found between the combination of QSampEn with either AAPE or DispEn, finding that the assessment of ordinal and dispersion patterns leads to distinct and complementary outcomes. Finally, the combination of the three entropy metrics has considerably overcome the results ever reported by other indices in similar studies.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321002818",
    "keywords": [
      "Algorithm",
      "Amplitude",
      "Artificial intelligence",
      "Clinical psychology",
      "Computer science",
      "Distress",
      "Economics",
      "Electroencephalography",
      "Entropy (arrow of time)",
      "Geometry",
      "Mathematics",
      "Metric (unit)",
      "Operations management",
      "Pattern recognition (psychology)",
      "Physics",
      "Psychiatry",
      "Psychology",
      "Quadratic equation",
      "Quantum mechanics",
      "Sample entropy",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "García-Martínez",
        "given_name": "Beatriz"
      },
      {
        "surname": "Fernández-Caballero",
        "given_name": "Antonio"
      },
      {
        "surname": "Alcaraz",
        "given_name": "Raúl"
      },
      {
        "surname": "Martínez-Rodrigo",
        "given_name": "Arturo"
      }
    ]
  },
  {
    "title": "COVID-19 Detection from X-ray Images using Multi-Kernel-Size Spatial-Channel Attention Network",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108055",
    "abstract": "Novel coronavirus 2019 (COVID-19) has spread rapidly around the world and is threatening the health and lives of people worldwide. Early detection of COVID-19 positive patients and timely isolation of the patients are essential to prevent its spread. Chest X-ray images of COVID-19 patients often show the characteristics of multifocality, bilateral hairy glass turbidity, patchy network turbidity, etc. It is crucial to design a method to automatically identify COVID-19 from chest X-ray images to help diagnosis and prognosis. Existing studies for the classification of COVID-19 rarely consider the role of attention mechanisms on the classification of chest X-ray images and fail to capture the cross-channel and cross-spatial interrelationships in multiple scopes. This paper proposes a multi-kernel-size spatial-channel attention method to detect COVID-19 from chest X-ray images. Our proposed method consists of three stages. The first stage is feature extraction. The second stage contains two parallel multi-kernel-size attention modules: multi-kernel-size spatial attention and multi-kernel-size channel attention. The two modules capture the cross-channel and cross-spatial interrelationships in multiple scopes using multiple 1D and 2D convolutional kernels of different sizes to obtain channel and spatial attention feature maps. The third stage is the classification module. We integrate the chest X-ray images from three public datasets: COVID-19 Chest X-ray Dataset Initiative, ActualMed COVID-19 Chest X-ray Dataset Initiative, and COVID-19 radiography database for evaluation. Experimental results demonstrate that the proposed method improves the performance of COVID-19 detection and achieves an accuracy of 98.2%.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321002429",
    "keywords": [
      "Artificial intelligence",
      "Channel (broadcasting)",
      "Combinatorics",
      "Computer science",
      "Coronavirus disease 2019 (COVID-19)",
      "Disease",
      "Feature (linguistics)",
      "Feature extraction",
      "Infectious disease (medical specialty)",
      "Kernel (algebra)",
      "Linguistics",
      "Mathematics",
      "Medicine",
      "Pathology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Radiography",
      "Radiology",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Fan",
        "given_name": "Yuqi"
      },
      {
        "surname": "Liu",
        "given_name": "Jiahao"
      },
      {
        "surname": "Yao",
        "given_name": "Ruixuan"
      },
      {
        "surname": "Yuan",
        "given_name": "Xiaohui"
      }
    ]
  },
  {
    "title": "GRNet: Graph-based remodeling network for multi-view semi-supervised classification",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.08.008",
    "abstract": "Multi-view semi-supervised classification (MSSC) focuses on exploring information from multiple views of labeled and unlabeled data to boost classification performance. However, most of the existing methods build models for each view individually, therefore, the potential relationship between different views can’t be fully explored. Additionally, they either focus on the correlations for consistency or maximize the independence for complementarity. Consistency and complementarity are equally important for multi-view learning. Therefore, this work proposes a novel Graph-based Remodeling Network for MSSC (GRNet), which can explore the potential relationship of multiple views and balance the consistency and complementation adaptively. Specifically, this model integrates multiple views and then generates reformed pseudo views by an attention-based ensemble learning strategy. Moreover, to exploit the information of unlabeled data, this work introduces graph regularization for such a goal. Extensive experiments on several datasets demonstrate the effectiveness and efficiency of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521002853",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Complementarity (molecular biology)",
      "Computer science",
      "Computer security",
      "Consistency (knowledge bases)",
      "Data mining",
      "Exploit",
      "Genetics",
      "Graph",
      "Machine learning",
      "Regularization (linguistics)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Xiao-li"
      },
      {
        "surname": "Zhu",
        "given_name": "Zhi-fan"
      },
      {
        "surname": "Song",
        "given_name": "Yan"
      },
      {
        "surname": "Fu",
        "given_name": "Hai-juan"
      }
    ]
  },
  {
    "title": "Weakly-supervised temporal attention 3D network for human action recognition",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108068",
    "abstract": "From a series of observations, we have inferred that human actions in videos are defined by a set of significant frames. In this paper, we propose a weakly-supervised temporal attention 3D network for human action recognition, called as TA3DNet, to accelerate 3D convolutional neural networks (3D CNNs) by temporally assigning different importance to each frame. First, we obtain short-term frames with long-term connection by regularly or randomly skipping frames to avoid temporal redundancy, and apply 3D convolutional layers to extract features for action recognition. Then, we apply a temporal attention module to assign different weights to each frame. We train the temporal attention module in a weakly-supervised manner that updates weights based on only class labels without event information and extra labels. Thus, TA3DNet reduces the number of input frames and constructs a lightweight network for action recognition. Experimental results demonstrate that TA3DNet achieves high performance on two challenging datasets (UCF101 and HMDB51) and outperforms state-of-the-art methods for action recognition.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321002557",
    "keywords": [
      "Action recognition",
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Convolutional neural network",
      "Event (particle physics)",
      "Frame (networking)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Physics",
      "Programming language",
      "Quantum mechanics",
      "Redundancy (engineering)",
      "Set (abstract data type)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Kim",
        "given_name": "Jonghyun"
      },
      {
        "surname": "Li",
        "given_name": "Gen"
      },
      {
        "surname": "Yun",
        "given_name": "Inyong"
      },
      {
        "surname": "Jung",
        "given_name": "Cheolkon"
      },
      {
        "surname": "Kim",
        "given_name": "Joongkyu"
      }
    ]
  },
  {
    "title": "PMMN: Pre-trained multi-Modal network for scene text recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.07.016",
    "abstract": "Scene Text Recognition (STR) task needs to consume large-amount data to develop a powerful recognizer, including visual data like images and linguistic data like texts. However, existing methods mainly leverage a one-stage training manner to train the entire framework end-to-end, which deeply relies on the well-annotated images and does not effectively use the data of the two modalities mentioned above. To solve this, in this paper, we propose a pre-trained multi-modal network (PMMN) that utilizes visual and linguistic data to pre-train the vision model and language model respectively to learn modality-specific knowledge for accurate scene text recognition. In detail, we first pre-train the proposed off-the-shelf vision model and language model to convergence. And then, we combine the pre-trained models in a unified framework for end-to-end fine-tuning and utilize the learned multi-modal information to interact with each other to generate robust features for character prediction. Extensive experiments are conducted to demonstrate the effectiveness of PMMN. The evaluation results on six benchmarks show that our proposed method exceeds most existing methods, achieving state-of-the-art performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521002622",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Economics",
      "Language model",
      "Leverage (statistics)",
      "Machine learning",
      "Management",
      "Modal",
      "Modalities",
      "Modality (human–computer interaction)",
      "Natural language processing",
      "Polymer chemistry",
      "Social science",
      "Sociology",
      "Speech recognition",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Yu"
      },
      {
        "surname": "Fu",
        "given_name": "Zilong"
      },
      {
        "surname": "Huang",
        "given_name": "Fuyu"
      },
      {
        "surname": "Liu",
        "given_name": "Yizhi"
      }
    ]
  },
  {
    "title": "Exploration of block-wise dynamic sparseness",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.08.013",
    "abstract": "Neural networks have achieved state of the art performance across a wide variety of machine learning tasks, often with large and computation-heavy models. Inducing sparseness as a way to reduce the memory and computation footprint of these models has seen significant research attention in recent years. In this paper, we present a new method for dynamic sparseness, whereby part of the computations are omitted dynamically, based on the input. For efficiency, we combined the idea of dynamic sparseness with block-wise matrix-vector multiplications. In contrast to static sparseness, which permanently zeroes out selected positions in weight matrices, our method preserves the full network capabilities by potentially accessing any trained weights. Yet, matrix vector multiplications are accelerated by omitting a pre-defined fraction of weight blocks from the matrix, based on the input. Experimental results on the task of language modeling, using recurrent and quasi-recurrent models, show that the proposed method can outperform static sparseness baselines. In addition, our method can reach similar language modeling perplexities as the dense baseline, at half the computational cost at inference time.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521003032",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Block (permutation group theory)",
      "Chemistry",
      "Composite material",
      "Computation",
      "Computer science",
      "Economics",
      "Fraction (chemistry)",
      "Geometry",
      "Inference",
      "Machine learning",
      "Management",
      "Materials science",
      "Mathematics",
      "Matrix (chemical analysis)",
      "Memory footprint",
      "Operating system",
      "Organic chemistry",
      "Recurrent neural network",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Hadifar",
        "given_name": "Amir"
      },
      {
        "surname": "Deleu",
        "given_name": "Johannes"
      },
      {
        "surname": "Develder",
        "given_name": "Chris"
      },
      {
        "surname": "Demeester",
        "given_name": "Thomas"
      }
    ]
  },
  {
    "title": "Weighted multi-view common subspace learning method",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.09.017",
    "abstract": "How to use multi-view data effectively has become one of the challenging problems in the computer vision community. The existing multi-view learning methods are mainly based on common subspace learning which aim to explore the discriminative information between multi-view data and find its potential common subspace. Most of the existing multi-view subspace learning methods rely on the within-class scatter matrix and between-class scatter matrix while capturing the discriminative information of multiple views. However, these methods just roughly minimize the within-class distance and maximize the between-class distance, and do not make full use of the intra-view and inter-view information. To address this problem, we propose a weighted common subspace learning method, which can effectively adjust the contribution ratio of between-class and within-class information through a weighted parameter, so that an optimized common subspace can be obtained. And we use the maximum scatter difference criterion as the metric of inter-view and intra-view after projection. Extensive experiments on the public data sets show the superiority of this method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521003433",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Covariance matrix",
      "Data mining",
      "Discriminative model",
      "Economics",
      "Estimation of covariance matrices",
      "Machine learning",
      "Mathematics",
      "Metric (unit)",
      "Operations management",
      "Pattern recognition (psychology)",
      "Projection (relational algebra)",
      "Scatter matrix",
      "Subspace topology"
    ],
    "authors": [
      {
        "surname": "An",
        "given_name": "Jing"
      },
      {
        "surname": "Liu",
        "given_name": "Xiaoxia"
      },
      {
        "surname": "Shi",
        "given_name": "Mei"
      },
      {
        "surname": "Guo",
        "given_name": "Jun"
      },
      {
        "surname": "Gong",
        "given_name": "Xiaoqing"
      },
      {
        "surname": "Li",
        "given_name": "Zhihui"
      }
    ]
  },
  {
    "title": "Mutual information regularized identity-aware facial expression recognition in compressed video",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108105",
    "abstract": "How to extract effective expression representations that invariant to the identity-specific attributes is a long-lasting problem for facial expression recognition (FER). Most of the previous methods process the RGB images of a sequence, while we argue that the off-the-shelf and valuable expression-related muscle movement is already embedded in the compression format. In this paper, we target to explore the inter-subject variations eliminated facial expression representation in the compressed video domain. In the up to two orders of magnitude compressed domain, we can explicitly infer the expression from the residual frames and possibly extract identity factors from the I frame with a pre-trained face recognition network. By enforcing the marginal independence of them, the expression feature is expected to be purer for the expression and be robust to identity shifts. Specifically, we propose a novel collaborative min-min game for mutual information (MI) minimization in latent space. We do not need the identity label or multiple expression samples from the same person for identity elimination. Moreover, when the apex frame is annotated in the dataset, the complementary constraint can be further added to regularize the feature-level game. In testing, only the compressed residual frames are required to achieve expression prediction. Our solution can achieve comparable or better performance than the recent decoded image-based methods on the typical FER benchmarks with about 3 times faster inference.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321002922",
    "keywords": [
      "Acoustics",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Expression (computer science)",
      "Facial expression",
      "Feature (linguistics)",
      "Identity (music)",
      "Linguistics",
      "Mutual information",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Programming language"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Xiaofeng"
      },
      {
        "surname": "Jin",
        "given_name": "Linghao"
      },
      {
        "surname": "Han",
        "given_name": "Xu"
      },
      {
        "surname": "You",
        "given_name": "Jane"
      }
    ]
  },
  {
    "title": "VSRNet: End-to-end video segment retrieval with text query",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108027",
    "abstract": "Users are sometimes interested in specific segments of an untrimmed video when using the video search engine. Targeting at this demand, we explore a novel research topic of text query based video segment retrieval (VSR). Different from the conventional video retrieval task or localizing text descriptions in a single video, it requires the retrieval of the most relevant video from a large collection as well as localizing the start and end timestamps of a segment that matches the text query best from the video. A direct solution is to perform video-level matching first, and then apply description localization among such video candidates. Such two-stage based methods are not able to utilize complementary information of each stage, and are time-consuming in inference. In this paper, We propose VSRNet, an end-to-end framework that efficiently retrieves video at segment granularity with two branches. In the first branch, individual videos and texts are mapped to a common space for stand-alone ranking. In the second branch, we propose a supervised text-aligned attention mechanism and calculate the response of every frame to the text query, from which the frames with high scores are aggregated as segment proposals. Extensive experiments conducted on ActivityNet Captions and DiDeMo verify the effectiveness of our method and show that our solution significantly outperforms the state of the art.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321002144",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Economics",
      "Frame (networking)",
      "Granularity",
      "Information retrieval",
      "Management",
      "Matching (statistics)",
      "Mathematics",
      "Operating system",
      "Ranking (information retrieval)",
      "Statistics",
      "Task (project management)",
      "Telecommunications",
      "Timestamp",
      "Video processing",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Sun",
        "given_name": "Xiao"
      },
      {
        "surname": "Long",
        "given_name": "Xiang"
      },
      {
        "surname": "He",
        "given_name": "Dongliang"
      },
      {
        "surname": "Wen",
        "given_name": "Shilei"
      },
      {
        "surname": "Lian",
        "given_name": "Zhouhui"
      }
    ]
  },
  {
    "title": "Skeleton-based human action evaluation using graph convolutional network for monitoring Alzheimer’s progression",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108095",
    "abstract": "Human action evaluation (HAE) involves judgments about the abnormality and quality of human actions. If performed effectively, HAE based on skeleton data can be used to monitor the outcomes of behavioral therapies for Alzheimer’s disease (AD). In this paper, we propose a two-task graph convolutional network (2T-GCN) to represent skeleton data for HAE tasks involving abnormality detection and quality evaluation. The network is first evaluated using the UI-PRMD dataset and demonstrates accurate abnormality detection. Regarding quality evaluation, in addition to laboratory-collected UI-PRMD data, we test the network on a set of real exercise data collected from patients with AD. A numerical score indicating the degree to which actions deviate from normal is taken to reflect the severity of AD; thus, we apply 2T-GCN to determine such scores. Experimental results show that numerical scores for certain exercises performed by patients with AD are consistent with their AD severity level as identified by clinical staff. This corroboration highlights the potential of our approach for monitoring AD and other neurodegenerative diseases.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032100282X",
    "keywords": [
      "Action recognition",
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Convolutional neural network",
      "Graph",
      "Human skeleton",
      "Pattern recognition (psychology)",
      "Programming language",
      "Skeleton (computer programming)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Yu",
        "given_name": "Bruce X.B."
      },
      {
        "surname": "Liu",
        "given_name": "Yan"
      },
      {
        "surname": "Chan",
        "given_name": "Keith C.C."
      },
      {
        "surname": "Yang",
        "given_name": "Qintai"
      },
      {
        "surname": "Wang",
        "given_name": "Xiaoying"
      }
    ]
  },
  {
    "title": "SCANet: A Spatial and Channel Attention based Network for Partial-to-Partial Point Cloud Registration",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.08.002",
    "abstract": "Point cloud registration plays an essential role in many areas, such as computer vision and robotics. However, traditional feature-based registration requires handcrafted descriptors for various scenarios, which is of low efficiency and flexibility; ICP and its locally optimal variants are sensitive to initialization, while globally optimal methods are of high computational time to overcome noise, outliers, and partial overlap. Learning-based registration can automatically and flexibly learn shape representation for different objects, but existing methods are of either low efficiency or low precision, and poorly perform in partial-to-partial point cloud registration. Thus, we present a simple spatial and channel attention based network, named SCANet, for partial-to-partial point cloud registration. A spatial self-attention aggregation (SSA) module is applied in a feature extraction sub-network to efficiently make use of the inter and global information of each point cloud in different levels, while a channel cross-attention regression (CCR) module is adopted in a pose estimation sub-network for information interaction between two input global feature vectors, enhancing relevant information and suppressing redundant information. Experimental results show that our SCANet achieves state-of-the-art performances in both accuracy and efficiency compared to existing non-deep learning and learning-based methods on partial visibility with Gaussian noise. Our source code is available at the project website https://github.com/zhouruqin/SCANet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521002798",
    "keywords": [
      "Artificial intelligence",
      "Channel (broadcasting)",
      "Cloud computing",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Deep learning",
      "Feature (linguistics)",
      "Feature extraction",
      "Flexibility (engineering)",
      "Geology",
      "Image (mathematics)",
      "Initialization",
      "Linguistics",
      "Mathematics",
      "Noise (video)",
      "Operating system",
      "Outlier",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Point cloud",
      "Programming language",
      "Remote sensing",
      "Spatial analysis",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Ruqin"
      },
      {
        "surname": "Li",
        "given_name": "Xixing"
      },
      {
        "surname": "Jiang",
        "given_name": "Wanshou"
      }
    ]
  },
  {
    "title": "ActivityExplorer: A semi-supervised approach to discover unknown activity classes in HAR systems",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.08.029",
    "abstract": "A semi-supervised activity recognition system is here proposed to deal with partially labeled video-sequences, where the uncertainty in the data comes from two different factors: only a subset of the data has a class label assigned and only part of the activity classes are known. In particular, the paper presents ActivityExplorer, an approach able to identify clusters of similar activity patterns within the dataset and to identify those clusters that might correspond to new activity classes, still unknown to the recognition system. These capabilities are realized thanks to a combination of metric learning, used to determine a suitable subspace for pattern classification, an advanced clustering technique and ad hoc indicators defined to estimate the membership of each pattern to known classes and possibly identify new activities.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521003226",
    "keywords": [
      "Activity recognition",
      "Artificial intelligence",
      "Class (philosophy)",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Economics",
      "Machine learning",
      "Metric (unit)",
      "Operations management",
      "Pattern recognition (psychology)",
      "Subspace topology"
    ],
    "authors": [
      {
        "surname": "Brighi",
        "given_name": "Marco"
      },
      {
        "surname": "Franco",
        "given_name": "Annalisa"
      },
      {
        "surname": "Maio",
        "given_name": "Dario"
      }
    ]
  },
  {
    "title": "A deep fusion framework for unlabeled data-driven tumor recognition",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108066",
    "abstract": "Traditional pattern recognition problems are usually accomplished through two successive stages of representation and classification, the generalization ability and stability are difficult to guarantee for small samples and category imbalance. For tackling these problems, an unlabeled data-driven representation learning classification (RLC) fused model is constructed by integrating representation learning and classification into one model, rather than simple putting the two stages together. The RLC fused model mainly focuses on interactive iteratively optimizing representation learning and classification in a model, guiding and reinforcing each other. Under the framework of RLC, a deep nonnegative matrix factorization (NMF) is adopted for representation learning by complementing the advantages of NMF and deep learning, and avoiding complex network structure and parameter modulation. The framework is called deep NMF-RLC fusion model, which can achieve good performance for binary classification even the simplest linear regression classifier is used. The model explores useful information embedded in unlabeled data, and is suitable for small training samples and unbalanced classification. The performance of the proposed framework is verified on genetic-based tumor recognition, which contains all three stages of early diagnosis, tumor type recognition and postoperative metastasis. Experiments show that, compared with the published state-of-the-art methods and results, there are significant improvements in classification accuracy, specificity and sensitivity.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321002533",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Binary classification",
      "Classifier (UML)",
      "Computer science",
      "Eigenvalues and eigenvectors",
      "Law",
      "Machine learning",
      "Matrix decomposition",
      "Non-negative matrix factorization",
      "Pattern recognition (psychology)",
      "Physics",
      "Political science",
      "Politics",
      "Quantum mechanics",
      "Representation (politics)",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Xiaohui"
      },
      {
        "surname": "Wu",
        "given_name": "Wenming"
      },
      {
        "surname": "Jiao",
        "given_name": "Licheng"
      },
      {
        "surname": "Jiao",
        "given_name": "Changzhe"
      },
      {
        "surname": "Jiao",
        "given_name": "Zhicheng"
      }
    ]
  },
  {
    "title": "Unsupervised neural domain adaptation for document image binarization",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108099",
    "abstract": "Binarization is a well-known image processing task, whose objective is to separate the foreground of an image from the background. One of the many tasks for which it is useful is that of preprocessing document images in order to identify relevant information, such as text or symbols. The wide variety of document types, alphabets, and formats makes binarization challenging. There are multiple proposals with which to solve this problem, from classical manually-adjusted methods, to more recent approaches based on machine learning. The latter techniques require a large amount of training data in order to obtain good results; however, labeling a portion of each existing collection of documents is not feasible in practice. This is a common problem in supervised learning, which can be addressed by using the so-called Domain Adaptation (DA) techniques. These techniques take advantage of the knowledge learned in one domain, for which labeled data are available, to apply it to other domains for which there are no labeled data. This paper proposes a method that combines neural networks and DA in order to carry out unsupervised document binarization. However, when both the source and target domains are very similar, this adaptation could be detrimental. Our methodology, therefore, first measures the similarity between domains in an innovative manner in order to determine whether or not it is appropriate to apply the adaptation process. The results reported in the experimentation, when evaluating up to 20 possible combinations among five different domains, show that our proposal successfully deals with the binarization of new document domains without the need for labeled data.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321002867",
    "keywords": [
      "Adaptation (eye)",
      "Artificial intelligence",
      "Artificial neural network",
      "Classifier (UML)",
      "Computer science",
      "Domain (mathematical analysis)",
      "Domain adaptation",
      "Economics",
      "Image (mathematics)",
      "Machine learning",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Operating system",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Preprocessor",
      "Process (computing)",
      "Similarity (geometry)",
      "Task (project management)",
      "Unsupervised learning"
    ],
    "authors": [
      {
        "surname": "Castellanos",
        "given_name": "Francisco J."
      },
      {
        "surname": "Gallego",
        "given_name": "Antonio-Javier"
      },
      {
        "surname": "Calvo-Zaragoza",
        "given_name": "Jorge"
      }
    ]
  },
  {
    "title": "Intelligent computing on time-series data analysis and prediction of COVID-19 pandemics",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.07.027",
    "abstract": "Covid-19 disease caused by novel coronavirus (SARS-CoV-2) is a highly contagious epidemic that originated in Wuhan, Hubei Province of China in late December 2019. World Health Organization (WHO) declared Covid-19 as a pandemic on 12th March 2020. Researchers and policy makers are designing strategies to control the pandemic in order to minimize its impact on human health and economy round the clock. The SARS-CoV-2 virus transmits mostly through respiratory droplets and through contaminated surfacesin human body.Securing an appropriate level of safety during the pandemic situation is a highly problematic issue which resulted from the transportation sector which has been hit hard by COVID-19. This paper focuses on developing an intelligent computing model for forecasting the outbreak of COVID-19. The Facebook Prophet model predicts 90 days future values including the peak date of the confirmed cases of COVID-19 for six worst hit countries of the world including India and six high incidence states of India. The model also identifies five significant changepoints in the growth curve of confirmed cases of India which indicate the impact of the interventions imposed by Government of India on the growth rate of the infection. The goodness-of-fit of the model measures 85% MAPE for all six countries and all six states of India. The above computational analysis may be able to throw some light on planning and management of healthcare system and infrastructure.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521002762",
    "keywords": [
      "Archaeology",
      "Business",
      "China",
      "Computer science",
      "Coronavirus disease 2019 (COVID-19)",
      "Development economics",
      "Disease",
      "Economic growth",
      "Economics",
      "Engineering",
      "Geography",
      "Government (linguistics)",
      "Infectious disease (medical specialty)",
      "Linguistics",
      "Machine learning",
      "Medicine",
      "Operations research",
      "Outbreak",
      "Pandemic",
      "Pathology",
      "Philosophy",
      "Psychiatry",
      "Psychological intervention",
      "Time series",
      "Virology"
    ],
    "authors": [
      {
        "surname": "Dash",
        "given_name": "Sujata"
      },
      {
        "surname": "Chakraborty",
        "given_name": "Chinmay"
      },
      {
        "surname": "Giri",
        "given_name": "Sourav K."
      },
      {
        "surname": "Pani",
        "given_name": "Subhendu Kumar"
      }
    ]
  },
  {
    "title": "Imbalanced image classification with complement cross entropy",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.07.017",
    "abstract": "Recently, deep learning models have achieved great success in computer vision applications, relying on large-scale class-balanced datasets. However, imbalanced class distributions still limit the wide applicability of these models due to degradation in performance. To solve this problem, in this paper, we concentrate on the study of cross entropy which mostly ignores output scores on incorrect classes. This work discovers that neutralizing predicted probabilities on incorrect classes improves the prediction accuracy for imbalanced image classification. This paper proposes a simple but effective loss named complement cross entropy based on this finding. The proposed loss makes the ground truth class overwhelm the other classes in terms of softmax probability, by neutralizing probabilities of incorrect classes, without additional training procedures. Along with it, this loss facilitates the models to learn key information especially from samples on minority classes. It ensures more accurate and robust classification results on imbalanced distributions. Extensive experiments on imbalanced datasets demonstrate the effectiveness of the proposed method compared to other state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552100266X",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Complement (music)",
      "Complementation",
      "Computer science",
      "Contextual image classification",
      "Cross entropy",
      "Data mining",
      "Deep learning",
      "Entropy (arrow of time)",
      "Gene",
      "Ground truth",
      "Image (mathematics)",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Phenotype",
      "Physics",
      "Principle of maximum entropy",
      "Quantum mechanics",
      "Softmax function"
    ],
    "authors": [
      {
        "surname": "Kim",
        "given_name": "Yechan"
      },
      {
        "surname": "Lee",
        "given_name": "Younkwan"
      },
      {
        "surname": "Jeon",
        "given_name": "Moongu"
      }
    ]
  },
  {
    "title": "Empowering Knowledge Distillation via Open Set Recognition for Robust 3D Point Cloud Classification",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.07.023",
    "abstract": "Real-world scenarios pose several challenges to deep learning based computer vision techniques despite their tremendous success in research. Deeper models provide better performance, but are challenging to deploy and knowledge distillation allows us to train smaller models with minimal loss in performance. A model also has to deal with open set samples from classes outside the ones it was trained on and should be able to identify them as unknown samples while classifying the known ones correctly. Finally, most existing image recognition research focuses only on using two-dimensional snapshots of the three-dimensional real world objects. In this work, we attempt to bridge these three research fields, which have been developed independently until now, despite being deeply interrelated in practice. We propose a joint knowledge distillation and open set recognition training methodology for three-dimensional object recognition. We demonstrate the effectiveness of the proposed method via various experiments on how it allows us to obtain a much smaller model, which takes a minimal hit in performance while being capable of open set recognition for 3D point cloud data.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521002701",
    "keywords": [
      "Artificial intelligence",
      "Bridge (graph theory)",
      "Chemistry",
      "Cloud computing",
      "Cognitive neuroscience of visual object recognition",
      "Computer science",
      "Data mining",
      "Distillation",
      "Geometry",
      "Image (mathematics)",
      "Internal medicine",
      "Machine learning",
      "Mathematics",
      "Medicine",
      "Object (grammar)",
      "Open research",
      "Operating system",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Point (geometry)",
      "Point cloud",
      "Programming language",
      "Set (abstract data type)",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Bhardwaj",
        "given_name": "Ayush"
      },
      {
        "surname": "Pimpale",
        "given_name": "Sakshee"
      },
      {
        "surname": "Kumar",
        "given_name": "Saurabh"
      },
      {
        "surname": "Banerjee",
        "given_name": "Biplab"
      }
    ]
  },
  {
    "title": "A distributed algorithm for graph semi-supervised learning",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.07.021",
    "abstract": "Graph semi-supervised learning (GSSL) plays an important role in data classification by leveraging the similarity across the graph topology and convex optimization with Laplacian-based regularization. However, the current algorithm to solve the problem is centralized approach calling for heavy computational cost, particularly when the data is of large volume. In this paper, an innovate distributed algorithm is proposed to solve the problem, which is based on the decomposition of the similar graph. Contrary to the centralized approach, the distributed algorithm only requires the neighboring information for solving the optimization. It is proved that difference between the solutions of the distributed algorithm and centralized counterpart is upper bounded. We apply the proposed algorithm to both the synthetic and real-world datasets. The numerical results verify the effectiveness of the proposed distributed algorithm.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521002713",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Bounded function",
      "Computer science",
      "Distributed algorithm",
      "Distributed computing",
      "Geometry",
      "Graph",
      "Laplacian matrix",
      "Line graph",
      "Mathematical analysis",
      "Mathematics",
      "Regular polygon",
      "Semi-supervised learning",
      "Theoretical computer science",
      "Topological graph theory",
      "Voltage graph"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Daxin"
      },
      {
        "surname": "Jiang",
        "given_name": "Junzheng"
      },
      {
        "surname": "Zhou",
        "given_name": "Fang"
      },
      {
        "surname": "Ouyang",
        "given_name": "Shan"
      }
    ]
  },
  {
    "title": "Mitigating severe over-parameterization in deep convolutional neural networks through forced feature abstraction and compression with an entropy-based heuristic",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108057",
    "abstract": "Convolutional Neural Networks (CNNs) such as ResNet-50, DenseNet-40 and ResNeXt-56 are severely over-parameterized, necessitating a consequent increase in the computational resources required for model training which scales exponentially for increments in model depth. In this paper, we propose an Entropy-Based Convolutional Layer Estimation (EBCLE) heuristic which is robust and simple, yet effective in resolving the problem of over-parameterization with regards to network depth of CNN model. The EBCLE heuristic employs a priori knowledge of the entropic data distribution of input datasets to determine an upper bound for convolutional network depth, beyond which identity transformations are prevalent offering insignificant contributions for enhancing model performance. Restricting depth redundancies by forcing feature compression and abstraction restricts over-parameterization while decreasing training time by 24.99% - 78.59% without degradation in model performance. We present empirical evidence to emphasize the relative effectiveness of broader, yet shallower models trained using the EBCLE heuristic, which maintains or outperforms baseline classification accuracies of narrower yet deeper models. The EBCLE heuristic is architecturally agnostic and EBCLE based CNN models restrict depth redundancies resulting in enhanced utilization of the available computational resources. The proposed EBCLE heuristic is a compelling technique for researchers to analytically justify their HyperParameter (HP) choices for CNNs. Empirical validation of the EBCLE heuristic in training CNN models was established on five benchmarking datasets (ImageNet32, CIFAR-10/100, STL-10, MNIST) and four network architectures (DenseNet, ResNet, ResNeXt and EfficientNet B0-B2) with appropriate statistical tests employed to infer any conclusive claims presented in this paper.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321002442",
    "keywords": [
      "Abstraction",
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Epistemology",
      "Feature (linguistics)",
      "Heuristic",
      "Hyperparameter",
      "Linguistics",
      "MNIST database",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Gowdra",
        "given_name": "Nidhi"
      },
      {
        "surname": "Sinha",
        "given_name": "Roopak"
      },
      {
        "surname": "MacDonell",
        "given_name": "Stephen"
      },
      {
        "surname": "Yan",
        "given_name": "Wei Qi"
      }
    ]
  },
  {
    "title": "Bias correction for linear discriminant analysis",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.07.026",
    "abstract": "Linear discriminant analysis (LDA) is perhaps one of the most fundamental statistical pattern recognition techniques. In this work, we explicitly present, for the first time, an asymptotically exact estimator of the LDA optimal intercept in terms of achieving the lowest overall risk in the classification of two multivariate Gaussian distributions with a common covariance matrix and arbitrary misclassification costs. The proposed estimator of the optimal bias term is developed based on the theory of random matrices of increasing dimension in which the observation dimension and the sample size tend to infinity while keeping their magnitudes comparable. The simple form of this estimator provides us with some analytical insights into the working mechanism of the bias correction in LDA. We then complement these analytical insights with numerical experiments. In particular, empirical results using real data show that insofar as the overall risk is concerned, the proposed bias-corrected form of LDA can outperform the conventional LDA classifier in a wide range of misclassification costs. At the same time, the superiority of the proposed form over LDA tends to be more evident as dimensionality or the ratio between class-specific costs increase.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521002750",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Covariance",
      "Covariance matrix",
      "Curse of dimensionality",
      "Dimension (graph theory)",
      "Estimation of covariance matrices",
      "Estimator",
      "Gaussian",
      "Linear discriminant analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Pure mathematics",
      "Quantum mechanics",
      "Sample size determination",
      "Scatter matrix",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Zollanvari",
        "given_name": "Amin"
      },
      {
        "surname": "Abibullaev",
        "given_name": "Berdakh"
      }
    ]
  },
  {
    "title": "A branch and bound irredundant graph algorithm for large-scale MLCS problems",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108059",
    "abstract": "Finding the multiple longest common subsequences (MLCS) among many long sequences (i.e., the large scale MLCS problem) has many important applications, such as gene alignment, disease diagnosis, and documents similarity check, etc. It is an NP-hard problem (Maier et al., 1978). The key bottle neck of this problem is that the existing state-of-the-art algorithms must construct a huge graph (called direct acyclic graph, briefly DAG), and the computer usually has no enough space to store and handle this graph. Thus the existing algorithms cannot solve the large scale MLCS problem. In order to quickly solve the large-scale MLCS problem within limited computer resources, this paper therefore proposes a branch and bound irredundant graph algorithm called Big-MLCS, which constructs a much smaller DAG (called Small-DAG) than the existing algorithms do by a branch and bound method, and designs a new data structure to efficiently store and handle Small-DAG. By these schemes, Big-MLCS is more efficient than the existing algorithms. Also, we compare the proposed algorithm with two state-of-the-art algorithms through the experiments, and the results show that the proposed algorithm outperforms the compared algorithms and is more suitable to large-scale MLCS problems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321002466",
    "keywords": [
      "Algorithm",
      "Computer science",
      "Directed acyclic graph",
      "Graph",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Chunyang"
      },
      {
        "surname": "Wang",
        "given_name": "Yuping"
      },
      {
        "surname": "Cheung",
        "given_name": "Yiuming"
      }
    ]
  },
  {
    "title": "Instance-Based Zero-Shot learning for semi-Automatic MeSH indexing",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.08.009",
    "abstract": "Zero-shot learning constitutes a variant of the broader category of weakly supervised learning algorithms. Its main asset is the possibility of identifying entities for which no training data are provided in advance. Under this extreme scenario, conventional supervised learning methods cannot operate properly, while consumption of human resources for obtaining even limited instances may be highly restricted, especially when the label space is quite complex because of its cardinality and the underlying semantic dependencies. However, removing the human factor from the learning loop under complicated tasks cannot guarantee robust performance. Thus, semi-automated solutions are widely accepted by both the research and industrial communities, favoring cooperation of human and machine, mainly for alleviating the spent effort of the former, and for acquiring safer predictions. In contrast with the majority of existing Zero-shot learning approaches, we propose a generalized instance-based method oriented towards tackling the Multi-label classification task without performing any transductive operations over the test instances. Instead, we aim to provide a label ranking of the unseen classes exploiting sentence-based semantic embeddings and label similarities, through a dedicated fine-tuned language representational model. We also use a pattern matching rule to further boost the ranking of our method. Some realistic assumptions are made in order for our approach to work correctly and provide said ranking. Results on a biomedical database with a semantically rich fine-grained label space are really promising, rendering its utilization as a helpful and computationally inexpensive tool for facilitating semi-automated indexing.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521002865",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Leverage (statistics)",
      "Machine learning",
      "Ranking (information retrieval)",
      "Search engine indexing"
    ],
    "authors": [
      {
        "surname": "Karlos",
        "given_name": "Stamatis"
      },
      {
        "surname": "Mylonas",
        "given_name": "Nikolaos"
      },
      {
        "surname": "Tsoumakas",
        "given_name": "Grigorios"
      }
    ]
  },
  {
    "title": "Collaborative filtering grounded on knowledge graphs",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.07.022",
    "abstract": "Matrix Factorization (MF) is a widely used collaborative filtering technique for effectively modeling a user-item interaction in recommender system. Despite the successful application of MF and its variants, the method proves to be effective only in situations where there is an abundance of user-item interactions. However, user-item interaction data are usually sparse, limiting the effectiveness of the method. In addressing this problem, recent methods have proposed to use knowledge graphs (KGs) as additional information to complement the sparse user-item interaction data. This has proved challenging given the complexity of the KG structure. In this paper, we propose a collaborative filtering method that takes advantage of knowledge graphs. More specifically, the embedding of a user and item are both grounded on the item’s attributes in the knowledge graph, and are aggregated with generic user and item representations modeled by MF for implicit recommendation. Our model has demonstrated to outperform the recent state-of-the-art method KGCN [18] in very sparse settings, showing an effective integration of KGs in recommender systems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521002725",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Collaborative filtering",
      "Complement (music)",
      "Complementation",
      "Computer science",
      "Data mining",
      "Eigenvalues and eigenvectors",
      "Embedding",
      "Gaussian",
      "Gene",
      "Graph",
      "Information retrieval",
      "Knowledge graph",
      "Machine learning",
      "Matrix decomposition",
      "Phenotype",
      "Physics",
      "Quantum mechanics",
      "Recommender system",
      "Sparse matrix",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Ya"
      },
      {
        "surname": "Mensah",
        "given_name": "Samuel"
      },
      {
        "surname": "Ma",
        "given_name": "Fei"
      },
      {
        "surname": "Wang",
        "given_name": "Hao"
      },
      {
        "surname": "Jiang",
        "given_name": "Zhongan"
      }
    ]
  },
  {
    "title": "Effective semi-supervised learning for structured data using Embedding GANs",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.07.019",
    "abstract": "The semi-supervised learning(SSL) was proposed to deal with the situation that only a few samples were labeled, so how to make the most use of the existing samples is crucial. In general, we apply data augmentation methods, like Generative Adversarial Networks(GAN), to increase the number of data when facing unstructured data. However, things get totally different with structured data, the continuity of neural networks limits their use in category variables. In this paper, we propose a semi-supervised Embedding GAN(EmGAN) to solve that problem. We add an embedding layer before the discriminator to better characterize category features and design a new loss function to further train our model. Moreover, the structures of the generator and discriminator are modified to match structured data. With the experiments of nonparametric statistical test, EmGAN shows its advantages in processing structured samples.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521002683",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Embedding",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Semi-supervised learning",
      "Supervised learning"
    ],
    "authors": [
      {
        "surname": "Deng",
        "given_name": "Xiaoheng"
      },
      {
        "surname": "Jiang",
        "given_name": "Ping"
      },
      {
        "surname": "Zhao",
        "given_name": "Dezheng"
      },
      {
        "surname": "Huang",
        "given_name": "Rong"
      },
      {
        "surname": "Shen",
        "given_name": "Hailan"
      }
    ]
  },
  {
    "title": "A Triplet network framework based automatic assessment of simulation quality for respiratory droplet propagation",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108060",
    "abstract": "Respiratory droplet propagation has been extensively explored with simulation and experimental methods. However, there still exists a huge gap between these methods, making automatic assessment of simulation quality quantitatively being a challenge. To address above problem, in this work, a triplet neural network framework with multi-scale CNN-BiLSTM network is developed. Firstly, Conditional Variational Auto-Encoder (CVAE) is utilized to generate multi-view simulations. Secondly, YOLOv3 is adopted to extract droplet regions of real image and simulation results. Then, a multi-scale CNN-BiLSTM network with attentive temporal pooling is designed to extract and aggregate temporal information across consecutive frames. Finally, all above networks are constructed into a triplet structure with triplet loss, and a regularization constraint being denoted as reconstruction term and prediction term is proposed. To demonstrate the performance of our approach, a new dataset is established including real sequences of cough droplets and simulation results. We validate the effectiveness and feasibility of our proposed framework using our dataset and two benchmarks, the PSB dataset and the ETH dataset, for 3D object retrieval. Our approach outperforms state-of-the-arts on our dataset and achieves comparative performance on PSB and ETH for 3D object retrieval, given quantitative quality assessment of simulation for droplet respiratory propagation automatically.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321002478",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Constraint (computer-aided design)",
      "Data mining",
      "Encoder",
      "Geometry",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Pooling",
      "Regularization (linguistics)"
    ],
    "authors": [
      {
        "surname": "Hu",
        "given_name": "Jinlong"
      },
      {
        "surname": "Xu",
        "given_name": "Songhua"
      },
      {
        "surname": "Ding",
        "given_name": "Xiangdong"
      }
    ]
  },
  {
    "title": "An optimal variable exponent model for Magnetic Resonance Images denoising",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.08.031",
    "abstract": "This paper investigates a novel PDE-constrained optimization model with discontinuous variable exponent p ( x ) identification. Since the parameter p is always related to a better approximation of the image gradient, its computation plays a critical role in preserving the image texture. Analytically, we include results on the approximation of this parameter as well as the resolution of the encountered PDE in a well posed framework. In addition, to resolve the PDE-constrained minimization problem, we proposed a modified primal-dual algorithm. Finally, numerical results are provided to compute the parameter p and also to remove high intensity of noise. The proposed algorithm simultaneously keep safe fine details and important features in medical image applications (Magnetic Resonance Images (MRI)) with numerous comparisons to show the performance of the proposed approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521003275",
    "keywords": [
      "Algorithm",
      "Applied mathematics",
      "Artificial intelligence",
      "Computation",
      "Computer science",
      "Exponent",
      "Image (mathematics)",
      "Linguistics",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Minification",
      "Noise (video)",
      "Noise reduction",
      "Philosophy",
      "Variable (mathematics)"
    ],
    "authors": [
      {
        "surname": "Hadri",
        "given_name": "Aissam"
      },
      {
        "surname": "Laghrib",
        "given_name": "Amine"
      },
      {
        "surname": "Oummi",
        "given_name": "Hssaine"
      }
    ]
  },
  {
    "title": "Effective fake news detection using graph and summarization techniques",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.07.020",
    "abstract": "Nowadays, fake news is widely spreading in various media, and this fake information is causing serious damage in many areas. Therefore, there is an increasing need to accurately detect fake news to prevent such damage. In this paper, we propose a novel method that uses graph and summarization techniques for fake news detection. Our proposed method represents the relationship of all sentences in a graph structure to accurately understand the context information of the document. Accordingly, the relationship between sentences in the graph is calculated as a score through the attention mechanism. Then, the summarization technique is used to reflect the sentence subject information in the graph update process. Our proposed method shows better performance than Karimi’s and BERT based models by approximately 10.34%p and 3.72%p, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521002695",
    "keywords": [
      "Artificial intelligence",
      "Automatic summarization",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Data mining",
      "Fake news",
      "Graph",
      "Information retrieval",
      "Internet privacy",
      "Natural language processing",
      "Paleontology",
      "Sentence",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Kim",
        "given_name": "Gihwan"
      },
      {
        "surname": "Ko",
        "given_name": "Youngjoong"
      }
    ]
  },
  {
    "title": "An enhanced and interpretable feature representation approach to support shape classification from binary images",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.08.020",
    "abstract": "Shape classification from binary images is a challenging task within the computer vision community. Commonly, contour and structural features are computed to describe the objects and code patterns robust against rotation, scaling, and shape deformation. However, current techniques get a high-dimensional feature space decreasing the system performance and the attribute interpretability. Here, we introduce an enhanced and interpretable feature representation approach to support shape classification from binary images. Our method, named EIFR, employs a bag of contour fragments-based feature estimation, intrinsically robust to occlusion and shape deformation. Then, a ReliefF-based feature selection is applied to filter non-discriminative attributes. In turn, a kernel-alignment-based projection is used to measure the feature relevance enhancing the data representation through the matching between a similarity matrix computed from filtered attributes and a kernel matrix built from the shape labels. Attained results on benchmark datasets prove that EIFR improves the curvature-based features’ interpretability and favors the classification performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521003135",
    "keywords": [
      "Active shape model",
      "Arithmetic",
      "Artificial intelligence",
      "Binary number",
      "Combinatorics",
      "Computer science",
      "Discriminative model",
      "Feature (linguistics)",
      "Feature extraction",
      "Feature vector",
      "Heat kernel signature",
      "Histogram",
      "Image (mathematics)",
      "Interpretability",
      "Kernel (algebra)",
      "Law",
      "Linguistics",
      "Local binary patterns",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Segmentation",
      "Similarity (geometry)",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Blandon",
        "given_name": "J.S."
      },
      {
        "surname": "Orozco-Gutierrez",
        "given_name": "A.A."
      },
      {
        "surname": "Alvarez-Meza",
        "given_name": "A.M."
      }
    ]
  },
  {
    "title": "Collaborative feature-weighted multi-view fuzzy c-means clustering",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108064",
    "abstract": "Fuzzy c-means (FCM) clustering had been extended for handling multi-view data with collaborative idea. However, these collaborative multi-view FCM treats multi-view data under equal importance of feature components. In general, different features should take different weights for clustering real multi-view data. In this paper, we propose a novel multi-view FCM (MVFCM) clustering algorithm with view and feature weights based on collaborative learning, called collaborative feature-weighted MVFCM (Co-FW-MVFCM). The Co-FW-MVFCM contains a two-step schema that includes a local step and a collaborative step. The local step is a single-view partition process to produce local partition clustering in each view, and the collaborative step is sharing information of their memberships between different views. These two steps are then continuing by an aggregation way to get a global result after collaboration. Furthermore, the embedded feature-weighted procedure in Co-FW-MVFCM can give feature reduction to exclude redundant/irrelevant feature components during clustering processes. Experiments with several data sets demonstrate that the proposed Co-FW-MVFCM algorithm can completely identify irrelevant feature components in each view and that, additionally, it can improve the performance of the algorithm. Comparisons of Co-FW-MVFCM with some existing MVFCM algorithms are made and also demonstrated the effectiveness and usefulness of the proposed Co-FW-MVFCM clustering algorithm.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032100251X",
    "keywords": [
      "Artificial intelligence",
      "Canopy clustering algorithm",
      "Cluster analysis",
      "Combinatorics",
      "Computer science",
      "Data mining",
      "FLAME clustering",
      "Feature (linguistics)",
      "Fuzzy clustering",
      "Fuzzy logic",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Partition (number theory)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Schema (genetic algorithms)"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Miin-Shen"
      },
      {
        "surname": "Sinaga",
        "given_name": "Kristina P."
      }
    ]
  },
  {
    "title": "COVID-index: A texture-based approach to classifying lung lesions based on CT images",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108083",
    "abstract": "COVID-19 is an infectious disease caused by a newly discovered type of coronavirus called SARS-CoV-2. Since the discovery of this disease in late 2019, COVID-19 has become a worldwide concern, mainly due to its high degree of contagion. As of April 2021, the number of confirmed cases of COVID-19 reported to the World Health Organization has already exceeded 135 million worldwide, while the number of deaths exceeds 2.9 million. Due to the impacts of the disease, efforts in the literature have intensified in terms of studying approaches aiming to detect COVID-19, with a focus on supporting and facilitating the process of disease diagnosis. This work proposes the application of texture descriptors based on phylogenetic relationships between species to characterize segmented CT volumes, and the subsequent classification of regions into COVID-19, solid lesion or healthy tissue. To evaluate our method, we use images from three different datasets. The results are promising, with an accuracy of 99.93%, a recall of 99.93%, a precision of 99.93%, an F1-score of 99.93%, and an AUC of 0.997. We present a robust, simple, and efficient method that can be easily applied to 2D and/or 3D images without limitations on their dimensionality.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321002703",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Coronavirus disease 2019 (COVID-19)",
      "Curse of dimensionality",
      "Disease",
      "Focus (optics)",
      "Image (mathematics)",
      "Infectious disease (medical specialty)",
      "Medicine",
      "Optics",
      "Pathology",
      "Pattern recognition (psychology)",
      "Physics",
      "Precision and recall",
      "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)",
      "Texture (cosmology)"
    ],
    "authors": [
      {
        "surname": "de Carvalho Brito",
        "given_name": "Vitória"
      },
      {
        "surname": "dos Santos",
        "given_name": "Patrick Ryan Sales"
      },
      {
        "surname": "de Sales Carvalho",
        "given_name": "Nonato Rodrigues"
      },
      {
        "surname": "de Carvalho Filho",
        "given_name": "Antonio Oseas"
      }
    ]
  },
  {
    "title": "Semi-supervised partial multi-label classification with low-rank and manifold constraints",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.08.005",
    "abstract": "Multi-Label Classification (MLC), processing data annotated with multiple labels simultaneously, is usually based on the assumption that the instances are all correctly labeled with the ground-truth labels. However this assumption is often not satisfied in real-world scenarios due to the high cost of manual labeling. In this paper, we consider a classification scenario in which only a part of the instances are redundantly annotated, and call this scenario a Semi-supervised Partial Multi-Label Classification (SPMLC). We propose a novel method Lion (semi-supervised partial multi-label classification with Low-rank and manIfOld coNstraints) to handle this particular scenario. Especially, the not exactly known ground-truth labels are regarded as latent labels, and the prediction is achieved by estimating the confidences of latent labels (i.e., the latent ground-truth confidences) in future data. Lion jointly learns the latent ground-truth confidences and the prediction model. The low-rank and manifold constraints are utilized to capture local label correlations and neighboring structure of instances, so as to accurately estimate the latent ground-truth confidences. Extensive experimental results validate the effectiveness of Lion.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521002828",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Engineering",
      "Ground truth",
      "Machine learning",
      "Manifold (fluid mechanics)",
      "Mathematics",
      "Mechanical engineering",
      "Pattern recognition (psychology)",
      "Rank (graph theory)"
    ],
    "authors": [
      {
        "surname": "Guan",
        "given_name": "Yuanyuan"
      },
      {
        "surname": "Zhang",
        "given_name": "Boxiang"
      },
      {
        "surname": "Li",
        "given_name": "Wenhui"
      },
      {
        "surname": "Wang",
        "given_name": "Ying"
      }
    ]
  },
  {
    "title": "Principal component analysis in the wavelet domain",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108096",
    "abstract": "This paper proposes a new principal component analysis method in the wavelet domain, which is useful for dimension reduction and feature extraction of multiple non-stationary time series. The proposed method is constructed using a novel combination of eigenanalysis and the local wavelet spectrum defined in the locally stationary wavelet process. Therefore, we can expect the proposed method to reflect a more generalized non-stationary time series beyond some limited types of signals that existing methods have performed. We investigate the theoretical results of estimated principal components and their loadings. The results of numerical examples, including the analysis of real seismic data and financial data, show the promising empirical properties of the proposed approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321002831",
    "keywords": [
      "Algorithm",
      "Applied mathematics",
      "Artificial intelligence",
      "Biology",
      "Cascade algorithm",
      "Computer science",
      "Dimension (graph theory)",
      "Dimensionality reduction",
      "Discrete wavelet transform",
      "Domain (mathematical analysis)",
      "Mathematical analysis",
      "Mathematics",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Principal component analysis",
      "Pure mathematics",
      "Series (stratigraphy)",
      "Stationary process",
      "Stationary wavelet transform",
      "Wavelet",
      "Wavelet transform"
    ],
    "authors": [
      {
        "surname": "Lim",
        "given_name": "Yaeji"
      },
      {
        "surname": "Kwon",
        "given_name": "Junhyeon"
      },
      {
        "surname": "Oh",
        "given_name": "Hee-Seok"
      }
    ]
  },
  {
    "title": "FFAVOD: Feature fusion architecture for video object detection",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.09.002",
    "abstract": "A significant amount of redundancy exists between consecutive frames of a video. Object detectors typically produce detections for one image at a time, without any capabilities for taking advantage of this redundancy. Meanwhile, many applications for object detection work with videos, including intelligent transportation systems, advanced driver assistance systems and video surveillance. Our work aims at taking advantage of the similarity between video frames to produce better detections. We propose FFAVOD, standing for feature fusion architecture for video object detection. We first introduce a novel video object detection architecture that allows a network to share feature maps between nearby frames. Second, we propose a feature fusion module that learns to merge feature maps to enhance them. We show that using the proposed architecture and the fusion module can improve the performance of three base object detectors on two object detection benchmarks containing sequences of moving road users. Additionally, to further increase performance, we propose an improvement to the SpotNet attention module. Using our architecture on the improved SpotNet detector, we obtain the state-of-the-art performance on the UA-DETRAC public benchmark as well as on the UAVDT dataset. Code is available at https://github.com/hu64/FFAVOD.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552100307X",
    "keywords": [
      "Architecture",
      "Art",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Computer vision",
      "Detector",
      "Feature (linguistics)",
      "Feature extraction",
      "Geodesy",
      "Geography",
      "Information retrieval",
      "Linguistics",
      "Merge (version control)",
      "Object (grammar)",
      "Object detection",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Redundancy (engineering)",
      "Telecommunications",
      "Video tracking",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Perreault",
        "given_name": "Hughes"
      },
      {
        "surname": "Bilodeau",
        "given_name": "Guillaume-Alexandre"
      },
      {
        "surname": "Saunier",
        "given_name": "Nicolas"
      },
      {
        "surname": "Héritier",
        "given_name": "Maguelonne"
      }
    ]
  },
  {
    "title": "MOON: Multi-hash codes joint learning for cross-media retrieval",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.07.018",
    "abstract": "In recent years, cross-media hashing technique has attracted increasing attention for its high computation efficiency and low storage cost. However, the existing approaches still have some limitations, which need to be explored. (1) A fixed hash length (e.g., 16bits or 32bits) is predefined before learning the binary codes. Therefore, these models need to be retrained when the hash length changes, that consumes additional computation power, reducing the scalability in practical applications. (2) Existing cross-modal approaches only explore the information in the original multimedia data to perform the hash learning, without exploiting the semantic information contained in the learned hash codes. To this end, we develop a novel Multiple hash cOdes jOint learNing method (MOON) for cross-media retrieval. Specifically, the developed MOON simultaneously learns the hash codes with multiple lengths in a unified framework. Besides, to enhance the underlying discrimination, we combine the clues from the multimodal data, semantic label and learned hash codes for hash learning. As far as we know, the proposed MOON is the first attempt to simultaneously learn different length hash codes without retraining in cross-media retrieval. Experiments on several databases show that our MOON can achieve promising performance, outperforming some recent competitive shallow and deep methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521002671",
    "keywords": [
      "Computer science",
      "Computer security",
      "Database",
      "Double hashing",
      "Dynamic perfect hashing",
      "Hash function",
      "Hash table",
      "Information retrieval",
      "Scalability",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Donglin"
      },
      {
        "surname": "Wu",
        "given_name": "Xiao-Jun"
      },
      {
        "surname": "Yin",
        "given_name": "He-Feng"
      },
      {
        "surname": "Kittler",
        "given_name": "Josef"
      }
    ]
  },
  {
    "title": "Stroke constrained attention network for online handwritten mathematical expression recognition",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108047",
    "abstract": "In this paper, we propose a novel stroke constrained attention network (SCAN) which treats stroke as the basic unit for encoder-decoder based online handwritten mathematical expression recognition (HMER). Unlike previous methods which use trace points or image pixels as basic units, SCAN makes full use of stroke-level information for better alignment and representation. The proposed SCAN can be adopted in both single-modal (online or offline) and multi-modal HMER. For single-modal HMER, SCAN first employs a CNN-GRU encoder to extract point-level features from input traces in online mode and employs a CNN encoder to extract pixel-level features from input images in offline mode, then use stroke constrained information to convert them into online and offline stroke-level features. Using stroke-level features can explicitly group points or pixels belonging to the same stroke, therefore reduces the difficulty of symbol segmentation and recognition via the decoder with attention mechanism. For multi-modal HMER, other than fusing multi-modal information in decoder, SCAN can also fuse multi-modal information in encoder by utilizing the stroke based alignments between online and offline modalities. The encoder fusion is a better way for combining multi-modal information as it implements the information interaction one step before the decoder fusion so that the advantages of multiple modalities can be exploited earlier and more adequately. Besides, we propose an approach combining the encoder fusion and decoder fusion, namely encoder-decoder fusion, which can further improve the performance. Evaluated on a benchmark published by CROHME competition, the proposed SCAN achieves the state-of-the-art performance. Furthermore, by conducting experiments on an additional task: online handwritten Chinese character recognition (HCCR), we demonstrate the generality of our proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032100234X",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Encoder",
      "Expression (computer science)",
      "Geodesy",
      "Geography",
      "Modal",
      "Operating system",
      "Pattern recognition (psychology)",
      "Pixel",
      "Polymer chemistry",
      "Programming language",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Jiaming"
      },
      {
        "surname": "Du",
        "given_name": "Jun"
      },
      {
        "surname": "Zhang",
        "given_name": "Jianshu"
      },
      {
        "surname": "Wang",
        "given_name": "Bin"
      },
      {
        "surname": "Ren",
        "given_name": "Bo"
      }
    ]
  },
  {
    "title": "Semi-supervised multi-Layer convolution kernel learning in credit evaluation",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108125",
    "abstract": "In many practical credit evaluation problems, a lot of manpower as well as financial and material resources are required to label samples. Therefore, in the process of labeling, only a small number of samples with category labels can be obtained to train classification models and a large number of customer samples is abandoned without category labels. To solve this problem, we introduce a semi-supervised support vector machine (SVM) technology and combines it with a multi-layer convolution kernel to construct a semi-supervised multi-layer convolution kernel SVM (SSMCK) for category customer credit assessment data sets. We first use a basic solution of the generalized differential operator to generate a base convolution kernel function in the H 1 space, and then use the multi-layer strategy of deep learning to construct the multi-layer convolution kernel in the H 2 and H 3 space (called the family of multi-layer convolution kernel) by using the kernel functions in the H 1 space. We further propose a semi-supervised multi-layer convolution kernel SVM algorithm based on the category center estimation and develop two novel SSMCK methods to improve the classification ability: the SSMCK based on multi-kernel learning (SSMCK-MKL) and the SSMCK based on alternative optimization (SSMCK-AO). Finally, experimental verification and analysis is carried out on three customer credit evaluation data sets. The results show that our methods outperforms or are comparable to some the state-of-the-art credit evaluation models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003125",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Combinatorics",
      "Computer science",
      "Construct (python library)",
      "Convolution (computer science)",
      "Convolution theorem",
      "Fourier analysis",
      "Fourier transform",
      "Fractional Fourier transform",
      "Kernel (algebra)",
      "Kernel embedding of distributions",
      "Kernel method",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Multiple kernel learning",
      "Pattern recognition (psychology)",
      "Polynomial kernel",
      "Programming language",
      "Radial basis function kernel",
      "Support vector machine",
      "Tree kernel"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Lixiang"
      },
      {
        "surname": "Cui",
        "given_name": "Lixin"
      },
      {
        "surname": "Weise",
        "given_name": "Thomas"
      },
      {
        "surname": "Li",
        "given_name": "Xinlu"
      },
      {
        "surname": "Wu",
        "given_name": "Zhize"
      },
      {
        "surname": "Nie",
        "given_name": "Feiping"
      },
      {
        "surname": "Chen",
        "given_name": "Enhong"
      },
      {
        "surname": "Tang",
        "given_name": "Yuanyan"
      }
    ]
  },
  {
    "title": "Top-rank convolutional neural network and its application to medical image-based diagnosis",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108138",
    "abstract": "Top-rank learning identifies a real-valued ranking function that will provide more absolute top samples. These are highly reliable positive samples that are ranked higher than the highest-ranked negative samples. Therefore, top-rank learning is useful for tasks that require reliable decisions. Additionally, it inherits the merits of the ranking functions, such as robustness to the unbalanced condition. However, conventional top-rank learning tasks are formulated as linear or kernel-based problems and are thus limited in coping with complicated tasks. In this study, we propose a Top-rank convolutional neural network (TopRank CNN) to realize top-rank learning with representation learning for complicated tasks. Given that the original objective function of top-rank learning suffers from overfitting, we employ the p -norm relaxation of the original loss function in the proposed method. We prove the usefulness of TopRank CNN experimentally with medical diagnosis tasks that require reliable decisions and robustness to the unbalanced condition.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003253",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Combinatorics",
      "Computer science",
      "Convolutional neural network",
      "Gene",
      "Learning to rank",
      "Machine learning",
      "Mathematics",
      "Overfitting",
      "Pattern recognition (psychology)",
      "Rank (graph theory)",
      "Ranking (information retrieval)",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Zheng",
        "given_name": "Yan"
      },
      {
        "surname": "Zheng",
        "given_name": "Yuchen"
      },
      {
        "surname": "Suehiro",
        "given_name": "Daiki"
      },
      {
        "surname": "Uchida",
        "given_name": "Seiichi"
      }
    ]
  },
  {
    "title": "Medical Image Encryption Scheme Using Multiple Chaotic Maps",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.10.033",
    "abstract": "Telemedicine and various tele-medicinal applications are revolutionizing a variety of healthcare departments through its innovative means of remote diagnosing and faster first aid administration. Digital images play an important role in these applications for ensuring better and faster health carfigure. These digital images, which usually contain confidential and diagnostic information about the patients, are usually transmitted through public networks among hospitals, doctors and patients. Consequently, there is a need to secure them while being stored and in transit to guarantee the privacy of the patient. However, unique properties of digital image data, such as high redundancy and correlation between the pixels of the image and their large size, make the conventional cryptographical algorithms insufficient for ensuring proper security while encrypting them. As the conventional algorithms seize to be a reliable solution, the need to develop improved and dedicated image encryption algorithms also arises. Chaotic systems are systems that appear random and unpredictable from the outside but are governed by deterministic equations or rules on the inside. Due to these properties, along with ergodicity and its heightened sensitivity to the initial conditions, chaotic systems are devised to be one of the best candidates for securing the storage and transmission of digital images. However, the security of a chaotic image encryption system depends upon the chaotic behavior demonstrated by the chaotic map applied in the image encryption system. Because of this, different attacks can be used to break a chaotic encryption system if the scheme is not well-structured. This paper introduces a chaotic image encryption scheme that incorporates two chaotic maps, namely, Arnold's Cat Map and 2D Logistic-Sine-Coupling Map(2D-LSCM), for increased randomness and security of the encrypted image. We also analyze the performance and security of our scheme and compare it with other prominent chaotic image encryption schemes.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521003913",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Chaotic",
      "Computer engineering",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Cryptography",
      "Data mining",
      "Digital image",
      "Encryption",
      "Image (mathematics)",
      "Image processing",
      "Operating system",
      "Pixel",
      "Redundancy (engineering)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Jain",
        "given_name": "Kurunandan"
      },
      {
        "surname": "Aji",
        "given_name": "Aravind"
      },
      {
        "surname": "Krishnan",
        "given_name": "Prabhakar"
      }
    ]
  },
  {
    "title": "Selection of diverse features with a diverse regularization",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108154",
    "abstract": "Many embedded feature selection methods ignore the correlation among the important features. To reduce correlation, some models introduce constraints to impose sparsity on features, some try to exploit the similarity and group features without changing the objective function. In this paper, we propose diverse feature selection (DFS), which simultaneously performs feature clustering and selection. Given a dataset with known class labels, we separate the features into a set of feature clusters where the features in the same cluster have a higher correlation with each other than with the features in different clusters. A diverse regularization (DR) is proposed to reduce the linear and nonlinear correlations among important features. Using this regularization, DFS can select features that are both informative and diverse. The experimental results on seven image datasets, five gene datasets as well as four other datasets demonstrate the superior performance of DFS.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003411",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Computer security",
      "Correlation",
      "Data mining",
      "Exploit",
      "Feature (linguistics)",
      "Feature selection",
      "Geometry",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Regularization (linguistics)"
    ],
    "authors": [
      {
        "surname": "Zhong",
        "given_name": "Weichan"
      },
      {
        "surname": "Chen",
        "given_name": "Xiaojun"
      },
      {
        "surname": "Wu",
        "given_name": "Qingyao"
      },
      {
        "surname": "Yang",
        "given_name": "Min"
      },
      {
        "surname": "Huang",
        "given_name": "Joshua Zhexue"
      }
    ]
  },
  {
    "title": "Knowledge memorization and generation for action recognition in still images",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108188",
    "abstract": "Human action recognition in visual data is one of the most fundamental challenges in computer vision. Existing approaches for this primary goal have been based on video data, often incorporating both color and dynamic flow information. Nevertheless, the majority of the visual data constitute still images, and for this reason, being able to recognize actions in still image is an ultimate objective of visual understanding with an extended list of applications. In this paper, we present a novel method that transfers the knowledge learned from action videos onto images to allow recognition of the principal action depicted in still image. Our intuition is that a generative model for knowledge transfer can be learned by taking advantage of the available action videos in the training stage to bridge images to videos. Based on this, we propose two complementary knowledge-transfer models utilizing fully connected networks to deliver the knowledge extracted from color and motion flow sequences to still images. We introduce a weighted reconstruction and classification loss to steer the generation procedure of the networks. In addition, we describe and analyze the influence of different data augmentation techniques, initialization strategies, and weighting coefficients for improving the performance. We observe that: both the transferred knowledge from color sequences and motion flow sequences can improve the performance of still image based human action recognition; the latter one which provides complementary dynamic information improves the performance a lot. We evaluate our models on two publicly available video based human action recognition datasets: UCF101 and HMDB51. To further validate the generalization ability of the proposed solution, we test the learned models from UCF101 dataset on two still image based human action recognition benchmarks: Willow7 Actions and the Sports. Our results demonstrate that the proposed method outperforms the baseline approaches with more than 2% accuracy, 3% accuracy, 3% accuracy and 5% mAP on UCF101, HMDB51, Sports and Willow 7 Actions datasets, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003757",
    "keywords": [
      "Action (physics)",
      "Action recognition",
      "Artificial intelligence",
      "Class (philosophy)",
      "Classifier (UML)",
      "Computer science",
      "Computer vision",
      "Image (mathematics)",
      "Initialization",
      "Machine learning",
      "Optical flow",
      "Pattern recognition (psychology)",
      "Physics",
      "Programming language",
      "Quantum mechanics"
    ],
    "authors": [
      {
        "surname": "Dong",
        "given_name": "Jian"
      },
      {
        "surname": "Yang",
        "given_name": "Wankou"
      },
      {
        "surname": "Yao",
        "given_name": "Yazhou"
      },
      {
        "surname": "Porikli",
        "given_name": "Fatih"
      }
    ]
  },
  {
    "title": "DeepLTRS: A deep latent recommender system based on user ratings and reviews",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.10.022",
    "abstract": "We introduce a deep latent recommender system named deepLTRS in order to provide users with high quality recommendations based on observed user ratings and texts of product reviews. The underlying motivation is that, when a user scores only a few products, the texts used in the reviews represent a significant source of information, thereby enhancing the predictive ability of the model. Our approach adopts a variational auto-encoder (VAE) architecture as a deep generative latent model for an ordinal matrix encoding ratings and a document-term matrix encoding the reviews. Taking into account both matrices as model inputs, deepLTRS uses a neural network to capture the relationship between latent factors and latent topics. Moreover, a user-majoring encoder and a product-majoring encoder are constructed to jointly capture user and product preferences. Due to the specificity of the model structure, an original row-column alternated mini-batch optimization algorithm is proposed to deal with user-product dependencies and computational burden. Numerical experiments on simulated and real-world data sets demonstrate that deepLTRS outperforms the state-of-the-art, in particular in context of extreme data sparsity.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521003822",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Association rule learning",
      "Autoencoder",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Data mining",
      "Encoder",
      "Encoding (memory)",
      "Generative grammar",
      "Generative model",
      "Geometry",
      "Information retrieval",
      "Latent variable",
      "Machine learning",
      "Mathematics",
      "Operating system",
      "Paleontology",
      "Product (mathematics)",
      "Recommender system"
    ],
    "authors": [
      {
        "surname": "Liang",
        "given_name": "Dingge"
      },
      {
        "surname": "Corneli",
        "given_name": "Marco"
      },
      {
        "surname": "Bouveyron",
        "given_name": "Charles"
      },
      {
        "surname": "Latouche",
        "given_name": "Pierre"
      }
    ]
  },
  {
    "title": "Protection motivation theory using multi-factor authentication for providing security over social networking sites",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.10.002",
    "abstract": "Nowadays, Passwords are to identify users of Social Networking Sites (SNS). However, there are some downsides to it such as the user forgetting their password or the account being hacked by an attacker. To retrieve their password, websites are asking for an alternate email address or security question. Due to its expanding features and popularity, social media has become more vulnerable. It is no secret that social media providers leave security to the discretion of users during the development, which raises serious concerns. Earlier research has been supported by the Protective Motivation Theory (PMT), which provides a hypothetical structure for analyzing the protection of Internet users. According to the current internet safety search, new engines incorporated into a PMT structure for perhaps the first time. Adaptation assessment variables, including the strength of behavior, the effectiveness of responses, and individual responsibility, were the most important predictors of online safety intentions. A multi-factor authentication mechanism linked to a trust model used on SNS. The needs, actions, and behaviors of users are particularly adaptable to confidence systems. The level of threat was also a significant predictor. The explanatory power of the standard PMT model with the multi-factor authentication technique was raised by 15% by incorporating new components such as past experiences, behavioral control, habitual strength, security and safety support, and individual responsibility. The results are considered in the advanced evolution of PMT in the context of Internet security for home computer users.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521003603",
    "keywords": [
      "Authentication (law)",
      "Biology",
      "Computer science",
      "Computer security",
      "Context (archaeology)",
      "Internet privacy",
      "Paleontology",
      "Password",
      "Popularity",
      "Psychology",
      "Social media",
      "Social psychology",
      "The Internet",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Mehraj",
        "given_name": "Haider"
      },
      {
        "surname": "Jayadevappa",
        "given_name": "D."
      },
      {
        "surname": "Haleem",
        "given_name": "Sulaima Lebbe Abdul"
      },
      {
        "surname": "Parveen",
        "given_name": "Rehana"
      },
      {
        "surname": "Madduri",
        "given_name": "Abhishek"
      },
      {
        "surname": "Ayyagari",
        "given_name": "Maruthi Rohit"
      },
      {
        "surname": "Dhabliya",
        "given_name": "Dharmesh"
      }
    ]
  },
  {
    "title": "3 D object recognition through a size function resulting from an invariant topological feature",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108131",
    "abstract": "In this paper, a critical points based descriptor for 3 D objects recognition is presented. It is based on the topological invariant provided by the critical points of the 3 D object. The critical points and the links between them are represented by a size function resulting from a measure function that captures the surface displacement along the 3 D object, and that encompasses invariance to affine transformations, articulations and torsions. In order to tackle the problems of partial matching of the 3 D objects, a well-suited metric learning method is used to weight the matchings according to their relevance. The proposed method’s performance was validated by different collections of 3 D objects. The obtained scores are favorably comparable to the related work.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003186",
    "keywords": [
      "Affine transformation",
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Geometry",
      "Invariant (physics)",
      "Mathematical physics",
      "Mathematics"
    ],
    "authors": [
      {
        "surname": "Alaoui Mhamdi",
        "given_name": "Mohammed Ayoub"
      },
      {
        "surname": "Ziou",
        "given_name": "Djemel"
      }
    ]
  },
  {
    "title": "Quantitative performance evaluation of object detectors in hazy environments",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.10.001",
    "abstract": "We present a quantitative performance analysis of a wide range of state-of-the-art object detection models, such as Mask R-CNN He et al.(2017)[8], RetinaNet Lin et al.(2017)[17] and EfficinetDet Tan et al.(2019)[28] in haze affected environments. This work uses two key performance metrics (Mean Average Precision and Localised Recall Precision) to provide a nuanced view of real world performance of these models in an on-road driving application. Our findings show that the presence of haze further exacerbates the performance differences between single-stage and multi-stage detection models. In addition, not all aspects of the model performance are affected equally. The inclusion of Local Recall Precision (LRP) Oksuz et al.(2018)[21] suggests that more recent models have much improved localisation performance even with similar false negative and false positive results. We also highlight some of the inherent limitations of Neural Network based approaches that could be addressed by Bayesian Neural Networks in the future.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521003597",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Detector",
      "Geology",
      "Object (grammar)",
      "Remote sensing",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Hodges",
        "given_name": "Cameron"
      },
      {
        "surname": "Bennamoun",
        "given_name": "Mohammed"
      },
      {
        "surname": "Boussaid",
        "given_name": "Farid"
      }
    ]
  },
  {
    "title": "Progressive global perception and local polishing network for lung infection segmentation of COVID-19 CT images",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108168",
    "abstract": "In this paper, a progressive global perception and local polishing (PCPLP) network is proposed to automatically segment the COVID-19-caused pneumonia infections in computed tomography (CT) images. The proposed PCPLP follows an encoder-decoder architecture. Particularly, the encoder is implemented as a computationally efficient fully convolutional network (FCN). In this study, a multi-scale multi-level feature recursive aggregation (mmFRA) network is used to integrate multi-scale features (viz. global guidance features and local refinement features) with multi-level features (viz. high-level semantic features, middle-level comprehensive features, and low-level detailed features). Because of this innovative aggregation of features, an edge-preserving segmentation map can be produced in a boundary-aware multiple supervision (BMS) way. Furthermore, both global perception and local perception are devised. On the one hand, a global perception module (GPM) providing a holistic estimation of potential lung infection regions is employed to capture more complementary coarse-structure information from different pyramid levels by enlarging the receptive fields without substantially increasing the computational burden. On the other hand, a local polishing module (LPM), which provides a fine prediction of the segmentation regions, is applied to explicitly heighten the fine-detail information and reduce the dilution effect of boundary knowledge. Comprehensive experimental evaluations demonstrate the effectiveness of the proposed PCPLP in boosting the learning ability to identify the lung infected regions with clear contours accurately. Our model is superior remarkably to the state-of-the-art segmentation models both quantitatively and qualitatively on a real CT dataset of COVID-19.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003551",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Encoder",
      "Feature (linguistics)",
      "Geometry",
      "Linguistics",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pyramid (geometry)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Mu",
        "given_name": "Nan"
      },
      {
        "surname": "Wang",
        "given_name": "Hongyu"
      },
      {
        "surname": "Zhang",
        "given_name": "Yu"
      },
      {
        "surname": "Jiang",
        "given_name": "Jingfeng"
      },
      {
        "surname": "Tang",
        "given_name": "Jinshan"
      }
    ]
  },
  {
    "title": "FoCL: Feature-oriented continual learning for generative models",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108127",
    "abstract": "In this paper, we propose a general framework in continual learning for generative models: Feature-oriented Continual Learning (FoCL). Unlike previous works that aim to solve the catastrophic forgetting problem by introducing regularization in the parameter space or image space, FoCL imposes regularization in the feature space. We show in our experiments that FoCL has faster adaptation to distributional changes in sequentially arriving tasks, and achieves state-of-the-art performance for generative models in task incremental learning. We discuss choices of combined regularization spaces towards different use case scenarios for boosted performance, e.g., tasks that have high variability in the background. Finally, we introduce a forgetfulness measure that fairly evaluates the degree to which a model suffers from forgetting. Interestingly, the analysis of our proposed forgetfulness score also implies that FoCL tends to have a mitigated forgetting for future tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003149",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Feature (linguistics)",
      "Feature learning",
      "Generative grammar",
      "Generative model",
      "Linguistics",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Lao",
        "given_name": "Qicheng"
      },
      {
        "surname": "Mortazavi",
        "given_name": "Mehrzad"
      },
      {
        "surname": "Tahaei",
        "given_name": "Marzieh"
      },
      {
        "surname": "Dutil",
        "given_name": "Francis"
      },
      {
        "surname": "Fevens",
        "given_name": "Thomas"
      },
      {
        "surname": "Havaei",
        "given_name": "Mohammad"
      }
    ]
  },
  {
    "title": "Weighted clustering: Towards solving the user’s dilemma",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108152",
    "abstract": "This paper makes a major step towards addressing a long-standing challenge in cluster analysis, known as the user’s dilemma, which is the problem of selecting an appropriate clustering algorithm for a specific task. A formal approach for addressing this challenge relies on the identification of succinct, user-friendly properties that capture formal differences amongst clustering techniques. While helpful for gaining insight into the nature of clustering paradigms, there is a theory-practice gap that has so far limited the utility of this approach: Formal properties typically highlight advantages of classical linkage-based algorithms, while practical experience shows that center-based methods are preferable for many applications. We present simple new properties that delineate core differences between common clustering paradigms and overcome this theory-practice gap. The properties we present give a formal understanding of the advantages of center-based approaches for some applications and insight into when different clustering paradigms should be used. These properties address how sensitive algorithms are to changes in element frequencies, which we capture in a generalized setting where every element is associated with a real-valued weight. To complement extensive formal analysis, we discuss how these properties can be applied in practice.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003393",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Cluster (spacecraft)",
      "Cluster analysis",
      "Complement (music)",
      "Complementation",
      "Computer science",
      "Data mining",
      "Dilemma",
      "Gene",
      "Geometry",
      "Machine learning",
      "Mathematics",
      "Phenotype",
      "Programming language",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Ackerman",
        "given_name": "Margareta"
      },
      {
        "surname": "Ben-David",
        "given_name": "Shai"
      },
      {
        "surname": "Brânzei",
        "given_name": "Simina"
      },
      {
        "surname": "Loker",
        "given_name": "David"
      }
    ]
  },
  {
    "title": "Graph representation learning for road type classification",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108174",
    "abstract": "We present a novel learning-based approach to graph representations of road networks employing state-of-the-art graph convolutional neural networks. Our approach is applied to realistic road networks of 17 cities from Open Street Map. While edge features are crucial to generate descriptive graph representations of road networks, graph convolutional networks usually rely on node features only. We show that the highly representative edge features can still be integrated into such networks by applying a line graph transformation. We also propose a method for neighborhood sampling based on a topological neighborhood composed of both local and global neighbors. We compare the performance of learning representations using different types of neighborhood aggregation functions in transductive and inductive tasks and in supervised and unsupervised learning. Furthermore, we propose a novel aggregation approach, Graph Attention Isomorphism Network, GAIN 1 . Our results show that GAIN outperforms state-of-the-art methods on the road type classification problem.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003617",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Feature learning",
      "Graph",
      "Graph isomorphism",
      "Line graph",
      "Pattern recognition (psychology)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Gharaee",
        "given_name": "Zahra"
      },
      {
        "surname": "Kowshik",
        "given_name": "Shreyas"
      },
      {
        "surname": "Stromann",
        "given_name": "Oliver"
      },
      {
        "surname": "Felsberg",
        "given_name": "Michael"
      }
    ]
  },
  {
    "title": "On the benefits of defining vicinal distributions in latent space",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.10.016",
    "abstract": "The vicinal risk minimization (VRM) principle is an empirical risk minimization (ERM) variant that replaces Dirac masses with vicinal functions. There is strong numerical and theoretical evidence showing that VRM outperforms ERM in terms of generalization if appropriate vicinal functions are chosen. Mixup Training (MT), a popular choice of vicinal distribution, improves generalization performance of models by introducing globally linear behavior in between training examples. Apart from generalization, recent works have shown that mixup trained models are relatively robust to input perturbations/corruptions and at same time are calibrated better than their non-mixup counterparts. In this work, we investigate the benefits of defining these vicinal distributions like mixup in latent space of generative models rather than in input space itself. We propose a new approach - VarMixup (Variational Mixup) - to better sample mixup images by using the latent manifold underlying the data. Our empirical studies on CIFAR-10, CIFAR-100 and Tiny-ImageNet demonstrates that models trained by performing mixup in the latent manifold learned by VAEs are inherently more robust to various input corruptions/perturbations, are significantly better calibrated and exhibit more local-linear loss landscapes.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521003755",
    "keywords": [
      "Algorithm",
      "Applied mathematics",
      "Artificial intelligence",
      "Computer science",
      "Engineering",
      "Generalization",
      "Latent variable",
      "Machine learning",
      "Manifold (fluid mechanics)",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Mechanical engineering",
      "Operating system",
      "Physics",
      "Quantum mechanics",
      "Space (punctuation)",
      "Vicinal"
    ],
    "authors": [
      {
        "surname": "Mangla",
        "given_name": "Puneet"
      },
      {
        "surname": "Singh",
        "given_name": "Vedant"
      },
      {
        "surname": "Havaldar",
        "given_name": "Shreyas"
      },
      {
        "surname": "Balasubramanian",
        "given_name": "Vineeth"
      }
    ]
  },
  {
    "title": "GAMI-Net: An explainable neural network based on generalized additive models with structured interactions",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108192",
    "abstract": "The lack of interpretability is an inevitable problem when using neural network models in real applications. In this paper, an explainable neural network based on generalized additive models with structured interactions (GAMI-Net) is proposed to pursue a good balance between prediction accuracy and model interpretability. GAMI-Net is a disentangled feedforward network with multiple additive subnetworks; each subnetwork consists of multiple hidden layers and is designed for capturing one main effect or one pairwise interaction. Three interpretability aspects are further considered, including a) sparsity, to select the most significant effects for parsimonious representations; b) heredity, a pairwise interaction could only be included when at least one of its parent main effects exists; and c) marginal clarity, to make main effects and pairwise interactions mutually distinguishable. An adaptive training algorithm is developed, where main effects are first trained and then pairwise interactions are fitted to the residuals. Numerical experiments on both synthetic functions and real-world datasets show that the proposed model enjoys superior interpretability and it maintains competitive prediction accuracy in comparison to the explainable boosting machine and other classic machine learning models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003484",
    "keywords": [
      "Additive model",
      "Artificial intelligence",
      "Artificial neural network",
      "Boosting (machine learning)",
      "Computer science",
      "Computer security",
      "Interpretability",
      "Machine learning",
      "Pairwise comparison",
      "Subnetwork"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Zebin"
      },
      {
        "surname": "Zhang",
        "given_name": "Aijun"
      },
      {
        "surname": "Sudjianto",
        "given_name": "Agus"
      }
    ]
  },
  {
    "title": "Certainty driven consistency loss on multi-teacher networks for semi-supervised learning",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108140",
    "abstract": "One of the successful approaches in semi-supervised learning is based on the consistency regularization. Typically, a student model is trained to be consistent with teacher prediction for the inputs under different perturbations. To be successful, the prediction targets given by teacher should have good quality, otherwise the student can be misled by teacher. Unfortunately, existing methods do not assess the quality of the teacher targets. In this paper, we propose a novel Certainty-driven Consistency Loss (CCL) that exploits the predictive uncertainty in the consistency loss to let the student dynamically learn from reliable targets. Specifically, we propose two approaches, i.e. Filtering CCL and Temperature CCL to either filter out uncertain predictions or pay less attention on them in the consistency regularization. We further introduce a novel decoupled framework to encourage model difference. Experimental results on SVHN, CIFAR-10, and CIFAR-100 demonstrate the advantages of our method over a few existing methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003277",
    "keywords": [
      "Artificial intelligence",
      "Certainty",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Consistency (knowledge bases)",
      "Epistemology",
      "Exploit",
      "Filter (signal processing)",
      "Geometry",
      "Machine learning",
      "Mathematics",
      "Philosophy",
      "Quality (philosophy)",
      "Regularization (linguistics)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Lu"
      },
      {
        "surname": "Tan",
        "given_name": "Robby T."
      }
    ]
  },
  {
    "title": "Point cloud denoising using non-local collaborative projections",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108128",
    "abstract": "Point cloud is important for object detection and recognition. The main challenge of point cloud denoising is to preserve the geometric structures. Several state-of-the-art point cloud denoising methods focus only on analyzing local geometric information, which is sensitive to noise and outliers. In this paper, we propose a novel point cloud denoising algorithm based on the characteristics of non-local self-similarity. First, we present an adaptive curvature threshold to select the edge points and tune their corresponding normals, which can preserve the sharp details. Then we propose a structure-aware descriptor called projective height vector to capture the local height variations by normal height projection and the most similar non-local projective height vectors are grouped into a height matrix to enhance the structure representation. Moreover, the proposed structure descriptor is invariant with rigid transformation. Finally, an improved weighted nuclear norm minimization is proposed to optimize the height matrix and reconstruct a high-quality point cloud. Rather than treating each singular value independently, each component in our proposed weight definition connects with the most important components to preserve the major structural information. Experiments on synthetic and scanned point cloud datasets demonstrate that our algorithm outperforms state-of-the-art methods in terms of reconstruction accuracy and structure preservation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003150",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Geometry",
      "Mathematics",
      "Noise reduction",
      "Normal",
      "Outlier",
      "Pattern recognition (psychology)",
      "Point cloud",
      "Rigid transformation",
      "Surface (topology)"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Yiyao"
      },
      {
        "surname": "Chen",
        "given_name": "Rui"
      },
      {
        "surname": "Zhao",
        "given_name": "Yiqiang"
      },
      {
        "surname": "Ai",
        "given_name": "Xiding"
      },
      {
        "surname": "Zhou",
        "given_name": "Guoqing"
      }
    ]
  },
  {
    "title": "Semantic manifold modularization-based ranking for image recommendation",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108100",
    "abstract": "As the Internet confronts the multimedia explosion, it becomes urgent to investigate personalized recommendation for alleviating information overload and improving users’ experience. Most personalized recommendation approaches pay their attention to collaborative filtering over users’ interactions, which suffers greatly from the highly sparse interactions. In image recommendation, visual correlations among images that users consumed provide a piece of intrinsic evidence to reveal users’ interests. It inspires us to investigate image recommendation over the dense visual graph of images instead of the sparse user interaction graph. In this paper, we propose a semantic manifold modularization-based ranking (MMR) for image recommendation. MMR leverages the dense visual manifold to propagate users’ historical records and infer user-image correlations for image recommendation. Especially, it constrains interest propagation within semantic visual compact groups by manifold modularization to make a tradeoff between users’ personality and graph smoothness in propagation. Experimental results demonstrate that user-consumed visual correlations play actively to capture users’ interests, and the proposed MMR can infer user-image correlations via visual manifold propagation for image recommendation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321002879",
    "keywords": [
      "Artificial intelligence",
      "Collaborative filtering",
      "Computer science",
      "Engineering",
      "Graph",
      "Image (mathematics)",
      "Information overload",
      "Information retrieval",
      "Manifold (fluid mechanics)",
      "Mechanical engineering",
      "Ranking (information retrieval)",
      "Recommender system",
      "The Internet",
      "Theoretical computer science",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Jian",
        "given_name": "Meng"
      },
      {
        "surname": "Guo",
        "given_name": "Jingjing"
      },
      {
        "surname": "Zhang",
        "given_name": "Chenlin"
      },
      {
        "surname": "Jia",
        "given_name": "Ting"
      },
      {
        "surname": "Wu",
        "given_name": "Lifang"
      },
      {
        "surname": "Yang",
        "given_name": "Xun"
      },
      {
        "surname": "Huo",
        "given_name": "Lina"
      }
    ]
  },
  {
    "title": "Deep adaptive group-based input normalization for financial trading",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.11.004",
    "abstract": "Deep Reinforcement Learning (DRL) is among the state-of-the-art approaches for training agents for decision-making problems, such as financial trading. However, training DRL agents for such tasks is not always straightforward, since the noisy and non-stationary nature of financial data aggravate the already unstable training of DRL models. As a result, using DRL methods for such tasks require devoting significant effort for hyper-parameter tuning, as well as for designing the appropriate input pre-processing schemes. The latter is especially important, given the non-stationary nature of financial data, along with the ability of DRL agents to easily overfit the data, limiting their generalization abilities. In this work, we propose overcoming these limitations by introducing a differentiable, parameterized normalization scheme that allows for learning how the data should be normalized, along with the DRL model. More specifically, we propose dynamically normalizing the input according to various time-series statistics, which allows for adapting the model on-the-fly to the current mode of the data. At the same time, employing a segmentation scheme for extracting the statistics of the data allows for better capturing the variations of the input time-series and leading to more stationary representations. The proposed method is formulated as a series of neural layers that can be efficiently implemented using virtually any DL framework. The effectiveness of the proposed method against various normalization approaches is validated using two FOREX datasets and a state-of-the-art policy-based DRL approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521003950",
    "keywords": [
      "Anthropology",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Generalization",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Normalization (sociology)",
      "Overfitting",
      "Reinforcement learning",
      "Sociology",
      "Time series"
    ],
    "authors": [
      {
        "surname": "Nalmpantis",
        "given_name": "Angelos"
      },
      {
        "surname": "Passalis",
        "given_name": "Nikolaos"
      },
      {
        "surname": "Tsantekidis",
        "given_name": "Avraam"
      },
      {
        "surname": "Tefas",
        "given_name": "Anastasios"
      }
    ]
  },
  {
    "title": "Hopfield-type neural ordinary differential equation for robust machine learning",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.10.008",
    "abstract": "Neural networks are vulnerable to adversarial input perturbations imperceptible to human, which calls for robust machine learning for safety-critical applications. In this paper, we propose a new neural ODE layer which is inspired by Hopfield-type neural networks. We prove that the proposed ODE layer has global asymptotic stability on the projected space, which implies the existence and uniqueness of its steady state. We further show that the proposed layer satisfies the local stability condition such that the output is Lipschitz continuous in the ODE layer input, guaranteeing that the norm of perturbation on the hidden state does not grow over time. By experiments we show that an appropriate level of stability constraints imposed on the proposed ODE layer can improve the adversarial robustness of ODE layers, and present a heuristic method for finding good hyperparameters for stability constraints.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521003664",
    "keywords": [
      "Applied mathematics",
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Control (management)",
      "Control theory (sociology)",
      "Differential equation",
      "Exponential stability",
      "Gene",
      "Hyperparameter",
      "Lipschitz continuity",
      "Machine learning",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Nonlinear system",
      "Ode",
      "Ordinary differential equation",
      "Perturbation (astronomy)",
      "Physics",
      "Quantum mechanics",
      "Robustness (evolution)",
      "Stability (learning theory)",
      "Uniqueness"
    ],
    "authors": [
      {
        "surname": "Shin",
        "given_name": "Yu-Hyun"
      },
      {
        "surname": "Baek",
        "given_name": "Seung Jun"
      }
    ]
  },
  {
    "title": "Knowledge-aware deep framework for collaborative skin lesion segmentation and melanoma recognition",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108075",
    "abstract": "Deep learning techniques have shown their superior performance in dermatologist clinical inspection. Nevertheless, melanoma diagnosis is still a challenging task due to the difficulty of incorporating the useful dermatologist clinical knowledge into the learning process. In this paper, we propose a novel knowledge-aware deep framework that incorporates some clinical knowledge into collaborative learning of two important melanoma diagnosis tasks, i.e., skin lesion segmentation and melanoma recognition. Specifically, to exploit the knowledge of morphological expressions of the lesion region and also the periphery region for melanoma identification, a lesion-based pooling and shape extraction (LPSE) scheme is designed, which transfers the structure information obtained from skin lesion segmentation into melanoma recognition. Meanwhile, to pass the skin lesion diagnosis knowledge from melanoma recognition to skin lesion segmentation, an effective diagnosis guided feature fusion (DGFF) strategy is designed. Moreover, we propose a recursive mutual learning mechanism that further promotes the inter-task cooperation, and thus iteratively improves the joint learning capability of the model for both skin lesion segmentation and melanoma recognition. Experimental results on two publicly available skin lesion datasets show the effectiveness of the proposed method for melanoma analysis.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321002624",
    "keywords": [
      "Artificial intelligence",
      "Cancer research",
      "Computer science",
      "Computer security",
      "Deep learning",
      "Dermatology",
      "Exploit",
      "Feature (linguistics)",
      "Lesion",
      "Linguistics",
      "Machine learning",
      "Medicine",
      "Melanoma",
      "Pathology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pooling",
      "Segmentation",
      "Skin lesion"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Xiaohong"
      },
      {
        "surname": "Jiang",
        "given_name": "Xudong"
      },
      {
        "surname": "Ding",
        "given_name": "Henghui"
      },
      {
        "surname": "Zhao",
        "given_name": "Yuqian"
      },
      {
        "surname": "Liu",
        "given_name": "Jun"
      }
    ]
  },
  {
    "title": "A model-based reinforcement learning method based on conditional generative adversarial networks",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.08.019",
    "abstract": "Deep reinforcement learning (DRL) integrates the advantages of the perception of deep learning and enables reinforcement learning scale to problems with high dimensional state and action spaces that were previously intractable. The success of DRL primarily relies on the high level representation ability of deep learning. To obtain a good performed representation model, excessive training samples and training time are necessary. However, collecting a large number of samples in real world is extremely expensive and time consuming. To mitigate the sample inefficiency problem, we propose a novel model-based reinforcement learning method by combining conditional generative adversarial networks (CGAN-MbRL) with the state-of-the-art policy learning method. The proposed CGAN-MbRL can directly deal with the high dimensional state, and mitigate the problem of sample inefficiency to some extent. Finally, the effectiveness of the proposed method is demonstrated through the illustrative data and the RL benchmark.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521003111",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Deep learning",
      "Economics",
      "Generative grammar",
      "Geodesy",
      "Geography",
      "Inefficiency",
      "Law",
      "Machine learning",
      "Microeconomics",
      "Political science",
      "Politics",
      "Reinforcement learning",
      "Representation (politics)",
      "Sample (material)"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Tingting"
      },
      {
        "surname": "Wang",
        "given_name": "Ying"
      },
      {
        "surname": "Li",
        "given_name": "Guixi"
      },
      {
        "surname": "Kong",
        "given_name": "Le"
      },
      {
        "surname": "Chen",
        "given_name": "Yarui"
      },
      {
        "surname": "Wang",
        "given_name": "Yuan"
      },
      {
        "surname": "Xie",
        "given_name": "Ning"
      },
      {
        "surname": "Yang",
        "given_name": "Jucheng"
      }
    ]
  },
  {
    "title": "SnipeDet: Attention-guided pyramidal prediction kernels for generic object detection",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.10.026",
    "abstract": "Using single-scale prediction kernels or Region of Interest (RoI) pooling in the prediction modules of modern object detectors is not very successful in matching different scales of objects. State-of-the-art detectors with the feature pyramid structure built on different resolutions of feature maps can help alleviate this problem. Although with this structure, single-scale prediction kernels or RoI pooling still struggles to detect small objects, and simultaneously, the former continues to encounter the misalignment problem on very large objects. In this paper, we propose the attention-guided pyramidal prediction kernels module with a customized IoU-adaptive loss function to deal with the misalignment problem between the prediction module and different scales of objects. To mitigate the effect of heavy detection head, we also introduce the salient object regions recognition module to identify these regions that have strong object cues. Additionally, interleaved subsampling, as the proposed feature enhancement approach, is used to generate highly discriminative feature representations. We refer to the detection framework constituted by these proposed methods as SnipeDet. Results show that SnipeDet achieves 41.1 AP at the speed of 15.4 FPS on the MS COCO test-dev set with 512 × 512 input images, which outperforms state-of-the-art one-stage detectors and has a better trade-off between speed and accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521003858",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Detector",
      "Discriminative model",
      "Feature (linguistics)",
      "Geometry",
      "Linguistics",
      "Matching (statistics)",
      "Mathematics",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Pooling",
      "Programming language",
      "Pyramid (geometry)",
      "Quantum mechanics",
      "Scale (ratio)",
      "Set (abstract data type)",
      "Statistics",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Suting"
      },
      {
        "surname": "Cheng",
        "given_name": "Zehua"
      },
      {
        "surname": "Zhang",
        "given_name": "Liangchen"
      },
      {
        "surname": "Zheng",
        "given_name": "Yujie"
      }
    ]
  },
  {
    "title": "Multi-label feature selection considering label supplementation",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108137",
    "abstract": "Multi-label feature selection is an efficient technique to alleviate the high dimensionality for multi-label learning. Existing multi-label feature selection methods based on information theory either deal with labels individually or treat all label relationships as redundancy. However, two important and being ignored issues are the different effects of label relationships and the dynamic changes of label relationships in measuring different candidate features. To address these issues, we first distinguish three types of label relationships: label independence, label redundancy and label supplementation. Second, we consider the changes of label relationships based on different features. By analyzing the differences and the changes of label relationships, two new methods named LSMFS and MLSMFS are proposed, which extracts all supplementary information and the maximum supplementary information of features for each label from other labels, respectively. Finally, experiments on fifteen benchmark multi-label data sets demonstrate the effectiveness of the proposed methods against nine other methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003241",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Curse of dimensionality",
      "Data mining",
      "Feature (linguistics)",
      "Feature selection",
      "Geodesy",
      "Geography",
      "Linguistics",
      "Machine learning",
      "Minimum redundancy feature selection",
      "Multi-label classification",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Redundancy (engineering)",
      "Selection (genetic algorithm)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Ping"
      },
      {
        "surname": "Liu",
        "given_name": "Guixia"
      },
      {
        "surname": "Gao",
        "given_name": "Wanfu"
      },
      {
        "surname": "Song",
        "given_name": "Jiazhi"
      }
    ]
  },
  {
    "title": "A survey on dorsal hand vein biometrics",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108122",
    "abstract": "Biometrics technology is one of the most important and effective solutions for personal authentication. In recent years, as one of the emerging biometrics technologies, dorsal hand vein (DHV) biometrics has received a lot of attention. In fact, DHV biometrics has been studied for more than 30 years, during which different problems related to DHV recognition have been addressed. In this paper, we conduct a comprehensive survey on the state-of-the-art in DHV biometrics. Nearly all important aspects of DHV biometrics have been summarized, including the developmental history, data acquisition, databases, preprocessing algorithms, feature extraction and matching algorithms, information fusion schemes and commercial products. We also discuss the challenges and future directions in DHV biometrics research.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003095",
    "keywords": [
      "Anatomy",
      "Artificial intelligence",
      "Biometrics",
      "Computer science",
      "Computer vision",
      "Dorsum",
      "Medicine"
    ],
    "authors": [
      {
        "surname": "Jia",
        "given_name": "Wei"
      },
      {
        "surname": "Xia",
        "given_name": "Wei"
      },
      {
        "surname": "Zhang",
        "given_name": "Bob"
      },
      {
        "surname": "Zhao",
        "given_name": "Yang"
      },
      {
        "surname": "Fei",
        "given_name": "Lunke"
      },
      {
        "surname": "Kang",
        "given_name": "Wenxiong"
      },
      {
        "surname": "Huang",
        "given_name": "Di"
      },
      {
        "surname": "Guo",
        "given_name": "Guodong"
      }
    ]
  },
  {
    "title": "Fast estimation for robust supervised classification with mixture models",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.10.020",
    "abstract": "Label noise is known to negatively impact the performance of classification algorithms. In this paper, we develop a model robust to label noise that uses both labelled and unlabelled samples. In particular, we propose a novel algorithm to optimize the model parameters that scales efficiently w.r.t. the number of training samples. Our contribution relies on a consensus formulation of the original objective function that is highly parallelizable. The optimization is performed with the Alternating Direction Method of Multipliers framework. Experimental results on synthetic datasets show an improvement of several orders of magnitude in terms of processing time, with no loss in terms of accuracy. Our method appears also tailored to handle real data with significant label noise.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521003792",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Evolutionary biology",
      "Function (biology)",
      "Image (mathematics)",
      "Machine learning",
      "Noise (video)",
      "Parallelizable manifold",
      "Pattern recognition (psychology)",
      "Synthetic data"
    ],
    "authors": [
      {
        "surname": "Giry Fouquet",
        "given_name": "Erwan"
      },
      {
        "surname": "Fauvel",
        "given_name": "Mathieu"
      },
      {
        "surname": "Mallet",
        "given_name": "Clément"
      }
    ]
  },
  {
    "title": "A maximum diversity-based path sparsification for geometric graph matching",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.09.019",
    "abstract": "This paper presents an effective dissimilarity measure for geometric graphs representing shapes. The proposed dissimilarity measure is a distance that combines a sparsification of the geometric graph based on the maximum diversity problem and a new node embedding that captures the topological neighborhood of nodes. The sparsification step aims to reduce the size of the graph and to correct the misdistribution of nodes on the geometric graph induced by the noise of image handling. Experimental evaluation shows that the sparsification algorithm retains the form of the shapes while decreasing the number of processed nodes which reduces the overall matching time. Furthermore, the proposed node embedding and similarity measure give better performance in comparison with existing graph matching approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521003457",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Data mining",
      "Embedding",
      "Engineering",
      "Factor-critical graph",
      "Graph",
      "Graph embedding",
      "Line graph",
      "Matching (statistics)",
      "Mathematics",
      "Measure (data warehouse)",
      "Node (physics)",
      "Pattern recognition (psychology)",
      "Random geometric graph",
      "Similarity measure",
      "Statistics",
      "Structural engineering",
      "Voltage graph"
    ],
    "authors": [
      {
        "surname": "Kiouche",
        "given_name": "Abd Errahmane"
      },
      {
        "surname": "Seba",
        "given_name": "Hamida"
      },
      {
        "surname": "Amrouche",
        "given_name": "Karima"
      }
    ]
  },
  {
    "title": "Deep learning for emotion driven user experiences",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.09.004",
    "abstract": "Deep knowledge about user characteristics and behaviors opens new and promising landscapes to the User Experience. Thanks to emerging machine learning techniques, a new form of communication channel among users and applications may be exploited for customizing and finely tuning the dynamic behavior of applications to the peculiarities of their users. This work investigates the empathic improvement of the User Experiences and exploits inferences on user expressions for activating gaming and entertainment events, that are adopted in a cinematic way to create dynamic application behaviors. The presented approach is applied to a third/first-person horror adventure and a classic table game. As already verified in a preliminary phase of the research, user impressions, collected as subjective evaluation in a controlled experiment, are positive and encourage further steps, like the evaluation of other inferences on users.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521003159",
    "keywords": [
      "Adventure",
      "Art",
      "Artificial intelligence",
      "Channel (broadcasting)",
      "Computer network",
      "Computer science",
      "Computer security",
      "Data mining",
      "Deep learning",
      "Entertainment",
      "Exploit",
      "Human–computer interaction",
      "Multimedia",
      "Table (database)",
      "User experience design",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Bisogni",
        "given_name": "Carmen"
      },
      {
        "surname": "Cascone",
        "given_name": "Lucia"
      },
      {
        "surname": "Castiglione",
        "given_name": "Aniello"
      },
      {
        "surname": "Passero",
        "given_name": "Ignazio"
      }
    ]
  },
  {
    "title": "CapsNet regularization and its conjugation with ResNet for signature identification",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107851",
    "abstract": "We propose a new regularization term for CapsNet that significantly improves the generalization power of the original method from small training data while requiring much fewer parameters, making it suitable for large input images. We also propose a very efficient DNN architecture that integrates CapsNet with ResNet to obtain the advantages of the two architectures. CapsNet allows a powerful understanding of the objects’ components and their positions, while ResNet provides efficient feature extraction and description. Our approach is general, and we demonstrate it on the problem of signature identification from images. To show our approach superiority, we provide several evaluations with different protocols. We also show that our approach outperforms the state-of-the-art on this problem with thorough experiments on three publicly available datasets CEDAR, MCYT, and UTSig.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321000388",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Botany",
      "Computer science",
      "Geometry",
      "Identification (biology)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Regularization (linguistics)",
      "Residual neural network",
      "Signature (topology)"
    ],
    "authors": [
      {
        "surname": "Jampour",
        "given_name": "Mahdi"
      },
      {
        "surname": "Abbaasi",
        "given_name": "Saeid"
      },
      {
        "surname": "Javidi",
        "given_name": "Malihe"
      }
    ]
  },
  {
    "title": "A unified weight learning and low-rank regression model for robust complex error modeling",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108147",
    "abstract": "One of the most important problems in regression-based error model is modeling the complex representation error caused by various corruptions and environment changes in images. For example, in robust face recognition, images are often affected by varying types and levels of corruptions, such as random pixel corruptions, block occlusions, or disguises. However, existing works are not robust enough to solve this problem due to they cannot model the complex corrupted errors very well. In this paper, we address this problem by a unified sparse weight learning and low-rank approximation regression model, which enables the random noises and contiguous occlusions in images to be treated simultaneously. For the random noise, we define a generalized correntropy (GC) function to match the error distribution. For the structured error caused by occlusions or disguises, we propose a GC function based rank approximation to measure the rank of error matrices. Since the proposed objective function is non-convex, an effective iterative optimization algorithm is developed to achieve the optimal weight learning and low-rank approximation. Extensive experimental results on three public face databases show that the proposed model can fit the error distribution and structure very well, thus obtain better recognition accuracies in comparison with the existing methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003344",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Combinatorics",
      "Computer science",
      "Evolutionary biology",
      "Function (biology)",
      "Law",
      "Mathematical optimization",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Rank (graph theory)",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Miaohua"
      },
      {
        "surname": "Gao",
        "given_name": "Yongsheng"
      },
      {
        "surname": "Zhou",
        "given_name": "Jun"
      }
    ]
  },
  {
    "title": "A two-level framework for place recognition with 3D LiDAR based on spatial relation graph",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108171",
    "abstract": "In the field of robotics, due to the complexity of real environments, place recognition using the 3D LiDAR is always a challenging problem. The spatial relations of internal structures underlying the LiDAR data from different places are distinguishable, which can be used to describe the environment. In this paper, we utilize the spatial relations of internal structures and propose a two-level framework for 3D LiDAR place recognition based on the spatial relation graph (SRG). At first, the proposed framework segments the point cloud into multiple clusters, then the features of the clusters and the spatial relation descriptors (SRDs) between the clusters are extracted, and the point cloud is represented by the SRG, which uses the clusters as the nodes and their spatial relations as the edges. After that, we propose a two-level matching model in which two different models are fused for accurately and efficiently matching the SRGs, including the upper-level searching model (U-LSM) and lower-level matching model (L-LMM). In the U-LSM, an incremental bag-of-words model is used to search for candidate SRGs through the distribution of the SRDs in the SRG. In the L-LMM, we utilize the improved spectral method to calculate similarities between the current SRG and the candidates. The experimental results demonstrate that our framework achieves good precision, recall and viewpoint robustness on both public benchmarks and self-built campus dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003587",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Gene",
      "Geography",
      "Graph",
      "Lidar",
      "Matching (statistics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Point cloud",
      "Precision and recall",
      "Relation (database)",
      "Remote sensing",
      "Robustness (evolution)",
      "Spatial analysis",
      "Spatial relation",
      "Statistics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Gong",
        "given_name": "Yansong"
      },
      {
        "surname": "Sun",
        "given_name": "Fengchi"
      },
      {
        "surname": "Yuan",
        "given_name": "Jing"
      },
      {
        "surname": "Zhu",
        "given_name": "Wenbin"
      },
      {
        "surname": "Sun",
        "given_name": "Qinxuan"
      }
    ]
  },
  {
    "title": "Deep transfer learning based classification model for covid-19 using chest CT-scans",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.08.035",
    "abstract": "COVID-19 is an infectious and contagious virus. As of this writing, more than 160 million people have been infected since its emergence, including more than 125,000 in Algeria. In this work, We first collected a dataset of 4986 COVID and non-COVID images confirmed by RT-PCR tests at Tlemcen hospital in Algeria. Then we performed a transfer learning on deep learning models that got the best results on the ImageNet dataset, such as DenseNet121, DenseNet201, VGG16, VGG19, Inception Resnet-V2, and Xception, in order to conduct a comparative study. Therefore, We have proposed an explainable model based on the DenseNet201 architecture and the GradCam explanation algorithm to detect COVID-19 in chest CT images and explain the output decision. Experiments have shown promising results and proven that the introduced model can be beneficial for diagnosing and following up patients with COVID-19.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521003366",
    "keywords": [
      "2019-20 coronavirus outbreak",
      "Artificial intelligence",
      "Computer science",
      "Coronavirus disease 2019 (COVID-19)",
      "Deep learning",
      "Disease",
      "Infectious disease (medical specialty)",
      "Machine learning",
      "Medicine",
      "Outbreak",
      "Pathology",
      "Pattern recognition (psychology)",
      "Residual neural network",
      "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)",
      "Transfer of learning",
      "Virology"
    ],
    "authors": [
      {
        "surname": "LAHSAINI",
        "given_name": "Ilyas"
      },
      {
        "surname": "EL HABIB DAHO",
        "given_name": "Mostafa"
      },
      {
        "surname": "CHIKH",
        "given_name": "Mohamed Amine"
      }
    ]
  },
  {
    "title": "Meta-learning based relation and representation learning networks for single-image deraining",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108124",
    "abstract": "Single-image deraining is a kind of computer vision task that aims to restore the image that be degraded by rain streaks, which motivates existing methods to either directly translate the rainy image to its clean one, or indirectly learn the rain residual based on the prior information. However, both methodologies harm the generalization ability due to the limited diversity of the training samples, comparing with the endless varieties of the real-world rainy images. Such fact inspires us to take the merit of meta-learning and propose a meta-learning based representation learning network to learn the transferable embeddings of the rainy/clean images, while their discrepancies are characterized by the relation vector, which is generated by the subsequent meta-learning based relation learning network. These networks are leveraged into the meta-learning based deraining network (MLDN) to enhance the generalization ability by removing the latent relation vector from the transferable embedding of the rainy image and generate high-quality deraining result. Superior performance is achieved by MLDN, which has averaged 4 % better than the state-of-the-arts.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003113",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Embedding",
      "Engineering",
      "Feature learning",
      "Generalization",
      "Image (mathematics)",
      "Law",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Meta learning (computer science)",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Relation (database)",
      "Representation (politics)",
      "Systems engineering",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Gao",
        "given_name": "Xinjian"
      },
      {
        "surname": "Wang",
        "given_name": "Yang"
      },
      {
        "surname": "Cheng",
        "given_name": "Jun"
      },
      {
        "surname": "Xu",
        "given_name": "Mingliang"
      },
      {
        "surname": "Wang",
        "given_name": "Meng"
      }
    ]
  },
  {
    "title": "Part-guided graph convolution networks for person re-identification",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108155",
    "abstract": "Recently, part-based deep models have achieved promising performance in person re-identification (Re-ID), yet these models ignore the inter-local relationship of the corresponding parts among pedestrian images and the intra-local relationship between adjacent parts in one pedestrian image. As a result, the feature representations are hard to learn the information from the same parts of other pedestrian images and are lack of the contextual information of pedestrian. In this paper, we propose a novel deep graph model named Part-Guided Graph Convolution Network (PGCN) for person Re-ID, which could simultaneously learn the inter-local relationship and the intra-local relationship for feature representations. Specifically, we construct the inter-local graph using the local features extracted from the same parts of pedestrian images and build the adjacency matrix using the similarity so as to mine the inter-local relationship. Meanwhile, we construct the intra-local graph using the local features extracted from different body parts in one pedestrian image, and propose the fractional dynamic mechanism (FDM) to accurately describe the correlations between adjacent parts in the optimization process. Finally, after the graph convolutional operation, the inter-local relationship and the intra-local relationship are injected into the feature representations of pedestrian images. Extensive experiments are conducted on Market-1501, CUHK03, DukeMTMC-reID and MSMT17, and the experimental results show the proposed PGCN exceeds state-of-the-art methods by an overwhelming margin.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003423",
    "keywords": [
      "Adjacency list",
      "Adjacency matrix",
      "Algorithm",
      "Artificial intelligence",
      "Chemical physics",
      "Computer science",
      "Engineering",
      "Feature (linguistics)",
      "Graph",
      "Image (mathematics)",
      "Linguistics",
      "Local structure",
      "Pattern recognition (psychology)",
      "Pedestrian",
      "Philosophy",
      "Physics",
      "Similarity (geometry)",
      "Theoretical computer science",
      "Transport engineering"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Zhong"
      },
      {
        "surname": "Zhang",
        "given_name": "Haijia"
      },
      {
        "surname": "Liu",
        "given_name": "Shuang"
      },
      {
        "surname": "Xie",
        "given_name": "Yuan"
      },
      {
        "surname": "Durrani",
        "given_name": "Tariq S."
      }
    ]
  },
  {
    "title": "A multi-task fully deep convolutional neural network for contactless fingerprint minutiae extraction",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108189",
    "abstract": "With the outbreak and wide spread of novel coronavirus (COVID-19), contactless fingerprint recognition has attracted more attention for personal recognition because it can provide significantly higher user convenience and hygiene than the traditional contact-based fingerprint recognition. However, it is still challenging to achieve a highly accurate recognition due to the low ridge-valley contrast and pose variances of contactless fingerprints. Minutiae points are a kind of ridge flow discontinuities, and robust and accurate extraction is an important step for most automatic fingerprint recognition algorithms. Most of existing methods are based on two stages which locate the minutiae points first and then compute their directions. The two-stage method cannot make full use of location and direction information. In this paper, we propose a multi-task fully deep convolutional neural network for jointly learning the minutiae location detection and its corresponding direction computation which operates directly on the whole gray scale contactless fingerprints. The proposed method consists of offline training and online testing stages. In the training stage, a fully deep convolutional neural network is built for the tasks of minutiae detection and its direction regression, with an attention mechanism to make the direction regression branch concentrate on the minutiae points. A new loss function is proposed to jointly learn the tasks of minutiae detection and its direction regression from the whole fingerprints. In the testing stage, the trained network is applied on the whole contactless fingerprint to generate the minutiae location and direction maps. The proposed multi-task leaning method performs better than the individual single task and it operates directly on the raw gray-scale contactless fingerprints without preprocessing. The results on three contactless fingerprint datasets show the proposed algorithm performs better than other minutiae extraction algorithms and the commercial software.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003526",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Feature extraction",
      "Fingerprint (computing)",
      "Fingerprint recognition",
      "Minutiae",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Zhao"
      },
      {
        "surname": "Liu",
        "given_name": "Shuxin"
      },
      {
        "surname": "Liu",
        "given_name": "Manhua"
      }
    ]
  },
  {
    "title": "Two-Stage Deep Learning Framework for Discrimination between COVID-19 and Community-Acquired Pneumonia from Chest CT scans",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.10.027",
    "abstract": "COVID-19 stay threatening the health infrastructure worldwide. Computed tomography (CT) was demonstrated as an informative tool for the recognition, quantification, and diagnosis of this kind of disease. It is urgent to design efficient deep learning (DL) approach to automatically localize and discriminate COVID-19 from other comparable pneumonia on lung CT scans. Thus, this study introduces a novel two-stage DL framework for discriminating COVID-19 from community-acquired pneumonia (CAP) depending on the detected infection region within CT slices. Firstly, a novel U-shaped network is presented to segment the lung area where the infection appears. Then, the concept of transfer learning is applied to the feature extraction network to empower the network capabilities in learning the disease patterns. After that, multi-scale information is captured and pooled via an attention mechanism for powerful classification performance. Thirdly, we propose an infection prediction module that use the infection location to guide the classification decision and hence provides interpretable classification decision. Finally, the proposed model was evaluated on public datasets and achieved great segmentation and classification performance outperforming the cutting-edge studies.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521003871",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Community-acquired pneumonia",
      "Computed tomography",
      "Computer science",
      "Coronavirus disease 2019 (COVID-19)",
      "Deep learning",
      "Disease",
      "Feature (linguistics)",
      "Feature extraction",
      "Infectious disease (medical specialty)",
      "Internal medicine",
      "Linguistics",
      "Machine learning",
      "Medicine",
      "Paleontology",
      "Pathology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pneumonia",
      "Radiology",
      "Segmentation",
      "Stage (stratigraphy)",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Abdel-Basset",
        "given_name": "Mohamed"
      },
      {
        "surname": "Hawash",
        "given_name": "Hossam"
      },
      {
        "surname": "Moustafa",
        "given_name": "Nour"
      },
      {
        "surname": "Elkomy",
        "given_name": "Osama M."
      }
    ]
  },
  {
    "title": "Hierarchical Object Relationship Constrained Monocular Depth Estimation.",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108116",
    "abstract": "Monocular depth estimation has been gaining growing momentum in recent years. Despite significant advances of this task, due to the inherent difficulty of reliably capturing contextual cues from RGB images, it remains challenging to accurately predict depth in scenes with complicated and cluttered spatial arrangement of objects. Instead of naively utilizing the primary features in the single RGB image, in this paper we propose a hierarchical object relationship constrained network for monocular depth estimation, which could enable accurate and smooth depth prediction from monocular RGB image. The key idea of our method is to exploit object-centric hierarchical relationship as contextual constraints to compensate for the regularity of spatial depth changing. In particular, we design a semantics-guided CNN network to encode the original image into a global context feature map and encode the objects’ relationship into a local relationship feature map simultaneously, so that we can leverage such effective and consolidated coding scheme over scenario samples to guide the depth prediction in a more accurate way. Benefiting from the local-to-global context constraints, our method can well respect the global depth changing and preserve the local depth details at the same time. In addition, our approach could make full use of the hierarchical semantic relationship across inner-object components and neighboring objects to define depth changing constraints. We conduct extensive experiments and make comprehensive evaluations on widely-used public datasets, and the experiments confirm that our method outperforms most state-of-the-art depth estimation methods in preserving the local details in depth.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003034",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Biochemistry",
      "Biology",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Depth map",
      "Depth perception",
      "ENCODE",
      "Feature (linguistics)",
      "Gene",
      "Geography",
      "Image (mathematics)",
      "Leverage (statistics)",
      "Linguistics",
      "Monocular",
      "Neuroscience",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Perception",
      "Philosophy",
      "RGB color model",
      "Spatial contextual awareness"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Shuai"
      },
      {
        "surname": "Shi",
        "given_name": "Jiaying"
      },
      {
        "surname": "Song",
        "given_name": "Wenfeng"
      },
      {
        "surname": "Hao",
        "given_name": "Aimin"
      },
      {
        "surname": "Qin",
        "given_name": "Hong"
      }
    ]
  },
  {
    "title": "Dynamic and reliable subtask tracker with general schatten p -norm regularization",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108129",
    "abstract": "Some multi-task trackers adopt an inaccurate shrink strategy to treat different rank components equally. Thus, their flexibility is vulnerable to some tracking challenges. To resolve this problem, we propose a spatial-aware reliable multi-subtask tracker via weighted Schatten p -norm regularization (SLRT-W), which dynamically chooses the suitable and reliable subset of the whole subtasks for tracking. Its major merits not only assign the flexible weights to different subtask rank components depending on their tracking contribution, but also preserve consistent spatial layout structure and correspondence of layered multi-subtask. Specifically, multiple layered subtasks correspond to different target subregions, they are cooperative and complement. A weighted Schatten p -norm is introduced to adaptively shrink different multi-subtask rank components, and emphasize important components as reliable ones. Then, a structured hyper-graph regularized term simultaneously exploits the intrinsic geometry correspondence among multiple layers of subtasks, and spatial layout structure inside each layer. We devise an alternatively generalized iterated shrinkage method to optimize the multi-subtask Schatten p -norm minimization. Finally, a robust decision-evaluation strategy is developed to choose the reliable multi-subtask tracking combination. Encouraging results on some challenging benchmarks demonstrate the proposed tracker performs favorably in robustness and accuracy, against some state-of-the-art trackers.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003162",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "BitTorrent tracker",
      "Chemistry",
      "Combinatorics",
      "Computer science",
      "Eye tracking",
      "Gene",
      "Iterated function",
      "Law",
      "Mathematical analysis",
      "Mathematics",
      "Minification",
      "Norm (philosophy)",
      "Political science",
      "Programming language",
      "Rank (graph theory)",
      "Regularization (linguistics)",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Fan",
        "given_name": "Baojie"
      },
      {
        "surname": "Cong",
        "given_name": "Yang"
      },
      {
        "surname": "Tian",
        "given_name": "Jiandong"
      },
      {
        "surname": "Tang",
        "given_name": "Yandong"
      }
    ]
  },
  {
    "title": "Sparse semi-supervised heterogeneous interbattery bayesian analysis",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108141",
    "abstract": "The Bayesian approach to feature extraction, known as factor analysis (FA), has been widely studied in machine learning to obtain a latent representation of the data. An adequate selection of the probabilities and priors of these bayesian models allows the model to better adapt to the data nature (i.e. heterogeneity, sparsity), obtaining a more representative latent space. The objective of this article is to propose a general FA framework capable of modelling any problem. To do so, we start from the Bayesian Inter-Battery Factor Analysis (BIBFA) model, enhancing it with new functionalities to be able to work with heterogeneous data, to include feature selection, and to handle missing values as well as semi-supervised problems. The performance of the proposed model, Sparse Semi-supervised Heterogeneous Interbattery Bayesian Analysis (SSHIBA), has been tested on different scenarios to evaluate each one of its novelties, showing not only a great versatility and an interpretability gain, but also outperforming most of the state-of-the-art algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003289",
    "keywords": [],
    "authors": [
      {
        "surname": "Sevilla-Salcedo",
        "given_name": "Carlos"
      },
      {
        "surname": "Gómez-Verdejo",
        "given_name": "Vanessa"
      },
      {
        "surname": "Olmos",
        "given_name": "Pablo M."
      }
    ]
  },
  {
    "title": "Mixability of integral losses: A key to efficient online aggregation of functional and probabilistic forecasts",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108175",
    "abstract": "In this paper we extend the setting of the online prediction with expert advice to function-valued forecasts. At each step of the online game several experts predict a function, and the learner has to efficiently aggregate these functional forecasts into a single forecast. We adapt basic mixable (and exponentially concave) loss functions to compare functional predictions and prove that these adaptations are also mixable (exp-concave). We call this phenomenon mixability (exp-concavity) of integral loss functions. As an application of our main result, we prove that various loss functions used for probabilistic forecasting are mixable (exp-concave). The considered losses include Sliced Continuous Ranked Probability Score, Energy-Based Distance, Optimal Transport Costs & Sliced Wasserstein-2 distance, Beta-2 & Kullback-Leibler divergences, Characteristic function and Maximum Mean Discrepancies.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003629",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Key (lock)",
      "Probabilistic logic"
    ],
    "authors": [
      {
        "surname": "Korotin",
        "given_name": "Alexander"
      },
      {
        "surname": "V’yugin",
        "given_name": "Vladimir"
      },
      {
        "surname": "Burnaev",
        "given_name": "Evgeny"
      }
    ]
  },
  {
    "title": "Knowledge base graph embedding module design for Visual question answering model",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108153",
    "abstract": "In this paper, a knowledge base graph embedding module is constructed to extend the versatility of knowledge-based VQA (Visual Question Answering) models. The knowledge base graph embedding module constructed in this paper extracts core entities from images and text, and maps them as knowledge base entities, then extracts the sub-graphs closely related to the core entities, and converts the sub-graphs into low-dimensional vectors to realize sub-graph embedding. In order to achieve good subgraph embedding, we first extracted two experimental knowledge bases with rich semantics from DBpedia: DBV and DBA. Based on these two knowledge bases, this paper selects several excellent models in knowledge base embedding as test models, including SE (structured embedding),SME(semantic matching energy function), and TransE model to produce link prediction. The results show that there is a clear correspondence between the entities of the DBV, which can achieve excellent node embedding. And the TransE model can achieve a good knowledge base embedding, so we built the knowledge base graph embedding module based on TransE. And then we construct a VQA model (KBSN) based on the knowledge base graph embedding. Experimental results on VQA2.0 and KB-VQA data sets prove that the knowledge base graph embedding module improves the accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032100340X",
    "keywords": [
      "Artificial intelligence",
      "Base (topology)",
      "Computer science",
      "Embedding",
      "Graph",
      "Graph embedding",
      "Knowledge base",
      "Knowledge graph",
      "Mathematical analysis",
      "Mathematics",
      "Question answering",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Zheng",
        "given_name": "Wenfeng"
      },
      {
        "surname": "Yin",
        "given_name": "Lirong"
      },
      {
        "surname": "Chen",
        "given_name": "Xiaobing"
      },
      {
        "surname": "Ma",
        "given_name": "Zhiyang"
      },
      {
        "surname": "Liu",
        "given_name": "Shan"
      },
      {
        "surname": "Yang",
        "given_name": "Bo"
      }
    ]
  },
  {
    "title": "Randomized trees for time series representation and similarity",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108097",
    "abstract": "Most of the temporal data mining tasks require representations to capture important characteristics of time series. Representation learning is challenging when time series differ in distributional characteristics and/or show irregularities such as varying lengths and missing observations. Moreover, when time series are multivariate, interactions between variables should be modeled efficiently. This study proposes a unified, flexible time series representation learning framework for both univariate and multivariate time series called Rand-TS. Rand-TS models density characteristics of each time series as a time-varying Gaussian distribution using random decision trees and embeds density information into a sparse vector. Rand-TS can work with time series of various lengths and missing observations, furthermore, it allows using customized features. We illustrate the classification performance of Rand-TS on 113 univariate, 19 multivariate along with 15 univariate time series with varying lengths from UCR database. The results show that in addition to its flexibility, Rand-TS provides competitive classification performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321002843",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Data mining",
      "Image (mathematics)",
      "Law",
      "Mathematics",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Series (stratigraphy)",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Görgülü",
        "given_name": "Berk"
      },
      {
        "surname": "Baydoğan",
        "given_name": "Mustafa Gökçe"
      }
    ]
  },
  {
    "title": "Prediction of pediatric activity intensity with wearable sensors and bi-directional LSTM models",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.08.030",
    "abstract": "Assessing activity intensity has its clinical importance to the treatment of diseases such as obesity. The metabolic equivalent of task (MET) is the objective numerical measure for assessing the intensity of general activities. Because daily activities vary, an activity cannot be easily mapped to a MET value, which makes intensity quantification of daily activities more challenging than monotonous activities. In this article, we use the data from wearable inertial measurement unit (IMU) sensors and a calorimetry machine to map the relationship between activity motions and intensities. In detail, we describe an end-to-end approach for predicting METs of preschoolers. Based on the collection of data from the two devices, we present a systematic approach to address the aforementioned challenges and to predict physical activity intensity of preschoolers. Specifically, a dynamic synchronization method is first proposed to deal with the displaced data series, which takes the dynamic time warping (DTW) as an evaluation criterion. Second, additional features are designed to reinforce the ability of intensity prediction. Third, proposed methods are tested on a two-layer bidirectional long short term memory (LSTM) network model to predict MET values. Our experimental results reveal the effectiveness of the end-to-end approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521003342",
    "keywords": [
      "Activity recognition",
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Dynamic time warping",
      "Embedded system",
      "Inertial measurement unit",
      "Intensity (physics)",
      "Machine learning",
      "Measure (data warehouse)",
      "Physics",
      "Quantum mechanics",
      "Units of measurement",
      "Wearable computer"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Li"
      },
      {
        "surname": "Qu",
        "given_name": "Xiao"
      },
      {
        "surname": "Zhang",
        "given_name": "Ting"
      },
      {
        "surname": "Wu",
        "given_name": "Jianxin"
      },
      {
        "surname": "Yin",
        "given_name": "Hao"
      },
      {
        "surname": "Guan",
        "given_name": "Hongyan"
      },
      {
        "surname": "Luo",
        "given_name": "Yan"
      }
    ]
  },
  {
    "title": "Potential Anchoring for imbalanced data classification",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108114",
    "abstract": "Data imbalance remains one of the factors negatively affecting the performance of contemporary machine learning algorithms. One of the most common approaches to reducing the negative impact of data imbalance is preprocessing the original dataset with data-level strategies. In this paper we propose a unified framework for imbalanced data over- and undersampling. The proposed approach utilizes radial basis functions to preserve the original shape of the underlying class distributions during the resampling process. This is done by optimizing the positions of generated synthetic observations with respect to the proposed potential resemblance loss. The final Potential Anchoring algorithm combines over- and undersampling within the proposed framework. The results of the experiments conducted on 60 imbalanced datasets show outperformance of Potential Anchoring over state-of-the-art resampling algorithms, including previously proposed methods that utilize radial basis functions to model class potential. Furthermore, the results of the analysis based on the proposed data complexity index show that Potential Anchoring is particularly well suited for handling naturally complex (i.e. not affected by the presence of noise) datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003010",
    "keywords": [
      "Anchoring",
      "Artificial intelligence",
      "Basis (linear algebra)",
      "Class (philosophy)",
      "Computer science",
      "Data mining",
      "Engineering",
      "Geometry",
      "Machine learning",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Preprocessor",
      "Process (computing)",
      "Resampling",
      "Structural engineering",
      "Undersampling"
    ],
    "authors": [
      {
        "surname": "Koziarski",
        "given_name": "Michał"
      }
    ]
  },
  {
    "title": "Temporal consistent portrait video segmentation",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108143",
    "abstract": "We explore a new video segmentation task, named portrait video segmentation (PVS), which aims to automatically segment the dominant person throughout a given portrait video. To achieve accurate and temporal-coherent segmentation results, a feature reconstruction based PVS method is developed under the meta-learning framework. Due to the dramatic pose variation and severe occlusion in portrait videos, feature reconstruction using existing optical flow models usually suffers from severe ghosting effects in reconstructed features. We mitigate this issue by presenting a soft correspondence network (SCN), which learns to facilitate feature reconstruction in an unsupervised fashion by softly assigning each pixel displacement probabilities between portrait frames. Based on the proposed SCN, a novel portrait segmentation network (PSN) is further designed, which explores the reconstructed features through feature aggregation blocks (FABs), yielding more reliable segmentation results. To capture temporal and target-specific cues, the parameters of FABs are determined by a meta-updater network which is trained offline in the meta-level. In addition, we introduce a new PVS dataset with high-quality segmentation annotations. Experimental results clearly demonstrate the effectiveness of the proposed PVS method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003307",
    "keywords": [
      "Art",
      "Art history",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Pattern recognition (psychology)",
      "Portrait",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Yifan"
      },
      {
        "surname": "Zhang",
        "given_name": "Wenbo"
      },
      {
        "surname": "Wang",
        "given_name": "Lijun"
      },
      {
        "surname": "Yang",
        "given_name": "Fenghua"
      },
      {
        "surname": "Lu",
        "given_name": "Huchuan"
      }
    ]
  },
  {
    "title": "On the receptive field misalignment in CAM-based visual explanations",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.10.024",
    "abstract": "Visual explanations aim at providing an understanding of the inner behavior of convolutional neural networks. Naturally, it is necessary to explore whether these methods themselves are reasonable and reliable. In this paper, we focus on Class Activation Mapping (CAM), a type of attractive explanations that has been widely applied to model diagnosis and weakly supervised tasks. Our contribution is two-fold. First, we identify an important but neglected issue that affects the reliability of CAM results: there is a misalignment between the effective receptive field and the implicit receptive field, where the former is determined by the model and the input, and the latter is determined by the upsampling function in CAM. Occlusion experiments are designed to empirically testify to its existence. Second, based on this finding, an adversarial marginal attack is proposed to fool the CAM-based method and the CNN model simultaneously. Experimental results demonstrate that the provided saliency map can be completely changed to another shape by only perturbing the area with 1-pixel width. The prototype code of the method is now available at https://github.com/xpf/CAM-Adversarial-Marginal-Attack.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521003810",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Biology",
      "Code (set theory)",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Evolutionary biology",
      "Field (mathematics)",
      "Focus (optics)",
      "Function (biology)",
      "Image (mathematics)",
      "Mathematics",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Power (physics)",
      "Programming language",
      "Pure mathematics",
      "Quantum mechanics",
      "Receptive field",
      "Reliability (semiconductor)",
      "Set (abstract data type)",
      "Upsampling"
    ],
    "authors": [
      {
        "surname": "Xia",
        "given_name": "Pengfei"
      },
      {
        "surname": "Niu",
        "given_name": "Hongjing"
      },
      {
        "surname": "Li",
        "given_name": "Ziqiang"
      },
      {
        "surname": "Li",
        "given_name": "Bin"
      }
    ]
  },
  {
    "title": "Statistical mechanical analysis for unweighted and weighted stock market networks",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108123",
    "abstract": "Financial markets are time-evolving complex systems containing different financial entities, such as banks, corporations and institutions that interact through transactions and respond to external economic and political events. They can be conveniently represented as a network structure. In this paper, we analyse the unweighted and weighted market networks from a statistical mechanical perspective. In particular, we propose a novel thermodynamic analogy to characterise the dynamic structural properties of time-evolving networks. The intricate pattern of edge connections in the network is modelled by using a heat bath analogy in which particles occupy the energy states according to the Boltzmann distribution. According to this analogy the occupation of the energy states is determined by the temperature of the heat bath, and the spectrum of energy states of the network is determined by the number of nodes and edges. For unweighted networks, the binary representation of the elements in the adjacency matrix can be modelled as a statistical ensemble, using the corresponding partition function to compute thermodynamic network characterisations. For weighted networks, on the other hand, the derived thermodynamic quantities together with their distribution of fluctuations identify the salient structure in the network evolution. We conduct experiments on time-evolving stock exchanges using data for the S&P500 Index Stock Exchanges over the past decade. The thermodynamic characterisations provide an excellent framework to identify epochs in which there is significant variance in network structure during financial crises induced by economic and political events.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003101",
    "keywords": [
      "Adjacency matrix",
      "Analogy",
      "Complex network",
      "Computer science",
      "Econometrics",
      "Economics",
      "Econophysics",
      "Financial crisis",
      "Financial networks",
      "Geology",
      "Graph",
      "Horse",
      "Linguistics",
      "Macroeconomics",
      "Mathematics",
      "Paleontology",
      "Philosophy",
      "Physics",
      "Statistical mechanics",
      "Statistical physics",
      "Stock market",
      "Systemic risk",
      "Theoretical computer science",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Jianjia"
      },
      {
        "surname": "Guo",
        "given_name": "Xingchen"
      },
      {
        "surname": "Li",
        "given_name": "Weimin"
      },
      {
        "surname": "Wu",
        "given_name": "Xing"
      },
      {
        "surname": "Zhang",
        "given_name": "Zhihong"
      },
      {
        "surname": "Hancock",
        "given_name": "Edwin R."
      }
    ]
  },
  {
    "title": "A nested U-shape network with multi-scale upsample attention for robust retinal vascular segmentation",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.107998",
    "abstract": "This paper presents a new nested U-shape attention network (NUA-Net) with improved robustness of lesions for effective vascular segmentation in retinal imaging. Unlike most of the current deep learning approaches which rely on vanilla upsample module to recover distinguishable features for segmentation, our attention-based multi-scale network extends the U-shape segmentation network by introducing a novel multi-scale upsample attention (MSUA) module to enhance vessel features in a hierarchical structure. The new approach connects encoder-decoder branches through a nested skip-connection pyramid architecture to extract discriminating retinal features from the rich local details. Experimental evaluations on five publicly available databases DRIVE, STARE, CHASE_DB, IOSTAR and HRF show the NUA-Net achieves 0.8043–0.8511 (Sensitivity), 0.9741–0.99 (Specificity) and 0.9646–0.9794 (Accuracy) respectively. The benchmark by cross-testing and separate-testing presents a state-of-the-art performance and better vessel preservation compared with other approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321001850",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biochemistry",
      "Cartography",
      "Chemistry",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Encoder",
      "Gene",
      "Geography",
      "Geometry",
      "Mathematics",
      "Network architecture",
      "Operating system",
      "Pattern recognition (psychology)",
      "Pyramid (geometry)",
      "Robustness (evolution)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Ruohan"
      },
      {
        "surname": "Li",
        "given_name": "Qin"
      },
      {
        "surname": "Wu",
        "given_name": "Jianrong"
      },
      {
        "surname": "You",
        "given_name": "Jane"
      }
    ]
  },
  {
    "title": "A practical artificial intelligence system to diagnose COVID-19 using computed tomography: A multinational external validation study",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.09.012",
    "abstract": "Computed tomography has gained an important role in the early diagnosis of COVID-19 pneumonia. However, the ever-increasing number of patients has overwhelmed radiology departments and has caused a reduction in quality of services. Artificial intelligence (AI) systems are the remedy to the current situation. However, the lack of application in real-world conditions has limited their consideration in clinical settings. This study validated a clinical AI system, COVIDiag, to aid radiologists in accurate and rapid evaluation of COVID-19 cases. 50 COVID-19 and 50 non-COVID-19 pneumonia cases were included from each of five centers: Argentina, Turkey, Iran, Netherlands, and Italy. The Dutch database included only 50 COVID-19 cases. The performance parameters namely sensitivity, specificity, accuracy, and area under the ROC curve (AUC) were computed for each database using COVIDiag model. The most common pattern of involvement among COVID-19 cases in all databases were bilateral involvement of upper and lower lobes with ground-glass opacities. The best sensitivity of 92.0% was recorded for the Italian database. The system achieved an AUC of 0.983, 0.914, 0.910, and 0.882 for Argentina, Turkey, Iran, and Italy, respectively. The model obtained a sensitivity of 86.0% for the Dutch database. COVIDiag model could diagnose COVID-19 pneumonia in all of cohorts with AUC of 0.921 (sensitivity, specificity, and accuracy of 88.8%, 87.0%, and 88.0%, respectively). Our study confirmed the accuracy of our proposed AI model (COVIDiag) in the diagnosis of COVID-19 cases. Furthermore, the system demonstrated consistent optimal diagnostic performance on multinational databases, which is critical to determine the generalizability and objectivity of the proposed COVIDiag model. Our results are significant as they provide real-world evidence regarding the applicability of AI systems in clinical medicine.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521003391",
    "keywords": [
      "2019-20 coronavirus outbreak",
      "Area under curve",
      "Area under the curve",
      "Artificial intelligence",
      "Computed tomography",
      "Computer science",
      "Coronavirus disease 2019 (COVID-19)",
      "Disease",
      "Electronic engineering",
      "Engineering",
      "Infectious disease (medical specialty)",
      "Internal medicine",
      "Medicine",
      "Nuclear medicine",
      "Outbreak",
      "Pathology",
      "Pediatrics",
      "Pharmacokinetics",
      "Pneumonia",
      "Procalcitonin",
      "Radiology",
      "Receiver operating characteristic",
      "Sensitivity (control systems)",
      "Sepsis",
      "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)"
    ],
    "authors": [
      {
        "surname": "Ardakani",
        "given_name": "Ali Abbasian"
      },
      {
        "surname": "Kwee",
        "given_name": "Robert M."
      },
      {
        "surname": "Mirza-Aghazadeh-Attari",
        "given_name": "Mohammad"
      },
      {
        "surname": "Castro",
        "given_name": "Horacio Matías"
      },
      {
        "surname": "Kuzan",
        "given_name": "Taha Yusuf"
      },
      {
        "surname": "Altintoprak",
        "given_name": "Kübra Murzoğlu"
      },
      {
        "surname": "Besutti",
        "given_name": "Giulia"
      },
      {
        "surname": "Monelli",
        "given_name": "Filippo"
      },
      {
        "surname": "Faeghi",
        "given_name": "Fariborz"
      },
      {
        "surname": "Acharya",
        "given_name": "U Rajendra"
      },
      {
        "surname": "Mohammadi",
        "given_name": "Afshin"
      }
    ]
  },
  {
    "title": "Vanishing boosted weights: A consistent algorithm to learn interpretable rules",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.08.016",
    "abstract": "Learning compact but highly accurate models that help in human decision-making is challenging. Most such scoring systems were constructed by human experts using some heuristics. In this contribution, we propose a principled method with theoretical guarantees to learn interpretable simple rules. We introduce Vanishing Boosted Weights (VBW) approach which is a corrective fine-tuning procedure on decision stumps. It is a simple method which surprisingly was never investigated. We propose its extension, Corrective Federated Averaging VBW, that is practical in a federated learning scenario. We illustrate by our numerical experiments both on simulated and real data that the novel approaches are competitive compared to the state-of-the-art methods, and outperform them.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521003081",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Epistemology",
      "Extension (predicate logic)",
      "Heuristics",
      "Machine learning",
      "Operating system",
      "Philosophy",
      "Programming language",
      "Simple (philosophy)",
      "State (computer science)"
    ],
    "authors": [
      {
        "surname": "Sokolovska",
        "given_name": "Nataliya"
      },
      {
        "surname": "Behbahani",
        "given_name": "Yasser Mohseni"
      }
    ]
  },
  {
    "title": "Multimodal-adaptive hierarchical network for multimedia sequential recommendation",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.08.023",
    "abstract": "Recommender system has a pivotal role in electronic economy especially for the online shopping platforms. Studies over the past two decades have proved that exploiting the inherent properties of items contributes a lot to the accuracy of multimedia sequential recommendation. There is no doubt that multimedia information including images and texts of a product have an impact on user’s purchase decision. However, modeling user’s dynamic preferences for multimodal (visual and textual in this paper) information over time is still a challenging problem. To solve this problem, we propose a Multimodal-Adaptive Hierarchical Network (MAHN for short) for multimedia sequential recommendation, which includes a hierarchical recurrent neural network and an information modulation module between the hierarchical structure. Specifically, the hierarchical recurrent neural network achieves the re-selection of multimodal information from the first layer to the second layer, the information modulation module realizes the selection of each modal information at time step t based on the previous time steps. Finally, to improve the generalization ability of our model, we adopt the multi-task training style to jointly optimize BPR loss and reconstruction loss of multimodal information. Experiments are conducted on two real world public datasets, and the results demonstrate that our model outperforms the other methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521003202",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Chemistry",
      "Computer science",
      "Economics",
      "Layer (electronics)",
      "Machine learning",
      "Management",
      "Multimedia",
      "Organic chemistry",
      "Recommender system",
      "Recurrent neural network",
      "Selection (genetic algorithm)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Han",
        "given_name": "Tengyue"
      },
      {
        "surname": "Niu",
        "given_name": "Shaozhang"
      },
      {
        "surname": "Wang",
        "given_name": "Pengfei"
      }
    ]
  },
  {
    "title": "MVDRNet: Multi-view diabetic retinopathy detection by combining DCNNs and attention mechanisms",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108104",
    "abstract": "Diabetic retinopathy (DR) detection has attracted much attention recently, and the deep learning algorithms have gained traction in this area. At present, DR screening by deep learning algorithms is often based on single-view fundus images, which usually leads to an unsatisfactory accuracy of DR grading due to the incomplete lesion features. In this paper, we proposed a novel diabetic retinopathy detection convolutional network for automatic DR detection by integrating multi-view fundus images. Compared to existing single-view DCNN-based DR detection methods, the proposed method has the following advantages. First, our method fully utilizes the lesion features from the retina with a field-of-view around 120 ∘ − 150 ∘ . Second, by introducing the attention mechanisms, more attention will be paid on the influential view and the performance can be improved. Besides, we also assign large weights to important channels in the network for effective feature extraction. Experiments are conducted on our collected multi-view DR dataset contained 15,468 images, in which each eye sample provides four-view images. The experimental results indicate that using multi-view images is suitable for automatic DR detection and our proposed method is superior to other benchmarking methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321002910",
    "keywords": [
      "Artificial intelligence",
      "Benchmarking",
      "Business",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Deep learning",
      "Diabetes mellitus",
      "Diabetic retinopathy",
      "Endocrinology",
      "Feature extraction",
      "Fundus (uterus)",
      "Marketing",
      "Medicine",
      "Ophthalmology",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Luo",
        "given_name": "Xiaoling"
      },
      {
        "surname": "Pu",
        "given_name": "Zuhui"
      },
      {
        "surname": "Xu",
        "given_name": "Yong"
      },
      {
        "surname": "Wong",
        "given_name": "Wai Keung"
      },
      {
        "surname": "Su",
        "given_name": "Jingyong"
      },
      {
        "surname": "Dou",
        "given_name": "Xiaoyan"
      },
      {
        "surname": "Ye",
        "given_name": "Baikang"
      },
      {
        "surname": "Hu",
        "given_name": "Jiying"
      },
      {
        "surname": "Mou",
        "given_name": "Lisha"
      }
    ]
  },
  {
    "title": "An improved watermarking algorithm for robustness and imperceptibility of data protection in the perception layer of internet of things",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.10.032",
    "abstract": "Emergence of Internet of Things (IoT) and modern digital applications such as digital financial services and deliveries make it easy to reproduce and re-distribute digital contents thus give room to so many copyright violations of illegal use of contents that need to be resolved. Researcher have been presenting the watermarking algorithms to prevent these illicit activities to a document before distribution. However, several issues have been identified for the digital transactions in the IoT. Thus, this research proposes a new text document image watermarking algorithm which emphasizes on two most important measures, visual quality, and robustness. To boost these measures, third least significant bit has been used for insertion. In addition, to further strengthen the technique, the Pascal Triangle is applied to determine the best position for embedding. Experimental results on the standard dataset have revealed that the proposed watermarking has achieved very encouraging results with PSNR and NCC averaged 54.95db and 0.98, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521003901",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Biology",
      "Chemistry",
      "Computer science",
      "Computer security",
      "Data mining",
      "Digital content",
      "Digital watermarking",
      "Embedding",
      "Gene",
      "Image (mathematics)",
      "Internet of Things",
      "Multimedia",
      "Neuroscience",
      "Pascal (unit)",
      "Perception",
      "Programming language",
      "Robustness (evolution)",
      "The Internet",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Hasan",
        "given_name": "Mohammad Kamrul"
      },
      {
        "surname": "Kamil",
        "given_name": "Samar"
      },
      {
        "surname": "Shafiq",
        "given_name": "Muhammad"
      },
      {
        "surname": "S",
        "given_name": "Yuvaraj"
      },
      {
        "surname": "Kumar",
        "given_name": "E. Saravana"
      },
      {
        "surname": "Vincent",
        "given_name": "Rajiv"
      },
      {
        "surname": "Nafi",
        "given_name": "Nazmus Shaker"
      }
    ]
  },
  {
    "title": "Doubly supervised parameter transfer classifier for diagnosis of breast cancer with imbalanced ultrasound imaging modalities",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108139",
    "abstract": "The bimodal ultrasound, namely B-mode ultrasound (BUS) and elastography ultrasound (EUS), provide complementary information to improve the diagnostic accuracy of breast cancers. However, in clinical practice, it is easier to acquire the labeled BUS images than the paired bimodal ultrasound data with shared labels due to the lack of EUS devices, especially in many rural hospitals. Thus, the single-modal BUS-based computer-aided diagnosis (CAD) generally has wide applications. Transfer learning (TL) can promote a BUS-based CAD model by transferring additional knowledge from EUS modality. To make full use of labeled paired bimodal data and the additional single-modal BUS images for knowledge transfer, a novel doubly supervised parameter transfer classifier (DSPTC) is proposed to well handle the TL between imbalanced modalities with the guidance of label information. Specifically, the proposed DSPTC consists of two loss functions corresponding to the paired bimodal ultrasound data with shared labels and the unpaired images with different labels, respectively. The former uses the loss function in the specially designed TL paradigm of support vector machine plus, while the latter adopts the Hilbert-Schmidt Independence Criterion (HSIC) for knowledge transfer between the unpaired images, which consist of the single-modal BUS images and the EUS images from the paired bimodal data. Consequently, the doubly supervised knowledge transfer is implemented by way of parameter transfer in a unified optimization framework. Two experiments are designed to evaluate the proposed DSPTC for the ultrasound-based diagnosis of breast cancers. The experimental results indicate that DSPTC outperforms all the compared algorithms, suggesting its wide potential applications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003265",
    "keywords": [
      "Artificial intelligence",
      "CAD",
      "Chemistry",
      "Classifier (UML)",
      "Computer science",
      "Engineering",
      "Engineering drawing",
      "Machine learning",
      "Medicine",
      "Modal",
      "Modalities",
      "Pattern recognition (psychology)",
      "Polymer chemistry",
      "Radiology",
      "Social science",
      "Sociology",
      "Support vector machine",
      "Transfer of learning",
      "Ultrasound"
    ],
    "authors": [
      {
        "surname": "Fei",
        "given_name": "Xiaoyan"
      },
      {
        "surname": "Zhou",
        "given_name": "Shichong"
      },
      {
        "surname": "Han",
        "given_name": "Xiangmin"
      },
      {
        "surname": "Wang",
        "given_name": "Jun"
      },
      {
        "surname": "Ying",
        "given_name": "Shihui"
      },
      {
        "surname": "Chang",
        "given_name": "Cai"
      },
      {
        "surname": "Zhou",
        "given_name": "Weijun"
      },
      {
        "surname": "Shi",
        "given_name": "Jun"
      }
    ]
  },
  {
    "title": "A new approach to training more interpretable model with additional segmentation",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.10.003",
    "abstract": "It is not straightforward to understand how the complicated deep learning models work because they are almost black boxes. To address this problem, various approaches have been developed to provide interpretability and applied in black-box deep learning models. However, the traditional interpretable machine learning only helps us to understand the models which have already been trained. Therefore, if the models are not properly trained, it is obvious that the interpretable machine learning will not work well. We propose a simple but effective method which trains models to improve interpretability for image classification. We also evaluate how well the models focus on appropriate objects, not just relying on classification accuracy. We use Class Activation Mapping (CAM) to train and evaluate the model interpretability. As a result, with VOC PASCAL 2012 datasets, when the ResNet50 model is trained by the proposed approach the 0.5IOU is 29.61%, while the model which is trained only by images and labels is 13.00%. The classification accuracy of the proposed approach is 75.03%, the existing method is 68.38%, and FCN is 60.69%. These evaluations show that the proposed approach is effective.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521003627",
    "keywords": [
      "Artificial intelligence",
      "Black box",
      "Cartography",
      "Class (philosophy)",
      "Computer science",
      "Deep learning",
      "Focus (optics)",
      "Geography",
      "Interpretability",
      "Machine learning",
      "Optics",
      "Pascal (unit)",
      "Pattern recognition (psychology)",
      "Physics",
      "Programming language",
      "Segmentation",
      "Train"
    ],
    "authors": [
      {
        "surname": "Shin",
        "given_name": "Sunguk"
      },
      {
        "surname": "Kim",
        "given_name": "Youngjoon"
      },
      {
        "surname": "Yoon",
        "given_name": "Ji Won"
      }
    ]
  },
  {
    "title": "3D Dental model segmentation with graph attentional convolution network",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.09.005",
    "abstract": "Precisely segmenting teeth from digitized 3D dental models is an essential task in computer-aided orthodontic surgical planning. In recent years, various deep learning-based methods have been proposed to process dental models for teeth segmentation, however, these methods usually ignore or coarsely model the dependency between vertices/mesh cells in local space, which fails to exploit local geometric details that are critical to capture complete teeth structure. In this paper, we propose a specific end-to-end network for teeth segmentation on 3D dental models. By constructing a graph for the raw mesh data, our network adopts a series of graph attentional convolution layers and a global structure branch to extract fine-grained local geometric feature and global feature, respectively. Subsequently, these two features are further fused to learn comprehensive information for cell-wise segmentation tasks. We have evaluated our network on a real-patient dataset of dental models acquired through 3D intraoral scanners, and experimental results show that our method outperforms state-of-the-art deep learning methods for 3D shape segmentation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521003172",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Convolution (computer science)",
      "Deep learning",
      "Exploit",
      "Feature (linguistics)",
      "Graph",
      "Linguistics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Process (computing)",
      "Segmentation",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Yue"
      },
      {
        "surname": "Zhang",
        "given_name": "Lingming"
      },
      {
        "surname": "Yang",
        "given_name": "Chongshi"
      },
      {
        "surname": "Tan",
        "given_name": "Yingyun"
      },
      {
        "surname": "Liu",
        "given_name": "Yang"
      },
      {
        "surname": "Li",
        "given_name": "Pengcheng"
      },
      {
        "surname": "Huang",
        "given_name": "Tianhao"
      },
      {
        "surname": "Gao",
        "given_name": "Chenqiang"
      }
    ]
  },
  {
    "title": "Explainable deep learning for efficient and robust pattern recognition: A survey of recent developments",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108102",
    "abstract": "Deep learning has recently achieved great success in many visual recognition tasks. However, the deep neural networks (DNNs) are often perceived as black-boxes, making their decision less understandable to humans and prohibiting their usage in safety-critical applications. This guest editorial introduces the thirty papers accepted for the Special Issue on Explainable Deep Learning for Efficient and Robust Pattern Recognition. They are grouped into three main categories: explainable deep learning methods, efficient deep learning via model compression and acceleration, as well as robustness and stability in deep learning. For each of the three topics, a survey of the representative works and latest developments is presented, followed by the brief introduction of the accepted papers belonging to this topic. The special issue should be of high relevance to the reader interested in explainable deep learning methods for efficient and robust pattern recognition applications and it helps promoting the future research directions in this field.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321002892",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Data science",
      "Deep learning",
      "Deep neural networks",
      "Field (mathematics)",
      "Gene",
      "Law",
      "Machine learning",
      "Mathematics",
      "Political science",
      "Pure mathematics",
      "Relevance (law)",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Bai",
        "given_name": "Xiao"
      },
      {
        "surname": "Wang",
        "given_name": "Xiang"
      },
      {
        "surname": "Liu",
        "given_name": "Xianglong"
      },
      {
        "surname": "Liu",
        "given_name": "Qiang"
      },
      {
        "surname": "Song",
        "given_name": "Jingkuan"
      },
      {
        "surname": "Sebe",
        "given_name": "Nicu"
      },
      {
        "surname": "Kim",
        "given_name": "Been"
      }
    ]
  },
  {
    "title": "Enhanced anomaly scores for isolation forests",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108115",
    "abstract": "Isolation Forest represents a variant of Random Forest largely and successfully employed for outlier detection. The main idea is that outliers are likely to get isolated in a tree after few splits. The anomaly score is therefore a function inversely related to the leaf depth. This paper proposes enhanced anomaly scores of the Isolation Forest by making two different contributions. The first consists in weighing the path traversed by an object to obtain a more informative anomaly score. The second contribution employs a different aggregation function to combine the tree scores. We thoroughly evaluate the proposed methodology by testing it on sixteen datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003022",
    "keywords": [
      "Anomaly (physics)",
      "Anomaly detection",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Condensed matter physics",
      "Isolation (microbiology)",
      "Mathematics",
      "Microbiology",
      "Physics"
    ],
    "authors": [
      {
        "surname": "Mensi",
        "given_name": "Antonella"
      },
      {
        "surname": "Bicego",
        "given_name": "Manuele"
      }
    ]
  },
  {
    "title": "Enhanced model for fake image detection (EMFID) using convolutional neural networks with histogram and wavelet based feature extractions",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.10.007",
    "abstract": "In recent scenario of digital world and image forensics image is forged or unforged to be checked. It is a critical process in the areas where the images are taken as crucial proof in decision making in several cases. Hence, for handling the delinquent activities on image, a novel model, which is capable of detecting or classifying the authenticated and altered images, is required. A new model called Enhanced Model for Fake Image Detection (EMFID) is used for classifying the obtained images as AUTHENTIC and FORGED. Moreover, this work also discusses about the images that are altered with the splicing methods that is performed by cropping and pasting certain image part from one to another original image. The proposed model comprises four phases such as Image Pre-processing, Histogram based Feature Extraction, Discrete Wavelet Transform based Feature Extraction, Image Classification with ConvolutionalNeural Networks (CNN). The test images that are obtained from data set are classified under Authentic Class and Forged class. The results are better with classification accuracy and model efficiency.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521003652",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Digital image",
      "Feature (linguistics)",
      "Feature detection (computer vision)",
      "Feature extraction",
      "Histogram",
      "Image (mathematics)",
      "Image processing",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Wavelet"
    ],
    "authors": [
      {
        "surname": "Sabitha",
        "given_name": "R."
      },
      {
        "surname": "Aruna",
        "given_name": "A."
      },
      {
        "surname": "Karthik",
        "given_name": "S."
      },
      {
        "surname": "Shanthini",
        "given_name": "J."
      }
    ]
  },
  {
    "title": "Prediction on transmission trajectory of COVID-19 based on particle swarm algorithm",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.09.003",
    "abstract": "This study aimed to predict the transmission trajectory of the 2019 Corona Virus Disease (COVID-19). The particle swarm optimization (PSO) algorithm was combined with the traditional susceptible exposed infected recovered (SEIR) infectious disease prediction model to propose a SEIR-PSO prediction model on the COVID-19. In addition, the domestic epidemic data from February 25, 2020 to March 20, 2020 in China were selected as the training set for analysis. The results showed that when the conversion rate, recovery rate, and mortality rate of the SEIR-PSO model were 1/5, 1/15, and 1/13, its predictive effect on the number of people diagnosed with COVID-19 was the closest to the real data; and the SEIR-PSO model showed a mean-square errors (MSE) value of 1304.35 and mean absolute error (MAE) value of 1069.18, showing the best prediction effect compared with the susceptible infectious susceptible (SIS) model and the SEIR model. In contrary to the standard particle swarm optimization (SPSO) and linear weighted particle swarm optimization (LPSO), which were two classical improved PSO algorithms, the reliability and diversity of the SEIR-PSO model were higher. In summary, the SEIR-PSO model showed excellent performance in predicting the time series of COVID-19 epidemic data, and showed reliable application value for the prevention and control of COVID-19 epidemic.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521003184",
    "keywords": [
      "Algorithm",
      "Computer science",
      "Coronavirus disease 2019 (COVID-19)",
      "Disease",
      "Environmental health",
      "Epidemic model",
      "Infectious disease (medical specialty)",
      "Mathematical optimization",
      "Mathematics",
      "Mean squared error",
      "Mean squared prediction error",
      "Medicine",
      "Particle swarm optimization",
      "Pathology",
      "Population",
      "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)",
      "Statistics",
      "Swarm behaviour",
      "Telecommunications",
      "Transmission (telecommunications)"
    ],
    "authors": [
      {
        "surname": "Ding",
        "given_name": "Caichang"
      },
      {
        "surname": "Chen",
        "given_name": "Yiqin"
      },
      {
        "surname": "Liu",
        "given_name": "Zhiyuan"
      },
      {
        "surname": "Liu",
        "given_name": "Tianyin"
      }
    ]
  },
  {
    "title": "Multi-label feature selection via manifold regularization and dependence maximization",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108149",
    "abstract": "Feature selection is able to select more discriminative features for classification and plays an important role in multi-label learning to alleviate the effect of the curse of dimensionality. Recently, the multi-label feature selection methods based on the sparse regression model have received increasing attentions. However, most of these methods directly project original data space to label space in the regression model, which is inappropriate because the linear assumption between data space and label space doesn't hold in most cases. In the paper, we propose a feature selection method named multi-label feature selection via manifold regularization and dependence maximization (MRDM). In the regression model of MRDM, the original data space is projected to a low-dimensional manifold space, which not only has the same topological structure with the original data, but also has a strong dependence with the class labels. Then, an objective function involving l 2 , 1 -norm regularization is formulated, and an alternating optimization-based iterative algorithm is designed to obtain the sparse coefficients for multi-label feature selection. Extensive experiments on various multi-label data sets demonstrate the superiority of the proposed method compared with some state-of-the-art multi-label feature selection methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003368",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Curse of dimensionality",
      "Dimensionality reduction",
      "Discriminative model",
      "Engineering",
      "Feature (linguistics)",
      "Feature selection",
      "Feature vector",
      "Law",
      "Linguistics",
      "Manifold (fluid mechanics)",
      "Mathematical optimization",
      "Mathematics",
      "Maximization",
      "Mechanical engineering",
      "Norm (philosophy)",
      "Overfitting",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Regularization (linguistics)"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Rui"
      },
      {
        "surname": "Wu",
        "given_name": "Zhejun"
      }
    ]
  },
  {
    "title": "Prognosticating the effect on Unemployment rate in the post-pandemic India via Time-Series Forecasting and Least Squares Approximation",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.10.012",
    "abstract": "The current paper aims to analytically visualize the future outcomes that the post-pandemic India might have in store for its citizens. We use time series forecasting on various collected data and combined the statistics of economics-deciding parameters to forecast the trends that might be prevalent in the next year. Since, the data contains a single anomalous trend, even the Prophet model could not learn this property from the data since this trend is not seasonal in nature. The current study proposes a novel architecture to deal with these rare unusual trends by combining two models - one learning normal usual patterns and the other getting trained on usual as well as rare anomalous patterns. It could help in dealing with sudden hike patterns like due to COVID-19 in the data, and lead to better forecasting on future timeframes. We combined the results of two distinct time-forecasting models trained on two sets of data of varying timeline lengths, using parameters obtained from Least Squares Approximation (LSA). The LSA helps us find an approximate vector approximation so as to obtain a model performing closely to the actual.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552100369X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Coronavirus disease 2019 (COVID-19)",
      "Disease",
      "Econometrics",
      "Epistemology",
      "Estimator",
      "Geography",
      "Infectious disease (medical specialty)",
      "Least-squares function approximation",
      "Machine learning",
      "Mathematics",
      "Medicine",
      "Meteorology",
      "Nowcasting",
      "Paleontology",
      "Pandemic",
      "Pathology",
      "Philosophy",
      "Property (philosophy)",
      "Series (stratigraphy)",
      "Statistics",
      "Time series",
      "Timeline"
    ],
    "authors": [
      {
        "surname": "Agrahari",
        "given_name": "Ashutosh"
      },
      {
        "surname": "Singh",
        "given_name": "Pawan"
      },
      {
        "surname": "Veer",
        "given_name": "Ankur"
      },
      {
        "surname": "Singh",
        "given_name": "Anshuman"
      },
      {
        "surname": "Vidyarthi",
        "given_name": "Ankit"
      },
      {
        "surname": "Khan",
        "given_name": "Baseem"
      }
    ]
  },
  {
    "title": "k NN-based feature learning network for semantic segmentation of point cloud data",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.10.023",
    "abstract": "Semantic segmentation of sensed point cloud data plays a significant role in scene understanding and reconstruction, robot navigation, etc. This paper presents a k NN-based 3D semantic segmentation network, which is a structural model for directly processing the unorganized point clouds. The network consists of three modules: point feature extraction, local feature extraction, and semantic segmentation. The first module is designed based on the simplified PointNet to extract powerful high-dimensional point features. Local feature extraction module, the key component of the proposed network, utilizes the k NN algorithm to search k -neighbors of each query point to extract the local and global features. Then the final semantic segmentation part concatenates the extracted features to learn and label the input point clouds. Experimental results on the indoor and outdoor datasets show that the proposed work settles the shortcoming of insufficient local feature extraction of existing models and promotes the accuracy of semantic segmentation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521003834",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Feature (linguistics)",
      "Feature extraction",
      "Geometry",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Point (geometry)",
      "Point cloud",
      "Segmentation",
      "Semantic feature"
    ],
    "authors": [
      {
        "surname": "Luo",
        "given_name": "Nan"
      },
      {
        "surname": "Wang",
        "given_name": "Yifeng"
      },
      {
        "surname": "Gao",
        "given_name": "Yun"
      },
      {
        "surname": "Tian",
        "given_name": "Yumin"
      },
      {
        "surname": "Wang",
        "given_name": "Quan"
      },
      {
        "surname": "Jing",
        "given_name": "Chuan"
      }
    ]
  },
  {
    "title": "Deep ladder reconstruction-classification network for unsupervised domain adaptation",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.10.009",
    "abstract": "Unsupervised Domain Adaptation aims to learn a classifier for an unlabeled target domain by transferring knowledge from a labeled source domain. Most existing approaches learn domain-invariant features by adapting the entire information of each image. However, forcing adaptation of domain-specific components can undermine the effectiveness of learned features. We propose a novel architecture called Deep Ladder Reconstruction-Classification Network (DLaReC) which is designed to learn cross-domain shared contents by suppressing domain-specific variations. The DLaReC adopts an encoder with cross-domain sharing and a target-domain reconstruction decoder. The encoder and decoder are connected with residual shortcuts at each intermediate layer. By this means, the domain-specific components are directly fed to the decoder for reconstruction, relieving the pressure to learn domain-specific variations at later layers of the shared encoder. Therefore, DLaReC allows the encoder to focus on learning cross-domain shared representations and ignore domain-specific variations. DLaReC is implemented by jointly learning three tasks: supervised classification of the source domain, unsupervised reconstruction of the target domain and cross-domain shared representation adaptation. Extensive experiments on Digit, Office31, ImageCLEF-DA and Office-Home datasets demonstrate the DLaReC outperforms state-of-the-art methods on the whole. The average accuracy on the Digit datasets, for instance, is improved from 95.6% to 96.9%. In addition, the result on Amazon → Webcam obtains significant improvement, i.e., from 91.1% to 94.7%.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521003676",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Domain (mathematical analysis)",
      "Domain adaptation",
      "Encoder",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Deng",
        "given_name": "Wanxia"
      },
      {
        "surname": "Su",
        "given_name": "Zhuo"
      },
      {
        "surname": "Qiu",
        "given_name": "Qiang"
      },
      {
        "surname": "Zhao",
        "given_name": "Lingjun"
      },
      {
        "surname": "Kuang",
        "given_name": "Gangyao"
      },
      {
        "surname": "Pietikäinen",
        "given_name": "Matti"
      },
      {
        "surname": "Xiao",
        "given_name": "Huaxin"
      },
      {
        "surname": "Liu",
        "given_name": "Li"
      }
    ]
  },
  {
    "title": "DDAT: Dual domain adaptive translation for low-resolution face verification in the wild",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108107",
    "abstract": "Low-resolution (LR) face verification has received much attention because of its wide applicability in real scenarios, especially in long-distance surveillance. However, the poor quality and scarcity of training data make the accuracy far from satisfactory. In this paper, we propose an end-to-end LR face translation and verification framework to improve the generation quality of face images and face verification accuracy simultaneously. We design a dual domain adaptive structure to generate high-quality images. On one hand, the structure can reduce the domain gap between training data and test data. On the other hand, the structure preserves identity consistency and low-level attributes. Meanwhile, in order to make the whole model more robust, we treat the generated images of the target domain as an extension of the training data. We conduct extensive comparative experiments on multiple benchmark data sets. Experimental results verify that our method achieves improved results in high-quality face generation and LR face verification. In particular, our model DDAT reduces FID to 18.63 and 39.55 on the source and the target domain from 254.7 and 206.19 of the up-sampling results, respectively. Our method outperforms competing approaches by more than 10 percentage points in terms of face verification accuracy on multiple surveillance benchmarks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321002946",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Consistency (knowledge bases)",
      "Data mining",
      "Domain (mathematical analysis)",
      "Epistemology",
      "Face (sociological concept)",
      "Facial recognition system",
      "Gene",
      "Geodesy",
      "Geography",
      "Mathematical analysis",
      "Mathematics",
      "Messenger RNA",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Quality (philosophy)",
      "Social science",
      "Sociology",
      "Training set",
      "Translation (biology)"
    ],
    "authors": [
      {
        "surname": "Jiao",
        "given_name": "Qianfen"
      },
      {
        "surname": "Li",
        "given_name": "Rui"
      },
      {
        "surname": "Cao",
        "given_name": "Wenming"
      },
      {
        "surname": "Zhong",
        "given_name": "Jian"
      },
      {
        "surname": "Wu",
        "given_name": "Si"
      },
      {
        "surname": "Wong",
        "given_name": "Hau-San"
      }
    ]
  },
  {
    "title": "Intelligent deep learning based bidirectional long short term memory model for automated reply of e-mail client prototype",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.10.021",
    "abstract": "E-mail is considered the commonly used and efficient way of communication over the globe. In the corporate sectors, the number of E-mails received every day is considerably high and the timely response to every E-mail is essential. Several researchers believe that natural language processing (NLP) techniques by the use of deep learning (DL) architectures have played a considerable part to reduce manual work for repeated E-mail responses and intended to develop E-mail systems with intelligent response function. In this view, this paper designs an intelligent DL enabled optimal bidirectional long short term memory (Bi-LSTM) technique for an automated E-mail reply (OBiLSTM-AER) of E-mail Client Prototype. The goal of the proposed model is to provide an automated E-mail reply solution for persons as well as corporates which receive massive identical E-mails daily. The presented model employs Glove and OBiLSTM model for feature extraction of receiving and response E-mails respectively. Finally, softmax classifier is applied to allocate the class labels. For improving the performance of the BiLSTM model, the hyperparameter tuning process takes place using an oppositional glowworm swarm optimization (OGSO) algorithm. An extensive set of simulations were performed to highlight the betterment of the proposed method and the results are examined interms of distinct measures.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521003809",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Deep learning",
      "Long short term memory",
      "Machine learning",
      "Natural language processing",
      "Physics",
      "Quantum mechanics",
      "Recurrent neural network",
      "Softmax function",
      "Term (time)"
    ],
    "authors": [
      {
        "surname": "V",
        "given_name": "Rajaraman P"
      },
      {
        "surname": "M",
        "given_name": "Prakash"
      }
    ]
  },
  {
    "title": "Semi-lexical languages: a formal basis for using domain knowledge to resolve ambiguities in deep-learning based computer vision",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.10.004",
    "abstract": "Human vision is able to compensate imperfections in sensory inputs from the real world by reasoning based on prior knowledge about the world. Deep learning has had a significant impact on computer vision due to its inherent ability in handling imprecision, but the absence of a reasoning framework based on domain knowledge limits its ability to interpret complex scenarios. We propose semi-lexical languages as a formal basis for reasoning with imperfect tokens provided by the real world. The power of deep learning is used to map the imperfect tokens into the alphabet of the language, and symbolic reasoning is used to determine the membership of input in the language. Semi-lexical languages have bindings that prevent the variations in which a semi-lexical token is interpreted in different parts of the input, thereby leaning on deduction to enhance the quality of recognition of individual tokens. We present case studies that demonstrate the advantage of using such a framework over pure deep learning and pure symbolic methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521003615",
    "keywords": [
      "Artificial intelligence",
      "Basis (linear algebra)",
      "Computer science",
      "Computer security",
      "Deep learning",
      "Domain (mathematical analysis)",
      "Geometry",
      "Imperfect",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Natural language processing",
      "Philosophy",
      "Security token"
    ],
    "authors": [
      {
        "surname": "Gangopadhyay",
        "given_name": "Briti"
      },
      {
        "surname": "Hazra",
        "given_name": "Somnath"
      },
      {
        "surname": "Dasgupta",
        "given_name": "Pallab"
      }
    ]
  },
  {
    "title": "Experts perception-based system to detect misinformation in health websites",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.11.008",
    "abstract": "Misinformation is a recurring problem that has experienced a significant growth in recent years due to the rapid development of the Internet. This development has driven the emergence of websites where their content is shared without control. This is even more dangerous in the health domain, given its specific nature and the increasing number of users searching for health-related information on the Internet. For these reasons, this information should be handled with special attention. In this paper, a novel system to detect misinformation in websites related to the health domain is presented. The proposed system uses text mining techniques and visual design features to estimate the trustworthiness of the website. It has been trained using human experts’ knowledge in the selected domain and their visual perception of the website design. Promising results have been obtained during the evaluation in the experimental stage.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521003998",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Control (management)",
      "Domain (mathematical analysis)",
      "Human–computer interaction",
      "Information retrieval",
      "Internet privacy",
      "Mathematical analysis",
      "Mathematics",
      "Misinformation",
      "Neuroscience",
      "Perception",
      "Psychology",
      "The Internet",
      "Trustworthiness",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "González-Fernández",
        "given_name": "César"
      },
      {
        "surname": "Fernández-Isabel",
        "given_name": "Alberto"
      },
      {
        "surname": "Martín de Diego",
        "given_name": "Isaac"
      },
      {
        "surname": "Fernández",
        "given_name": "Rubén R."
      },
      {
        "surname": "Viseu Pinheiro",
        "given_name": "J.F.J."
      }
    ]
  },
  {
    "title": "Human trajectory prediction and generation using LSTM models and GANs",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108136",
    "abstract": "Human trajectory prediction is an important topic in several application domains, ranging from self-driving cars to environment design and planning, from socially-aware robots to intelligent tracking systems. This complex subject comes with different challenges, such as human-space interaction, human-human interaction, multimodality, and generalizability. Currently, these challenges, especially generalizability, have not been completely explored by state-of-the-art works. This work attempts to fill this gap by proposing and defining new methods and metrics to help understand trajectories. In particular, new deep learning models based on Long Short-Term Memory and Generative Adversarial Network architectures are used in both unimodal and multimodal contexts. These approaches are evaluated with new error metrics, which normalize some biases in standard metrics. Tests have been assessed using newly collected datasets characterized by a higher diversity and lower linearity than those used in state-of-the-art works. The results prove that the proposed models and datasets are comparable to and yield better generalizability than state-of-the-art works. Moreover, we also prove that our datasets better represent multimodal scenarios (allowing for multiple possible behaviors) and that human trajectories are moderately influenced by their spatial region and slightly influenced by their date and time.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032100323X",
    "keywords": [
      "Artificial intelligence",
      "Astronomy",
      "Computer science",
      "Discriminative model",
      "Generalizability theory",
      "Generative grammar",
      "Machine learning",
      "Mathematics",
      "Physics",
      "Statistics",
      "Trajectory"
    ],
    "authors": [
      {
        "surname": "Rossi",
        "given_name": "Luca"
      },
      {
        "surname": "Paolanti",
        "given_name": "Marina"
      },
      {
        "surname": "Pierdicca",
        "given_name": "Roberto"
      },
      {
        "surname": "Frontoni",
        "given_name": "Emanuele"
      }
    ]
  },
  {
    "title": "On the transferability of adversarial perturbation attacks against fingerprint based authentication systems",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.10.015",
    "abstract": "The growing availability of cheap and reliable fingerprint acquisition scanners is resulting in an increasing spread of Fingerprint-based Authentication Systems (FAS) in consumer electronics. This has giving rise to a new wave in research on both smarter spoofing attacks, aimed to bypass a FAS by using a counterfeit fingerprint, and on more effective Liveness Detectors (LD), aimed to discern authentic (live) fingerprints from fake ones. As in many other computer vision tasks, deep Convolutional Neural Networks (CNN) demonstrated to be very effective also for fingerprint liveness detection. However, we showed that it is possible to adapt adversarial perturbation approaches to mislead CNN-based LD. In this paper, we want to make a step further toward the design of a black-box attack by investigating whether it is possible to transfer a perturbation across different CNN liveness detectors in the case of a target LD very different from the one used to compute the perturbations. To this aim, we designed an attack scenario where a shadow LD (i.e. an adaptation of the substitute technique for the liveness detection application) is used to generate an adversarial fingerprint in a white-box setting before submitting it to the real target LD, invoked in a total back-box manner. Finally, we analysed the impact that such attack has on the authentication system, also analysing if and to what extent the scanner and the spoofing material combinations affect the success of the attack.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521003731",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Authentication (law)",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Convolutional neural network",
      "Counterfeit",
      "Deep learning",
      "Fingerprint (computing)",
      "Fingerprint recognition",
      "Law",
      "Liveness",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Political science",
      "Spoofing attack",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Marrone",
        "given_name": "Stefano"
      },
      {
        "surname": "Sansone",
        "given_name": "Carlo"
      }
    ]
  },
  {
    "title": "Training object detectors from few weakly-labeled and many unlabeled images",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108164",
    "abstract": "Weakly-supervised object detection attempts to limit the amount of supervision by dispensing the need for bounding boxes, but still assumes image-level labels on the entire training set. In this work, we study the problem of training an object detector from one or few images with image-level labels and a larger set of completely unlabeled images. This is an extreme case of semi-supervised learning where the labeled data are not enough to bootstrap the learning of a detector. Our solution is to train a weakly-supervised student detector model from image-level pseudo-labels generated on the unlabeled set by a teacher classifier model, bootstrapped by region-level similarities to labeled images. Building upon the recent representative weakly-supervised pipeline PCL [1], our method can use more unlabeled images to achieve performance competitive or superior to many recent weakly-supervised detection solutions. Code will be made available at https://github.com/zhaohui-yang/NSOD.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003514",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Bounding overwatch",
      "Classifier (UML)",
      "Computer science",
      "Computer vision",
      "Detector",
      "Image (mathematics)",
      "Labeled data",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Pipeline (software)",
      "Programming language",
      "Semi-supervised learning",
      "Set (abstract data type)",
      "Supervised learning",
      "Telecommunications",
      "Training set"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Zhaohui"
      },
      {
        "surname": "Shi",
        "given_name": "Miaojing"
      },
      {
        "surname": "Xu",
        "given_name": "Chao"
      },
      {
        "surname": "Ferrari",
        "given_name": "Vittorio"
      },
      {
        "surname": "Avrithis",
        "given_name": "Yannis"
      }
    ]
  },
  {
    "title": "Automatic Annotation Algorithm of Medical Radiological Images using Convolutional Neural Network",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.09.011",
    "abstract": "In order to address the problems of time-consuming, low accuracy and poor convergence effect of traditional image automatic annotation algorithm, an Automatic annotation algorithm of medical radiological images based on convolutional neural network (CNN) is proposed. First of all, the image gradient information model was constructed, the edge contour feature of medical radiation image was initialized, the automatic segmentation model of medical radiation image was established by block template matching method, and the automatic segmentation processing of medical radiation image was completed. Secondly, by fusing the contour and gray information of image segmentation, the multi-resolution feature is extracted by using the three-dimensional distributed pixel sequence of image. The fusion feature decomposition of the image was obtained based on CNN, and the automatic annotation of medical radiation image was completed. The results show that the image segmentation effect of the proposed algorithm is good, the number of feature points is accurate, and the accuracy of multi-resolution feature extraction is as high as 98.7%. The convergence of image annotation is good, short time-consumption, and the F1 measurement value of the algorithm is high, and the overall performance is good.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521003378",
    "keywords": [
      "Artificial intelligence",
      "Automatic image annotation",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Feature (linguistics)",
      "Feature extraction",
      "Image (mathematics)",
      "Image processing",
      "Image segmentation",
      "Image texture",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Xiaofeng"
      },
      {
        "surname": "Wang",
        "given_name": "Yanwei"
      },
      {
        "surname": "Cai",
        "given_name": "Yingjie"
      }
    ]
  },
  {
    "title": "Efficient skew detection and correction in scanned document images through clustering of probabilistic hough transforms",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.09.014",
    "abstract": "Documents scanning is still one of the widely used documents digitization steps; however, skew in scanned documents is inevitable. If this skew is not corrected, the extraction of region/s of interest (RoI) and further processing like; detection and classification on such RoI becomes difficult. It has been shown that skew detection and correction significantly improve Optical Character Recognition (OCR) systems’ accuracy. This paper introduces a novel, robust and straightforward skew detection method for scanned documents, which uses Probabilistic Hough Transformation (PHT) for line detection in a first step and clusters the lines in a second step based on parallelism. The cluster with maximum parallel lines represents the expected skewed lines. The proposed method is tested on real scanned images taken from the Document Image Skew Estimation Contest (DISEC’13), Pashto, and Tobacco800 datasets. The proposed method performs well both in terms of accuracy and efficiency. It is efficient and robust to noise. Furthermore, we show that it also works on Arabic and Latin scripts.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521003408",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Cluster analysis",
      "Computer science",
      "Computer vision",
      "Digitization",
      "Gene",
      "Geometry",
      "Hough transform",
      "Image (mathematics)",
      "Line (geometry)",
      "Mathematics",
      "Object detection",
      "Optical character recognition",
      "Pattern recognition (psychology)",
      "Preprocessor",
      "Probabilistic logic",
      "Skew",
      "Telecommunications",
      "Transformation (genetics)"
    ],
    "authors": [
      {
        "surname": "Ahmad",
        "given_name": "Riaz"
      },
      {
        "surname": "Naz",
        "given_name": "Saeeda"
      },
      {
        "surname": "Razzak",
        "given_name": "Imran"
      }
    ]
  },
  {
    "title": "Symbols Detection and Classification using Graph Neural Networks",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.09.020",
    "abstract": "In this paper, we propose a method to both extract and classify symbols in floorplan images. This method relies on the very recent developments of Graph Neural Networks (GNN). In the proposed approach, floorplan images are first converted into Region Adjacency Graphs (RAGs). In order to achieve both classification and extraction, two different GNNs are used. The first one aims at classifying each node of the graph while the second targets the extraction of clusters corresponding to symbols. In both cases, the model is able to take into account edge features. Each model is firstly evaluated independently before combining both tasks simultaneously, increasing the quickness of the results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521003469",
    "keywords": [
      "Adjacency list",
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Data mining",
      "Embedded system",
      "Engineering",
      "Floorplan",
      "Graph",
      "Node (physics)",
      "Pattern recognition (psychology)",
      "Structural engineering",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Renton",
        "given_name": "Guillaume"
      },
      {
        "surname": "Balcilar",
        "given_name": "Muhammet"
      },
      {
        "surname": "Héroux",
        "given_name": "Pierre"
      },
      {
        "surname": "Gaüzère",
        "given_name": "Benoît"
      },
      {
        "surname": "Honeine",
        "given_name": "Paul"
      },
      {
        "surname": "Adam",
        "given_name": "Sébastien"
      }
    ]
  },
  {
    "title": "Conditional information gain networks as sparse mixture of experts",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108151",
    "abstract": "Deep neural network models owe their representational power and high performance in classification tasks to the high number of learnable parameters. Running deep neural network models in limited-resource environments is a problematic task. Models employing conditional computing aim to reduce the computational burden while retaining model performance on par with more complex neural network models. This paper, proposes a new model, Conditional Information Gain Networks as Sparse Mixture of Experts (sMoE-CIGNs). A CIGN model is a neural tree that allows conditionally skipping parts of the tree based on routing mechanisms inserted into the architecture. These routing mechanisms are based on differentiable Information Gain objectives. CIGN groups semantically similar samples in the leaves, enabling simpler classifiers to focus on differentiating between similar classes. This lets the CIGN model attain high classification performances with lighter models. We further improve the basic CIGN model by proposing a sparse mixture of experts model for difficult to classify samples that may get routed to suboptimal branches. If a sample has routing confidence higher than a specific threshold, the sample may be routed towards multiple child nodes. The classification decision can then be taken as a mixture of these expert decisions. We learn the optimal routing thresholds by Bayesian Optimization over a validation set by minimizing a weighted loss, including the classification accuracy and the number of multiplication and accumulations (MAC). We show the effectiveness of the CIGN models enhanced with the Sparse Mixture of Experts approach with extensive tests on MNIST, Fashion MNIST, CIFAR 100 and UCI-USPS datasets, as well as comparisons with methods from the literature. sMoE-CIGN models can retain high generalization performance, on par with a thick unconditional model while keeping the operation burden at the same level with a much thinner model. 1 1 This article is an extended version of reference “Bicici U.C., Keskin C., Akarun L., Conditional information gain networks”, which appeared in Proceedings of the 24th International Conference on Pattern Recognition (ICPR) (2018).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003381",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer network",
      "Computer science",
      "Data mining",
      "Economics",
      "MNIST database",
      "Machine learning",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Programming language",
      "Routing (electronic design automation)",
      "Set (abstract data type)",
      "Task (project management)",
      "Tree (set theory)"
    ],
    "authors": [
      {
        "surname": "Bicici",
        "given_name": "Ufuk Can"
      },
      {
        "surname": "Akarun",
        "given_name": "Lale"
      }
    ]
  },
  {
    "title": "Attribute-driven image captioning via soft-switch pointer",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.08.021",
    "abstract": "Visual attributes detection provides rich semantic concepts for image captioning. Some previous methods attempt to directly encode the attributes into vectors and generate the corresponding captions, which ignore the correlations between the image regions and attributes. In this paper, we consider to bridge the gap between visual features and detected attributes: first to look at a specific region of the image and second to decide which attribute to attend to. We propose an attribute-driven image captioning approach consisting of two parts: the visual positioning part and the attribute selection part. Specifically, we introduce the pointer-generator network into the second part of our model as a soft-switch, which determines whether to generate a word through the hidden state or point to a detected attribute at each decoding step. Qualitative and Quantitative experiments show that our model can improve the coverage of key visual attributes and significantly boost the overall performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521003196",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Closed captioning",
      "Computer science",
      "Computer vision",
      "Decoding methods",
      "ENCODE",
      "Gene",
      "Generator (circuit theory)",
      "Geometry",
      "Image (mathematics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Pointer (user interface)",
      "Power (physics)",
      "Quantum mechanics",
      "Visualization",
      "Word (group theory)"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Yujie"
      },
      {
        "surname": "Long",
        "given_name": "Jiefeng"
      },
      {
        "surname": "Xu",
        "given_name": "Suping"
      },
      {
        "surname": "Shang",
        "given_name": "Lin"
      }
    ]
  },
  {
    "title": "Performing Attack Halting Process with Digital Pattern and Proactive Model Resolving the Security Issues in IoT Based Models",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.11.009",
    "abstract": "The major development of the Internet of Things (IoT) has improved wireless technology in general and multiple sectors. Hardware security representations are crucial in maintaining the trustworthiness, credibility, and credibility of integrated circuits (ICs) and digital equipment. Phlashing attacks have just been termed Permanent Denial of Service (PDoS) attacks since they behave similarly to a standard DoS attack but still have long-term implications. To prevent a PDoS intrusion that would block all Internet connectivity and related functionalities on a computer. A phlashing is a cyber-attack where an entity intentionally blocks another individual basis team from accessing internet resources also including network communication and financing. This article aims to provide an analysis of cyber-attack avoidance is identified and ceased by worldwide records across the last year to analyze data and developments in cyber-attacks. Attacks can be tracked better as the same form of attack is performed again by using a database containing the attack signatures. Our study revealed, by using the Recurrent Neural Network (RNN) algorithm, that our system could provide adequate detection efficiency while still correcting the detectors whenever an intruder is observed.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521004001",
    "keywords": [
      "Attack model",
      "Block (permutation group theory)",
      "Computer network",
      "Computer science",
      "Computer security",
      "Credibility",
      "Denial-of-service attack",
      "Geometry",
      "Intrusion detection system",
      "Law",
      "Mathematics",
      "Operating system",
      "Political science",
      "Process (computing)",
      "Sybil attack",
      "The Internet",
      "Wireless sensor network",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Vijayakumar",
        "given_name": "M."
      },
      {
        "surname": "Shiny Angel",
        "given_name": "T.S."
      }
    ]
  },
  {
    "title": "Triplet interactive attention network for cross-modality person re-identification",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.10.010",
    "abstract": "Cross-modality (RGB-Infrared) person re-identification (ReID) has extreme superiority on low illumination ReID over conventional RGB-RGB matching due to its excellent capability on bridging cross-modality discrepancy. Previous works often focus on extracting modality-invariant global features, whereas the interaction among pairwise samples provides the most important clues for identifying persons. To explore the interactive clues among cross-modality pedestrian images, we propose a Triplet Interactive Attention Network (TIAN) to imitate the pairwise interactions and generate the attention scores for triplet samples. It can discover the fine-grained difference between pedestrian images and is optimized by successive attention losses to strengthen the feature learning capability of the backbone network. Meanwhile, the triplet interactive attention loss further alleviates the cross-modality discrepancy, based on the maximum mean discrepancy constraint. To demonstrate the effectiveness of our proposed TIAN, experiments on SYSU-MM01 and RegDB datasets show obvious superiority over popular cross-modality ReID methods, and the evaluation of main modules also reveals the novelties of our TIAN method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521003688",
    "keywords": [
      "Artificial intelligence",
      "Attention network",
      "Biology",
      "Botany",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Identification (biology)",
      "Linguistics",
      "Matching (statistics)",
      "Mathematics",
      "Modalities",
      "Modality (human–computer interaction)",
      "Pairwise comparison",
      "Pattern recognition (psychology)",
      "Philosophy",
      "RGB color model",
      "Social science",
      "Sociology",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Chenrui"
      },
      {
        "surname": "Chen",
        "given_name": "Ping"
      },
      {
        "surname": "Lei",
        "given_name": "Tao"
      },
      {
        "surname": "Meng",
        "given_name": "Hongying"
      }
    ]
  },
  {
    "title": "Deep neural networks ensemble to detect COVID-19 from CT scans",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108135",
    "abstract": "Research on Coronavirus Disease 2019 (COVID-19) detection methods has increased in the last months as more accurate automated toolkits are required. Recent studies show that CT scan images contain useful information to detect the COVID-19 disease. However, the scarcity of large and well balanced datasets limits the possibility of using detection approaches in real diagnostic contexts as they are unable to generalize. Indeed, the performance of these models quickly becomes inadequate when applied to samples captured in different contexts (e.g., different equipment or populations) from those used in the training phase. In this paper, a novel ensemble-based approach for more accurate COVID-19 disease detection using CT scan images is proposed. This work exploits transfer learning using pre-trained deep networks (e.g., VGG, Xception, and ResNet) evolved with a genetic algorithm, combined into an ensemble architecture for the classification of clustered images of lung lobes. The study is validated on a new dataset obtained as an integration of existing ones. The results of the experimental evaluation show that the ensemble classifier ensures effective performance, also exhibiting better generalization capabilities.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003228",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Classifier (UML)",
      "Computer science",
      "Computer security",
      "Coronavirus disease 2019 (COVID-19)",
      "Deep learning",
      "Disease",
      "Ensemble forecasting",
      "Ensemble learning",
      "Exploit",
      "Generalization",
      "Infectious disease (medical specialty)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Medicine",
      "Pathology",
      "Pattern recognition (psychology)",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Aversano",
        "given_name": "Lerina"
      },
      {
        "surname": "Bernardi",
        "given_name": "Mario Luca"
      },
      {
        "surname": "Cimitile",
        "given_name": "Marta"
      },
      {
        "surname": "Pecori",
        "given_name": "Riccardo"
      }
    ]
  },
  {
    "title": "Light gradient boosting machine-based phishing webpage detection model using phisher website features of mimic URLs",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.09.018",
    "abstract": "With the advent of the 20th century, the popularity of digital service usages is increasing every day. The internet has always been a popular communication method, and phishing webpages have been a challenging issue for more than two decades. Especially, E-commerce and other global companies face enormous challenges due to phishing of websites. Many developed countries have reported substantial economic loss due to unwanted phishing activities. With the exponential increase of digital communications, these phishing activities are going to be increased. There is a need for an effective intrinsic phishing detection technique. Phishing websites have some unique features by which they can be identified. In this research, a Light gradient boosting machine-based phishing email detection model using phisher websites' features of mimic URLs has been proposed. The primary objective is to develop a highly secured and accurate model for successful identification of security breach through websites phishing. With the performance comparison of other ensemble as well as state-of-the-art machine learning models, the proposed model resulted high performance accuracy and proved to a robust approach for phishing activity.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521003445",
    "keywords": [
      "Artificial intelligence",
      "Boosting (machine learning)",
      "Computer science",
      "Data mining",
      "Gradient boosting",
      "Information retrieval",
      "Pattern recognition (psychology)",
      "Phishing",
      "Random forest",
      "The Internet",
      "Web page",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Oram",
        "given_name": "Etuari"
      },
      {
        "surname": "Dash",
        "given_name": "Pandit Byomakesha"
      },
      {
        "surname": "Naik",
        "given_name": "Bighnaraj"
      },
      {
        "surname": "Nayak",
        "given_name": "Janmenjoy"
      },
      {
        "surname": "Vimal",
        "given_name": "S."
      },
      {
        "surname": "Nataraj",
        "given_name": "Sathees Kumar"
      }
    ]
  },
  {
    "title": "Weighted distances on the truncated hexagonal grid",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.09.015",
    "abstract": "Recently chamfer distances have been developed not only on the usual integer grids, but also on some non traditional grids including grids which are not lattices. In this paper the truncated hexagonal grid is considered: its pixels are dodecagons and two shaped (oriented) triangles. Two types of ‘natural’ neighborhood relations are considered on the grid, consequently two weights are used to describe the chamfer distances. Formulae to compute the minimal weights of a connecting path, i.e., the distance of any two pixels, are provided to cases depending on the relative ratio of the weights. Some properties of these distances, including metricity are also analysed. Digital disks based on the weighted distances are also investigated. In some cases, these disks may not be convex, moreover they may contain holes. The conditions of holes are characterised.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552100341X",
    "keywords": [
      "Artificial intelligence",
      "Chamfer (geometry)",
      "Chemistry",
      "Combinatorics",
      "Computer science",
      "Crystallography",
      "Geometry",
      "Grid",
      "Hexagonal crystal system",
      "Hexagonal tiling",
      "Integer (computer science)",
      "Mathematics",
      "Path (computing)",
      "Pixel",
      "Programming language",
      "Regular polygon"
    ],
    "authors": [
      {
        "surname": "Kovács",
        "given_name": "Gergely"
      },
      {
        "surname": "Nagy",
        "given_name": "Benedek"
      },
      {
        "surname": "Vizvári",
        "given_name": "Béla"
      }
    ]
  },
  {
    "title": "Explainable boosted linear regression for time series forecasting",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108144",
    "abstract": "Time series forecasting involves collecting and analyzing past observations to develop a model to extrapolate such observations into the future. Forecasting of future events is important in many fields to support decision making as it contributes to reducing the future uncertainty. We propose explainable boosted linear regression (EBLR) algorithm for time series forecasting, which is an iterative method that starts with a base model, and explains the model’s errors through regression trees. At each iteration, the path leading to highest error is added as a new variable to the base model. In this regard, our approach can be considered as an improvement over general time series models since it enables incorporating nonlinear features by residual explanation. More importantly, use of the single rule that contributes to the error most enables access to interpretable results. The proposed approach extends to probabilistic forecasting through generating prediction intervals based on the empirical error distribution. We conduct a detailed numerical study with EBLR and compare against various other approaches. We observe that EBLR substantially improves the base model performance through extracted features, and provide a comparable performance to other well established approaches. The interpretability of the model predictions and high predictive accuracy of EBLR makes it a promising method for time series forecasting.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003319",
    "keywords": [],
    "authors": [
      {
        "surname": "Ilic",
        "given_name": "Igor"
      },
      {
        "surname": "Görgülü",
        "given_name": "Berk"
      },
      {
        "surname": "Cevik",
        "given_name": "Mucahit"
      },
      {
        "surname": "Baydoğan",
        "given_name": "Mustafa Gökçe"
      }
    ]
  },
  {
    "title": "Automated delineation of corneal layers on OCT images using a boundary-guided CNN",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108158",
    "abstract": "Accurate segmentation of corneal layers depicted on optical coherence tomography (OCT) images is very helpful for quantitatively assessing and diagnosing corneal diseases (e.g., keratoconus and dry eye). In this study, we presented a novel boundary-guided convolutional neural network (CNN) architecture (BG-CNN) to simultaneously extract different corneal layers and delineate their boundaries. The developed BG-CNN architecture used three convolutional blocks to construct two network modules on the basis of the classical U-Net network. We trained and validated the network on a dataset consisting of 1,712 OCT images acquired on 121 subjects using a 10-fold cross-validation method. Our experiments showed an average dice similarity coefficient (DSC) of 0.9691, an intersection over union (IOU) of 0.9411, and a Hausdorff distance (HD) of 7.4423 pixels. Compared with several other classical networks, namely U-Net, Attention U-Net, Asymmetric U-Net, BiO-Net, CE-Net, CPFnte, M-Net, and Deeplabv3, on the same dataset, the developed network demonstrated a promising performance, suggesting its unique strength in segmenting corneal layers depicted on OCT images.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003459",
    "keywords": [
      "Artificial intelligence",
      "Boundary (topology)",
      "Cartography",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Cornea",
      "Geography",
      "Geometry",
      "Image (mathematics)",
      "Intersection (aeronautics)",
      "Keratoconus",
      "Mathematical analysis",
      "Mathematics",
      "Net (polyhedron)",
      "Optical coherence tomography",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Pixel",
      "Segmentation",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Lei"
      },
      {
        "surname": "Shen",
        "given_name": "Meixiao"
      },
      {
        "surname": "Chang",
        "given_name": "Qian"
      },
      {
        "surname": "Shi",
        "given_name": "Ce"
      },
      {
        "surname": "Chen",
        "given_name": "Yang"
      },
      {
        "surname": "Zhou",
        "given_name": "Yuheng"
      },
      {
        "surname": "Zhang",
        "given_name": "Yanchun"
      },
      {
        "surname": "Pu",
        "given_name": "Jiantao"
      },
      {
        "surname": "Chen",
        "given_name": "Hao"
      }
    ]
  },
  {
    "title": "ADCCF: Adaptive deep concatenation coder framework for visual question answering",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.10.028",
    "abstract": "Multimodal teaching activity faces significant problems in Visual Question Answering (VQA), which involves simultaneous comprehension with reduced performance fidelity. However, Conventional methods are employed for portrayal and queries in a defined manner, which fails to accomplish the required performance accuracy rate. For elucidating the excellent image and question representation, this paper suggests an Adaptive Deep Concatenated Coder Framework (ADCCF) that enrolls both the image and question attributes simultaneously with the optimized residual layer. The Coder Framework comprises of cascaded layers of Encoder-Decoder architecture, which captures rich, meaningful query characteristics and image details through the use of keywords employing significant object areas in the picture. ADCCF layer has an encoder segment that blueprints the self-recognition of queries in which questions are concatenated to limit the answers and decoder segment blueprints the commanded-recognition of images. The simulation results of ADCCF are tested with both the VQA datasets 1.0 and 2.0 and manifests an improved performance accuracy ratio of 72.45% for 1.0 dataset and 73.57% for 2.0 datasets, thus proving the reliability of the proposed framework.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521003883",
    "keywords": [
      "Algorithm",
      "Arithmetic",
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Concatenation (mathematics)",
      "Encoder",
      "Image (mathematics)",
      "Law",
      "Layer (electronics)",
      "Mathematics",
      "Natural language processing",
      "Object (grammar)",
      "Operating system",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Residual"
    ],
    "authors": [
      {
        "surname": "Manogaran",
        "given_name": "Gunasekaran"
      },
      {
        "surname": "Shakeel",
        "given_name": "P. Mohamed"
      },
      {
        "surname": "Burhanuddin",
        "given_name": "M.A."
      },
      {
        "surname": "Baskar",
        "given_name": "S"
      },
      {
        "surname": "Saravanan",
        "given_name": "Vijayalakshmi"
      },
      {
        "surname": "Crespo",
        "given_name": "Rubén González"
      },
      {
        "surname": "Martínez",
        "given_name": "Oscar Sanjuán"
      }
    ]
  },
  {
    "title": "Multi party secure data access management in cloud using user centric block chain data encryption",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.10.029",
    "abstract": "Cloud is the most dominant platform being used by different organizations towards providing service access to their users and customers. Services provided are intended to store and maintain data belongs to the organizations. Access restriction and data access management are identified as key issue at the point of storage management in cloud. The data security in cloud decides the Quality of Service (QoS) performance of the entire environment. Towards this, different approaches available to handle the storage management and access restriction issues. Still, the approaches are not up to the expected range of performance to maximize the QoS performance. Also, the methods faces higher data leakage, security issues as well as higher impact of tampering, which increases the requirement of block chains towards secure data access. To improve the performance, an efficient multi party secure data access management (MSDAM) is presented in this article. The method is focused to provide data handle in different parties which perform authentication on the data update requests. Also, the method distributes the key set and scheme set for different user at different session which is framed according to user centric features. The method initializes key sets and scheme sets to be distributed to the user in a least fact randomization model. Also, the method generates block chains for the user data which is encrypted using User Centric Block chain Encryption (UCBE) scheme where the key and schemes are selected with randomization technique. Similarly, the hash code has been generated using Distancing method. The proposed method improves the performance in all the QoS parameters of the cloud.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552100386X",
    "keywords": [
      "Access control",
      "Access management",
      "Authentication (law)",
      "Cloud computing",
      "Computer network",
      "Computer science",
      "Computer security",
      "Data security",
      "Encryption",
      "Key (lock)",
      "Key management",
      "Operating system"
    ],
    "authors": [
      {
        "surname": "S",
        "given_name": "Arulananth T"
      },
      {
        "surname": "M",
        "given_name": "Baskar"
      },
      {
        "surname": "V",
        "given_name": "Anbarasu"
      },
      {
        "surname": "R",
        "given_name": "Thiagarajan"
      },
      {
        "surname": "T",
        "given_name": "Rajendran"
      },
      {
        "surname": "A",
        "given_name": "Balaji"
      }
    ]
  },
  {
    "title": "Combined secure approach based on whale optimization to improve the data classification for data analytics",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.10.018",
    "abstract": "The data clustering technique plays a significant role for the process of analyzing the data in various fields such as, data mining, big data and image processing. As the health care domain needs various data processing to detecting and diagnosing the disease, in existing image and data mining helps to identify and diagnosis the disease specific to cancer as it need lot of attention for clustering those data with proper detection and accuracy. Apart from the categories of skin cancer types like breast cancer, blood cancer, skin cancer, etc., skin cancer is more complicated disease as it needs proper detection at the earlier stage and treatment. In this paper, we have proposed a combined approach of neural based K means approach and whale data classification-based skin cancer approach. In this approach, we have applied multi-layer (K-Means with whale optimization algorithm) data classification to detect the cluster region. Then whale approach with data classification helps to determine the mass density of user-based data cluster and also train the classified data to optimize along with the predominant features. Finally, this combined approach to classify the skin cancer with respect to segmentation, feature, optimization. This firefly optimization helps to reduce the detection error rate at the early stage along with data accuracy and sensitivity. Regarding the security aspects, the optimization algorithm is secure against DoS and DDoS to ensure data privacy and confidentiality based on the data accuracy and detection time parameters discussed. Our proposed method will be evaluated with the existing methods like K-Means with genetic; K-means with firefly optimization methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521003780",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Data classification",
      "Data mining",
      "Pattern recognition (psychology)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Sarada",
        "given_name": "B"
      },
      {
        "surname": "Vinayaka Murthy",
        "given_name": "M"
      },
      {
        "surname": "Udaya Rani",
        "given_name": "V"
      }
    ]
  },
  {
    "title": "GGAC: Multi-relational image gated GCN with attention convolutional binary neural tree for identifying disease with chest X-rays",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108113",
    "abstract": "Using medical images for disease identification is an important application in the medical field. Graph Convolutional Network (GCN) is proposed to model multi-relational image and generate more informative image representations. Recently, the relations between medical images are utilized to identify diseases. This paper proposes a Gated GCN with Attention Convolutional Binary Neural Tree (GGAC) for Multi-Relational Image Identifying Disease. GGAC extracts the discriminative features of the image, strengthen the ability to model medical images, understands images representation deeply and then well captures the multi-modal relation between images. Firstly, an Attention Convolutional Binary Neural Tree based on the attention mechanism is designed to extract the fine-grained features of the images, and use the attention conversion operation on the edge of the tree structure to enhance the network’s acquisition of key image features. Secondly, a Gated GCN is proposed to improve GCN performance by solving the problem of the weight distribution of different neighbors in the same-order neighborhood. Thirdly, a GCN propagation rule is used to transfer messages in multi-relational Graph and then solves the message passing problem of high-dimensional feature data in GCN. Finally, we verify GGAC on a multi-relational graph constructed on the Chest X-rays14. It can be seen from the experiment that overfitting and underfitting can be solved to a certain extent through the extraction and inference of the features of the multi-relational graph, and then GGAC has better performance than the state-of-the-art methods, and keeps good in model complexity.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003009",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Convolutional neural network",
      "Discriminative model",
      "Feature extraction",
      "Graph",
      "Histogram",
      "Image (mathematics)",
      "Local binary patterns",
      "Overfitting",
      "Pattern recognition (psychology)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Bing"
      },
      {
        "surname": "Kang",
        "given_name": "Yan"
      },
      {
        "surname": "Zhang",
        "given_name": "Lan"
      },
      {
        "surname": "Li",
        "given_name": "Hao"
      }
    ]
  },
  {
    "title": "Generalized error path algorithm",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108112",
    "abstract": "Model selection with cross validation (CV) is very popular in machine learning. However, CV with grid and other common search strategies cannot guarantee to find the model with minimum CV error, which is often the ultimate goal of model selection. Recently, various solution path algorithms have been proposed for several important learning algorithms including support vector classification, Lasso, and so on. However, they still do not guarantee to find the model with minimum CV error. In this paper, we first show that the solution paths produced by various algorithms have the property of piecewise linearity. Then, we prove that a large class of error (or loss) functions are piecewise constant, linear, or quadratic w.r.t. the regularization parameter, based on the solution path. Finally, we propose a new generalized error path algorithm (GEP), and prove that it will find the model with minimum CV error in a finite number of steps for the entire range of the regularization parameter. The experimental results on a variety of datasets not only confirm our theoretical findings, but also show that the best model with our GEP has better generalization error on the test data, compared to the grid search, manual search, and random search.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321002995",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Generalization",
      "Generalization error",
      "Geometry",
      "Hyperparameter optimization",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Model selection",
      "Path (computing)",
      "Piecewise",
      "Programming language",
      "Quadratic equation",
      "Regularization (linguistics)",
      "Search algorithm",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Gu",
        "given_name": "Bin"
      },
      {
        "surname": "Ling",
        "given_name": "Charles X."
      }
    ]
  },
  {
    "title": "Novel framework for multimodal biometric image authentication using visual share neural network",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.09.016",
    "abstract": "Over the years world is revolutionized with the advent of technology, the field of transmission too is made easier with the use of different technologies but security during transmission is still a threat. Visual cryptography (VC) is a cryptographic technique that encrypts visual information (pictures, text, etc.) where the decrypted content appears as a visual image. Visual cryptography enables to transmit images securely as well as maintains data confidentiality. The security of multimodal biometric data is a challenging task in the current world scenario as various domains are prone to more threats. In order to secure and authenticate multimodal biometric data a new framework is proposed. The proposed framework provides an optimal solution for securing and authenticating images during transmission. In this paper, three types of biometric inputs were resolute and taken as input: Fingerprint, Face and Iris images. A new shuffling approach which is based on a pixel element is used for creating shares for every plane. These shares are used in the decoding process for reconstruction. The three reconstructed images are passed through the bilateral filter in order to eliminate noise and preserve edges. Each biometric data (BD) utilizes different image segmentation technique: (1) Binary segmentation for finger-print image, (2) face segmentation for face image, and (3) Daugman's integrodifferential operational for iris image. Similarly, feature extraction modules are executed on three segmented images. These features extracted from the reconstructed images are used to train the Visual Sharing Neural Network (VSNN) along with Recurrent Neural Network- Bidirectional Long Short-Term Memory (RNN-BiLSTM) model. In the testing process, the input image is classified using VSNN and it checks for matching BD in the database before providing the authentication to individual. The simulation result, of the proposed module, gives the PSNR as 38.58, MSE as 9, and NCC as 1, 95% of accuracy, 100% of sensitivity, and 5% of error rate, 98.8% of identification rate, 0.45% of false acceptance rate, and 0.97% of false rejection rate.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521003421",
    "keywords": [
      "Artificial intelligence",
      "Authentication (law)",
      "Biometrics",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Cryptography",
      "Feature (linguistics)",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Secret sharing",
      "Segmentation",
      "Visual cryptography"
    ],
    "authors": [
      {
        "surname": "Gayathri",
        "given_name": "M."
      },
      {
        "surname": "Malathy",
        "given_name": "C."
      }
    ]
  },
  {
    "title": "Modulating scalable Gaussian processes for expressive statistical learning",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108121",
    "abstract": "For a learning task, Gaussian process (GP) is interested in learning the statistical relationship between inputs and outputs, since it offers not only the prediction mean but also the associated variability. The vanilla GP however is hard to learn complicated distribution with the property of, e.g., heteroscedastic noise, multi-modality and non-stationarity, from massive data due to the Gaussian marginal and the cubic complexity. To this end, this article studies new scalable GP paradigms including the non-stationary heteroscedastic GP, the mixture of GPs and the latent GP, which introduce additional latent variables to modulate the outputs or inputs in order to learn richer, non-Gaussian statistical representation. Particularly, we resort to different variational inference strategies to arrive at analytical or tighter evidence lower bounds (ELBOs) of the marginal likelihood for efficient and effective model training. Extensive numerical experiments against state-of-the-art GP and neural network (NN) counterparts on various tasks verify the superiority of these scalable modulated GPs, especially the scalable latent GP, for learning diverse data distributions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003083",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Database",
      "Gaussian",
      "Gaussian process",
      "Heteroscedasticity",
      "Inference",
      "Latent variable",
      "Law",
      "Machine learning",
      "Marginal distribution",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Political science",
      "Politics",
      "Quantum mechanics",
      "Random variable",
      "Representation (politics)",
      "Scalability",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Haitao"
      },
      {
        "surname": "Ong",
        "given_name": "Yew-Soon"
      },
      {
        "surname": "Jiang",
        "given_name": "Xiaomo"
      },
      {
        "surname": "Wang",
        "given_name": "Xiaofang"
      }
    ]
  },
  {
    "title": "Real-time and light-weighted unsupervised video object segmentation network",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108120",
    "abstract": "Video object segmentation is one of the most practical computer vision tasks, especially in the unsupervised case, which has no manually labeled segmentation mask at the beginning of a video sequence. In this paper, we propose a new real-time unsupervised video object segmentation network. Based on the encoder-decoder framework, we present a Dynamic ASPP module and a RNN-Conv module. The former adds a dynamic selection mechanism into the Astrous Spatial Pyramid Pooling structure, and then the dilated convolutional kernels adaptively select appropriate features according to the scales by the channel attention mechanism. Compared with directly concatenating the dilated convolutional features, dynamically selecting feature maps reduces the amount of parameters and makes the module more efficient. The RNN-Conv module incorporates the RNN units with external convolutional blocks, aggregating the temporal features of a video sequence with the spatial information extracted by the convolutional network. We stack this module to extract deeper spatiotemporal features than the traditional RNN network. This module helps to avoid the gradient disappearance and explosion during network training. We test our network on the popular video object segmentation datasets. The experiment results demonstrate the effectiveness of our model. 1 1 Our code is available at https://github.com/Sanyuan-Zhao/Real-Time-and-Light-Weighted-UVOS",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003071",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Encoder",
      "Feature (linguistics)",
      "Linguistics",
      "Object (grammar)",
      "Operating system",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Pooling",
      "Pyramid (geometry)",
      "Recurrent neural network",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Zongji"
      },
      {
        "surname": "Zhao",
        "given_name": "Sanyuan"
      },
      {
        "surname": "Shen",
        "given_name": "Jianbing"
      }
    ]
  },
  {
    "title": "Community-based k -shell decomposition for identifying influential spreaders",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108130",
    "abstract": "How to identify the most influential nodes in a network for the maximization of influence spread is a great challenge. Known methods like k -shell decomposition determine core nodes who individually might be the most influential spreaders for the spreading originating in a single origin. However, these techniques are not suitable for determining multiple origins that together lead to the most effective spreading. The reason is that core nodes are often found to be located closely to each other, which results in large overlapping regions rather than spreading far across the network. In this paper, we propose a new algorithm, called community-based k -shell decomposition, by which a network can be viewed as multiple hierarchically ordered structures each branching off from the innermost shell to the periphery shell. To alleviate the overlap problem, our algorithm pursues a greedy strategy that preferably selects core nodes from different communities in the network, thus maximizing the joint influence of multiple origins. We systematically evaluate our algorithm against competing algorithms on multiple networks with varying network characteristics, and find that our algorithm outperforms other algorithms on networks that exhibit community structures, and the stronger communities, the better performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003174",
    "keywords": [
      "Algorithm",
      "Approximation algorithm",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Core (optical fiber)",
      "Decomposition",
      "Ecology",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Sun",
        "given_name": "Peng Gang"
      },
      {
        "surname": "Miao",
        "given_name": "Qiguang"
      },
      {
        "surname": "Staab",
        "given_name": "Steffen"
      }
    ]
  },
  {
    "title": "Spatial reasoning for few-shot object detection",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108118",
    "abstract": "Although modern object detectors rely heavily on a significant amount of training data, humans can easily detect novel objects using a few training examples. The mechanism of the human visual system is to interpret spatial relationships among various objects and this process enables us to exploit contextual information by considering the co-occurrence of objects. Thus, we propose a spatial reasoning framework that detects novel objects with only a few training examples in a context. We infer geometric relatedness between novel and base RoIs (Region-of-Interests) to enhance the feature representation of novel categories using an object detector well trained on base categories. We employ a graph convolutional network as the RoIs and their relatedness are defined as nodes and edges, respectively. Furthermore, we present spatial data augmentation to overcome the few-shot environment where all objects and bounding boxes in an image are resized randomly. Using the PASCAL VOC and MS COCO datasets, we demonstrate that the proposed method significantly outperforms the state-of-the-art methods and verify its efficacy through extensive ablation studies.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003058",
    "keywords": [
      "Anomaly detection",
      "Artificial intelligence",
      "Bounding overwatch",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Convolutional neural network",
      "Exploit",
      "Graph",
      "Image (mathematics)",
      "Minimum bounding box",
      "Object (grammar)",
      "Object detection",
      "Pascal (unit)",
      "Pattern recognition (psychology)",
      "Programming language",
      "Spatial contextual awareness",
      "Spatial intelligence",
      "Spatial relation",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Kim",
        "given_name": "Geonuk"
      },
      {
        "surname": "Jung",
        "given_name": "Hong-Gyu"
      },
      {
        "surname": "Lee",
        "given_name": "Seong-Whan"
      }
    ]
  },
  {
    "title": "Learnable low-rank latent dictionary for subspace clustering",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108142",
    "abstract": "Recently, Self-Expressive-based Subspace Clustering (SESC) has been widely applied in pattern clustering and machine learning as it aims to learn a representation that can faithfully reflect the correlation between data points. However, most existing SESC methods directly use the original data as the dictionary, which miss the intrinsic structure (e.g., low-rank and nonlinear) of the real-word data. To address this problem, we propose a novel Projection Low-Rank Subspace Clustering (PLRSC) method by integrating feature extraction and subspace clustering into a unified framework. In particular, PLRSC learns a projection transformation to extract the low-dimensional features and utilizes a low-rank regularizer to ensure the informative and important structures of the extracted features. The extracted low-rank features effectively enhance the self-expressive property of the dictionary. Furthermore, we extend PLRSC to a nonlinear version (i.e., NPLRSC) by integrating a nonlinear activator into the projection transformation. NPLRSC cannot only effectively extract features but also guarantee the data structure of the extracted features. The corresponding optimization problem is solved by the Alternating Direction Method (ADM), and we also prove that the algorithm converges to a stationary point. Experimental results on the real-world datasets validate the superior of our model over the existing subspace clustering methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003290",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Cluster analysis",
      "Clustering high-dimensional data",
      "Combinatorics",
      "Computer science",
      "Data point",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Projection (relational algebra)",
      "Rank (graph theory)",
      "Subspace topology"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Yesong"
      },
      {
        "surname": "Chen",
        "given_name": "Shuo"
      },
      {
        "surname": "Li",
        "given_name": "Jun"
      },
      {
        "surname": "Luo",
        "given_name": "Lei"
      },
      {
        "surname": "Yang",
        "given_name": "Jian"
      }
    ]
  },
  {
    "title": "DQN-based gradual fisheye image rectification",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.08.025",
    "abstract": "Fisheye image rectification is a classic and important task in the computer vision area, which is generally treated as a pre-processing step in many application scenarios. Most of the existing fisheye image rectification methods focused mainly on building a direct (one-step) projection relationship between fisheye images and corrected images. Although these methods have achieved impressive performance, they depended heavily on data distribution and cannot work well on images whose distortion parameters are out of range. To address this issue, we propose a multi-step gradual image rectification scheme. In particular, we treat the fisheye image rectification problem as one Markov Decision Process and employ a widely-used deep reinforcement learning method (i.e., Deep Q-Network) to solve the problem. In addition, we build a new large-scale fisheye image dataset to evaluate our method, which contains 50,000 training images and 5000 test images. Experimental results show the proposed method can better handle images with a wide range of distortion.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521003299",
    "keywords": [
      "Algorithm",
      "Amplifier",
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Composite material",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Distortion (music)",
      "Image (mathematics)",
      "Image rectification",
      "Materials science",
      "Operating system",
      "Physics",
      "Power (physics)",
      "Process (computing)",
      "Projection (relational algebra)",
      "Quantum mechanics",
      "Range (aeronautics)",
      "Rectification"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Jie"
      },
      {
        "surname": "Wei",
        "given_name": "Shikui"
      },
      {
        "surname": "Liao",
        "given_name": "Lixin"
      },
      {
        "surname": "Zhao",
        "given_name": "Yao"
      }
    ]
  },
  {
    "title": "Learning graph edit distance by graph neural networks",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108132",
    "abstract": "The emergence of geometric deep learning as a novel framework to deal with graph-based representations has faded away traditional approaches in favor of completely new methodologies. In this paper, we propose a new framework able to combine the advances on deep metric learning with traditional approximations of the graph edit distance. Hence, we propose an efficient graph distance based on the novel field of geometric deep learning. Our method employs a message passing neural network to capture the graph structure, and thus, leveraging this information for its use on a distance computation. The performance of the proposed graph distance is validated on two different scenarios. On the one hand, in a graph retrieval of handwritten words i.e. keyword spotting, showing its superior performance when compared with (approximate) graph edit distance benchmarks. On the other hand, demonstrating competitive results for graph similarity learning when compared with the current state-of-the-art on a recent benchmark dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003198",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computation",
      "Computer science",
      "Deep learning",
      "Distance",
      "Edit distance",
      "Graph",
      "Shortest path problem",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Riba",
        "given_name": "Pau"
      },
      {
        "surname": "Fischer",
        "given_name": "Andreas"
      },
      {
        "surname": "Lladós",
        "given_name": "Josep"
      },
      {
        "surname": "Fornés",
        "given_name": "Alicia"
      }
    ]
  },
  {
    "title": "COLI: Collaborative clustering missing data imputation",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.11.011",
    "abstract": "Missing data imputation plays an important role in the data cleansing process. Clustering algorithms have been widely used for missing data imputation, yet, there is little research done on the use of clustering ensemble for missing data imputation, which aggregates multiple clustering results. This paper proposes a novel collaborative clustering-based imputation method, called COLI, which uses the imputation quality as a key criterion for the exchange of information between different clustering results. To the best of our knowledge, this is the first study on the impact of collaborative clustering on imputation performance. The main contributions of this paper are three-fold. A novel missing value imputation based on collaborative clustering is proposed, three amputation strategies are used to induce missingness on various complete and publicly available datasets with different mechanisms, distributions, and ratios, which allows evaluating the imputation quality of the proposed method in estimating missing values of various numerical datasets with different missingness mechanisms, distributions, and ratios. The proposed method is compared to several state-of-the-art imputation methods and attained results demonstrate that the proposed method is an effective method for handling missing data.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521004025",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Imputation (statistics)",
      "Machine learning",
      "Missing data"
    ],
    "authors": [
      {
        "surname": "Wan",
        "given_name": "Daoming"
      },
      {
        "surname": "Razavi-Far",
        "given_name": "Roozbeh"
      },
      {
        "surname": "Saif",
        "given_name": "Mehrdad"
      },
      {
        "surname": "Mozafari",
        "given_name": "Niloofar"
      }
    ]
  },
  {
    "title": "Self-Fusion Convolutional Neural Networks",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.08.022",
    "abstract": "Efficiency is an important concern for practical applications, therefore, it is of great importance to build effective lightweight networks. This paper proposes a novel lightweight feature self-fusion convolutional (SFC) module, which consists of self-fusion and point-wise convolution. The core of SFC is a three-step self-fusion. First, each input feature map is expanded to a high dimensional space individually, prohibiting connections with other input channels. Then, in the second step, we fuse all features from the same input in the high dimensional space to enhance the representation ability. Finally, we compress high dimensional features to a low dimensional space. After self-fusion, we connect all features by one point-wise convolution. Compared to inverted bottleneck, SFC module decreases the number of parameters by replacing the dense connections among channels with self-fusion. To the best of our knowledge, SFC is the first method to build lightweight networks by feature self-fusion. We then build a new network namely SFC-Net, by stacking SFC modules. Experimental results on the CIFAR and downsampled ImageNet datasets demonstrate our SFC-Net achieves better performance to some previous popular CNNs with fewer number of parameters and achieves comparable performance compared to other previous lightweight architectures. The code is available at https://github.com/Yankeegsj/Self-fusion.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521003160",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Bottleneck",
      "Code (set theory)",
      "Computer science",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Electrical engineering",
      "Embedded system",
      "Engineering",
      "Feature (linguistics)",
      "Fuse (electrical)",
      "Fusion",
      "Fusion rules",
      "Geometry",
      "Image (mathematics)",
      "Image fusion",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Point (geometry)",
      "Programming language",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Gong",
        "given_name": "Shenjian"
      },
      {
        "surname": "Zhang",
        "given_name": "Shanshan"
      },
      {
        "surname": "Yang",
        "given_name": "Jian"
      },
      {
        "surname": "Yuen",
        "given_name": "Pong Chi"
      }
    ]
  },
  {
    "title": "Manifold learning with structured subspace for multi-label feature selection",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108169",
    "abstract": "Nowadays, multi-label learning is ubiquitous in practical applications, in which multi-label data is always confronted with the curse of high-dimensional features. Feature selection has been shown to effectively improve learning performance by selecting discriminative features. Conventional multi-label feature selection only focuses on associating input features with corresponding labels while neglecting the potential structural information, i.e., instance correlations and label correlations. To tackle this problem, we propose manifold learning with structured subspace for multi-label feature selection. Specifically, we first uncover a latent subspace for a more compact and accurate data representation, and take advantage of the subspace to explore the correlations among instances. Then, we explore label correlations in manifold learning to guarantee the global and local structural consistency of labels. Besides, l 2 , 1 -norm is introduced into loss function and sparse regularization to facilitate feature selection process. A detail optimization algorithm is presented to solve the objective function of the proposed method. Extensive experiments on real-world data show the superiority of the proposed method under various metrics.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003563",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Curse of dimensionality",
      "Dimensionality reduction",
      "Discriminative model",
      "Feature (linguistics)",
      "Feature learning",
      "Feature selection",
      "Linguistics",
      "Machine learning",
      "Nonlinear dimensionality reduction",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Subspace topology"
    ],
    "authors": [
      {
        "surname": "Fan",
        "given_name": "Yuling"
      },
      {
        "surname": "Liu",
        "given_name": "Jinghua"
      },
      {
        "surname": "Liu",
        "given_name": "Peizhong"
      },
      {
        "surname": "Du",
        "given_name": "Yongzhao"
      },
      {
        "surname": "Lan",
        "given_name": "Weiyao"
      },
      {
        "surname": "Wu",
        "given_name": "Shunxiang"
      }
    ]
  },
  {
    "title": "Improving the accuracy of global forecasting models using time series data augmentation",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108148",
    "abstract": "Forecasting models that are trained across sets of many time series, known as Global Forecasting Models (GFM), have shown recently promising results in forecasting competitions and real-world applications, outperforming many state-of-the-art univariate forecasting techniques. In most cases, GFMs are implemented using deep neural networks, and in particular Recurrent Neural Networks (RNN), which require a sufficient amount of time series to estimate their numerous model parameters. However, many time series databases have only a limited number of time series. In this study, we propose a novel, data augmentation based forecasting framework that is capable of improving the baseline accuracy of the GFM models in less data-abundant settings. We use three time series augmentation techniques: GRATIS, moving block bootstrap (MBB), and dynamic time warping barycentric averaging (DBA) to synthetically generate a collection of time series. The knowledge acquired from these augmented time series is then transferred to the original dataset using two different approaches: the pooled approach and the transfer learning approach. When building GFMs, in the pooled approach, we train a model on the augmented time series alongside the original time series dataset, whereas in the transfer learning approach, we adapt a pre-trained model to the new dataset. In our evaluation on competition and real-world time series datasets, our proposed variants can significantly improve the baseline accuracy of GFM models and outperform state-of-the-art univariate forecasting methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003356",
    "keywords": [],
    "authors": [
      {
        "surname": "Bandara",
        "given_name": "Kasun"
      },
      {
        "surname": "Hewamalage",
        "given_name": "Hansika"
      },
      {
        "surname": "Liu",
        "given_name": "Yuan-Hao"
      },
      {
        "surname": "Kang",
        "given_name": "Yanfei"
      },
      {
        "surname": "Bergmeir",
        "given_name": "Christoph"
      }
    ]
  },
  {
    "title": "Correlation-based structural dropout for convolutional neural networks",
    "journal": "Pattern Recognition",
    "year": "2021",
    "doi": "10.1016/j.patcog.2021.108117",
    "abstract": "Convolutional neural networks (CNNs) easily suffer from the over-fitting problem since they are often over-parameterized in the case of small training datasets. The conventional dropout that drops feature units randomly works well for fully connected networks, but fails to regularize CNNs well due to high spatial correlation of the intermediate features, which allows the dropped information to flow through the network, thus leading to the problem of under-dropping. To better regularize CNNs, some structural dropout methods such as SpatialDropout and DropBlock have been proposed by dropping feature units in continuous regions randomly. However, these methods may suffer from the over-dropping problem by discarding the critical discriminative features, thus limiting the performance of CNNs. To address these issues, we propose a novel structural dropout method, Correlation based Dropout (CorrDrop), to regularize CNNs by dropping feature units based on feature correlation. Unlike the previous dropout methods, our CorrDrop can focus on the discriminative information and drops features in a spatial-wise or channel-wise manner. Extensive experiments on different datasets, network architectures, and various tasks (e.g., image classification and object localization) demonstrate the superiority of our method over other methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003046",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Correlation",
      "Discriminative model",
      "Dropout (neural networks)",
      "Feature (linguistics)",
      "Focus (optics)",
      "Geometry",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Optics",
      "Parameterized complexity",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics"
    ],
    "authors": [
      {
        "surname": "Zeng",
        "given_name": "Yuyuan"
      },
      {
        "surname": "Dai",
        "given_name": "Tao"
      },
      {
        "surname": "Chen",
        "given_name": "Bin"
      },
      {
        "surname": "Xia",
        "given_name": "Shu-Tao"
      },
      {
        "surname": "Lu",
        "given_name": "Jian"
      }
    ]
  },
  {
    "title": "Deformable scene text detection using harmonic features and modified pixel aggregation network",
    "journal": "Pattern Recognition Letters",
    "year": "2021",
    "doi": "10.1016/j.patrec.2021.10.006",
    "abstract": "Although text detection methods have addressed several challenges in the past, there is a dearth of effective methods for text detection in deformable images, such as images containing text embedded on cloth, banners, rubber, sports jerseys, uniforms, etc. This is because deformable regions contain surfaces of arbitrarily shapes, which lead to poor text quality. This paper presents a new method for deformable text detection in natural scene images. It is observed that although the shapes of characters change in a deformable region, the pixel values and spatial relationship between the pixels do not change. This motivated us to explore extraction of Maximally Stable Extremal Regions (MSER) in an image in which pixels that share common features are grouped into components. The unique character shape variations led us to explore harmonic features to represent the component shape variations, using which a classifier classifies text and non-text components from the output of the MSER step. Additionally, the objective of developing a lightweight method with low computational cost motivated us to introduce a modified Pixel Aggression Network (PAN) for text deformable text detection at a component level. Comprehensive experiments which include experiments on our Deformable Text Dataset (DTD) and standard natural scene text datasets, namely, MSRATD-500, ICDAR 2019 MLT, Total-Text, CTW1500, ICDAR 2019 ArT and DSTA1500 datasets show that the proposed model outperforms the existing methods for our dataset as well as the standard datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521003640",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Computer vision",
      "Image (mathematics)",
      "Pattern recognition (psychology)",
      "Pixel",
      "Text detection"
    ],
    "authors": [
      {
        "surname": "Jain",
        "given_name": "Tanmay"
      },
      {
        "surname": "Palaiahnakote",
        "given_name": "Shivakumara"
      },
      {
        "surname": "Pal",
        "given_name": "Umapada"
      },
      {
        "surname": "Liu",
        "given_name": "Cheng-Lin"
      }
    ]
  }
]