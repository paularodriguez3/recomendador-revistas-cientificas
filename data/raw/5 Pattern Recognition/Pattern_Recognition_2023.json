[
  {
    "title": "Query-guided networks for few-shot fine-grained classification and person search",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109049",
    "abstract": "Few-shot fine-grained classification and person search appear as distinct tasks and literature has treated them separately. But a closer look unveils important similarities: both tasks target categories that can only be discriminated by specific object details; and the relevant models should generalize to new categories, not seen during training. We propose a novel unified Query-Guided Network (QGN) applicable to both tasks. QGN consists of a Query-guided Siamese-Squeeze-and-Excitation subnetwork which re-weights both the query and gallery features across all network layers, a Query-guided Region Proposal subnetwork for query-specific localisation, and a Query-guided Similarity subnetwork for metric learning. QGN improves on a few recent few-shot fine-grained datasets, outperforming other techniques on CUB by a large margin. QGN also performs competitively on the person search CUHK-SYSU and PRW datasets, where we perform in-depth analysis.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322005295",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Computer security",
      "Data mining",
      "Economics",
      "Engineering",
      "Image (mathematics)",
      "Information retrieval",
      "Machine learning",
      "Margin (machine learning)",
      "Mechanical engineering",
      "Metric (unit)",
      "Nearest neighbor search",
      "Object (grammar)",
      "One shot",
      "Operations management",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Query expansion",
      "Shot (pellet)",
      "Similarity (geometry)",
      "Subnetwork"
    ],
    "authors": [
      {
        "surname": "Munjal",
        "given_name": "Bharti"
      },
      {
        "surname": "Flaborea",
        "given_name": "Alessandro"
      },
      {
        "surname": "Amin",
        "given_name": "Sikandar"
      },
      {
        "surname": "Tombari",
        "given_name": "Federico"
      },
      {
        "surname": "Galasso",
        "given_name": "Fabio"
      }
    ]
  },
  {
    "title": "Towards prior gap and representation gap for long-tailed recognition",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109012",
    "abstract": "Most deep learning models are elaborately designed for balanced datasets, and thus they inevitably suffer performance degradation in practical long-tailed recognition tasks, especially to the minority classes. There are two crucial issues in learning from imbalanced datasets: skew decision boundary and unrepresentative feature space. In this work, we establish a theoretical framework to analyze the sources of these two issues from Bayesian perspective, and find that they are closely related to the prior gap and the representation gap, respectively. Under this framework, we show that existing long-tailed recognition methods manage to remove either the prior gap or the presentation gap. Different from these methods, we propose to simultaneously remove the two gaps to achieve more accurate long-tailed recognition. Specifically, we propose the prior calibration strategy to remove the prior gap and introduce three strategies (representative feature extraction, optimization strategy adjustment and effective sample modeling) to mitigate the representation gap. Extensive experiments on five benchmark datasets validate the superiority of our method against the state-of-the-art competitors.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322004927",
    "keywords": [
      "Artificial intelligence",
      "Bayesian probability",
      "Benchmark (surveying)",
      "Computer science",
      "Feature (linguistics)",
      "Feature learning",
      "Geodesy",
      "Geography",
      "Law",
      "Linguistics",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Perspective (graphical)",
      "Philosophy",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Skew",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Ming-Liang"
      },
      {
        "surname": "Zhang",
        "given_name": "Xu-Yao"
      },
      {
        "surname": "Wang",
        "given_name": "Chuang"
      },
      {
        "surname": "Liu",
        "given_name": "Cheng-Lin"
      }
    ]
  },
  {
    "title": "Disentangling the correlated continuous and discrete generative factors of data",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109055",
    "abstract": "Real-world data typically include discrete generative factors, such as category labels and the existence of objects, as well as continuous generative factors. Continuous generative factors may be dependent on or independent of discrete generative factors. For instance, an intra-class variation of a category is dependent on the discrete generative factor, whereas a common variation of all categories is not. Most previous attempts to integrate discrete generative factors into disentanglement assumed statistical independence between the continuous and discrete variables. In this paper, we propose a Variational Autoencoder(VAE) model capable of disentangling both continuous generative factors. To represent these generative factors, we introduce two sets of continuous latent variables: a private variable and a public variable. The private and public variables represent the intra-class variations and common variations in categories, respectively. Our proposed framework models the private variable as a Gaussian mixture and the public variable as a Gaussian. Each mode of the private variable is responsible for a class of discrete variables. Our proposed model, called Discond-VAE, DISentangles the class-dependent CONtinuous factors from the Discrete factors by introducing private variables. The experiments showed that Discond-VAE could discover private and public factors from the data. Moreover, even under the dataset with only public factors, Discond-VAE does not fail and adapts private variables to represent public factors.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322005350",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Autoencoder",
      "Class (philosophy)",
      "Computer science",
      "Econometrics",
      "Gaussian",
      "Generative grammar",
      "Generative model",
      "Latent variable",
      "Mathematical analysis",
      "Mathematics",
      "Physics",
      "Quantum mechanics",
      "Statistics",
      "Variable (mathematics)",
      "Variables"
    ],
    "authors": [
      {
        "surname": "Choi",
        "given_name": "Jaewoong"
      },
      {
        "surname": "Hwang",
        "given_name": "Geonho"
      },
      {
        "surname": "Kang",
        "given_name": "Myungjoo"
      }
    ]
  },
  {
    "title": "LAE-Net: A locally-adaptive embedding network for low-light image enhancement",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109039",
    "abstract": "In the low-light enhancement task, one of the major challenges lies in how to balance the image enhancement properties of light intensity, detail presentation and color fidelity. In natural scenes, the multi-distribution of frequency and illumination characteristics in the spatial domain makes the balance more difficult. To solve this problem, we propose a Locally-Adaptive Embedding Network, namely LAE-Net, to realize high-quality low-light image enhancement with locally-adaptive kernel selection and feature adaptation for multi-distribution issues. Specifically, for the frequency multi-distribution, we rethink the spatial-frequency characteristic of human eyes, experimentally explore the relationship among the receptive field size, the image spatial frequency and the light enhancement properties, and propose an Entropy-Inspired Kernel-Selection Convolution, where each neuron can adaptively adjust the receptive field size according to its spatial frequency characterized by information entropy. For the illumination multi-distribution, we propose an Illumination Attentive Transfer subnet, where the neurons can simultaneously sense global consistency and local details, and accordingly hint where to focus the efforts on, thereby adjusting the refined features. Extensive experiments with ablation analysis show the effectiveness of our method and the proposed method outperforms many related state-of-the-art techniques on four benchmark datasets: MEF, LIME, NPE and DICM.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322005192",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Embedding",
      "Entropy (arrow of time)",
      "Fidelity",
      "Light field",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Spatial frequency",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Xiaokai"
      },
      {
        "surname": "Ma",
        "given_name": "Weihao"
      },
      {
        "surname": "Ma",
        "given_name": "Xiaorui"
      },
      {
        "surname": "Wang",
        "given_name": "Jie"
      }
    ]
  },
  {
    "title": "Conditional GAN with 3D discriminator for MRI generation of Alzheimer’s disease progression",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109061",
    "abstract": "Many studies aim to predict the degree of deformation on affected brain regions as Alzheimer’s disease (AD) progresses. However, those studies have been often limited since it is difficult to obtain sequential longitudinal MR data of affected patients. Recently, conditional generative adversarial networks (cGANs) have been used to estimate the changes between unpaired images by modeling their differences. However, generating high-quality 3D magnetic resonance (MR) brain images with cGANs requires a large amount of computation. Previous models have been mostly designed to operate in 2D space taking individual slices or down-sampled 3D space, but these approaches often cause spatial artifacts such as discontinuities between slices or unnatural changes in 3D space. To address these limitations, we propose a novel cGAN that can synthesize high-quality 3D MR images at different stages of AD by integrating an additional module that ensures smooth and realistic transitions in 3D space. Specifically, the proposed cGAN model consists of an attention-based 2D generator, a 2D discriminator, and a 3D discriminator that is able to synthesize continuous 2D slices along the axial view resulting in good quality 3D MR volumes. Moreover, we propose an adaptive identity loss so that relevant transformations take place without compromising the features to identify patients. In our experiments, the proposed method showed better image generation performance than previously proposed GAN methods in terms of image quality and image generation suitable for the condition.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322005416",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Classification of discontinuities",
      "Computation",
      "Computer science",
      "Computer vision",
      "Detector",
      "Discriminator",
      "Generator (circuit theory)",
      "Image (mathematics)",
      "Image quality",
      "Mathematical analysis",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Physics",
      "Power (physics)",
      "Quantum mechanics",
      "Space (punctuation)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Jung",
        "given_name": "Euijin"
      },
      {
        "surname": "Luna",
        "given_name": "Miguel"
      },
      {
        "surname": "Park",
        "given_name": "Sang Hyun"
      }
    ]
  },
  {
    "title": "Scene table structure recognition with segmentation collaboration and alignment",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2022.12.014",
    "abstract": "This study addresses the challenge of scene table structure recognition (S-TSR). In contrast to the well-aligned tables in PDF documents or screenshots, natural scene tables tend to be inclined, rotated, and curved, thereby damaging the structure of the table. Consequently, many state-of-the-art methods cannot operate effectively owing to a lack of well-aligned priors. We propose a novel segmentation collaboration and alignment network (SCAN) to address the problem of table structure recognition in natural scenarios. Our SCAN combines the location and logical information of table cells through the segmentation collaboration module to segment the cell region accurately. The proposed cell alignment module aligns the segmentation result to restore the distorted table structure. The experimental results demonstrate that our method is robust to different challenging S-TSR sub-scenarios and achieves new state-of-the-art performance, with TEDS scores of 90.7 and 98.5 on the S-TSR benchmarks WTW and TAL_OCR_TABLE, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522003828",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Pattern recognition (psychology)",
      "Segmentation",
      "Table (database)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Hongyi"
      },
      {
        "surname": "Xue",
        "given_name": "Yang"
      },
      {
        "surname": "Zhang",
        "given_name": "Jiaxin"
      },
      {
        "surname": "Jin",
        "given_name": "Lianwen"
      }
    ]
  },
  {
    "title": "Self-regularized prototypical network for few-shot semantic segmentation",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109018",
    "abstract": "The deep CNNs in image semantic segmentation typically require a large number of densely-annotated images for training and have difficulties in generalizing to unseen object categories. Therefore, few-shot segmentation has been developed to perform segmentation with just a few annotated examples. In this work, we tackle the few-shot segmentation using a self-regularized prototypical network (SRPNet) based on prototype extraction for better utilization of the support information. The proposed SRPNet extracts class-specific prototype representations from support images and generates segmentation masks for query images by a distance metric - the fidelity. A direct yet effective prototype regularization on support set is proposed in SRPNet, in which the generated prototypes are evaluated and regularized on the support set itself. The extent to which the generated prototypes restore the support mask imposes an upper limit on performance. The performance on the query set should never exceed the upper limit no matter how complete the knowledge is generalized from support set to query set. With the specific prototype regularization, SRPNet fully exploits knowledge from the support and offers high-quality prototypes that are representative for each semantic class and meanwhile discriminative for different classes. The query performance is further improved by an iterative query inference (IQI) module that combines a set of regularized prototypes. Our proposed SRPNet achieves new state-of-art performance on 1-shot and 5-shot segmentation benchmarks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322004988",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Discriminative model",
      "Economics",
      "Exploit",
      "Image segmentation",
      "Inference",
      "Metric (unit)",
      "Operations management",
      "Pattern recognition (psychology)",
      "Programming language",
      "Regularization (linguistics)",
      "Scale-space segmentation",
      "Segmentation",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Ding",
        "given_name": "Henghui"
      },
      {
        "surname": "Zhang",
        "given_name": "Hui"
      },
      {
        "surname": "Jiang",
        "given_name": "Xudong"
      }
    ]
  },
  {
    "title": "CSR: Cascade Conditional Variational Auto Encoder with Socially-aware Regression for Pedestrian Trajectory Prediction",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109030",
    "abstract": "Pedestrian trajectory prediction is a key technology in many real applications such as video surveillance, social robot navigation, and autonomous driving, and significant progress has been made in this research topic. However, there remain two limitations of previous studies. First, the losses of the last time steps are heavier weighted than that of the beginning time steps in the objective function at the learning stage, causing the prediction errors generated at the beginning to accumulate to large errors at the last time steps at the inference stage. Second, the prediction results of multiple pedestrians in the prediction horizon might be socially incompatible with the interactions modeled by past trajectories. To overcome these limitations, this work proposes a novel trajectory prediction method called CSR, which consists of a cascaded conditional variational autoencoder (CVAE) module and a socially-aware regression module. The CVAE module estimates the future trajectories in a cascaded sequential manner. Specifically, each CVAE concatenates the past trajectories and the predicted location points so far as the input and predicts the adjacent location at the following time step. The socially-aware regression module generates offsets from the estimated future trajectories to produce the corrected predictions, which are more reasonable and accurate than the estimated trajectories. Experiments results demonstrate that the proposed method exhibits significant improvements over state-of-the-art methods on the Stanford Drone Dataset (SDD) and the ETH/UCY dataset of approximately 38.0% and 22.2%, respectively. The code is available at https://github.com/zhouhao94/CSR.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322005106",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Astronomy",
      "Autoencoder",
      "Code (set theory)",
      "Computer science",
      "Computer security",
      "Deep learning",
      "Encoder",
      "Inference",
      "Key (lock)",
      "Machine learning",
      "Mathematics",
      "Operating system",
      "Physics",
      "Programming language",
      "Regression",
      "Set (abstract data type)",
      "Statistics",
      "Trajectory"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Hao"
      },
      {
        "surname": "Ren",
        "given_name": "Dongchun"
      },
      {
        "surname": "Yang",
        "given_name": "Xu"
      },
      {
        "surname": "Fan",
        "given_name": "Mingyu"
      },
      {
        "surname": "Huang",
        "given_name": "Hai"
      }
    ]
  },
  {
    "title": "Hiding data hiding",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2022.12.008",
    "abstract": "Data hiding (DH) is typically referred to as the art of hiding secret data into a given media for covert communication. In this paper, we make the first step towards hiding “data hiding” (HDH), which disguises DH tools, including a data embedding tool and a data extraction tool, as a deep neural network (DNN) with a normal task. In the method, after training a DNN for style transfer and DH from scratch, while the trained DNN can transfer the style of an image to the target style, it can also hide additional data into a cover image and extract data from a stego image. In other words, the DH tools are hidden by the DNN. Experimental results have shown the feasibility, applicability and superiority.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522003762",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Cover (algebra)",
      "Economics",
      "Embedding",
      "Engineering",
      "Image (mathematics)",
      "Information hiding",
      "Management",
      "Mechanical engineering",
      "Pattern recognition (psychology)",
      "Steganography",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Hanzhou"
      },
      {
        "surname": "Li",
        "given_name": "Chen"
      },
      {
        "surname": "Liu",
        "given_name": "Gen"
      },
      {
        "surname": "Zhang",
        "given_name": "Xinpeng"
      }
    ]
  },
  {
    "title": "Bi-path Combination YOLO for Real-time Few-shot Object Detection",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2022.11.025",
    "abstract": "Few-shot object detection (FSOD) has more attention in recent years as the quantitative limitation of instances during the model training. Previous works based on meta learning and transfer learning focus on the detection precision but ignore the inferring speed, which is difficult to apply in amounts of applications. In this letter, to keep a high inferring speed and a comparable detection precision, we propose a real-time detector entitled Bi-path Combination You Only Look Once (BC-YOLO) for FSOD. BC-YOLO can be categorized as a transfer learning based one-stage object detector with a two-phase training scheme. It is particularly composed of bi-path parallel detection branches which detect base and novel class objects respectively and commonly detect objects with a discriminator in the inferring stage. Moreover, to elevate the model generalization trained from few-shot objects, we further propose an Attentive DropBlock algorithm to make the detector focus on the entire details of objects instead of the local discriminative regions. Extensive experiments on PASCAL VOC 2007 and MS COCO 2014 datasets demonstrate that our method can achieve a better tradeoff between speed and precision than state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522003555",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Detector",
      "Discriminative model",
      "Discriminator",
      "Engineering",
      "Focus (optics)",
      "Mechanical engineering",
      "Object detection",
      "One shot",
      "Optics",
      "Pascal (unit)",
      "Path (computing)",
      "Pattern recognition (psychology)",
      "Physics",
      "Programming language",
      "Single shot",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Xia",
        "given_name": "Ruiyang"
      },
      {
        "surname": "Li",
        "given_name": "Guoquan"
      },
      {
        "surname": "Huang",
        "given_name": "Zhengwen"
      },
      {
        "surname": "Meng",
        "given_name": "Hongying"
      },
      {
        "surname": "Pang",
        "given_name": "Yu"
      }
    ]
  },
  {
    "title": "An accurate stereo matching method based on color segments and edges",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.108996",
    "abstract": "Stereo matching algorithms of binocular vision suffer from low accuracy when dealing with natural scenes (such as industrial robot scenes). Biological vision is sensitive to object edges; it divides objects by their edges, and then perceives their distances. Similar to the biological eye mechanism, this study proposes a matching algorithm that combines segment- and edge-matching to obtain the disparity. In segment matching, pixel strings from the same row of the left and right images are divided into pixel segments, whose colors and lengths are used as clues to determine several types of matching pixel segment pairs according to non-crossing mapping. The analysis of the spatial state yields several types of stimulus bars. Disparities can be obtained from the relation between pixel segment pairs and stimulus bars. In edge matching, the DTW (Dynamic Time Warping) algorithm and the gradient are used to determine the initial edge pixel matching results. The remaining edge point disparity is obtained by fitting a fill to the existing edge point disparity. Finally, segment and edge matching results are combined to check and fill and post-processing. This new matching method transforms pixel matching to pixel segment matching and edge matching, which can reduces the time complexity. The algorithm can be implemented in an industrial robot environment for high-precision needle threading guidance, which neither traditional binocular matching nor deep learning matching algorithms can do.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322004769",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Edge detection",
      "Enhanced Data Rates for GSM Evolution",
      "Image (mathematics)",
      "Image processing",
      "Matching (statistics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pixel",
      "Statistics",
      "Template matching"
    ],
    "authors": [
      {
        "surname": "Wei",
        "given_name": "Hui"
      },
      {
        "surname": "Meng",
        "given_name": "Lingjiang"
      }
    ]
  },
  {
    "title": "Environmental Sound Classiﬁcation on the Edge: A Pipeline for Deep Acoustic Networks on Extremely Resource-Constrained Devices",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109025",
    "abstract": "Significant efforts are being invested to bring state-of-the-art classification and recognition to edge devices with extreme resource constraints (memory, speed, and lack of GPU support). Here, we demonstrate the first deep network for acoustic recognition that is small, flexible and compression-friendly yet achieves state-of-the-art performance for raw audio classification. Rather than handcrafting a once-off solution, we present a generic pipeline that automatically converts a large deep convolutional network via compression and quantization into a network for resource-impoverished edge devices. After introducing ACDNet, which produces above state-of-the-art accuracy on ESC-10 (96.65%), ESC-50 (87.10%), UrbanSound8K (84.45%) and AudioEvent (92.57%), we describe the compression pipeline and show that it allows us to achieve 97.22% size reduction and 97.28% FLOP reduction while maintaining close to state-of-the-art accuracy 96.25%, 83.65%, 78.27% and 89.69% on these datasets. We describe a successful implementation on a standard off-the-shelf microcontroller and, beyond laboratory benchmarks, report successful tests on real-world datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322005052",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Cloud computing",
      "Computer engineering",
      "Computer hardware",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Edge computing",
      "Edge device",
      "Enhanced Data Rates for GSM Evolution",
      "Geometry",
      "Mathematics",
      "Microcontroller",
      "Operating system",
      "Pipeline (software)",
      "Quantization (signal processing)",
      "Real-time computing",
      "Reduction (mathematics)",
      "Resource (disambiguation)",
      "State (computer science)"
    ],
    "authors": [
      {
        "surname": "Mohaimenuzzaman",
        "given_name": "Md"
      },
      {
        "surname": "Bergmeir",
        "given_name": "Christoph"
      },
      {
        "surname": "West",
        "given_name": "Ian"
      },
      {
        "surname": "Meyer",
        "given_name": "Bernd"
      }
    ]
  },
  {
    "title": "Online and offline streaming feature selection methods with bat algorithm for redundancy analysis",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109007",
    "abstract": "Streaming feature selection (SFS), is the task of selecting the most informative features in dealing with high-dimensional or incrementally growing problems. Several SFS algorithms have been proposed in the literature. However, they do not consider all feature subsets at the redundancy analysis step due to computational concerns. Moreover, they do not reconsider previously removed features which leads to losing most of the useful information. In this paper, the redundancy analysis step is defined as a binary optimization problem. Then, a binary bat algorithm (BBA) is adopted to find the minimal informative subsets. In this way, a large number of feature subsets can be considered effectively at the redundancy analysis step. In addition, an effective priority list is used to maintain previously removed redundant features. Such a list allows the re-examination of informative features. As a result, it is possible to consider the mutual information between features that are not streamed in an small time interval. Experimental studies on fifteen different types of datasets show that our approach is superior to state-of-the-art online and offline streaming feature selection methods in terms of classification accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322004873",
    "keywords": [
      "Algorithm",
      "Arithmetic",
      "Artificial intelligence",
      "Binary number",
      "Computer science",
      "Data mining",
      "Feature (linguistics)",
      "Feature selection",
      "Linguistics",
      "Mathematics",
      "Mutual information",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Redundancy (engineering)",
      "Selection (genetic algorithm)"
    ],
    "authors": [
      {
        "surname": "Eskandari",
        "given_name": "S."
      },
      {
        "surname": "Seifaddini",
        "given_name": "M."
      }
    ]
  },
  {
    "title": "Better pseudo-label: Joint domain-aware label and dual-classifier for semi-supervised domain generalization",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.108987",
    "abstract": "With the goal of directly generalizing trained model to unseen target domains, domain generalization (DG), a newly proposed learning paradigm, has attracted considerable attention. Previous DG models usually require a sufficient quantity of annotated samples from observed source domains during training. In this paper, we relax this requirement about full annotation and investigate semi-supervised domain generalization (SSDG) where only one source domain is fully annotated along with the other domains totally unlabeled in the training process. With the challenges of tackling the domain gap between observed source domains and predicting unseen target domains, we propose a novel deep framework via joint domain-aware labels and dual-classifier to produce high-quality pseudo-labels. Concretely, to predict accurate pseudo-labels under domain shift, a domain-aware pseudo-labeling module is developed. Also, considering inconsistent goals between generalization and pseudo-labeling: former prevents overfitting on all source domains while latter might overfit the unlabeled source domains for high accuracy, we employ a dual-classifier to independently perform pseudo-labeling and domain generalization in the training process. When accurate pseudo-labels are generated for unlabeled source domains, the domain mixup operation is applied to augment new domains between labeled and unlabeled domains, which is beneficial for boosting the generalization capability of the model. Extensive results on publicly available DG benchmark datasets show the efficacy of our proposed SSDG method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322004678",
    "keywords": [
      "Annotation",
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Boosting (machine learning)",
      "Classifier (UML)",
      "Computer science",
      "Domain (mathematical analysis)",
      "Generalization",
      "Geodesy",
      "Geography",
      "Labeled data",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Overfitting",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Ruiqi"
      },
      {
        "surname": "Qi",
        "given_name": "Lei"
      },
      {
        "surname": "Shi",
        "given_name": "Yinghuan"
      },
      {
        "surname": "Gao",
        "given_name": "Yang"
      }
    ]
  },
  {
    "title": "Introduction to the Special Issue on Pattern Recognition-driven User Experiences (PRUE)",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2022.11.030",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522003592",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computation",
      "Computer science",
      "Computer vision",
      "Economics",
      "Image (mathematics)",
      "Metric (unit)",
      "Operations management",
      "Pattern recognition (psychology)",
      "Programming language",
      "Set (abstract data type)",
      "Texture (cosmology)"
    ],
    "authors": []
  },
  {
    "title": "Co-Attention Fusion Network for Multimodal Skin Cancer Diagnosis",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.108990",
    "abstract": "Recently, multimodal image-based methods have shown great performance in skin cancer diagnosis. These methods usually use convolutional neural networks (CNNs) to extract the features of two modalities (i.e., dermoscopy and clinical images), and fuse these features for classification. However, they commonly have the following two shortcomings: 1) the feature extraction processes of the two modalities are independent and lack cooperation, which may lead to limited representation ability of the extracted features, and 2) the multimodal fusion operation is a simple concatenation followed by convolutions, thus causing rough fusion features. To address these two issues, we propose a co-attention fusion network (CAFNet), which uses two branches to extract the features of dermoscopy and clinical images and a hyper-branch to refine and fuse these features at all stages of the network. Specifically, the hyper-branch is composed of multiple co-attention fusion (CAF) modules. In each CAF module, we first design a co-attention (CA) block with a cross-modal attention mechanism to achieve the cooperation of two modalities, which enhances the representation ability of the extracted features through mutual guidance between the two modalities. Following the CA block, we further propose an attention fusion (AF) block that dynamically selects appropriate fusion ratios to conduct the pixel-wise multimodal fusion, which can generate fine-grained fusion features. In addition, we propose a deep-supervised loss and a combined prediction method to obtain a more robust prediction result. The results show that CAFNet achieves the average accuracy of 76.8% on the seven-point checklist dataset and outperforms state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322004708",
    "keywords": [
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Combinatorics",
      "Computer science",
      "Concatenation (mathematics)",
      "Convolutional neural network",
      "Deep learning",
      "Electrical engineering",
      "Engineering",
      "Feature (linguistics)",
      "Feature extraction",
      "Fuse (electrical)",
      "Fusion",
      "Geometry",
      "Image (mathematics)",
      "Image fusion",
      "Law",
      "Linguistics",
      "Mathematics",
      "Modalities",
      "Modality (human–computer interaction)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "He",
        "given_name": "Xiaoyu"
      },
      {
        "surname": "Wang",
        "given_name": "Yong"
      },
      {
        "surname": "Zhao",
        "given_name": "Shuang"
      },
      {
        "surname": "Chen",
        "given_name": "Xiang"
      }
    ]
  },
  {
    "title": "A weakly supervised deep active contour model for nodule segmentation in thyroid ultrasound images",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2022.12.015",
    "abstract": "Nodule segmentation is crucial for thyroid ultrasound image analysis. Recent progress in this task is driven by deep learning methods and large high-quality annotated datasets. However, obtaining pixel-wise labels are labor-intensive and time-consuming. Weakly supervised learning is proposed to address limited annotations with simple labels, but most are not satisfactory in capturing blurred boundaries due to the lack of attention to edges. In this paper, we propose a novel weakly supervised deep active contour model for nodule segmentation in thyroid ultrasound images. The key idea is to deform an initial contour iteratively to match thyroid nodule boundary by regressing vertex offsets which are obtained by gradient similarity and statistical information. First, a polygon contour is adopted as initial label, which aims to reduce interference from surrounding organs. Second, contour deformation network deforms initial contour by regressing vertex-wise offsets. We introduce a loss function based on level-set theory and an auxiliary edge attention module to enhance ability to capture fuzzy boundaries. As demonstrated by experimental results, our model achieved a dice coefficient of 0.87±0.07 and Harsdorf distance of 17.11±7.95, outperforming other state-of-the-art weakly supervised methods. The segmentation results are effective with fully supervised methods, but our work lighten annotation burdens. Therefore, our algorithm can be effectively used in clinical diagnosis.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522003816",
    "keywords": [
      "Active contour model",
      "Artificial intelligence",
      "Artificial neural network",
      "Boundary (topology)",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Image segmentation",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pixel",
      "Segmentation",
      "Supervised learning"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Zhizhou"
      },
      {
        "surname": "Zhou",
        "given_name": "Shichong"
      },
      {
        "surname": "Chang",
        "given_name": "Cai"
      },
      {
        "surname": "Wang",
        "given_name": "Yuanyuan"
      },
      {
        "surname": "Guo",
        "given_name": "Yi"
      }
    ]
  },
  {
    "title": "Sticks and STONES may build my bones: Deep learning reconstruction of limb rotations in stick figures",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2022.12.012",
    "abstract": "Monitoring and analyzing physical activity is becoming an important task in both clinical and non-clinical settings. To accomplish this desideratum, stick figures are often used as abstractions of human poses and movements by representing body segments as straight lines (sticks). Despite their straightforwardness, this minimalist representation is incomplete as it lacks the segments’ longitudinal rotations, and therefore, is insufficient for applications requiring full 3D kinematic data. We introduce STONES, an advanced machine learning approach for estimating longitudinal body segment rotations of based on stick figures defined from a minimal set of body points. Our approach relies on a recurrent deep neural network, which takes 3D joint positions from a minimalist stick figure representation, such as those acquired by conventional depth camera sensors, and completes it with accurate longitudinal segment rotations. We validated our approach via a test scenario based on exergaming activities (e.g., lunges, squats, and kicks), which are becoming an emerging trend in several healthcare sectors, and our estimations show a fit above 98% and mean errors of approximately 1 ∘ . Our deep learning approach effectively surpasses other machine learning-based strategies and closely matches the accuracy of state-of-the-art motion capture systems while running at real-time speeds.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522003804",
    "keywords": [
      "Anatomy",
      "Artificial intelligence",
      "Computer graphics (images)",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Geology",
      "Medicine",
      "Orthodontics"
    ],
    "authors": [
      {
        "surname": "Fernandes",
        "given_name": "Francisco"
      },
      {
        "surname": "Roupa",
        "given_name": "Ivo"
      },
      {
        "surname": "Gonçalves",
        "given_name": "Sérgio B."
      },
      {
        "surname": "Moita",
        "given_name": "Gonçalo"
      },
      {
        "surname": "da Silva",
        "given_name": "Miguel Tavares"
      },
      {
        "surname": "Pereira",
        "given_name": "João"
      },
      {
        "surname": "Jorge",
        "given_name": "Joaquim"
      },
      {
        "surname": "Neptune",
        "given_name": "Richard R."
      },
      {
        "surname": "Lopes",
        "given_name": "Daniel Simões"
      }
    ]
  },
  {
    "title": "Incremental learning for transductive support vector machine",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.108982",
    "abstract": "Semi-supervised learning is ubiquitous in real-world machine learning applications due to its good performance for handling the data where only a few number of samples are labeled while most of then are unlabeled. Transductive support vector machine (TSVM) is an important semi-supervised learning method which formulates the problem as a nonconvex combinatorial optimization problem. The infinitesimal annealing algorithm is a novel training method of TSVM which can alleviate the impact of the combinatorial and non-convex natures in TSVM and achieve a fast training of TSVM. However, it is still a challenging problem to handle large-scale data for TSVM even using the infinitesimal annealing algorithm. To mitigate this problem, in this paper, we propose an incremental learning algorithm for TSVM (ILTSVM) based on the path following technique under the framework of infinitesimal annealing. Specifically, for new samples, we call CP-Step to change the solution and partition by increasing the size of the penalty coefficient. The difference between training labeled samples and training unlabeled samples is that the variation range of the penalty coefficient of labeled samples is larger than that of unlabeled samples. If in the process of CP-Step, pseudo-labels of unlabeled samples are classified incorrectly, call DJ-Step to flip the pseudo-labels, and use incremental and decremental algorithms to make the KKT condition satisfied. We also analyze the time complexity and convergence of ILTSVM. The experimental results show that compared with other incremental or batch learning algorithms, our algorithm is the most effective and fastest method for training TSVM.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322004629",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Binary classification",
      "Computer science",
      "Convergence (economics)",
      "Economic growth",
      "Economics",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Simulated annealing",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Haiyan"
      },
      {
        "surname": "Yu",
        "given_name": "Ying"
      },
      {
        "surname": "Jia",
        "given_name": "Yizhen"
      },
      {
        "surname": "Gu",
        "given_name": "Bin"
      }
    ]
  },
  {
    "title": "Retinal image enhancement with artifact reduction and structure retention",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.108968",
    "abstract": "Enhancement of low-quality retinal fundus images is beneficial to clinical diagnosis of ophthalmic diseases and computer-aided analysis. Enhancement accuracy is a challenge for image generation models, especially when there is no supervision by paired images. To reduce artifacts and retain structural consistency for accuracy improvement, we develop an unpaired image generation method for fundus image enhancement with the proposed high-frequency extractor and feature descriptor. Specifically, we summarize three causes of tiny vessel-like artifacts which always appear in other image generation methods. A high frequency prior is incorporated into our model to reduce artifacts by the proposed high-frequency extractor. In addition, the feature descriptor is trained alternately with the generator using segmentation datasets and generated image pairs to ensure the fidelity of the image structure. Pseudo-label loss is proposed to improve the performance of the feature descriptor. Experimental results show that the proposed method performs better than other methods both qualitatively and quantitatively. The enhancement can improve the performance of segmentation and classification in retinal images.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322004484",
    "keywords": [
      "Artifact (error)",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Consistency (knowledge bases)",
      "Feature (linguistics)",
      "Feature extraction",
      "Image (mathematics)",
      "Image quality",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Bingyu"
      },
      {
        "surname": "Zhao",
        "given_name": "He"
      },
      {
        "surname": "Cao",
        "given_name": "Lvchen"
      },
      {
        "surname": "Liu",
        "given_name": "Hanruo"
      },
      {
        "surname": "Wang",
        "given_name": "Ningli"
      },
      {
        "surname": "Li",
        "given_name": "Huiqi"
      }
    ]
  },
  {
    "title": "TIGER: A Tucker-based instrument for gesture recognition with inertial sensors",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2022.11.028",
    "abstract": "Gesture input is prevalent for interacting with computer systems, such as mobile and wearable devices. However, representation and acquisition of motion gestures using inertial sensors already built into these devices, e.g., accelerometers and gyroscopes, usually require processing of large training sets because of the high sampling frequency of the collected data. Consequently, one challenge that arises in the classification process of motion gestures for interactive systems is delivering robust and accurate predictions of user input in interactive time. Reducing the size of the training datasets is one way of using more efficiently the computational resources required to run established machine learning algorithms on mobile and wearable devices. To this end, we introduce TIGER, a multilinear tensor-based instrument for motion gesture recognition that (i) uses the Tucker2 decomposition to reduce the dimensionality of the training set by extracting features from the data reported by inertial sensors and (ii) leverages ensemble learning to increase gesture recognition accuracy for devices with built-in inertial sensors.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522003543",
    "keywords": [
      "Accelerometer",
      "Aerospace engineering",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Curse of dimensionality",
      "Embedded system",
      "Engineering",
      "Gesture",
      "Gesture recognition",
      "Gyroscope",
      "Inertial measurement unit",
      "Mobile device",
      "Multilinear map",
      "Operating system",
      "Physics",
      "Process (computing)",
      "Programming language",
      "Quantum mechanics",
      "Set (abstract data type)",
      "Wearable computer",
      "Wearable technology"
    ],
    "authors": [
      {
        "surname": "Bilius",
        "given_name": "Laura-Bianca"
      },
      {
        "surname": "Pentiuc",
        "given_name": "Ştefan-Gheorghe"
      },
      {
        "surname": "Vatavu",
        "given_name": "Radu-Daniel"
      }
    ]
  },
  {
    "title": "Decoupling multi-task causality for improved skin lesion segmentation and classification",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.108995",
    "abstract": "Multi-task learning has been used widely in many computer aided diagnosis applications recently, while the trade-off between different tasks remains challenging. Also, the inherent causality is less studied. In this paper, we focus on skin lesion analysis, including lesion classification, detection and segmentation. By defining the chain relationship (i.e., lesion detection boosts contour segmentation, and segmentation boosts lesion classification in turn), and further decoupling each pair-wise causality (e.g., detection to segmentation) from the Pareto efficiency view, we can solve the common trade-off issue between multi-task. On this basis, we propose a novel paradigm to improve the skin lesion segmentation and classification separately, and favourable feature fusion ways for each task are explored. Moreover, to address the huge model size problem, we design an effective model compression scheme (MCS). Extensive experiments on the ISIC2017 and PH2 datasets are conducted to evaluate the proposed paradigm. The results demonstrate that the popular models such as ResNet, DenseNet and UNet for lesion analysis can be boosted by applying the proposed paradigm, and the designed MCS reduces the amount of model parameters efficiently. We achieve performance improvements on skin lesion segmentation and classification without strenuous network design and soaring model complexity. This proposed approach is promising for the multi-task diagnosis setting in other medical applications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322004757",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Control engineering",
      "Decoupling (probability)",
      "Deep learning",
      "Engineering",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Song",
        "given_name": "Lei"
      },
      {
        "surname": "Wang",
        "given_name": "Haoqian"
      },
      {
        "surname": "Wang",
        "given_name": "Z. Jane"
      }
    ]
  },
  {
    "title": "An overhead-free region-based JPEG framework for task-driven image compression",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2022.11.020",
    "abstract": "An increasing amount of captured images are streamed to a remote server or stored in a device for deep neural network (DNN) inference. In most cases, raw images are compressed with encoding algorithms such as JPEG to cope with resource limitations. However, the standard JPEG optimized for human visual systems may induce significant accuracy loss in DNN inference tasks. In addition, the standard JPEG compresses all regions in an image at the same quality level, while some areas may not contain valuable information for the target task. In this paper, we propose a target-driven JPEG compression framework that performs region-adaptive quantization of the DCT coefficients. The region-based quality map is generated from an end-to-end trainable neural network. In addition, we present a deep learning approach to remove the requirement of storing the overhead information induced by the region-based encoding process. Our framework can be easily implemented on devices with commonly used JPEG and also produce images that achieve a higher compression rate with minimum degradation of the classification accuracy. The codes are available at https://github.com/jshye/qf-map-generator.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552200349X",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer vision",
      "Data compression",
      "Discrete cosine transform",
      "Economics",
      "Encoding (memory)",
      "Image (mathematics)",
      "Image compression",
      "Image processing",
      "JPEG",
      "JPEG 2000",
      "Lossless JPEG",
      "Management",
      "Operating system",
      "Overhead (engineering)",
      "Quantization (signal processing)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Jeong",
        "given_name": "Seonghye"
      },
      {
        "surname": "Jeong",
        "given_name": "Seongmoon"
      },
      {
        "surname": "Woo",
        "given_name": "Simon S."
      },
      {
        "surname": "Ko",
        "given_name": "Jong Hwan"
      }
    ]
  },
  {
    "title": "Artistic neural style transfer using CycleGAN and FABEMD by adaptive information selection",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2022.11.026",
    "abstract": "Neural Style Transfer (NST) comprises a class of computer vision methods that manipulate digital images to reformulate the visual content of one input image adopting visual features of an another image set. Artistic NST is the particular case of NST where the visual features are extracted from painting images. The combination of Cycle-Consistent Adversarial Networks (CycleGANs) with Fast and Adaptive Bidimensional Empirical Mode Decomposition (FABEMD) is proposed to adopt the specific artist’s style on images effectively, where the cycle-consistency loss is modified to incorporate texture information by estimating the corresponding Bidimensional Intrinsic Mode Functions (BIMFs). An adaptive approach for identifying the optimal BIMF number that must be considered in order to manipulate the required amount of the involved texture, is proposed. For this purpose, the computation of a metric is considered for each BIMF to characterise the texture of each image or major intensity alterations at local scale. Experimental results reveal that adaptive comixture of texture features comprises an efficient approach in such artistic applications. Qualitative and quantitative results demonstrate that the proposed framework outperforms state-of-the-art (SoA) methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522003567",
    "keywords": [],
    "authors": [
      {
        "surname": "Batziou",
        "given_name": "Elissavet"
      },
      {
        "surname": "Ioannidis",
        "given_name": "Konstantinos"
      },
      {
        "surname": "Patras",
        "given_name": "Ioannis"
      },
      {
        "surname": "Vrochidis",
        "given_name": "Stefanos"
      },
      {
        "surname": "Kompatsiaris",
        "given_name": "Ioannis"
      }
    ]
  },
  {
    "title": "Graph-embedded subspace support vector data description",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.108999",
    "abstract": "In this paper, we propose a novel subspace learning framework for one-class classification. The proposed framework presents the problem in the form of graph embedding. It includes the previously proposed subspace one-class techniques as its special cases and provides further insight on what these techniques actually optimize. The framework allows to incorporate other meaningful optimization goals via the graph preserving criterion and reveals a spectral solution and a spectral regression-based solution as alternatives to the previously used gradient-based technique. We combine the subspace learning framework iteratively with Support Vector Data Description applied in the subspace to formulate Graph-Embedded Subspace Support Vector Data Description. We experimentally analyzed the performance of newly proposed different variants. We demonstrate improved performance against the baselines and the recently proposed subspace learning methods for one-class classification.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322004794",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Embedding",
      "Geometry",
      "Graph",
      "Graph embedding",
      "Linear subspace",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Random subspace method",
      "Subspace topology",
      "Support vector machine",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Sohrab",
        "given_name": "Fahad"
      },
      {
        "surname": "Iosifidis",
        "given_name": "Alexandros"
      },
      {
        "surname": "Gabbouj",
        "given_name": "Moncef"
      },
      {
        "surname": "Raitoharju",
        "given_name": "Jenni"
      }
    ]
  },
  {
    "title": "Lane Detection with Versatile AtrousFormer and Local Semantic Guidance",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109053",
    "abstract": "Lane detection is one of the core functions in autonomous driving and has aroused widespread attention recently. The networks to segment lane instances, especially with bad appearance, must be able to explore lane distribution properties. Most existing methods tend to resort to CNN-based techniques. A few have a try on incorporating the recent adorable, the seq2seq Transformer [1]. However, their innate drawbacks of weak global information collection ability and exorbitant computation overhead prohibit a wide range of the further applications. In this work, we propose Global Atrous Transformer (AtrousFormer) to solve the problem. Its variant local AtrousFormer is interleaved into feature extractor to enhance extraction. Their collecting information first by rows and then by columns in a dedicated manner finally equips our network with stronger information gleaning ability and better computation efficiency. To further improve the performance, we also propose a local semantic guided decoder to delineate the identities and shapes of lanes more accurately. Extensive results on three challenging benchmarks (CULane, TuSimple, and BDD100K) show that our network performs favorably against the state of the arts.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322005337",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computation",
      "Computer science",
      "Distributed computing",
      "Electrical engineering",
      "Engineering",
      "Extractor",
      "Feature extraction",
      "Pattern recognition (psychology)",
      "Process engineering",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Jiaxing"
      },
      {
        "surname": "Zhang",
        "given_name": "Lihe"
      },
      {
        "surname": "Lu",
        "given_name": "Huchuan"
      }
    ]
  },
  {
    "title": "OW-TAL: Learning Unknown Human Activities for Open-World Temporal Action Localization",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109027",
    "abstract": "Current temporal action localization methods work well on a closed-world assumption, in which all action categories to be localized are known as a priori. However, this assumption doesn’t apply to open-world scenarios, as novel categories that never appeared in the training stage will be encountered without explicit supervision. Distinct from the closed-world setting, localizing actions under the open-world setup poses two significant challenges: 1) identifying unknown actions from diverse knowns and localizing their temporal boundaries. 2) defying forgetting of previous actions when incrementally updating knowledge of identified unknown actions. To address the aforementioned challenges, we develop a two-branch framework with Unknown and Known action modeling Networks, a.k.a. UK-Net, for the problem of Open-World Temporal Action Localization (OW-TAL). The potential patterns underlying unknown and known actions, as well as their dynamic transformation, are modeled in a unified pipeline. Specifically, a self-attention based position-sensitive module is designed to produce actionness scores for unknown actions in a class-agnostic way. Besides, an iterative optimization strategy is developed to enable knowledge derived from known categories to be shared with the unknowns. In addition, a self-paced learning strategy is proposed to instructionally guide class-incremental learning while defying catastrophic forgetting. Benefiting from the above components, our UK-Net yields superior performance on three challenging datasets, i.e., THUMOS14, ActivityNet1.2, and MUSES. Experimental results also demonstrate the competitive performance of our method when compared with traditional closed-world counterparts.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322005076",
    "keywords": [
      "A priori and a posteriori",
      "Action (physics)",
      "Artificial intelligence",
      "Class (philosophy)",
      "Cognitive psychology",
      "Computer science",
      "Epistemology",
      "Forgetting",
      "Machine learning",
      "Philosophy",
      "Physics",
      "Pipeline (software)",
      "Programming language",
      "Psychology",
      "Quantum mechanics"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Yaru"
      },
      {
        "surname": "Zhang",
        "given_name": "Xiao-Yu"
      },
      {
        "surname": "Shi",
        "given_name": "Haichao"
      }
    ]
  },
  {
    "title": "Dynamic dense CRF inference for video segmentation and semantic SLAM",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109023",
    "abstract": "The dense conditional random field (dense CRF) is an effective post-processing tool for image/video segmentation and semantic SLAM. In this paper, we extend the traditional dense CRF inference algorithm to incremental sensor data modelling. The algorithm efficiently infers the maximum a posteriori probability (MAP) solution for a dynamically changing dense CRF model that is applied to incremental multi-class video segmentation and semantic SLAM. The computational cost is roughly proportional to the total change in the Gaussian pairwise edges of the dense CRF. In our system, with an increase in the number of frames of the sensor data, MAP calculations take approximately the same time to compute the overall three-dimensional dense CRF modelled for the entire video. Compared with the traditional dense CRF for video segmentation, this method is more suitable for incremental (in-line) video segmentation and robot semantic SLAM. The results of experiments show that if part of a pairwise edge is altered, our dynamic algorithm is significantly faster than the widely known standard dense CRF algorithm. In addition, the accuracy of its inference does not change. Several multi-class video segmentation tests confirmed the efficiency of inference of the algorithm. In another application, we used the dynamic dense CRF to incrementally integrate robot SLAM and video segmentation. The results show that an accurate SLAM can improve the accuracy of video segmentation, and the computational cost of the dense CRF MAP can be constrained over a constant range. The application of our algorithm is not limited to video segmentation: It is generic, and can be used to yield similar improvements in many optimization solutions for MAP in dynamically changing models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322005039",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Conditional random field",
      "Image segmentation",
      "Inference",
      "Mathematics",
      "Maximum a posteriori estimation",
      "Maximum likelihood",
      "Pairwise comparison",
      "Pattern recognition (psychology)",
      "Scale-space segmentation",
      "Segmentation",
      "Statistics",
      "Video processing",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "You",
        "given_name": "Mingyu"
      },
      {
        "surname": "Luo",
        "given_name": "Chaoxian"
      },
      {
        "surname": "Zhou",
        "given_name": "Hongjun"
      },
      {
        "surname": "Zhu",
        "given_name": "Shaoqing"
      }
    ]
  },
  {
    "title": "Noise-robust oversampling for imbalanced data classification",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109008",
    "abstract": "The class imbalance problem is characterized by an unequal data distribution in which majority classes have a greater number of data samples than minority classes. Oversampling methods generate samples for minority classes to balance the data distribution. However, the generated minority samples may overlap with majority samples, resulting in noise. In this paper, we propose a noise-robust oversampling algorithm for mixed-type and multi-class imbalanced data. Our proposed noise-robust designs include an algorithm to eliminate noise within clusters of data samples, adaptive embedding to generate samples safely, and a safe boundary for enlarging class boundaries. The heterogeneous distance metric and adapted decomposition strategy render our noise-robust algorithm suitable for mixed-type and multi-class imbalanced data. Experimental results on 20 benchmark datasets demonstrate the effectiveness of the proposed algorithm.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322004885",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Benchmark (surveying)",
      "Class (philosophy)",
      "Computer network",
      "Computer science",
      "Data mining",
      "Economics",
      "Embedding",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Mathematics",
      "Metric (unit)",
      "Noise (video)",
      "Operations management",
      "Oversampling",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Yongxu"
      },
      {
        "surname": "Liu",
        "given_name": "Yan"
      },
      {
        "surname": "Yu",
        "given_name": "Bruce X.B."
      },
      {
        "surname": "Zhong",
        "given_name": "Shenghua"
      },
      {
        "surname": "Hu",
        "given_name": "Zhejing"
      }
    ]
  },
  {
    "title": "A new algorithm for support vector regression with automatic selection of hyperparameters",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.108989",
    "abstract": "The hyperparameters in support vector regression (SVR) determine the effectiveness of the support vectors with fitting and predictions. However, the choice of these hyperparameters has always been challenging in both theory and practice. The ν -support vector regression eliminates the need to specify an ϵ value elegantly, but at the cost of specifying or postulating a ν value. We propose an extended primal objective function arising from probability regularization leading to an automatic selection of ϵ , and we can express ν as an explicit function of ϵ . The resultant hyperparameter values can be interpreted as ‘working’ values required only in training but not testing or prediction. This regularized algorithm, namely ϵ * -SVR, automatically provides a data-dependent ϵ and is found to have a close connection to the ν -support vector regression in the sense that ν as a fraction is a sensible function of ϵ . The ϵ * -SVR automatically selects both ν and ϵ values. We illustrate these findings with some public benchmark datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322004691",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Computer science",
      "Evolutionary biology",
      "Function (biology)",
      "Geodesy",
      "Geography",
      "Hyperparameter",
      "Hyperparameter optimization",
      "Machine learning",
      "Mathematics",
      "Regression",
      "Regularization (linguistics)",
      "Statistics",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "You-Gan"
      },
      {
        "surname": "Wu",
        "given_name": "Jinran"
      },
      {
        "surname": "Hu",
        "given_name": "Zhi-Hua"
      },
      {
        "surname": "McLachlan",
        "given_name": "Geoffrey J."
      }
    ]
  },
  {
    "title": "ScribbleNet: Efficient interactive annotation of urban city scenes for semantic segmentation",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109011",
    "abstract": "Annotation is a crucial first step in the semantic segmentation of urban images that facilitates the development of autonomous navigation systems. However, annotating complex urban images is time-consuming and challenging. It requires significant human effort making it expensive and error-prone. To reduce human effort during annotation, multiple images need to be annotated in a short time-span. In this paper, we introduce ScribbleNet, an interactive image segmentation algorithm to address this issue. Our approach provides users with a pre-segmented image that iteratively improves the segmentation using scribble as an annotation input. This method is based on conditional inference and exploits the learnt correlations in a deep neural network (DNN). ScribbleNet can: (1) work with urban city scenes captured in unseen environments, (2) annotate new classes not present in the training set, and (3) correct several labels at once. We compare this method with other interactive segmentation approaches on multiple datasets such as CityScapes, BDD, Mapillary Vistas, KITTI, and IDD. ScribbleNet reduces the annotation time of an image by up to 14.7 × over manual annotation and up to 5.4 × over the current approaches. The algorithm is integrated into the publicly available LabelMe image annotation tool and will be released as an open-source software.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322004915",
    "keywords": [
      "Annotation",
      "Artificial intelligence",
      "Artificial neural network",
      "Automatic image annotation",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Exploit",
      "Image (mathematics)",
      "Image retrieval",
      "Image segmentation",
      "Inference",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Programming language",
      "Segmentation",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Sambaturu",
        "given_name": "Bhavani"
      },
      {
        "surname": "Gupta",
        "given_name": "Ashutosh"
      },
      {
        "surname": "Jawahar",
        "given_name": "C.V."
      },
      {
        "surname": "Arora",
        "given_name": "Chetan"
      }
    ]
  },
  {
    "title": "A landmark-free approach for automatic, dense and robust correspondence of 3D faces",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.108971",
    "abstract": "Global dense registration of 3D faces commonly prioritizes correspondences of facial landmarks which are fiducial points for the anatomical structures. However, it is not always easy to pre-annotate the landmarks accurately in raw scans of 3D faces. Contrary to the current state-of-the-art in dense 3D face correspondence, we propose a general framework without pre-annotated landmarks, which promotes its robustness and allows the meshes to deform in a uniform manner. The proposed framework includes two stages: first the correspondences are established using a template face; and then we select some well-reconstructed samples to build a prior model and leverage it into the correspondence process of other samples. In both stages, the dense registration is revisited in two perspectives: semantic and topological correspondence. In the latter stage, we further incorporate shape and normal statistics of 3D faces to regularize the correspondence process for more robust results. This provides a feasible way to handle data with noises and occlusions, as well as large deformation caused by facial expressions. Our basic idea is to gradually refine the correspondence of individual points in a way global-to-local. At the same time, we solve the local-to-global deformation based on the refined correspondences. The two processes are alternated, and aided by some confidence checks for each individual points. In the experiments, the proposed method is evaluated both qualitatively and quantitatively on three datasets including two publicly available ones: FRGC v2.0 and BU-3DFE datasets, demonstrating its effectiveness.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322004514",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer graphics (images)",
      "Computer science",
      "Computer vision",
      "Face (sociological concept)",
      "Gene",
      "Landmark",
      "Leverage (statistics)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Polygon mesh",
      "Process (computing)",
      "Robustness (evolution)",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Fan",
        "given_name": "Zhenfeng"
      },
      {
        "surname": "Hu",
        "given_name": "Xiyuan"
      },
      {
        "surname": "Chen",
        "given_name": "Chen"
      },
      {
        "surname": "Wang",
        "given_name": "Xiaolian"
      },
      {
        "surname": "Peng",
        "given_name": "Silong"
      }
    ]
  },
  {
    "title": "Conditional mixture modeling and model-based clustering",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.108994",
    "abstract": "Due to a potentially high number of parameters, finite mixture models are often at the risk of overparameterization even for a moderate number of components. This can lead to overfitting individual components and result in mixture order underestimation. One of the most popular approaches to address this issue is to reduce the number of parameters by considering parsimonious models. The vast majority of techniques in this direction focuses on the reparameterization of covariance matrices associated with mixture components. We propose an alternative approach based on the parsimonious parameterization of location parameters that enjoys remarkable modeling flexibility especially in the presence of non-compact clusters. Due to an attractive closed form formulation, speedy parameter estimation is available by means of the EM algorithm. The utility of the proposed method is illustrated on synthetic and well-known classification data sets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322004745",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Cluster analysis",
      "Computer science",
      "Covariance",
      "Data mining",
      "Flexibility (engineering)",
      "Mathematical optimization",
      "Mathematics",
      "Mixture model",
      "Overfitting",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Melnykov",
        "given_name": "Volodymyr"
      },
      {
        "surname": "Wang",
        "given_name": "Yang"
      }
    ]
  },
  {
    "title": "A multi-scenario text generation method based on meta reinforcement learning",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2022.11.031",
    "abstract": "Multi-scenario text generation is an essential task in natural language generation because of the multi-scene interlaced property of real-world problems. Traditional methods typically train the multi-scenario text generation models based on maximum likelihood estimation, which may suffer from the problem of exposure bias. Reinforcement learning (RL) based text generation methods could mitigate the exposure bias problem to some extent. However, the RL-based text generation methods are limited to the single-scenario tasks, which cannot be straightforwardly generalized to new scenario tasks. To address this problem, in this paper, we propose a multi-scenario text generation method based on meta RL (MetaRL-TG), which implements the method of model-agnostic meta-learning (MAML) in the framework of RL-based text generation. The proposed MetaRL-TG method first learns the initial parameters from multiple training tasks, then fine-tunes them in the target task. Thus, the proposed method is expected to efficiently achieve high-quality generated text in the new scenario. Finally, the effectiveness and generalization capability of the proposed method are demonstrated for eight scenarios through English test datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522003609",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Economics",
      "Epistemology",
      "Generalization",
      "Machine learning",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Meta learning (computer science)",
      "Philosophy",
      "Property (philosophy)",
      "Reinforcement learning",
      "Task (project management)",
      "Text generation"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Tingting"
      },
      {
        "surname": "Li",
        "given_name": "Guixi"
      },
      {
        "surname": "Song",
        "given_name": "Yajing"
      },
      {
        "surname": "Wang",
        "given_name": "Yuan"
      },
      {
        "surname": "Chen",
        "given_name": "Yarui"
      },
      {
        "surname": "Yang",
        "given_name": "Jucheng"
      }
    ]
  },
  {
    "title": "Capture and control content discrepancies via normalised flow transfer",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2022.12.017",
    "abstract": "Unsupervised Style Transfer (UST) has recently been a hot topic in Computer Vision, and this type of work has been exemplified by CycleGAN. Although the existing UST methods have proven to be useful, in many circumstances, we want to be able to manage not just the transformation of a single piece of instance, but also the morphological features of the two data sets’ underlying distributions. To this end, we propose a novel framework called Normalised Flow Transfer (NFT), where a reversible probability transform using the normalised flow method is developed to transfer the data in the first domain to the second, so as to exhibit their probabilities under both domains in the mapping process. A particularly interesting application under this framework is that when the data sets in two domains contain numerous clusters based on their finite class labels, we can control the distribution pattern across the two domains and apply any constraints on the underlying distributions of the same class. The experimental results show that not only can we devise many new complex style transfer functions, but also our framework has better image generation capabilities in terms of evaluation metrics, including mean square error, inception score and Frechet inception distance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522003853",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Class (philosophy)",
      "Computer science",
      "Data mining",
      "Domain (mathematical analysis)",
      "Flow (mathematics)",
      "Gene",
      "Geometry",
      "Image (mathematics)",
      "Mathematical analysis",
      "Mathematics",
      "Operating system",
      "Parallel computing",
      "Pattern recognition (psychology)",
      "Probability distribution",
      "Process (computing)",
      "Statistics",
      "Transfer (computing)",
      "Transformation (genetics)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Can"
      },
      {
        "surname": "Xu",
        "given_name": "Richard Yi Da"
      },
      {
        "surname": "Zhang",
        "given_name": "Xu"
      },
      {
        "surname": "Huang",
        "given_name": "Wanming"
      }
    ]
  },
  {
    "title": "Image inpainting via spatial projections",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109040",
    "abstract": "Image inpainting is now-a-days sought after due to its wide variety of applications in the reconstruction of the corrupted image, occlusion removal, reflection removal, etc. Existing image inpainting approaches utilize different types of attention mechanisms to inpaint the image and produce visibly admirable results. These methods are more concerned at weighing the feature maps of the hole region with some weight from the non-hole region. But, due to the lack of spatial contextual correlation in the attention maps, the inpainted image may suffer from the inconsistencies among hole and non-hole regions. Transformer-based inpainting methods give significant results by capturing the relationship between the patches with a compromise of high computational complexity. In this context, we propose a novel spatial projection layer (SPL) without any attention mechanism to project the spatial contextual information in the hole region from non-hole regions for producing a spatially plausible inpainted image. The SPL is proposed mainly to focus on the non-hole spatial information in the high-level feature maps for filling the hole regions efficiently. Also, while training the network, we propose the use of edge loss with a Canny edge operator for image inpainting to focus on the relevant edges instead of noise contents. Analysis with the extensive experiments, ablation, and user study on the proposed architecture demonstrates the superiority over existing state-of-the-art methods for image inpainting. The code is available at: https://github.com/shrutiphutke/spatial_projection_inpainting.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322005209",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Enhanced Data Rates for GSM Evolution",
      "Feature (linguistics)",
      "Filling-in",
      "Focus (optics)",
      "Geography",
      "Image (mathematics)",
      "Image processing",
      "Image restoration",
      "Inpainting",
      "Linguistics",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics"
    ],
    "authors": [
      {
        "surname": "Phutke",
        "given_name": "Shruti S"
      },
      {
        "surname": "Murala",
        "given_name": "Subrahmanyam"
      }
    ]
  },
  {
    "title": "SaberNet: Self-attention based effective relation network for few-shot learning",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109024",
    "abstract": "Few-shot learning is an essential and challenging field in machine learning since the agent needs to learn novel concepts with a few data. Recent methods aim to learn comparison or relation between query and support samples to tackle few-shot tasks but have not exceeded human performance and made full use of relations in few-shot tasks. Humans can recognize multiple variants of objects located anywhere in images and compare the relation among learned instances. Inspired by the human learning mechanism, we explore the definition of relations in relation networks and propose self-attention relation modules for feature and learning ability. First, we introduce vision self-attention to generate and purify features in few-shot learning. The comparison of different patches leads the backbone to infer relations between local features, which enforces feature extraction focus on more details. Second, we propose task-specific feature augmentation modules to infer relations and weight different contributions of components in few-shot tasks. The proposed SaberNet is conceptually simple and empirically powerful. Its performance surpasses the baseline a great margin, including pushing 5-way 1-shot CUB accuracy to 89.75% (12.73% absolute improvement), Cars to 76.71% (12.99% absolute improvement) and Flowers to 84.33% (7.67% absolute improvement).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322005040",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Data mining",
      "Economics",
      "Engineering",
      "Feature (linguistics)",
      "Field (mathematics)",
      "Focus (optics)",
      "Linguistics",
      "Machine learning",
      "Management",
      "Margin (machine learning)",
      "Mathematics",
      "Mechanical engineering",
      "One shot",
      "Optics",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Pure mathematics",
      "Relation (database)",
      "Relationship extraction",
      "Shot (pellet)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Zijun"
      },
      {
        "surname": "Hu",
        "given_name": "Zhengping"
      },
      {
        "surname": "Luo",
        "given_name": "Weiwei"
      },
      {
        "surname": "Hu",
        "given_name": "Xiao"
      }
    ]
  },
  {
    "title": "Continuous label distribution learning",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109056",
    "abstract": "Label distribution learning (LDL) is a suitable paradigm to deal with label ambiguity through learning the correlations among different labels. Most existing label distribution learning methods consider the labels to be discrete and directly establish the mapping from features to labels. However, in many real-world applications, labels naturally form a continuous distribution, which is ignored by the existing methods. As a result, the distribution information of labels can not be accurately described and finally affects the whole learning system. The goal of this paper is to propose a novel approach which can capture the continuous distribution of different labels explicitly and effectively. Specifically, we propose Continuous Label Distribution Learning (CLDL) which describes labels as a continuous density function and learns the distribution information of the labels in the latent space. In this way, the high-order correlations among different labels can be effectively extracted and only a few parameters for describing the continuous distribution need to be learned. Extensive description degree prediction experiments on real-world datasets validate the superiority of CLDL over the existing approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322005362",
    "keywords": [
      "Ambiguity",
      "Artificial intelligence",
      "Computer science",
      "Distribution (mathematics)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Programming language"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Xingyu"
      },
      {
        "surname": "An",
        "given_name": "Yuexuan"
      },
      {
        "surname": "Xu",
        "given_name": "Ning"
      },
      {
        "surname": "Geng",
        "given_name": "Xin"
      }
    ]
  },
  {
    "title": "SHCNet: A semi-supervised hypergraph convolutional networks based on relevant feature selection for hyperspectral image classification",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2022.12.004",
    "abstract": "Hyperspectral imagery classification is a challenging task due to the large number of spectral bands, and low number of labeled samples. To overcome these issues, we propose a novel approach for hyperspectral image classification based on feature selection and semi-supervised hypergraph convolutional network working with small number of labeled samples. Firstly, we propose a new unsupervised feature selection method based on an information theoretic criterion. Relevant spectral features are automatically selected while preserving the physical properties of hyperspectral data. Secondly, we construct a spectro-spatial hypergraph in order to represent the complex relationships between pixels. Finally, we propose a semi-supervised hypergraph convolutional network which integrates local vertex features and hypergraph topology in the convolutional layers. The aim of this step is to preserve the spectro-spatial features and to cope with the high correlation between hypernodes during classification. The main advantage of the proposed approach is to allow the automatic selection of relevant spectral bands while preserving the spatial and spectral features. In addition, by accounting for the relationship between pixels leads to improved classification results even when the number of labeled samples is low. Experiments are conducted on two real hyperspectral images show that the proposed approach reaches competitive good performances, and achieves better classification performances compared to state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522003737",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Discrete mathematics",
      "Feature (linguistics)",
      "Feature extraction",
      "Feature selection",
      "Hypergraph",
      "Hyperspectral imaging",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pixel",
      "Selection (genetic algorithm)"
    ],
    "authors": [
      {
        "surname": "Sellami",
        "given_name": "Akrem"
      },
      {
        "surname": "Farah",
        "given_name": "Mohamed"
      },
      {
        "surname": "Dalla Mura",
        "given_name": "Mauro"
      }
    ]
  },
  {
    "title": "A unified model for the sparse optimal scoring problem",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.108976",
    "abstract": "Optimal scoring (OS), an equivalent form of linear discriminant analysis (LDA), is an important supervised learning method and dimensionality reduction tool. However, it is still a challenge for the classical OS on small sample size (SSS) datasets. In this paper, to find sparse discriminant vectors, we propose a unified model for sparse optimal scoring (SOS) by virtue of the generalized ℓ q -norm ( 0 ≤ q ≤ 1 ). To overcome the difficulty in treating the generalized ℓ q -norm, we propose an efficient alternative direction method of multipliers (ADMM), where proximity operator of ℓ q -norm is employed for different q values. Meanwhile, the convergence results of our method are also established. Numerical experiments on artificial and benchmark datasets demonstrate the effectiveness and feasibility of our proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322004563",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Curse of dimensionality",
      "Dimensionality reduction",
      "Discriminant",
      "Geodesy",
      "Geography",
      "Law",
      "Linear discriminant analysis",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Norm (philosophy)",
      "Pattern recognition (psychology)",
      "Political science"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Guoquan"
      },
      {
        "surname": "Yang",
        "given_name": "Linxi"
      },
      {
        "surname": "Zhao",
        "given_name": "Kequan"
      }
    ]
  },
  {
    "title": "Learning based method for near field acoustic range estimation in spherical harmonics domain using intensity vectors",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2022.11.022",
    "abstract": "Near-field acoustic range estimation is considered one of the least explored research problems in digital signal processing under noise and reverberant conditions. This letter develops a new learning-based range estimation technique utilizing the spherical harmonics intensity (SH-INT) coefficients. The conventional range estimation in the spherical harmonics (SH) domain relies on the pressure coefficients. However, at high frequencies, these coefficients of different order and range overlap and hinder the accuracy of range estimation. On the contrary, the SH-INT coefficients are well distinguished at high frequencies for various orders and ranges, making these features favorable for accurate range estimation using learning algorithms. Since the SH-INT coefficients in the radial direction are independent of the source signal and vary with range, a convolutional neural network (CNN) model has been adopted to map the SH-INT coefficients with the range classes. The performance of the proposed spherical harmonic intensity (SH-INT) features in the context of near-field range estimation is validated by conducting exhaustive experiments on simulated and real data. Further, the error in near-field source range estimates is characterized using root mean square error (RMSE) criteria. The results are impactful and encourage the use of this method for practical near-field source range estimation applications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522003506",
    "keywords": [
      "Acoustics",
      "Algorithm",
      "Composite material",
      "Computer science",
      "Harmonic",
      "Harmonics",
      "Intensity (physics)",
      "Materials science",
      "Mathematical analysis",
      "Mathematics",
      "Mean squared error",
      "Optics",
      "Physics",
      "Quantum mechanics",
      "Range (aeronautics)",
      "Spherical harmonics",
      "Statistics",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Dwivedi",
        "given_name": "Priyadarshini"
      },
      {
        "surname": "Routray",
        "given_name": "Gyanajyoti"
      },
      {
        "surname": "Hegde",
        "given_name": "Rajesh M."
      }
    ]
  },
  {
    "title": "Online change-point detection with kernels",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109022",
    "abstract": "Change-points in time series data are usually defined as the time instants at which changes in their properties occur. Detecting change-points is critical in a number of applications as diverse as detecting credit card and insurance frauds, or intrusions into networks. Recently the authors introduced an online kernel-based change-point detection method built upon direct estimation of the density ratio on consecutive time intervals. This paper further investigates this algorithm, making improvements and analyzing its behavior in the mean and mean square sense, in the absence and presence of a change point. These theoretical analyses are validated with Monte Carlo simulations. The detection performance of the algorithm is illustrated through experiments on real-world data and compared to state of the art methodologies.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322005027",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Change detection",
      "Combinatorics",
      "Computer science",
      "Credit card",
      "Credit card fraud",
      "Estimator",
      "Geometry",
      "Kernel (algebra)",
      "Kernel density estimation",
      "Mathematics",
      "Monte Carlo method",
      "Paleontology",
      "Payment",
      "Point (geometry)",
      "Series (stratigraphy)",
      "Statistics",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Ferrari",
        "given_name": "André"
      },
      {
        "surname": "Richard",
        "given_name": "Cédric"
      },
      {
        "surname": "Bourrier",
        "given_name": "Anthony"
      },
      {
        "surname": "Bouchikhi",
        "given_name": "Ikram"
      }
    ]
  },
  {
    "title": "Transformed domain convolutional neural network for Alzheimer's disease diagnosis using structural MRI",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109031",
    "abstract": "Structural magnetic resonance imaging (sMRI) has become a prevalent and potent imaging modality for the computer-aided diagnosis (CAD) of neurological diseases like dementia. Recently, a handful of deep learning techniques such as convolutional neural networks (CNNs) have been proposed to diagnose Alzheimer's disease (AD) by learning the atrophy patterns available in sMRIs. Although CNN-based techniques have demonstrated superior performance and characteristics compared to conventional learning-based classifiers, their diagnostic performance still needs to be improved for reliable classification results. The drawback of current CNN-based approaches is the requirement to locate discriminative landmark (LM) locations by identifying regions of interest (ROIs) in sMRIs, thus the performance of the whole framework is highly influenced by the LM detection step. To overcome this issue, we propose a novel three-dimensional Jacobian domain convolutional neural network (JD-CNN) to diagnose AD subjects and achieve excellent classification performance without the involvement of the LM detection framework. We train the proposed JD-CNN model on the basis of features generated by transforming the sMRI from the spatial domain to the Jacobian domain. The proposed JD-CNN is evaluated on baseline T1-weighted sMRI data collected from 154 healthy control (HC) and 84 Alzheimer's disease (AD) subjects in the Alzheimer's disease neuroimaging initiative (ADNI) database. The proposed JD-CNN exhibits superior classification performance to previously reported state-of-the-art techniques.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322005118",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Dementia",
      "Discriminative model",
      "Disease",
      "Machine learning",
      "Medicine",
      "Neuroimaging",
      "Neuroscience",
      "Pathology",
      "Pattern recognition (psychology)",
      "Psychology"
    ],
    "authors": [
      {
        "surname": "Qasim Abbas",
        "given_name": "S."
      },
      {
        "surname": "Chi",
        "given_name": "Lianhua"
      },
      {
        "surname": "Chen",
        "given_name": "Yi-Ping Phoebe"
      }
    ]
  },
  {
    "title": "Adversarial scratches: Deployable attacks to CNN classifiers",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.108985",
    "abstract": "A growing body of work has shown that deep neural networks are susceptible to adversarial examples. These take the form of small perturbations applied to the model’s input which lead to incorrect predictions. Unfortunately, most literature focuses on visually imperceivable perturbations to be applied to digital images that often are, by design, impossible to be deployed to physical targets. We present Adversarial Scratches: a novel L 0 black-box attack, which takes the form of scratches in images, and which possesses much greater deployability than other state-of-the-art attacks. Adversarial Scratches leverage Bézier Curves to reduce the dimension of the search space and possibly constrain the attack to a specific location. We test Adversarial Scratches in several scenarios, including a publicly available API and images of traffic signs. Results show that our attack achieves higher fooling rate than other deployable state-of-the-art methods, while requiring significantly fewer queries and modifying very few pixels.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322004654",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Deep neural networks",
      "Dimension (graph theory)",
      "Leverage (statistics)",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pixel",
      "Pure mathematics"
    ],
    "authors": [
      {
        "surname": "Giulivi",
        "given_name": "Loris"
      },
      {
        "surname": "Jere",
        "given_name": "Malhar"
      },
      {
        "surname": "Rossi",
        "given_name": "Loris"
      },
      {
        "surname": "Koushanfar",
        "given_name": "Farinaz"
      },
      {
        "surname": "Ciocarlie",
        "given_name": "Gabriela"
      },
      {
        "surname": "Hitaj",
        "given_name": "Briland"
      },
      {
        "surname": "Boracchi",
        "given_name": "Giacomo"
      }
    ]
  },
  {
    "title": "Image manipulation detection by multiple tampering traces and edge artifact enhancement",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109026",
    "abstract": "Image manipulation detection has attracted considerable attention owing to the increasing security risks posed by fake images. Previous studies have proven that tampering traces hidden in images are essential for detecting manipulated regions. However, existing methods have limitations in generalization and the ability to tackle post-processing methods. This paper presents a novel Network to learn and Enhance Multiple tampering Traces (EMT-Net), including noise distribution and visual artifacts. For better generalization, EMT-Net extracts global and local noise features from noise maps using transformers and captures local visual artifacts from original RGB images using convolutional neural networks. Moreover, we enhance fused tampering traces using the proposed edge artifacts enhancement modules and edge supervision strategy to discover subtle edge artifacts hidden in images. Thus, EMT-Net can prevent the risks of losing slight visual clues against well-designed post-processing methods. Experimental results indicate that the proposed method can detect manipulated regions and outperform state-of-the-art approaches under comprehensive quantitative metrics and visual qualities. In addition, EMT-Net shows robustness when various post-processing methods further manipulate images.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322005064",
    "keywords": [
      "Artifact (error)",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Edge detection",
      "Enhanced Data Rates for GSM Evolution",
      "Gene",
      "Generalization",
      "Image (mathematics)",
      "Image processing",
      "Mathematical analysis",
      "Mathematics",
      "Noise (video)",
      "Pattern recognition (psychology)",
      "RGB color model",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Lin",
        "given_name": "Xun"
      },
      {
        "surname": "Wang",
        "given_name": "Shuai"
      },
      {
        "surname": "Deng",
        "given_name": "Jiahao"
      },
      {
        "surname": "Fu",
        "given_name": "Ying"
      },
      {
        "surname": "Bai",
        "given_name": "Xiao"
      },
      {
        "surname": "Chen",
        "given_name": "Xinlei"
      },
      {
        "surname": "Qu",
        "given_name": "Xiaolei"
      },
      {
        "surname": "Tang",
        "given_name": "Wenzhong"
      }
    ]
  },
  {
    "title": "Query efficient black-box adversarial attack on deep neural networks",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109037",
    "abstract": "Deep neural networks (DNNs) have demonstrated excellent performance on various tasks, yet they are under the risk of adversarial examples that can be easily generated when the target model is accessible to an attacker (white-box setting). As plenty of machine learning models have been deployed via online services that only provide query outputs from inaccessible models (e.g., Google Cloud Vision API2), black-box adversarial attacks raise critical security concerns in practice rather than white-box ones. However, existing query-based black-box adversarial attacks often require excessive model queries to maintain a high attack success rate. Therefore, in order to improve query efficiency, we explore the distribution of adversarial examples around benign inputs with the help of image structure information characterized by a Neural Process, and propose a Neural Process based black-box adversarial attack (NP-Attack) in this paper. Our proposed NP-Attack could be further boosted when applied with surrogate models or tiling tricks. Extensive experiments show that NP-Attack could greatly decrease the query counts under the black-box setting.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322005179",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Artificial neural network",
      "Black box",
      "Cloud computing",
      "Computer science",
      "Computer security",
      "Deep learning",
      "Deep neural networks",
      "Machine learning",
      "Operating system",
      "Process (computing)",
      "Threat model",
      "White box"
    ],
    "authors": [
      {
        "surname": "Bai",
        "given_name": "Yang"
      },
      {
        "surname": "Wang",
        "given_name": "Yisen"
      },
      {
        "surname": "Zeng",
        "given_name": "Yuyuan"
      },
      {
        "surname": "Jiang",
        "given_name": "Yong"
      },
      {
        "surname": "Xia",
        "given_name": "Shu-Tao"
      }
    ]
  },
  {
    "title": "From Distortion Manifold to Perceptual Quality: a Data Efficient Blind Image Quality Assessment Approach",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109047",
    "abstract": "Though current no-reference image quality assessment (NR-IQA) approaches have achieved impressive performance gain thanks to deep learning techniques, it is claimed that the risk of over-fitting exists. To improve model generalization ability, most of the current researches incorporate mass data to train or tune the data-driven models. However, the process of image data collection and quality label annotation is quite time-consuming and labour-intensive. Therefore, in this paper, we explore an alternative solution to promote model generalizability but with relatively small fractions of training data. Compared with previous approaches which make effort to approximate the whole complex image distribution, we propose to explicitly learn an image distortion manifold first, which lies in a much lower dimension space and also representative in capturing general degradation patterns. We then project the images to their perceived quality from the learned manifold to obtain quality predictions. Since the manifold embeds general distortion features despite of varying image contents, it can be learned with relatively small amount of samples. In order to learn the manifold and quality projection, we introduce a two-branched network to learn both low level distortions and high level semantics. We also propose a simple but efficient training framework, composing of a masked labelling strategy and a gradual weighting curriculum to fulfill the task. Thanks to the learned distortion manifold, the proposed model achieves superior generalizability compared with previous models. Extensive experiments demonstrate its effectiveness in terms of training with limited data, testing on large scale images, and with unseen types of distorted images.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322005271",
    "keywords": [
      "Algorithm",
      "Amplifier",
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Benchmark (surveying)",
      "Computer network",
      "Computer science",
      "Dimension (graph theory)",
      "Dimensionality reduction",
      "Distortion (music)",
      "Engineering",
      "Epistemology",
      "Generalizability theory",
      "Generalization",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Image quality",
      "Machine learning",
      "Manifold (fluid mechanics)",
      "Mathematical analysis",
      "Mathematics",
      "Mechanical engineering",
      "Medicine",
      "Nonlinear dimensionality reduction",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Projection (relational algebra)",
      "Pure mathematics",
      "Quality (philosophy)",
      "Radiology",
      "Statistics",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Su",
        "given_name": "Shaolin"
      },
      {
        "surname": "Yan",
        "given_name": "Qingsen"
      },
      {
        "surname": "Zhu",
        "given_name": "Yu"
      },
      {
        "surname": "Sun",
        "given_name": "Jinqiu"
      },
      {
        "surname": "Zhang",
        "given_name": "Yanning"
      }
    ]
  },
  {
    "title": "Back-compatible Color QR Codes for colorimetric applications",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.108981",
    "abstract": "Color correction techniques in digital photography often rely on the use of color correction charts, which require including this relatively large object in the field of view. We propose here to use QR Codes to pack these color charts in a compact form factor, in a fully compatible manner with conventional black and white QR Codes; this is, without losing any of their easy location, sampling and digital data storage features. First, we present an algorithm to build these new colored QR Codes that preserves the original QR Code functionality - much more than other coloring proposals based on the random substitution of black and white pixels by colors - that relies on the ability of the native CRC code to correct and counteract these alterations. Second, we demonstrate that, as a result, these QR Codes can allocate far many more colors than the conventional color correction charts, enabling much more accurate color correction schemes in a more convenient and usable format.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322004617",
    "keywords": [
      "Artificial intelligence",
      "Code (set theory)",
      "Color balance",
      "Color correction",
      "Color image",
      "Colored",
      "Composite material",
      "Computational photography",
      "Computer graphics (images)",
      "Computer science",
      "Computer vision",
      "Image (mathematics)",
      "Image processing",
      "Materials science",
      "Multimedia",
      "Programming language",
      "Set (abstract data type)",
      "USable"
    ],
    "authors": [
      {
        "surname": "Benito-Altamirano",
        "given_name": "Ismael"
      },
      {
        "surname": "Martínez-Carpena",
        "given_name": "David"
      },
      {
        "surname": "Casals",
        "given_name": "Olga"
      },
      {
        "surname": "Fábrega",
        "given_name": "Cristian"
      },
      {
        "surname": "Waag",
        "given_name": "Andreas"
      },
      {
        "surname": "Prades",
        "given_name": "Joan Daniel"
      }
    ]
  },
  {
    "title": "Few-shot learning with unsupervised part discovery and part-aligned similarity",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.108986",
    "abstract": "Few-shot learning aims to recognize novel concepts with only a few examples. To this end, previous studies resort to acquiring a strong inductive bias via meta-learning on a group of similar tasks, which however needs a large labeled base dataset to sample training tasks. In this paper, we show that such inductive bias can be learned from a flat collection of unlabeled images, and instantiated as transferable representations among seen and unseen classes. Specifically, we propose a novel unsupervised Part Discovery Network (PDN) to learn transferable representations from unlabeled images, which automatically selects the most discriminative part from an input image and then maximizes its similarities to the global view of the input and other neighbors with similar semantics. To better leverage the learned representations for few-shot learning, we further propose Part-Aligned Similarity (PAS), the key of which is to measure image similarities based on a set of discriminative and aligned parts. We conduct extensive studies on five popular few-shot learning datasets to evaluate our approach. The experimental results show that our approach outperforms previous unsupervised methods by a large margin and is even comparable with state-of-the-art supervised methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322004666",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Categorization",
      "Computer science",
      "Discriminative model",
      "Image (mathematics)",
      "Leverage (statistics)",
      "Machine learning",
      "Margin (machine learning)",
      "Pattern recognition (psychology)",
      "Programming language",
      "Semantics (computer science)",
      "Set (abstract data type)",
      "Similarity (geometry)",
      "Supervised learning",
      "Unsupervised learning"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Wentao"
      },
      {
        "surname": "Zhang",
        "given_name": "Zhang"
      },
      {
        "surname": "Wang",
        "given_name": "Wei"
      },
      {
        "surname": "Wang",
        "given_name": "Liang"
      },
      {
        "surname": "Wang",
        "given_name": "Zilei"
      },
      {
        "surname": "Tan",
        "given_name": "Tieniu"
      }
    ]
  },
  {
    "title": "Collaborative Learning with Unreliability Adaptation for Semi-Supervised Image Classification",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109032",
    "abstract": "Constructing training goals for unlabeled data is crucial for image classification in the semi-supervised setting. Consistency regularization typically encourages a model to produce consistent predictions with the given training goals, while unreliability adaptation aims to learn the transition probabilities from model predictions to training goals, instead of enforcing their consistency. In this paper, we present a model of Collaborative learning with Unreliability Adaptation (CoUA), in which multiple constituent networks collaboratively learn with each other by adapting their predictions. Toward this end, an additional adaptation module is incorporated into each network to learn a transition probability from its own prediction to that of the paired network. Therefore, the networks can exchange training experience, without being overly sensitive to the unreliability of predictions. To further enhance the collaborative learning, each network is encouraged to produce consistent predictions with the consensus results, while being resistant to the adversarial perturbations against others. Therefore, the networks are able to mutually reinforce each other. We perform extensive experiments on multiple image classification benchmarks to verify the superiority of the co-adaptation based collaborative learning mechanism.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032200512X",
    "keywords": [
      "Adaptation (eye)",
      "Adversarial system",
      "Artificial intelligence",
      "Computer science",
      "Consistency (knowledge bases)",
      "Machine learning",
      "Optics",
      "Physics",
      "Regularization (linguistics)"
    ],
    "authors": [
      {
        "surname": "Huo",
        "given_name": "Xiaoyang"
      },
      {
        "surname": "Zeng",
        "given_name": "Xiangping"
      },
      {
        "surname": "Wu",
        "given_name": "Si"
      },
      {
        "surname": "Shen",
        "given_name": "Wenjun"
      },
      {
        "surname": "Wong",
        "given_name": "Hau-San"
      }
    ]
  },
  {
    "title": "Locating robust patterns based on invariant of LTP-based features",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2022.11.008",
    "abstract": "Efficiently representing Dynamic Textures (DTs) based on salient features is one of the considerable challenges in computer vision. Locating these features can be obstructed due to the impact of encoding factors. In this article, a novel concept of Robust Local Ternary Patterns (RLTP) is introduced in consideration of the invariance of Local Ternary Patterns (LTP) subject to the deviation of thresholds. Our locating process is able to simultaneously encapsulate the discrimination of local features, and deal with the noise sensibility caused by a small gray-scale change of local neighbors. RLTP is then adapted to the completed LTP model to form an efficient operator for capturing completely properties of RLTP. Finally, RLTP is taken into account for the DT description, where the robust patterns of spatial-temporal features and optical-flow-based motions are exploited to improve the performance. Experiments have clearly corroborated the efficacy of our proposed approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522003373",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Invariant (physics)",
      "Mathematical physics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Salient",
      "Scale invariance",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Nguyen",
        "given_name": "Thanh Tuan"
      },
      {
        "surname": "Nguyen",
        "given_name": "Thanh Phuong"
      },
      {
        "surname": "Thirion-Moreau",
        "given_name": "Nadège"
      }
    ]
  },
  {
    "title": "Deep autoregressive models with spectral attention",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109014",
    "abstract": "Time series forecasting is an important problem across many domains, playing a crucial role in multiple real-world applications. In this paper, we propose a forecasting architecture that combines deep autoregressive models with a Spectral Attention (SA) module, which merges global and local frequency domain information in the model’s embedded space. By characterizing in the spectral domain the embedding of the time series as occurrences of a random process, our method can identify global trends and seasonality patterns. Two spectral attention models, global and local to the time series, integrate this information within the forecast and perform spectral filtering to remove time series’s noise. The proposed architecture has a number of useful properties: it can be effectively incorporated into well-known forecast architectures, requiring a low number of parameters and producing explainable results that improve forecasting accuracy. We test the Spectral Attention Autoregressive Model (SAAM) on several well-known forecast datasets, consistently demonstrating that our model compares favorably to state-of-the-art approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322004940",
    "keywords": [
      "Artificial intelligence",
      "Autoregressive integrated moving average",
      "Autoregressive model",
      "Autoregressive–moving-average model",
      "Biology",
      "Computer science",
      "Data mining",
      "Econometrics",
      "Embedding",
      "Image (mathematics)",
      "Machine learning",
      "Mathematics",
      "Noise (video)",
      "Operating system",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Process (computing)",
      "Series (stratigraphy)",
      "Time series"
    ],
    "authors": [
      {
        "surname": "Moreno-Pino",
        "given_name": "Fernando"
      },
      {
        "surname": "Olmos",
        "given_name": "Pablo M."
      },
      {
        "surname": "Artés-Rodríguez",
        "given_name": "Antonio"
      }
    ]
  },
  {
    "title": "Multi-scale multi-hierarchy attention convolutional neural network for fetal brain extraction",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109029",
    "abstract": "Fetal brain extraction from in utero magnetic resonance imaging (MRI) scans is a key step for fetal brain development analysis. As the unpredicted fetal motion and maternal breathing generally result in blurring and ghosting in the slices of phase encoding direction, using the conventional 3D convolutional neural networks for fetal brain extraction with pseudo 3D fetal brain MR scans will lead to sub-optimal extraction performance. To address this issue, in this paper, we propose a novel multi-scale multi-hierarchy attention convolutional neural network (MSMHA-CNN) for fetal brain extraction in MR images. Specifically, to effectively utilize the 3D contextual information of the in utero MR image for fetal brain extraction, we employ multiple convolutional operations with different local receptive fields (i.e., with different kernel sizes) in each layer to learn the multi-scale feature representation for fetal brain extraction. To effectively use the learned multi-scale feature maps, we introduce a channel-wise spatial attention architecture to adaptively fuse those multi-scale feature maps derived from convolutional operations with different kernel sizes. In this way, the learned multi-scale features can be explicitly used to fetal brain extraction process. Besides, to take advantage of high-level feature maps at all spatial resolutions, we adopt the feature pyramid architecture to learn multi-hierarchy features for boosting the performance. We compare our proposed method with several state-of-the-art methods on two in utero MRI scan datasets (a total of 180 scans) for fetal brain extraction. The experimental results suggest the superior performance of the proposed MSMHA-CNN in comparison with its competitors.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032200509X",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Feature (linguistics)",
      "Feature extraction",
      "Kernel (algebra)",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Sun",
        "given_name": "Liang"
      },
      {
        "surname": "Shao",
        "given_name": "Wei"
      },
      {
        "surname": "Zhu",
        "given_name": "Qi"
      },
      {
        "surname": "Wang",
        "given_name": "Meiling"
      },
      {
        "surname": "Li",
        "given_name": "Gang"
      },
      {
        "surname": "Zhang",
        "given_name": "Daoqiang"
      }
    ]
  },
  {
    "title": "A full data augmentation pipeline for small object detection based on generative adversarial networks",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.108998",
    "abstract": "Object detection accuracy on small objects, i.e., objects under 32 × 32 pixels, lags behind that of large ones. To address this issue, innovative architectures have been designed and new datasets have been released. Still, the number of small objects in many datasets does not suffice for training. The advent of the generative adversarial networks (GANs) opens up a new data augmentation possibility for training architectures without the costly task of annotating huge datasets for small objects. In this paper, we propose a full pipeline for data augmentation for small object detection which combines a GAN-based object generator with techniques of object segmentation, image inpainting, and image blending to achieve high-quality synthetic data. The main component of our pipeline is DS-GAN, a novel GAN-based architecture that generates realistic small objects from larger ones. Experimental results show that our overall data augmentation method improves the performance of state-of-the-art models up to 11.9% AP s @ . 5 on UAVDT and by 4.7% AP s @ . 5 on iSAID, both for the small objects subset and for a scenario where the number of training instances is limited.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322004782",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Generative grammar",
      "Generator (circuit theory)",
      "Image (mathematics)",
      "Inpainting",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Physics",
      "Pipeline (software)",
      "Pixel",
      "Power (physics)",
      "Programming language",
      "Quantum mechanics",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Bosquet",
        "given_name": "Brais"
      },
      {
        "surname": "Cores",
        "given_name": "Daniel"
      },
      {
        "surname": "Seidenari",
        "given_name": "Lorenzo"
      },
      {
        "surname": "Brea",
        "given_name": "Víctor M."
      },
      {
        "surname": "Mucientes",
        "given_name": "Manuel"
      },
      {
        "surname": "Bimbo",
        "given_name": "Alberto Del"
      }
    ]
  },
  {
    "title": "DisRFC: a dissimilarity-based Random Forest Clustering approach",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109036",
    "abstract": "In this paper we present a novel Random Forest Clustering approach, called Dissimilarity Random Forest Clustering (DisRFC), which requires in input only pairwise dissimilarities. Thanks to this characteristic, the proposed approach is appliable to all those problems which involve non-vectorial representations, such as strings, sequences, graphs or 3D structures. In the proposed approach, we first train an Unsupervised Dissimilarity Random Forest (UD-RF), a novel variant of Random Forest which is completely unsupervised and based on dissimilarities. Then, we exploit the trained UD-RF to project the patterns to be clustered in a binary vectorial space, where the clustering is finally derived using fast and effective K-means procedures. In the paper we introduce different variants of DisRFC, which are thoroughly and positively evaluated on 12 different problems, also in comparison with alternative state-of-the-art approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322005167",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Computer security",
      "Data mining",
      "Exploit",
      "Mathematics",
      "Pairwise comparison",
      "Pattern recognition (psychology)",
      "Random forest"
    ],
    "authors": [
      {
        "surname": "Bicego",
        "given_name": "Manuele"
      }
    ]
  },
  {
    "title": "GripNet: Graph information propagation on supergraph for heterogeneous graphs",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.108973",
    "abstract": "Heterogeneous graph representation learning aims to learn low-dimensional vector representations of different types of entities and relations to empower downstream tasks. Existing popular methods either capture semantic relationships but indirectly leverage node/edge attributes in a complex way, or leverage node/edge attributes directly without taking semantic relationships into account. When involving multiple convolution operations, they also have poor scalability. To overcome these limitations, this paper proposes a flexible and efficient Graph information propagation Network (GripNet) framework. Specifically, we introduce a new supergraph data structure consisting of supervertices and superedges. A supervertex is a semantically-coherent subgraph. A superedge defines an information propagation path between two supervertices. GripNet learns new representations for the supervertex of interest by propagating information along the defined path using multiple layers. We construct multiple large-scale graphs and evaluate GripNet against competing methods to show its superiority in link prediction, node classification, and data integration. The code and data are available at https://github.com/nyxflower/GripNet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322004538",
    "keywords": [
      "Combinatorics",
      "Computer science",
      "Epigraph",
      "Graph",
      "Mathematical optimization",
      "Mathematics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Hao"
      },
      {
        "surname": "Sang",
        "given_name": "Shengqi"
      },
      {
        "surname": "Bai",
        "given_name": "Peizhen"
      },
      {
        "surname": "Li",
        "given_name": "Ruike"
      },
      {
        "surname": "Yang",
        "given_name": "Laurence"
      },
      {
        "surname": "Lu",
        "given_name": "Haiping"
      }
    ]
  },
  {
    "title": "Game-theoretic hypergraph matching with density enhancement",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109035",
    "abstract": "Feature matching plays a fundamental role in computer vision and pattern recognition. As straightforward comparison of feature descriptors is not enough to provide reliable matching results in many situations, graph matching makes use of the pairwise relationship between features to improve matching accuracy. Hypergraph matching further employs the relationship among multiple features to provide more invariance between feature correspondences. Existing hypergraph matching algorithms usually solve an assignment problem, where outliers may result in a large number of false matches. In this paper we cast the hypergraph matching problem as a non-cooperative multi-player game, and obtain the matches by extracting the evolutionary stable strategies. Our algorithm exerts a strong constraint on the consistency of obtained matches, and false matches are excluded effectively. In order to increase the number of matches without increasing the computation load evidently, we present a density enhancement method to improve the matching results. We further propose two methods to enforce the one-to-one constraint, thereby removing false matches and maintaining a high matching accuracy. Experiments with both synthetic and real datasets validate the effectiveness of our algorithm.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322005155",
    "keywords": [
      "3-dimensional matching",
      "Algorithm",
      "Artificial intelligence",
      "Blossom algorithm",
      "Computer science",
      "Consistency (knowledge bases)",
      "Constraint (computer-aided design)",
      "Discrete mathematics",
      "Feature (linguistics)",
      "Geometry",
      "Hypergraph",
      "Linguistics",
      "Matching (statistics)",
      "Mathematics",
      "Outlier",
      "Pairwise comparison",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Hou",
        "given_name": "Jian"
      },
      {
        "surname": "Yuan",
        "given_name": "Huaqiang"
      },
      {
        "surname": "Pelillo",
        "given_name": "Marcello"
      }
    ]
  },
  {
    "title": "A self-attentive model for tracing knowledge and engagement in parallel",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2022.11.016",
    "abstract": "Knowledge tracing refers to the task of modeling students’ evolving knowledge state according to their historical learning trajectories. Although many research has devoted to exploiting the latent information of question-concept bipartite graph, most of them need to split the graph into question-question and concept-concept graph to obtain questions’ and concepts’ representation respectively. This operation will cause information loss of original bipartite graph. To this end, an adaptive heterogeneous graph embedding module is proposed to utilize the latent information of graph adequately. Besides, many works ignore that students’ engagement could also impact students’ learning performance. Therefore, a student engagement encoder and a student knowledge state encoder are designed to capture students’ engagement and knowledge respectively. To be specific, the question-concept bipartite graph is input to graph embedding module directly to obtain each question’s and concept’s representation, the two encoders use these representation and students’ learning history to encode their engagement and knowledge state.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522003464",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Parallel computing",
      "Programming language",
      "Theoretical computer science",
      "Tracing"
    ],
    "authors": [
      {
        "surname": "Jiang",
        "given_name": "Hua"
      },
      {
        "surname": "Xiao",
        "given_name": "Bing"
      },
      {
        "surname": "Luo",
        "given_name": "Yintao"
      },
      {
        "surname": "Ma",
        "given_name": "Junliang"
      }
    ]
  },
  {
    "title": "Exploratory Adversarial Attacks on Graph Neural Networks for Semi-Supervised Node Classification",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109042",
    "abstract": "Graph neural networks (GNNs) have been successfully used to analyze non-Euclidean network data. Recently, there emerge a number of works to investigate the robustness of GNNs by adding adversarial noises into the graph topology, where the gradient-based attacks are widely studied due to their inherent efficiency and high effectiveness. However, the gradient-based attacks often lead to sub-optimal results due to the discrete structure of graph data. To address this issue, we propose a novel exploratory adversarial attack (termed as EpoAtk) to boost the gradient-based perturbations on graphs. The exploratory strategy in EpoAtk includes three phases, generation, evaluation and recombination, with the goal of sidestepping the possible misinformation that the maximal gradient provides. In particular, our evaluation phase introduces a self-training objective containing three effective evaluation functions to fully exploit the useful information of unlabeled nodes. EpoAtk is evaluated on multiple benchmark datasets for the task of semi-supervised node classification in different attack settings. Extensive experimental results demonstrate that the proposed method achieves consistent and significant improvements over the state-of-the-art adversarial attacks with the same attack budgets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322005222",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer security",
      "Data mining",
      "Exploit",
      "Gene",
      "Graph",
      "Machine learning",
      "Robustness (evolution)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Lin",
        "given_name": "Xixun"
      },
      {
        "surname": "Zhou",
        "given_name": "Chuan"
      },
      {
        "surname": "Wu",
        "given_name": "Jia"
      },
      {
        "surname": "Yang",
        "given_name": "Hong"
      },
      {
        "surname": "Wang",
        "given_name": "Haibo"
      },
      {
        "surname": "Cao",
        "given_name": "Yanan"
      },
      {
        "surname": "Wang",
        "given_name": "Bin"
      }
    ]
  },
  {
    "title": "Underwater sEMG-based recognition of hand gestures using tensor decomposition",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2022.11.021",
    "abstract": "Amputees have limited ability to complete specific movements because of the loss of hands. Prosthetic hands can help amputees as an effective human-computer interaction system in their daily lives, and some amputees need to use the prosthetic hands for underwater operations. Therefore, it is necessary to solve the problem of using prosthetic hands underwater. There are two main problems in underwater surface Electromyogram (sEMG) signal recognition. The underwater sEMG signals are disturbed by noise, and the traditional sEMG features are easily affected by noise, decreasing the recognition accuracy of underwater sEMG signals. It is difficult for subjects to acquire quantity training data underwater, and satisfactory sEMG recognition accuracy needs to be obtained based on small datasets. Tensor decomposition has the advantage of finding potential features of signals, and it is widely used in many fields. Tucker tensor decomposition was used for feature extraction and recognition of underwater sEMG signals. Seven subjects were selected to complete four hand gestures underwater and two-channel sEMG signals were collected. Wavelet transform was applied to generate a three-dimensional tensor and extracted signal features by tensor decomposition. The recognition accuracy based on K-Nearest Neighbor reaches 96.43%. The results show that the proposed sEMG feature extraction method based on tensor decomposition helps improve the recognition accuracy of underwater sEMG signals, which provides a basis for applying prosthetic hands in a water environment.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522003518",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Feature extraction",
      "Geology",
      "Gesture",
      "Image (mathematics)",
      "Linguistics",
      "Mathematics",
      "Noise (video)",
      "Oceanography",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Pure mathematics",
      "SIGNAL (programming language)",
      "Speech recognition",
      "Tensor (intrinsic definition)",
      "Tensor decomposition",
      "Underwater"
    ],
    "authors": [
      {
        "surname": "Xue",
        "given_name": "Jianing"
      },
      {
        "surname": "Sun",
        "given_name": "Zhe"
      },
      {
        "surname": "Duan",
        "given_name": "Feng"
      },
      {
        "surname": "Caiafa",
        "given_name": "Cesar F."
      },
      {
        "surname": "Solé-Casals",
        "given_name": "Jordi"
      }
    ]
  },
  {
    "title": "RIS-GAN: Self-Supervised GANs via Recovering Initial State of Subimages",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2022.12.005",
    "abstract": "Self-supervised methods play a vital role in representation learning without the need for annotated data and learn feature representations where data itself provides supervision. However, it is still challenging for the discriminator to learn reliable representations. To address this issue, we propose the self-supervised GANs via recovering initial state of subimages (RIS-GAN). Recovering initial state (RIS) task requires the network to recover the original state information of each sub-part of a composite image. Specifically, the network should predict the rotation angle and the initial position of subimages, and also predict which original image each subimage belongs to. To recover these information, the network should capture intra-image and inter-image information and extract high-similarity features between those patches belonging to the same original images. Besides, we propose local-discriminator, to improve the ability of the discriminator to judge global features from local patch features. To accomplish the above task, our proposed model should extract multi-level information to improve the understanding of image structure and content, and the ability to mine valuable information from image patches. We experimentally demonstrate the state-of-the-art performance of our RIS-GAN with respect to other self-supervised GAN methods on CIFAR-10, ImageNet 32 × 32, CelebA, and STL-10 datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522003713",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Detector",
      "Discriminator",
      "Economics",
      "Feature (linguistics)",
      "Image (mathematics)",
      "Law",
      "Linguistics",
      "Management",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Rotation (mathematics)",
      "Similarity (geometry)",
      "State (computer science)",
      "Task (project management)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Wei",
        "given_name": "Ying"
      },
      {
        "surname": "Zhang",
        "given_name": "Jianwei"
      },
      {
        "surname": "Zhong",
        "given_name": "Longqi"
      }
    ]
  },
  {
    "title": "UAVformer: A Composite Transformer Network for Urban Scene Segmentation of UAV Images",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109019",
    "abstract": "Urban scenes segmentation based on UAV (Unmanned aerial vehicle) view is a fundamental task for the applications of smart city such as city planning, land use monitoring, traffic monitoring, and crowd estimation. While urban scenes in UAV image characteristic by large scale variation of objects size and complexity background, which posed challenges to urban scenes segmentation of UAV image. The feature extracting backbone of existing networks cannot extract complex features of UAV image effectively, which limits the performance of urban scenes segmentation. To design segmentation network capable of extracting features of large scale variation urban ground scenes, this study proposed a novel composite transformer network for urban scenes segmentation of UAV image. A composite backbone with aggregation windows multi-head self-attention transformer blocks is proposed to make the extracted features more representatives by adaptive multi-level features fusion, and the full utilisation of contextual information and local information. Position attention modules are inserted in each stage between encoder and decoder to further enhance the spatial attention of extracted feature maps. Finally, a V-shaped decoder which is capable of utilising multi-level features is designed to get accurately dense prediction. The accuracy of urban scenes segmentation could significantly be enhanced in this way and successfully segmented the large scale variation objects from UAV views. Extensive ablation experiments and comparative experiments for the proposed network have been conducted on the public available urban scenes segmentation datasets for UAV imagery. Experimental results have demonstrated the effectiveness of designed network structure and the superiority of proposed network over state-of-the-art methods. Specifically, reached 53.2% mIoU on the UAVid dataset and 77.6% mIoU on the UDD6 dataset, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032200499X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Encoder",
      "Image segmentation",
      "Operating system",
      "Pattern recognition (psychology)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Yi",
        "given_name": "Shi"
      },
      {
        "surname": "Liu",
        "given_name": "Xi"
      },
      {
        "surname": "Li",
        "given_name": "Junjie"
      },
      {
        "surname": "Chen",
        "given_name": "Ling"
      }
    ]
  },
  {
    "title": "Margin embedding net for robust margin collaborative representation-based classification",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.108991",
    "abstract": "Collaborative Representation-based Classification method (CRC) shows great potential in classification task. However, redundancies in both features and samples limit the application of CRC seriously. The existing works only solve one of them and ignore the other, which leads to performance degradation. To address this problem, we explore collaborative representation mechanism and propose a classification method termed Robust Margin Collaborative Representation-based Classification (RMCRC) which uses a few but more representative robust marginal samples to eliminate redundancy between samples. As the performance of RMCRC is related to robust marginal samples and class separability assumption closely, we further propose a feature extraction method termed Margin Embedding Net (MEN) for RMCRC. In MEN, virtual samples are generated by a generative model to enhance effectiveness of robust marginal samples and generalizability of RMCRC. Then, an embedding network with triplet loss is used to eliminate the redundancy in features and ensure the assumption is satisfied. Specifically, we construct triplet according to the collaborative representation. Hence, MEN fits RMCRC very well. Extensive experimental results validate effectiveness of proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032200471X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Embedding",
      "Generalizability theory",
      "Law",
      "Machine learning",
      "Margin (machine learning)",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Redundancy (engineering)",
      "Representation (politics)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Zheng",
        "given_name": "Zhichao"
      },
      {
        "surname": "Sun",
        "given_name": "Huaijiang"
      },
      {
        "surname": "Zhou",
        "given_name": "Ying"
      }
    ]
  },
  {
    "title": "A novel ensemble model with two-stage learning for joint dialog act recognition and sentiment classification",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2022.11.032",
    "abstract": "Dialog act recognition (DAR) and sentiment classification (SC) are two correlative tasks to capture speakers’ intentions, where dialog act and sentiment can indicate the explicit and the implicit intentions separately. Current state-of-the-art methods usually leverage the graph neural networks or attention mechanism to capture the contextual dependency of an utterance, thereby aiding in efficient identification of the speakers’ intentions. Although existing attention mechanisms can automatically obtain the corresponding coefficient of each utterance based on the utterance-level embeddings, they often ignore the reliability of the utterance-level embeddings. This promotes us to propose an ensemble model with two-stage learning for joint DAR and SC. Firstly, we employ the BiLSTM as the encoder to capture the features of utterances, which are referred to “confidence vector” of the utterance-level embedding. Further, we introduce an edge-aware graph attention network to improve the classifier’s performance by using the confidence vector to selectively leverage the contextual information. Experimental results on two benchmark datasets show that our framework obtains state-of-the-art performances against all baselines, demonstrating the effectiveness of our method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522003671",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Dialog box",
      "Encoder",
      "Leverage (statistics)",
      "Machine learning",
      "Natural language processing",
      "Operating system",
      "Pattern recognition (psychology)",
      "Sentiment analysis",
      "Speech recognition",
      "Utterance",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Yujun"
      },
      {
        "surname": "Yao",
        "given_name": "Enguang"
      },
      {
        "surname": "Liu",
        "given_name": "Chaoyue"
      },
      {
        "surname": "Liu",
        "given_name": "Qidong"
      },
      {
        "surname": "Xu",
        "given_name": "Mingliang"
      }
    ]
  },
  {
    "title": "Investigating intrinsic degradation factors by multi-branch aggregation for real-world underwater image enhancement",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109041",
    "abstract": "Recently, improving the visual quality of underwater images has received extensive attentions in both computer vision and ocean engineering fields. However, existing works mostly focus on directly learning clear images from degraded observations but without careful investigations on the intrinsic degradation factors, thus require mass training data and lack generalization ability. In this work, we propose a new method, named Multi-Branch Aggregation Network (termed as MBANet) to partially address the above issue. Specifically, by analyzing underwater degradation factors from the perspective of both color distortions and veil effects, MBANet first constructs a multi-branch multi-variable architecture to obtain one intermediate coarse result and two degraded factors. We then establish a physical model inspired process to fully utilize our estimated degraded factors and thus obtain the desired clear output images. A series of evaluations on multiple datasets show the superiority of our method against existing state-of-the-art approaches, both in execution speed and accuracy. Furthermore, we demonstrate that our MBANet can significantly improve the performance of salience object detection in the underwater environment.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322005210",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Degradation (telecommunications)",
      "Generalization",
      "Geology",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Oceanography",
      "Operating system",
      "Pattern recognition (psychology)",
      "Perspective (graphical)",
      "Process (computing)",
      "Salience (neuroscience)",
      "Telecommunications",
      "Underwater"
    ],
    "authors": [
      {
        "surname": "Xue",
        "given_name": "Xinwei"
      },
      {
        "surname": "Li",
        "given_name": "Zexuan"
      },
      {
        "surname": "Ma",
        "given_name": "Long"
      },
      {
        "surname": "Jia",
        "given_name": "Qi"
      },
      {
        "surname": "Liu",
        "given_name": "Risheng"
      },
      {
        "surname": "Fan",
        "given_name": "Xin"
      }
    ]
  },
  {
    "title": "Answering knowledge-based visual questions via the exploration of Question Purpose",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109015",
    "abstract": "Visual question answering has been greatly advanced by deep learning technologies, but still remains an open problem subjected to two aspects of factors. First, previous works estimate the correctness of each candidate answer mainly by its semantic correlations with visual questions, overlooking the fact that some questions and their answers are semantically inconsistent. Second, previous works that require external knowledge mainly uses the knowledge facts retrieved by key words or visual objects. However, the retrieved knowledge facts may only be related to the semantics of the question, but are useless or even misleading for answer prediction. To address these issues, we investigate how to capture the purpose of visual questions and propose a Purpose Guided Visual Question Answering model, called PGVQA. It mainly has two appealing properties: (1) It can estimate the correctness of candidate answers based on the Question Purpose (QP) that reveals which aspects of the concept are examined by visual questions. This is helpful for avoiding the negative effect of the semantic inconsistency between answers and questions. (2) It can incorporate the knowledge facts accordant with the QP into answer prediction, which helps to improve the probability of answering visual questions correctly. Empirical studies on benchmark datasets show that PGVQA achieves state-of-the-art performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322004952",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Computer security",
      "Correctness",
      "Geodesy",
      "Geography",
      "Information retrieval",
      "Key (lock)",
      "Machine learning",
      "Natural language processing",
      "Programming language",
      "Question answering",
      "Semantics (computer science)",
      "Visualization"
    ],
    "authors": [
      {
        "surname": "Song",
        "given_name": "Lingyun"
      },
      {
        "surname": "Li",
        "given_name": "Jianao"
      },
      {
        "surname": "Liu",
        "given_name": "Jun"
      },
      {
        "surname": "Yang",
        "given_name": "Yang"
      },
      {
        "surname": "Shang",
        "given_name": "Xuequn"
      },
      {
        "surname": "Sun",
        "given_name": "Mingxuan"
      }
    ]
  },
  {
    "title": "GCM: Efficient video recognition with glance and combine module",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.108970",
    "abstract": "In this work, we present an efficient and powerful building block for video action recognition, dubbed Glance and Combine Module (GCM). In order to obtain a broader perspective of the video features, GCM introduces an extra glancing operation with a larger receptive field over both the spatial and temporal dimensions, and combines features with different receptive fields for further processing. We show in our ablation studies that the proposed GCM is much more efficient than other forms of 3D spatio-temporal convolutional blocks. We build a series of GCM networks by stacking GCM repeatedly, and train them from scratch on the target datasets directly. On the Kinetics-400 dataset which focuses more on appearance rather than action, our GCM networks can achieve similar accuracy as others without pre-training on ImageNet. For the more action-centric recognition datasets such as Something-Something (V1 & V2) and Multi-Moments in Time, the GCM networks achieve state-of-the-art performance with less than two thirds the computational complexity of other models. With only 19.2 GFLOPs of computation, our GCMNet 15 can obtain 63.9% top-1 classification accuracy on Something-Something-V2 validation set under single-crop testing. On the fine-grained action recognition dataset FineGym, we beat the previous state-of-the-art accuracy achieved with 2-stream methods by more than 6% using only RGB input.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322004502",
    "keywords": [
      "Action recognition",
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Block (permutation group theory)",
      "Class (philosophy)",
      "Climate change",
      "Computation",
      "Computer science",
      "Convolutional neural network",
      "Ecology",
      "Field (mathematics)",
      "GCM transcription factors",
      "General Circulation Model",
      "Geometry",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pure mathematics",
      "RGB color model"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Yichen"
      },
      {
        "surname": "Huang",
        "given_name": "Ziyuan"
      },
      {
        "surname": "Yang",
        "given_name": "Xulei"
      },
      {
        "surname": "Ang",
        "given_name": "Marcelo"
      },
      {
        "surname": "Ng",
        "given_name": "Teck Khim"
      }
    ]
  },
  {
    "title": "LongReMix: Robust learning with high confidence samples in a noisy label environment",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109013",
    "abstract": "State-of-the-art noisy-label learning algorithms rely on an unsupervised learning to classify training samples as clean or noisy, followed by a semi-supervised learning (SSL) that minimises the empirical vicinal risk using a labelled set formed by samples classified as clean, and an unlabelled set with samples classified as noisy. The classification accuracy of such noisy-label learning methods depends on the precision of the unsupervised classification of clean and noisy samples, and the robustness of SSL to small clean sets. We address these points with a new noisy-label training algorithm, called LongReMix, which improves the precision of the unsupervised classification of clean and noisy samples and the robustness of SSL to small clean sets with a two-stage learning process. The stage one of LongReMix finds a small but precise high-confidence clean set, and stage two augments this high-confidence clean set with new clean samples and oversamples the clean data to increase the robustness of SSL to small clean sets. We test LongReMix on CIFAR-10 and CIFAR-100 with introduced synthetic noisy labels, and the real-world noisy-label benchmarks CNWL (Red Mini-ImageNet), WebVision, Clothing1M, and Food101-N. The results show that our LongReMix produces significantly better classification accuracy than competing approaches, particularly in high noise rate problems. Furthermore, our approach achieves state-of-the-art performance in most datasets. The code is available at https://github.com/filipe-research/LongReMix.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322004939",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Confidence interval",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Cordeiro",
        "given_name": "Filipe R."
      },
      {
        "surname": "Sachdeva",
        "given_name": "Ragav"
      },
      {
        "surname": "Belagiannis",
        "given_name": "Vasileios"
      },
      {
        "surname": "Reid",
        "given_name": "Ian"
      },
      {
        "surname": "Carneiro",
        "given_name": "Gustavo"
      }
    ]
  },
  {
    "title": "Weakly-supervised semantic segmentation via online pseudo-mask correcting",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2022.11.024",
    "abstract": "Many existing weakly-supervised semantic segmentation methods focus on obtaining more accurate pseudo-masks with weak labels. So far pseudo-masks have come close to the ground truth. However, the potential of these high-quality pseudo-masks has not been fully explored. This is because pseudo-masks inevitably contain partial noisy labels. Deep segmentation networks tend to overfit noisy labels, which leads to poor generalization performance. In this work, we propose a new method to mitigate the damage caused by noisy labels. First, We use the exponential moving average (EMA) model of the online segmentation model as the teacher. Then, predictions from the teacher model are used to correct pseudo-masks online. Besides, learning with noisy labels has been extensively studied in classification tasks. We also introduce these anti-noise techniques and find them also effective for the segmentation task. Our proposed method can be easily embedded into existing weakly-supervised semantic segmentation algorithms and bring 2.3% IoU improvement without expensive computational cost. It also achieves the state-of-the-art performance on the PASCAL VOC 2012 benchmark dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522003531",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Computer science",
      "Deep learning",
      "Focus (optics)",
      "Geodesy",
      "Geography",
      "Ground truth",
      "Image (mathematics)",
      "Machine learning",
      "Noise (video)",
      "Optics",
      "Overfitting",
      "Pascal (unit)",
      "Pattern recognition (psychology)",
      "Physics",
      "Programming language",
      "Segmentation",
      "Supervised learning"
    ],
    "authors": [
      {
        "surname": "Feng",
        "given_name": "Jiapei"
      },
      {
        "surname": "Wang",
        "given_name": "Xinggang"
      },
      {
        "surname": "Li",
        "given_name": "Te"
      },
      {
        "surname": "Ji",
        "given_name": "Shanshan"
      },
      {
        "surname": "Liu",
        "given_name": "Wenyu"
      }
    ]
  },
  {
    "title": "Learning to recover lost details from the dark",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2022.11.029",
    "abstract": "Deep convolutional neural networks (CNNs) have achieved significant milestones in low-light image enhancement. We note that existing methods still struggle to recover lost details. (1) They employ image-to-image mapping for enhancement, which may excel at global adjustment but lack attention to detail recovery. (2) They train CNN models with low-light/normal-light (L/N) image pairs that are generally non-pixel-level fully aligned, which causes the enhanced result to be prone to the problem of blurred details. To this end, we propose a novel CNN framework for learning to recover lost details from the dark. Specifically, we adopt a point-to-point mapping for better recovering lost details by estimating color affine transformations. And we design an associated group-codec mechanism to decouple the alignment problem from the enhancing process, thus preventing blurred details. Besides, data scarcity is an outstanding problem in low-light image enhancement; although some unsupervised models may deal with it, they would produce artifacts that cover real details. Hence, we propose a dynamic data collection method to acquire L/N image pairs. And we have collected a large and diverse low-light dataset to supplement existing benchmarks. Experiments conducted on our and public datasets show that our method is very competitive compared with state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522003580",
    "keywords": [
      "Affine transformation",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Deep learning",
      "Geometry",
      "Image (mathematics)",
      "Mathematics",
      "Operating system",
      "Pixel",
      "Point (geometry)",
      "Process (computing)",
      "Pure mathematics"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Maomei"
      },
      {
        "surname": "Tang",
        "given_name": "Lei"
      },
      {
        "surname": "Zhong",
        "given_name": "Sheng"
      },
      {
        "surname": "Luo",
        "given_name": "Hangzai"
      },
      {
        "surname": "Peng",
        "given_name": "Jinye"
      }
    ]
  },
  {
    "title": "BH2I-GAN: Bidirectional Hash_code-to-Image Translation using Multi-Generative Multi-Adversarial Nets",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109010",
    "abstract": "Given the benefits of high retrieval efficiency and low storage cost, hashing method has received an increasing attention. In particular, deep learning-based hashing has been widely used in data mining and information retrieval. However, almost all the existing methods only achieve the goal of high retrieval precision, and limit the evaluation of hashing methods to objective aspect. In this paper, we propose a novel bidirectional hash_code-to-image translation model by using multi-generative multi-adversarial nets to reduce storage cost truly and obtain satisfactory user acceptance on the basis of high retrieval precision. Firstly, we propose supervised manifold metric to reduce Hamming distance between similar instances while increasing the Hamming distance between dissimilar instances, which have been proved to be helpful for high retrieval precision and good user acceptance. Then, we utilize multi-generative and multi-adversarial networks to construct hash mapping and inverse hash generation. During inverse generation, theoretical analysis is conducted to show that inverse hash network can avoid unstable training and mode collapse. Besides, we prove that Poisson distribution induced by hash codes can be initialized as generative distribution to fit real distribution. Experimental results show that our method outperforms several state-of-the-art approaches on three popular datasets. Specifically, ours yields average about 9.3% increment in Mean Average Precision(MAP) on three datasets, and achieves over 90% user satisfaction. Besides, it successfully reduces storage cost by 1,634 times in COCO 2017 large-scale dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322004903",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Block code",
      "Computer science",
      "Computer security",
      "Data mining",
      "Decoding methods",
      "Double hashing",
      "Economics",
      "Hamming code",
      "Hamming distance",
      "Hamming space",
      "Hash function",
      "Hash table",
      "Metric (unit)",
      "Operations management",
      "Theoretical computer science",
      "Universal hashing"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Liming"
      },
      {
        "surname": "Zeng",
        "given_name": "Xianhua"
      },
      {
        "surname": "Li",
        "given_name": "Weisheng"
      },
      {
        "surname": "Xie",
        "given_name": "Yicai"
      }
    ]
  },
  {
    "title": "Deep Learning Pipeline for Spotting Macro- and Micro-expressions in Long Video Sequences Based on Action Units and Optical Flow",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2022.12.001",
    "abstract": "This paper is an extension of our previously published ACM Multimedia 2022 paper, which was ranked 3rd in the macro-expressions (MaEs) and micro-expressions (MEs) spotting task of the FME challenge 2021. In our earlier work, a deep learning framework based on facial action units (AUs) was proposed to emphasize both local and global features to deal with the MaEs and MEs spotting tasks. In this paper, an advanced Concat-CNN model is proposed to not only utilize facial action units (AU) features, which our previous work proved were more effective in detecting MaEs, but also to fuse the optical flow features to improve the detection performance of MEs. The advanced Concat-CNN proposed in this paper not only considers the intra-features correlation of a single frame but also the inter-features correlation between frames. Further, we devise a new adaptive re-labeling method by labeling the emotional frames with distinctive scores. This method takes into account the dynamic changes in expressions to further improve the overall detection performance. Compared with our earlier work and several existing works, the newly proposed deep learning pipeline is able to achieve a better performance in terms of the overall F1-scores: 0.2623 on CAS(ME) 2 , 0.2839 on CAS(ME) 2 -cropped, and 0.3241 on SAMM-LV, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522003683",
    "keywords": [
      "Action (physics)",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Economics",
      "Electrical engineering",
      "Engineering",
      "Frame (networking)",
      "Fuse (electrical)",
      "Image (mathematics)",
      "Keyword spotting",
      "Macro",
      "Management",
      "Optical flow",
      "Pattern recognition (psychology)",
      "Physics",
      "Pipeline (software)",
      "Programming language",
      "Quantum mechanics",
      "Speech recognition",
      "Spotting",
      "Task (project management)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Bo"
      },
      {
        "surname": "Wu",
        "given_name": "Jianming"
      },
      {
        "surname": "Ikeda",
        "given_name": "Kazushi"
      },
      {
        "surname": "Hattori",
        "given_name": "Gen"
      },
      {
        "surname": "Sugano",
        "given_name": "Masaru"
      },
      {
        "surname": "Iwasawa",
        "given_name": "Yusuke"
      },
      {
        "surname": "Matsuo",
        "given_name": "Yutaka"
      }
    ]
  },
  {
    "title": "Neural graph embeddings as explicit low-rank matrix factorization for link prediction",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.108977",
    "abstract": "Learning good quality neural graph embeddings has long been achieved by minimzing the pointwise mutual information (PMI) for co-occuring nodes in simulated random walks. This design choice has been mostly popularized by the direct application of the highly-successful word embedding algorithm word2vec to predicting the formation of new links in social, co-citation, and biological networks. However, such a skeuomorphic design of graph embedding methods entails a truncation of information coming from pairs of nodes with low PMI. To circumvent this issue, we propose an improved approach to learning low-rank factorization embeddings that incorporate information from such unlikely pairs of nodes and show that it can improve the link prediction performance of baseline methods from 1.2% to 24.2%. Based on our results and observations, we outline further steps that could improve the design of next graph embedding algorithms that are based on matrix factorizaion.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322004575",
    "keywords": [
      "Adjacency matrix",
      "Artificial intelligence",
      "Combinatorics",
      "Computer network",
      "Computer science",
      "Eigenvalues and eigenvectors",
      "Embedding",
      "Graph",
      "Graph embedding",
      "Link (geometry)",
      "Mathematical analysis",
      "Mathematics",
      "Matrix decomposition",
      "Physics",
      "Pointwise",
      "Quantum mechanics",
      "Random walk",
      "Rank (graph theory)",
      "Statistics",
      "Theoretical computer science",
      "Word2vec"
    ],
    "authors": [
      {
        "surname": "Agibetov",
        "given_name": "Asan"
      }
    ]
  },
  {
    "title": "GDRL: An interpretable framework for thoracic pathologic prediction",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2022.12.020",
    "abstract": "Deep learning methods have shown significant performance in medical image analysis tasks. However, they generally act like “black box” without explanations in both feature extraction and decision processes, leading to lack of clinical insights and high risk assessments. To aid deep learning in envisioning diseases with visual clues, we propose a novel Group-Disentangled Representation Learning framework (GDRL). The key contribution is that GDRL completely disentangles latent space into disease concepts with abundant and non-overlapping feature related explanations, thus enhancing interpretability in feature extraction and decision processes. Furthermore, we introduce an implicit group-swap structure by emphasizing the linking relationship between semantical concepts of disease and low-level visual features, other than explicit explanations on general objects and their attributes. We demonstrate our framework on predicting four categories of diseases from chest X-ray images. The AUROC of GDRL on ChestX-ray14 for thoracic pathologic prediction are 0.8630, 0.8980, 0.9269 and 0.8653 respectively, and we showcase the potential of our framework in enhancing interpretability of the factors contributing to different diseases.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522003865",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Deep learning",
      "Disease",
      "Economics",
      "Feature (linguistics)",
      "Feature extraction",
      "Feature learning",
      "Finance",
      "Interpretability",
      "Law",
      "Linguistics",
      "Machine learning",
      "Medicine",
      "Pathology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Swap (finance)"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Yirui"
      },
      {
        "surname": "Li",
        "given_name": "Hao"
      },
      {
        "surname": "Feng",
        "given_name": "Xi"
      },
      {
        "surname": "Casanova",
        "given_name": "Andrea"
      },
      {
        "surname": "Abate",
        "given_name": "Andrea F."
      },
      {
        "surname": "Wan",
        "given_name": "Shaohua"
      }
    ]
  },
  {
    "title": "Rethinking referring relationships from a perspective of mask-level relational reasoning",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109044",
    "abstract": "Referring relationship aims at localizing subject and object entities in an image, according to a triple text < subject, predicate, object > . Previous methods use iterative attention to shift between image regions for modeling predicate. However, predicate sometimes is implicit and difficult to be represented in the image domain. Convolution modeling method to express predicate is simple and inappropriate. Besides, relational reasoning information in the text itself is not fully utilized. To this end, we rethink referring relationship from a mask-level relational reasoning perspective to improve model interpretability. For text-to-image reasoning, we design Mask Generate and Mask Transfer modules, so as to fully integrate the text priors into the reasoning and prediction of masks. For image-to-text reasoning, we propose an unsupervised triple reconstruction method to guide text-to-image reasoning and improve multimodal generalization. By bi-directional reasoning between image and text, the proposed method MRR fully conforms to the multimodal relational reasoning process. Experiments show that MRR achieves state-of-the-art performance on two datasets of referring relationships, VRD and Visual Genome.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322005246",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Interpretability",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Perspective (graphical)",
      "Predicate (mathematical logic)",
      "Programming language",
      "Visual reasoning"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Chengyang"
      },
      {
        "surname": "Zhu",
        "given_name": "Liping"
      },
      {
        "surname": "Tian",
        "given_name": "Gangyi"
      },
      {
        "surname": "Hou",
        "given_name": "Yi"
      },
      {
        "surname": "Zhou",
        "given_name": "Heng"
      }
    ]
  },
  {
    "title": "Pyramid Geometric Consistency Learning For Semantic Segmentation",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109020",
    "abstract": "Semantic segmentation is a critical in vision fields. Randomly transforms each image into different augmented samples and supervise the views with transformed semantics labels. However, even if the views are expanded from the same sample, the prediction results obtained by the same network will be very different. Therefore, we argue that between the augmented samples, the transformation-equivariance and the representational consistency also need to be supervised. Motivated by this, we propose a simple cross-data augmentation for semantic segmentation, in which we also leverage the pixel-level consistency constraint learning between pairs of augmented samples. As a result, our scheme significantly can improve the performances of existing semantic segmentation models without additional computation overhead. We verified the effectiveness of this method on Deeplab V3 Plus. Experiments show that our method can achieve stable performance improvement on mainstream data sets such as Pascal VOC 2012, Camvid, Cityscapes, etc.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322005003",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Consistency (knowledge bases)",
      "Gene",
      "Geometry",
      "Leverage (statistics)",
      "Machine learning",
      "Mathematics",
      "Pascal (unit)",
      "Pattern recognition (psychology)",
      "Programming language",
      "Pyramid (geometry)",
      "Segmentation",
      "Semantics (computer science)",
      "Transformation (genetics)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Xian"
      },
      {
        "surname": "Li",
        "given_name": "Qiang"
      },
      {
        "surname": "Quan",
        "given_name": "Zhibin"
      },
      {
        "surname": "Yang",
        "given_name": "Wankou"
      }
    ]
  },
  {
    "title": "Adaptive momentum variance for attention-guided sparse adversarial attacks",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.108979",
    "abstract": "The phenomenon that deep neural networks are vulnerable to adversarial examples has been found for several years. Under the black-box setting, transfer-based methods usually produce the adversarial examples on a white-box model, which serves as the surrogate model in the black-box attack, and hope that the same adversarial examples can also fool the black-box model. However, these methods have high success rates for the surrogate model and exhibit weak transferability for the black-box model. In addition, some studies have shown that deep neural networks are also vulnerable to sparse alterations of the input, but existing sparse attacks mainly focus on the number of attacked pixels without restricting the size of the perturbations, which is perceptible to human eyes. To address the above problems, we propose a transfer-based sparse attack method, called adaptive momentum variance based iterative gradient method with a class activation map, where the method considers a simple adaptive momentum variance and a refining perturbation mechanism to improve the transferability of adversarial examples. Also, a class activation map, which is also known as attention mechanism, is employed to explore the relationship between the number of the perturbed pixels and the attack performance in the case of limiting the intensity of perturbation. The proposed method is compared with a number of the state-of-the-art transfer-based adversarial attack methods on the ImageNet dataset, and the empirical results demonstrate that our method achieves a significant increase in transferability with only attacking about 50% of the pixels.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322004599",
    "keywords": [
      "Accounting",
      "Adversarial system",
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Black box",
      "Business",
      "Computer science",
      "Deep learning",
      "Deep neural networks",
      "Logit",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Pixel",
      "Surrogate model",
      "Transferability",
      "Variance (accounting)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Chao"
      },
      {
        "surname": "Yao",
        "given_name": "Wen"
      },
      {
        "surname": "Wang",
        "given_name": "Handing"
      },
      {
        "surname": "Jiang",
        "given_name": "Tingsong"
      }
    ]
  },
  {
    "title": "Adaptive cooperative exploration for reinforcement learning from imperfect demonstrations",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2022.12.003",
    "abstract": "In reinforcement learning, exploration is an important way to learn new skills, but it is usually inefficient when faced with huge state-action space or sparse extrinsic rewards. Generally, expert demonstrations can assist the policy learning by leading the agent to imitate or explore these data. However, the demonstrations are often imperfect due to the data collection noise or immature expert. To this end, we propose a novel adaptive cooperative exploration method that can effectively alleviate the issues of imperfect demonstrations and improve the policy learning with them. Specifically, we propose a cooperative learning module to encourage two agents to explore diversely with it and then fuse the learned policies. Meanwhile, the adaptive self-supervised exploration method is presented to dynamically explore the demonstrations considering the environmental feedback. Therefore, the proposed method can achieve effective utilization of the imperfect demonstrations for policy learning. Experimental results demonstrate the effectiveness of the proposed method on MuJoCo benchmark.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522003725",
    "keywords": [
      "Action (physics)",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Imperfect",
      "Linguistics",
      "Machine learning",
      "Noise (video)",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Reinforcement learning"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Fuxian"
      },
      {
        "surname": "Ji",
        "given_name": "Naye"
      },
      {
        "surname": "Ni",
        "given_name": "Huajian"
      },
      {
        "surname": "Li",
        "given_name": "Shijian"
      },
      {
        "surname": "Li",
        "given_name": "Xi"
      }
    ]
  },
  {
    "title": "Single image super-resolution based on directional variance attention network",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.108997",
    "abstract": "Recent advances in single image super-resolution (SISR) explore the power of deep convolutional neural networks (CNNs) to achieve better performance. However, most of the progress has been made by scaling CNN architectures, which usually raise computational demands and memory consumption. This makes modern architectures less applicable in practice. In addition, most CNN-based SR methods do not fully utilize the informative hierarchical features that are helpful for final image recovery. In order to address these issues, we propose a directional variance attention network (DiVANet), a computationally efficient yet accurate network for SISR. Specifically, we introduce a novel directional variance attention (DiVA) mechanism to capture long-range spatial dependencies and exploit inter-channel dependencies simultaneously for more discriminative representations. Furthermore, we propose a residual attention feature group (RAFG) for parallelizing attention and residual block computation. The output of each residual block is linearly fused at the RAFG output to provide access to the whole feature hierarchy. In parallel, DiVA extracts most relevant features from the network for improving the final output and preventing information loss along the successive operations inside the network. Experimental results demonstrate the superiority of DiVANet over the state of the art in several datasets, while maintaining relatively low computation and memory footprint. The code is available at https://github.com/pbehjatii/DiVANet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322004770",
    "keywords": [
      "Accounting",
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Block (permutation group theory)",
      "Business",
      "Computation",
      "Computer science",
      "Convolutional neural network",
      "Data mining",
      "Discriminative model",
      "Feature (linguistics)",
      "Footprint",
      "Geometry",
      "Image (mathematics)",
      "Linguistics",
      "Mathematics",
      "Memory footprint",
      "Operating system",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Residual",
      "Variance (accounting)"
    ],
    "authors": [
      {
        "surname": "Behjati",
        "given_name": "Parichehr"
      },
      {
        "surname": "Rodriguez",
        "given_name": "Pau"
      },
      {
        "surname": "Fernández",
        "given_name": "Carles"
      },
      {
        "surname": "Hupont",
        "given_name": "Isabelle"
      },
      {
        "surname": "Mehri",
        "given_name": "Armin"
      },
      {
        "surname": "Gonzàlez",
        "given_name": "Jordi"
      }
    ]
  },
  {
    "title": "Grouping-based Oversampling in Kernel Space for Imbalanced Data Classification",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.108992",
    "abstract": "The class-imbalanced classification is a difficult problem because not only traditional classifiers are more biased towards the majority classes and inclined to generate incorrect predictions, but also the existing algorithms often have difficulty tackling this kind of problem with the class overlapping. Oversampling is a widely used and effective method to obtain balanced samples for imbalanced data, but the existing oversampling methods usually result in more serious class overlapping due to improper choice of the reference samples. To circumvent this shortcoming, according to the different possibilities of minority class samples appearing in the overlapping regions in the feature space, a grouping scheme for the minority class samples is first designed to identify the overlapping region samples. Then, a new oversampling method based on this grouping scheme is proposed to make the new samples far away from the overlapping region and rectify the decision boundary properly. Subsequently, a new effective classification algorithm is developed for imbalanced data. Extensive experiments show that the proposed algorithm is superior to the seventeen benchmark algorithms in terms of three performance metrics, especially on high imbalance ratio data sets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322004721",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Benchmark (surveying)",
      "Boundary (topology)",
      "Class (philosophy)",
      "Combinatorics",
      "Computer network",
      "Computer science",
      "Data mining",
      "Decision boundary",
      "Feature (linguistics)",
      "Geodesy",
      "Geography",
      "Kernel (algebra)",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Operating system",
      "Oversampling",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Scheme (mathematics)",
      "Space (punctuation)",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Ren",
        "given_name": "Jinjun"
      },
      {
        "surname": "Wang",
        "given_name": "Yuping"
      },
      {
        "surname": "Cheung",
        "given_name": "Yiu-ming"
      },
      {
        "surname": "Gao",
        "given_name": "Xiao-Zhi"
      },
      {
        "surname": "Guo",
        "given_name": "Xiaofang"
      }
    ]
  },
  {
    "title": "DcTr: Noise-robust point cloud completion by dual-channel transformer with cross-attention",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109051",
    "abstract": "Current point cloud completion research mainly utilizes the global shape representation and local features to recover the missing regions of 3D shape for the partial point cloud. However, these methods suffer from inefficient utilization of local features and unstructured points prediction in local patches, hardly resulting in a well-arranged structure for points. To tackle these problems, we propose to employ Dual-channel Transformer and Cross-attention (CA) for point cloud completion (DcTr). The DcTr is apt at using local features and preserving a well-structured generation process. Specifically, the dual-channel transformer leverages point-wise attention and channel-wise attention to summarize the deconvolution patterns used in the previous Dual-channel Transformer Point Deconvolution (DCTPD) stage to produce the deconvolution in the current DCTPD stage. Meanwhile, we employ cross-attention to convey the geometric information from the local regions of incomplete point clouds for the generation of complete ones at different resolutions. In this way, we can generate the locally compact and structured point cloud by capturing the structure characteristic of 3D shape in local patches. Our experimental results indicate that DcTr outperforms the state-of-the-art point cloud completion methods under several benchmarks and is robust to various kinds of noise.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322005313",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Deconvolution",
      "Electrical engineering",
      "Engineering",
      "Point cloud",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Fei",
        "given_name": "Ben"
      },
      {
        "surname": "Yang",
        "given_name": "Weidong"
      },
      {
        "surname": "Ma",
        "given_name": "Lipeng"
      },
      {
        "surname": "Chen",
        "given_name": "Wen-Ming"
      }
    ]
  },
  {
    "title": "BP-triplet net for unsupervised domain adaptation: A Bayesian perspective",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.108993",
    "abstract": "Triplet loss, one of the deep metric learning (DML) methods, is to learn the embeddings where examples from the same class are closer than examples from different classes. Motivated by DML, we propose an effective BP-triplet Loss for unsupervised domain adaption (UDA) from the perspective of Bayesian learning and we name the model as BP-Triplet Net. In previous metric learning based methods for UDA, sample pairs across domains are treated equally, which is not appropriate due to the domain bias. In our work, considering the different importance of pair-wise samples for both feature learning and domain alignment, we deduce our BP-triplet loss for effective UDA from the perspective of Bayesian learning. Our BP-triplet loss adjusts the weights of pair-wise samples in intra-domain and inter-domain. Especially, it can self attend to the hard pairs (including hard positive pair and hard negative pair). Together with the commonly used adversarial loss for domain alignment, the quality of target pseudo labels is progressively improved. Our method achieved low joint error of the ideal source and target hypothesis. The expected target error can then be upper bounded following Ben-David’s theorem. Comprehensive evaluations on four benchmark datasets demonstrate the effectiveness of the proposed approach for UDA. Code is available at https://github.com/wangshanshanAHU/BP-Triplet-Net.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322004733",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Bayesian probability",
      "Benchmark (surveying)",
      "Bounded function",
      "Code (set theory)",
      "Computer science",
      "Domain (mathematical analysis)",
      "Economics",
      "Feature (linguistics)",
      "Geodesy",
      "Geography",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Metric (unit)",
      "Operations management",
      "Pattern recognition (psychology)",
      "Perspective (graphical)",
      "Philosophy",
      "Programming language",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Shanshan"
      },
      {
        "surname": "Zhang",
        "given_name": "Lei"
      },
      {
        "surname": "Wang",
        "given_name": "Pichao"
      },
      {
        "surname": "Wang",
        "given_name": "MengZhu"
      },
      {
        "surname": "Zhang",
        "given_name": "Xingyi"
      }
    ]
  },
  {
    "title": "AEDNet: Adaptive Edge-Deleting Network For Subgraph Matching",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109033",
    "abstract": "Subgraph matching is to find all subgraphs in a data graph that are isomorphic to an existing query graph. Subgraph matching is an NP-hard problem, yet has found its applications in many areas. Many learning-based methods have been proposed for graph matching, whereas few have been designed for subgraph matching. The subgraph matching problem is generally more challenging, mainly due to the different sizes between the two graphs, resulting in considerable large space of solutions. Also the extra edges existing in the data graph connecting to the matched nodes may lead to two matched nodes of two graphs having different adjacency structures and often being identified as distinct objects. Due to the extra edges, the existing learning based methods often fail to generate sufficiently similar node-level embeddings for matched nodes. This study proposes a novel Adaptive Edge-Deleting Network (AEDNet) for subgraph matching. The proposed method is trained in an end-to-end fashion. In AEDNet, a novel sample-wise adaptive edge-deleting mechanism removes extra edges to ensure consistency of adjacency structure of matched nodes, while a unidirectional cross-propagation mechanism ensures consistency of features of matched nodes. We applied the proposed method on six datasets with graph sizes varying from 20 to 2300. Our evaluations on six open datasets demonstrate that the proposed AEDNet outperforms six state-of-the-arts and is much faster than the exact methods on large graphs.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322005131",
    "keywords": [
      "Adjacency list",
      "Algorithm",
      "Artificial intelligence",
      "Bioinformatics",
      "Biology",
      "Combinatorics",
      "Computer science",
      "Consistency (knowledge bases)",
      "Degeneracy (biology)",
      "Factor-critical graph",
      "Graph",
      "Graph factorization",
      "Induced subgraph isomorphism problem",
      "Line graph",
      "Matching (statistics)",
      "Mathematics",
      "Statistics",
      "Subgraph isomorphism problem",
      "Theoretical computer science",
      "Voltage graph"
    ],
    "authors": [
      {
        "surname": "Lan",
        "given_name": "Zixun"
      },
      {
        "surname": "Ma",
        "given_name": "Ye"
      },
      {
        "surname": "Yu",
        "given_name": "Limin"
      },
      {
        "surname": "Yuan",
        "given_name": "Linglong"
      },
      {
        "surname": "Ma",
        "given_name": "Fei"
      }
    ]
  },
  {
    "title": "A semi-automatic data integration process of heterogeneous databases",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.01.007",
    "abstract": "One of the most difficult issues today, is the integration of data from various sources. Thus, it arises the need of automatic Data Integration (DI) methods. However, in the literature there are fully automatic or semi-automatic DI techniques, but they require the involvement of IT-experts with specific domain skills. In this paper we present a novel DI methodology for which it is not required the involvement of IT-experts; in this methodology syntactically/semantically similar entities present in the sources are merged, by exploiting an information retrieval technique, a clustering method and a trained neural network. Although the suggested process is completely automated, we planned some interactions with the Company Manager, a figure who is not required to have IT-skills, but whose only contribution will be to define limits and tolerance thresholds during the DI process, based on the interests of the company. The validity of the proposed approach showed an integration accuracy between 99 % − 100 % .",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523000132",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Cluster analysis",
      "Computer science",
      "Data integration",
      "Data mining",
      "Database",
      "Domain (mathematical analysis)",
      "Information integration",
      "Information retrieval",
      "Mathematical analysis",
      "Mathematics",
      "Process (computing)",
      "Programming language"
    ],
    "authors": [
      {
        "surname": "Barbella",
        "given_name": "Marcello"
      },
      {
        "surname": "Tortora",
        "given_name": "Genoveffa"
      }
    ]
  },
  {
    "title": "A framework based on local cores and synthetic examples generation for self-labeled semi-supervised classification",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109060",
    "abstract": "Self-labeled techniques are semi-supervised classification models that overcome the shortage of labeled samples via an iterative process. Most relevant proposals are inspired by boosting schemes to iteratively enlarge labeled data, but these methods are constrained by the number and distribution of the initial labeled data. Up to the present, the only exceptions which can solve the above problem are SEG-SSC, k-means-SSC and LC-SSC. However, SEG-SSC relies on too many parameters. Besides, it is hard to improve the distribution of the initial labeled data when the initial labeled set can not roughly represent the distribution of the original data. k-means-SSC and LC-SSC fail to significantly improve the number of the initial labeled data by a limited number of representative points. To address the above issues, this paper proposes a framework based on local cores and synthetic examples generation for self-labeled semi-supervised classification (LCSEG-SSC). First, a new method for finding local cores on labeled and unlabeled data is proposed to improve the distribution of the initial labeled data. Second, STOPF or active labeling is used to predict found local cores. Third, a new example generation technique is proposed to create synthetic labeled samples, intending to improve the number of the initial labeled data. After that, any self-labeled with boosting schemes can be executed on the improved labeled data effectively. Intensive experiments prove that LCSEG-SSC outperforms state-of-the-art methods, especially in a relatively low ratio of labeled data.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322005404",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Boosting (machine learning)",
      "Computer science",
      "Data set",
      "Economic shortage",
      "Government (linguistics)",
      "Labeled data",
      "Linguistics",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Semi-supervised learning",
      "Synthetic data"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Junnan"
      },
      {
        "surname": "Zhou",
        "given_name": "MingQiang"
      },
      {
        "surname": "Zhu",
        "given_name": "Qingsheng"
      },
      {
        "surname": "Wu",
        "given_name": "Quanwang"
      }
    ]
  },
  {
    "title": "Enhancing 3D-2D Representations for Convolution Occupancy Networks",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109097",
    "abstract": "Convolutional Occupancy Networks (ConvONet) have gained popularity in object-level and scene-level reconstruction. However, how to better represent the 3D features for ConvONet remains an open question. In this paper, we propose to improve the representation for ConvONet by enhancing both 3D positional information and 3D-2D correlations. Considering that position information acts as the fundamental component of a 3D shape, we propose a Position-Aware Transformer (PAT) architecture that incorporates the Adaptive Multi-Scale Position Encoding (AMSPE) into the self-attention computation. By leveraging both global and local position aggregations in a multi-level manner, AMSPE enables better representations of both coarse and fine structures of the 3D shape. Meanwhile, since projecting 3D features to 2D planes for convolution inevitably introduces ambiguous or noisy representations, we propose a 3D Correlation-Guided Enhancement (CGE) network to bridge the gap between 3D and 2D shape representations. Specifically, we leverage the projected 3D correlations from PAT as the structural guidance, then compute the 3D Correlation-Guided Attentions (CGAs) to enhance the most representative features in the 2D space. In this way, the proposed architecture preserves the most informative structural representations while alleviating the impact of the mis-projected and noisy features. Experiments on ShapeNet and indoor scene dataset demonstrate the superiority of our method. Both quantitative and qualitative experiments show that our method achieves state-of-the-art performance for implicit-based 3D reconstruction.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322005775",
    "keywords": [
      "Architectural engineering",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer vision",
      "Convolution (computer science)",
      "Engineering",
      "Mathematics",
      "Occupancy",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Mao",
        "given_name": "Qing"
      },
      {
        "surname": "Li",
        "given_name": "Rui"
      },
      {
        "surname": "Zhu",
        "given_name": "Yu"
      },
      {
        "surname": "Sun",
        "given_name": "Jinqiu"
      },
      {
        "surname": "Zhang",
        "given_name": "Yanning"
      }
    ]
  },
  {
    "title": "Deep MinCut: Learning Node Embeddings by Detecting Communities",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109126",
    "abstract": "We present Deep MinCut (DMC), an unsupervised approach to learn node embeddings for graph-structured data. It derives node representations based on their membership in communities. As such, the embeddings directly provide insights into the graph structure, so that a separate clustering step is no longer needed. DMC learns both, node embeddings and communities, simultaneously by minimizing the mincut loss, which captures the number of connections between communities. Striving for high scalability, we also propose a training process for DMC based on minibatches. We provide empirical evidence that the communities learned by DMC are meaningful and that the node embeddings are competitive in different node classification benchmarks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322006069",
    "keywords": [
      "Artificial intelligence",
      "Computer network",
      "Computer science",
      "Deep learning",
      "Engineering",
      "Node (physics)",
      "Structural engineering",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Duong",
        "given_name": "Chi Thang"
      },
      {
        "surname": "Nguyen",
        "given_name": "Thanh Tam"
      },
      {
        "surname": "Hoang",
        "given_name": "Trung-Dung"
      },
      {
        "surname": "Yin",
        "given_name": "Hongzhi"
      },
      {
        "surname": "Weidlich",
        "given_name": "Matthias"
      },
      {
        "surname": "Nguyen",
        "given_name": "Quoc Viet Hung"
      }
    ]
  },
  {
    "title": "Self-paced resistance learning against overfitting on noisy labels",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109080",
    "abstract": "Noisy labels composed of correct and corrupted ones are pervasive in practice. They might significantly deteriorate the performance of convolutional neural networks (CNNs), because CNNs are easily overfitted on corrupted labels. To address this issue, inspired by an observation, deep neural networks might first memorize the probably correct-label data and then corrupt-label samples, we propose a novel yet simple self-paced resistance framework to resist corrupted labels, without using any clean validation data. The proposed framework first utilizes the memorization effect of CNNs to learn a curriculum, which contains confident samples and provides meaningful supervision for other training samples. Then it adopts selected confident samples and a proposed resistance loss to update model parameters; the resistance loss tends to smooth model parameters’ update or attain equivalent prediction over each class, thereby resisting model overfitting on corrupted labels. Finally, we unify these two modules into a single loss function and optimize it in an alternative learning. Extensive experiments demonstrate the significantly superior performance of the proposed framework over recent state-of-the-art methods on noisy-label data. Source codes of the proposed method are available on https://github.com/xsshi2015/Self-paced-Resistance-Learning.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032200560X",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Machine learning",
      "Overfitting",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Shi",
        "given_name": "Xiaoshuang"
      },
      {
        "surname": "Guo",
        "given_name": "Zhenhua"
      },
      {
        "surname": "Li",
        "given_name": "Kang"
      },
      {
        "surname": "Liang",
        "given_name": "Yun"
      },
      {
        "surname": "Zhu",
        "given_name": "Xiaofeng"
      }
    ]
  },
  {
    "title": "SC-GAN: Subspace Clustering based GAN for Automatic Expression Manipulation",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109072",
    "abstract": "In recent years, the topics of facial attribute manipulation and decomposition have gained great popularity in computer vision and human computer interaction. Even though such methods have been preliminarily employed in some photo beautification applications, it still remains challenging due to the highly-versatile facial attributes and their drastic appearance changes subject to variation of deformation, illumination, pose, etc. The prevailing problems are especially severe when we are faced with group photos involving many faces. To overcome such critical limitations and discover more meaningful visual attributes and their possible decompositions, we develop a subspace clustering based generative adversarial network (SC-GAN) in this paper. Our SC-GAN can simultaneously decompose multiple subspaces and generate diverse samples correspondingly, thus the training of the generative models could be more effectively guided by facial attribute and its decomposition and manipulation in a natural and meaningful fashion. Our SC-GAN incorporates the SIFT K-means cluster, which could split the holistic semantic facial space into different subspaces without supervision, and help the new GAN generate more convincing results within specific subspaces. Extensive experiments and comprehensive evaluations confirm that, our method can greatly reduce the unexpected influences caused by portrait diversities and outperform the state-of-the-art facial attribute manipulation approaches. 1 1 https://github.com/buaaswf/SC-GAN/",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322005520",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Cluster analysis",
      "Computer science",
      "Computer vision",
      "Decomposition",
      "Ecology",
      "Generative grammar",
      "Geometry",
      "Hierarchical clustering",
      "Linear subspace",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Subspace topology"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Shuai"
      },
      {
        "surname": "Liu",
        "given_name": "Liang"
      },
      {
        "surname": "Liu",
        "given_name": "Ji"
      },
      {
        "surname": "Song",
        "given_name": "Wenfeng"
      },
      {
        "surname": "Hao",
        "given_name": "Aimin"
      },
      {
        "surname": "Qin",
        "given_name": "Hong"
      }
    ]
  },
  {
    "title": "A goal-driven unsupervised image segmentation method combining graph-based processing and Markov random fields",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109082",
    "abstract": "Image segmentation is the process of partitioning a digital image into a set of homogeneous regions (according to some homogeneity criterion) to facilitate a subsequent higher-level analysis. In this context, the present paper proposes an unsupervised and graph-based method of image segmentation, which is driven by an application goal, namely, the generation of image segments associated with a user-defined and application-specific goal. A graph, together with a random grid of source elements, is defined on top of the input image. From each source satisfying a goal-driven predicate, called seed, a propagation algorithm assigns a cost to each pixel on the basis of similarity and topological connectivity, measuring the degree of association with the reference seed. Then, the set of most significant regions is automatically extracted and used to estimate a statistical model for each region. Finally, the segmentation problem is expressed in a Bayesian framework in terms of probabilistic Markov random field (MRF) graphical modeling. An ad hoc energy function is defined based on parametric models, a seed-specific spatial feature, a background-specific potential, and local-contextual information. This energy function is minimized through graph cuts and, more specifically, the alpha-beta swap algorithm, yielding the final goal-driven segmentation based on the maximum a posteriori (MAP) decision rule. The proposed method does not require deep a priori knowledge (e.g., labelled datasets), as it only requires the choice of a goal-driven predicate and a suited parametric model for the data. In the experimental validation with both magnetic resonance (MR) and synthetic aperture radar (SAR) images, the method demonstrates robustness, versatility, and applicability to different domains, thus allowing for further analyses guided by the generated products.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322005623",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Cut",
      "Image segmentation",
      "Markov random field",
      "Mathematics",
      "Maximum a posteriori estimation",
      "Maximum likelihood",
      "Pattern recognition (psychology)",
      "Scale-space segmentation",
      "Segmentation",
      "Segmentation-based object categorization",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Trombini",
        "given_name": "Marco"
      },
      {
        "surname": "Solarna",
        "given_name": "David"
      },
      {
        "surname": "Moser",
        "given_name": "Gabriele"
      },
      {
        "surname": "Dellepiane",
        "given_name": "Silvana"
      }
    ]
  },
  {
    "title": "Meta-learning-based adversarial training for deep 3D face recognition on point clouds",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109065",
    "abstract": "Recently, deep face recognition using 2D face images has made great advances mainly due to the readily available large-scale face data. However, deep face recognition using 3D face scans, especially on point clouds, has been far from fully explored. In this paper, we propose a novel meta-learning-based adversarial training (MLAT) algorithm for deep 3D face recognition (3DFR) on point clouds. It consists of two alternate modules: adversarial sample generating for 3D face data augmentation and meta-learning-based deep network training. In the first module, adversarial samples of given 3D face scans are dynamically generated based on current deep 3DFR model. In the second module, a meta-learning framework is designed to avoid the performance decrease caused by the generated adversarial samples. Overall, MLAT algorithm combines the adversarial sample generating and meta-learning-based network training in a uniform framework, in which adversarial samples and network parameters are optimized alternately. Thus, it can continuously generate diverse and suitable adversarial samples, and then the meta-learning framework can further improve the accuracy of 3DFR model. Comprehensive experimental results show that the proposed approach consistently achieves competitive rank-one recognition accuracies on the BU-3DFE (100%), Bosphorus (99.78%), BU-4DFE (98.02%) and FRGC v2 (98.01%) database, and thereby substantiate its superiority.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322005453",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Deep learning",
      "Face (sociological concept)",
      "Facial recognition system",
      "Geometry",
      "Machine learning",
      "Mathematics",
      "Meteorology",
      "Pattern recognition (psychology)",
      "Physics",
      "Point (geometry)",
      "Point cloud",
      "Sample (material)",
      "Social science",
      "Sociology",
      "Training (meteorology)"
    ],
    "authors": [
      {
        "surname": "Yu",
        "given_name": "Cuican"
      },
      {
        "surname": "Zhang",
        "given_name": "Zihui"
      },
      {
        "surname": "Li",
        "given_name": "Huibin"
      },
      {
        "surname": "Sun",
        "given_name": "Jian"
      },
      {
        "surname": "Xu",
        "given_name": "Zongben"
      }
    ]
  },
  {
    "title": "Multi-scale enhanced graph convolutional network for mild cognitive impairment detection",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109106",
    "abstract": "As an early stage of Alzheimer's disease (AD), mild cognitive impairment (MCI) is able to be detected by analyzing the brain connectivity networks. For this reason, we devise a new framework via multi-scale enhanced graph convolutional network (MSE-GCN) for MCI detection, which integrates the structural and functional information from the diffusion tensor imaging (DTI) and resting-state functional magnetic resonance imaging (R-fMRI), respectively. Specifically, both information in the brain connective networks is first integrated based on the local weighted clustering coefficients (LWCC), which is concatenated as the feature vector for representing a population graph's vertice. Simultaneously, the gender and age information in each subject are integrated with the structural and functional features to construct a sparse graph. Then, various parallel graph convolutional network (GCN) layers with multiple inputs are designed from the embedding from random walk embeddings in the GCN to identify the essential MCI graph information. Finally, all GCN layers’ outputs are concatenated via the fully connection layer to perform disease detection. The experimental results on the public Alzheimer's Disease Neuroimaging Initiative (ADNI) database show that our method is promising to detect MCI and superior to other competing algorithms, with a mean classification accuracy of 90.39% in the detection tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322005866",
    "keywords": [
      "Artificial intelligence",
      "Cartography",
      "Cognition",
      "Cognitive impairment",
      "Computer science",
      "Geography",
      "Graph",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Psychology",
      "Scale (ratio)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Lei",
        "given_name": "Baiying"
      },
      {
        "surname": "Zhu",
        "given_name": "Yun"
      },
      {
        "surname": "Yu",
        "given_name": "Shuangzhi"
      },
      {
        "surname": "Hu",
        "given_name": "Huoyou"
      },
      {
        "surname": "Xu",
        "given_name": "Yanwu"
      },
      {
        "surname": "Yue",
        "given_name": "Guanghui"
      },
      {
        "surname": "Wang",
        "given_name": "Tianfu"
      },
      {
        "surname": "Zhao",
        "given_name": "Cheng"
      },
      {
        "surname": "Chen",
        "given_name": "Shaobin"
      },
      {
        "surname": "Yang",
        "given_name": "Peng"
      },
      {
        "surname": "Song",
        "given_name": "Xuegang"
      },
      {
        "surname": "Xiao",
        "given_name": "Xiaohua"
      },
      {
        "surname": "Wang",
        "given_name": "Shuqiang"
      }
    ]
  },
  {
    "title": "Image classification using graph neural network and multiscale wavelet superpixels",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.01.003",
    "abstract": "Prior studies using graph neural networks (GNNs) for image classification have focused on graphs generated from a regular grid of pixels or similar-sized superpixels. In the latter, a single target number of superpixels is defined for an entire dataset irrespective of differences across images and their intrinsic multiscale structure. On the contrary, this study investigates image classification using graphs generated from an image-specific number of multiscale superpixels. We propose WaveMesh, a new wavelet-based superpixeling algorithm, where the number and sizes of superpixels in an image are systematically computed based on its content. WaveMesh superpixel graphs are structurally different from similar-sized superpixel graphs. We use SplineCNN, a state-of-the-art network for image graph classification, to compare WaveMesh and similar-sized superpixels. Using SplineCNN, we perform extensive experiments on three benchmark datasets under three local-pooling settings: 1) no pooling, 2) GraclusPool, and 3) WavePool, a novel spatially heterogeneous pooling scheme tailored to WaveMesh superpixels. Our experiments demonstrate that SplineCNN learns from multiscale WaveMesh superpixels on-par with similar-sized superpixels. In all WaveMesh experiments, GraclusPool performs poorer than no pooling / WavePool, showing that poor cluster assignment negatively affects the performance of the network while learning from multiscale superpixels.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552300003X",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Geodesy",
      "Geography",
      "Graph",
      "Image (mathematics)",
      "Pattern recognition (psychology)",
      "Pixel",
      "Pooling",
      "Theoretical computer science",
      "Wavelet"
    ],
    "authors": [
      {
        "surname": "Vasudevan",
        "given_name": "Varun"
      },
      {
        "surname": "Bassenne",
        "given_name": "Maxime"
      },
      {
        "surname": "Islam",
        "given_name": "Md Tauhidul"
      },
      {
        "surname": "Xing",
        "given_name": "Lei"
      }
    ]
  },
  {
    "title": "Automatically detecting human-object interaction by an instance part-level attention deep framework",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109110",
    "abstract": "Automatically detecting human-object interactions (HOIs) from an image is a very important but challenging task in computer vision. One of the significant problems in HOI detection is that similar human-object interactions are difficult to distinguish. Recently, many instance-centric HOI detection schemes, based on appearance features and coarse spatial information, have been proposed. These methods, however, lack the capacity of capturing and analyzing the fine-grained context between human poses and object parts, which plays a crucial role in HOI detection. To address these problems, we propose a novel instance part-level attention deep framework for HOI detection. Specifically, our approach consists of a human/object-part detection phase and an HOI detection phase. In the former phase, a part-level visual pattern estimation model is designed for capturing the fine-grained human body parts and object parts. In the latter phase, a self-attention-based deep network is proposed to learn the visual composite around the human-object pair that implicitly expresses the consistent spatial, scale, co-occurrence, and viewpoint relationships among human body parts and object parts across images, which are effective for predicting HOI. To the best of our knowledge, we are the first to propose a framework where the fine-grained part-level mutual context of a human-object pair is extracted to improve HOI detection. By comparing our approach with state-of-the-art HOI detection methods on benchmark datasets, we demonstrated that our proposed framework outperformed the existing HOI detection methods, such as significantly improving the performance of part-level visual pattern estimation, HOI detection, and the quality of the self-attention-based deep network structure.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322005908",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Human interaction",
      "Human–computer interaction",
      "Object (grammar)",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Bai",
        "given_name": "Lin"
      },
      {
        "surname": "Chen",
        "given_name": "Fenglian"
      },
      {
        "surname": "Tian",
        "given_name": "Yang"
      }
    ]
  },
  {
    "title": "A unified low-order information-theoretic feature selection framework for multi-label learning",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109111",
    "abstract": "The approximation of low-order information-theoretic terms for feature selection approaches has achieved success in addressing high-dimensional multi-label data. However, three critical issues exist in such type of approaches: (1) existing approaches are devised based on single heuristic variable correlation assumption, which biases towards some specific scene; (2) high-order variable correlations are ignored by cumulative summation low-order information-theoretic terms; (3) abundant approaches confuse researchers to devise and utilize appropriate approaches. To address these issues, two types of probability distribution assumption in terms of candidate features and labels are derived based on low-order variable correlations. Afterwords, clearing up all information-theoretic terms, we propose a unified feature selection framework including three low-order information-theoretic terms for multi-label learning named Selected Terms of Feature Selection (STFS). STFS contains high-order variable correlations in the form of low-order information-theoretic terms. Furthermore, many previous multi-label feature selection approaches can be reduced to special forms of STFS. Finally, extensive experiments conducted on twelve benchmark data sets in comparison to seven state-of-the-art approaches demonstrate the classification superiority of STFS.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032200591X",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Data mining",
      "Economics",
      "Feature (linguistics)",
      "Feature selection",
      "Finance",
      "Geodesy",
      "Geography",
      "Heuristic",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Order (exchange)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Selection (genetic algorithm)",
      "Variable (mathematics)"
    ],
    "authors": [
      {
        "surname": "Gao",
        "given_name": "Wanfu"
      },
      {
        "surname": "Hao",
        "given_name": "Pingting"
      },
      {
        "surname": "Wu",
        "given_name": "Yang"
      },
      {
        "surname": "Zhang",
        "given_name": "Ping"
      }
    ]
  },
  {
    "title": "Deep learning for image inpainting: A survey",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109046",
    "abstract": "Image inpainting has been widely exploited in the field of computer vision and image processing. The main purpose of image inpainting is to produce visually plausible structure and texture for the missing regions of damaged images. In the past decade, the success of deep learning has brought new opportunities to many vision tasks, which promoted the development of a large number of deep learning-based image inpainting methods. Although these methods have many similarities, they also have their own characteristics due to the differences in data types, application scenarios, computing platforms, etc. It is necessary to classify and summarize these methods to provide a reference for the research community. In this survey, we present a comprehensive overview of recent advances in deep learning-based image inpainting. First, we categorize the deep learning-based techniques from multiple perspectives: inpainting strategies, network structures, and loss functions. Second, we summarize the open source codes and representative public datasets, and introduce the evaluation metrics for quantitative comparisons. Third, we summarize the real-world applications of image inpainting in different scenarios, and give a detailed analysis on the performance of different inpainting algorithms. At last, we conclude the survey and discuss about the future directions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032200526X",
    "keywords": [
      "Artificial intelligence",
      "Categorization",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Field (mathematics)",
      "Image (mathematics)",
      "Image processing",
      "Inpainting",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pure mathematics"
    ],
    "authors": [
      {
        "surname": "Xiang",
        "given_name": "Hanyu"
      },
      {
        "surname": "Zou",
        "given_name": "Qin"
      },
      {
        "surname": "Nawaz",
        "given_name": "Muhammad Ali"
      },
      {
        "surname": "Huang",
        "given_name": "Xianfeng"
      },
      {
        "surname": "Zhang",
        "given_name": "Fan"
      },
      {
        "surname": "Yu",
        "given_name": "Hongkai"
      }
    ]
  },
  {
    "title": "Binary feature learning with local spectral context-aware attention for classification of hyperspectral images",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109123",
    "abstract": "The classification of hyperspectral images (HSIs) has achieved success in applications. For many approaches, features are directly extracted from whole spectral pixels, which can not well describe local characteristics. These methods are also susceptible to noise since each feature code is learned individually. Accordingly, a binary feature learning method with local spectral context-aware attention (BFLSC) is proposed for the classification. Specifically, for training samples, we first build the local spectrum models (LSMs) to describe local spectral properties, where each training sample is segmented into some parts and the difference between the central value and its neighborhoods is calculated in each part. Then, we construct the BFLSC model to learn a projection and binary features of training samples. In such model, the spectral context-awareness attention is established to collaboratively learn binary feature codes by enforcing one shift between 0/1 of each LSM, which enhances the robustness and stability of binary leaning. We also introduce the loss constraint, even distribution constraint, and variance constraint to reduce information loss and improve the quality of learned feature distribution. Additionally, an optimization scheme is designed to obtain the solution of the BFLSC model. Further, the learned binary features are added to train the support vector machine (SVM). For each testing sample, the LSMs are first extracted, and then mapped into binary features by the learned projection. The trained SVM is finally used for the mapped binary features to predict the label of the testing sample. Experimental results validate that our BFLSC realizes the better performance compared with some advanced approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322006033",
    "keywords": [
      "Artificial intelligence",
      "Binary classification",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Feature (linguistics)",
      "Feature learning",
      "Geology",
      "Histogram",
      "Hyperspectral imaging",
      "Image (mathematics)",
      "Linguistics",
      "Local binary patterns",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Xing",
        "given_name": "Changda"
      },
      {
        "surname": "Duan",
        "given_name": "Chaowei"
      },
      {
        "surname": "Wang",
        "given_name": "Zhisheng"
      },
      {
        "surname": "Wang",
        "given_name": "Meiling"
      }
    ]
  },
  {
    "title": "AugFCOS: Augmented fully convolutional one-stage object detection network",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109098",
    "abstract": "As a pioneering work of introducing the idea of full convolutional network into the field of object detection, the fully convolutional one-stage object detection network (FCOS) has the advantage of excellent performance with low memory overhead. However, there are certain problems with FCOS that merit more research: the centerness quality assessment loss does not decrease during the late training stage, and its adaptive training sample selection (ATSS) relies heavily on the hyperparameter. To solve the aforementioned problems, we propose a novel object detection network, named augmented fully convolutional one-stage object detection network (AugFCOS). First of all, we propose an improved dynamic optimization loss (DOL) to mitigate the impact of the original centerness loss not decreasing. Then, a Robust Training Sample Selection (RTSS) is proposed to get rid of the dependence of hyper-parameter in ATSS of FCOS. Finally, a novel mixed attention feature pyramid network (MAFPN) is presented to enhance the multi-scale representation ability of feature pyramid network (FPN) and further improve the ability of multi-scale detection. The experimental results on MS COCO demonstrate the effectiveness of our proposed AugFCOS, where AugFCOS achieves approximate 2.0% to 2.9% increase when compared with ATSS and FCOS.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322005787",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Feature (linguistics)",
      "Feature selection",
      "Geometry",
      "Hyperparameter",
      "Law",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Object (grammar)",
      "Object detection",
      "Operating system",
      "Overhead (engineering)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Pyramid (geometry)",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Xiuwei"
      },
      {
        "surname": "Guo",
        "given_name": "Wei"
      },
      {
        "surname": "Xing",
        "given_name": "Yinghui"
      },
      {
        "surname": "Wang",
        "given_name": "Wenna"
      },
      {
        "surname": "Yin",
        "given_name": "Hanlin"
      },
      {
        "surname": "Zhang",
        "given_name": "Yanning"
      }
    ]
  },
  {
    "title": "Distance-based positive and unlabeled learning for ranking",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109085",
    "abstract": "Learning to rank – producing a ranked list of items specific to a query and with respect to a set of supervisory items – is a problem of general interest. The setting we consider is one in which no analytic description of what constitutes a good ranking is available. Instead, we have a collection of representations and supervisory information consisting of a (target item, interesting items set) pair. We demonstrate analytically, in simulation, and in real data examples that learning to rank via combining representations using an integer linear program is effective when the supervision is as light as “these few items are similar to your item of interest.” While this nomination task is quite general, for specificity we present our methodology from the perspective of vertex nomination in graphs. The methodology described herein is model agnostic.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322005659",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Economics",
      "Graph",
      "Information retrieval",
      "Law",
      "Learning to rank",
      "Machine learning",
      "Management",
      "Mathematics",
      "Nomination",
      "Perspective (graphical)",
      "Political science",
      "Programming language",
      "Rank (graph theory)",
      "Ranking (information retrieval)",
      "Set (abstract data type)",
      "Task (project management)",
      "Theoretical computer science",
      "Vertex (graph theory)"
    ],
    "authors": [
      {
        "surname": "Helm",
        "given_name": "Hayden S."
      },
      {
        "surname": "Basu",
        "given_name": "Amitabh"
      },
      {
        "surname": "Athreya",
        "given_name": "Avanti"
      },
      {
        "surname": "Park",
        "given_name": "Youngser"
      },
      {
        "surname": "Vogelstein",
        "given_name": "Joshua T."
      },
      {
        "surname": "Priebe",
        "given_name": "Carey E."
      },
      {
        "surname": "Winding",
        "given_name": "Michael"
      },
      {
        "surname": "Zlatic",
        "given_name": "Marta"
      },
      {
        "surname": "Cardona",
        "given_name": "Albert"
      },
      {
        "surname": "Bourke",
        "given_name": "Patrick"
      },
      {
        "surname": "Larson",
        "given_name": "Jonathan"
      },
      {
        "surname": "Abdin",
        "given_name": "Marah"
      },
      {
        "surname": "Choudhury",
        "given_name": "Piali"
      },
      {
        "surname": "Yang",
        "given_name": "Weiwei"
      },
      {
        "surname": "White",
        "given_name": "Christopher W."
      }
    ]
  },
  {
    "title": "Clean affinity matrix learning with rank equality constraint for multi-view subspace clustering",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109118",
    "abstract": "The existing multi-view subspace clustering (MVSC) algorithm still has certain limitations. First, the affinity matrix obtained by them is not clean and robust enough since the original multi-view data usually contain noise. Second, they also have defects in exploring the consistency between views. To compensate for these two shortcomings, we propose a novel MVSC, i.e., clean affinity matrix learning with rank equality constraint (CAMR) for MVSC. By borrowing the idea from robust principal component analysis (RPCA), the representation matrix of each view obtained by low-rank representation (LRR) is first cleaned up to obtain a cleaner and more robust affinity matrix. In addition, the rank constraint is utilized to explore the same clustering properties between different views. An objective function solution method based on an augmented Lagrange multiplier (ALM) is designed and tested on four widely employed datasets to verify that CAMR has better clustering performance than certain state-of-the-art methods. We provide the code of CAMR at https://github.com/zhaojinbiao/CAMR.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322005982",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Chemistry",
      "Chromatography",
      "Cluster analysis",
      "Combinatorics",
      "Computer science",
      "Constraint (computer-aided design)",
      "Geometry",
      "Mathematical optimization",
      "Mathematics",
      "Matrix (chemical analysis)",
      "Rank (graph theory)",
      "Subspace topology"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Jinbiao"
      },
      {
        "surname": "Lu",
        "given_name": "Gui-Fu"
      }
    ]
  },
  {
    "title": "A Lie algebra representation for efficient 2D shape classification",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109078",
    "abstract": "Riemannian manifold plays a vital role as a powerful mathematical tool in computer vision, with important applications in curved shape analysis and classification. Significant progress has recently been made by Riemannian framework based methods that achieved state-of-the-art classification accuracy and robustness. However, these Riemannian manifold and Lie group methods require a very high computational complexity and do not include a description of the shape regions. This paper presents a novel mathematical tool, called Block Diagonal Symmetric Positive Definite Matrix Lie Algebra (BDSPDMLA) to represent curves, which extends the existing Lie group representations to a compact yet informative Lie algebra representation. The proposed Lie algebra based method addresses the computational bottleneck problem of the Riemannian framework based methods. In addition, it allows the natural fusion of various regions information with curved shape features for a more discriminative shape description. Here the region information is represented by values of distance maps, local binary patterns (LBP) and image intensity. Extensive experiments on five publicly available databases demonstrate that the proposed Lie algebra based method can achieve a speed of over ten thousand times faster than the Riemannian manifold and Lie group based baseline methods, while obtaining comparable accuracies for 2D shape classification.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322005581",
    "keywords": [
      "Adjoint representation",
      "Adjoint representation of a Lie algebra",
      "Affine Lie algebra",
      "Algebra over a field",
      "Current algebra",
      "Fundamental representation",
      "Graded Lie algebra",
      "Law",
      "Lie algebra",
      "Lie conformal algebra",
      "Lie superalgebra",
      "Mathematics",
      "Political science",
      "Politics",
      "Pure mathematics",
      "Representation (politics)",
      "Representation theory",
      "Weight"
    ],
    "authors": [
      {
        "surname": "Yu",
        "given_name": "Xiaohan"
      },
      {
        "surname": "Gao",
        "given_name": "Yongsheng"
      },
      {
        "surname": "Bennamoun",
        "given_name": "Mohammed"
      },
      {
        "surname": "Xiong",
        "given_name": "Shengwu"
      }
    ]
  },
  {
    "title": "ImageNet-Patch: A dataset for benchmarking machine learning robustness against adversarial patches",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109064",
    "abstract": "Adversarial patches are optimized contiguous pixel blocks in an input image that cause a machine-learning model to misclassify it. However, their optimization is computationally demanding, and requires careful hyperparameter tuning, potentially leading to suboptimal robustness evaluations. To overcome these issues, we propose ImageNet-Patch, a dataset to benchmark machine-learning models against adversarial patches. The dataset is built by first optimizing a set of adversarial patches against an ensemble of models, using a state-of-the-art attack that creates transferable patches. The corresponding patches are then randomly rotated and translated, and finally applied to the ImageNet data. We use ImageNet-Patch to benchmark the robustness of 127 models against patch attacks, and also validate the effectiveness of the given patches in the physical domain (i.e., by printing and applying them to real-world objects). We conclude by discussing how our dataset could be used as a benchmark for robustness, and how our methodology can be generalized to other domains. We open source our dataset and evaluation code at https://github.com/pralab/ImageNet-Patch.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322005441",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Benchmarking",
      "Biochemistry",
      "Business",
      "Chemistry",
      "Computer science",
      "Gene",
      "Machine learning",
      "Marketing",
      "Pattern recognition (psychology)",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Pintor",
        "given_name": "Maura"
      },
      {
        "surname": "Angioni",
        "given_name": "Daniele"
      },
      {
        "surname": "Sotgiu",
        "given_name": "Angelo"
      },
      {
        "surname": "Demetrio",
        "given_name": "Luca"
      },
      {
        "surname": "Demontis",
        "given_name": "Ambra"
      },
      {
        "surname": "Biggio",
        "given_name": "Battista"
      },
      {
        "surname": "Roli",
        "given_name": "Fabio"
      }
    ]
  },
  {
    "title": "Differentiable Mean Opinion Score Regularization for Perceptual Speech Enhancement",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.01.011",
    "abstract": "Many speech enhancement methods require perceptual quality metrics for evaluation. The “holy grail” of perceptual speech quality assessment is human subjective ratings, known as the mean opinion score. However, acquiring human ratings is time-consuming, laborious, and expensive. Existing objective quality metrics, on the other hand, are efficient and easy to compute but do not correlate well with human ratings. In this paper, we propose a relatively lightweight deep-learning-based model to predict the human ratings of speech signals. Since it is differentiable, it can be easily employed as a perceptual regularization to improve existing deep-learning-based speech enhancement methods. Experimental results demonstrate that the predictions of our proposed model correlate well with human judgments. We present application in speech enhancement and show that, interestingly, while there is a degradation in performance in terms of traditional objective metrics, there is a significant improvement in the perceptual quality and the naturalness of the enhanced speech.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552300017X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Deep learning",
      "Economics",
      "Epistemology",
      "Machine learning",
      "Mean opinion score",
      "Metric (unit)",
      "Naturalness",
      "Neuroscience",
      "Noise reduction",
      "Operations management",
      "Perception",
      "Philosophy",
      "Physics",
      "Psychology",
      "Quality (philosophy)",
      "Quantum mechanics",
      "Regularization (linguistics)",
      "Speech enhancement",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Rosenbaum",
        "given_name": "Tomer"
      },
      {
        "surname": "Cohen",
        "given_name": "Israel"
      },
      {
        "surname": "Winebrand",
        "given_name": "Emil"
      },
      {
        "surname": "Gabso",
        "given_name": "Ofri"
      }
    ]
  },
  {
    "title": "DBPNet: A dual-branch pyramid network for document super-resolution",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2022.12.013",
    "abstract": "Convolutional neural networks (CNN), aiming to preserve the structural and texture in- formation lost in the initial low-resolution (LR) images, has been widely used to improve the quality of LR images. Most of the existing super-resolution methods focus on image super-resolution reconstruction in natural scenes, while a few super-resolution methods for document images focus on text data. However, most CNN models focus more on the reconstruction quality of the entire document and ignore the feature difference between the text region and the image region which both exist in the document image. To address this issue, this paper proposes a dual-branch pyramid network (DBPNet) for document super-resolution by taking into consideration about the texture difference between the text region and the image region. DBPNet consists of a region segmentation module (RSM), two parallel pyramid SR branches (PPSRB), and text regions, with a pyramid edge restoration module (ERM) and a region fusion module (RFM) for region stacking. Furthermore, to verify that the model can be adapted to super-resolution reconstruction of document images with more features, we construct a new document super-resolution (SR) dataset namely DSR2021. It contains paired LR and HR images involving English, Chinese, Japanese and Korean languages. Experiments on two document image datasets (DSR2021 and Text330) demonstrate that our method outperforms several state-of-the-art methods quantitatively and qualitatively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552200383X",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Dual (grammatical number)",
      "Feature (linguistics)",
      "Focus (optics)",
      "Geometry",
      "Image (mathematics)",
      "Linguistics",
      "Literature",
      "Mathematics",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Pyramid (geometry)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Peng",
        "given_name": "Jibing"
      },
      {
        "surname": "Yi",
        "given_name": "Yaohua"
      },
      {
        "surname": "Yu",
        "given_name": "Changhui"
      },
      {
        "surname": "Yin",
        "given_name": "Aiguo"
      }
    ]
  },
  {
    "title": "Multi-scale self-attention-based feature enhancement for detection of targets with small image sizes",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2022.12.026",
    "abstract": "In this paper, we propose a feature enhancement method based on multi-scale self-attention, mainly including a multi-scale feature combination module and a self-attention module. The multi-scale feature combination module integrates the multi-layers’ features extracted from the backbone network in both the top-down and bottom-up directions. Then, the shallow and deep features are combined. The self-attention module enhances the feature representation by assigning attention weights to the features that have intrinsic connection to the features of the target. The multi-scale self-attention-based feature enhancement method improves the performance for detecting targets with small image sizes in complex scenes by mutual combination between deep and shallow features and between local and global features. The experimental results show the effectiveness of the proposed feature enhancement method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522003944",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Feature detection (computer vision)",
      "Image (mathematics)",
      "Image processing",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Scale (ratio)"
    ],
    "authors": [
      {
        "surname": "Deng",
        "given_name": "Ying"
      },
      {
        "surname": "Hu",
        "given_name": "Xingliang"
      },
      {
        "surname": "Li",
        "given_name": "Bing"
      },
      {
        "surname": "Zhang",
        "given_name": "Congxuan"
      },
      {
        "surname": "Hu",
        "given_name": "Weiming"
      }
    ]
  },
  {
    "title": "Auto-weighted Tensor Schatten p-Norm for Robust Multi-view Graph Clustering",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109083",
    "abstract": "Recently, tensor-singular value decomposition based tensor-nuclear norm ( t -TNN) has achieved impressive performance for multi-view graph clustering. This primarily ascribes the superiority of t -TNN in exploring high-order structure information among views. However, 1) t -TNN cannot ideally approximate to the original rank minimization, which produces the suboptimal graph tensor; in addition, t -TNN treats different singular values equally, such that the larger singular values corresponding to certain significant feature information (i.e., prior information) has not been utilized fully; 2) the data of original high-dimensional space are often corrupted by noise and outliers, which always makes adaptive neighbors graph learning (ANGL) generate low-quality affinity graphs. To address these issues, we propose a novel multi-view graph clustering method termed auto-weighted tensor Schatten p -norm ( t -ATSN) for robust multi-view graph clustering ( t -ATSN-RMGC). Concretely, we first propose t -SVD based t -ATSN with 0 < p < 1 to make the learned graph tensor better approximate the target rank than t -TNN. Meanwhile, it can also automatically and appropriately shrink singular values for constructing a more refined graph tensor, so as to fully capture spatial structure in the graph tensor. Moreover, we introduce the Geman McClure loss function to enhance the robustness of ANGL for noise and outliers. Experimental results on benchmarks across different scenarios and sizes show that the proposed method consistently outperforms state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322005635",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Cluster analysis",
      "Clustering coefficient",
      "Combinatorics",
      "Computer science",
      "Eigenvalues and eigenvectors",
      "Graph",
      "Mathematics",
      "Outlier",
      "Pattern recognition (psychology)",
      "Physics",
      "Pure mathematics",
      "Quantum mechanics",
      "Singular value",
      "Singular value decomposition",
      "Tensor (intrinsic definition)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Xingfeng"
      },
      {
        "surname": "Ren",
        "given_name": "Zhenwen"
      },
      {
        "surname": "Sun",
        "given_name": "Quansen"
      },
      {
        "surname": "Xu",
        "given_name": "Zhi"
      }
    ]
  },
  {
    "title": "Dual quaternion ambisonics array for six-degree-of-freedom acoustic representation",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2022.12.006",
    "abstract": "Spatial audio methods are gaining a growing interest due to the spread of immersive audio experiences and applications, such as virtual and augmented reality. For these purposes, 3D audio signals are often acquired through arrays of Ambisonics microphones, each comprising four capsules that decompose the sound field in spherical harmonics. In this paper, we propose a dual quaternion representation of the spatial sound field acquired through an array of two First Order Ambisonics (FOA) microphones. The audio signals are encapsulated in a dual quaternion that leverages quaternion algebra properties to exploit correlations among them. This augmented representation with 6 degrees of freedom (6DOF) involves a more accurate coverage of the sound field, resulting in a more precise sound localization and a more immersive audio experience. We evaluate our approach on a sound event localization and detection (SELD) benchmark. We show that our dual quaternion SELD model with temporal convolution blocks (DualQSELD-TCN) achieves better results with respect to real and quaternion-valued baselines thanks to our augmented representation of the sound field. Full code is available at: https://github.com/ispamm/DualQSELD-TCN.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522003749",
    "keywords": [
      "Acoustics",
      "Ambisonics",
      "Art",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Convolution (computer science)",
      "Dual (grammatical number)",
      "Field (mathematics)",
      "Geometry",
      "Law",
      "Literature",
      "Loudspeaker",
      "Mathematical analysis",
      "Mathematics",
      "Physics",
      "Political science",
      "Politics",
      "Pure mathematics",
      "Quaternion",
      "Representation (politics)",
      "Spherical harmonics"
    ],
    "authors": [
      {
        "surname": "Grassucci",
        "given_name": "Eleonora"
      },
      {
        "surname": "Mancini",
        "given_name": "Gioia"
      },
      {
        "surname": "Brignone",
        "given_name": "Christian"
      },
      {
        "surname": "Uncini",
        "given_name": "Aurelio"
      },
      {
        "surname": "Comminiello",
        "given_name": "Danilo"
      }
    ]
  },
  {
    "title": "Reversible attack based on adversarial perturbation and reversible data hiding in YUV colorspace",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2022.12.018",
    "abstract": "Recent research on using adversarial perturbation to prevent malicious models from accessing image data has led to the corruption of image data, making images useless in other fields, especially in digital forensics. To prevent malicious models from retrieving images and ensure that authorized models can recover original image data without distortion, the reversible attack technique is rising. However, attack ability, reversibility, and image visual quality are three major challenges for existing reversible attack techniques. In this paper, a novel reversible attack method based on adversarial perturbation and reversible data hiding in YUV colorspace is proposed. We first add adversarial perturbation into the luminance channel. Then, the luminance channel distortion caused by adversarial perturbation is embedded into chrominance channels by reversible data hiding to achieve the reversible attack. In particular, the class activation mapping module is introduced to narrow the perturbation region to reduce the amount of embedded data. Experimental results on the ImageNet dataset demonstrated that the proposed method achieves better attack ability and image visual quality and ensures that original images can be recovered without distortion.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522003877",
    "keywords": [
      "Adversarial system",
      "Amplifier",
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Chrominance",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Distortion (music)",
      "Image (mathematics)",
      "Image quality",
      "Information hiding",
      "Luminance",
      "Perturbation (astronomy)",
      "Physics",
      "Quantum mechanics"
    ],
    "authors": [
      {
        "surname": "Yin",
        "given_name": "Zhaoxia"
      },
      {
        "surname": "Chen",
        "given_name": "Li"
      },
      {
        "surname": "Lyu",
        "given_name": "Wanli"
      },
      {
        "surname": "Luo",
        "given_name": "Bin"
      }
    ]
  },
  {
    "title": "GraphDPI: Partial label disambiguation by graph representation learning via mutual information maximization",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109133",
    "abstract": "Partial label learning (PLL) is a weakly supervised learning framework where each training instance is associated with more than one candidate label, and only one of them is the true label. Most of the existing PLL algorithms directly disambiguate the candidate labels according to the instance feature similarity, but fail to discover the latent semantic relationship over the entire dataset. In this paper, method GraphDPI, an innovative deep partial label disambiguation by graph representation via mutual information maximization, is proposed. This method can capture the semantic clusters with the most unique information in the latent space and automatically adapt to different feature distributions. Specifically, a new sampling method based on the graph is proposed to estimate mutual information, extending GCN to the field of weakly supervised learning. Therefore, the graph representation of the data can contain more distinguishing information to disambiguate candidate labels by maximizing the mutual information of the local graph representation and the global one. Furthermore, the triplet loss is introduced to fully exploit the relationship between instances and extract the latent embedding representation over the entire dataset. It thereby can make the model output as large as possible on the inter-class variation and as small as possible on the intra-class variation. Finally, the candidate labels can be disambiguated by the difference between semantic clusters. Experiments reveal the overwhelming performances of GraphDPI.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322006136",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Embedding",
      "Feature (linguistics)",
      "Feature learning",
      "Graph",
      "Linguistics",
      "Machine learning",
      "Mutual information",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Fan",
        "given_name": "Jinfu"
      },
      {
        "surname": "Yu",
        "given_name": "Yang"
      },
      {
        "surname": "Huang",
        "given_name": "Linqing"
      },
      {
        "surname": "Wang",
        "given_name": "Zhongjie"
      }
    ]
  },
  {
    "title": "PFEMed: Few-shot medical image classification using prior guided feature enhancement",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109108",
    "abstract": "Deep learning-based methods have recently demonstrated outstanding performance on general image classification tasks. As optimization of these methods is dependent on a large amount of labeled data, their application in medical image classification is limited. To address this issue, we propose PFEMed, a novel few-shot classification method for medical images. To extract general and specific features from medical images, this method employs a dual-encoder structure, that is, one encoder with fixed weights pre-trained on public image classification datasets and another encoder trained on the target medical dataset. In addition, we introduce a novel prior-guided Variational Autoencoder (VAE) module to enhance the robustness of the target feature, which is the concatenation of the general and specific features. Then, we match the target features extracted from both the support and query medical image samples and predict the category attribution of the query examples. Extensive experiments on several publicly available medical image datasets show that our method outperforms current state-of-the-art few-shot methods by a wide margin, particularly outperforming MetaMed on the Pap smear dataset by over 2.63%.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032200588X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Contextual image classification",
      "Engineering",
      "Feature (linguistics)",
      "Image (mathematics)",
      "Linguistics",
      "Materials science",
      "Mathematics",
      "Mechanical engineering",
      "Metallurgy",
      "One shot",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Shot (pellet)"
    ],
    "authors": [
      {
        "surname": "Dai",
        "given_name": "Zhiyong"
      },
      {
        "surname": "Yi",
        "given_name": "Jianjun"
      },
      {
        "surname": "Yan",
        "given_name": "Lei"
      },
      {
        "surname": "Xu",
        "given_name": "Qingwen"
      },
      {
        "surname": "Hu",
        "given_name": "Liang"
      },
      {
        "surname": "Zhang",
        "given_name": "Qi"
      },
      {
        "surname": "Li",
        "given_name": "Jiahui"
      },
      {
        "surname": "Wang",
        "given_name": "Guoqiang"
      }
    ]
  },
  {
    "title": "Mitigating the effect of dataset shift in clustering",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109058",
    "abstract": "Dataset shift is a relevant topic in unsupervised learning since many applications face evolving environments, causing an important loss of generalization and performance. Most techniques that deal with this issue are designed for data stream clustering, whose goal is to process sequences of data efficiently under Big Data. In this study, we claim dataset shift is an issue for static clustering tasks in which data is collected over a long period. To mitigate it, we propose Time-weighted kernel k -means, a k -means variant that includes a time-dependent weighting process. We do this via the induced ordered weighted average (IOWA) operator. The weighting process acts as a gradual forgetting mechanism, prioritizing recent examples over outdated ones in the clustering algorithm. The computational experiments show the potential Time-weighted kernel k -means has in evolving environments.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322005386",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Maldonado",
        "given_name": "Sebastián"
      },
      {
        "surname": "Saltos",
        "given_name": "Ramiro"
      },
      {
        "surname": "Vairetti",
        "given_name": "Carla"
      },
      {
        "surname": "Delpiano",
        "given_name": "José"
      }
    ]
  },
  {
    "title": "Multi-label feature selection via robust flexible sparse regularization",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109074",
    "abstract": "Multi-label feature selection is an efficient technique to deal with the high dimensional multi-label data by selecting the optimal feature subset. Existing researches have demonstrated that l 1 -norm and l 2 , 1 -norm are promising roles for multi-label feature selection. However, two important issues are ignored when existing l 1 -norm and l 2 , 1 -norm based methods select discriminative features for multi-label data. First, l 1 -norm can enforce sparsity on each feature across all instances while numerous selected features lack discrimination due to the generated zero weight values. Second, l 2 , 1 -norm not only neglects label-specific features but also ignores the redundancy among features. To this end, we design a Robust Flexible Sparse Regularization norm (RFSR), furthermore, proposing a global optimization framework named Robust Flexible Sparse regularized multi-label Feature Selection (RFSFS) based on RFSR. Finally, an efficient alternating multipliers based optimization scheme is developed to iteratively optimize RFSFS. Empirical studies on fifteen benchmark multi-label data sets demonstrate the effectiveness and efficiency of RFSFS.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322005544",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Data mining",
      "Discriminative model",
      "Feature selection",
      "Geodesy",
      "Geography",
      "Law",
      "Machine learning",
      "Minimum redundancy feature selection",
      "Norm (philosophy)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Political science",
      "Redundancy (engineering)",
      "Regularization (linguistics)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Yonghao"
      },
      {
        "surname": "Hu",
        "given_name": "Liang"
      },
      {
        "surname": "Gao",
        "given_name": "Wanfu"
      }
    ]
  },
  {
    "title": "Improving adversarial robustness by learning shared information",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109054",
    "abstract": "We consider the problem of improving the adversarial robustness of neural networks while retaining natural accuracy. Motivated by the multi-view information bottleneck formalism, we seek to learn a representation that captures the shared information between clean samples and their corresponding adversarial samples while discarding these samples’ view-specific information. We show that this approach leads to a novel multi-objective loss function, and we provide mathematical motivation for its components towards improving the robust vs. natural accuracy tradeoff. We demonstrate enhanced tradeoff compared to current state-of-the-art methods with extensive evaluation on various benchmark image datasets and architectures. Ablation studies indicate that learning shared representations is key to improving performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322005349",
    "keywords": [
      "Adversarial system",
      "Art",
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Bottleneck",
      "Chemistry",
      "Computer science",
      "Deep neural networks",
      "Embedded system",
      "Feature learning",
      "Formalism (music)",
      "Gene",
      "Information bottleneck method",
      "Machine learning",
      "Musical",
      "Mutual information",
      "Robustness (evolution)",
      "Theoretical computer science",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Yu",
        "given_name": "Xi"
      },
      {
        "surname": "Smedemark-Margulies",
        "given_name": "Niklas"
      },
      {
        "surname": "Aeron",
        "given_name": "Shuchin"
      },
      {
        "surname": "Koike-Akino",
        "given_name": "Toshiaki"
      },
      {
        "surname": "Moulin",
        "given_name": "Pierre"
      },
      {
        "surname": "Brand",
        "given_name": "Matthew"
      },
      {
        "surname": "Parsons",
        "given_name": "Kieran"
      },
      {
        "surname": "Wang",
        "given_name": "Ye"
      }
    ]
  },
  {
    "title": "A consistent and flexible framework for deep matrix factorizations",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109102",
    "abstract": "Deep matrix factorizations (deep MFs) are recent unsupervised data mining techniques inspired by constrained low-rank approximations. They aim to extract complex hierarchies of features within high-dimensional datasets. Most of the loss functions proposed in the literature to evaluate the quality of deep MF models and the underlying optimization frameworks are not consistent because different losses are used at different layers. In this paper, we introduce two meaningful loss functions for deep MF and present a generic framework to solve the corresponding optimization problems. We illustrate the effectiveness of this approach through the integration of various constraints and regularizations, such as sparsity, nonnegativity and minimum-volume. The models are successfully applied on both synthetic and real data, namely for hyperspectral unmixing and extraction of facial features.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322005829",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Combinatorics",
      "Composite material",
      "Computer science",
      "Deep learning",
      "Eigenvalues and eigenvectors",
      "Hyperspectral imaging",
      "Materials science",
      "Mathematical optimization",
      "Mathematics",
      "Matrix (chemical analysis)",
      "Matrix decomposition",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Rank (graph theory)"
    ],
    "authors": [
      {
        "surname": "De Handschutter",
        "given_name": "Pierre"
      },
      {
        "surname": "Gillis",
        "given_name": "Nicolas"
      }
    ]
  },
  {
    "title": "Domain Generalization by Joint-Product Distribution Alignment",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109086",
    "abstract": "In this work, we address the problem of domain generalization for classification, where the goal is to learn a classification model on a set of source domains and generalize it to a target domain. The source and target domains are different, which weakens the generalization ability of the learned model. To tackle the domain difference, we propose to align a joint distribution and a product distribution using a neural transformation, and minimize the Relative Chi-Square (RCS) divergence between the two distributions to learn that transformation. In this manner, we conveniently achieve the alignment of multiple domains in the neural transformation space. Specifically, we show that the RCS divergence can be explicitly estimated as the maximal value of a quadratic function, which allows us to perform joint-product distribution alignment by minimizing the divergence estimate. We demonstrate the effectiveness of our solution through comparison with the state-of-the-art methods on several image classification datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322005660",
    "keywords": [
      "Algorithm",
      "Architectural engineering",
      "Artificial intelligence",
      "Biochemistry",
      "Biology",
      "Chemistry",
      "Computer science",
      "Distribution (mathematics)",
      "Divergence (linguistics)",
      "Domain (mathematical analysis)",
      "Engineering",
      "Evolutionary biology",
      "Function (biology)",
      "Gene",
      "Generalization",
      "Geometry",
      "Joint (building)",
      "Joint probability distribution",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Product (mathematics)",
      "Programming language",
      "Quadratic equation",
      "Set (abstract data type)",
      "Statistics",
      "Transformation (genetics)"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Sentao"
      },
      {
        "surname": "Wang",
        "given_name": "Lei"
      },
      {
        "surname": "Hong",
        "given_name": "Zijie"
      },
      {
        "surname": "Yang",
        "given_name": "Xiaowei"
      }
    ]
  },
  {
    "title": "Multi-Source Change-Point Detection over Local Observation Models",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109116",
    "abstract": "In this work, we address the problem of change-point detection (CPD) on high-dimensional, multi-source, and heterogeneous sequential data with missing values. We present a new CPD methodology based on local latent variable models and adaptive factorizations that enhances the fusion of multi-source observations with different statistical data-type and face the problem of high dimensionality. Our motivation comes from behavioral change detection in healthcare measured by smartphone monitored data and Electronic Health Records. Due to the high dimension of the observations and the differences in the relevance of each source information, other works fail in obtaining reliable estimates of the change-points location. This leads to methods that are not sensitive enough when dealing with interspersed changes of different intensity within the same sequence or partial missing components. Through the definition of local observation models (LOMs), we transfer the local CP information to homogeneous latent spaces and propose several factorizations that weight the contribution of each source to the global CPD. With the presented methods we demonstrate a reduction in both the detection delay and the number of not-detected CPs, together with robustness against the presence of missing values on a synthetic dataset. We illustrate its application on real-world data from a smartphone-based monitored study and add explainability on the degree of each source contributing to the detection.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322005969",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Change detection",
      "Chemistry",
      "Computer science",
      "Curse of dimensionality",
      "Data mining",
      "Dimensionality reduction",
      "Gene",
      "Machine learning",
      "Missing data",
      "Pattern recognition (psychology)",
      "Robustness (evolution)",
      "Sensor fusion"
    ],
    "authors": [
      {
        "surname": "Romero-Medrano",
        "given_name": "Lorena"
      },
      {
        "surname": "Artés-Rodríguez",
        "given_name": "Antonio"
      }
    ]
  },
  {
    "title": "Generating diverse augmented attributes for generalized zero shot learning",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.01.005",
    "abstract": "Generalized Zero-Shot Learning (GZSL) has become an important research due to its powerful ability of recognizing unseen objects. Generative methods, converting conventional GZSL to fully supervised learning, can achieve competing performance, and most of them use semantic attributes plus Gaussian noise to enrich generated features. The visual features obtained in this way are consistent with the semantic description. However, the reality is that the semantic description of the visual features of the same category should be different, because the appearance of images differs from each other although they belong to a same category, i.e., mapping from semantic attributes to visual features should be a “many to many” relationship rather than “one to many”. Therefore, we propose a novel method to generate diverse augmented attribute, which are subsequently utilized to synthesize features. We construct a semantic generator based on a pre-trained semantic mapper, which augments the category semantics. Using the augmented category semantics to generate visual features will result in a better fit of the generated visual features to the distribution of real features. The proposed method can well solve the pseudo diversity of visual features generated by most generative GZSL methods. We evaluate the proposed method on five popular benchmark datasets, and the results show that it can achieve the state-of-the-art performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523000041",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Construct (python library)",
      "Generative grammar",
      "Generative model",
      "Generator (circuit theory)",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Physics",
      "Power (physics)",
      "Programming language",
      "Quantum mechanics",
      "Semantics (computer science)"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Xiaojie"
      },
      {
        "surname": "Shen",
        "given_name": "Yuming"
      },
      {
        "surname": "Wang",
        "given_name": "Shidong"
      },
      {
        "surname": "Zhang",
        "given_name": "Haofeng"
      }
    ]
  },
  {
    "title": "Conference on graphics, patterns and images",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2022.12.002",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522003701",
    "keywords": [
      "Acoustics",
      "Ambisonics",
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Computer science",
      "Convolution (computer science)",
      "Field (mathematics)",
      "Geodesy",
      "Geography",
      "Geometry",
      "Law",
      "Loudspeaker",
      "Mathematics",
      "Physics",
      "Political science",
      "Politics",
      "Pure mathematics",
      "Quaternion",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Proença",
        "given_name": "Hugo"
      },
      {
        "surname": "Menotti",
        "given_name": "David"
      },
      {
        "surname": "Paiva",
        "given_name": "Afonso"
      },
      {
        "surname": "Baranoski",
        "given_name": "Gladimir"
      }
    ]
  },
  {
    "title": "High-order manifold regularized multi-view subspace clustering with robust affinity matrices and weighted TNN",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109067",
    "abstract": "Multi-view subspace clustering achieves impressive performance for high-dimensional data. However, many of these models do not sufficiently mine the intrinsic information among samples and consider the robustness problem of the affinity matrices, resulting in the degradation of clustering performance. To address these problems, we propose a novel high-order manifold regularized multi-view subspace clustering with robust affinity matrices and a weighted tensor nuclear norm (TNN) model (termed HMRMSC) to characterize real-world data. Specifically, all the similarity matrices of different views are first stacked into a third-order tensor. However, the constructed tensor may contain an additional inter-class representation since the data are usually noisy. Then, we use a technique similar to tensor principal component analysis (TPCA) to obtain a more robust similarity tensor, which is constrained by the so-called weighted TNN since the original TNN treats each singular value equally and usually considers no prior information of singular values. In addition, a high-order manifold regularized term is also added to utilize the manifold information of data. Finally, all the steps are unified into a framework, which is resolved by the augmented Lagrange multiplier (ALM) method. Experimental results on six representative datasets show that our model outperforms several state-of-the-art counterparts.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322005477",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Cluster analysis",
      "Computer science",
      "Eigenvalues and eigenvectors",
      "Engineering",
      "Gene",
      "Law",
      "Manifold (fluid mechanics)",
      "Mathematics",
      "Matrix norm",
      "Mechanical engineering",
      "Norm (philosophy)",
      "Pattern recognition (psychology)",
      "Physics",
      "Political science",
      "Principal component analysis",
      "Pure mathematics",
      "Quantum mechanics",
      "Robust principal component analysis",
      "Robustness (evolution)",
      "Singular value",
      "Subspace topology",
      "Tensor (intrinsic definition)"
    ],
    "authors": [
      {
        "surname": "Cai",
        "given_name": "Bing"
      },
      {
        "surname": "Lu",
        "given_name": "Gui-Fu"
      },
      {
        "surname": "Yao",
        "given_name": "Liang"
      },
      {
        "surname": "Li",
        "given_name": "Hua"
      }
    ]
  },
  {
    "title": "Curvilinear Structure Tracking Based on Dynamic Curvature-penalized Geodesics",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109079",
    "abstract": "Geodesic models are considered as a fundamental and powerful tool in the applications of curvilinear structure extraction, where the target structures are usually modeled as geodesic paths connecting prescribed points. Despite great advances in geodesic models, it still remains an unsolved problem of detecting weak curvilinear structures from complicated scenarios. In this paper, a dynamic high-order geodesic model for curvilinear structure extraction is introduced to alleviate the shortcuts or short branches combination problems suffered in the classical geodesic approaches. For that purpose, we take into account the nonlocal pattern of curvilinear structures and the local curvature of geodesic paths for the construction of geodesic metrics. Accordingly, the proposed model is able to blend the benefits from the on-the-fly nonlocal smoothness property, curvature regularization and appearance coherence penalization. The nonlocal smoothness property carried out via a local bending operator is constructed to provide a quantitative measure of geodesic advancing directions, meanwhile the coherence penalization is established to guarantee the consistency of the local appearance features extracted via a vessel detector. The experiment results on synthetic and real images illustrate that the proposed method obtains outperformance when compared to the classical geodesic-based tracing algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322005593",
    "keywords": [
      "Algorithm",
      "Coherence (philosophical gambling strategy)",
      "Computer science",
      "Convexity",
      "Curvature",
      "Curvilinear coordinates",
      "Economics",
      "Financial economics",
      "Geodesic",
      "Geometry",
      "Mathematical analysis",
      "Mathematics",
      "Smoothness",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Li"
      },
      {
        "surname": "Wang",
        "given_name": "Mingzhu"
      },
      {
        "surname": "Zhou",
        "given_name": "Shuwang"
      },
      {
        "surname": "Shu",
        "given_name": "Minglei"
      },
      {
        "surname": "Cohen",
        "given_name": "Laurent D."
      },
      {
        "surname": "Chen",
        "given_name": "Da"
      }
    ]
  },
  {
    "title": "Elastic-band transform for visualization and detection",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.01.010",
    "abstract": "This paper presents a new multiscale transformation for statistical analysis of one-dimensional data such as time series under the concept of the scale-space approach. The proposed method uses regular observations (eye scanning) with a range of different intervals. The new approach, termed ‘elastic-band transform,’ can be considered as a collection of observations over various intervals (length of elastic-band) of viewing. It is motivated by how people look at an object, such as a sequence of data repeatedly to overview a global structure of the object and find some specific features of it. Some measures based on the transformed elastic-bands are discussed for describing characteristics of data, and multiscale visualizations induced by the measures are developed for a better understanding of data. Several numerical experiments are performed to demonstrate the usefulness of the proposed transform for visualization and detection.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523000156",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Biology",
      "Chemistry",
      "Composite material",
      "Computer science",
      "Data mining",
      "Gene",
      "Materials science",
      "Object (grammar)",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Range (aeronautics)",
      "Scale (ratio)",
      "Series (stratigraphy)",
      "Transformation (genetics)",
      "Visualization"
    ],
    "authors": [
      {
        "surname": "Choi",
        "given_name": "Guebin"
      },
      {
        "surname": "Oh",
        "given_name": "Hee-Seok"
      }
    ]
  },
  {
    "title": "Parzen Window Approximation on Riemannian Manifold",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109081",
    "abstract": "In graph motivated learning, label propagation largely depends on data affinity represented as edges between connected data points. The affinity assignment implicitly assumes even distribution of data on the manifold. This assumption may not hold and may lead to inaccurate metric assignment due to drift towards high-density regions. The drift affected heat kernel based affinity with a globally fixed Parzen window either discards genuine neighbors or forces distant data points to become a member of the neighborhood. This yields a biased affinity matrix. In this paper, the bias due to uneven data sampling on the Riemannian manifold is catered to by a variable Parzen window determined as a function of neighborhood size, ambient dimension, flatness range, etc. Additionally, affinity adjustment is used which offsets the effect of uneven sampling responsible for the bias. An affinity metric which takes into consideration the irregular sampling effect to yield accurate label propagation is proposed. Extensive experiments on synthetic and real-world data sets confirm that the proposed method increases the classification accuracy significantly and outperforms existing Parzen window estimators in graph Laplacian manifold regularization methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322005611",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Curse of dimensionality",
      "Data point",
      "Engineering",
      "Estimator",
      "Filter (signal processing)",
      "Graph",
      "Heat kernel",
      "Intrinsic dimension",
      "Kernel density estimation",
      "Laplacian matrix",
      "Manifold (fluid mechanics)",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Mechanical engineering",
      "Regularization (linguistics)",
      "Riemannian manifold",
      "Sampling (signal processing)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Abhishek",
        "given_name": ""
      },
      {
        "surname": "Kumar Yadav",
        "given_name": "Rakesh"
      },
      {
        "surname": "Verma",
        "given_name": "Shekhar"
      }
    ]
  },
  {
    "title": "Unauthorized AI cannot recognize me: Reversible adversarial example",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109048",
    "abstract": "In this study, we propose a new methodology to control how user’s data is recognized and used by AI via exploiting the properties of adversarial examples. For this purpose, we propose reversible adversarial example (RAE), a new type of adversarial example. A remarkable feature of RAE is that the image can be correctly recognized and used by the AI model specified by the user because the authorized AI can recover the original image from the RAE exactly by eliminating adversarial perturbation. On the other hand, other unauthorized AI models cannot recognize it correctly because it functions as an adversarial example. Moreover, RAE can be considered as one type of encryption to computer vision since reversibility guarantees the decryption. To realize RAE, we combine three technologies, adversarial example, reversible data hiding for exact recovery of adversarial perturbation, and encryption for selective control of AIs who can remove adversarial perturbation. Experimental results show that the proposed method can achieve comparable attack ability with the corresponding adversarial attack method and similar visual quality with the original image, including white-box attacks and black-box attacks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322005283",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Jiayang"
      },
      {
        "surname": "Zhang",
        "given_name": "Weiming"
      },
      {
        "surname": "Fukuchi",
        "given_name": "Kazuto"
      },
      {
        "surname": "Akimoto",
        "given_name": "Youhei"
      },
      {
        "surname": "Sakuma",
        "given_name": "Jun"
      }
    ]
  },
  {
    "title": "Cross-domain structure learning for visual data recognition",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109127",
    "abstract": "Unsupervised domain adaptation methods are used to train an effect model by utilizing available knowledge from a labeled source domain for solving tasks in an unlabeled target domain. The most difficult challenge is determining methods to reduce distribution discrepancies and extract the largest number of domain-invariant features between the source and target domains to improve model performance. With the aim of minimizing the domain shift and maximizing domain-invariant feature extraction, we propose a cross-domain structure learning (CDSL) method for visual data recognition, which incorporates global distribution alignment and local discriminative structure preservation to capture the common, underlying features between domains. Specifically, we design a simple but effective classwise structure learning strategy with a specific compactness hierarchy to promote intraclass knowledge transfer and reduce the risk of negative transfer between domains. We also extend CDSL to different kinds of kernelization to address complex situations in the real world. Extensive experiments on several visual data benchmarks demonstrate the effectiveness of our proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322006070",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Discriminative model",
      "Domain (mathematical analysis)",
      "Feature extraction",
      "Graph",
      "Kernelization",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Theoretical computer science",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Lu",
        "given_name": "Yuwu"
      },
      {
        "surname": "Luo",
        "given_name": "Xingping"
      },
      {
        "surname": "Wen",
        "given_name": "Jiajun"
      },
      {
        "surname": "Lai",
        "given_name": "Zhihui"
      },
      {
        "surname": "Li",
        "given_name": "Xuelong"
      }
    ]
  },
  {
    "title": "Jigsaw-ViT: Learning jigsaw puzzles in vision transformer",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2022.12.023",
    "abstract": "The success of Vision Transformer (ViT) in various computer vision tasks has promoted the ever-increasing prevalence of this convolution-free network. The fact that ViT works on image patches makes it potentially relevant to the problem of jigsaw puzzle solving, which is a classical self-supervised task aiming at reordering shuffled sequential image patches back to their original form. Solving jigsaw puzzle has been demonstrated to be helpful for diverse tasks using Convolutional Neural Networks (CNNs), such as feature representation learning, domain generalization and fine-grained classification. In this paper, we explore solving jigsaw puzzle as a self-supervised auxiliary loss in ViT for image classification, named Jigsaw-ViT. We show two modifications that can make Jigsaw-ViT superior to standard ViT: discarding positional embeddings and masking patches randomly. Yet simple, we find that the proposed Jigsaw-ViT is able to improve on both generalization and robustness over the standard ViT, which is usually rather a trade-off. Numerical experiments verify that adding the jigsaw puzzle branch provides better generalization to ViT on large-scale image classification on ImageNet. Moreover, such auxiliary loss also improves robustness against noisy labels on Animal-10N, Food-101N, and Clothing1M, as well as adversarial examples. Our implementation is available at https://yingyichen-cyy.github.io/Jigsaw-ViT.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522003920",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Convolutional neural network",
      "Gene",
      "Generalization",
      "Jigsaw",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Mathematics education",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Robustness (evolution)",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Yingyi"
      },
      {
        "surname": "Shen",
        "given_name": "Xi"
      },
      {
        "surname": "Liu",
        "given_name": "Yahui"
      },
      {
        "surname": "Tao",
        "given_name": "Qinghua"
      },
      {
        "surname": "Suykens",
        "given_name": "Johan A.K."
      }
    ]
  },
  {
    "title": "BehavePassDB: Public Database for Mobile Behavioral Biometrics and Benchmark Evaluation",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109089",
    "abstract": "Mobile behavioral biometrics have become a popular topic of research, reaching promising results in terms of authentication, exploiting a multimodal combination of touchscreen and background sensor data. However, there is no way of knowing whether state-of-the-art classifiers in the literature can distinguish between the notion of user and device. In this article, we present a new database, BehavePassDB, structured into separate acquisition sessions and tasks to mimic the most common aspects of mobile Human-Computer Interaction (HCI). BehavePassDB is acquired through a dedicated mobile app installed on the subjects devices, also including the case of different users on the same device for evaluation. We propose a standard experimental protocol and benchmark for the research community to perform a fair comparison of novel approaches with the state of the art 1 1 https://github.com/BiDAlab/MobileB2C_BehavePassDB/. . We propose and evaluate a system based on Long-Short Term Memory (LSTM) architecture with triplet loss and modality fusion at score level.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322005696",
    "keywords": [
      "Alternative medicine",
      "Artificial intelligence",
      "Authentication (law)",
      "Benchmark (surveying)",
      "Biometrics",
      "Computer science",
      "Computer security",
      "Geodesy",
      "Geography",
      "Human–computer interaction",
      "Machine learning",
      "Medicine",
      "Mobile apps",
      "Mobile device",
      "Modality (human–computer interaction)",
      "Pathology",
      "Protocol (science)",
      "Touchscreen",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Stragapede",
        "given_name": "Giuseppe"
      },
      {
        "surname": "Vera-Rodriguez",
        "given_name": "Ruben"
      },
      {
        "surname": "Tolosana",
        "given_name": "Ruben"
      },
      {
        "surname": "Morales",
        "given_name": "Aythami"
      }
    ]
  },
  {
    "title": "Support subsets estimation for support vector machines retraining",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109117",
    "abstract": "The availability of new data in previously trained Machine Learning (ML) models usually requires retraining and adjustment of the model. Support Vector Machines (SVMs) are widely used in ML because of their strong mathematical foundations and flexibility. However, SVM training is computationally expensive, both in time and memory. Hence, the training phase might be a limitation in problems where the model is updated regularly. As a solution, new methods for training and updating SVMs have been proposed in the past. In this paper, we introduce the concept of Support Subset and a new retraining methodology for SVMs. A Support Subset is a subset of the training set, such that retraining a ML model with this subset and the new data is equivalent to training with all the data. The performance of the proposal is evaluated in a variety of experiments on simulated and real datasets in terms of time, quality of the solution, resultant support vectors, and amount of employed data. The promising results provide a new research line for improving the effectiveness and adaptability of the proposed technique, including its generalization to other ML models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322005970",
    "keywords": [
      "Adaptability",
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Business",
      "Computation",
      "Computer science",
      "Data mining",
      "Ecology",
      "Flexibility (engineering)",
      "Generalization",
      "International trade",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Programming language",
      "Retraining",
      "Set (abstract data type)",
      "Statistics",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Aceña",
        "given_name": "Víctor"
      },
      {
        "surname": "Martín de Diego",
        "given_name": "Isaac"
      },
      {
        "surname": "R． Fernández",
        "given_name": "Rubén"
      },
      {
        "surname": "M． Moguerza",
        "given_name": "Javier"
      }
    ]
  },
  {
    "title": "Towards Parameter-Free Clustering for Real-World Data",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109062",
    "abstract": "While many clustering algorithms have been published, existing algorithms are often afflicted by some problems in processing real-world data. We present an algorithm to deal with two of these problems in this paper. First, the majority of clustering algorithms depend on one or more parameters. Second, some algorithms are not suitable for clusters of Gaussian distribution, whereas clusters of many real datasets are of Gaussian distribution approximately. Our algorithm generates clusters sequentially, and each cluster is obtained by expanding an initial cluster. The initial cluster is extracted with the dominant set algorithm, and we study the correlation between the pairwise data similarity matrix and clustering result to determine the involved scaling parameter adaptively. In expanding the initial cluster, we improve the density peak algorithm so that the expansion will not cross the boundary between two clusters, and the involved density parameter has little influence on clustering results. In our algorithm, the cluster expansion enables our algorithm to work well with clusters of Gaussian distribution, and two involved parameters can be fixed or determined adaptively. Our algorithm goes a step forward in parameter-free clustering for real-world data, and it is shown to perform better than or comparably to some commonly used algorithms with parameters in experiments with synthetic datasets composed of Gaussian clusters and real datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322005428",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "CURE data clustering algorithm",
      "Canopy clustering algorithm",
      "Cluster (spacecraft)",
      "Cluster analysis",
      "Clustering high-dimensional data",
      "Computer science",
      "Correlation clustering",
      "Data mining",
      "Data stream clustering",
      "Determining the number of clusters in a data set",
      "Fuzzy clustering",
      "Gaussian",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Programming language",
      "Quantum mechanics",
      "Single-linkage clustering",
      "k-medians clustering"
    ],
    "authors": [
      {
        "surname": "Hou",
        "given_name": "Jian"
      },
      {
        "surname": "Yuan",
        "given_name": "Huaqiang"
      },
      {
        "surname": "Pelillo",
        "given_name": "Marcello"
      }
    ]
  },
  {
    "title": "Joint Graph Learning and Matching for Semantic Feature Correspondence",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109059",
    "abstract": "In recent years, powered by the learned discriminative representation via graph neural network (GNN) models, deep graph matching methods have made great progresses in the task of matching semantic features. However, these methods usually rely on heuristically generated graph patterns, which may introduce unreliable relationships to hurt the matching performance. In this paper, we propose a joint graph learning and matching network, named GLAM, to explore reliable graph structures for boosting graph matching. GLAM adopts a pure attention-based framework for both graph learning and graph matching. Specifically, it employs two types of attention mechanisms, self-attention and cross-attention for the task. The self-attention discovers the relationships between features to further update feature representations over the learnt structures; and the cross-attention computes cross-graph correlations between the two feature sets to be matched for feature reconstruction. Moreover, the final matching solution is directly derived from the output of the cross-attention layer, without employing a specific matching decision module. The proposed method is evaluated on three popular visual matching benchmarks (Pascal VOC, Willow Object and SPair-71k), and it outperforms previous state-of-the-art graph matching methods on all benchmarks. Furthermore, the graph patterns learnt by our model are validated to be able to remarkably enhance previous deep graph matching methods by replacing their handcrafted graph structures with the learnt ones.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322005398",
    "keywords": [
      "Artificial intelligence",
      "Attention network",
      "Computer science",
      "Discriminative model",
      "Factor-critical graph",
      "Feature learning",
      "Graph",
      "Line graph",
      "Matching (statistics)",
      "Mathematics",
      "Pascal (unit)",
      "Pattern recognition (psychology)",
      "Programming language",
      "Semantic feature",
      "Semantic matching",
      "Statistics",
      "Theoretical computer science",
      "Voltage graph"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "He"
      },
      {
        "surname": "Wang",
        "given_name": "Tao"
      },
      {
        "surname": "Li",
        "given_name": "Yidong"
      },
      {
        "surname": "Lang",
        "given_name": "Congyan"
      },
      {
        "surname": "Jin",
        "given_name": "Yi"
      },
      {
        "surname": "Ling",
        "given_name": "Haibin"
      }
    ]
  },
  {
    "title": "Video representation learning for temporal action detection using global-local attention",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109135",
    "abstract": "Video representation is of significant importance for temporal action detection. The two sub-tasks of temporal action detection, i.e., action classification and action localization, have different requirements for video representation. Specifically, action classification requires video representations to be highly discriminative, so that action features and background features are as dissimilar as possible. For action localization, it is crucial to obtain information about the action itself and the surrounding context for accurate prediction of action boundaries. However, the previous methods failed to extract the optimal representations for the two sub-tasks, whose representations for both sub-tasks are obtained in a similar way. In this paper, a Global-Local Attention (GLA) mechanism is proposed to produce a more powerful video representation for temporal action detection without introducing additional parameters. The global attention mechanism predicts each action category by integrating features in the entire video that are similar to the action while suppressing other features, thus enhancing the discriminability of video representation during the training process. The local attention mechanism uses a Gaussian weighting function to integrate each action and its surrounding contextual information, thereby enabling precise localization of the action. The effectiveness of GLA is demonstrated on THUMOS’14 and ActivityNet-1.3 with a simple one-stage action detection network, achieving state-of-the-art performance among the methods using only RGB images as input. The inference speed of the proposed model reaches 1373 FPS on a single Nvidia Titan Xp GPU. The generalizability of GLA to other detection architectures is verified using R-C3D and Decouple-SSAD, both of which achieve consistent improvements. The experimental results demonstrate that designing representations with different properties for the two sub-tasks leads to better performance for temporal action detection compared to the representations obtained in a similar way.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032200615X",
    "keywords": [
      "Action (physics)",
      "Action recognition",
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Computer vision",
      "Law",
      "Pattern recognition (psychology)",
      "Physics",
      "Political science",
      "Politics",
      "Quantum mechanics",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Tang",
        "given_name": "Yiping"
      },
      {
        "surname": "Zheng",
        "given_name": "Yang"
      },
      {
        "surname": "Wei",
        "given_name": "Chen"
      },
      {
        "surname": "Guo",
        "given_name": "Kaitai"
      },
      {
        "surname": "Hu",
        "given_name": "Haihong"
      },
      {
        "surname": "Liang",
        "given_name": "Jimin"
      }
    ]
  },
  {
    "title": "ScanMix: Learning from Severe Label Noise via Semantic Clustering and Semi-Supervised Learning",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109121",
    "abstract": "We propose a new training algorithm, ScanMix, that explores semantic clustering and semi-supervised learning (SSL) to allow superior robustness to severe label noise and competitive robustness to non-severe label noise problems, in comparison to the state of the art (SOTA) methods. ScanMix is based on the expectation maximisation framework, where the E-step estimates the latent variable to cluster the training images based on their appearance and classification results, and the M-step optimises the SSL classification and learns effective feature representations via semantic clustering. We present a theoretical result that shows the correctness and convergence of ScanMix, and an empirical result that shows that ScanMix has SOTA results on CIFAR-10/-100 (with symmetric, asymmetric and semantic label noise), Red Mini-ImageNet (from the Controlled Noisy Web Labels), Clothing1M and WebVision. In all benchmarks with severe label noise, our results are competitive to the current SOTA.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032200601X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Cluster analysis",
      "Computer science",
      "Correctness",
      "Gene",
      "Image (mathematics)",
      "Machine learning",
      "Noise (video)",
      "Pattern recognition (psychology)",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Sachdeva",
        "given_name": "Ragav"
      },
      {
        "surname": "Cordeiro",
        "given_name": "Filipe Rolim"
      },
      {
        "surname": "Belagiannis",
        "given_name": "Vasileios"
      },
      {
        "surname": "Reid",
        "given_name": "Ian"
      },
      {
        "surname": "Carneiro",
        "given_name": "Gustavo"
      }
    ]
  },
  {
    "title": "Towards open-set text recognition via label-to-prototype learning",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109109",
    "abstract": "Scene text recognition is a popular research topic which is also extensively utilized in the industry. Although many methods have achieved satisfactory performance for the close-set text recognition challenges, these methods lose feasibility in open-set scenarios, where collecting data or retraining models for novel characters could yield a high cost. For example, annotating samples for foreign languages can be expensive, whereas retraining the model each time when a “novel” character is discovered from historical documents costs both time and resources. In this paper, we introduce and formulate a new open-set text recognition task which demands the capability to spot and recognize novel characters without retraining. A label-to-prototype learning framework is also proposed as a baseline for the new task. Specifically, the framework introduces a generalizable label-to-prototype mapping function to build prototypes (class centers) for both seen and unseen classes. An open-set predictor is then utilized to recognize or reject samples according to the prototypes. The implementation of rejection capability over out-of-set characters allows automatic spotting of unknown characters in the incoming data stream. Extensive experiments show that our method achieves promising performance on a variety of zero-shot, close-set, and open-set text recognition datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322005891",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Programming language",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Chang"
      },
      {
        "surname": "Yang",
        "given_name": "Chun"
      },
      {
        "surname": "Qin",
        "given_name": "Hai-Bo"
      },
      {
        "surname": "Zhu",
        "given_name": "Xiaobin"
      },
      {
        "surname": "Liu",
        "given_name": "Cheng-Lin"
      },
      {
        "surname": "Yin",
        "given_name": "Xu-Cheng"
      }
    ]
  },
  {
    "title": "Improving edit-based unsupervised sentence simplification using fine-tuned BERT",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.01.009",
    "abstract": "Word suggestion in unsupervised sentence simplification aims to replace complex words of a given sentence with their simpler alternatives. This is mostly done without considering their context within the input sentence. In this paper, we propose a technique that brings context awareness to word suggestion by merging pre-trained BERT models with a successful edit-based unsupervised sentence simplification model. More importantly, we show that only by fine-tuning the BERT model on simple English corpora, simplification results can be improved and even outperform some of the competing supervised methods. Finally, we introduce a framework that involves filtering an arbitrary amount of unlabeled in-domain text for tuning the model in situations where labeled data, as simple and complex, is scarce. This preprocessing step also speeds up the training process by ignoring fine-tuning on unnecessary samples.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523000168",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Domain (mathematical analysis)",
      "Epistemology",
      "Geometry",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Natural language processing",
      "Operating system",
      "Paleontology",
      "Philosophy",
      "Preprocessor",
      "Process (computing)",
      "Sentence",
      "Simple (philosophy)",
      "Unsupervised learning",
      "Word (group theory)"
    ],
    "authors": [
      {
        "surname": "Rashid",
        "given_name": "Mohammad Amin"
      },
      {
        "surname": "Amirkhani",
        "given_name": "Hossein"
      }
    ]
  },
  {
    "title": "A multimodal hyperlapse method based on video and songs’ emotion alignment",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2022.08.014",
    "abstract": "With the recent growth in the use of social media and new digital devices like smartphones and wearable cameras, people are often recording long first-person videos of their daily activities. These videos are usually very long and tiring to watch, bringing the need to speed them up. Recent fast-forward methods do not consider the background music to be inserted into the video, which could make them even more enjoyable. In this paper, we present a new fast-forward method that considers the information present in the video and the background music. We use neural networks to automatically recognize the emotions induced in the video and song and combine the contents in the accelerated video through a new method of frame selection that has as main objective to maximize the similarity of the induced emotions. We present quantitative and qualitative experiments on a large dataset with different videos and songs, showing that our method achieves the best performance in matching emotion similarity, also keeping the video’s visual quality.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522002537",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Emotion recognition",
      "Multimodality",
      "Speech recognition",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "de Matos",
        "given_name": "Diognei"
      },
      {
        "surname": "Ramos",
        "given_name": "Washington"
      },
      {
        "surname": "Silva",
        "given_name": "Michel"
      },
      {
        "surname": "Romanhol",
        "given_name": "Luiz"
      },
      {
        "surname": "Nascimento",
        "given_name": "Erickson R."
      }
    ]
  },
  {
    "title": "Multi-stage image denoising with the wavelet transform",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109050",
    "abstract": "Deep convolutional neural networks (CNNs) are used for image denoising via automatically mining accurate structure information. However, most of existing CNNs depend on enlarging depth of designed networks to obtain better denoising performance, which may cause training difficulty. In this paper, we propose a multi-stage image denoising CNN with the wavelet transform (MWDCNN) via three stages, i.e., a dynamic convolutional block (DCB), two cascaded wavelet transform and enhancement blocks (WEBs) and a residual block (RB). DCB uses a dynamic convolution to dynamically adjust parameters of several convolutions for making a tradeoff between denoising performance and computational costs. WEB uses a combination of signal processing technique (i.e., wavelet transformation) and discriminative learning to suppress noise for recovering more detailed information in image denoising. To further remove redundant features, RB is used to refine obtained features for improving denoising effects and reconstruct clean images via improved residual dense architectures. Experimental results show that the proposed MWDCNN outperforms some popular denoising methods in terms of quantitative and qualitative analysis. Codes are available at https://github.com/hellloxiaotian/MWDCNN.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322005301",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Block (permutation group theory)",
      "Chemistry",
      "Computer science",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Discriminative model",
      "Gene",
      "Geometry",
      "Image (mathematics)",
      "Image denoising",
      "Mathematics",
      "Multiview Video Coding",
      "Noise (video)",
      "Noise reduction",
      "Non-local means",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Residual",
      "Transformation (genetics)",
      "Video denoising",
      "Video tracking",
      "Wavelet",
      "Wavelet transform"
    ],
    "authors": [
      {
        "surname": "Tian",
        "given_name": "Chunwei"
      },
      {
        "surname": "Zheng",
        "given_name": "Menghua"
      },
      {
        "surname": "Zuo",
        "given_name": "Wangmeng"
      },
      {
        "surname": "Zhang",
        "given_name": "Bob"
      },
      {
        "surname": "Zhang",
        "given_name": "Yanning"
      },
      {
        "surname": "Zhang",
        "given_name": "David"
      }
    ]
  },
  {
    "title": "Adaptive dynamic networks for object detection in aerial images",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2022.12.022",
    "abstract": "In this paper, we propose an entropy-dynamic resolution detection (EDRdet) method for object detection in aerial images. Most conventional object detection methods usually detect each region in aerial images directly with a fixed resolution, so that the resolution of the hard-to-detect region in the whole aerial image is not enough and that of the easy-to-detect region is not necessary. We argue that different resolutions of regions are required for efficient and accurate object detection in aerial images. Our EDRdet dynamically adjusts the resolution of each region via measuring the challenge of the detection process, therefore, our model can achieve a good trade-off between the computational cost and the accuracy effectively. Specifically, our EDRdet searches hard-to-detect regions through low-resolution global context information, and inputs higher-resolution patches to supplement lost feature at low resolution for fine-grained detection. We apply the entropy to determine the regions in aerial images required to be detected at higher resolutions, and where the entropy represents the uncertainty of the detection result. Moreover, we design a patch sampling algorithm to make the selected regions sparse to further improve the efficiency of patch generation. Extensive experiments on the DOTA and Visdrone2019 datasets verify that our EDRdet can reduce the computational cost and improve the model accuracy effectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522003907",
    "keywords": [
      "Aerial image",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Entropy (arrow of time)",
      "Feature (linguistics)",
      "Image (mathematics)",
      "Image resolution",
      "Linguistics",
      "Object detection",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quantum mechanics"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Zhenyu"
      },
      {
        "surname": "Yan",
        "given_name": "Haibin"
      }
    ]
  },
  {
    "title": "Simultaneous Robust Matching Pursuit for Multi-view Learning",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109100",
    "abstract": "Joint sparse representation (JSR) has attracted massive attention with many successful applications in pattern recognition recently. In this paper, we propose a novel robust multi-view JSR method referred to as Simultaneous Robust Matching Pursuit (SRMP) based on the outlier-resistant M-estimator originating from robust statistics. Because of the complexity of the objective function, we design an efficient optimization algorithm to implement SRMP based on the half-quadratic theory. In addition, we have also extended the proposed method for the problems of multi-view subspace clustering and multi-view pattern classification, respectively. The experimental results corroborate the efficacy and robustness of SRMP for multi-view data recovery, subspace clustering and classification.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322005805",
    "keywords": [
      "Artificial intelligence",
      "Compressed sensing",
      "Computer science",
      "Computer vision",
      "Eye movement",
      "Matching (statistics)",
      "Matching pursuit",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Smooth pursuit",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Yulong"
      },
      {
        "surname": "Kou",
        "given_name": "Kit Ian"
      },
      {
        "surname": "Chen",
        "given_name": "Hong"
      },
      {
        "surname": "Tang",
        "given_name": "Yuan Yan"
      },
      {
        "surname": "Li",
        "given_name": "Luoqing"
      }
    ]
  },
  {
    "title": "Introduction to the special section “pattern recognition for Recent and Future Advances On intelligent systems” (SS:ISPR22)",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2022.12.025",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522003932",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Epipolar geometry",
      "Feature (linguistics)",
      "Image (mathematics)",
      "Linguistics",
      "Parallax",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Similarity (geometry)",
      "Texture (cosmology)"
    ],
    "authors": [
      {
        "surname": "Bennour",
        "given_name": "Akram"
      },
      {
        "surname": "Ensarri",
        "given_name": "Tolga"
      },
      {
        "surname": "Salem",
        "given_name": "Abdelbadeeh"
      }
    ]
  },
  {
    "title": "SIT-SR 3D: Self-supervised slice interpolation via transfer learning for 3D volume super-resolution",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.01.008",
    "abstract": "We present SIT-SR 3D, a novel self-supervised method for 3D single image super-resolution (SISR). Scaling 2D SISR networks to 3D SISR requires code redesign, high computing resources, and 3D ground-truth. However, we circumvent this by (1) using a pre-trained 2D SISR for indirect supervision and (2) using a novel consistency loss to learn frame interpolation. Any pre-trained state of the art 2D SISR method can replace the 2D SISR used in SIT-SR 3D, thus transferring the merits of 2D to 3D and ensuring modularity. We trained two end-to-end 3D baselines in a supervised setting; a 3D RRDBNet trained only with L1 loss and a 3D ESRGAN trained with adversarial and perceptual loss. We show that the proposed pipeline's self-supervised version is qualitatively better than the baselines. When trained in a supervised setting, SIT-SR 3D achieves better PSNR than its counterparts. Furthermore, our pipeline uses fewer parameters compared to the baselines. We demonstrate our results on an open-source digital rock CT dataset. Our code and pre-trained models will be made publicly available.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523000144",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Code (set theory)",
      "Computer science",
      "Computer vision",
      "Consistency (knowledge bases)",
      "Genetics",
      "Ground truth",
      "Image (mathematics)",
      "Interpolation (computer graphics)",
      "Modularity (biology)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Pipeline (software)",
      "Programming language",
      "Set (abstract data type)",
      "Source code"
    ],
    "authors": [
      {
        "surname": "Sarmad",
        "given_name": "Muhammad"
      },
      {
        "surname": "Ruspini",
        "given_name": "Leonardo Carlos"
      },
      {
        "surname": "Lindseth",
        "given_name": "Frank"
      }
    ]
  },
  {
    "title": "ML-DSVM+: A meta-learning based deep SVM+ for computer-aided diagnosis",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109076",
    "abstract": "Transfer learning (TL) can improve the performance of a single-modal medical imaging-based computer-aided diagnosis (CAD) by transferring knowledge from related imaging modalities. Support vector machine plus (SVM+) is a supervised TL classifier specially designed for TL between the paired data in the source and target domains with shared labels. In this work, a novel deep neural network (DNN) based SVM+ (DSVM+) algorithm is proposed for single-modal imaging-based CAD. DSVM+ integrates the bi-channel DNNs and SVM+ classifier into a unified framework to improve the performance of both feature representation and classification. In particular, a new coupled hinge loss function is developed to conduct bidirectional TL between the source and target domains, which further promotes knowledge transfer together with the feature representation under the guidance of shared labels. To alleviate the overfitting caused by the increased parameters in DNNs for limited training samples, the meta-learning based DSVM+ (ML-DSVM+) is further developed, which designs randomly selecting samples from the training data instead of other CAD tasks for meta-tasks. This sampling strategy also can avoid the issue of class imbalance. ML-DSVM+ is evaluated on three medical imaging datasets. It achieves the best results of 88.26±1.40%, 90.45±5.00%, and 87.63±5.56% on accuracy, sensitivity and specificity, respectively, on the Bimodal Breast Ultrasound Image dataset, 90.00±1.05%, 72.55±3.87%, and 96.40±2.26% of the corresponding indices on the Alzheimer's Disease Neuroimaging Initiative dataset, and 85.76±3.12% of classification accuracy, 88.73±7.22% of sensitivity, and 82.60±1.56% of specificity for the Autism Brain Imaging Data Exchange dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322005568",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "CAD",
      "Chemistry",
      "Classifier (UML)",
      "Computer science",
      "Engineering",
      "Engineering drawing",
      "Machine learning",
      "Modal",
      "Overfitting",
      "Pattern recognition (psychology)",
      "Polymer chemistry",
      "Support vector machine",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Han",
        "given_name": "Xiangmin"
      },
      {
        "surname": "Wang",
        "given_name": "Jun"
      },
      {
        "surname": "Ying",
        "given_name": "Shihui"
      },
      {
        "surname": "Shi",
        "given_name": "Jun"
      },
      {
        "surname": "Shen",
        "given_name": "Dinggang"
      }
    ]
  },
  {
    "title": "Thermal to Visual Person Re-Identification Using Collaborative Metric Learning Based on Maximum Margin Matrix Factorization",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109069",
    "abstract": "Thermal to visual person re-identification (T2V-ReID) is a cross-domain image retrieval problem. In this problem, the matching of a person’s image takes place, where the image is taken by different cameras (thermal and visual) at different times. This problem has numerous applications in night-time security surveillance. It is challenging due to the large intra-class variations and cross-domain discrepancies. Recently, deep metric learning methods are proposed for this problem. Still, there is a scope to improve the metric learning by generalizing the metric. In this paper, we have proposed the collaborative metric learning using Maximum Margin Matrix Factorization. It uses the group-wise similarities and collaboratively predicts the similarities. We can learn a more generalized metric by utilizing the maximized margin in this method. The proposed method is tested on the RegDB and RGB-D-T data sets, and the method outperforms the existing works in the few-shot learning settings.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322005490",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Computer vision",
      "Eigenvalues and eigenvectors",
      "Engineering",
      "Identification (biology)",
      "Machine learning",
      "Margin (machine learning)",
      "Mathematics",
      "Matrix (chemical analysis)",
      "Matrix decomposition",
      "Metric (unit)",
      "Operations management",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics"
    ],
    "authors": [
      {
        "surname": "Gavini",
        "given_name": "Yaswanth"
      },
      {
        "surname": "Agarwal",
        "given_name": "Arun"
      },
      {
        "surname": "Mehtre",
        "given_name": "B.M."
      }
    ]
  },
  {
    "title": "Reflections of an ancient document processor",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.01.006",
    "abstract": "The bulk of the documents that affect our lives are digital or born digital. Our laborious investigations of layout, script, font and graphics, are turning into mere exercises with little influence on pursuits outside the Document Analysis and Recognition (DAR) community. Recent performance improvements on such tasks, even if based on deep learning and AI, are as much the result of advances in computer hardware as of breakthroughs in document research. It is time to automate tasks beyond transcription. This Commentary addresses our mission, our approach to some technical issues, and the role of AI in DAR. Opportunities for a wider role for document analysis include more pervasive application of statistical decision theory, integrated genre analysis, summarization, interpretation and information extraction, bolder goals in content analysis, and alternative modalities, induced by the open source movement, for sharing research results. Importantly, expanding the scope of our research incurs increased responsibility for retaining human prerogatives in critical decision making and preserving essential human skills like good writing and discriminative reading.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523000065",
    "keywords": [
      "Artificial intelligence",
      "Automatic summarization",
      "Computer graphics (images)",
      "Computer science",
      "Data science",
      "Discriminative model",
      "Graphics",
      "Human–computer interaction",
      "Information retrieval",
      "Linguistics",
      "Modalities",
      "Philosophy",
      "Programming language",
      "Reading (process)",
      "Scope (computer science)",
      "Social science",
      "Sociology",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Nagy",
        "given_name": "George"
      }
    ]
  },
  {
    "title": "CNN‐Transformer for visual‐tactile fusion applied in road recognition of autonomous vehicles",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2022.11.023",
    "abstract": "Reliable autonomous driving requires comprehensive environment perception, among which the road recognition is critical for autonomous vehicles to achieve adaptability, reliability, and safety. Existing equipped sensors such as cameras, LiDAR, and accelerometers have been adopted widely for road recognition. However, single sensor based recognition methods present challenges in balancing high accuracy and adaptability. In this study, we propose a visual-tactile fusion road recognition system for autonomous vehicles. The visual modality is derived from the captured road images, and the tactile modality information comes from the designed intelligent tire system, containing a low-cost piezoelectric sensor. For accurate road recognition, we propose a multimodal fusion recognition network based on the CNN-transformer architecture. The visual and tactile modalities are fed into modality-specific SE-CNNs, which emphasize valuable input information to obtain weighted features. These features are subsequently input to \"bottleneck\" based fusion transformer encoders and output the recognition results. We design a fusion feature extractor to enhance the fusion representation capability and improve accuracy. The vehicle field experiments are conducted to build the dataset consisting of four road surfaces, and the results show that the proposed network achieves an accuracy of 99.48% in road recognition task.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552200352X",
    "keywords": [
      "Adaptability",
      "Artificial intelligence",
      "Biology",
      "Bottleneck",
      "Computer science",
      "Computer vision",
      "Ecology",
      "Electrical engineering",
      "Embedded system",
      "Engineering",
      "Modality (human–computer interaction)",
      "Pattern recognition (psychology)",
      "Sensor fusion",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Shi",
        "given_name": "Runwu"
      },
      {
        "surname": "Yang",
        "given_name": "Shichun"
      },
      {
        "surname": "Chen",
        "given_name": "Yuyi"
      },
      {
        "surname": "Wang",
        "given_name": "Rui"
      },
      {
        "surname": "Zhang",
        "given_name": "Mengyue"
      },
      {
        "surname": "Lu",
        "given_name": "Jiayi"
      },
      {
        "surname": "Cao",
        "given_name": "Yaoguang"
      }
    ]
  },
  {
    "title": "Unsupervised learning of global factors in deep generative models",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109130",
    "abstract": "We present a novel deep generative model based on non i.i.d. variational autoencoders that captures global dependencies among observations in a fully unsupervised fashion. In contrast to the recent semi-supervised alternatives for global modeling in deep generative models, our approach combines a mixture model in the local or data-dependent space and a global Gaussian latent variable, which lead us to obtain three particular insights. First, the induced latent global space captures interpretable disentangled representations with no user-defined regularization in the evidence lower bound (as in β -VAE and its generalizations). Second, we show that the model performs domain alignment to find correlations and interpolate between different databases. Finally, we study the ability of the global space to discriminate between groups of observations with non-trivial underlying structures, such as face images with shared attributes or defined sequences of digits images.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322006100",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Contrast (vision)",
      "Deep learning",
      "Domain (mathematical analysis)",
      "Gaussian",
      "Generative grammar",
      "Generative model",
      "Latent variable",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Mixture model",
      "Operating system",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Regularization (linguistics)",
      "Space (punctuation)"
    ],
    "authors": [
      {
        "surname": "Peis",
        "given_name": "Ignacio"
      },
      {
        "surname": "Olmos",
        "given_name": "Pablo M."
      },
      {
        "surname": "Artés-Rodríguez",
        "given_name": "Antonio"
      }
    ]
  },
  {
    "title": "Video Object Segmentation using Point-based Memory Network",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109073",
    "abstract": "Recent years have witnessed the prevalence of memory-based methods for Semi-supervised Video Object Segmentation (SVOS) which utilise past frames efficiently for label propagation. When conducting feature matching, fine-grained multi-scale feature matching has typically been performed using all query points, which inevitably results in redundant computations and thus makes the fusion of multi-scale results ineffective. In this paper, we develop a new Point-based Memory Network, termed as PMNet, to perform fine-grained feature matching on hard samples only, assuming that easy samples can already obtain satisfactory matching results without the need for complicated multi-scale feature matching. Our approach first generates an uncertainty map from the initial decoding outputs. Next, the fine-grained features at uncertain locations are sampled to match the memory features on the same scale. Finally, the matching results are further decoded to provide a refined output. The point-based scheme works with the coarsest feature matching in a complementary and efficient manner. Furthermore, we propose an approach to adaptively perform global or regional matching based on the motion history of memory points, making our method more robust against ambiguous backgrounds. Experimental results on several benchmark datasets demonstrate the superiority of our proposed method over state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322005532",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Computer vision",
      "Decoding methods",
      "Feature (linguistics)",
      "Geodesy",
      "Geography",
      "Geometry",
      "Linguistics",
      "Matching (statistics)",
      "Mathematics",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Point (geometry)",
      "Quantum mechanics",
      "Scale (ratio)",
      "Segmentation",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Gao",
        "given_name": "Mingqi"
      },
      {
        "surname": "Han",
        "given_name": "Jungong"
      },
      {
        "surname": "Zheng",
        "given_name": "Feng"
      },
      {
        "surname": "Yu",
        "given_name": "James J.Q."
      },
      {
        "surname": "Montana",
        "given_name": "Giovanni"
      }
    ]
  },
  {
    "title": "Fuzzy Superpixel-based Image Segmentation",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109045",
    "abstract": "This article presents a multi-phase image segmentation methodology based on fuzzy superpixel decomposition, aggregation and merging. First, a collection of layers of dense fuzzy superpixels is generated by the variational fuzzy decomposition algorithm. Then a layer of refined superpixels is extracted by aggregating various layers of dense fuzzy superpixels using the hierarchical normalized cuts. Finally, the refined superpixels are projected into the low dimensional feature spaces by the multidimensional scaling and the segmentation result is obtained via the mean-shift-based merging approach with the spatial bandwidth adjustment strategy. Our algorithm utilizes the superimposition of fuzzy superpixels to impose more accurate spatial constraints on the final segmentation through the fuzzy superpixel aggregation. The fuzziness of superpixels also provides spatial features to measure affinities between fuzzy superpixels and refined superpixels, and guide the merging process. Comparative experiments with the existing approaches reveal a superior performance of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322005258",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Feature (linguistics)",
      "Fuzzy logic",
      "Image segmentation",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Ng",
        "given_name": "Tsz Ching"
      },
      {
        "surname": "Choy",
        "given_name": "Siu Kai"
      },
      {
        "surname": "Lam",
        "given_name": "Shu Yan"
      },
      {
        "surname": "Yu",
        "given_name": "Kwok Wai"
      }
    ]
  },
  {
    "title": "UDNet: Uncertainty-aware deep network for salient object detection",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109099",
    "abstract": "Most of the existing deep learning based salient object detection (SOD) models adopt multi-level feature fusion strategies, and have achieved remarkable progress. However, current SOD models still suffer from the uncertainty dilemma in predicting salient probabilities of the pixels surrounding the contour of salient objects. To solve this issue, we propose a novel uncertainty-aware SOD model, where multiple supervision signals, i.e., internal contour uncertainty map, saliency map and external contour uncertainty map, are used to guide the network to not only focus on the pixels in the salient object but also shift its partial attention to the pixels surrounding the contour of salient objects. Furthermore, we introduce a new feature interaction module to aggregate internal contour uncertainty features, saliency features and external contour uncertainty features in the decoding stage, aiming to enhance the model’s ability in dealing with the “uncertain” pixels. Extensive experiments on four public benchmark datasets demonstrate the superiority of the proposed method over the existing state-of-the-art SOD methods. Furthermore, the proposed method shows better attribute-based performance on the SOC dataset, suggesting that the proposed model can also handle challenging scenarios in SOD.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322005799",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Salient"
    ],
    "authors": [
      {
        "surname": "Fang",
        "given_name": "Yuming"
      },
      {
        "surname": "Zhang",
        "given_name": "Haiyan"
      },
      {
        "surname": "Yan",
        "given_name": "Jiebin"
      },
      {
        "surname": "Jiang",
        "given_name": "Wenhui"
      },
      {
        "surname": "Liu",
        "given_name": "Yang"
      }
    ]
  },
  {
    "title": "Deep graph clustering with multi-level subspace fusion",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109077",
    "abstract": "Attributed graph clustering combines both node attributes and graph structure information of data samples and has demonstrated satisfactory performance in various applications. However, how to choose the proper neighborhood for attributed graph clustering remains to be a challenge. A larger neighborhood may cause over-smoothed representations with less discrimination for clustering while the short-range ignore distant nodes and fails to capture the global information. In this paper, we propose a novel deep attributed graph clustering network with a multi-level subspace fusion module to address this issue. The first contribution of our work is to insert multiple self-expressive modules between low-level and high-level layers to promote more favorable features for clustering. The constraint of shared self-expressive matrix facilitates to preserve intrinsic structure without pre-defined neighborhoods as the previous methods do. Moreover, we introduce a novel loss function that leverages traditional reconstruction and the proposed structure fusion loss to effectively preserve multi-level clustering structures with both global and local discriminative features. Extensive experiments on public benchmark datasets validate the effectiveness of our proposed model compared with the state-of-the-art attribute graph clustering competitors by considerable margins.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032200557X",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Discriminative model",
      "Graph",
      "Pattern recognition (psychology)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Wang"
      },
      {
        "surname": "Wang",
        "given_name": "Siwei"
      },
      {
        "surname": "Guo",
        "given_name": "Xifeng"
      },
      {
        "surname": "Zhu",
        "given_name": "En"
      }
    ]
  },
  {
    "title": "An analysis of ConformalLayers' robustness to corruptions in natural images",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2022.11.002",
    "abstract": "ConformalLayers are sequential Convolutional Neural Networks (CNNs) that use activation functions defined as geometric operations in the conformal model for Euclidean geometry. Such construction turns the layers of sequential CNNs associative, leading to a significant reduction in the use of computing resources at inference time. After the layer’s association, both the processing time and memory used per batch entry become independent (i.e., constant) of the network’s depth. They depend only on the size of the input and output. The cost of conventional sequential CNNs, on the other hand, presents linear growth. This paper evaluates the robustness of the classification of ConformalLayers-based CNNs against different kinds of corruptions typically found in natural images. Our results show that the mean top-1 error rates of vanilla CNNs are smaller than ConformalLayer-based CNNs on clean images. Still, our approach outperforms other optimization techniques based on network quantization, and the relative difference to vanilla networks tends to reduce in the presence of image corruptions. Furthermore, we show that processing time and CO2 emission rates are much lower for ConformalLayer-based CNNs with a depth greater than seven and two, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522003324",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Gene",
      "Geology",
      "Mathematics",
      "Natural (archaeology)",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Sousa",
        "given_name": "Eduardo Vera"
      },
      {
        "surname": "Vasconcelos",
        "given_name": "Cristina Nader"
      },
      {
        "surname": "Fernandes",
        "given_name": "Leandro A.F."
      }
    ]
  },
  {
    "title": "Weighted distances in the Cairo pattern",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.01.004",
    "abstract": "The Cairo Pattern is an interesting tiling of the plane. It consists of pentagons which completely cover the plane without overlapping. The Cairo Pattern is the dual of a semi-regular grid. It is assumed that one can walk on the pentagons such that it is possible to step only two neighboring pentagons. Different types of steps have different weights. The distance of two pentagons is the length of the minimal path between them. All distances are based on a linear programming model. The main tool of the analysis is the simplex method of linear programming. The analysis also uses the special properties of the model of minimal path. All minimal paths are given explicitly. Therefore, the complexity of our final result is O ( 1 ) inspite of the fact that the general simplex method is thought to be exponential.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523000028",
    "keywords": [
      "Algorithm",
      "Combinatorics",
      "Computer science",
      "Cover (algebra)",
      "Engineering",
      "Exponential function",
      "Geometry",
      "Grid",
      "Linear programming",
      "Mathematical analysis",
      "Mathematics",
      "Mechanical engineering",
      "Path (computing)",
      "Plane (geometry)",
      "Programming language",
      "Simplex",
      "Simplex algorithm"
    ],
    "authors": [
      {
        "surname": "Turgay",
        "given_name": "Neşet Deniz"
      },
      {
        "surname": "Nagy",
        "given_name": "Benedek"
      },
      {
        "surname": "Kovács",
        "given_name": "Gergely"
      },
      {
        "surname": "Vizvári",
        "given_name": "Béla"
      }
    ]
  },
  {
    "title": "Wavelet detail perception network for single image super-resolution",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2022.12.021",
    "abstract": "Single image super-resolution (SR) is an important topic in computer vision because of its ability to generate high-resolution (HR) images. Traditional SR methods do not pay attention to high-frequency detail perception in the reconstruction process, resulting in unrealistic high-frequency details of images. To address the problem of over-smoothing of details, a novel wavelet detail perception network (WDPNet) is proposed in this study. Different from traditional SR methods that directly restore high-resolution images, the proposed WDPNet decomposes images into low-frequency and high-frequency sub-images by wavelet transform and then uses different models to train these sub-images. Moreover, low-frequency structures are also provided to the high-frequency model to further recover and enhance high-frequency details through the proposed low-to-high information delivery (L2HID) and detail perception enhancement (DPE) mechanisms. Finally, the low-frequency and high-frequency models are fused and weighted to different degrees to enhance image details further. Compared with the state-of-the-art methods, the experimental results show that the proposed WDPNet achieves better performance and visual results in image detail perception.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522003890",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer vision",
      "Geography",
      "High resolution",
      "Image (mathematics)",
      "Neuroscience",
      "Operating system",
      "Pattern recognition (psychology)",
      "Perception",
      "Process (computing)",
      "Remote sensing",
      "Smoothing",
      "Wavelet",
      "Wavelet transform"
    ],
    "authors": [
      {
        "surname": "Hsu",
        "given_name": "Wei-Yen"
      },
      {
        "surname": "Jian",
        "given_name": "Pei-Wen"
      }
    ]
  },
  {
    "title": "Learning comprehensive global features in person re-identification: Ensuring discriminativeness of more local regions",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109068",
    "abstract": "Person re-identification (Re-ID) aims to retrieve person images from a large gallery given a query image of a person of interest. Global information and fine-grained local features are both essential for the representation. However, global embedding learned by naive classification model tends to be trapped in the most discriminative local region, leading to poor evaluation performance. To address the issue, we propose a novel baseline network that learns strong global feature termed as Comprehensive Global Embedding (CGE), ensuring more local regions of global feature maps to be discriminative. In this work, two key modules are proposed including Non-parameterized Local Classifier (NLC) and Global Logits Revise (GLR). The NLC is designed to obtain a score vector of each local region on feature maps in a non-parametric manner. The GLR module directly revises the logits such that the subsequent cross entropy loss up-weights the loss assigned to samples with hard-to-learn local regions. The convergence of the deep model indicates more local regions (the number of local regions is manually defined) on the feature maps of each sample are discriminative. We implement these two modules on two strong baseline methods including the BagTricks (BOT) [1] and AGW [2]. The network achieves 65.9% mAP, 85.1% rank1 on MSMT17, 86.4% mAP, 87.4% rank1 on CUHK03 labeled, 84.2% mAP, 85.9% rank1 on CUHK03 detected, and 92.2% mAP, 96.3% rank1 on Market-1501. The results show that the proposed baseline achieves a new state-of-the-art when using only global embedding during inference without any re-ranking technique.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322005489",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Computer science",
      "Identification (biology)",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Xi",
        "given_name": "Jiali"
      },
      {
        "surname": "Huang",
        "given_name": "Jianqiang"
      },
      {
        "surname": "Zheng",
        "given_name": "Shibao"
      },
      {
        "surname": "Zhou",
        "given_name": "Qin"
      },
      {
        "surname": "Schiele",
        "given_name": "Bernt"
      },
      {
        "surname": "Hua",
        "given_name": "Xian-Sheng"
      },
      {
        "surname": "Sun",
        "given_name": "Qianru"
      }
    ]
  },
  {
    "title": "Regularizing autoencoders with wavelet transform for sequence anomaly detection",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109084",
    "abstract": "Nowadays, systems or entities are usually monitored by devices, generating large amounts of time series. Detecting anomalies in them help prevent potential losses, thus arousing much research interest. Existing studies have adopted autoencoders to detect anomalies, where reconstruction errors are used to indicate outliers. However, sometimes autoencoders may also reconstruct anomalies well due to the learned general features in latent spaces. To solve the above problem, we propose to regularize autoencoders to grasp specific features of normal sequences. Specifically, spectral unique patterns are captured by statistical analysis on discrete wavelet transform (DWT) coefficients of input sequences, restricting latent spaces to reflect unique patterns of normal sequences in both time and frequency domains. Furthermore, a Weight Controller calculating sample-adaptive regularization weights is designed to fully utilize the regularization effect. Extensive experiments on three public benchmarks demonstrate the effectiveness and superiority of the proposed model compared with state-of-the-art algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322005647",
    "keywords": [
      "Algorithm",
      "Anomaly detection",
      "Artificial intelligence",
      "Computer science",
      "Discrete wavelet transform",
      "GRASP",
      "Outlier",
      "Pattern recognition (psychology)",
      "Programming language",
      "Regularization (linguistics)",
      "Wavelet",
      "Wavelet transform"
    ],
    "authors": [
      {
        "surname": "Yao",
        "given_name": "Yueyue"
      },
      {
        "surname": "Ma",
        "given_name": "Jianghong"
      },
      {
        "surname": "Ye",
        "given_name": "Yunming"
      }
    ]
  },
  {
    "title": "Zero-shot ear cross-dataset transfer for person recognition on mobile devices",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.01.012",
    "abstract": "Smartphones contain personal and private data to be protected, such as everyday communications or bank accounts. Several biometric techniques have been developed to unlock smartphones, among which ear biometrics represents a natural and promising opportunity even though the ear can be used in other biometric and multi-biometric applications. A problem in generalizing research results to real-world applications is that the available ear datasets present different characteristics and some bias. This paper stems from a study about the effect of mixing multiple datasets during the training of an ear recognition system. The main contribution is the evaluation of a robust pipeline that learns to combine data from different sources and highlights the importance of pre-training encoders on auxiliary tasks. The reported experiments exploit eight diverse training datasets to demonstrate the generalization capabilities of the proposed approach. Performance evaluation includes testing with collections not seen during training and assessing zero-shot cross-dataset transfer. The results confirm that mixing different sources provides an insightful perspective on the datasets and competitive results with some existing benchmarks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523000181",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Biometrics",
      "Botany",
      "Computer science",
      "Computer security",
      "Data mining",
      "Exploit",
      "Generalization",
      "Identification (biology)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Parallel computing",
      "Pattern recognition (psychology)",
      "Perspective (graphical)",
      "Pipeline (software)",
      "Programming language",
      "Speech recognition",
      "Transfer (computing)",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Freire-Obregón",
        "given_name": "David"
      },
      {
        "surname": "De Marsico",
        "given_name": "Maria"
      },
      {
        "surname": "Barra",
        "given_name": "Paola"
      },
      {
        "surname": "Lorenzo-Navarro",
        "given_name": "Javier"
      },
      {
        "surname": "Castrillón-Santana",
        "given_name": "Modesto"
      }
    ]
  },
  {
    "title": "A novel dual-channel graph convolutional neural network for facial action unit recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.01.001",
    "abstract": "Facial Action Unit (AU) recognition is a challenging problem, where the subtle muscle movement brings diverse AU representations. Recently, AU relations are utilized to assist AU recognition and improve the understanding of AUs. Nevertheless, simply adopting the regular Bayesian networks or the relations between AUs and emotions is not enough for modelling complex AU relations. To provide quantitative measurement of AU relations using the knowledge from FACS, we propose an AU relation quantization autoencoder. Moreover, to cope with the diversity of AUs generated from the individual representation differences and other environmental impacts, we propose a dual-channel graph convolutional neural network (DGCN) obtaining both inherent and random AU relations. The first channel is FACS-based relation graph convolution channel (FACS-GCN) embedding prior knowledge of FACS, and it adjusts the network to the inherent AU dependent relations. The second channel is data-learning-based relation graph convolution channel (DLR-GCN) based on metric learning, and it provides robustness for individual differences and environmental changes. Comprehensive experiments have been conducted on three public datasets: CK+, RAF-AU and DISFA. The experimental results demonstrate that our proposed DGCN can extract the hidden relations well, thereby achieving great performance in AU recognition.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523000016",
    "keywords": [
      "Action recognition",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Class (philosophy)",
      "Computer science",
      "Convolutional neural network",
      "Embedding",
      "Gene",
      "Graph",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Robustness (evolution)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Jia",
        "given_name": "Xibin"
      },
      {
        "surname": "Xu",
        "given_name": "Shaowu"
      },
      {
        "surname": "Zhou",
        "given_name": "Yuhan"
      },
      {
        "surname": "Wang",
        "given_name": "Luo"
      },
      {
        "surname": "Li",
        "given_name": "Weiting"
      }
    ]
  },
  {
    "title": "Approximating global illumination with ambient occlusion and environment light via generative adversarial networks",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2022.12.007",
    "abstract": "Calculating global illumination in computer graphics is difficult, especially for complex scenes. This is due to the interreflections of the rays of light and the interactions with the materials of the objects composing the scene. Solutions based on ambient light approximations have been implemented. However, these are computationally intensive and produce images with less precision, as these solutions ignore the ambient lighting component by adopting coarse approximations. In this paper, we propose a method capable of approximating the global illumination effect. Our idea is to compute the global illumination by adding three images (direct illumination, environmental light, and ambient occlusion). Direct illumination is calculated by a reference method. Environmental illumination is computed using an adversarial neural network from a single 2D image. Ambient occlusion is generated using conditional adversarial neural networks with an attention mechanism to pay more attention to the relevant image features during the training step. We use two image masks to keep the object’s position in the screen space, which allows efficient reconstruction of the final result. Our solution produces quality images compared to reference images and does not require any computation in the 3D scene or screen space.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522003750",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Component (thermodynamics)",
      "Computation",
      "Computer graphics",
      "Computer graphics (images)",
      "Computer science",
      "Computer vision",
      "Economics",
      "Finance",
      "Generative adversarial network",
      "Global illumination",
      "Graphics",
      "Image (mathematics)",
      "Object (grammar)",
      "Physics",
      "Position (finance)",
      "Rendering (computer graphics)",
      "Thermodynamics"
    ],
    "authors": [
      {
        "surname": "Abbas",
        "given_name": "Fayçal"
      },
      {
        "surname": "Malah",
        "given_name": "Mehdi"
      },
      {
        "surname": "Babahenini",
        "given_name": "Mohamed Chaouki"
      }
    ]
  },
  {
    "title": "Tripartite sub-image histogram equalization for slightly low contrast gray-tone image enhancement",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109043",
    "abstract": "In this paper, a neoteric tripartite sub-image histogram equalization method is proposed to enhance slightly low contrast gray-tone images, which is a less explored area in the literature. An image is decomposed into three sub-images to preserve its mean brightness, and the histograms of the sub-images are calculated. Then, the snipping procedure is applied to each histogram to constrain the pace of contrast enhancement. Subsequently, the equalization of the three histograms is performed independently, and finally, the three equalized sub-images are composed into a single image. The proposed method offers better outcomes as compared to several common and state-of-the-art histogram equalization-based methods regarding contrast improvement, blind/reference-less image spatial quality evaluator, mean brightness preservation, peak signal-to-noise ratio, mean structural similarity, gradient magnitude similarity deviation, feature similarity, bit-plane to bit-plane similarity, and visual image quality.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322005234",
    "keywords": [
      "Adaptive histogram equalization",
      "Artificial intelligence",
      "Balanced histogram thresholding",
      "Brightness",
      "Color image",
      "Color normalization",
      "Computer science",
      "Computer vision",
      "Contrast (vision)",
      "Feature (linguistics)",
      "Histogram",
      "Histogram equalization",
      "Histogram matching",
      "Image (mathematics)",
      "Image histogram",
      "Image processing",
      "Image quality",
      "Image texture",
      "Linguistics",
      "Mathematics",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Rahman",
        "given_name": "Hafijur"
      },
      {
        "surname": "Paul",
        "given_name": "Gour Chandra"
      }
    ]
  },
  {
    "title": "Egocentric video co-summarization using transfer learning and refined random walk on a constrained graph",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109128",
    "abstract": "In this paper, we address the problem of egocentric video co-summarization. We show how a shot level accurate summary can be obtained in a time-efficient manner using random walk on a constrained graph in transfer learned feature space with label refinement. While applying transfer learning, we propose a new loss function capturing egocentric characteristics in a pre-trained ResNet on the set of auxiliary egocentric videos. Transfer learning is used to generate i) an improved feature space and ii) a set of labels to be used as seeds for the test egocentric video. A complete weighted graph is created for a test video in the new transfer learned feature space with shots as the vertices. We derive two types of cluster label constraints in form of Must-Link (ML) and Cannot-link (CL) based on the similarity of the shots. ML constraints are used to prune the complete graph which is shown to result in substantial computational advantage, especially, for the long duration videos. We derive expressions for the number of vertices and edges for the ML-constrained graph and show that this graph remains connected. Random walk is applied to obtain labels of the unmarked shots in this new graph. CL constraints are applied to refine the cluster labels. Finally, shots closest to individual cluster centres are used to build the summary. Experiments on the short duration videos as in CoSum and TVSum datasets and long duration videos as in ADL and EPIC-Kitchens datasets clearly demonstrate the advantage of our solution over several state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322006082",
    "keywords": [
      "Artificial intelligence",
      "Automatic summarization",
      "Computer science",
      "Computer vision",
      "Graph",
      "Mathematics",
      "Parallel computing",
      "Pattern recognition (psychology)",
      "Random walk",
      "Statistics",
      "Theoretical computer science",
      "Transfer (computing)",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Sahu",
        "given_name": "Abhimanyu"
      },
      {
        "surname": "Chowdhury",
        "given_name": "Ananda S."
      }
    ]
  },
  {
    "title": "Detecting group concept drift from multiple data streams",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109113",
    "abstract": "Concept drift may lead to a sharp downturn in the performance of streaming in data-based algorithms, caused by unforeseeable changes in the underlying distribution of data. In this paper, we are mainly concerned with concept drift across multiple data streams, and in situations where the drift of each data stream cannot be detected in time, due to slight underlying distribution drifts. We call this group concept drift. When compared to the detection of concept drift for a single data stream, the challenges of detecting group concept drift arise from three aspects: first, the training data become more complex; second, the underlying distribution becomes more complex; and third, the correlations between data streams become more complex. To address these challenges, the key idea of our method is to construct a distribution free test statistic, free from any underlying distribution in multiple data streams. Then, for streaming data, we design an online learning algorithm to obtain this test statistic, thereby determining the concept drift caused by the hypothesis test. The experiment evaluations with both synthetic and real-world datasets prove that our method can accurately detect concept drift from multiple data streams.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322005933",
    "keywords": [
      "Artificial intelligence",
      "Computer network",
      "Computer science",
      "Computer security",
      "Concept drift",
      "Construct (python library)",
      "Data mining",
      "Data stream",
      "Data stream mining",
      "Key (lock)",
      "Mathematics",
      "Programming language",
      "STREAMS",
      "Statistic",
      "Statistical hypothesis testing",
      "Statistics",
      "Streaming data",
      "Telecommunications",
      "Test statistic"
    ],
    "authors": [
      {
        "surname": "Yu",
        "given_name": "Hang"
      },
      {
        "surname": "Liu",
        "given_name": "Weixu"
      },
      {
        "surname": "Lu",
        "given_name": "Jie"
      },
      {
        "surname": "Wen",
        "given_name": "Yimin"
      },
      {
        "surname": "Luo",
        "given_name": "Xiangfeng"
      },
      {
        "surname": "Zhang",
        "given_name": "Guangquan"
      }
    ]
  },
  {
    "title": "Specialized re-ranking: A novel retrieval-verification framework for cloth changing person re-identification",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109070",
    "abstract": "Cloth changing person re-identification(Re-ID) can work under more complicated scenarios with higher security than normal Re-ID and biometric techniques and is therefore extremely valuable in applications. Meanwhile, the wide range of appearance flexibility results in more similar-looking, confusing images, which is the weakness of the widely used retrieval methods. In this work, we shed light on how to handle these similar images. Specifically, we propose a novel retrieval-verification framework. Given an image, the retrieval module will search for a shot list of similar images quickly. Our proposed verification network will then compare the probe image with these candidate images by contrasting local details for their similarity scores. An innovative ranking strategy is also introduced to achieve a good balance between retrieval and verification results. Comprehensive experiments are conducted to show the effectiveness of our framework and its capability in improving the state-of-the-art methods remarkably on both synthetic and realistic datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322005507",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Computer science",
      "Data mining",
      "Identification (biology)",
      "Information retrieval",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Ranking (information retrieval)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Renjie"
      },
      {
        "surname": "Fang",
        "given_name": "Yu"
      },
      {
        "surname": "Song",
        "given_name": "Huaxin"
      },
      {
        "surname": "Wan",
        "given_name": "Fangbin"
      },
      {
        "surname": "Fu",
        "given_name": "Yanwei"
      },
      {
        "surname": "Kato",
        "given_name": "Hirokazu"
      },
      {
        "surname": "Wu",
        "given_name": "Yang"
      }
    ]
  },
  {
    "title": "Semi-supervised adaptive kernel concept factorization",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109114",
    "abstract": "Kernelized concept factorization (KCF) has shown its advantage on handling data with nonlinear structures; however, the kernels involved in the existing KCF-based methods are empirically predefined, which may compromise the performance. In this paper, we propose semi-supervised adaptive kernel concept factorization (SAKCF), which integrates the data representation and kernel learning into a unified model to make the two learning processes adapt to each other. SAKCF extends traditional KCF in a semi-supervised manner, which encourages the high-dimensional representation to be consistent with both the limited supervisory and local geometric information. Besides, an alternating iterative algorithm is proposed to solve the resulting constrained optimization problem. Experimental results on six real-world data sets verify the effectiveness and advantages of our SAKCF over state-of-the-art methods when applied on the clustering task.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322005945",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Factorization",
      "Kernel (algebra)",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Wenhui"
      },
      {
        "surname": "Hou",
        "given_name": "Junhui"
      },
      {
        "surname": "Wang",
        "given_name": "Shiqi"
      },
      {
        "surname": "Kwong",
        "given_name": "Sam"
      },
      {
        "surname": "Zhou",
        "given_name": "Yu"
      }
    ]
  },
  {
    "title": "Robust learning from noisy web data for fine-Grained recognition",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109063",
    "abstract": "Due to DNNs’ memorization effect, label noise lessens the performance of the web-supervised fine-grained visual categorization task. Previous literature primarily relies on small-loss instances for subsequent training. The current state-of-the-art approach JoCoR additionally employs explicit consistency constraints to make clean samples more confident. However, a joint loss designed for both sample selection criteria and parameter updating is not competent for training a robust model in the presence of web noise. Especially, false positives are assigned with larger weights, causing the model to pay more attention to misclassified noisy images. Besides, leveraging weight decay to forget discarded noisy instances is too slow and implicit to take effect. Therefore, we propose a simple yet effective approach named MS-DeJOR (Multi-Scale training with Decoupled Joint Optimization and Refurbishment). In contrast to JoCoR, we decouple sample selection from training procedure to handle the above problems. Specifically, a negative entropy term is applied to prevent false positives from being overemphasized. The model can explicitly forget those samples identified as noise by imposing such a regularization term on all training data. Furthermore, we use accumulated predictions to refurbish the noisy labels and re-weight training images to boost the model performance. A multi-scale feature enhancement module is adopted to extract discriminative and subtle feature representations. Extensive experiments show that MS-DeJOR yields state-of-the-art performances on three web-supervised fine-grained datasets, demonstrating the effectiveness of our approach. The data and source code have been available at https://github.com/msdejor/MS-DeJOR.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032200543X",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Discriminative model",
      "False positive paradox",
      "Feature selection",
      "Gene",
      "Image (mathematics)",
      "Machine learning",
      "Noise (video)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Regularization (linguistics)",
      "Robustness (evolution)",
      "Source code"
    ],
    "authors": [
      {
        "surname": "Cai",
        "given_name": "Zhenhuang"
      },
      {
        "surname": "Xie",
        "given_name": "Guo-Sen"
      },
      {
        "surname": "Huang",
        "given_name": "Xingguo"
      },
      {
        "surname": "Huang",
        "given_name": "Dan"
      },
      {
        "surname": "Yao",
        "given_name": "Yazhou"
      },
      {
        "surname": "Tang",
        "given_name": "Zhenmin"
      }
    ]
  },
  {
    "title": "End-to-end kernel learning via generative random Fourier features",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109057",
    "abstract": "Random Fourier features (RFFs) provide a promising way for kernel learning in a spectral case. Current RFFs-based kernel learning methods usually work in a two-stage way. In the first-stage process, learning an optimal feature map is often formulated as a target alignment problem, which aims to align the learned kernel with a pre-defined target kernel (usually the ideal kernel). In the second-stage process, a linear learner is conducted with respect to the mapped random features. Nevertheless, the pre-defined kernel in target alignment is not necessarily optimal for the generalization of the linear learner. Instead, in this paper, we consider a one-stage process that incorporates the kernel learning and linear learner into a unifying framework. To be specific, a generative network via RFFs is devised to implicitly learn the kernel, followed by a linear classifier parameterized as a full-connected layer. Then the generative network and the classifier are jointly trained by solving an empirical risk minimization (ERM) problem to reach a one-stage solution. This end-to-end scheme naturally allows deeper features, in correspondence to a multi-layer structure, and shows superior generalization performance over the classical two-stage, RFFs-based methods in real-world classification tasks. Moreover, inspired by the randomized resampling mechanism of the proposed method, its enhanced adversarial robustness is investigated and experimentally verified.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322005374",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Classifier (UML)",
      "Combinatorics",
      "Computer science",
      "Generative grammar",
      "Kernel (algebra)",
      "Kernel embedding of distributions",
      "Kernel method",
      "Linear classifier",
      "Machine learning",
      "Mathematics",
      "Multiple kernel learning",
      "Pattern recognition (psychology)",
      "Radial basis function kernel",
      "String kernel",
      "Support vector machine",
      "Tree kernel",
      "Variable kernel density estimation"
    ],
    "authors": [
      {
        "surname": "Fang",
        "given_name": "Kun"
      },
      {
        "surname": "Liu",
        "given_name": "Fanghui"
      },
      {
        "surname": "Huang",
        "given_name": "Xiaolin"
      },
      {
        "surname": "Yang",
        "given_name": "Jie"
      }
    ]
  },
  {
    "title": "Graph-based image gradients aggregated with random forests",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2022.08.015",
    "abstract": "Gradient methods subject images to a series of operations to enhance some characteristics and facilitate image analysis, usually the contours of large objects. We argue that a gradient must show other characteristics, such as minor components and large uniform regions, particularly for the image segmentation task where subjective concepts such as region coherence and similarity are hard to interpret from the pixel information. This work extends the formalism of a previously proposed graph-based image gradient method that uses edge-weighted graphs aggregated with Random Forest (RF) to create descriptive gradients. We aim to explore more extensive input image areas and make changes driven by the RF mechanics. We evaluated the proposals on the edge and segmentation tasks, analyzing the gradient characteristics that most impacted the final segmentation. The experiments indicated that sharp thick contours are crucial, whereas fuzzy maps yielded the worst results even when created from deep methods with more precise edge maps. Also, we analyzed how uniform regions and small details impacted the final segmentation. Statistical analysis on the segmentation task demonstrated that the gradients created by the proposed are significantly better than most of the best edge maps methods and validated our original choices of attributes.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522002525",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Enhanced Data Rates for GSM Evolution",
      "Image segmentation",
      "Morphological gradient",
      "Pattern recognition (psychology)",
      "Pixel",
      "Random forest",
      "Scale-space segmentation",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Almeida",
        "given_name": "Raquel"
      },
      {
        "surname": "Kijak",
        "given_name": "Ewa"
      },
      {
        "surname": "Malinowski",
        "given_name": "Simon"
      },
      {
        "surname": "Patrocínio Jr",
        "given_name": "Zenilton K.G."
      },
      {
        "surname": "Araújo",
        "given_name": "Arnaldo A."
      },
      {
        "surname": "Guimarães",
        "given_name": "Silvio J.F."
      }
    ]
  },
  {
    "title": "A novel soft-coded error-correcting output codes algorithm",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109122",
    "abstract": "Error-Correcting Output Codes (ECOC) algorithms enable multiclass classification by reassigning multiple classes to the positive/negative group with the class reassignment schemes being recorded as binary/ternary hard-coded (HC) codematrices. Different classes tend to get diverse subordination degrees to the positive/negative group, providing clues to correct potential errors. However, the HC codematrices are unable to provide the information in the subordination degrees. In this paper, a Soft-Coded ECOC (SC-ECOC) scheme, namely, the Sequential Forward Floating Selection algorithm, is proposed by filling codematrices with real values instead of hard codes to improve classification performance. This algorithm divides multiple classes into two groups by maximizing the ratio of inter-group distance to intra-group distance. Then a new measure coverage is designed to evaluate the subordination degrees of different classes to both groups, which are set as the elements to form a codematrix. Furthermore, a self-adaptive strategy adjusts the value of each element to fit learners better. Experiments are carried out to verify the performance of our algorithm on various data sets, and results confirm that our algorithm can achieve more balanced results compared with the traditional HC ECOC algorithms. Besides, the values of soft codes correlate with the difficulty level of various classes to improve the multiclass classification ability.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322006021",
    "keywords": [
      "Algorithm",
      "Computer science"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Kun-Hong"
      },
      {
        "surname": "Gao",
        "given_name": "Jie"
      },
      {
        "surname": "Xu",
        "given_name": "Yong"
      },
      {
        "surname": "Feng",
        "given_name": "Kai-Jie"
      },
      {
        "surname": "Ye",
        "given_name": "Xiao-Na"
      },
      {
        "surname": "Liong",
        "given_name": "Sze-Teng"
      },
      {
        "surname": "Chen",
        "given_name": "Li-Yan"
      }
    ]
  },
  {
    "title": "Density peaks clustering based on balance density and connectivity",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109052",
    "abstract": "Density peaks clustering (DPC) algorithm regards the density peaks as the potential cluster centers, and assigns the non-center point into the cluster of its nearest higher-density neighbor. Although DPC can discover clusters with arbitrary shapes, it has some limitations. On the one hand, the density measure of DPC fails to eliminate the density difference among different clusters, which affects the accuracy of recognizing cluster center. On the other hand, the nearest higher-density point is determined without considering connectivity, which leads to continuously clustering errors. Therefore, DPC fails to obtain satisfactory clustering results on datasets with great density difference among clusters. In order to eliminate these limitations, a novel DPC algorithm based on balance density and connectivity (BC-DPC) is proposed. First, the balance density is proposed to eliminate the density difference among different clusters to accurately recognize cluster centers. Second, the connectivity between a data point and its nearest higher-density point is guaranteed by mutual nearest neighbor relationship to avoid continuously clustering errors. Finally, a fast search strategy is proposed to find the nearest higher-density point. The experimental results on synthetic, UCI, and image datasets demonstrate the efficiency and effectiveness of the proposed algorithm in this paper.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322005325",
    "keywords": [
      "Artificial intelligence",
      "Balance (ability)",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Medicine",
      "Pattern recognition (psychology)",
      "Physical medicine and rehabilitation"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Qinghua"
      },
      {
        "surname": "Dai",
        "given_name": "Yongyang"
      },
      {
        "surname": "Wang",
        "given_name": "Guoyin"
      }
    ]
  },
  {
    "title": "A graphical approach for filter pruning by exploring the similarity relation between feature maps",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2022.12.028",
    "abstract": "The vast majority of repetitive pruning and retraining techniques used on CNNs require multi-stage optimization, which undermines the potential computing savings from pruning. The similarity relationship between the output feature maps of the filters is first observed and represented as a graphical model in this paper in order to solve this issue. Vertex redundancy in a graph is determined using the degree of a vertex and the weight of its edges. Pruning is then carried out “one-shot” by removing the filters associated with redundant feature maps from each layer. In order to preserve performance, the network must finally be retrained. The CIFAR-10 dataset is identified using the classifier produced by applying this proposed methodology to several deep learning models. In this instance, both the parameters reduction and FLOPs reduction of the trimmed model are significantly improved. This method, tested on VGG-16, reduces parameters by 90.8% and FLOPs by 60.0% while sacrificing little accuracy (=0.16%) in comparison to the baseline. The results based on ResNet-110 show that applying the proposed approach can reduce FLOPs and parameters by 70.4% while preserving baseline accuracy with a loss of just 0.05%.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522003968",
    "keywords": [
      "Agronomy",
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Classifier (UML)",
      "Computer science",
      "FLOPS",
      "Feature (linguistics)",
      "Graph",
      "Linguistics",
      "Mathematics",
      "Operating system",
      "Parallel computing",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pruning",
      "Redundancy (engineering)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Jiang"
      },
      {
        "surname": "Shao",
        "given_name": "Haijian"
      },
      {
        "surname": "Zhai",
        "given_name": "Shengjie"
      },
      {
        "surname": "Jiang",
        "given_name": "Yingtao"
      },
      {
        "surname": "Deng",
        "given_name": "Xing"
      }
    ]
  },
  {
    "title": "IDA: Improving distribution analysis for reducing data complexity and dimensionality in hyperspectral images",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109096",
    "abstract": "Hyperspectral images (HSIs) are known for their high dimensionality and wide spectral bands that increase redundant information and complicate classification. Outliers and mixed data are common problems in HSIs. Thus, preprocessing methods are essential in enhancing and reducing data complexity, redundant information, and the number of bands. This study introduces a novel feature reduction method (FRM) called improving distribution analysis (IDA). IDA works to increase the correlation between related data, decrease the distance between big and small data, and correct each value's location to be inside its group range. In IDA, the input data passes through three stages. Getting rid of outliers and improving data correlation is the first step. The second stage involves increasing the variance. The third is to simplify the data and normalize the distribution. IDA is compared with four popular FRMs in four available HSIs. It is also tested and evaluated in various classification models, including spatial, spectral, and spectral-spatial models. The experimental results demonstrate that IDA performs admirably in enhancing data distribution, reducing complexity, and accelerating performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322005763",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Curse of dimensionality",
      "Data cube",
      "Data mining",
      "Dimensionality reduction",
      "Hyperspectral imaging",
      "Mathematics",
      "Outlier",
      "Pattern recognition (psychology)",
      "Preprocessor",
      "Spatial analysis",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "AL-Alimi",
        "given_name": "Dalal"
      },
      {
        "surname": "Al-qaness",
        "given_name": "Mohammed A.A."
      },
      {
        "surname": "Cai",
        "given_name": "Zhihua"
      },
      {
        "surname": "Alawamy",
        "given_name": "Eman Ahmed"
      }
    ]
  },
  {
    "title": "Learning isometry-invariant representations for point cloud analysis",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109087",
    "abstract": "3D shape analysis has drawn broad attention due to its increasing demands in various fields. Despite that impressive performance has been achieved on several databases, most researchers focus their efforts on improving the performance of shape classification, retrieval, segmentation, etc. They neglect the fact that the disturbances, such as orientation and deformation, may impact much on the perception, restricting the capacity of generalizing to real applications where the prior of orientation and pose is often unknown. In this paper, we conduct shape analysis on point clouds and propose the point projection feature, which is rotation-invariant. Specifically, a novel architecture is designed to mine features of different levels. We adopt a PointNet-based backbone to extract global feature for the point cloud, and the graph aggregation operation to perceive local pose variance in the Euclidean space or geodesic space. An efficient key point descriptor is designed to assign each point with different response and help recognize the overall geometry. Furthermore, a novel dataset, PKUnon-rigid, is built that is composed of non-rigid 3D objects, based on which we evaluate the capacity of several mainstream methods in terms of processing non-rigid shapes. Mathematical analyses and experimental results demonstrate that the proposed method can extract isometry-invariant representations for 3D shape analysis tasks without rotation augmentation, and outperforms other state-of-the-art methods. The proposed dataset is publicly available at https://github.com/tasx0823/PKUnon-rigid.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322005672",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Invariant (physics)",
      "Isometry (Riemannian geometry)",
      "Mathematical physics",
      "Mathematics",
      "Point cloud",
      "Pure mathematics"
    ],
    "authors": [
      {
        "surname": "Sun",
        "given_name": "Xiao"
      },
      {
        "surname": "Huang",
        "given_name": "Yang"
      },
      {
        "surname": "Lian",
        "given_name": "Zhouhui"
      }
    ]
  },
  {
    "title": "Rethinking 3D cost aggregation in stereo matching",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.02.011",
    "abstract": "In the stereo matching task, the 3D convolution network can effectively aggregate the cost volume with the strong representation ability to model the spatial and depth dimensions but with the disadvantage of a high computational cost. In this letter, we revisit the 3D convolution network and its common variant, and then propose the Depth Shift Module (DSM) to model the cost volume in the depth dimension which could imitate the 3D convolution function with the computational complexity of the 2D convolution. The proposed DSM is easy to extend to present 3D cost aggregation methods in stereo matching with less inference time, lower computational complexity, and minor precision loss. Moreover, a novel compact but efficient stereo matching framework named HybridNet is proposed. This framework can hybridize the 2D convolution layer with the proposed DSM to effectively aggregate the cost volume. The proposed HybridNet achieves a better trade-off between the performance, computational complexity, and model size (e.g., 30% less than the size of AANet and 25% less than the size of PSMNet) in public open-source datasets (e.g., Scene Flow and KITTI Stereo 2015). The relevant code is available at https://github.com/GANWANSHUI/HybridNet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523000417",
    "keywords": [
      "Aggregate (composite)",
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Composite material",
      "Computational complexity theory",
      "Computer science",
      "Computer vision",
      "Convolution (computer science)",
      "Dimension (graph theory)",
      "Evolutionary biology",
      "Function (biology)",
      "Inference",
      "Matching (statistics)",
      "Materials science",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Pure mathematics",
      "Quantum mechanics",
      "Statistics",
      "Stereopsis",
      "Volume (thermodynamics)"
    ],
    "authors": [
      {
        "surname": "Gan",
        "given_name": "Wanshui"
      },
      {
        "surname": "Wu",
        "given_name": "Wenhao"
      },
      {
        "surname": "Chen",
        "given_name": "Shifeng"
      },
      {
        "surname": "Zhao",
        "given_name": "Yuxiang"
      },
      {
        "surname": "Wong",
        "given_name": "Pak Kin"
      }
    ]
  },
  {
    "title": "Active anomaly detection based on deep one-class classification",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2022.12.009",
    "abstract": "Active learning has been utilized as an efficient tool in building anomaly detection models by leveraging expert feedback. In an active learning framework, a model queries samples to be labeled by experts and re-trains the model with the labeled data samples. It unburdens in obtaining annotated datasets while improving anomaly detection performance. However, most of the existing studies focus on helping experts identify as many abnormal data samples as possible, which is a sub-optimal approach for one-class classification-based deep anomaly detection. In this paper, we tackle two essential problems of active learning for Deep SVDD: query strategy and semi-supervised learning method. First, rather than solely identifying anomalies, our query strategy selects uncertain samples according to an adaptive boundary. Second, we apply noise contrastive estimation in training a one-class classification model to incorporate both labeled normal and abnormal data effectively. We analyze that the proposed query strategy and semi-supervised loss individually improve an active learning process of anomaly detection and further improve when combined together on seven anomaly detection datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522003774",
    "keywords": [
      "Active learning (machine learning)",
      "Anomaly (physics)",
      "Anomaly detection",
      "Artificial intelligence",
      "Artificial neural network",
      "Class (philosophy)",
      "Computer science",
      "Condensed matter physics",
      "Data mining",
      "Deep learning",
      "Focus (optics)",
      "Machine learning",
      "One-class classification",
      "Operating system",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Process (computing)",
      "Semi-supervised learning",
      "Supervised learning",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Kim",
        "given_name": "Minkyung"
      },
      {
        "surname": "Kim",
        "given_name": "Junsik"
      },
      {
        "surname": "Yu",
        "given_name": "Jongmin"
      },
      {
        "surname": "Choi",
        "given_name": "Jun Kyun"
      }
    ]
  },
  {
    "title": "Extending the kinematic theory of rapid movements with new primitives",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.02.021",
    "abstract": "The Kinematic Theory of rapid movements, and its associated Sigma-Lognormal, model 2D spatiotemporal trajectories. It is constructed mainly as a temporal overlap of curves between virtual target points. Specifically, it uses an arc and a lognormal as primitives for the representation of the trajectory and velocity, respectively. This paper proposes developing this model, in what we call the Kinematic Theory Transform, which establishes a mathematical framework that allows further primitives to be used. Mainly, we evaluate Euler curves to link virtual target points and Gaussian, Beta, Gamma, Double-bounded lognormal, and Generalized Extreme Value functions to model the bell-shaped velocity profile. Using these primitives, we report reconstruction results with spatiotemporal trajectories executed by human beings, animals, and anthropomorphic robots.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523000508",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Astronomy",
      "Bounded function",
      "Classical mechanics",
      "Computer science",
      "Computer vision",
      "Gaussian",
      "Kinematics",
      "Law",
      "Log-normal distribution",
      "Mathematical analysis",
      "Mathematics",
      "Physics",
      "Political science",
      "Politics",
      "Quantum mechanics",
      "Representation (politics)",
      "Statistics",
      "Trajectory"
    ],
    "authors": [
      {
        "surname": "Ferrer",
        "given_name": "Miguel A."
      },
      {
        "surname": "Diaz",
        "given_name": "Moises"
      },
      {
        "surname": "Quintana",
        "given_name": "Jose Juan"
      },
      {
        "surname": "Carmona-Duarte",
        "given_name": "Cristina"
      },
      {
        "surname": "Plamondon",
        "given_name": "Réjean"
      }
    ]
  },
  {
    "title": "An accuracy-enhanced group recommendation approach based on DEMATEL",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.02.008",
    "abstract": "Group recommendations aim to suggest items to a group of users based on their preferences. Many group recommendations often consider various factors to calculate the influence of each group member and then assign weights to complete the group recommendation, aiming to maximize group member satisfaction. However, most group recommendations tend to focus more on the calculation process of group members’ influence while ignoring the process of assigning their weights, which may lead to low group members’ satisfaction. To solve this problem, a novel group recommendation approach called GroupRecD is proposed to assign weights of users scientifically and reasonably based on data mining and DEMATEL technique. To demonstrate the availability and effectiveness of GroupRecD , we conduct extensive experiments on the MovieLens 100k dataset and use three evaluation metrics including GSM, RECALL, and nDCG to evaluate the approach. Experimental results demonstrate that GroupRecD outperforms other comparison approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523000284",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Collaborative filtering",
      "Computer science",
      "Data mining",
      "Group (periodic table)",
      "Information retrieval",
      "Machine learning",
      "MovieLens",
      "Operating system",
      "Organic chemistry",
      "Process (computing)",
      "Recommender system"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Yuqing"
      },
      {
        "surname": "Qi",
        "given_name": "Lianyong"
      },
      {
        "surname": "Dou",
        "given_name": "Ruihan"
      },
      {
        "surname": "Shen",
        "given_name": "Shigen"
      },
      {
        "surname": "Hou",
        "given_name": "Linlin"
      },
      {
        "surname": "Liu",
        "given_name": "Yuwen"
      },
      {
        "surname": "Yang",
        "given_name": "Yihong"
      },
      {
        "surname": "Kong",
        "given_name": "Lingzhen"
      }
    ]
  },
  {
    "title": "Modeling andness in multilabel classification to recognize mutual information",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.02.009",
    "abstract": "Recognizing valid (statistical) dependencies in pattern recognition improves quality of learning extremely. In particular, in a multi-label classification scenario identifying such dependencies among labels can increase the performance of such approaches. In real-world applications, we are dealing in many cases with andness, e.g., a heavy and old car, exhibits naturally a high fuel consumption. Many real problems are affected by andness to some extend, therefore, considering such kind of dependencies in a learning process can be beneficial to capture more insight and interactions from data. To this end, this paper presents a novel approach to model andness dependencies in a multi-label classification scenario. In the experimental part, we demonstrate the real gains achieved by applying this approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523000338",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Epistemology",
      "Machine learning",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Process (computing)",
      "Quality (philosophy)"
    ],
    "authors": [
      {
        "surname": "Fallah Tehrani",
        "given_name": "Ali"
      }
    ]
  },
  {
    "title": "Dead pixel test using effective receptive field",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.02.018",
    "abstract": "Deep neural networks have been used in various fields, but their internal behavior in how they understand images is not well known. In this study, we discuss two counterintuitive properties of convolutional neural networks (CNNs). First, we evaluated the size of the receptive field of CNNs with their classification accuracy. Previous studies have attempted to increase the size of the receptive field for performance gain. However, we observed that some CNNs with a smaller receptive field can achieve higher classification accuracy. In this regard, we claim that a larger receptive field does not guarantee improved classification accuracy. Second, using the effective receptive field, we examined the contribution of each pixel to the output of CNN. Intuitively, each pixel is expected to equally contribute to the final output, but we found that there exist pixels in a partially dead state with little contribution to the output. We reveal that the reason for dead pixels lies in even stride operations with odd-sized kernels in CNN and propose a kernel padding method to remove the dead pixels. We demonstrated the vulnerability of CNNs with dead pixels when we detect a noise or small box that is on dead pixels. Our findings on dead pixels should be understood and considered in practical applications of CNN.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523000478",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Field (mathematics)",
      "Kernel (algebra)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pixel",
      "Pure mathematics",
      "Receptive field"
    ],
    "authors": [
      {
        "surname": "Kim",
        "given_name": "Bum Jun"
      },
      {
        "surname": "Choi",
        "given_name": "Hyeyeon"
      },
      {
        "surname": "Jang",
        "given_name": "Hyeonah"
      },
      {
        "surname": "Lee",
        "given_name": "Dong Gu"
      },
      {
        "surname": "Jeong",
        "given_name": "Wonseok"
      },
      {
        "surname": "Kim",
        "given_name": "Sang Woo"
      }
    ]
  },
  {
    "title": "Subscripto multiplex: A Riemannian symmetric positive definite strategy for offline signature verification",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.02.002",
    "abstract": "The human handwritten signature is considered to be a significant biometric trait. In the case of offline signatures, the problem is addressed as an image recognition task. On the other hand, the visual representation of symmetric positive definitive matrices, usually by means of the covariance descriptor of the image feature maps, forms a specific Riemannian manifold with a widespread usage and a favorable performance in a plethora of applications. Surprisingly, no records of offline-signature-verification-oriented research in the space of symmetric positive definitive matrix have been found up to now. In this work, we propose, for the first time in offline signature-verification literature, mapping of handwritten signature images in points of the tangent space of a connected symmetric positive definitive manifold for verification purposes. Furthermore, based on the principles of differential geometry, we address the notorious limited training problem of offline signature verification in this manifold by proposing two different feature augmentation methods. The efficiency of the proposed method is evaluated using three popular datasets of Western and Asian origin. Error rates against skilled and random forgery in both baselines as well augmentation scenarios are strong indicators of the informative and highly discriminative nature of symmetric positive definitive manifold oriented representation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523000259",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biometrics",
      "Computer science",
      "Discriminative model",
      "Engineering",
      "Feature (linguistics)",
      "Feature vector",
      "Geometry",
      "Image (mathematics)",
      "Law",
      "Linguistics",
      "Manifold (fluid mechanics)",
      "Mathematics",
      "Mechanical engineering",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Pure mathematics",
      "Representation (politics)",
      "Riemannian manifold",
      "Signature (topology)"
    ],
    "authors": [
      {
        "surname": "Zois",
        "given_name": "Elias N."
      },
      {
        "surname": "Said",
        "given_name": "Salem"
      },
      {
        "surname": "Tsourounis",
        "given_name": "Dimitrios"
      },
      {
        "surname": "Alexandridis",
        "given_name": "Alex"
      }
    ]
  },
  {
    "title": "COVID-19 and Rumors: A Dynamic Nested Optimal Control Model",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109186",
    "abstract": "Unfortunately, the COVID-19 outbreak has been accompanied by the spread of rumors and depressing news. Herein, we develop a dynamic nested optimal control model of COVID-19 and its rumor outbreaks. The model aims to curb the epidemics by reducing the number of individuals infected with COVID-19 and reducing the number of rumor-spreaders while minimizing the cost associated with the control interventions. We use the modified approximation Karush–Kuhn–Tucker conditions with the Hamiltonian function to simplify the model before solving it using a genetic algorithm. The present model highlights three prevention measures that affect COVID-19 and its rumor outbreaks. One represents the interventions to curb the COVID-19 pandemic. The other two represent interventions to increase awareness, disseminate the correct information, and impose penalties on the spreaders of false rumors. The results emphasize the importance of interventions in curbing the spread of the COVID-19 pandemic and its associated rumor problems alike.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322006653",
    "keywords": [
      "2019-20 coronavirus outbreak",
      "Artificial intelligence",
      "Computer science",
      "Control (management)",
      "Coronavirus disease 2019 (COVID-19)",
      "Disease",
      "Environmental health",
      "Epidemic model",
      "Infectious disease (medical specialty)",
      "Medicine",
      "Outbreak",
      "Pandemic",
      "Pathology",
      "Political science",
      "Population",
      "Psychiatry",
      "Psychological intervention",
      "Public relations",
      "Rumor",
      "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)",
      "Virology"
    ],
    "authors": [
      {
        "surname": "Hezam",
        "given_name": "Ibrahim M."
      },
      {
        "surname": "Almshnanah",
        "given_name": "Abdulkarem"
      },
      {
        "surname": "Mubarak",
        "given_name": "Ahmed A."
      },
      {
        "surname": "Das",
        "given_name": "Amrit"
      },
      {
        "surname": "Foul",
        "given_name": "Abdelaziz"
      },
      {
        "surname": "Alrasheedi",
        "given_name": "Adel Fahad"
      }
    ]
  },
  {
    "title": "Secure latent fingerprint storage and self-recovered reconstruction using POB number system",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.02.007",
    "abstract": "Latent fingerprints are the unintentionally deposited fingerprint impressions gathered from the crime scenes. Many criminal investigation agencies consider latent fingerprints as a significant court accepted evidence. A typical latent fingerprint comes in low quality. Hence, a slight modification in the latent fingerprint may induce a marked shift in the recognition performance. Due to this, wrongdoers behind the crime scenes may try to remove or alter the latent fingerprint information by accessing the fingerprint database. Unlike regular fingerprint enrollment, retaking a latent fingerprint is not always possible. Preserving the latent fingerprints in a single database makes it vulnerable to single-point attacks. Hence, this paper presents a secure way to store and retrieve latent fingerprint information using POB-based ( n , n ) VSS technique. The proposed method encrypts each latent fingerprint as n secret shares, and stores them in n distinct databases. The distributed storage protects the data from single-point attacks. Along with secure storage, we also introduce a self-recovery mechanism in the case of fingerprint share tampering. The self-recovery mechanism protects the latent fingerprint from different tampering attacks. The proposed method has been evaluated using NIST Special Database4 (NIST-SD4) and IIIT Delhi latent fingerprint datasets. The experimental results show that the proposed technique offers secure distributed storage with lossless reconstruction of latent fingerprint images whenever needed. The proposed self-recovery mechanism enables the recovery of latent fingerprint images even in the case of share tampering.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523000302",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Data mining",
      "Fingerprint (computing)",
      "Fingerprint recognition",
      "Image (mathematics)",
      "Latent image",
      "NIST",
      "Pattern recognition (psychology)",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Muhammed",
        "given_name": "Ajnas"
      },
      {
        "surname": "Pais",
        "given_name": "Alwyn Roshan"
      }
    ]
  },
  {
    "title": "Relational distance and document-level contrastive pre-training based relation extraction model",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.02.012",
    "abstract": "Document-level relation extraction has multi-entity and multi-mention compared to sentence-level, existing sentence-level relation extraction models cannot meet the requirements of document-level relation extraction. Existing graph-based document-level models usually design points and edges manually, which often introduces man-made noise; while the Transformer-based models cannot deeply solve the difficulties such as coreference resolution by designing pre-training tasks or other methods. In this paper, we propose a new Relational Distance and Document-level Contrastive Pre-training (RDDCP) based relation extraction model, which achieves coreference resolution by simple and effective mention replacement; we also introduce the concept of relational distance to achieve document-level contrastive pre-training, and find the most likely relational mention pairs from the plural mention pairs existing in the document-level dataset for contrastive learning; for the relation information in distant mentions ignored by the relational distance, we quantified the distances as weights and incorporated the information with weights into the embedding representation of entities. Each entity presents different entity embedding representations in different entity pairs. We conducted experiments on three popular datasets and the RDDCP model outperformed GAIN, SSAN and ATLOP as well as other baseline models in terms of performance and time complexity.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523000429",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Coreference",
      "Data mining",
      "Embedding",
      "Information extraction",
      "Natural language processing",
      "Physics",
      "Quantum mechanics",
      "Relation (database)",
      "Relationship extraction",
      "Resolution (logic)",
      "Sentence",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Dong",
        "given_name": "Yihao"
      },
      {
        "surname": "Xu",
        "given_name": "Xiaolong"
      }
    ]
  },
  {
    "title": "Locality-aware subgraphs for inductive link prediction in knowledge graphs",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.02.004",
    "abstract": "Recent methods for inductive reasoning on Knowledge Graphs (KGs) transform the link prediction problem into a graph classification task. They first extract a subgraph around each target link based on the k -hop neighborhood of the target entities, encode the subgraphs using a Graph Neural Network (GNN), then learn a function that maps subgraph structural patterns to link existence. Although these methods have witnessed great successes, increasing k often leads to an exponential expansion of the neighborhood, thereby degrading the GNN expressivity due to oversmoothing. In this paper, we formulate the subgraph extraction as a local clustering procedure that aims at sampling tightly-related subgraphs around the target links, based on a personalized PageRank (PPR) approach. Empirically, on three real-world KGs, we show that reasoning over subgraphs extracted by PPR-based local clustering can lead to a more accurate link prediction model than relying on neighbors within fixed hop distances. Furthermore, we investigate graph properties such as average clustering coefficient and node degree, and show that there is a relation between these and the performance of subgraph-based link prediction.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523000314",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Cluster analysis",
      "Clustering coefficient",
      "Combinatorics",
      "Computer science",
      "ENCODE",
      "Gene",
      "Graph",
      "Link (geometry)",
      "Mathematics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Mohamed",
        "given_name": "Hebatallah A."
      },
      {
        "surname": "Pilutti",
        "given_name": "Diego"
      },
      {
        "surname": "James",
        "given_name": "Stuart"
      },
      {
        "surname": "Del Bue",
        "given_name": "Alessio"
      },
      {
        "surname": "Pelillo",
        "given_name": "Marcello"
      },
      {
        "surname": "Vascon",
        "given_name": "Sebastiano"
      }
    ]
  },
  {
    "title": "M-CBN: Manifold constrained joint image dehazing and super-resolution based on chord boosting network",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109166",
    "abstract": "This paper proposes a Manifold Constrained Chord Boosting Network (M-CBN), which incorporates the super-resolution principle to achieve image dehazing. M-CBN is a task-specific image restoration network that explicitly learns the mapping from Low-resolution (LR) hazy images to High-resolution (HR) haze-free images. Hence, we design a preliminary image degradation to imitate super-resolution training on hazy images. In M-CBN, a plug-and-play Cross-linked Dual Projection Module (CDPM) for skip connections is developed. In CDPM, back-projections for HR encoder features and LR decoder features are cross-linked for better recovery of spatial information, and a cross-resolution spatial attention is designed to enhance fusion features. Then, to boost the generation of image details and textures, we propose a Chord Residual Module (CRM), which can separately process High-frequency (HF) and Low-frequency (LF) features by progressive inner-frequency updating and dense inter-frequency cross-collaboration to enhance decoding features. Finally, a manifold constraint dual discriminator is established. The static discriminator explicitly constrains dehazed images in the expected manifold to unify the joint learning of image dehazing and super-resolution. And the dynamic discriminator implicitly optimizes the network by adversarial training. Extensive experiments on general, dense and non-homogeneous haze datasets and cross-domain dehazing tasks show the proposed M-CBN presents high-quality dehazed results with natural colors and clear details.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322006458",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Boosting (machine learning)",
      "Computer science",
      "Computer vision",
      "Detector",
      "Discriminator",
      "Encoder",
      "Operating system",
      "Residual",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Pengyu"
      },
      {
        "surname": "Zhu",
        "given_name": "Hongqing"
      },
      {
        "surname": "Zhang",
        "given_name": "Han"
      },
      {
        "surname": "Wang",
        "given_name": "Nan"
      }
    ]
  },
  {
    "title": "Transformer-based language models for mental health issues: A survey",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.02.016",
    "abstract": "Early identification and prevention of mental health stresses and their outcomes has become of urgent importance worldwide. To this purpose, artificial intelligence provides a body of advanced computational tools that can effectively support decision-making clinical processes by modeling and analyzing the presence of a variety of mental health issues, particularly when these can be detected in text data. In this regard, Transformer-based language models (TLMs) have demonstrated exceptional efficacy in a number of NLP tasks also in the health domain. To the best of our knowledge, the use of TLMs for specifically addressing mental health issues has not been deeply investigated so far. In this paper, we aim to fill this gap in the literature by providing the first survey of methods using TLMs for text-based identification of mental health issues.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523000430",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Computer science",
      "Data science",
      "Electrical engineering",
      "Engineering",
      "Identification (biology)",
      "Machine learning",
      "Mental health",
      "Psychiatry",
      "Psychology",
      "Transformer",
      "Variety (cybernetics)",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Greco",
        "given_name": "Candida M."
      },
      {
        "surname": "Simeri",
        "given_name": "Andrea"
      },
      {
        "surname": "Tagarelli",
        "given_name": "Andrea"
      },
      {
        "surname": "Zumpano",
        "given_name": "Ester"
      }
    ]
  },
  {
    "title": "Person-Specific Face Spoofing Detection Based on a Siamese Network",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109148",
    "abstract": "Face spoofing detection is an essential prerequisite for face recognition applications. Previous face spoofing detection methods usually trained a binary classifier to classify the input face as a spoof face or a real face before face recognition, and client identity information was not utilized. In this paper, we propose a person-specific face spoofing detection method to employ client identity information for face spoofing detection. In our method, face spoofing is detected after face recognition rather than before face recognition; that is, the input face is recognized first, and the client identity is used to assist face spoofing detection. We train a deep Siamese network with image pairs. Each image pair consists of two real face images or one real and one spoof face image. The face images in each pair come from the same client. The deep Siamese network is trained by joint Bayesian loss together with contrastive loss and softmax loss. In testing, an input face image is recognized first, then the real face image of the identified client is retrieved, and an image pair is formed by the test face image and the retrieved real face image. The image pair is classified by the trained Siamese network to determine whether the input test image is a real face or not. The experimental results demonstrate the effectiveness of our method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322006276",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Deep learning",
      "Face (sociological concept)",
      "Face detection",
      "Facial recognition system",
      "Image (mathematics)",
      "Object-class detection",
      "Pattern recognition (psychology)",
      "Social science",
      "Sociology",
      "Softmax function",
      "Spoofing attack"
    ],
    "authors": [
      {
        "surname": "Pei",
        "given_name": "Mingtao"
      },
      {
        "surname": "Yan",
        "given_name": "Bin"
      },
      {
        "surname": "Hao",
        "given_name": "Huiling"
      },
      {
        "surname": "Zhao",
        "given_name": "Meng"
      }
    ]
  },
  {
    "title": "Sequence patterns and HMM profiles to predict proteome wide zinc finger motifs",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109134",
    "abstract": "Zinc finger (ZnF) is an important class of nucleic acid and protein recognition domain, wherein, zinc ion is the inorganic co-factor that forms a tetrahedral geometry with the cysteine and/or histidine residues. ZnF domains take up diverse architectures with different ZnF motifs and have a wide range of biological functions. Nonetheless, predicting the ZnF motif(s) from the sequence is quite challenging. To this end, 74 unique ZnF sequence patterns are collected from the literature and classified into 32 different classes. Since the shorter length of ZnF sequence patterns leads to inaccurate predictions, ZnF domain Pfam HMM profiles defined under 6 ZnF Pfam clans (215 HMM profiles) and a few undefined Pfam clans (74 HMM profiles) are used for the prediction. A web server, namely, ZnF-Prot (https://project.iith.ac.in/znprot/) is developed which can predict the presence of 31 ZnF domains in a protein/proteome sequence of any organism. The use of ZnF sequence patterns and Pfam HMM profiles resulted in an accurate prediction of 610 test cases (taken randomly from 249 organisms) considered here. Additionally, the application of ZnF-Prot is demonstrated by considering Arabidopsis thaliana, Homo sapiens, Saccharomyces cerevisiae, Caenorhabditis elegans and Ciona intestinalis proteomes as test cases, wherein, 87–96% of the predicted ZnF motifs are cross-validated.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322006148",
    "keywords": [
      "Artificial intelligence",
      "Bioinformatics",
      "Biology",
      "Computational biology",
      "Computer science",
      "Gene",
      "Genetics",
      "Hidden Markov model",
      "Human proteome project",
      "Peptide sequence",
      "Protein sequencing",
      "Proteome",
      "Proteomics",
      "Sequence analysis",
      "Transcription factor",
      "Zinc finger"
    ],
    "authors": [
      {
        "surname": "Sathyaseelan",
        "given_name": "Chakkarai"
      },
      {
        "surname": "Patro",
        "given_name": "L Ponoop Prasad"
      },
      {
        "surname": "Rathinavelan",
        "given_name": "Thenmalarchelvi"
      }
    ]
  },
  {
    "title": "Similarity learning with deep CRF for person re-identification",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109151",
    "abstract": "The core of person re-identification (Re-ID) lies in robustly estimating similarities for each probe-gallery image pair. A common practice in existing works is to calculate the similarity of each image pair independently, ignoring relations between different probe-gallery pairs. In this paper, we present a deep learning conditional random field (Deep-CRF) graph to model group-wise similarities within a batch of images, and regard the Re-ID task as a CRF node labeling problem. Unlike the existing deep CRF based approach where the CRF inference is only involved in the training stage, our method intends to fully exploit the potential of CRF model, exhibiting inference consistency in both training and testing. Specifically, we design unary potentials for computing each probe-gallery similarity separately. To efficiently encode relationships between different probe-gallery pairs, pairwise potentials are built on an arbitrary node pair whose learning is achieved by a joint matching strategy using bidirectional LSTM. We pose the CRF inference as a RNN learning process, where unary and pairwise potentials are jointly optimized in an end-to-end manner. Extensive experiments on three large-scale person Re-ID datasets demonstrate the effectiveness of the proposed method. Our Deep-CRF achieves the best results compared with the previous graph-based deep learning approaches and substantially exceeds the existing deep CRF framework by 8% in Rank1 accuracy on CUHK03 dataset. It also behaves competitive among the current state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322006306",
    "keywords": [
      "Artificial intelligence",
      "CRFS",
      "Combinatorics",
      "Computer science",
      "Conditional random field",
      "Consistency (knowledge bases)",
      "Deep learning",
      "Engineering",
      "Graph",
      "Image (mathematics)",
      "Inference",
      "Machine learning",
      "Matching (statistics)",
      "Mathematics",
      "Node (physics)",
      "Pairwise comparison",
      "Pattern recognition (psychology)",
      "Similarity (geometry)",
      "Statistics",
      "Structural engineering",
      "Theoretical computer science",
      "Unary operation"
    ],
    "authors": [
      {
        "surname": "Xiang",
        "given_name": "Jun"
      },
      {
        "surname": "Huang",
        "given_name": "Ziyuan"
      },
      {
        "surname": "Jiang",
        "given_name": "Xiaoping"
      },
      {
        "surname": "Hou",
        "given_name": "Jianhua"
      }
    ]
  },
  {
    "title": "Iterative embedding distillation for open world vehicle recognition",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109140",
    "abstract": "Vehicle recognition poses a practical but challenging problem in many real-world surveillance applications. Since vehicle recognition is an open-set problem, it is a critical issue to learn a discriminative visual embedding space rather than a well-performing classifier. In this paper, we propose an iterative embedding distillation (IED) framework for open-set vehicle recognition. The soft target in knowledge distillation is utilized to establish the interclass relations from an instance level rather than a category level. Towards the open-set problem, we extend knowledge distillation to embedding distillation in an iterative learning way, in which three types of loss functions are studied to iteratively transfer the distributions of embeddings from the teacher network to the student network. To demonstrate the universal nature of IED, we implement the IED framework on two basic convolutional neural networks and verify it using the cross-dataset testing protocols without retraining or fine-tuning. Extensive experimental results show that IED obtains quite encouraging results and outperforms state-of-the-art methods on various large-scale vehicle recognition datasets including VeRi-776, Vehicle-ID, Vehicle-1M, VD1 and VD2.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322006203",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Classifier (UML)",
      "Computer science",
      "Convolutional neural network",
      "Discrete mathematics",
      "Discriminative model",
      "Distillation",
      "Embedding",
      "Machine learning",
      "Mathematics",
      "Open set",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Programming language",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Duan",
        "given_name": "Junxian"
      },
      {
        "surname": "Wu",
        "given_name": "Xiang"
      },
      {
        "surname": "Hu",
        "given_name": "Yibo"
      },
      {
        "surname": "Fu",
        "given_name": "Chaoyou"
      },
      {
        "surname": "Wang",
        "given_name": "Zi"
      },
      {
        "surname": "He",
        "given_name": "Ran"
      }
    ]
  },
  {
    "title": "Large motion anime head animation using a cascade pose transform network",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109181",
    "abstract": "We study the problem of talking head animation from a single image where a target anime talking head is generated to mimic the change of facial expression and head movement of source anime characters. Most existing methods focus on generating talking heads from real humans. However, few efforts have been made to create anime talking head. Compared with human head generation, the key challenges of anime head generation are: how to align the pose and facial expression of the target head with that of the source head without explicit facial landmarks. To address this, we propose CPTNetV2, a cascaded pose transform network that unifies face pose transformation and head pose transformation. At the core of CPTNetV2 is the implicit encoding of facial changes and head movement by a pose vector. Given the pose vector, we introduce a mask generator to animate facial expression (e.g., close eyes and open mouth) and a grid generator to simulate head movement, followed by a fusion module to generate talking heads. To tackle large displacement and improve the quality of generation, we further design a details inpainting module with pose vector decomposition to reduce the receptive field of network required for pose transformation. In particular, we collect an anime talking head dataset AniHead-2K that includes around 2000 anime characters with different face/head poses. Extensive experiments on AniHead-2K demonstrate that CPTNetV2 can achieve arbitrary pose transformation conditioned on the target pose vector and outperforms other state-of-the-art methods. We also verify the effectiveness of each module through ablative studies. Additional results show that CPTNetV2 has good generalization and is applicable to generate anime talking head even based on human videos. The dataset will be made available at: https://github.com/zhangjiale487/AniHead-2K.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322006604",
    "keywords": [
      "Absorption (acoustics)",
      "Acoustics",
      "Animation",
      "Anime",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer graphics (images)",
      "Computer science",
      "Computer vision",
      "Face (sociological concept)",
      "Facial expression",
      "Gene",
      "Generator (circuit theory)",
      "Geology",
      "Geomorphology",
      "Head (geology)",
      "Human head",
      "Physics",
      "Power (physics)",
      "Quantum mechanics",
      "Social science",
      "Sociology",
      "Transformation (genetics)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Jiale"
      },
      {
        "surname": "Liu",
        "given_name": "Chengxin"
      },
      {
        "surname": "Xian",
        "given_name": "Ke"
      },
      {
        "surname": "Cao",
        "given_name": "Zhiguo"
      }
    ]
  },
  {
    "title": "Automated labeling of training data for improved object detection in traffic videos by fine-tuned deep convolutional neural networks",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.01.015",
    "abstract": "The exponential increase in the use of technology in road management systems has led to real-time visual information in thousands of locations on road networks. A previous step in preventing or detecting accidents involves identifying vehicles on the road. The application of convolutional neural networks in object detection has significantly improved this field, enhancing classical computer vision techniques. Although, there are deficiencies due to the low detection rate provided by the available pre-trained models, especially for small objects. The main drawback is that they require manual labeling of the vehicles that appear in the images from each IP camera located on the road network to retrain the model. This task is not feasible if we have thousands of cameras distributed across the extensive road network of each nation or state. Our proposal presented a new automatic procedure for detecting small-scale objects in traffic sequences. In the first stage, vehicle patterns detected from a set of frames are generated automatically through an offline process, using super-resolution techniques and pre-trained object detection networks. Subsequently, the object detection model is retrained with the previously obtained data, adapting it to the analyzed scene. Finally, already online and in real-time, the retrained model is used in the rest of the traffic sequence or the video stream generated by the camera. This framework has been successfully tested on the NGSIM and the GRAM datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523000223",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Engineering",
      "Field (mathematics)",
      "Mathematics",
      "Object (grammar)",
      "Object detection",
      "Operating system",
      "Pattern recognition (psychology)",
      "Process (computing)",
      "Programming language",
      "Pure mathematics",
      "Set (abstract data type)",
      "Systems engineering",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "García-Aguilar",
        "given_name": "Iván"
      },
      {
        "surname": "García-González",
        "given_name": "Jorge"
      },
      {
        "surname": "Luque-Baena",
        "given_name": "Rafael Marcos"
      },
      {
        "surname": "López-Rubio",
        "given_name": "Ezequiel"
      }
    ]
  },
  {
    "title": "Cross-modal co-feedback cellular automata for RGB-T saliency detection",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109138",
    "abstract": "Saliency cellular automata (CA), a temporally evolving model to efficiently locate salient object, has achieved great progress in saliency detection task. However, most the previous CA models originate from RGB data and are thus limited in some extreme scenes. Inspired by the observation that thermal infrared data (T) can overcome the limitation of RGB data themselves in some cases and RGB-T saliency detection has gained more and more attention. In this paper, we contribute a novel RGB-T saliency detection approach via Cross-modal Co-feedback Cellular Automata (C 3 A). Before this, we firstly present a novel weighted background-based map (WBM) to give each superpixel(image patch) an initial saliency value. Then C 3 A is proposed to improve the quality of the WBM. Specifically, it firstly establishes two complementary cellular automata (CA) mechanisms dependent on RGB and thermal infrared data, which respectively refine the WBM based on two different perspectives. To bridge RGB-T modalities, an iterative cross-modal co-feedback framework is contributed to optimize constantly their results. In other words, we regard the result of one modality(RGB or T)-induced CA as important feedback to update and optimize another modality(T or RGB)-induced CA during the iteration. Two modalities constantly pull out the useful and confident data to the opposite side, and so two CAs’ results are constantly refined until a stable state is generated, we then integrate the results of two modalities-induced CAs into the saliency map. Finally, a novel boundary-guided pixel-wise refinement (BPR) technology is proposed to further overcome the influence of inaccurate superpixel segmentation to the C 3 A and refine the saliency map generated by our C 3 A. For fairness, the proposed method is compared with other state-of-the-art methods on three RGB-T datasets, experimental results show the superiority of our model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322006185",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Modality (human–computer interaction)",
      "Pattern recognition (psychology)",
      "Pixel",
      "RGB color model"
    ],
    "authors": [
      {
        "surname": "Pang",
        "given_name": "Yu"
      },
      {
        "surname": "Wu",
        "given_name": "Hao"
      },
      {
        "surname": "Wu",
        "given_name": "Chengdong"
      }
    ]
  },
  {
    "title": "Few-shot symbol classification via self-supervised learning and nearest neighbor",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.01.014",
    "abstract": "The recognition of symbols within document images is one of the most relevant steps involved in the Document Analysis field. While current state-of-the-art methods based on Deep Learning are capable of adequately performing this task, they generally require a vast amount of data that has to be manually labeled. In this paper, we propose a self-supervised learning-based method that addresses this task by training a neural-based feature extractor with a set of unlabeled documents and performs the recognition task considering just a few reference samples. Experiments on different corpora comprising music, text, and symbol documents report that the proposal is capable of adequately tackling the task with high accuracy rates of up to 95% in few-shot settings. Moreover, results show that the presented strategy outperforms the base supervised learning approaches trained with the same amount of data that, in some cases, even fail to converge. This approach, hence, stands as a lightweight alternative to deal with symbol classification with few annotated data.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552300020X",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Economics",
      "Engineering",
      "Extractor",
      "Feature (linguistics)",
      "Field (mathematics)",
      "Linguistics",
      "Machine learning",
      "Management",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Process engineering",
      "Programming language",
      "Pure mathematics",
      "Set (abstract data type)",
      "Supervised learning",
      "Symbol (formal)",
      "Task (project management)",
      "k-nearest neighbors algorithm"
    ],
    "authors": [
      {
        "surname": "Alfaro-Contreras",
        "given_name": "María"
      },
      {
        "surname": "Ríos-Vila",
        "given_name": "Antonio"
      },
      {
        "surname": "Valero-Mas",
        "given_name": "Jose J."
      },
      {
        "surname": "Calvo-Zaragoza",
        "given_name": "Jorge"
      }
    ]
  },
  {
    "title": "Age detection from handwriting using different feature classification models",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.02.001",
    "abstract": "Digitized handwritten documents have been used for various purposes, including age detection, a crucial area of research in fields like forensic investigation and medical diagnosis. Automated age recognition is deemed to be a difficult task, due to the great degree of similarity and overlap across people’s handwriting. Consequently, the efficiency of the classification system is determined by extracting pertinent features from handwritten documents. This research proposes a set of age-related features suggested by a graphologist to classify handwritten documents into two age groups: youth adult and mature adult. The extracted features are: slant irregularity (SI), pen pressure irregularity (PPI), text lines irregularity (TLI) and the percentage of black and white pixels (PWB). Support Vector Machines (SVM) and Neural Network (NN) classifiers have been used to train, validate and test the proposed approach using two different datasets: the FSHS and the Khatt datasets. When applied to the FSHS dtaset using SVM and NN approaches, the proposed method resulted in a classification accuracy of 71% and 63.5%, respectively. Meanwhile, when applied to the Khatt dataset, our method outperformed state-of-the-art methods with a classification accuracy of 65.2% and 67% utilising SVM and NN classifiers, respectively. These are the best rates available right now in this field.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523000272",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Feature (linguistics)",
      "Feature extraction",
      "Field (mathematics)",
      "Handwriting",
      "Handwriting recognition",
      "Image (mathematics)",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Pure mathematics",
      "Set (abstract data type)",
      "Similarity (geometry)",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "AL-Qawasmeh",
        "given_name": "Najla"
      },
      {
        "surname": "Khayyat",
        "given_name": "Muna"
      },
      {
        "surname": "Suen",
        "given_name": "Ching Y."
      }
    ]
  },
  {
    "title": "Real-time siamese multiple object tracker with enhanced proposals",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109141",
    "abstract": "Maintaining the identity of multiple objects in real-time video is a challenging task, as it is not always feasible to run a detector on every frame. Thus, motion estimation systems are often employed, which either do not scale well with the number of targets or produce features with limited semantic information. To solve the aforementioned problems and allow the tracking of dozens of arbitrary objects in real-time, we propose SiamMOTION. SiamMOTION includes a novel proposal engine that produces quality features through an attention mechanism and a region-of-interest extractor fed by an inertia module and powered by a feature pyramid network. Finally, the extracted tensors enter a comparison head that efficiently matches pairs of exemplars and search areas, generating quality predictions via a pairwise depthwise region proposal network and a multi-object penalization module. SiamMOTION has been validated on five public benchmarks, achieving leading performance against current state-of-the-art trackers. Code available at: https://www.github.com/lorenzovaquero/SiamMOTION",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322006215",
    "keywords": [
      "Acoustics",
      "Artificial intelligence",
      "BitTorrent tracker",
      "Code (set theory)",
      "Computer science",
      "Computer vision",
      "Economics",
      "Epistemology",
      "Eye tracking",
      "Feature (linguistics)",
      "Frame (networking)",
      "Identity (music)",
      "Linguistics",
      "Management",
      "Object (grammar)",
      "Optics",
      "Pairwise comparison",
      "Philosophy",
      "Physics",
      "Programming language",
      "Pyramid (geometry)",
      "Quality (philosophy)",
      "Set (abstract data type)",
      "Task (project management)",
      "Telecommunications",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Vaquero",
        "given_name": "Lorenzo"
      },
      {
        "surname": "Brea",
        "given_name": "Víctor M."
      },
      {
        "surname": "Mucientes",
        "given_name": "Manuel"
      }
    ]
  },
  {
    "title": "Multi-head second-order pooling for graph transformer networks",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.01.017",
    "abstract": "Graph transformer networks (GTNs) have great potential in graph-related tasks, particularly graph classification. GTNs use self-attention mechanism to extract both semantic and structural information, after which a class token is used as the global representation for graph classification.However, the class token completely abandons all information held by nodes, which leads to a huge loss of information and neglects the dependencies among different feature dimensions. Therefore, we propose a Multi-head Global Second-Order Pooling (MGSOP) to utilize the rich second-order statistics of all nodes. To this end, a second-order pooling module incorporated with multi-head structure generates multiple covariance representations for GTNs. Then, the first-order representation and concatenation of all covariance matrices are combined by an adaptive fusion module for better performance on different types of data. Furthermore, we introduce an efficient dropout-based normalization to utilize the geometry of covariance representation. Experiments conducted on six real-world graph benchmarks show our MGSOP outperforms its counterparts and achieves competitive results against state-of-the-arts.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523000235",
    "keywords": [
      "Algorithm",
      "Anthropology",
      "Artificial intelligence",
      "Computer science",
      "Covariance",
      "Data mining",
      "Graph",
      "Mathematics",
      "Normalization (sociology)",
      "Pattern recognition (psychology)",
      "Pooling",
      "Sociology",
      "Statistics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Dong",
        "given_name": "Zhe"
      },
      {
        "surname": "Wang",
        "given_name": "Qilong"
      },
      {
        "surname": "Zhu",
        "given_name": "Pengfei"
      }
    ]
  },
  {
    "title": "Event-driven daily activity recognition with enhanced emergent modeling",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109149",
    "abstract": "With the population aging, elderly health monitoring is triggering more studies on daily activity recognition as the fundamental of ambient assisted living. It is remarkable that activity recognition remains difficulties including how to adequately extract feature structure and settle the issue of activity confusion. To address these challenges, we propose a novel activity modeling method under the emergent paradigm with marker-based stigmergy and the directed-weighted network with additional context-aware information. In the modeling process, stigmergy is first introduced to aggregate the context information at the low level for generating activity pheromone trails, and then the constructed stigmergic trails are represented in form of directed-weighted network with distinguishability of individual pheromone source corresponding to location. The potential advantage is that the robust trails with distinguishable individual initial positions are feasible to supplement user’s daily habits and thus both inter-class and intra-class distances can be kept at acceptable levels. Experiments on Aruba demonstrates that the proposed emergent modeling method can effectively deal with the problems of feature extraction and activity ambiguity and achieve good classification performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322006288",
    "keywords": [
      "Activity recognition",
      "Artificial intelligence",
      "Computer science",
      "Event (particle physics)",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Zimin"
      },
      {
        "surname": "Wang",
        "given_name": "Guoli"
      },
      {
        "surname": "Guo",
        "given_name": "Xuemei"
      }
    ]
  },
  {
    "title": "DuaFace: Data uncertainty in angular based loss for face recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.01.013",
    "abstract": "The Data Uncertainty inherently existed in feature continuous mapping space itself and the training dataset. In this paper, a general loss function DuaFace based on Data Uncertainty and Angular/cosine-margin-based loss is proposed to study the influence of Data Uncertainty on traditional Angular based loss functions. Correspondingly, insightful analysis on how incorporating Data Uncertainty estimation helps reducing the adverse effects of noisy samples and affects the process of feature learning are also provided. Moreover, extensive experiments conducted on Face Recognition demonstrate its superiority over state-of-the-arts.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523000193",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Data mining",
      "Evolutionary biology",
      "Face (sociological concept)",
      "Facial recognition system",
      "Feature (linguistics)",
      "Feature vector",
      "Function (biology)",
      "Geometry",
      "Linguistics",
      "Machine learning",
      "Margin (machine learning)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Social science",
      "Sociology",
      "Trigonometric functions"
    ],
    "authors": [
      {
        "surname": "Jiang",
        "given_name": "Fazhen"
      },
      {
        "surname": "Yang",
        "given_name": "Xiaoyuan"
      },
      {
        "surname": "Ren",
        "given_name": "Huwei"
      },
      {
        "surname": "Li",
        "given_name": "Zhengze"
      },
      {
        "surname": "Shen",
        "given_name": "Kangqing"
      },
      {
        "surname": "Jiang",
        "given_name": "Jin"
      },
      {
        "surname": "Li",
        "given_name": "Yixiao"
      }
    ]
  },
  {
    "title": "Automatic generation of scientific papers for data augmentation in document layout analysis",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.01.018",
    "abstract": "Document layout analysis is an important task to extract information from scientific literature. Deep-learning solutions for document layout analysis require large collections of training data that are not always available. We generate a large number of synthetic pages to subsequently train a neural network to perform document object detection. The proposed pipeline allows users to deal with less common layouts for which it is not easy to find large annotated datasets. High-quality annotations for a small collection of papers are obtained through a semi-automatic approach. Then, a generative model, based on LayoutTransformer, is used to generate plausible layouts that are subsequently populated with random information to perform data augmentation. We evaluate the proposed method considering scientific articles with two different types of layouts: double and single columns. For double-column papers, we improve detection by 1% starting from 385 manually annotated scientific articles. For single-column papers, we improve detection by 49% starting from 218 articles.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523000247",
    "keywords": [
      "Artificial intelligence",
      "Column (typography)",
      "Computer science",
      "Data mining",
      "Economics",
      "Frame (networking)",
      "Information retrieval",
      "Management",
      "Pattern recognition (psychology)",
      "Pipeline (software)",
      "Programming language",
      "Task (project management)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Pisaneschi",
        "given_name": "Lorenzo"
      },
      {
        "surname": "Gemelli",
        "given_name": "Andrea"
      },
      {
        "surname": "Marinai",
        "given_name": "Simone"
      }
    ]
  },
  {
    "title": "Prototype-Guided Feature Learning for Unsupervised Domain Adaptation",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109154",
    "abstract": "Unsupervised Domain Adaptation transfers knowledge from the source domain to the target domain. It makes remarkable progress in alleviating the label-shortage problem in machine learning. Existing methods focus on aligning the two domain distributions directly. However, due to domain discrepancy, there may be some samples in the source domain being unnecessary or even harmful to the target tasks. Avoiding transferring knowledge from these samples is crucial. Existing researches are limited in this area. To this end, we propose a new unsupervised domain adaptation approach named the prototype-guided feature learning. The proposed method contains three main innovations. Firstly, we propose to utilize the more representative source-domain samples, class prototypes, to learn a domain-invariant subspace with the target samples. Secondly, the modified nearest class prototype method is proposed to predict the target samples by exploiting the structural information of the target domain efficiently. Thirdly, a multi-stage label filtering method is proposed to alleviate the mislabeling problem during training. Extensive experiments manifest that our method is competitive compared to the current mainstream unsupervised domain adaptive methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322006331",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Domain (mathematical analysis)",
      "Domain adaptation",
      "Feature (linguistics)",
      "Focus (optics)",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Subspace topology",
      "Unsupervised learning"
    ],
    "authors": [
      {
        "surname": "Du",
        "given_name": "Yongjie"
      },
      {
        "surname": "Zhou",
        "given_name": "Deyun"
      },
      {
        "surname": "Xie",
        "given_name": "Yu"
      },
      {
        "surname": "Lei",
        "given_name": "Yu"
      },
      {
        "surname": "Shi",
        "given_name": "Jiao"
      }
    ]
  },
  {
    "title": "Soft precision and recall",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.02.005",
    "abstract": "Precision and recall are classical measures used in machine learning. However, they are based on exact matching. This results in binary classification where the predicted item is either a true or false positive despite inexact matching is often preferred in pattern recognition. To address this problem, we introduce soft variants of precision and recall based on application-specific similarity measure. 2022 Elsevier Ltd. All rights reserved.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523000296",
    "keywords": [
      "Arithmetic",
      "Artificial intelligence",
      "Binary classification",
      "Binary number",
      "Cognitive psychology",
      "Computer science",
      "Data mining",
      "Image (mathematics)",
      "Machine learning",
      "Matching (statistics)",
      "Mathematics",
      "Measure (data warehouse)",
      "Pattern recognition (psychology)",
      "Precision and recall",
      "Psychology",
      "Recall",
      "Similarity (geometry)",
      "Statistics",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Fränti",
        "given_name": "Pasi"
      },
      {
        "surname": "Mariescu-Istodor",
        "given_name": "Radu"
      }
    ]
  },
  {
    "title": "Shape robustness in style enhanced cross domain semantic segmentation",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109143",
    "abstract": "This paper focuses on domain adaptation method based on style transfer. Previous methods based on style transfer pay attention to the transformation of texture features between domains and maintain semantic consistency to the greatest extent. However, these methods have different effects on domain gaps in different types of categories. The categories with large texture difference and small structure difference can be improved better. For the categories with small texture difference and large structure difference, it causes negative transfer. In this paper, a shape robustness enhanced domain adaptive segmentation algorithm is proposed. Firstly, we adopt adjustable style transfer methods to enhance the style diversity of source domain images. Next, we differentiated different types of image features to weaken the negative transfer in the process of adversarial training. The results of this paper on general data sets GTA5 and SYNTHIA are better than other style transfer methods. Further experiments show that we improve the shape robustness of style enhancement method in domain adaptive segmentation task.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322006161",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Classifier (UML)",
      "Computer science",
      "Computer vision",
      "Domain adaptation",
      "Gene",
      "Geography",
      "Image segmentation",
      "Pattern recognition (psychology)",
      "Robustness (evolution)",
      "Segmentation",
      "Style (visual arts)"
    ],
    "authors": [
      {
        "surname": "Zhu",
        "given_name": "Siyu"
      },
      {
        "surname": "Tian",
        "given_name": "Yingjie"
      }
    ]
  },
  {
    "title": "Exploring modality-shared appearance features and modality-invariant relation features for cross-modality person Re-IDentification",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109145",
    "abstract": "Most existing cross-modality person Re-IDentification works rely on discriminative modality-shared features for reducing cross-modality variations and intra-modality variations. Despite their preliminary success, such modality-shared appearance features cannot capture enough modality-invariant discriminative information due to a massive discrepancy between RGB and IR images. To address this issue, on top of appearance features, we further capture the modality-invariant relations among different person parts (referred to as modality-invariant relation features), which help to identify persons with similar appearances but different body shapes. To this end, a Multi-level Two-streamed Modality-shared Feature Extraction (MTMFE) sub-network is designed, where the modality-shared appearance features and modality-invariant relation features are first extracted in a shared 2D feature space and a shared 3D feature space, respectively. The two features are then fused into the final modality-shared features such that both cross-modality variations and intra-modality variations can be reduced. Besides, a novel cross-modality center alignment loss is proposed to further reduce the cross-modality variations. Experimental results on several benchmark datasets demonstrate that our proposed method exceeds state-of-the-art algorithms by a wide margin.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322006240",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Discriminative model",
      "Feature (linguistics)",
      "Invariant (physics)",
      "Linguistics",
      "Mathematical physics",
      "Mathematics",
      "Modality (human–computer interaction)",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Nianchang"
      },
      {
        "surname": "Liu",
        "given_name": "Jianan"
      },
      {
        "surname": "Luo",
        "given_name": "Yongjiang"
      },
      {
        "surname": "Zhang",
        "given_name": "Qiang"
      },
      {
        "surname": "Han",
        "given_name": "Jungong"
      }
    ]
  },
  {
    "title": "Unsupervised Feature Selection via Neural Networks and Self-Expression with Adaptive Graph Constraint",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109173",
    "abstract": "Unsupervised feature selection (UFS), which selects the most important feature subset and eliminates the unnecessary information for the upcoming data analysis, is a significant problem in machine learning and has been explored for years. Most UFS methods map features into a pseudo label space by multiplying a projection matrix constrained with sparsity to learn the mapping from the features to the labels. However, the mapping relationship is usually not linear, and linear regression may result in a suboptimal selection. To address this issue, we propose a novel UFS method, called neural networks embedded self-expression (NNSE). NNSE replaces the linear regression of traditional spectral analysis methods with neural networks to learn the pseudo label space. Besides, we embed neural networks into the self-expression model to improve the representative ability by preserving the local structure with an adaptive graph regularization module. Then we propose an efficient alternative iterative algorithm to solve the proposed model. Experimental results on 8 public datasets show NNSE outperforms the other state-of-the-art methods. Moreover, experimental results are also presented to show the convergence of the proposed method. The source code is available at: https://github.com/misteru/NNSE.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322006525",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Constraint (computer-aided design)",
      "Feature (linguistics)",
      "Feature selection",
      "Feature vector",
      "Geometry",
      "Graph",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Regularization (linguistics)",
      "Source code",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "You",
        "given_name": "Mengbo"
      },
      {
        "surname": "Yuan",
        "given_name": "Aihong"
      },
      {
        "surname": "He",
        "given_name": "Dongjian"
      },
      {
        "surname": "Li",
        "given_name": "Xuelong"
      }
    ]
  },
  {
    "title": "Batch normalization embeddings for deep domain generalization",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109115",
    "abstract": "Domain generalization aims at training machine learning models to perform robustly across different and unseen domains. Several methods train models from multiple datasets to extract domain-invariant features, hoping to generalize to unseen domains. Instead, first we explicitly train domain-dependent representations leveraging ad-hoc batch normalization layers to collect independent domain’s statistics. Then, we propose to use these statistics to map domains in a shared latent space, where membership to a domain is measured by means of a distance function. At test time, we project samples from an unknown domain into the same space and infer properties of their domain as a linear combination of the known ones. We apply the same mapping strategy at training and test time, learning both a latent representation and a powerful but lightweight ensemble model. We show a significant increase in classification accuracy over current state-of-the-art techniques on popular domain generalization benchmarks: PACS, Office-31 and Office-Caltech.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322005957",
    "keywords": [
      "Anthropology",
      "Artificial intelligence",
      "Computer science",
      "Domain (mathematical analysis)",
      "Embedding",
      "Ensemble learning",
      "Generalization",
      "Invariant (physics)",
      "Law",
      "Machine learning",
      "Mathematical analysis",
      "Mathematical physics",
      "Mathematics",
      "Normalization (sociology)",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Programming language",
      "Representation (politics)",
      "Sociology",
      "Test data"
    ],
    "authors": [
      {
        "surname": "Segu",
        "given_name": "Mattia"
      },
      {
        "surname": "Tonioni",
        "given_name": "Alessio"
      },
      {
        "surname": "Tombari",
        "given_name": "Federico"
      }
    ]
  },
  {
    "title": "A Novel Label Enhancement Algorithm Based on Manifold Learning",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109189",
    "abstract": "We propose a label enhancement model to solve the multi-label learning (MLL) problem by using the incremental subspace learning to enrich the label space and to improve the ability of label recognition. In particular, we use the incremental estimation of the feature function representing the manifold structure to guide the construction of the label space and to transform the local topology from the feature space to the label space. First, we build a recursive form for incremental estimation of the feature function representing the feature space information. Second, the label propagation is used to obtain the hidden supervisory information of labels in the data. Finally, an enhanced maximum entropy model based on conditional random field is established as the objective, to obtain the predicted label distribution. The enriched label information in the manifold space obtained in first step and the estimated label distributions provided in second step are employed to train this enhanced maximum entropy model by a gradient-descent iterative optimization to obtain the label distribution predictor’s parameters with enhanced accuracy. We evaluate our method on 24 real-world datasets. Experimental results demonstrate that our label enhancement manifold learning model has advantages in predictive performance over the latest MLL methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322006689",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Conditional probability distribution",
      "Conditional random field",
      "Dimensionality reduction",
      "Engineering",
      "Entropy (arrow of time)",
      "Feature (linguistics)",
      "Feature vector",
      "Gradient descent",
      "Linguistics",
      "Manifold (fluid mechanics)",
      "Mathematics",
      "Mechanical engineering",
      "Nonlinear dimensionality reduction",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Statistics",
      "Subspace topology",
      "k-nearest neighbors algorithm"
    ],
    "authors": [
      {
        "surname": "Tan",
        "given_name": "Chao"
      },
      {
        "surname": "Chen",
        "given_name": "Sheng"
      },
      {
        "surname": "Geng",
        "given_name": "Xin"
      },
      {
        "surname": "Ji",
        "given_name": "Genlin"
      }
    ]
  },
  {
    "title": "Uncertainty-Aware Scene Graph Generation",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2022.12.011",
    "abstract": "As a challenging task in computer vision, Scene graph generation (SGG) aims to model the underlying semantic relationships among objects in a given image for scene understanding. Due to the increasing scale and subjectivity, the annotations of existing SGG benchmarks inevitably suffer from some uncertainty issues, resulting in the models hardly learning the relationships comprehensively. In this work, we address the uncertainty from the perspectives of both classifier parameters and relationship labels. On one hand, we handle the classifier uncertainty via learning a Bayesian classifier reparameterization, of which the weights are sampled from a latent space spanned with a prior distribution. On the other hand, we assume that each relationship label is sampled from a latent label space and mitigate the label uncertainty via estimating the latent relationship distribution. As a result, the distribution of the classifier parameters are comprehensively learned under the supervision of the estimated relationship labels, thus improving the model’s generalization ability. Experimental results on the popular benchmark demonstrate that the proposed strategies significantly improve different baseline models on different SGG tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522003798",
    "keywords": [
      "Artificial intelligence",
      "Bayesian probability",
      "Classifier (UML)",
      "Computer science",
      "Graph",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Prior probability",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Xuewei"
      },
      {
        "surname": "Wu",
        "given_name": "Tao"
      },
      {
        "surname": "Zheng",
        "given_name": "Guangcong"
      },
      {
        "surname": "Yu",
        "given_name": "Yunlong"
      },
      {
        "surname": "Li",
        "given_name": "Xi"
      }
    ]
  },
  {
    "title": "Semantic-aware self-supervised depth estimation for stereo 3D detection",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.02.006",
    "abstract": "Besides the 3D object supervision, the auxiliary disparity supervision is usually indispensable when training a stereo-based 3D object detector. The disparity supervision is either transformed from LiDAR points or generated from pre-trained models. However, the former suffers from the high cost and over-sensitivity to airborne particles of LiDAR devices, and the latter from the limited cross-dataset transferability of contemporary stereo matching models. To alleviate those problems, we propose a self-supervision framework for stereo-based 3D detection that relies on neither LiDARs nor external models. A Depth-based Self-supervision (DSelf) is proposed to unify the coordinate spaces of self-supervised losses and detection into a 3D space. However, the DSelf supervision is dense compared with the sparse LiDAR points, which introduces redundancy and irrelevancy into the stereo features. A Semantic-Aware Sampler (SASampler) is proposed to address the problems by an unbalanced sampling of foreground and background pixels. Combining our SASampler and DSelf supervision, the resultant detector (named S3D) achieves state-of-the-art detection results without explicit disparity supervisions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523000326",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Detector",
      "Geography",
      "Lidar",
      "Logit",
      "Machine learning",
      "Matching (statistics)",
      "Mathematics",
      "Object detection",
      "Operating system",
      "Pattern recognition (psychology)",
      "Pixel",
      "Redundancy (engineering)",
      "Remote sensing",
      "Statistics",
      "Stereo cameras",
      "Stereopsis",
      "Telecommunications",
      "Transferability"
    ],
    "authors": [
      {
        "surname": "Sun",
        "given_name": "Hanqing"
      },
      {
        "surname": "Cao",
        "given_name": "Jiale"
      },
      {
        "surname": "Pang",
        "given_name": "Yanwei"
      }
    ]
  },
  {
    "title": "The edge-preservation similarity for comparing rooted, unordered, node-labeled trees",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.02.017",
    "abstract": "Rooted trees are ubiquitous data structures which are used to model hierarchical objects from a plethora of different application domains. For various downstream analysis tasks, measures are needed that quantify (dis-)similarity between rooted trees. Many such measures exist, e. g., the widely used tree edit distance (TED). However, there are few algorithms to compute (dis-)similarity measures which are specifically designed for rooted, unordered, node-labeled trees and support input trees of different orders. To close this gap in the literature, we introduce the edge-preservation similarity (EPS). We show how to exactly compute EPS via integer quadratic programming on small instances and present a scalable 4-approximation algorithm. An evaluation on tree representations of pseudoknotted RNA secondary structures and acyclic molecular graphs shows that both exact and approximate (normalized) EPS better preserves functional similarities between the compared RNAs and molecules than the often-used TED. Python implementations of our algorithms and scripts to reproduce the results are available on GitHub: https://github.com/bionetslab/edge-preservation-similarity.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552300048X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Database",
      "Edit distance",
      "Engineering",
      "Enhanced Data Rates for GSM Evolution",
      "Image (mathematics)",
      "Mathematics",
      "Node (physics)",
      "Programming language",
      "Python (programming language)",
      "Scalability",
      "Scripting language",
      "Similarity (geometry)",
      "Structural engineering",
      "Theoretical computer science",
      "Tree (set theory)"
    ],
    "authors": [
      {
        "surname": "Boria",
        "given_name": "Nicolas"
      },
      {
        "surname": "Kiederle",
        "given_name": "Jana"
      },
      {
        "surname": "Yger",
        "given_name": "Florian"
      },
      {
        "surname": "Blumenthal",
        "given_name": "David B."
      }
    ]
  },
  {
    "title": "Fast subspace clustering by learning projective block diagonal representation",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109152",
    "abstract": "Block Diagonal Representation (BDR) has attracted massive attention in subspace clustering, yet the high computational cost limits its widespread application. To address this issue, we propose a novel approach called Projective Block Diagonal Representation (PBDR), which rapidly pursues a representation matrix with the block diagonal structure. Firstly, an effective sampling strategy is utilized to select a small subset of the original large-scale data. Then, we learn a projection mapping to match the block diagonal representation matrix on the selected subset. After training, we employ the learned projection mapping to quickly generate the representation matrix with an ideal block diagonal structure for the original large-scale data. Additionally, we further extend the proposed PBDR model ( i . e . , PBDR ℓ 1 and PBDR * ) by capturing the global or local structure of the data to enhance block diagonal coding capability. This paper also proves the effectiveness of the proposed model theoretically. Especially, this is the first work to directly learn a representation matrix with a block diagonal structure to handle the large-scale subspace clustering problem. Finally, experimental results on publicly available datasets show that our approaches achieve faster and more accurate clustering results compared to the state-of-the-art block diagonal-based subspace clustering approaches, which demonstrates its practical usefulness.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322006318",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Block matrix",
      "Cluster analysis",
      "Combinatorics",
      "Computer science",
      "Diagonal",
      "Eigenvalues and eigenvectors",
      "Geometry",
      "Law",
      "Linear subspace",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Political science",
      "Politics",
      "Projection (relational algebra)",
      "Quantum mechanics",
      "Representation (politics)",
      "Subspace topology"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Yesong"
      },
      {
        "surname": "Chen",
        "given_name": "Shuo"
      },
      {
        "surname": "Li",
        "given_name": "Jun"
      },
      {
        "surname": "Xu",
        "given_name": "Chunyan"
      },
      {
        "surname": "Yang",
        "given_name": "Jian"
      }
    ]
  },
  {
    "title": "Mix-ViT: Mixing attentive vision transformer for ultra-fine-grained visual categorization",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109131",
    "abstract": "Ultra-fine-grained visual categorization (ultra-FGVC) moves down the taxonomy level to classify sub-granularity categories of fine-grained objects. This inevitably poses a challenge, i.e., classifying highly similar objects with limited samples, which impedes the performance of recent advanced vision transformer methods. To that end, this paper introduces Mix-ViT, a novel mixing attentive vision transformer to address the above challenge towards improved ultra-FGVC. The core design is a self-supervised module that mixes the high-level sample tokens and learns to predict whether a token has been substituted after attentively substituting tokens. This drives the model to understand the contextual discriminative details among inter-class samples. Via incorporating such a self-supervised module, the network gains more knowledge from the intrinsic structure of input data and thus improves generalization capability with limited training sample. The proposed Mix-ViT achieves competitive performance on seven publicly available datasets, demonstrating the potential of vision transformer compared to CNN for the first time in addressing the challenging ultra-FGVC tasks. The code is available at https://github.com/Markin-Wang/MixViT",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322006112",
    "keywords": [
      "Artificial intelligence",
      "Categorization",
      "Computer science",
      "Computer security",
      "Discriminative model",
      "Electrical engineering",
      "Engineering",
      "Granularity",
      "Machine learning",
      "Operating system",
      "Pattern recognition (psychology)",
      "Security token",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Yu",
        "given_name": "Xiaohan"
      },
      {
        "surname": "Wang",
        "given_name": "Jun"
      },
      {
        "surname": "Zhao",
        "given_name": "Yang"
      },
      {
        "surname": "Gao",
        "given_name": "Yongsheng"
      }
    ]
  },
  {
    "title": "Robust low tubal rank tensor completion via factor tensor norm minimization",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109169",
    "abstract": "Recent research has demonstrated that low tubal rank recovery based on tensor has received extensive attention. In this correspondence, we define tensor double nuclear norm and tensor Frobenius/nuclear hybrid norm to induce a surrogate for tensor tubal rank, and prove that they are equivalent to tensor Schatten- p norm for p = 1 / 2 and p = 2 / 3 . Based on the definition, we propose two novel tractable tensor completion models called Double Nuclear norm regularized Tensor Completion (DNTC) and Frobenius/Nuclear hybrid norm regularized Tensor Completion (FNTC) by integrating these two norm minimization and factorization methods into a joint learning framework. Furthermore, we adopt invertible linear transforms to obtain low tubal rank tensors, which makes the model more flexible and effective. Two efficient algorithms are designed to solve the proposed tensor completion models by incorporating the convexity of the factor norms. Comprehensive experiments are conducted on synthetic and real datasets to achieve better results in comparison with some state-of-the-art approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322006483",
    "keywords": [
      "Algebra over a field",
      "Combinatorics",
      "Eigenvalues and eigenvectors",
      "Invertible matrix",
      "Law",
      "Mathematical optimization",
      "Mathematics",
      "Matrix norm",
      "Norm (philosophy)",
      "Physics",
      "Political science",
      "Pure mathematics",
      "Quantum mechanics",
      "Rank (graph theory)",
      "Tensor (intrinsic definition)"
    ],
    "authors": [
      {
        "surname": "Jiang",
        "given_name": "Wei"
      },
      {
        "surname": "Zhang",
        "given_name": "Jun"
      },
      {
        "surname": "Zhang",
        "given_name": "Changsheng"
      },
      {
        "surname": "Wang",
        "given_name": "Lijun"
      },
      {
        "surname": "Qi",
        "given_name": "Heng"
      }
    ]
  },
  {
    "title": "JRA-Net: Joint representation attention network for correspondence learning",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109180",
    "abstract": "In this paper, we propose a Joint Representation Attention Network (JRA-Net), an end-to-end network, to establish reliable correspondences for image pairs. The initial correspondences generated by the local feature descriptor usually suffer from heavy outliers, which makes the network unable to learn a powerful enough representation for distinguishing inliers and outliers. To this end, we design a novel attention mechanism. The proposed attention mechanism not only takes into account the correlations between global context and geometric information, but also introduces the joint representation of different scales to suppress trivial correspondences and highlight crucial correspondences. In addition, to improve the generalization ability of attention mechanism, we present an innovative weight function, to effectively adjust the importance of the attention mechanism in a learning manner. Finally, by combining the above components, the proposed JRA-Net is able to effectively infer the probabilities of correspondences being inliers. Empirical experiments on challenging datasets demonstrate the effectiveness and generalization of JRA-Net. We achieve remarkable improvements compared with the current state-of-the-art approaches on outlier rejection and relative pose estimation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322006598",
    "keywords": [
      "Architectural engineering",
      "Artificial intelligence",
      "Attention network",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Engineering",
      "Feature (linguistics)",
      "Feature learning",
      "Generalization",
      "Geometry",
      "Joint (building)",
      "Law",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Net (polyhedron)",
      "Outlier",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Shi",
        "given_name": "Ziwei"
      },
      {
        "surname": "Xiao",
        "given_name": "Guobao"
      },
      {
        "surname": "Zheng",
        "given_name": "Linxin"
      },
      {
        "surname": "Ma",
        "given_name": "Jiayi"
      },
      {
        "surname": "Chen",
        "given_name": "Riqing"
      }
    ]
  },
  {
    "title": "Self-Supervised Leaf Segmentation under Complex Lighting Conditions",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109021",
    "abstract": "As an essential prerequisite task in image-based plant phenotyping, leaf segmentation has garnered increasing attention in recent years. While self-supervised learning is emerging as an effective alternative to various computer vision tasks, its adaptation for image-based plant phenotyping remains rather unexplored. In this work, we present a self-supervised leaf segmentation framework consisting of a self-supervised semantic segmentation model, a color-based leaf segmentation algorithm, and a self-supervised color correction model. The self-supervised semantic segmentation model groups the semantically similar pixels by iteratively referring to the self-contained information, allowing the pixels of the same semantic object to be jointly considered by the color-based leaf segmentation algorithm for identifying the leaf regions. Additionally, we propose to use a self-supervised color correction model for images taken under complex illumination conditions. Experimental results on datasets of different plant species demonstrate the potential of the proposed self-supervised framework in achieving effective and generalizable leaf segmentation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322005015",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer vision",
      "Image segmentation",
      "Machine learning",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Pixel",
      "Scale-space segmentation",
      "Segmentation",
      "Segmentation-based object categorization",
      "Supervised learning"
    ],
    "authors": [
      {
        "surname": "Lin",
        "given_name": "Xufeng"
      },
      {
        "surname": "Li",
        "given_name": "Chang-Tsun"
      },
      {
        "surname": "Adams",
        "given_name": "Scott"
      },
      {
        "surname": "Kouzani",
        "given_name": "Abbas Z."
      },
      {
        "surname": "Jiang",
        "given_name": "Richard"
      },
      {
        "surname": "He",
        "given_name": "Ligang"
      },
      {
        "surname": "Hu",
        "given_name": "Yongjian"
      },
      {
        "surname": "Vernon",
        "given_name": "Michael"
      },
      {
        "surname": "Doeven",
        "given_name": "Egan"
      },
      {
        "surname": "Webb",
        "given_name": "Lawrence"
      },
      {
        "surname": "Mcclellan",
        "given_name": "Todd"
      },
      {
        "surname": "Guskich",
        "given_name": "Adam"
      }
    ]
  },
  {
    "title": "PLFace: Progressive Learning for Face Recognition with Mask Bias",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109142",
    "abstract": "The outbreak of the COVID-19 coronavirus epidemic has promoted the development of masked face recognition (MFR). Nevertheless, the performance of regular face recognition is severely compromised when the MFR accuracy is blindly pursued. More facts indicate that MFR should be regarded as a mask bias of face recognition rather than an independent task. To mitigate mask bias, we propose a novel Progressive Learning Loss (PLFace) that achieves a progressive training strategy for deep face recognition to learn balanced performance for masked/mask-free faces recognition based on margin losses. Particularly, our PLFace adaptively adjusts the relative importance of masked and mask-free samples during different training stages. In the early stage of training, PLFace mainly learns the feature representations of mask-free samples. At this time, the regular sample embeddings shrink to the corresponding prototype, which represents the center of each class while being stored in the last linear layer. In the later stage of training, PLFace converges on mask-free samples and further focuses on masked samples until the masked sample embeddings are also gathered in the center of the class. The entire training process emphasizes the paradigm that normal samples shrink first and masked samples gather afterward. Extensive experimental results on popular regular and masked face benchmarks demonstrate that our proposed PLFace can effectively eliminate mask bias in face recognition. Compared to state-of-the-art competitors, PLFace significantly improves the accuracy of MFR while maintaining the performance of normal face recognition.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322006227",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Coronavirus disease 2019 (COVID-19)",
      "Disease",
      "Face (sociological concept)",
      "Face masks",
      "Facial recognition system",
      "Feature (linguistics)",
      "Infectious disease (medical specialty)",
      "Linguistics",
      "Machine learning",
      "Margin (machine learning)",
      "Medicine",
      "Operating system",
      "Pathology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Process (computing)",
      "Sample (material)",
      "Social science",
      "Sociology",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Baojin"
      },
      {
        "surname": "Wang",
        "given_name": "Zhongyuan"
      },
      {
        "surname": "Wang",
        "given_name": "Guangcheng"
      },
      {
        "surname": "Jiang",
        "given_name": "Kui"
      },
      {
        "surname": "Han",
        "given_name": "Zhen"
      },
      {
        "surname": "Lu",
        "given_name": "Tao"
      },
      {
        "surname": "Liang",
        "given_name": "Chao"
      }
    ]
  },
  {
    "title": "Combining Deep Denoiser and Low-rank Priors for Infrared Small Target Detection",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109184",
    "abstract": "Many existing low-rank methods have achieved good detection performance in uniform scenes, but they suffer from a high false alarm rate in complex noisy scenes. Therefore, it is important to improve the detection performance of low-rank models in noisy scenes. In this paper, we first formulate an implicit regularizer by plugging a denoising neural network (termed as deep denoiser), which can learn deep image priors from a large number of natural images. Then, we use the weighted sum of weighted tensor nuclear norm for more accurate background estimation. Finally, alternating direction multiplier method is used to solve the model under the plug-and-play framework. By integrating low-rank prior with deep denoiser prior, our model achieves higher accuracy. Experiments on different scenes demonstrate that our method achieves an improved performance in terms of visual effects and quantitative metrics. Specially, the overall accuracy of AUC value ( AU C OA ) achieved by the proposed method on Sequences 1-6 are 1.24 % , 1.16 % , 0.63 % , 1.9 % , 0.82 % , 2.06 % higher than those achieved by the second top performing methods, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032200663X",
    "keywords": [
      "Artificial intelligence",
      "Bayesian probability",
      "Combinatorics",
      "Computer science",
      "Constant false alarm rate",
      "Mathematics",
      "Noise reduction",
      "Pattern recognition (psychology)",
      "Prior probability",
      "Rank (graph theory)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Ting"
      },
      {
        "surname": "Yin",
        "given_name": "Qian"
      },
      {
        "surname": "Yang",
        "given_name": "Jungang"
      },
      {
        "surname": "Wang",
        "given_name": "Yingqian"
      },
      {
        "surname": "An",
        "given_name": "Wei"
      }
    ]
  },
  {
    "title": "M 2 RNet: Multi-modal and multi-scale refined network for RGB-D salient object detection",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109139",
    "abstract": "Salient object detection is a fundamental topic in computer vision, which has promising application prospects. The previous methods based on RGB-D may potentially suffer from the incompatibility of multi-modal feature fusion and the insufficiency of multi-scale feature aggregation. To tackle these two dilemmas, we propose a novel multi-modal and multi-scale refined network (M 2 RNet). Specifically, three essential components are presented in this network. The nested dual attention module (NDAM) explicitly exploits the combined features of RGB and depth flows. The adjacent interactive aggregation module (AIAM) gradually integrates the neighbor features of high, middle and low levels. The joint hybrid optimization loss (JHOL) makes the predictions have a prominent outline. Extensive experiments quantitatively and qualitatively demonstrate that our method outperforms other state-of-the-art approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322006197",
    "keywords": [
      "Algorithm",
      "Architectural engineering",
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Engineering",
      "Feature (linguistics)",
      "Joint (building)",
      "Linguistics",
      "Modal",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Polymer chemistry",
      "Quantum mechanics",
      "RGB color model",
      "Salient",
      "Scale (ratio)"
    ],
    "authors": [
      {
        "surname": "Fang",
        "given_name": "Xian"
      },
      {
        "surname": "Jiang",
        "given_name": "Mingfeng"
      },
      {
        "surname": "Zhu",
        "given_name": "Jinchao"
      },
      {
        "surname": "Shao",
        "given_name": "Xiuli"
      },
      {
        "surname": "Wang",
        "given_name": "Hongpeng"
      }
    ]
  },
  {
    "title": "LPCL: Localized prominence contrastive learning for self-supervised dense visual pre-training",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109185",
    "abstract": "Self-supervised pre-training has attracted increasing attention given its promising performance in training backbone networks without using labels. By far, most methods focus on image classification with datasets containing iconic objects and simple background, e.g. ImageNet. However, these methods show sub-optimal performance for dense prediction tasks (e.g. object detection and scene parsing) when directly pre-training on datasets (e.g. PASCAL VOC and COCO) with multiple objects and cluttered backgrounds. Researchers explored self-supervised dense pre-training methods by adapting recent image pre-training methods. Nevertheless, they require a large number of negative samples and a long training time to reach reasonable performance. In this paper, we propose LPCL, a novel self-supervised representation learning method for dense predictions to settle these issues. To guide the instance information in multi-instance datasets, we define an online object patch selection module to select the local patches with the high possibility of containing instance area in the augmented views efficiently during learning. After obtaining the patches, we present a novel multi-level contrastive learning method considering the instance representation of global-level, local-level and position-level without using negative samples. We conduct extensive experiments with LPCL directly pre-trained on PASCAL VOC and COCO. For PASCAL VOC image classification task, our model achieves state-of-the-art 86.2 % accuracy pre-trained on COCO( + 9.7 % top-1 accuracy compared with baseline BYOL). On object detection, instance segmentation and semantic segmentation task, our proposed model also achieved competitive results compared with other state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322006641",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Machine learning",
      "Object detection",
      "Parsing",
      "Pascal (unit)",
      "Pattern recognition (psychology)",
      "Programming language",
      "Segmentation",
      "Training set"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Zihan"
      },
      {
        "surname": "Zhu",
        "given_name": "Hongyuan"
      },
      {
        "surname": "Cheng",
        "given_name": "Hao"
      },
      {
        "surname": "Mi",
        "given_name": "Siya"
      },
      {
        "surname": "Zhang",
        "given_name": "Yu"
      },
      {
        "surname": "Geng",
        "given_name": "Xin"
      }
    ]
  },
  {
    "title": "Local Selective Vision Transformer for Depth Estimation Using a Compound Eye Camera",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.02.010",
    "abstract": "A compound eye camera is a hemispherical camera made by mimicking the structure of an insect’s eye. In general, a compound eye camera is composed of a set of single eye cameras. The compound eye camera has various advantages due to its unique structure and can be used in various vision tasks. In order to apply the compound eye camera to various vision tasks using 3D information, depth estimation is required. However, due to the difference between the compound eye image and the 2D RGB image, it is hard to use the existing depth estimation methods directly. In this paper, we propose a transformer-based neural network for eye-wise depth estimation, which is suitable for the compound eye image. We modify the self-attention module with local selective self-attention to take advantage of the compound eye’s hemispherical structure. In addition, we reduce the computational amount and increase the performance through the eye selection module. Using the proposed local selective self-attention and eye selection modules, we are able to improve the performance without large-scale pre-training. Compared to the ResNet-based depth estimation network, our method showed 2.8% and 1.4% higher performance on the GAZEBO and Matterport3D datasets, respectively, with 15.3% fewer network parameters.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523000405",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Compound eye",
      "Computer science",
      "Computer vision",
      "Optics",
      "Physics",
      "RGB color model"
    ],
    "authors": [
      {
        "surname": "Oh",
        "given_name": "Wooseok"
      },
      {
        "surname": "Yoo",
        "given_name": "Hwiyeon"
      },
      {
        "surname": "Ha",
        "given_name": "Taeoh"
      },
      {
        "surname": "Oh",
        "given_name": "Songhwai"
      }
    ]
  },
  {
    "title": "Optimum Bayesian thresholds for rebalanced classification problems using class-switching ensembles",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109158",
    "abstract": "Asymmetric label switching is an effective and principled method for creating a diverse ensemble of learners for imbalanced classification problems. This technique can be combined with other rebalancing mechanisms, such as those based on cost policies or class proportion modifications. In this study, and under the Bayesian theory framework, we specify the optimal decision thresholds for the combination of these mechanisms. In addition, we propose using a gating network to aggregate the learners contributions as an additional mechanism to improve the overall performance of the system.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322006379",
    "keywords": [
      "Aggregate (composite)",
      "Artificial intelligence",
      "Bayesian network",
      "Bayesian probability",
      "Class (philosophy)",
      "Composite material",
      "Computer science",
      "Epistemology",
      "Gating",
      "Machine learning",
      "Materials science",
      "Mechanism (biology)",
      "Neuroscience",
      "Philosophy",
      "Psychology"
    ],
    "authors": [
      {
        "surname": "Gutiérrez-López",
        "given_name": "Aitor"
      },
      {
        "surname": "González-Serrano",
        "given_name": "Francisco-Javier"
      },
      {
        "surname": "Figueiras-Vidal",
        "given_name": "Aníbal R."
      }
    ]
  },
  {
    "title": "Detection confidence driven multi-object tracking to recover reliable tracks from unreliable detections",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109107",
    "abstract": "Multi-object tracking (MOT) systems often rely on accurate object detectors; however, accurate detectors are not available in every application domain. We present Robust Confidence Tracking (RCT), an offline MOT algorithm designed for settings where detection quality is poor. Whereas prior methods simply threshold and discard detection confidence information, RCT relies on the exact detection confidence values to increase track quality throughout the entire tracking pipeline. This innovation (along with some simple and well-studied heuristics) allows RCT to achieve robust performance with minimal identity switches, even when provided with completely unfiltered detections. To compare trackers in the presence of unreliable detections, we present a challenging real-world underwater fish tracking dataset, FISHTRAC. In an large-scale evaluation across FISHTRAC, UA-DETRAC, and MOTChallenge data, RCT outperforms a wide variety of trackers, including deep trackers and more classic approaches. We have publically released our FISHTRAC codebase and training dataset at https://github.com/tmandel/fish-detrac, which will facilitate comparing trackers on understudied problems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322005878",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "BitTorrent tracker",
      "Computer science",
      "Computer vision",
      "Eye tracking",
      "Geodesy",
      "Geography",
      "Heuristics",
      "Object (grammar)",
      "Object detection",
      "Operating system",
      "Pattern recognition (psychology)",
      "Pedagogy",
      "Psychology",
      "Tracking (education)",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Mandel",
        "given_name": "Travis"
      },
      {
        "surname": "Jimenez",
        "given_name": "Mark"
      },
      {
        "surname": "Risley",
        "given_name": "Emily"
      },
      {
        "surname": "Nammoto",
        "given_name": "Taishi"
      },
      {
        "surname": "Williams",
        "given_name": "Rebekka"
      },
      {
        "surname": "Panoff",
        "given_name": "Max"
      },
      {
        "surname": "Ballesteros",
        "given_name": "Meynard"
      },
      {
        "surname": "Suarez",
        "given_name": "Bobbie"
      }
    ]
  },
  {
    "title": "SAPENet: Self-Attention based Prototype Enhancement Network for Few-shot Learning",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109170",
    "abstract": "Few-shot learning considers the problem of learning unseen categories given only a few labeled samples. As one of the most popular few-shot learning approaches, Prototypical Networks have received considerable attention owing to their simplicity and efficiency. However, a class prototype is typically obtained by averaging a few labeled samples belonging to the same class, which treats the samples as equally important and is thus prone to learning redundant features. Herein, we propose a self-attention based prototype enhancement network (SAPENet) to obtain a more representative prototype for each class. SAPENet utilizes multi-head self-attention mechanisms to selectively augment discriminative features in each sample feature map, and generates channel attention maps between intra-class sample features to attentively retain informative channel features for that class. The augmented feature maps and attention maps are finally fused to obtain representative class prototypes. Thereafter, a local descriptor-based metric module is employed to fully exploit the channel information of the prototypes by searching k similar local descriptors of the prototype for each local descriptor in the unlabeled samples for classification. We performed experiments on multiple benchmark datasets: miniImageNet, tieredImageNet, and CUB-200-2011. The experimental results on these datasets show that SAPENet achieves a considerable improvement compared to Prototypical Networks and also outperforms related state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322006495",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Channel (broadcasting)",
      "Chemistry",
      "Chromatography",
      "Class (philosophy)",
      "Computer network",
      "Computer science",
      "Computer security",
      "Discriminative model",
      "Economics",
      "Exploit",
      "Feature (linguistics)",
      "Feature learning",
      "Geodesy",
      "Geography",
      "Linguistics",
      "Machine learning",
      "Metric (unit)",
      "Operations management",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Sample (material)"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Xilang"
      },
      {
        "surname": "Choi",
        "given_name": "Seon Han"
      }
    ]
  },
  {
    "title": "A discriminatively deep fusion approach with improved conditional GAN (im-cGAN) for facial expression recognition",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109157",
    "abstract": "Considering most deep learning-based methods heavily depend on huge labels, it is still a challenging issue for facial expression recognition to extract discriminative features of training samples with limited labels. Given above, we propose a discriminatively deep fusion (DDF) approach based on an improved conditional generative adversarial network (im-cGAN) to learn abstract representation of facial expressions. First, we employ facial images with action units (AUs) to train the im-cGAN to generate more labeled expression samples. Subsequently, we utilize global features learned by the global-based module and the local features learned by the region-based module to obtain the fused feature representation. Finally, we design the discriminative loss function (D-loss) that expands the inter-class variations while minimizing the intra-class distances to enhance the discrimination of fused features. Experimental results on JAFFE, CK+, Oulu-CASIA, and KDEF datasets demonstrate the proposed approach is superior to some state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322006367",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Deep learning",
      "Discriminative model",
      "Expression (computer science)",
      "Facial expression",
      "Facial expression recognition",
      "Facial recognition system",
      "Feature (linguistics)",
      "Fusion",
      "Generative adversarial network",
      "Generative grammar",
      "Law",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Programming language",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Sun",
        "given_name": "Zhe"
      },
      {
        "surname": "Zhang",
        "given_name": "Hehao"
      },
      {
        "surname": "Bai",
        "given_name": "Jiatong"
      },
      {
        "surname": "Liu",
        "given_name": "Mingyang"
      },
      {
        "surname": "Hu",
        "given_name": "Zhengping"
      }
    ]
  },
  {
    "title": "A novel multi-branch wavelet neural network for sparse representation based object classification",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109155",
    "abstract": "Recent advances in acquisition and display technologies have led to an enormous amount of visual data, which requires appropriate storage and management tools. One of the fundamental needs is the design of efficient image classification and recognition solutions. In this paper, we propose a wavelet neural network approach for sparse representation-based object classification. The proposed approach aims to exploit the advantages of sparse coding, multi-scale wavelet representation as well as neural networks. More precisely, a wavelet transform is firstly applied to the image datasets. The generated approximation and detail wavelet subbands are then fed into a multi-branch neural network architecture. This architecture produces multiple sparse codes that are efficiently combined during the classification stage. Extensive experiments, carried out on various types of standard object datasets, have shown the efficiency of the proposed method compared to the existing sparse coding and deep learning-based methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322006343",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Coding (social sciences)",
      "Computer science",
      "Computer security",
      "Contextual image classification",
      "Exploit",
      "Image (mathematics)",
      "Law",
      "Mathematics",
      "Neural coding",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Sparse approximation",
      "Statistics",
      "Wavelet",
      "Wavelet transform"
    ],
    "authors": [
      {
        "surname": "Nguyen",
        "given_name": "Tan-Sy"
      },
      {
        "surname": "Luong",
        "given_name": "Marie"
      },
      {
        "surname": "Kaaniche",
        "given_name": "Mounir"
      },
      {
        "surname": "Ngo",
        "given_name": "Long H."
      },
      {
        "surname": "Beghdadi",
        "given_name": "Azeddine"
      }
    ]
  },
  {
    "title": "Optical character correction of large-curvature annular sector text in polar coordinate system",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.02.013",
    "abstract": "Optical character recognition (OCR) of complex morphologies represented by large-curvature annular sector text (AST) is a very challenging task. A three-segment text recognition framework consisting of detection, correction and recognition is currently an effective method for dealing with complex morphological OCR. Optical character correction (OCC) is a key component in processing largecurvature AST. This paper proposes an OCC method in the polar coordinate system, which consists of control point preprocessing, polar coordinate transformation, and image remapping. The control point preprocessing is used to normalize the control points of the large curvature AST region; the polar coordinate transformation is to convert the pixels in the rectangular coordinate system to the polar coordinate system; image remapping maps the original image in polar coordinate system to polar coordinate space for re-representation. The method proposed in this paper can be used in conjunction with most detection and recognition modules and is applicable to any language type. Furthermore, the correction process consumes very little computational resources and has little impact on the speed of text detection and recognition. Experimental results show that the proposed method outperforms state-of-the-art algorithms in large curvature AST correction and recognition experiments",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523000454",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Astronomy",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Coordinate space",
      "Coordinate system",
      "Curvature",
      "Gene",
      "Geometry",
      "Image (mathematics)",
      "Mathematics",
      "Operating system",
      "Optical character recognition",
      "Pattern recognition (psychology)",
      "Physics",
      "Pixel",
      "Point (geometry)",
      "Polar",
      "Polar coordinate system",
      "Preprocessor",
      "Process (computing)",
      "Transformation (genetics)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Ruiping"
      },
      {
        "surname": "Cao",
        "given_name": "Wei"
      },
      {
        "surname": "Wu",
        "given_name": "Shihong"
      },
      {
        "surname": "Jia",
        "given_name": "Meng"
      },
      {
        "surname": "Wang",
        "given_name": "Xiaoping"
      }
    ]
  },
  {
    "title": "A label distribution manifold learning algorithm",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109112",
    "abstract": "In this paper, we propose a novel label distribution manifold learning (LDML) method for solving the multilabel distribution learning problem. First, using manifold learning, we extract the accurate and reduced-dimension features of the training data. Second, we estimate the unknown label distributions associated with the extracted reduced-dimension features based on multi-output kernel regression. Third, we use the extracted reduced-dimension features and their associated estimated label distributions to form an enhanced maximum entropy model, which enables us to accurately and efficiently estimate the unknown true label distributions for the training data. We refer to this algorithm as the LDML. We also propose to apply the tangent space alignment regression in the second stage, and the resulting algorithm is called the LDML-R. The LDML-R has better label distribution learning performance than the LDML but imposes higher complexity than the latter. We evaluate the proposed LDML and LDML-R algorithms on 15 real-world data sets with ground-truth label distributions, and the experimental results obtained show that our method has advantages in terms of learning accuracy compared to the latest multi-label distribution learning approaches. We also use another 10 real-world multi-class data sets, which do not have the ground-truth label distributions, to demonstrate the superior multilabel classification performance of our LDML-R algorithm over the existing state-of-the-art multi-label classification algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322005921",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Curse of dimensionality",
      "Dimension (graph theory)",
      "Dimensionality reduction",
      "Distribution (mathematics)",
      "Engineering",
      "Entropy (arrow of time)",
      "Ground truth",
      "Intrinsic dimension",
      "Kernel (algebra)",
      "Machine learning",
      "Manifold (fluid mechanics)",
      "Mathematical analysis",
      "Mathematics",
      "Mechanical engineering",
      "Nonlinear dimensionality reduction",
      "Pattern recognition (psychology)",
      "Physics",
      "Pure mathematics",
      "Quantum mechanics",
      "Tangent space"
    ],
    "authors": [
      {
        "surname": "Tan",
        "given_name": "Chao"
      },
      {
        "surname": "Chen",
        "given_name": "Sheng"
      },
      {
        "surname": "Geng",
        "given_name": "Xin"
      },
      {
        "surname": "Ji",
        "given_name": "Genlin"
      }
    ]
  },
  {
    "title": "Classification of single-view object point clouds",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109137",
    "abstract": "Object point cloud classification has drawn great research attention since the release of benchmarking datasets, such as the ModelNet and the ShapeNet. These benchmarks assume point clouds covering complete surfaces of object instances, for which plenty of high-performing methods have been developed. However, their settings deviate from those often met in practice, where, due to (self-)occlusion, a point cloud covering partial surface of an object is captured from an arbitrary view. We show in this paper that performance of existing point cloud classifiers drops drastically under the considered single-view, partial setting; the phenomenon is consistent with the observation that semantic category of a partial object surface is less ambiguous only when its distribution on the whole surface is clearly specified. To this end, we argue for a single-view, partial setting where supervised learning of object pose estimation should be accompanied with classification. Technically, we propose a baseline method of Pose-Accompanied Point cloud classification Network (PAPNet); built upon S E ( 3 ) -equivariant convolutions, the PAPNet learns intermediate pose transformations for equivariant features defined on vector fields, which makes the subsequent classification easier (ideally) in the category-level, canonical pose. By adapting existing ModelNet40 and ScanNet datasets to the single-view, partial setting, experiment results can verify the necessity of object pose estimation and superiority of our PAPNet to existing classifiers.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322006173",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Equivariant map",
      "Geometry",
      "Linguistics",
      "Mathematics",
      "Object (grammar)",
      "Oblique case",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Point (geometry)",
      "Point cloud",
      "Pose",
      "Pure mathematics"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Zelin"
      },
      {
        "surname": "Liu",
        "given_name": "Kangjun"
      },
      {
        "surname": "Chen",
        "given_name": "Ke"
      },
      {
        "surname": "Ding",
        "given_name": "Changxing"
      },
      {
        "surname": "Wang",
        "given_name": "Yaowei"
      },
      {
        "surname": "Jia",
        "given_name": "Kui"
      }
    ]
  },
  {
    "title": "Generalized minimum error entropy for robust learning",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109188",
    "abstract": "The applications of error entropy (EE) are sometimes limited because its shape cannot be flexibly adjusted by the default Gaussian kernel function to adapt to noise variation and thus lowers the performance of algorithms based on minimum error entropy (MEE) criterion. In this paper, a generalized EE (GEE) is proposed by introducing the generalized Gaussian density (GGD) as its kernel function to improve the robustness of EE. In addition, GEE can be further improved to reduce its computational load by the quantized GEE (QGEE). Furthermore, two learning criteria, called generalized minimum error entropy (GMEE) and quantized generalized minimum error entropy (QGMEE), are developed based on GEE and QGEE, and new adaptive filtering (AF), kernel recursive least squares (KRLS), and multilayer perceptron (MLP) based on the proposed criteria are presented. Several numerical simulations indicate that the performance of proposed algorithms performs better than that of algorithms based on MEE.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322006677",
    "keywords": [
      "Algorithm",
      "Applied mathematics",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Entropy (arrow of time)",
      "Gaussian",
      "Mathematical optimization",
      "Mathematics",
      "Multilayer perceptron",
      "Perceptron",
      "Physics",
      "Quantum mechanics"
    ],
    "authors": [
      {
        "surname": "He",
        "given_name": "Jiacheng"
      },
      {
        "surname": "Wang",
        "given_name": "Gang"
      },
      {
        "surname": "Cao",
        "given_name": "Kui"
      },
      {
        "surname": "Diao",
        "given_name": "He"
      },
      {
        "surname": "Wang",
        "given_name": "Guotai"
      },
      {
        "surname": "Peng",
        "given_name": "Bei"
      }
    ]
  },
  {
    "title": "G 2 DA: Geometry-guided dual-alignment learning for RGB-infrared person re-identification",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109150",
    "abstract": "RGB-Infrared (IR) person re-identification aims to retrieve person-of-interest from heterogeneous cameras, easily suffering from large image modality discrepancy caused by different sensing wavelength ranges. Existing works usually minimize such discrepancy by aligning modality distribution of global features, while neglecting deep semantics and high-order structural relations within each class. This might render the misalignment between heterogeneous samples. In this paper, we propose Geometry-Guided Dual-Alignment (G 2 DA) learning, which yields better sample-level modality alignment for RGB-IR ReID by solving a graph-enabled distribution matching task that maximizes agreement between multi-modality node representations considering edge topology. Specifically, we covert RGB/IR images into semantic-aligned graphs, in which whole-part features and their similarities are represented by nodes and associated edges, respectively. To simultaneously implement node- and edge-wise alignment (Dual Alignment), we introduce Optimal Transport (OT) as a metric to calculate cross-modality human body matching scores. By minimizing the displacement cost across RGB-IR graphs, G 2 DA could learn not just modality-invariant but structurally consistent cross-modality representations. Furthermore, we advance a Message Fusion Attention (MFA) mechanism to adaptively smooth the node representations within each RGB/IR graph, effectively alleviating occlusions caused by other individuals and/or objects. Extensive experiments on two standard benchmark datasets validate the superiority of G 2 DA, yielding competitive performance against previous state-of-the-arts.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032200629X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Graph",
      "Matching (statistics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "RGB color model",
      "Statistics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Wan",
        "given_name": "Lin"
      },
      {
        "surname": "Sun",
        "given_name": "Zongyuan"
      },
      {
        "surname": "Jing",
        "given_name": "Qianyan"
      },
      {
        "surname": "Chen",
        "given_name": "Yehansen"
      },
      {
        "surname": "Lu",
        "given_name": "Lijing"
      },
      {
        "surname": "Li",
        "given_name": "Zhihang"
      }
    ]
  },
  {
    "title": "A Learnable Gradient operator for face presentation attack detection",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109146",
    "abstract": "Face presentation attack detection (PAD) aims to protect the security of face recognition systems. The existing depth-supervised method using stacked vanilla convolutions cannot explicitly extract efficient fine-grained information (e.g., spatial gradient magnitude) for the distinction between bona fide and attack presentations. To address this issue, the Sobel operator has been demonstrated effective to acquire gradient magnitude due to the fast calculation capacity for high-frequency information. However, the Sobel operator is hand-crafted so cannot deal with complex textures. Differently, we develop a learnable gradient operator (LGO) to adaptively learn gradient information in a data-driven way, which is a generalization of existing gradient operators and effectively captures detailed discriminative clues from raw pixels. In parallel, we propose an adaptive gradient loss for better optimization. Extensive experimental comparisons with the state-of-the-art methods on the widely used Replay-Attack, CASIA-FASD, OULU-NPU, and SiW datasets demonstrate the superior performance of the proposed approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322006252",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Convolution (computer science)",
      "Discriminative model",
      "Edge detection",
      "Face (sociological concept)",
      "Gene",
      "Generalization",
      "Gradient descent",
      "Image (mathematics)",
      "Image gradient",
      "Image processing",
      "Mathematical analysis",
      "Mathematics",
      "Operator (biology)",
      "Pattern recognition (psychology)",
      "Repressor",
      "Sobel operator",
      "Social science",
      "Sociology",
      "Transcription factor"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Caixun"
      },
      {
        "surname": "Yu",
        "given_name": "Bingyao"
      },
      {
        "surname": "Zhou",
        "given_name": "Jie"
      }
    ]
  },
  {
    "title": "Center and Scale Prediction: Anchor-free Approach for Pedestrian and Face Detection",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109071",
    "abstract": "Object detection traditionally requires sliding-window classifier in modern deep learning based approaches. However, both of these approaches requires tedious configurations in bounding boxes. Generally speaking, single-class object detection is to tell where the object is, and how big it is. Traditional methods combine the ”where” and ”how” subproblems into a single one through the overall judgement of various scales of bounding boxes. In view of this, we are interesting in whether the ”where” and ”how” subproblems can be separated into two independent subtasks to ease the problem definition and the difficulty of training. Accordingly, we provide a new perspective where detecting objects is approached as a high-level semantic feature detection task. Like edges, corners, blobs and other feature detectors, the proposed detector scans for feature points all over the image, for which the convolution is naturally suited. However, unlike these traditional low-level features, the proposed detector goes for a higher-level abstraction, that is, we are looking for central points where there are objects, and modern deep models are already capable of such a high-level semantic abstraction. Like blob detection, we also predict the scales of the central points, which is also a straightforward convolution. Therefore, in this paper, pedestrian and face detection is simplified as a straightforward center and scale prediction task through convolutions. This way, the proposed method enjoys an anchor-free setting, considerably reducing the difficulty in training configuration and hyper-parameter optimization. Though structurally simple, it presents competitive accuracy on several challenging benchmarks, including pedestrian detection and face detection. Furthermore, a cross-dataset evaluation is performed, demonstrating a superior generalization ability of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322005519",
    "keywords": [
      "Abstraction",
      "Artificial intelligence",
      "Artificial neural network",
      "Bounding overwatch",
      "Classifier (UML)",
      "Computer science",
      "Computer vision",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Detector",
      "Engineering",
      "Epistemology",
      "Face (sociological concept)",
      "Feature (linguistics)",
      "Linguistics",
      "Object detection",
      "Pattern recognition (psychology)",
      "Pedestrian",
      "Pedestrian detection",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Scale (ratio)",
      "Social science",
      "Sociology",
      "Telecommunications",
      "Transport engineering"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Wei"
      },
      {
        "surname": "Hasan",
        "given_name": "Irtiza"
      },
      {
        "surname": "Liao",
        "given_name": "Shengcai"
      }
    ]
  },
  {
    "title": "Multilabel Prototype Generation for data reduction in K-Nearest Neighbour classification",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109190",
    "abstract": "Prototype Generation (PG) methods are typically considered for improving the efficiency of the k -Nearest Neighbour ( k NN) classifier when tackling high-size corpora. Such approaches aim at generating a reduced version of the corpus without decreasing the classification performance when compared to the initial set. Despite their large application in multiclass scenarios, very few works have addressed the proposal of PG methods for the multilabel space. In this regard, this work presents the novel adaptation of four multiclass PG strategies to the multilabel case. These proposals are evaluated with three multilabel k NN-based classifiers, 12 corpora comprising a varied range of domains and corpus sizes, and different noise scenarios artificially induced in the data. The results obtained show that the proposed adaptations are capable of significantly improving—both in terms of efficiency and classification performance—the only reference multilabel PG work in the literature as well as the case in which no PG method is applied, also presenting statistically superior robustness in noisy scenarios. Moreover, these novel PG strategies allow prioritising either the efficiency or efficacy criteria through its configuration depending on the target scenario, hence covering a wide area in the solution space not previously filled by other works.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322006690",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Gene",
      "Machine learning",
      "Nearest neighbour",
      "Noisy data",
      "Pattern recognition (psychology)",
      "Robustness (evolution)",
      "k-nearest neighbors algorithm"
    ],
    "authors": [
      {
        "surname": "Valero-Mas",
        "given_name": "Jose J."
      },
      {
        "surname": "Gallego",
        "given_name": "Antonio Javier"
      },
      {
        "surname": "Alonso-Jiménez",
        "given_name": "Pablo"
      },
      {
        "surname": "Serra",
        "given_name": "Xavier"
      }
    ]
  },
  {
    "title": "Face anti-spoofing using feature distilling and global attention learning",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109147",
    "abstract": "Face anti-spoofing (FAS) is essential to assure the security of face recognition systems. Recently, some deep learning based FAS methods have achieved promising results under intra-dataset testing. However, they often fail in generalizing to unseen attacks due to the failure of extracting intrinsic features from face images. In this paper, we propose an end-to-end FAS method which consists of an anti-interference feature distillation module, a global spatial attention learning module and a pyramid binary mask supervision module. The deep features from the pretrained ResNet34 network are first distilled at multiple levels to capture intrinsic information via removing interference of features. Then, the multi-level distilled features are further refined by using a global spatial learning mechanism. Finally, the pyramid pixel-wise supervision is assembled to boost performance. Extensive experimental results on five benchmark datasets show the superior performance of our proposed method on intra-dataset testing and on cross-dataset testing.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322006264",
    "keywords": [
      "Artificial intelligence",
      "Attention network",
      "Benchmark (surveying)",
      "Channel (broadcasting)",
      "Computer network",
      "Computer science",
      "Deep learning",
      "Face (sociological concept)",
      "Facial recognition system",
      "Feature (linguistics)",
      "Feature extraction",
      "Feature learning",
      "Geodesy",
      "Geography",
      "Geometry",
      "Interference (communication)",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pyramid (geometry)",
      "Social science",
      "Sociology",
      "Spoofing attack"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Rui"
      },
      {
        "surname": "Wang",
        "given_name": "Xin"
      }
    ]
  },
  {
    "title": "Temporal augmented contrastive learning for micro-expression recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.02.003",
    "abstract": "Micro-expressions (MEs) can reveal the hidden but real emotion and are usually caused spontaneously. However, the characteristics of subtlety and temporariness with the lack of sufficient ME datasets make it hard for recognition. In this paper, we propose an adaptively temporal augmented momentum contrastive learning to alleviate these problems. For the small scale, we pre-train the model on a new interpolated dataset via contrastive learning with momentum contrast (MoCo). For the subtle and rapid facial movements, we augment the temporal dynamics using an adaptive dropout operation to redundant frames. Specifically, we use a recursive way to create a new interpolated dataset from raw datasets firstly. Then we design a shallow model with an inflated inception module as the encoder of the contrastive learning. Afterward, we pre-train the model on the new dataset via momentum contrastive learning. During the pre-training, we propose adaptively temporal augmentation via generative adversarial learning. After the pre-training, we take the encoder out and finetune it for recognition. Finally, we perform extensive experiments and ablation studies on three ME datasets. The results demonstrate the effectiveness of the MoCo-like pre-training and the temporal augmentation for recognition. Moreover, the pre-trained model outperforms other state-of-the-art models based on optical flow.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523000260",
    "keywords": [
      "Artificial intelligence",
      "Autoencoder",
      "Computer science",
      "Deep learning",
      "Dropout (neural networks)",
      "Generative grammar",
      "Generative model",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Tianhao"
      },
      {
        "surname": "Shang",
        "given_name": "Lin"
      }
    ]
  },
  {
    "title": "Weakly supervised adversarial learning via latent space for hyperspectral target detection",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109125",
    "abstract": "As an advanced technique in remote sensing, hyperspectral target detection (HTD) is widely concerned in civilian and military applications. However, the limitation of prior and mixed pixels phenomenon makes HTD models sensitive to data corruption under various interference from environment. In this work, a novel two-stage detection framework based on adversarial learning is proposed, which extracts spectral features in latent space through background reconstruction under weak supervision. To address the issues of insufficient utilization of both background information and limited prior knowledge, the generative adversarial network (GAN) is applied to estimate background in a weakly supervised manner with target-based constraints and channel-wise attention, which produces the detection proposal in the first stage. Then, a refined result is produced in the second stage, in which the input data consists of the refined data and refined feature map based on previous detection proposal. To provide samples for weakly supervised learning (WSL), the pseudo datasets are produced by a coarse sample selection procedure, which makes full use of limited prior information. Finally, an exponential constrained nonlinear function is adopted to acquire pixel-level prediction via suppressing the background and combining features from different stages. Experiments on real hyperspectral images (HSIs) captured by different sensors at various scenes verify the effectiveness of the proposed framework.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322006057",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Computer science",
      "Feature (linguistics)",
      "Hyperspectral imaging",
      "Linguistics",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pixel"
    ],
    "authors": [
      {
        "surname": "Qin",
        "given_name": "Haonan"
      },
      {
        "surname": "Xie",
        "given_name": "Weiying"
      },
      {
        "surname": "Li",
        "given_name": "Yunsong"
      },
      {
        "surname": "Jiang",
        "given_name": "Kai"
      },
      {
        "surname": "Lei",
        "given_name": "Jie"
      },
      {
        "surname": "Du",
        "given_name": "Qian"
      }
    ]
  },
  {
    "title": "Watching the BiG artifacts: Exposing DeepFake videos via Bi-granularity artifacts",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109179",
    "abstract": "Recent years have witnessed significant advances in AI-based face manipulation techniques, known as DeepFakes, which has brought severe threats to society. Hence, an emerging and increasingly important research topic is how to detect DeepFake videos. In this paper, we propose a new DeepFake detection method based on Bi-granularity artifacts (BiG-Arts). We observe that the most of DeepFake video generation can commonly introduce bi-granularity artifacts: the intrinsic-granularity artifacts and extrinsic-granularity artifacts. Specifically, the intrinsic-granularity artifacts are caused by a common series of operations in model generation such as up-convolution or up-sampling, while the extrinsic-granularity artifacts are introduced by a common step in post-processing that blends the synthesized face to original video. To this end, we formulate DeepFake detection as multi-task learning problem, to simultaneously predict the intrinsic and extrinsic artifacts. Benefiting from the guidance of detecting Bi-granularity artifacts, our method is notably boosted in both within-datasets and cross-datasets scenarios. Extensive experiments are conducted on several DeepFake datasets, which corroborates the superiority of our method. Our method has been contributed as a part of the solution to achieve the Top-1 rank in DFGC competition (https://competitions.codalab.org/competitions/29583).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322006586",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Convolution (computer science)",
      "Data mining",
      "Data science",
      "Face (sociological concept)",
      "Filter (signal processing)",
      "Granularity",
      "Machine learning",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Rank (graph theory)",
      "Sampling (signal processing)",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Han"
      },
      {
        "surname": "Li",
        "given_name": "Yuezun"
      },
      {
        "surname": "Lin",
        "given_name": "Dongdong"
      },
      {
        "surname": "Li",
        "given_name": "Bin"
      },
      {
        "surname": "Wu",
        "given_name": "Junqiang"
      }
    ]
  },
  {
    "title": "Opinion-Climate-Based Hegselmann-Krause dynamics",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.01.016",
    "abstract": "Opinion dynamics, which focuses on the opinion evolution process of a group of agents, is an important problem in Cyber-Physical-Social Systems (CPSS). Although the classic Hegselmann-Krause model and its variants have been intensively studied, the opinion climate, which is the perceived majority of opinions of a given social group at a given time, has not been considered as a crucial factor in the evolution of agents’ opinions. In this paper, inspired by the theory of the spiral of silence, we propose a novel opinion-climate-based social Hegselmann–Krause dynamics model, where the opinion climate is quantified by calculating the relative proportion of people who hold positive or negative opinions. The evolution patterns of agents’ opinions that depends on not only himself and his neighbors, but also the environment are then recognized. Extensive experiments on both artificially generated and real Cyber-Physical-Social datasets show that when the opinion climate is taken into consideration, agents will eventually reach a consensus on some specific events. Moreover, the smaller the influence weight of the opinion climate, the greater the time required to reach a consensus. The results can well explain the group polarization phenomena we observed nowadays in our society and provide an important idea for Cyber-Physical-Social Services.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523000211",
    "keywords": [
      "Acoustics",
      "Artificial intelligence",
      "Computer science",
      "Law",
      "Physics",
      "Political science",
      "Politics",
      "Psychology",
      "Public opinion",
      "Silence",
      "Social group",
      "Social media",
      "Social psychology",
      "Sociology",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Han"
      },
      {
        "surname": "Li",
        "given_name": "Ziqi"
      },
      {
        "surname": "Guan",
        "given_name": "Anqi"
      },
      {
        "surname": "Xu",
        "given_name": "Minghua"
      },
      {
        "surname": "Wang",
        "given_name": "Bang"
      }
    ]
  },
  {
    "title": "Jointly Texture Enhanced and Stereo Captured Network for Stereo Image Super-Resolution",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.02.015",
    "abstract": "Stereo image super-resolution (StereoISR) aims at recovering high resolution (HR) details and realistic textures from low-resolution (LR) left-right pairs with stereo correspondence. Recently, some advanced 2D methods have achieved good performance in realistic texture reconstruction via texture attention mechanism in a reference-based training way. However, the existing 3D super-resolution (SR) datasets lack the HR reference to obtain the LR-to-HR reference pair for the reference-based training. To overcome this problem, we propose a novel StereoISR network with a joint texture and parallax attention module (JTPAM), where the down-sampled LR (named super-low-resolution (SLR)) image and the LR image are taken as the SLR-to-LR reference pair, to simulate the texture conversion relationship between the LR and HR images. Additionally, the parallax attention mechanism is utilized to maintain the stereo correspondence by calculating the feature similarity along the Epipolar line. Experiment results on the Flickr1024, Middlebury, KITTI 2012 and KITTI 2015 datasets show that our network achieves the state-of-the-art (SOTA) performance over the compared methods, and the stereo SR images generated by our network have abundant binocular information and clear texture details.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523000442",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Epipolar geometry",
      "Feature (linguistics)",
      "Image (mathematics)",
      "Linguistics",
      "Parallax",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Similarity (geometry)",
      "Stereo image",
      "Texture (cosmology)"
    ],
    "authors": [
      {
        "surname": "Jin",
        "given_name": "Kangjun"
      },
      {
        "surname": "Wang",
        "given_name": "Xuejin"
      },
      {
        "surname": "Shao",
        "given_name": "Feng"
      }
    ]
  },
  {
    "title": "Finding compact and well-separated clusters: Clustering using silhouette coefficients",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109144",
    "abstract": "Finding compact and well-separated clusters in data sets is a challenging task. Most clustering algorithms try to minimize certain clustering objective functions. These functions usually reflect the intra-cluster similarity and inter-cluster dissimilarity. However, the use of such functions alone may not lead to the finding of well-separated and, in some cases, compact clusters. Therefore additional measures, called cluster validity indices, are used to estimate the true number of well-separated and compact clusters. Some of these indices are well-suited to be included into the optimization model of the clustering problem. Silhouette coefficients are among such indices. In this paper, a new optimization model of the clustering problem is developed where the clustering function is used as an objective and silhouette coefficients are used to formulate constraints. Then an algorithm, called CLUSCO (CLustering Using Silhouette COefficients), is designed to construct clusters incrementally. Three schemes are discussed to reduce the computational complexity of the algorithm. Its performance is evaluated using fourteen real-world data sets and compared with that of three state-of-the-art clustering algorithms. Results show that the CLUSCO is able to compute compact clusters which are significantly better separable in comparison with those obtained by other algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322006239",
    "keywords": [
      "Artificial intelligence",
      "CURE data clustering algorithm",
      "Cluster (spacecraft)",
      "Cluster analysis",
      "Computer science",
      "Correlation clustering",
      "Data mining",
      "Determining the number of clusters in a data set",
      "Fuzzy clustering",
      "Image (mathematics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Programming language",
      "Silhouette",
      "Similarity (geometry)",
      "Single-linkage clustering",
      "k-medians clustering"
    ],
    "authors": [
      {
        "surname": "Bagirov",
        "given_name": "Adil M."
      },
      {
        "surname": "Aliguliyev",
        "given_name": "Ramiz M."
      },
      {
        "surname": "Sultanova",
        "given_name": "Nargiz"
      }
    ]
  },
  {
    "title": "A novel DAGAN for synthesizing garment images based on design attribute disentangled representation",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109248",
    "abstract": "In online costume design, it is vital to preview the design effect rapidly by entangling design attributes from reference images. This paper proposes a novel method, named design attributes generative adversarial network (DAGAN) for synthesizing garment images based on design attribute disentangled representation. The garment style is disentangled into the shape, texture, shadow, and decoration design attributes. The shape mask, repeating texture region, Laplace image gradient, and local logo are leveraged as visual representations for clothing design attributes from reference images. Following the design sequence from global to local, GDA-Net and LDA-Net in DAGAN entangle global and local design attributes, respectively, in the latent space. Then, the desired garment image is synthesized to represent design intentions explicitly. The DA-dataset for clothing design attributes is released. Extensive experiments demonstrate that the DAGAN is robust to various instances of design attributes on Design Attributes dataset (DA-dataset), and that is superior to the cross-domain transfer models in entangling design attributes from reference images.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322007270",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Clothing",
      "Computer science",
      "Computer vision",
      "Engineering",
      "Generative Design",
      "History",
      "Image (mathematics)",
      "Law",
      "Metric (unit)",
      "Operations management",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Fang",
        "given_name": "Naiyu"
      },
      {
        "surname": "Qiu",
        "given_name": "Lemiao"
      },
      {
        "surname": "Zhang",
        "given_name": "Shuyou"
      },
      {
        "surname": "Wang",
        "given_name": "Zili"
      },
      {
        "surname": "Hu",
        "given_name": "Kerui"
      },
      {
        "surname": "Wang",
        "given_name": "Kang"
      }
    ]
  },
  {
    "title": "Weight matrix sharing for multi-label learning",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109156",
    "abstract": "Multi-label learning on real-world data is a challenging task due to sparse labels, missing labels, and sparse structures. Some existing approaches are effective in addressing the former two issues. In this paper, we propose a shared weight matrix with low-rank and sparse regularization for multi-label learning (2SML) algorithm to address the issues simultaneously. First, two explicit correlation matrices are constructed from the feature matrix and label matrix. Second, we select informative labels by instance representativeness to learn implicit correlations. Third, a feature manifold and label manifold are employed to guide the shared weight learning process. Extensive experiments are undertaken on multiple benchmark datasets with and without missing labels. The results show that the proposed method outperforms the state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322006355",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Composite material",
      "Computer science",
      "Feature (linguistics)",
      "Gaussian",
      "Geodesy",
      "Geography",
      "Linguistics",
      "Machine learning",
      "Materials science",
      "Matrix (chemical analysis)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Regularization (linguistics)",
      "Sparse matrix"
    ],
    "authors": [
      {
        "surname": "Qian",
        "given_name": "Kun"
      },
      {
        "surname": "Min",
        "given_name": "Xue-Yang"
      },
      {
        "surname": "Cheng",
        "given_name": "Yusheng"
      },
      {
        "surname": "Min",
        "given_name": "Fan"
      }
    ]
  },
  {
    "title": "Scalable clustering by aggregating representatives in hierarchical groups",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109230",
    "abstract": "Appropriately handling the scalability of clustering is a long-standing challenge for the study of clustering techniques and is of fundamental interest to researchers in the community of data mining and knowledge discovery. In comparison to other clustering methods, hierarchical clustering demonstrates better interpretability of clustering results but poor scalability while handling large-scale data. Thus, more comprehensive studies on this problem need to be conducted. This paper develops a new scalable hierarchical clustering model called Election Tree, which can detect the most representative point for each sub-cluster via the process of node election in split data and adjust the members in sub-clusters by the operations of node merging and swap. Extensive experiments on real-world datasets reveal that the proposed computational framework has better clustering accuracy as opposed to the competing baseline methods. Meanwhile, with respect to the scalability tests on incremental synthetic datasets, the results show that the new model has a significantly lower time consumption than the state-of-the-art hierarchical clustering models such as PERCH, GRINCH, SCC and other classic baselines.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322007099",
    "keywords": [
      "Brown clustering",
      "CURE data clustering algorithm",
      "Cluster analysis",
      "Computer science",
      "Consensus clustering",
      "Correlation clustering",
      "Data mining",
      "Database",
      "Economics",
      "Engineering",
      "Finance",
      "Hierarchical clustering",
      "Hierarchical clustering of networks",
      "Interpretability",
      "Machine learning",
      "Node (physics)",
      "Scalability",
      "Structural engineering",
      "Swap (finance)"
    ],
    "authors": [
      {
        "surname": "Xie",
        "given_name": "Wen-Bo"
      },
      {
        "surname": "Liu",
        "given_name": "Zhen"
      },
      {
        "surname": "Das",
        "given_name": "Debarati"
      },
      {
        "surname": "Chen",
        "given_name": "Bin"
      },
      {
        "surname": "Srivastava",
        "given_name": "Jaideep"
      }
    ]
  },
  {
    "title": "PSA-Det3D: Pillar set abstraction for 3D object detection",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.03.016",
    "abstract": "Small object detection for 3D point cloud is a challenging problem because of two limitations: (1) The sparsity of point clouds significantly increases the difficulty of perceiving small objects. (2) The occlusion of small objects can easily break the shape of their point clouds. To alleviate these problems, we design a point-based detection network PSA-Det3D which mainly consists of a pillar set abstraction (PSA) and a foreground point compensation (FPC). The PSA improves the query approach of set abstraction, which benefits the point-wise feature aggregation for small objects. The FPC fuses the foreground points and the estimated centers to select the candidate points, which effectively improves the detection performance for occluded objects. Extensive experiments show that our proposed PSA-Det3D achieves higher performance on all categories. For small object detection, our method outperforms existing point based algorithms on the KITTI 3D detection benchmark.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523000818",
    "keywords": [
      "Abstraction",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Computer vision",
      "Epistemology",
      "Feature (linguistics)",
      "Geodesy",
      "Geography",
      "Geometry",
      "Linguistics",
      "Mathematics",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Point (geometry)",
      "Point cloud",
      "Programming language",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Zhicong"
      },
      {
        "surname": "Zheng",
        "given_name": "Zhijie"
      },
      {
        "surname": "Zhao",
        "given_name": "Jingwen"
      },
      {
        "surname": "Hu",
        "given_name": "Haifeng"
      },
      {
        "surname": "Wang",
        "given_name": "Zixin"
      },
      {
        "surname": "Chen",
        "given_name": "Dihu"
      }
    ]
  },
  {
    "title": "Weakly Supervised Instance Segmentation via Category-aware Centerness Learning with Localization Supervision",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109165",
    "abstract": "Deep convolutional neural networks (DCNN) trained with pixel-level segmentation masks achieve high performance in the task of instance segmentation. The difficulty of acquiring such annotation limits the application and popularization of the DCNN-based approaches. To address the issue, a weakly supervised approach is proposed in the paper which performs instance segmentation only with the supervision of bounding box or coarse localization annotation. A novel DCNN model is constructed which consists of two branches: the centerness branch and the segmentation branch. The former branch is to learn the semantically spatial importance over the areas of object instances under the localization supervision. Object proposals with exact boundaries are automatically generated and are then ranked under the guidance of the output of the centerness branch. The most matched instance proposal is assigned to each object, which is then used to supervise the segmentation branch. The losses are calculated by both the outputs of the two branches and the entire DCNN model is trained end-to-end. Experiments are extensively conducted to verify the effectiveness. With the supervision of precise bounding box annotation, our approach achieves state-of-the-art (SOTA) accuracy in the comparison with recent related works. And in the case of coarse localization annotation, our approach only deduces a slight reduction in accuracy, which significantly outperforms other approaches. The excellent performance demonstrates that our approach would be helpful to further alleviate the workload of image annotation while maintaining competitive accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322006446",
    "keywords": [
      "Annotation",
      "Artificial intelligence",
      "Bounding overwatch",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Economics",
      "Image (mathematics)",
      "Image segmentation",
      "Machine learning",
      "Management",
      "Minimum bounding box",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Segmentation",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Jiabin"
      },
      {
        "surname": "Su",
        "given_name": "Hu"
      },
      {
        "surname": "He",
        "given_name": "Yonghao"
      },
      {
        "surname": "Zou",
        "given_name": "Wei"
      }
    ]
  },
  {
    "title": "Pure graph-guided multi-view subspace clustering",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109187",
    "abstract": "Multi-view subspace clustering approaches have shown outstanding performance in revealing similarity relationships and complex structures hidden in data. Despite the progress, previous multi-view clustering methods still face two challenges: (1) it is difficult to simultaneously achieve sparsity and connectivity of the affinity graph; (2) existing methods usually separate the graph learning step from the clustering process, which leads to unsatisfactory clustering performance as the final results critically rely on the learned graph. In this paper, we propose to achieve a structured consensus graph for multi-view subspace clustering by leveraging the sparsity and connectivity of each affinity graph. In the proposed method, the pure graph for each view is searched by finding the good neighbors. The multiple pure graphs are further fused into a consensus graph with a block-diagonal structure. That is, the consensus graph is enforced to contain exactly c connected components where c is the number of the clusters. Hence the label to each sample can be directly assigned since each connected component precisely corresponds to an individual cluster. As a result, the proposed model seamlessly accomplishes the subtasks including graph construction, pure graph learning (i.e., good neighbors searching), and cluster label allocation in a mutual reinforcement manner. Extensive experimental results demonstrate the superiority and reliability of our proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322006665",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Clustering coefficient",
      "Computer science",
      "Data mining",
      "Graph",
      "Pattern recognition (psychology)",
      "Subspace topology",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Hongjie"
      },
      {
        "surname": "Huang",
        "given_name": "Shudong"
      },
      {
        "surname": "Tang",
        "given_name": "Chenwei"
      },
      {
        "surname": "Zhang",
        "given_name": "Yancheng"
      },
      {
        "surname": "Lv",
        "given_name": "Jiancheng"
      }
    ]
  },
  {
    "title": "Position-aware and structure embedding networks for deep graph matching",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109242",
    "abstract": "Graph matching refers to the process of establishing node correspondences based on edge-to-edge constraints between graph nodes. This can be formulated as a combinatorial optimization problem under node permutation and pairwise consistency constraints. The main challenge of graph matching is to effectively find the correct match while reducing the ambiguities produced by similar nodes and edges. In this paper, we present a novel end-to-end neural framework that converts graph matching to a linear assignment problem in a high-dimensional space. This is combined with relative position information at the node level, and high-order structural arrangement information at the subgraph level. By capturing the relative position attributes of nodes between different graphs and the subgraph structural arrangement attributes, we can improve the performance of graph matching tasks, and establish reliable node-to-node correspondences. Our method can be generalized to any graph embedding setting, which can be used as components to deal with various graph matching problems answered with deep learning methods. We validate our method on several real-world tasks, by providing ablation studies to evaluate the generalization capability across different categories. We also compare state-of-the-art alternatives to demonstrate performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032200721X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Embedding",
      "Factor-critical graph",
      "Graph",
      "Graph embedding",
      "Graph factorization",
      "Line graph",
      "Matching (statistics)",
      "Mathematics",
      "Pairwise comparison",
      "Statistics",
      "Theoretical computer science",
      "Voltage graph"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Dongdong"
      },
      {
        "surname": "Dai",
        "given_name": "Yuxing"
      },
      {
        "surname": "Zhang",
        "given_name": "Lichi"
      },
      {
        "surname": "Zhang",
        "given_name": "Zhihong"
      },
      {
        "surname": "Hancock",
        "given_name": "Edwin R."
      }
    ]
  },
  {
    "title": "Meta-hallucinating prototype for few-shot learning promotion",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109235",
    "abstract": "An effective way for few-shot learning (FSL) is to establish a metric space where the distance between a query and the prototype of each class is computed for classification, and the key lies on hallucinating the appropriate prototypes for each class of the given FSL task. Most existing prototypical approaches hallucinate the class-wise prototype based on the given support samples with an equal contribution assumption, i.e., each support sample contributes equally to the corresponding prototype. However, due to the limited-data regime as well as the strict assumption, the hallucinated prototypes often deviate from the ideal ones that are determined by the sample distribution of each unseen class, and thus causing poor generalization performance. To mitigate this problem, we present a prototype meta-hallucination approach which shows two aspects of advantages. On one hand, instead of directly inferring the complicated sample distribution, it meta-learns to establish a difference distribution based generative model that infers the distribution of inter-sample difference and synthesizes new labeled samples through fusing the sampled inter-sample difference and each given support sample. This empowers us to augment the support set with more content-diverse samples and is beneficial to reduce the bias in prototype hallucination. On the other hand, we argue that each support sample may contribute no-equally to the ideal prototype that it belongs to and their relations vary with class characteristics. Following this, our approach meta-learns to dynamically re-weight all support samples in prototype hallucination, which makes it flexible to locate the ideal prototype for each unseen class based on its characteristics. Experiments on four FSL benchmark datasets show that our approach can effectively improve the performance of the prototypical baseline and outperform several state-of-the-art competitors with a clear margin.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322007142",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Chromatography",
      "Class (philosophy)",
      "Computer science",
      "Divergence (linguistics)",
      "Economics",
      "Epistemology",
      "Generalization",
      "Hallucinating",
      "Ideal (ethics)",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Metric (unit)",
      "Operations management",
      "Philosophy",
      "Sample (material)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Lei"
      },
      {
        "surname": "Zhou",
        "given_name": "Fei"
      },
      {
        "surname": "Wei",
        "given_name": "Wei"
      },
      {
        "surname": "Zhang",
        "given_name": "Yanning"
      }
    ]
  },
  {
    "title": "Causal reasoning for algorithmic fairness in voice controlled cyber-physical systems",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.03.014",
    "abstract": "Automated speaker recognition is enabling personalized interactions with the voice-based interfaces and assistants part of the modern cyber-physical-social systems. Prior studies have unfortunately uncovered disparate impacts across demographic groups on the outcomes of speaker recognition systems and consequently proposed a range of countermeasures. Understanding why a speaker recognition system may lead to this disparate performance for different (groups of) individuals, going beyond mere data imbalance reasons and black-box countermeasures, is an essential yet under-explored perspective. In this paper, we propose an explanatory framework that aims to provide a better understanding of how speaker recognition models perform as the underlying voice characteristics on which they are tested change. With our framework, we evaluate two state-of-the-art speaker recognition models, comparing their fairness in terms of security, through a systematic analysis of the impact of more than twenty voice characteristics. Our findings include important takeaways to enable voice controlled cyber-physical-social systems for everyone. Source code and data are available at https://bit.ly/EA-PRLETTERS.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552300079X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Cyber-physical system",
      "Data science",
      "Human–computer interaction",
      "Operating system",
      "Perspective (graphical)",
      "Speaker recognition",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Fenu",
        "given_name": "Gianni"
      },
      {
        "surname": "Marras",
        "given_name": "Mirko"
      },
      {
        "surname": "Medda",
        "given_name": "Giacomo"
      },
      {
        "surname": "Meloni",
        "given_name": "Giacomo"
      }
    ]
  },
  {
    "title": "Improving handgun detection through a combination of visual features and body pose-based data",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109252",
    "abstract": "Early detection of the presence of dangerous objects such as handguns in Closed-Circuit Television (CCTV) images is vital to reduce the potential damage. In this work, a novel method for automatic detection of handguns in CCTV-like images based on a combination architecture which leverages body pose estimation is proposed. Weapon appearance features along with body pose features are combined to perform robust detection in typical surveillance environments where appearance features alone are not sufficient (e.g., because the handgun may appear too small or dark). Both CNN and recent transformer-based architectures are applied for visual feature extraction. Experiments on multiple datasets show that this approach improves state-of-the-art pose-based handgun detectors. An ablation study is also performed to verify the contribution of the pose processing branch and the false positive filter.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322007312",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Feature extraction",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pose"
    ],
    "authors": [
      {
        "surname": "Ruiz-Santaquiteria",
        "given_name": "Jesus"
      },
      {
        "surname": "Velasco-Mata",
        "given_name": "Alberto"
      },
      {
        "surname": "Vallez",
        "given_name": "Noelia"
      },
      {
        "surname": "Deniz",
        "given_name": "Oscar"
      },
      {
        "surname": "Bueno",
        "given_name": "Gloria"
      }
    ]
  },
  {
    "title": "ALVLS: Adaptive local variances-Based levelset framework for medical images segmentation",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109257",
    "abstract": "Medical image segmentation is a very challenging task, not only because the intensity of the medical image itself is not uniform, but also it may be accompanied by the impact of noise. Although mathematics, computer science, medicine, and other interdisciplinary fields have begun to study the problem of medical image segmentation, and have put forward a variety of segmentation algorithms, there is still much room for further improvement and enhancement. In the process of medical image collection and reconstruction, it is easy to produce intensity inhomogeneity and noises, as well as interference from other tissues, resulting in the difficulty of accurate segmentation. In this paper, we propose the adaptive local variances-based level set (ALVLS) model to segment medical images with intensity inhomogeneity and noises, including cardiac MR images, brain MR images, and breast ultrasound images. According to the variance difference information, the ALVLS model can adjust the effect of the area term adaptively. The local intensity variances are designed to optimize the ability to resist noise, which improves the segmentation accuracy of medical images. We also propose the two-layer level set model for segmenting left ventricles and left epicardium simultaneously. Experimental results for medical images and synthetic images show the desirable performance of the ALVLS model in accuracy, efficiency, and robustness to noise. In medical image competition, the Dice coefficient is used to calculate the similarity between the segmentation result and the ground truth. Thus we do comparisons with other methods and show that the Dice coefficient of the proposed method is higher than other testing methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322007361",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Gene",
      "Ground truth",
      "Image (mathematics)",
      "Image segmentation",
      "Medical imaging",
      "Noise (video)",
      "Pattern recognition (psychology)",
      "Robustness (evolution)",
      "Scale-space segmentation",
      "Segmentation",
      "Segmentation-based object categorization",
      "Sørensen–Dice coefficient"
    ],
    "authors": [
      {
        "surname": "Shu",
        "given_name": "Xiu"
      },
      {
        "surname": "Yang",
        "given_name": "Yunyun"
      },
      {
        "surname": "Liu",
        "given_name": "Jun"
      },
      {
        "surname": "Chang",
        "given_name": "Xiaojun"
      },
      {
        "surname": "Wu",
        "given_name": "Boying"
      }
    ]
  },
  {
    "title": "Coherence-aware context aggregator for fast video object segmentation",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109214",
    "abstract": "Semi-supervised video object segmentation (VOS) is a highly challenging problem that has attracted much research attention in recent years. Temporal context plays an important role in VOS by providing object clues from the past frames. However, most of the prevailing methods directly use the predicted temporal results to guide the segmentation of the current frame, while ignoring the coherence of temporal context, which may be misleading and degrade the performance. In this paper, we propose a novel model named Coherence-aware Context Aggregator (CCA) for VOS, which consists of three modules. First, a coherence-aware module (CAM) is proposed to evaluate the coherence of the predicted result of the current frame and then fuses the coherent features to update the temporal context. CAM can determine whether the prediction is accurate, thus guiding the update of the temporal context and avoiding the introduction of erroneous information. Second, we devise a spatio-temporal context aggregation (STCA) module to aggregate the temporal context with the spatial feature of the current frame to learn a robust and discriminative target representation in the decoder part. Third, we design a refinement module to refine the coarse feature generated from the STCA module for more precise segmentation. Additionally, CCA uses a cropping strategy and takes small-size images as input, thus making it computationally efficient and achieving a real-time running speed. Extensive experiments on four challenging benchmarks show that CCA achieves a better trade-off between efficiency and accuracy compared to state-of-the-art methods. The code will be public.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322006938",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Coherence (philosophical gambling strategy)",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Context model",
      "Discriminative model",
      "Feature (linguistics)",
      "Frame (networking)",
      "Linguistics",
      "Mathematics",
      "News aggregator",
      "Object (grammar)",
      "Operating system",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Segmentation",
      "Statistics",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Lan",
        "given_name": "Meng"
      },
      {
        "surname": "Zhang",
        "given_name": "Jing"
      },
      {
        "surname": "Wang",
        "given_name": "Zengmao"
      }
    ]
  },
  {
    "title": "Multi-scale local-temporal similarity fusion for continuous sign language recognition",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109233",
    "abstract": "Continuous sign language recognition (cSLR) is a public significant task that transcribes a sign language video into an ordered gloss sequence. It is important to capture the fine-grained gloss-level details, since there is no explicit alignment between sign video frames and the corresponding glosses. Among the past works, one promising way is to adopt a one-dimensional convolutional network (1D-CNN) to temporally fuse the sequential frames. However, CNNs are agnostic to similarity or dissimilarity, and thus are unable to capture local consistent semantics within temporally neighboring frames. To address the issue, we propose to adaptively fuse local features via temporal similarity for this task. Specifically, we devise a Multi-scale Local-Temporal Similarity Fusion Network (mLTSF-Net) as follows: 1) In terms of a specific video frame, we firstly select its similar neighbours with multi-scale receptive regions to accommodate different lengths of glosses. 2) To ensure temporal consistency, we then use position-aware convolution to temporally convolve each scale of selected frames. 3) To obtain a local-temporally enhanced frame-wise representation, we finally fuse the results of different scales using a content-dependent aggregator. We train our model in an end-to-end fashion, and the experimental results on RWTH-PHOENIX-Weather 2014 datasets (RWTH) demonstrate that our model achieves competitive performance compared with several state-of-the-art models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322007129",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Electrical engineering",
      "Engineering",
      "Frame (networking)",
      "Fuse (electrical)",
      "Image (mathematics)",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Scale (ratio)",
      "Sign language",
      "Similarity (geometry)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Xie",
        "given_name": "Pan"
      },
      {
        "surname": "Cui",
        "given_name": "Zhi"
      },
      {
        "surname": "Du",
        "given_name": "Yao"
      },
      {
        "surname": "Zhao",
        "given_name": "Mengyi"
      },
      {
        "surname": "Cui",
        "given_name": "Jianwei"
      },
      {
        "surname": "Wang",
        "given_name": "Bin"
      },
      {
        "surname": "Hu",
        "given_name": "Xiaohui"
      }
    ]
  },
  {
    "title": "3D hand pose estimation from a single RGB image by weighting the occlusion and classification",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109217",
    "abstract": "In this paper, a new framework for 3D hand pose estimation using a single RGB image is proposed. The framework is composed of two blocks. The first block formulates the hand pose estimation as a classification problem. Since the human hand can perform numerous poses, the classification network needs a huge number of parameters. So, we propose to classify hand poses based on three different aspects, including hand gesture, hand direction, and palm direction. In this way, the number of parameters will be significantly reduced. The motivation behind the classification block is that the model deals with the image as a whole and extracts global features. Furthermore, the output of the classification model is a valid pose that does not include any unexpected angle at joints. The second block estimates the 3D coordinates of the hand joints and focuses more on the details of the image pattern. RGB-based 3D hand pose estimation is an inherently ill-posed problem due to the lack of depth information in the 2D image. We propose to use the occlusion status of the hand joints to solve this problem. The occlusion status of the joints has been labeled manually. Some joints are partially occluded, and we propose to compute the extent of the occlusion by semantic segmentation. The existing methods in this field mostly used synthetic datasets. But all the models proposed in this paper are trained on more than 50 K real images. Extensive experiments on our new dataset and two other benchmark datasets show that the proposed method can achieve good performance. We also analyze the validity of the predicted poses, and the results show that the classification block increases the validity of the poses.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322006963",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Block (permutation group theory)",
      "Computer science",
      "Computer vision",
      "Geodesy",
      "Geography",
      "Geometry",
      "Image (mathematics)",
      "Mathematics",
      "Medicine",
      "Pattern recognition (psychology)",
      "Pose",
      "RGB color model",
      "Radiology",
      "Segmentation",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Mahdikhanlou",
        "given_name": "Khadijeh"
      },
      {
        "surname": "Ebrahimnezhad",
        "given_name": "Hossein"
      }
    ]
  },
  {
    "title": "Consistent attack: Universal adversarial perturbation on embodied vision navigation",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.03.001",
    "abstract": "Embodied agents in vision navigation coupled with deep neural networks have attracted increasing attention. However, deep neural networks have been shown vulnerable to malicious adversarial noises, which may potentially cause catastrophic failures in Embodied Vision Navigation. Among different adversarial noises, universal adversarial perturbations (UAP), i.e., a constant image-agnostic perturbation applied on every input frame of the agent, play a critical role in Embodied Vision Navigation since they are computation-efficient and application-practical during the attack. However, existing UAP methods ignore the system dynamics of Embodied Vision Navigation and might be sub-optimal. In order to extend UAP to the sequential decision setting, we formulate the disturbed environment under the universal noise δ , as a δ -disturbed Markov Decision Process ( δ -MDP). Based on the formulation, we analyze the properties of δ -MDP and propose two novel Consistent Attack methods, named Reward UAP and Trajectory UAP, for attacking Embodied agents, which consider the dynamic of the MDP and calculate universal noises by estimating the disturbed distribution and the disturbed Q function. For various victim models, our Consistent Attack can cause a significant drop in their performance in the PointGoal task in Habitat with different datasets and different scenes. Extensive experimental results indicate that there exist serious potential risks for applying Embodied Vision Navigation methods to the real world.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523000661",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer vision",
      "Embodied cognition"
    ],
    "authors": [
      {
        "surname": "Ying",
        "given_name": "Chengyang"
      },
      {
        "surname": "Qiaoben",
        "given_name": "You"
      },
      {
        "surname": "Zhou",
        "given_name": "Xinning"
      },
      {
        "surname": "Su",
        "given_name": "Hang"
      },
      {
        "surname": "Ding",
        "given_name": "Wenbo"
      },
      {
        "surname": "Ai",
        "given_name": "Jianyong"
      }
    ]
  },
  {
    "title": "Laplacian Lp norm least squares twin support vector machine",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109192",
    "abstract": "Semi-supervised learning has become a hot learning framework, where large amounts of unlabeled data and small amounts of labeled data are available during the training process. The recently proposed Laplacian least squares twin support vector machine (Lap-LSTSVM) is an excellent tool to solve the semi-supervised classification problem. Motivated by the success of Lap-LSTSVM, in this paper, we propose a novel Laplacian Lp norm least squares twin support vector machine (Lap-LpLSTSVM). There are several advantages of our proposed method: (1) The performance of our proposed Lap-LpLSTSVM can be improved by the adjustability of the value of p . (2) The introduced Lp norm graph regularization term can efficiently exploit the geometric information embedded in the data. (3) An efficient iterative strategy is employed to solve the optimization problem. Besides, to demonstrate that our proposed method can make use of unlabeled data effectively, least squares twin support vector machine (LSTSVM) which only uses the same labeled data is used to compare with our proposed method. The experimental results on both synthetic and real-world datasets show that our proposed method outperforms other state-of-the-art methods and can also deal with noisy datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322006719",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Estimator",
      "Graph",
      "Laplace operator",
      "Laplacian matrix",
      "Law",
      "Least squares support vector machine",
      "Least-squares function approximation",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Norm (philosophy)",
      "Pattern recognition (psychology)",
      "Political science",
      "Regularization (linguistics)",
      "Statistics",
      "Support vector machine",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Xie",
        "given_name": "Xijiong"
      },
      {
        "surname": "Sun",
        "given_name": "Feixiang"
      },
      {
        "surname": "Qian",
        "given_name": "Jiangbo"
      },
      {
        "surname": "Guo",
        "given_name": "Lijun"
      },
      {
        "surname": "Zhang",
        "given_name": "Rong"
      },
      {
        "surname": "Ye",
        "given_name": "Xulun"
      },
      {
        "surname": "Wang",
        "given_name": "Zhijin"
      }
    ]
  },
  {
    "title": "Automated lesion segmentation in fundus images with many-to-many reassembly of features",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109191",
    "abstract": "Existing CNN-based segmentation approaches have achieved remarkable progresses on segmenting objects in regular sizes. However, when migrating them to segment tiny retinal lesions, they encounter challenges. The feature reassembly operators that they adopt are prone to discard the subtle activations about tiny lesions and fail to capture long-term dependencies. This paper aims to solve these issues and proposes a novel Many-to-Many Reassembly of Features (M2MRF) for tiny lesion segmentation. Our proposed M2MRF reassembles features in a dimension-reduced feature space and simultaneously aggregates multiple features inside a large predefined region into multiple output features. In this way, subtle activations about small lesions can be maintained as much as possible and long-term spatial dependencies can be captured to further enhance the lesion features. Experimental results on two lesion segmentation benchmarks, i.e., DDR and IDRiD, show that 1) our M2MRF outperforms existing feature reassembly operators, and 2) equipped with our M2MRF, the HRNetV2 is able to achieve substantially better performances and generalisation ability than existing methods. Our code is made publicly available at https://github.com/CVIU-CSU/M2MRF-Lesion-Segmentation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322006707",
    "keywords": [
      "Artificial intelligence",
      "Business",
      "Computer science",
      "Computer vision",
      "Dimension (graph theory)",
      "Feature (linguistics)",
      "Fundus (uterus)",
      "Lesion",
      "Linguistics",
      "Market segmentation",
      "Marketing",
      "Mathematics",
      "Medicine",
      "Pathology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pure mathematics",
      "Radiology",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Qing"
      },
      {
        "surname": "Liu",
        "given_name": "Haotian"
      },
      {
        "surname": "Ke",
        "given_name": "Wei"
      },
      {
        "surname": "Liang",
        "given_name": "Yixiong"
      }
    ]
  },
  {
    "title": "Annotator-dependent uncertainty-aware estimation of gait relative attributes",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109197",
    "abstract": "In this paper, we describe an uncertainty-aware estimation framework for gait relative attributes. We specifically design a two-stream network model that takes a pair of gait videos as input. It then outputs a corresponding pair of Gaussian distributions of gait absolute attribute scores and annotator-dependent gait relative attribute label distributions. Moreover, we propose a differentiable annotator-independent uncertainty layer to estimate the gait relative attribute score distribution from the absolute distributions then map it to a relative attribute label distribution using the computation of cumulative distribution functions. Furthermore, we propose another annotator-dependent uncertainty layer to estimate the uncertainty on the gait relative attribute labels in terms of a set of trainable transition matrices. Finally, we design a joint loss function on the relative attribute label distribution to learn the model parameters. Experiments on two gait relative attribute datasets demonstrated the effectiveness of the proposed method against baselines in quantitative and qualitative evaluations.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322006768",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Data mining",
      "Differentiable function",
      "Distribution (mathematics)",
      "Gait",
      "Gaussian",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Physiology",
      "Programming language",
      "Quantum mechanics",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Shehata",
        "given_name": "Allam"
      },
      {
        "surname": "Makihara",
        "given_name": "Yasushi"
      },
      {
        "surname": "Muramatsu",
        "given_name": "Daigo"
      },
      {
        "surname": "Ahad",
        "given_name": "Md Atiqur Rahman"
      },
      {
        "surname": "Yagi",
        "given_name": "Yasushi"
      }
    ]
  },
  {
    "title": "Editorial for special section 'IbPRIA22′",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.02.020",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552300051X",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Computer science",
      "Context (archaeology)",
      "Convolutional neural network",
      "Data science",
      "Debris",
      "Deep learning",
      "Engineering",
      "Garbage",
      "Geography",
      "Marine debris",
      "Meteorology",
      "Programming language",
      "Systems engineering",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Georgieva",
        "given_name": "Petia"
      }
    ]
  },
  {
    "title": "Feature learning network with transformer for multi-label image classification",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109203",
    "abstract": "The purpose of multi-label image classification task is to accurately assign a set of labels to the objects in images. Although promising results have been achieved, most of the existing methods cannot effectively learn multi-scale features, so it is difficult to identify small-scale objects from images. Besides, current attention-based methods tend to learn the most salient feature regions in images, but fail to excavate various potential useful features concealed by the most salient feature, thus limiting the further improvement of model performance. To address above issues, we propose a novel Feature Learning network based on Transformer to learn salient features and excavate potential useful features (FL-Tran). Specifically, in order to solve the problem that current methods are difficult to identify small-scale objects, we first present a novel multi-scale fusion module (MSFM) to align high-level features and low-level features to learn multi-scale features. Additionally, a spatial attention module (SAM) utilizing transformer encoder is introduced to capture salient object features in images to enhance the model performance. Furthermore, we devise a feature enhancement and suppression module (FESM) with the aim of excavating potential useful features concealed by the most salient features. By suppressing the most salient features obtained in current SAM layer, and then forcing subsequent SAM layer to excavate potential salient features in feature maps, FL-Tran model can learn various useful features more comprehensively. Extensive experiments on MS-COCO 2014, PASCAL VOC 2007, and NUS-WIDE datasets demonstrate that our proposed FL-Tran model outperforms current state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322006823",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Electrical engineering",
      "Encoder",
      "Engineering",
      "Feature (linguistics)",
      "Feature learning",
      "Linguistics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Salient",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Wei"
      },
      {
        "surname": "Dou",
        "given_name": "Peng"
      },
      {
        "surname": "Su",
        "given_name": "Tao"
      },
      {
        "surname": "Hu",
        "given_name": "Haifeng"
      },
      {
        "surname": "Zheng",
        "given_name": "Zhijie"
      }
    ]
  },
  {
    "title": "Learning to restore multiple image degradations simultaneously",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109250",
    "abstract": "Image corruptions are common in the real world, for example images in the wild may come with unknown blur, bias field, noise, or other kinds of non-linear distributional shifts, thus hampering encoding methods and rendering downstream task unreliable. Image upgradation requires a complicated balance between high-level contextualised information and spatial specific details. Existing approaches to solving the problems are designed to focus on single corruption, which unavoidably results in poor performance when the acquisitions suffer from multiple degradations. In this study, we investigate the possibility of handling multiple degradations and enhancing the quality of images via deblurring, bias field correction, and denoising. To tackle the problems with propagating errors caused by independent learning, we propose a unified and scalable framework, which consists of three special decoders. Two decoders learn artifact attention from provided images thereby generating realistic individual artifact and multiple artifacts on single image; the third decoder is trained towards removing artifact on the synthetic image with multiple corruptions thereby generating high quality image. We additionally provide improvements over previous image degradation synthesis approaches by modelling multiple image degradations directly from data observations. We first create a toy MNIST dataset and investigate the properties of the proposed algorithm. We then use brain MRI datasets to demonstrate our method’s robustness, including both simulated (where necessary) and real-world artifacts. In addition, our method can be used for single/or multiple degradation(s) synthesis by implementing the learned degradation operators in a new domain from a given dataset. The code will be released upon acceptance of the paper.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322007294",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Database",
      "Deblurring",
      "Gene",
      "Image (mathematics)",
      "Image processing",
      "Image quality",
      "Image restoration",
      "Noise (video)",
      "Pattern recognition (psychology)",
      "Rendering (computer graphics)",
      "Robustness (evolution)",
      "Scalability"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Le"
      },
      {
        "surname": "Bronik",
        "given_name": "Kevin"
      },
      {
        "surname": "Papież",
        "given_name": "Bartłomiej W."
      }
    ]
  },
  {
    "title": "YOLOSA: Object detection based on 2D local feature superimposed self-attention",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.03.003",
    "abstract": "We analyzed the network structure of real-time object detection models and found that the features in the feature concatenation stage are very rich. Applying an attention module here can effectively improve the detection accuracy of the model. However, the commonly used attention module or self-attention module shows poor performance in detection accuracy and inference efficiency. Therefore, we propose a novel self-attention module, called 2D local feature superimposed self-attention, for the feature concatenation stage of the neck network. This self-attention module reflects global features through local features and local receptive fields. We also propose and optimize an efficient decoupled head and AB-OTA, and achieve SOTA results. Average precisions of 49.0% (71FPS, 14ms), 46.1% (85FPS, 11.7ms), and 39.1% (107FPS, 9.3ms) were obtained for large, medium, and small-scale models built using our proposed improvements. Our models exceeded YOLOv5 by 0.8% – 3.1% in average precision.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523000673",
    "keywords": [
      "Artificial intelligence",
      "Attention network",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Concatenation (mathematics)",
      "Feature (linguistics)",
      "Inference",
      "Linguistics",
      "Mathematics",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Weisheng"
      },
      {
        "surname": "Huang",
        "given_name": "Lin"
      }
    ]
  },
  {
    "title": "A new image decomposition approach using pixel-wise analysis sparsity model",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109241",
    "abstract": "Decomposing an image into two ‘simpler’ layers has been widely used in low-level vision tasks, such as image recovery and enhancement. It is an ill-posed problem since the number of unknowns are larger than the input. In this paper, a two-step strategy is introduced, including task-aware priors estimate and a decomposition model. A pixel-wise analysis sparsity model is proposed to regularize the separation layers, which supposes the transformed image generated with analysis operator is sparse. Unlike regularizing all pixels with one penalty weight, we try to estimate each pixel’s sparsity level with task-aware priors and to achieve pixel-wise sparse penalty. Additionally, one separation layer is regularized with both synthesis sparsity model and pixel-wise analysis sparsity model to exploit their complementary mechanisms. Unlike the analysis one utilizing image local features, the synthesis one exploits an over-complete dictionary and non-local similarity cues to provide flexible prior for regularizing the decomposition results. The proposed model is solved by an alternating optimization algorithm. We evaluate it with two applications, Retinex model and rain streaks removal. Extensive experiments on multiple enhancement datasets, many synthetic and real rainy images demonstrate that our method can remove imaging noise during Retinex decomposition, and can produce high fidelity deraining results. It achieves competing performance in terms of quantitative metrics and visual quality compared with the state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322007208",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Bayesian probability",
      "Biology",
      "Color constancy",
      "Computer science",
      "Computer vision",
      "Decomposition",
      "Ecology",
      "Image (mathematics)",
      "Noise (video)",
      "Noise reduction",
      "Pattern recognition (psychology)",
      "Pixel",
      "Prior probability"
    ],
    "authors": [
      {
        "surname": "Du",
        "given_name": "Shuangli"
      },
      {
        "surname": "Liu",
        "given_name": "Yiguang"
      },
      {
        "surname": "Zhao",
        "given_name": "Minghua"
      },
      {
        "surname": "Xu",
        "given_name": "Zhenyu"
      },
      {
        "surname": "Li",
        "given_name": "Jie"
      },
      {
        "surname": "You",
        "given_name": "Zhenzhen"
      }
    ]
  },
  {
    "title": "Deep attentive time warping",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109201",
    "abstract": "Similarity measures for time series are important problems for time series classification. To handle the nonlinear time distortions, Dynamic Time Warping (DTW) has been widely used. However, DTW is not learnable and suffers from a trade-off between robustness against time distortion and discriminative power. In this paper, we propose a neural network model for task-adaptive time warping. Specifically, we use the attention model, called the bipartite attention model, to develop an explicit time warping mechanism with greater distortion invariance. Unlike other learnable models using DTW for warping, our model predicts all local correspondences between two time series and is trained based on metric learning, which enables it to learn the optimal data-dependent warping for the target task. We also propose to induce pre-training of our model by DTW to improve the discriminative power. Extensive experiments demonstrate the superior effectiveness of our model over DTW and its state-of-the-art performance in online signature verification.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032200680X",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Discriminative model",
      "Dynamic time warping",
      "Gene",
      "Image warping",
      "Pattern recognition (psychology)",
      "Robustness (evolution)",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Matsuo",
        "given_name": "Shinnosuke"
      },
      {
        "surname": "Wu",
        "given_name": "Xiaomeng"
      },
      {
        "surname": "Atarsaikhan",
        "given_name": "Gantugs"
      },
      {
        "surname": "Kimura",
        "given_name": "Akisato"
      },
      {
        "surname": "Kashino",
        "given_name": "Kunio"
      },
      {
        "surname": "Iwana",
        "given_name": "Brian Kenji"
      },
      {
        "surname": "Uchida",
        "given_name": "Seiichi"
      }
    ]
  },
  {
    "title": "Coarse-to-fine feature representation based on deformable partition attention for melanoma identification",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109247",
    "abstract": "In the histopathological melanoma image diagnosis system, manual identification of super-scale slides with dense cells is tedious, time-consuming, and subjective. To deal with this problem, we propose an automatic identification network based on the deformable partition attention to identify lots of dense slides as an assistant. A coarse-to-fine strategy is adopted in feature representation and qualitative identification to improve the identification accuracy of melanomas and nevi. First of all, because it is difficult to extract features in the lesion area with blurred boundaries and uneven distribution, we develop a deformable partition attention module, which integrates the advantage of the attention mechanism and deformable convolution. The module overcomes the limitation of rectangular convolution and gradually refines the channel and spatial features, which enriches feature representation by combining global and local features. Secondly, to address the problem of difficult convergence and poor recognition rate caused by the excessive non-aligned distance between benign-malignant and benign subcategories, we propose a progressive architecture via a coarse sub-network closely followed by a fine sub-network. Moreover, to further increase the inter-class differences and reduce the intra-class disparities, we propose a joint loss function to mine hard samples, which effectively improves the identification performance. Experimental results on the clinical dataset show that the proposed algorithm has higher sensitivity and specificity and outperforms state-of-the-art deep neural networks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322007269",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Botany",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Convolution (computer science)",
      "Feature (linguistics)",
      "Identification (biology)",
      "Law",
      "Linguistics",
      "Mathematics",
      "Partition (number theory)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Dong"
      },
      {
        "surname": "Yang",
        "given_name": "Jing"
      },
      {
        "surname": "Du",
        "given_name": "Shaoyi"
      },
      {
        "surname": "Han",
        "given_name": "Hongcheng"
      },
      {
        "surname": "Ge",
        "given_name": "Yuyan"
      },
      {
        "surname": "Zhu",
        "given_name": "Longfei"
      },
      {
        "surname": "Li",
        "given_name": "Ce"
      },
      {
        "surname": "Xu",
        "given_name": "Meifeng"
      },
      {
        "surname": "Zheng",
        "given_name": "Nanning"
      }
    ]
  },
  {
    "title": "Linear discriminant analysis with generalized kernel constraint for robust image classification",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109196",
    "abstract": "Linear discriminant analysis (LDA) as a classical supervised dimensionality reduction method has shown powerful capability in various image classification tasks. The purpose of LDA seeks an optimal linear transformation that maps the original data to a low-dimensional space. Inspired by the fact that the kernel trick can capture the nonlinear similarity of features, we propose a novel generalized distance constraint dubbed intra-class and inter-class kernel constraint (IIKC). The proposed IIKC explicitly models the category kernel distance and focuses on helping the original LDA capture more discriminant features in order to further improve the separability and magnitude difference between nearby data points. Our proposed method with IIKC aims to achieve maximum category separability by minimizing the intra-class kernel distances as well as maximizing the inter-class kernel distance, simultaneously. Extensive experimental results on six publicly available benchmark databases illustrate that the LDA-based methods embedded with the proposed IIKC significantly improve the discrimination ability and achieve a better classification performance than the original and state-of-the-art LDA algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322006756",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biochemistry",
      "Chemistry",
      "Combinatorics",
      "Computer science",
      "Constraint (computer-aided design)",
      "Dimensionality reduction",
      "Gene",
      "Geodesy",
      "Geography",
      "Geometry",
      "Kernel (algebra)",
      "Kernel Fisher discriminant analysis",
      "Kernel method",
      "Linear discriminant analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Support vector machine",
      "Transformation (genetics)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Shuyi"
      },
      {
        "surname": "Zhang",
        "given_name": "Hengmin"
      },
      {
        "surname": "Ma",
        "given_name": "Ruijun"
      },
      {
        "surname": "Zhou",
        "given_name": "Jianhang"
      },
      {
        "surname": "Wen",
        "given_name": "Jie"
      },
      {
        "surname": "Zhang",
        "given_name": "Bob"
      }
    ]
  },
  {
    "title": "Joint classification and prediction of random curves using heavy‐tailed process functional regression",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109213",
    "abstract": "We propose a heavy-tailed process functional regression to jointly perform classification and prediction of time-varying functional data. We use two independent scale mixtures of Gaussian Processes to respectively model random effects and random errors, yielding robust inferences against both magnitude and shape outliers. We classify random curves by posterior predictive probabilities of class labels and offer a weighted prediction of future curve trends. A Bayesian estimation procedure is implemented through an MCMC sampling algorithm. The performance of classification and prediction of the proposed model is evaluated using simulated studies and some real data sets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322006926",
    "keywords": [
      "Artificial intelligence",
      "Bayesian probability",
      "Computer science",
      "Gaussian",
      "Gaussian process",
      "Machine learning",
      "Mathematics",
      "Outlier",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Regression",
      "Scale (ratio)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Cao",
        "given_name": "Chunzheng"
      },
      {
        "surname": "Liu",
        "given_name": "Xin"
      },
      {
        "surname": "Cao",
        "given_name": "Shuren"
      },
      {
        "surname": "Shi",
        "given_name": "Jian Qing"
      }
    ]
  },
  {
    "title": "Editorial for pattern recognition letters special issue on face-based emotion understanding",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.02.022",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523000521",
    "keywords": [
      "Agronomy",
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Channel (broadcasting)",
      "Computer network",
      "Computer science",
      "Economics",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Management",
      "Pattern recognition (psychology)",
      "Pruning",
      "Selection (genetic algorithm)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Jingting"
      },
      {
        "surname": "Yap",
        "given_name": "Moi Hoon"
      },
      {
        "surname": "Cheng",
        "given_name": "Wen-Huang"
      },
      {
        "surname": "See",
        "given_name": "John"
      },
      {
        "surname": "Hong",
        "given_name": "Xiaopeng"
      },
      {
        "surname": "Li",
        "given_name": "Xiaobai"
      },
      {
        "surname": "Wang",
        "given_name": "Su-Jing"
      }
    ]
  },
  {
    "title": "Micro-expression spotting with multi-scale local transformer in long videos",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.03.012",
    "abstract": "Micro-expression analysis by computer vision techniques has attracted much attention as it can reveal the human emotions automatically. Among the analysis tasks, the temporal spotting is the most challenging task for achieving expression-aware frames from long video sequences. Compared to the well studied recognition task, more researches need to be devoted to the spotting task for further improving the performance and benefiting the subsequent tasks. So, in this paper, we propose a convolutional transformer based deep model for micro-expression spotting in long video sequences. A 3D convolutional subnetwork is firstly employed to extract the visual features from the temporal frames in a fixed-size sliding window of original video sequence. Then a multi-scale local transformer module is designed based on the visual features to model the correlation between frames in a local window. By leveraging the correlation information, the description of face movement becomes more representative for various-duration micro-expressions. Finally, the multi-head classifier and the corresponding estimator are jointly combined to predict the temporal position for spotting micro-expressions. The proposed method is evaluated on two publicly-available datasets, namely CAS(ME) 2 and SAMM-LV, and achieves the promising performance of 0.2770 F1-score on SAMM-LV and 0.1373 F1-score on CAS(ME) 2 . The code is publicly available on GitHub (https://github.com/xiazhaoqiang/MULT-MicroExpressionSpot).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523000776",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Computer vision",
      "Operating system",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Sliding window protocol",
      "Speech recognition",
      "Spotting",
      "Transformer",
      "Voltage",
      "Window (computing)"
    ],
    "authors": [
      {
        "surname": "Guo",
        "given_name": "Xupeng"
      },
      {
        "surname": "Zhang",
        "given_name": "Xiaobiao"
      },
      {
        "surname": "Li",
        "given_name": "Lei"
      },
      {
        "surname": "Xia",
        "given_name": "Zhaoqiang"
      }
    ]
  },
  {
    "title": "Hybrid optimization with unconstrained variables on partial point cloud registration",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109267",
    "abstract": "3D point cloud registration is a fundamental problem in computer vision (CV) and computer graphics (CG). Recently, a series of learning-based algorithms have been proposed to show the advantages in registration accuracy and inference speed. However, those learning-based methods usually ignore transformations with constrained rotations and translations in registration. In this paper, we propose a novel hybrid optimization method to solve the constrained rotational and translational transformations. A mapping function is introduced to deal with the restrained variables in optimization. Our method achieves superior performance on the Multi-View Partial Point dataset, which won the first place on the registration challenge in ICCV 2021. The method is also validated on the synthetic datasets ModelNet, ICL-NUIM, and the realistic 3DMatch dataset. We demonstrate that the global optimization methods still have great potential research for point cloud registration. The code is available at https://github.com/Dizzy-cell/HOUV.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322007464",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Cloud computing",
      "Code (set theory)",
      "Computer graphics",
      "Computer graphics (images)",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Geometry",
      "Graphics",
      "Image (mathematics)",
      "Image registration",
      "Inference",
      "Machine learning",
      "Mathematics",
      "Operating system",
      "Optimization problem",
      "Point (geometry)",
      "Point cloud",
      "Programming language",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Yan",
        "given_name": "Yuanjie"
      },
      {
        "surname": "An",
        "given_name": "Junyi"
      },
      {
        "surname": "Zhao",
        "given_name": "Jian"
      },
      {
        "surname": "Shen",
        "given_name": "Furao"
      }
    ]
  },
  {
    "title": "Comparing filter and wrapper approaches for feature selection in handwritten character recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.02.028",
    "abstract": "It is generally agreed that the selection of an appropriate set of features is a fundamental process in the development of any pattern recognition system. Its purpose is to identify the truly distinctive subset of features to reduce the size of the search space, without decreasing the classification performance. This problem is particularly relevant in the field of handwriting recognition, due to the enormous variability of character shape, which has led to the development of a large variety of feature sets that are becoming increasingly larger in terms of the number of attributes. While promising, the results achieved so far have several limitations, which include, among others, the computational complexity of selecting and evaluating feature subsets and the difficulty in evaluating the interactions among features. In a previous study, we tried to overcome some of the above limitations by adopting a feature-ranking-based technique: a large study was carried out considering different filter-based techniques for feature subset evaluation. The aim of this work is to extend the previous study by presenting a broad comparison between filter and wrapper techniques for feature selection in the field of handwritten character recognition. In the experiments, we analysed one of the most effective and widely used set of features in handwriting recognition, applied to standard real-word databases of handwritten characters. The experimental results confirmed that filter and wrapper approaches achieve similar performances, with the former selecting fewer features at a lower computational cost.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523000582",
    "keywords": [
      "Artificial intelligence",
      "Character (mathematics)",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Feature extraction",
      "Feature selection",
      "Field (mathematics)",
      "Filter (signal processing)",
      "Geometry",
      "Handwriting",
      "Handwriting recognition",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Pure mathematics",
      "Ranking (information retrieval)",
      "Selection (genetic algorithm)",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Cilia",
        "given_name": "Nicole Dalia"
      },
      {
        "surname": "D’Alessandro",
        "given_name": "Tiziana"
      },
      {
        "surname": "De Stefano",
        "given_name": "Claudio"
      },
      {
        "surname": "Fontanella",
        "given_name": "Francesco"
      },
      {
        "surname": "Scotto di Freca",
        "given_name": "Alessandra"
      }
    ]
  },
  {
    "title": "Transformer vision-language tracking via proxy token guided cross-modal fusion",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.02.023",
    "abstract": "Tracking by vision-language is an emergent topic. Previous researchers mainly adopt CNN and sequential models for video and language encoding, however, their methods are limited by poor generalization performance. To address this problem, this paper presents a novel vision-language tracking framework based on Transformer. Specifically, our proposed framework contains the image encoder, language encoder, cross-modal fusion module, and task-specific heads. We adopt the residual network and BERT for image and language embedding, respectively. More importantly, we propose a proxy token guided cross-modal fusion module based on the transformer network, which can link the vision and language features effectively and efficiently. The proxy token acts as a proxy for word embeddings and interacts with the visual feature. By absorbing vision information, the proxy token is used to modulate word embeddings and make them attend to the visual feature. Finally, we get the organically fused features via a dynamic modal aggregation method and feed them into the task-specific heads for tracking. Extensive experiments demonstrate that our method set new state-of-the-art on multiple language-assisted tracking datasets, including OTB-LANG, LaSOT, TNL2K, and a newly proposed Ref-LTB50 annotated with dense language specifications. Source code of this paper will be publicly available.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523000545",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Encoder",
      "Eye tracking",
      "Modal",
      "Natural language processing",
      "Operating system",
      "Physics",
      "Polymer chemistry",
      "Quantum mechanics",
      "Security token",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Haojie"
      },
      {
        "surname": "Wang",
        "given_name": "Xiao"
      },
      {
        "surname": "Wang",
        "given_name": "Dong"
      },
      {
        "surname": "Lu",
        "given_name": "Huchuan"
      },
      {
        "surname": "Ruan",
        "given_name": "Xiang"
      }
    ]
  },
  {
    "title": "The neglected background cues can facilitate finger vein recognition",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109199",
    "abstract": "Recently, finger vein based biometric authentication has attracted considerable attention due to its high efficiency and high security. However, most existing finger vein representation methods focus on vein traits while ignoring background cues, although background cues also convey identity information specific to each individual. In this paper, we leverage background intensity variations in finger vein images as new features to enrich discriminative representation, and accordingly propose a new descriptor named Intensity Orientation Vector (IOV). IOV, scaleable to reflect characteristics of finger tissues, offers additional informative cues for finger vein representation. Furthermore, we propose a new learning scheme named Semantic Similarity Preserved Discrete Binary Feature Learning (SSP-DBFL) for finger vein recognition. Unlike the most bimodal binary feature representation methods, SSP-DBFL preserves high-level semantic similarity in a common Hamming space to exploit the consensus between vein traits and background cues. Specifically, given a finger vein image, we first extract the direction difference vectors (DDV) as the main vein traits and the IOV as the auxiliary background cues. Subsequently, we jointly learn projection functions from these two types of features in a supervised manner, converting the two features into discriminative binary codes with their semantic similarity preserved. Finally, the binary codes are pooled into histogram-based vectors for finger vein representation. Extensive experiments are conducted on five widely used finger vein databases and demonstrate the effectiveness of our proposed IOV and SSP-DBFL.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322006781",
    "keywords": [
      "Artificial intelligence",
      "Biometrics",
      "Computer science",
      "Computer vision",
      "Discriminative model",
      "Feature extraction",
      "Feature learning",
      "Histogram",
      "Image (mathematics)",
      "Law",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Pengyang"
      },
      {
        "surname": "Zhao",
        "given_name": "Shuping"
      },
      {
        "surname": "Xue",
        "given_name": "Jing-Hao"
      },
      {
        "surname": "Yang",
        "given_name": "Wenming"
      },
      {
        "surname": "Liao",
        "given_name": "Qingmin"
      }
    ]
  },
  {
    "title": "A pyramid input augmented multi-scale CNN for GGO detection in 3D lung CT images",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109261",
    "abstract": "This paper proposes a new convolutional neural network (CNN) with multi-scale processing for detecting ground-glass opacity nodules (GGO) in 3D computed tomography (CT) images, which is referred to as PiaNet for short. PiaNet consists of a feature-extraction module and a prediction module. The former module is constructed by introducing pyramid multi-scale source connections into a contracting-expanding structure. Besides, a new multi-receptive-field convolution block (MRCB) is presented to fuse the convolutions with multiple kernels of varying sizes for capturing features in each scale of information better. The latter module includes a bounding-box regressor and a classifier that are employed to simultaneously recognize GGO nodules and estimate bounding boxes at multiple scales. To train the proposed PiaNet, a two-stage transfer learning strategy is developed. In the first stage, the feature-extraction module is embedded into a classifier network that is trained on a large data set of GGO and non-GGO patches, which are generated by performing data augmentation from a small number of annotated CT scans. In the second stage, the pretrained feature-extraction module is loaded into PiaNet, and then PiaNet is fine-tuned using the annotated CT scans. We evaluate the proposed PiaNet with the LIDC-IDRI dataset. The experimental results demonstrate that our method outperforms state-of-the-art counterparts, including the Subsolid CAD and Aidence systems and CPM-Net and S4ND and GA-SSD methods. PiaNet achieves a sensitivity of 93.6% with only one false positive per scan.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322007403",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Feature (linguistics)",
      "Feature extraction",
      "Image (mathematics)",
      "Linguistics",
      "Minimum bounding box",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Weihua"
      },
      {
        "surname": "Liu",
        "given_name": "Xiabi"
      },
      {
        "surname": "Luo",
        "given_name": "Xiongbiao"
      },
      {
        "surname": "Wang",
        "given_name": "Murong"
      },
      {
        "surname": "Han",
        "given_name": "Guanghui"
      },
      {
        "surname": "Zhao",
        "given_name": "Xinming"
      },
      {
        "surname": "Zhu",
        "given_name": "Zheng"
      }
    ]
  },
  {
    "title": "Understanding and combating robust overfitting via input loss landscape analysis and regularization",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109229",
    "abstract": "Adversarial training is widely used to improve the robustness of deep neural networks to adversarial attack. However, adversarial training is prone to overfitting, and the cause is far from clear. This work sheds light on the mechanisms underlying overfitting through analyzing the loss landscape w.r.t. the input. We find that robust overfitting results from standard training, specifically the minimization of the clean loss, and can be mitigated by regularization of the loss gradients. Moreover, we find that robust overfitting turns severer during adversarial training partially because the gradient regularization effect of adversarial training becomes weaker due to the increase in the loss landscape’s curvature. To improve robust generalization, we propose a new regularizer to smooth the loss landscape by penalizing the weighted logits variation along the adversarial direction. Our method significantly mitigates robust overfitting and achieves the highest robustness and efficiency compared to similar previous methods. Code is available at https://github.com/TreeLLi/Combating-RO-AdvLC.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322007087",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Biology",
      "Computer science",
      "Deep learning",
      "Deep neural networks",
      "Gene",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Overfitting",
      "Pattern recognition (psychology)",
      "Regularization (linguistics)",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Lin"
      },
      {
        "surname": "Spratling",
        "given_name": "Michael"
      }
    ]
  },
  {
    "title": "Detecting and grouping keypoints for multi-person pose estimation using instance-aware attention",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109232",
    "abstract": "Bottom-up human pose estimation models detect keypoints and learn associative information between keypoints, usually requiring human predefined offset fields or embeddings for keypoints grouping (clustering). In this paper, we present a brand new method that can entirely solve these problems based on Transformer, making the grouping process free of the human-defined associative signals. Specifically, the self-attention in vision Transformer measures feature similarity between any pair of locations, which provides a metric space to associate keypoints together into corresponding human instances. However, the naive attention patterns formed in Transformer are still not subjectively controlled, so there is no guarantee that the keypoints only attend to the instances to which they belong. To address it we propose a novel approach of supervising self-attention to be instance-aware, simultaneously accomplishing multi-person keypoint detection and clustering. By doing so, we can group the detected keypoints to their corresponding instances, according to the pairwise attention scores. An additional benefit of our method is that the instance segmentation results of any number of people can be directly obtained from the supervised attention matrix, thereby simplifying the pixel assignment pipeline. The qualitative and quantitative results on the COCO shows that, with a very simple architecture design, our method can achieve comparable performance against the CNN-based bottom-up counterparts with fewer parameters, which also demonstrate a promising way to control self-attention mechanism behavior for specific purposes.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322007117",
    "keywords": [
      "Artificial intelligence",
      "Associative property",
      "Cluster analysis",
      "Computer science",
      "Machine learning",
      "Mathematics",
      "Offset (computer science)",
      "Pairwise comparison",
      "Pattern recognition (psychology)",
      "Physics",
      "Pose",
      "Programming language",
      "Pure mathematics",
      "Quantum mechanics",
      "Segmentation",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Sen"
      },
      {
        "surname": "Feng",
        "given_name": "Ze"
      },
      {
        "surname": "Wang",
        "given_name": "Zhicheng"
      },
      {
        "surname": "Li",
        "given_name": "Yanjie"
      },
      {
        "surname": "Zhang",
        "given_name": "Shoukui"
      },
      {
        "surname": "Quan",
        "given_name": "Zhibin"
      },
      {
        "surname": "Xia",
        "given_name": "Shu-tao"
      },
      {
        "surname": "Yang",
        "given_name": "Wankou"
      }
    ]
  },
  {
    "title": "Adversarial training with distribution normalization and margin balance",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109182",
    "abstract": "Adversarial training is the most effective method to improve adversarial robustness. However, it does not explicitly regularize the feature space during training. Adversarial attacks usually move a sample iteratively along the direction which causes the steepest ascent of classification loss by crossing decision boundary. To alleviate this problem, we propose to regularize the distributions of different classes to increase the difficulty of finding an attacking direction. Specifically, we propose two strategies named Distribution Normalization (DN) and Margin Balance (MB) for adversarial training. The purpose of DN is to normalize the features of each class to have identical variance in every direction, in order to eliminate easy-to-attack intra-class directions. The purpose of MB is to balance the margins between different classes, making it harder to find confusing class directions (i.e., those with smaller margins) to attack. When integrated with adversarial training, our method can significantly improve adversarial robustness. Extensive experiments under white-box, black-box, and adaptive attacks demonstrate the effectiveness of our method over other state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322006616",
    "keywords": [
      "Adversarial system",
      "Anthropology",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Decision boundary",
      "Feature vector",
      "Gene",
      "Machine learning",
      "Margin (machine learning)",
      "Mathematical optimization",
      "Mathematics",
      "Normalization (sociology)",
      "Pattern recognition (psychology)",
      "Robustness (evolution)",
      "Sociology",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Cheng",
        "given_name": "Zhen"
      },
      {
        "surname": "Zhu",
        "given_name": "Fei"
      },
      {
        "surname": "Zhang",
        "given_name": "Xu-Yao"
      },
      {
        "surname": "Liu",
        "given_name": "Cheng-Lin"
      }
    ]
  },
  {
    "title": "A Sampling-Based Density Peaks Clustering Algorithm for Large-Scale Data",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109238",
    "abstract": "With the rapid development of information technology, massive amount of data is generated. How to discover useful information to support decision-making has become one of the focuses of scholar's research. Clustering is thought to be one of the main means to deal with large-scale data. Density peaks clustering (DPC) is an effective density-based clustering algorithm which is widely applied in numerous fields because of its satisfactory performance. However, the computational complexity of DPC is O ( N 2 ) which is not friendly to large-scale data. To solve this issue, a sampling-based density peaks clustering algorithm for large-scale data (SDPC) is proposed. Firstly, a sampling method is used to reduce the distance calculations. Secondly, approximate representatives are identified by an improved TI search strategy which further accelerates the clustering process. Afterwards, the approximate representatives are clustered by DPC. Finally, the remaining points are allocated to the same cluster as its nearest representatives. Experimental results on both synthetic datasets and real-world datasets illustrate that SDPC is more efficient than DPC, while its clustering performance maintains the same level as DPC.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322007178",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "CURE data clustering algorithm",
      "Canopy clustering algorithm",
      "Cluster (spacecraft)",
      "Cluster analysis",
      "Computer science",
      "Computer vision",
      "Correlation clustering",
      "Data mining",
      "Filter (signal processing)",
      "Operating system",
      "Physics",
      "Process (computing)",
      "Programming language",
      "Quantum mechanics",
      "Sampling (signal processing)",
      "Scale (ratio)"
    ],
    "authors": [
      {
        "surname": "Ding",
        "given_name": "Shifei"
      },
      {
        "surname": "Li",
        "given_name": "Chao"
      },
      {
        "surname": "Xu",
        "given_name": "Xiao"
      },
      {
        "surname": "Ding",
        "given_name": "Ling"
      },
      {
        "surname": "Zhang",
        "given_name": "Jian"
      },
      {
        "surname": "Guo",
        "given_name": "Lili"
      },
      {
        "surname": "Shi",
        "given_name": "Tianhao"
      }
    ]
  },
  {
    "title": "TETFN: A text enhanced transformer fusion network for multimodal sentiment analysis",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109259",
    "abstract": "Multimodal sentiment analysis (MSA), which aims to recognize sentiment expressed by speakers in videos utilizing textual, visual and acoustic cues, has attracted extensive research attention in recent years. However, textual, visual and acoustic modalities often contribute differently to sentiment analysis. In general, text contains more intuitive sentiment-related information and outperforms nonlinguistic modalities in MSA. Seeking a strategy to take advantage of this property to obtain a fusion representation containing more sentiment-related information and simultaneously preserving inter- and intra-modality relationships becomes a significant challenge. To this end, we propose a novel method named Text Enhanced Transformer Fusion Network (TETFN), which learns text-oriented pairwise cross-modal mappings for obtaining effective unified multimodal representations. In particular, it incorporates textual information in learning sentiment-related nonlinguistic representations through text-based multi-head attention. In addition to preserving consistency information by cross-modal mappings, it also retains the differentiated information among modalities through unimodal label prediction. Furthermore, the vision pre-trained model Vision-Transformer is utilized to extract visual features from the original videos to preserve both global and local information of a human face. Extensive experiments on benchmark datasets CMU-MOSI and CMU-MOSEI demonstrate the superior performance of the proposed TETFN over state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322007385",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Consistency (knowledge bases)",
      "Geodesy",
      "Geography",
      "Law",
      "Machine learning",
      "Modalities",
      "Natural language processing",
      "Pairwise comparison",
      "Pattern recognition (psychology)",
      "Physics",
      "Political science",
      "Politics",
      "Quantum mechanics",
      "Representation (politics)",
      "Sentiment analysis",
      "Social science",
      "Sociology",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Di"
      },
      {
        "surname": "Guo",
        "given_name": "Xutong"
      },
      {
        "surname": "Tian",
        "given_name": "Yumin"
      },
      {
        "surname": "Liu",
        "given_name": "Jinhui"
      },
      {
        "surname": "He",
        "given_name": "LiHuo"
      },
      {
        "surname": "Luo",
        "given_name": "Xuemei"
      }
    ]
  },
  {
    "title": "On better detecting and leveraging noisy samples for learning with severe label noise",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109210",
    "abstract": "Despite the success of learning with noisy labels, existing approaches show limited performance when the noise level is extremely high, since deep neural networks (DNNs) are easily overfit to the training set with corrupted labels. In this paper, we introduce Lipschitz regularization to prevent the DNNs from over-fitting to noisy labels quickly. Meanwhile, to better detect and leverage the noisy samples, we propose a Lipschitz regularization based framework with a combination of adaptive modeling and detection module and improved semi-supervised learning. We propose to adaptively model the real distribution of the training set, and the implicit individual clean/noisy distribution, instead of parametric models. With Bayes’ rule, we then compute the posterior probability of a sample being clean, which provides a dynamic threshold for the detection of noisy labels. To reduce training instability caused by less labeled data with severe label noise, we improve the semi-supervised learning by combining the advantages of Mixup and FixMatch. It can not only increase the diversity of unlabeled samples, but also improve the generalization capability of the DNNs to avoid over-fitting. Experiments on several benchmarks demonstrate that our approach achieves comparable results with the state-of-the-art methods in the less-noisy environment, and obtains a substantial improvement ( ∼ 8% and ∼ 6% in accuracy on CIFAR-10 and CIFAR-100 respectively) with severe noise.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322006896",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Bayesian probability",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Deep neural networks",
      "Gene",
      "Image (mathematics)",
      "Leverage (statistics)",
      "Machine learning",
      "Mathematics",
      "Noise (video)",
      "Overfitting",
      "Parametric statistics",
      "Pattern recognition (psychology)",
      "Prior probability",
      "Regularization (linguistics)",
      "Robustness (evolution)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Miao",
        "given_name": "Qing"
      },
      {
        "surname": "Wu",
        "given_name": "Xiaohe"
      },
      {
        "surname": "Xu",
        "given_name": "Chao"
      },
      {
        "surname": "Zuo",
        "given_name": "Wangmeng"
      },
      {
        "surname": "Meng",
        "given_name": "Zhaopeng"
      }
    ]
  },
  {
    "title": "An ensemble hierarchical clustering algorithm based on merits at cluster and partition levels",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109255",
    "abstract": "Ensemble clustering has emerged as a combination of several basic clustering algorithms to achieve high quality final clustering. However, this technique is challenging due to the complexities in primary clusters such as overlapping, vagueness, instability and uncertainty. Typically, ensemble clustering uses all the primary clusters into partitions for consensus, where the merits of a cluster or a partition can be considered to improve the quality of the consensus. In general, the robustness of a partition may be poorly measured, while having some high-quality clusters. Inspired by the evaluation of cluster and partition, this paper proposes an ensemble hierarchical clustering algorithm based on the cluster consensus selection approach. Here, the selection of a subset of primary clusters from partitions based on their merit level is emphasized. Merit level is defined using the development of Normalized Mutual Information measure. Clusters of basic clustering algorithms that satisfy the predefined threshold of this measure are selected to participate in the final consensus. In addition, the consensus of the selected primary clusters to create the final clusters is performed based on the clusters clustering technique. In this technique, the selected primary clusters are re-clustered to create hyper-clusters. Finally, the final clusters are formed by assigning instances to hyper-clusters with the highest similarity. Here, an innovative criterion based on merit and cluster size for defining similarity is presented. The performance of the proposed algorithm has been proven by extensive experiments on real-world datasets from the UCI repository compared to state-of-the-art algorithms such as CPDM, ENMI, IDEA, CFTLC and SSCEN.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322007348",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "CURE data clustering algorithm",
      "Chemistry",
      "Cluster (spacecraft)",
      "Cluster analysis",
      "Combinatorics",
      "Computer science",
      "Consensus clustering",
      "Correlation clustering",
      "Data mining",
      "Fuzzy clustering",
      "Gene",
      "Hierarchical clustering",
      "Mathematics",
      "Partition (number theory)",
      "Pattern recognition (psychology)",
      "Programming language",
      "Robustness (evolution)",
      "Similarity measure",
      "Single-linkage clustering"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Qirui"
      },
      {
        "surname": "Gao",
        "given_name": "Rui"
      },
      {
        "surname": "Akhavan",
        "given_name": "Hoda"
      }
    ]
  },
  {
    "title": "Small-object detection based on YOLOv5 in autonomous driving systems",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.03.009",
    "abstract": "With the rapid advancements in the field of autonomous driving, the need for faster and more accurate object detection frameworks has become a necessity. Many recent deep learning-based object detectors have shown compelling performance for the detection of large objects in a variety of real-time driving applications. However, the detection of small objects such as traffic signs and traffic lights is a challenging task owing to the complex nature of such objects. Additionally, the complexity present in a few images due to the existence of foreground/background imbalance and perspective distortion caused by adverse weather and low-lighting conditions further makes it difficult to detect small objects accurately. In this letter, we investigate how an existing object detector can be adjusted to address specific tasks and how these modifications can impact the detection of small objects. To achieve this, we explore and introduce architectural changes to the popular YOLOv5 model to improve its performance in the detection of small objects without sacrificing the detection accuracy of large objects, particularly in autonomous driving. We will show that our modifications barely increase the computational complexity but significantly improve the detection accuracy and speed. Compared to the conventional YOLOv5, the proposed iS-YOLOv5 model increases the mean Average Precision (mAP) by 3.35% on the BDD100K dataset. Nevertheless, our proposed model improves the detection speed by 2.57 frames per second (FPS) compared to the YOLOv5 model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523000727",
    "keywords": [
      "Amplifier",
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Detector",
      "Distortion (music)",
      "Economics",
      "Field (mathematics)",
      "Management",
      "Mathematics",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Perspective (graphical)",
      "Pure mathematics",
      "Real-time computing",
      "Task (project management)",
      "Telecommunications",
      "Variety (cybernetics)"
    ],
    "authors": [
      {
        "surname": "Mahaur",
        "given_name": "Bharat"
      },
      {
        "surname": "Mishra",
        "given_name": "K.K."
      }
    ]
  },
  {
    "title": "Cross-scale cascade transformer for multimodal human action recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.02.024",
    "abstract": "Human action recognition can benefit from multimodal information to address the classification problem under complex situations. However, existing works either use score fusion or perform simple feature integration methods to combine multiple heterogeneous modalities which failed to effectively utilize multimodal complementary information. In this paper, we proposed a Cross-Scale Cascade Multimodal Fusion Transformer (CSCMFT) to perform interaction and fusion among modalities of multi-scale features, thus obtaining a multimodal complementary representation for RGB-D-based human action recognition. Cross-Modal Cross-Scale Mixer (CCM) is the basic component in CSCMFT, which captures cross-modal relations and propagates the fused information across scales. Furthermore, our CSCMFT can still achieve significant improvements when applied to different multimodal combinations, indicating its generality and scalability. Experimental results show that CSCMFT fully exploits complementary semantic information between RGB and depth maps and outperforms state-of-the-art RGB-D-based methods on NTU RGB+D 60 & 120 and PKU-MMD datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523000533",
    "keywords": [
      "Action recognition",
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Computer security",
      "Database",
      "Electrical engineering",
      "Engineering",
      "Exploit",
      "Generality",
      "Machine learning",
      "Modalities",
      "Pattern recognition (psychology)",
      "Psychology",
      "Psychotherapist",
      "RGB color model",
      "Scalability",
      "Social science",
      "Sociology",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Zhen"
      },
      {
        "surname": "Cheng",
        "given_name": "Qin"
      },
      {
        "surname": "Song",
        "given_name": "Chengqun"
      },
      {
        "surname": "Cheng",
        "given_name": "Jun"
      }
    ]
  },
  {
    "title": "Local Linear Embedding with Adaptive Neighbors",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109205",
    "abstract": "Dimensionality reduction is one of the most important techniques in the field of data mining. It embeds high-dimensional data into a low-dimensional vector space while keeping the main information as much as possible. Locally Linear Embedding (LLE) as a typical manifold learning algorithm computes neighborhood preserving embeddings of high-dimensional inputs. Based on the thought of LLE, we propose a novel unsupervised dimensionality reduction model called Local Linear Embedding with Adaptive Neighbors (LLEAN). To achieve a desirable dimensionality reduction result, we impose adaptive neighbor strategy and adopt a projection matrix to project data into an optimal subspace. The relationship between every pair-wise data is investigated to help reveal the data structure. Augmented Lagrangian Multiplier (ALM) is devised in optimization procedure to effectively solve the proposed objective function. Comprehensive experiments on toy data and benchmark datasets have been done and the results show that LLEAN outperforms other state-of-the-art dimensionality reduction methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322006847",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Augmented Lagrangian method",
      "Benchmark (surveying)",
      "Cluster analysis",
      "Clustering high-dimensional data",
      "Computer science",
      "Curse of dimensionality",
      "Diffusion map",
      "Dimensionality reduction",
      "Embedding",
      "Geodesy",
      "Geography",
      "Geometry",
      "Linear subspace",
      "Mathematics",
      "Nonlinear dimensionality reduction",
      "Pattern recognition (psychology)",
      "Projection (relational algebra)",
      "Subspace topology"
    ],
    "authors": [
      {
        "surname": "Xue",
        "given_name": "Jiaqi"
      },
      {
        "surname": "Zhang",
        "given_name": "Bin"
      },
      {
        "surname": "Qiang",
        "given_name": "Qianyao"
      }
    ]
  },
  {
    "title": "Joint optimization of scoring and thresholding models for online multi-label classification",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109167",
    "abstract": "Existing online multi-label classification works cannot well handle the online label thresholding problem and lack regret analysis for their online algorithms. This paper proposes a novel framework of joint optimization of scoring and thresholding models for online multi-label classification, with the aim to overcome the above drawbacks. The key feature of our framework is that both scoring and thresholding models are included as important components of the online multi-label classifier and are incorporated into one online optimization problem. Based on this framework, we present two adaptive label thresholding algorithms and two fixed thresholding algorithms. For each type of algorithms, a first-order method and a second-order one are provided for updating the online multi-label classifier. Both methods enjoy a closed-form update. Our proposed algorithms are proved to achieve a sub-linear regret. Using Mercer kernels, two first-order algorithms can be extended to handle nonlinear multi-label prediction tasks. Experiments show the advantage of the adaptive and the fixed thresholding algorithms, in terms of various multi-label performance metrics.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032200646X",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Computer security",
      "Data mining",
      "Image (mathematics)",
      "Key (lock)",
      "Machine learning",
      "Multi-label classification",
      "Pattern recognition (psychology)",
      "Regret",
      "Thresholding"
    ],
    "authors": [
      {
        "surname": "Zhai",
        "given_name": "Tingting"
      },
      {
        "surname": "Wang",
        "given_name": "Hao"
      },
      {
        "surname": "Tang",
        "given_name": "Hongcheng"
      }
    ]
  },
  {
    "title": "Visual question answering from another perspective: CLEVR mental rotation tests",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109209",
    "abstract": "Different types of mental rotation tests have been used extensively in psychology to understand human visual reasoning and perception. Understanding what an object or visual scene would look like from another viewpoint is a challenging problem that is made even harder if it must be performed from a single image. We explore a controlled setting whereby questions are posed about the properties of a scene if that scene was observed from another viewpoint. To do this we have created a new version of the CLEVR dataset that we call CLEVR Mental Rotation Tests (CLEVR-MRT). Using CLEVR-MRT we examine standard methods, show how they fall short, then explore novel neural architectures that involve inferring volumetric representations of a scene. These volumes can be manipulated via camera-conditioned transformations to answer the question. We examine the efficacy of different model variants through rigorous ablations and demonstrate the efficacy of volumetric representations.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322006884",
    "keywords": [
      "Artificial intelligence",
      "Cognition",
      "Cognitive psychology",
      "Computer science",
      "Computer vision",
      "Mental image",
      "Mental rotation",
      "Neuroscience",
      "Object (grammar)",
      "Perception",
      "Perspective (graphical)",
      "Psychology",
      "Question answering",
      "Rotation (mathematics)",
      "Visual perception"
    ],
    "authors": [
      {
        "surname": "Beckham",
        "given_name": "Christopher"
      },
      {
        "surname": "Weiss",
        "given_name": "Martin"
      },
      {
        "surname": "Golemo",
        "given_name": "Florian"
      },
      {
        "surname": "Honari",
        "given_name": "Sina"
      },
      {
        "surname": "Nowrouzezahrai",
        "given_name": "Derek"
      },
      {
        "surname": "Pal",
        "given_name": "Christopher"
      }
    ]
  },
  {
    "title": "AdaNS: Adaptive negative sampling for unsupervised graph representation learning",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109266",
    "abstract": "Recently, unsupervised graph representation learning has attracted considerable attention through effectively encoding graph-structured data without semantic annotations. To accelerate its training, noise contrastive estimation (NCE) samples uniformly negative examples to fit an unnormalized graph model. However, this uniform sampling strategy may easily lead to slow convergence, even the vanishing gradient problem. In this paper, we theoretically show that sampling those hard negatives close to the current anchor can relieve the above difficulties. With this finding, we then propose an Adaptive Negative Sampling strategy, namely AdaNS, which efficiently samples the hard negatives from the mixing distribution regarding the dimensional elements of the current node representation. Experiments show that our AdaNS sampling strategy applied on top of representative unsupervised models, e.g., DeepWalk, GraphSAGE, can outperform the existing negative sampling strategies in the tasks of node classification and visualization. This also further demonstrates that sampling those hard negatives can bring performance improvements for learning the node representations.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322007452",
    "keywords": [
      "Adaptive sampling",
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Engineering",
      "Feature learning",
      "Filter (signal processing)",
      "Graph",
      "Law",
      "Machine learning",
      "Mathematics",
      "Monte Carlo method",
      "Node (physics)",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Sampling (signal processing)",
      "Statistics",
      "Structural engineering",
      "Theoretical computer science",
      "Unsupervised learning"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Yu"
      },
      {
        "surname": "Hu",
        "given_name": "Liang"
      },
      {
        "surname": "Gao",
        "given_name": "Wanfu"
      },
      {
        "surname": "Cao",
        "given_name": "Xiaofeng"
      },
      {
        "surname": "Chang",
        "given_name": "Yi"
      }
    ]
  },
  {
    "title": "Learning spatiotemporal embedding with gated convolutional recurrent networks for translation initiation site prediction",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109234",
    "abstract": "Accurately predicting translation initiation sites (TIS) from genomic sequences is crucial for understanding gene regulation and function. TIS prediction methods’ feature vectors are not discriminative enough to lead to unsatisfactory predictive results. In this work, we devise an efficient gated convolutional recurrent network (GCR-Net) with residual learning to dynamically extract dependency patterns of raw genomic sequences in an efficient fusion strategy and successfully improve the performance of the TIS prediction. GCR-Net mainly includes exponential gated convolutional residual networks (EGCRN) and bidirectional gated recurrent unit (Bi-GRU) networks. Particularly, we devise the novel EGCRN to extract multiple complex patterns of the spatial dimension from genomic sequences, where we design an exponential gated linear unit (EGLU) to reduce the vanishing gradient problem. Moreover, we combine EGLU with shortcut connections to develop the stacked gated mechanism based on convolutions that benefit information propagation across layers. Then, we use Bi-GRU with identity connections to learn long-term dependency patterns of the temporal dimension from genomic sequences. Besides, we evaluate our GCR-Net model on four TIS datasets, and experiments demonstrate that GCR-Net is an efficient deep learning-based TIS prediction tool and obtains superior performance compared to the baseline methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322007130",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Biology",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Dependency (UML)",
      "Discriminative model",
      "Embedding",
      "Gene",
      "Machine learning",
      "Messenger RNA",
      "Pattern recognition (psychology)",
      "Residual",
      "Translation (biology)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Weihua"
      },
      {
        "surname": "Guo",
        "given_name": "Yanbu"
      },
      {
        "surname": "Wang",
        "given_name": "Bingyi"
      },
      {
        "surname": "Yang",
        "given_name": "Bei"
      }
    ]
  },
  {
    "title": "Deep hybrid model for single image dehazing and detail refinement",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109227",
    "abstract": "Deep learning technologies have been applied in Single Image Dehazing (SID) tasks successfully. However, most SID algorithms seldom consider to refine image details during dehazing. Therefore, there exist some detail-loss regions in dehazed results. To solve this issue, we design a deep hybrid network to improve dehazing performance and remedy the loss of details. Different from existing algorithms that usually ignore detail refinement and adopt a unified framework to remove haze, we propose to treat dehazing and detail refinement as two separate tasks, so that each task could be solved via different ways. Particularly, we design two sub-networks with a multi-term loss function. First, for removing haze effectively, we introduce the Squeeze-and-Excitation (SE) to design a haze residual attention sub-network, which is used to reconstruct the dehazed image. Second, as for remedying details, we take the previous dehazed image as the input to a detail refinement sub-network, where the image details can be enhanced via multi-scale contextual information aggregation. Through the joint training of two sub-network, the haze can be removed clearly and the image details can be preserved well. Moreover, the detail refinement sub-network can be detached into other existing dehazing methods to improve their model performances. Extensive experiments also verify the superiority of our proposed network against recently proposed state-of-the-arts.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322007063",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Haze",
      "Image (mathematics)",
      "Image processing",
      "Image restoration",
      "Meteorology",
      "Physics",
      "Residual"
    ],
    "authors": [
      {
        "surname": "Jiang",
        "given_name": "Nanfeng"
      },
      {
        "surname": "Hu",
        "given_name": "Kejian"
      },
      {
        "surname": "Zhang",
        "given_name": "Ting"
      },
      {
        "surname": "Chen",
        "given_name": "Weiling"
      },
      {
        "surname": "Xu",
        "given_name": "Yiwen"
      },
      {
        "surname": "Zhao",
        "given_name": "Tiesong"
      }
    ]
  },
  {
    "title": "An experimental study on marine debris location and recognition using object detection",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2022.12.019",
    "abstract": "The large amount of debris in our oceans is a global problem that dramatically impacts marine fauna and flora. While a large number of human-based campaigns have been proposed to tackle this issue, these efforts have been deemed insufficient due to the insurmountable amount of existing litter. In response to that, there exists a high interest in the use of autonomous underwater vehicles (AUV) that may locate, identify, and collect this garbage automatically. To perform such a task, AUVs consider state-of-the-art object detection techniques based on deep neural networks due to their reported high performance. Nevertheless, these techniques generally require large amounts of data with fine-grained annotations. In this work, we explore the capabilities of the reference object detector Mask Region-based Convolutional Neural Networks for automatic marine debris location and classification in the context of limited data availability. Considering the recent CleanSea corpus, we pose several scenarios regarding the amount of available train data and study the possibility of mitigating the adverse effects of data scarcity with synthetic marine scenes. Our results achieve a new state of the art in the task, establishing a new reference for future research. In addition, it is shown that the task still has room for improvement and that the lack of data can be somehow alleviated, yet to a limited extent.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522003889",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Computer science",
      "Context (archaeology)",
      "Convolutional neural network",
      "Debris",
      "Deep learning",
      "Engineering",
      "Geography",
      "Machine learning",
      "Marine debris",
      "Meteorology",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Systems engineering",
      "Task (project management)",
      "Underwater"
    ],
    "authors": [
      {
        "surname": "Sánchez-Ferrer",
        "given_name": "Alejandro"
      },
      {
        "surname": "Valero-Mas",
        "given_name": "Jose J."
      },
      {
        "surname": "Gallego",
        "given_name": "Antonio Javier"
      },
      {
        "surname": "Calvo-Zaragoza",
        "given_name": "Jorge"
      }
    ]
  },
  {
    "title": "A generality hard channel pruning with adaptive compression rate selection for HRNet",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.03.007",
    "abstract": "Network pruning extends a promising prospect to neural network compression. However, applications of existing methods to various vision tasks, e.g., human pose estimation, are limited by the parameter distribution difference, shortage of general pruning strategies and high pruning cost. As a complex and widely used backbone network, HRNet is an effective benchmark for studying these issues. We propose an adaptive Hard channel pruning method with an Adaptive Compression Rate to replace manual experience in pruned model selection, called HACR. It first designs a hard criterion that takes γ and β to measure channel importance with the insight of the consistency relationship between the distribution adjustment of the channel and the final output distribution. Then, a partial pruning strategy is advanced for the complex structural network by only pruning channels without cross-layer integration and keeping the network infrastructure. Next, we adaptively predict an optimal compression rate based on the validation metric of unfine-tuned models, providing a reference for compression parameter selection. The processes of HACR are simplified to adaptive pruned model selection and fine-tuning with lower pruning cost. Furthermore, the numerical distribution of γ and β is used to extend HACR in common pruning task benchmarks, based on the a principle that lower level channels should be preferentially retained. We conduct HRNet pruning experiments on COCO2017 and MPII, and promote extension study on CIFAR-10 to verify the generality of strategy. The results show that HACR is effective on multiple tasks, e.g. HRNet-W48 reduces 58.2% Params and 50.1% FLOPS, and VGG-16 reduces 91.5% Params and 67.5% FLOPS with only 0.3% precision loss.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523000740",
    "keywords": [
      "Agronomy",
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Channel (broadcasting)",
      "Computer network",
      "Computer science",
      "Data compression ratio",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Image compression",
      "Image processing",
      "Machine learning",
      "Pruning",
      "Selection (genetic algorithm)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Dongjingdian"
      },
      {
        "surname": "Gao",
        "given_name": "Shouwan"
      },
      {
        "surname": "Chen",
        "given_name": "Pengpeng"
      },
      {
        "surname": "Cheng",
        "given_name": "Lei"
      }
    ]
  },
  {
    "title": "A multi-layer memory sharing network for video captioning",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109202",
    "abstract": "Over the past several years, video captioning has received much attention in computer vision and machine learning communities. Many models utilize an RNN-based decoder to generate sentences describing the content of a video. They have achieved much progress; however, few methods adopt a decoder with more than three layers because an RNN-based model with more layers may become hard to train, time-consuming or even deteriorate at a certain depth. To address the limitation, we propose a Multi-layer memory sharing Network, MesNet for short, which allows more layers to be stacked without compromising performance. In MesNet, we construct a novel memory sharing structure to strengthen the connections between layers and make the model easier to train. More specifically, we design an Enhanced Gated Recurrent Unit (En-GRU) and stack it to construct a deeper network. Unlike traditional RNN-based multi-layer networks, the memory states of all layers in MesNet are cross-used at each iteration to mimic the brain’s complex connections. Extensive experiments on MSVD and MSR-VTT demonstrate that our method performs well and outperforms some state-of-the-art methods significantly. Our code is available at https://github.com/nbbb/MesNet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322006811",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Chemistry",
      "Closed captioning",
      "Code (set theory)",
      "Computer network",
      "Computer science",
      "Construct (python library)",
      "Decoding methods",
      "Image (mathematics)",
      "Layer (electronics)",
      "Organic chemistry",
      "Programming language",
      "Recurrent neural network",
      "Set (abstract data type)",
      "Stack (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Niu",
        "given_name": "Tian-Zi"
      },
      {
        "surname": "Dong",
        "given_name": "Shan-Shan"
      },
      {
        "surname": "Chen",
        "given_name": "Zhen-Duo"
      },
      {
        "surname": "Luo",
        "given_name": "Xin"
      },
      {
        "surname": "Huang",
        "given_name": "Zi"
      },
      {
        "surname": "Guo",
        "given_name": "Shanqing"
      },
      {
        "surname": "Xu",
        "given_name": "Xin-Shun"
      }
    ]
  },
  {
    "title": "Twin SVM for conditional probability estimation in binary and multiclass classification",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109253",
    "abstract": "In this paper, we estimate the conditional probability function by presenting a new twin SVM model (CPTWSVM) in binary and multiclass classification problems. The motivation of CPTWSVM is to implement the empirical risk minimization on training data, which is hard to realize in traditional twin SVMs. In each subproblem of CPTWSVM, it measures the empirical risk and outputs the corresponding probability estimate of each class, which eliminates the problems of inconsistent measurement in twin SVMs. Though an additional discriminant objective function is introduced, the optimization problem size of each subproblem is smaller than conditional probability SVM, and is solved by block decomposition algorithm efficiently. In addition, we extend CPTWSVM to multiclass classification by estimating the conditional probability of each class, and maintaining the above properties. Numerical experiments on benchmark and real application datasets demonstrate that CPTWSVM outputs the estimate of probability and the data projection well, resulting in better generalization ability than some leading TWSVMs communities, in terms of binary and multiclass classification.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322007324",
    "keywords": [
      "Arithmetic",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Binary classification",
      "Binary number",
      "Computer science",
      "Conditional probability",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Mathematics",
      "Multiclass classification",
      "Pattern recognition (psychology)",
      "Statistics",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Shao",
        "given_name": "Yuan-Hai"
      },
      {
        "surname": "Lv",
        "given_name": "Xiao-Jing"
      },
      {
        "surname": "Huang",
        "given_name": "Ling-Wei"
      },
      {
        "surname": "Bai",
        "given_name": "Lan"
      }
    ]
  },
  {
    "title": "Brain-like retinex: A biologically plausible retinex algorithm for low light image enhancement",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109195",
    "abstract": "Retinex theory was first proposed by Land and McCann [1], where retinex is a portmanteau derived from the words of retina and cortex, implying that both the retina and cerebral cortex may participate in the perception of lightness and color. However, there are no recent reports on how the retina and visual cortex perform retinex decomposition. In this paper, we propose a biologically plausible solution to retinex decomposition. We develop an algorithm motivated by the primate’s retinal circuit to detect textural gradients, design an algorithm originating from the visual cortex to extract image contours, and thus split image edges into image contours and textural gradients. Then, we establish a variational model for retinex decomposition by using image contours and textural gradients to encode discontinuities in illumination and variations in reflectance, respectively. We also apply the proposed retinex model to low light image enhancement, high dynamic resolution image toning, and color constancy. Experiments show consistent superiority of the proposed algorithm. The code is available at Github.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322006744",
    "keywords": [
      "Artificial intelligence",
      "Color constancy",
      "Computer science",
      "Computer vision",
      "Image (mathematics)",
      "Lightness",
      "Neuroscience",
      "Psychology",
      "Visual cortex"
    ],
    "authors": [
      {
        "surname": "Cai",
        "given_name": "Rongtai"
      },
      {
        "surname": "Chen",
        "given_name": "Zekun"
      }
    ]
  },
  {
    "title": "A Deep Learning-based Fast Fake News Detection Model for Cyber-Physical Social Services",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.02.026",
    "abstract": "With the prevalence of social network service in cyber-Physical space, flow of various fake news has been a rough issue for operators of social service. Although many theoretical outcomes have been produced in recent years, they are generally challenged by processing speed of semantic modeling. To solve this issue, this paper presents a deep learning-based fast fake news detection model for cyber-physical social services. Taking Chinese text as the objective, each character in Chinese text is directly adopted as the basic processing unit. Considering the fact that the news are generally short texts and can be remarkably featured by some keywords, convolution-based neural computing framework is adopted to extract feature representation for news texts. Such design is able to ensure both processing speed and detection ability in scenes of Chinese short texts. At last, some experiments are conducted for evaluation on a real-world dataset collected from a Chinese social media. The results show that the proposal possesses lower training time cost as well as higher classification accuracy compared with baseline methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523000569",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Cyber-physical system",
      "Data science",
      "Deep learning",
      "Fake news",
      "Internet privacy",
      "Machine learning",
      "Operating system",
      "Social media",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Qin"
      },
      {
        "surname": "Guo",
        "given_name": "Zhiwei"
      },
      {
        "surname": "Zhu",
        "given_name": "Yanyan"
      },
      {
        "surname": "Vijayakumar",
        "given_name": "Pandi"
      },
      {
        "surname": "Castiglione",
        "given_name": "Aniello"
      },
      {
        "surname": "Gupta",
        "given_name": "Brij B."
      }
    ]
  },
  {
    "title": "TreEnhance: A tree search method for low-light image enhancement",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109249",
    "abstract": "In this paper we present TreEnhance, an automatic method for low-light image enhancement capable of improving the quality of digital images. The method combines tree search theory, and in particular the Monte Carlo Tree Search (MCTS) algorithm, with deep reinforcement learning. Given as input a low-light image, TreEnhance produces as output its enhanced version together with the sequence of image editing operations used to obtain it. During the training phase, the method repeatedly alternates two main phases: a generation phase, where a modified version of MCTS explores the space of image editing operations and selects the most promising sequence, and an optimization phase, where the parameters of a neural network, implementing the enhancement policy, are updated. Two different inference solutions are proposed for the enhancement of new images: one is based on MCTS and is more accurate but more time and memory consuming; the other directly applies the learned policy and is faster but slightly less precise. As a further contribution, we propose a guided search strategy that “reverses” the enhancement procedure that a photo editor applied to a given input image. Unlike other methods from the state of the art, TreEnhance does not pose any constraint on the image resolution and can be used in a variety of scenarios with minimal tuning. We tested the method on two datasets: the Low-Light dataset and the Adobe Five-K dataset obtaining good results from both a qualitative and a quantitative point of view.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322007282",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer science",
      "Computer vision",
      "Constraint (computer-aided design)",
      "Genetics",
      "Geometry",
      "Image (mathematics)",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Point (geometry)",
      "Sequence (biology)",
      "Tree (set theory)"
    ],
    "authors": [
      {
        "surname": "Cotogni",
        "given_name": "Marco"
      },
      {
        "surname": "Cusano",
        "given_name": "Claudio"
      }
    ]
  },
  {
    "title": "Prior depth-based multi-view stereo network for online 3D model reconstruction",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109198",
    "abstract": "This study addresses the online multi-view stereo (MVS) problem when reconstructing precise 3D models in real time. To solve this problem, most previous studies adopted a motion stereo approach that sequentially estimates depth maps from multiple localized images captured in a local time window. To compute the depth maps quickly, the motion stereo methods process down-sampled images or use a simplified algorithm for cost volume regularization; therefore, they generally produce reconstructed 3D models that are inaccurate. In this paper, we propose a novel online MVS method that accurately reconstructs high-resolution 3D models. This method infers prior depth information based on sequentially estimated depths and leverages it to estimate depth maps more precisely. The method constructs a cost volume by using the prior-depth-based visibility information and then fuses the prior depths into the cost volume. This approach significantly improves the stereo matching performance and completeness of the estimated depths. Extensive experiments showed that the proposed method outperforms other state-of-the-art MVS and motion stereo methods. In particular, it significantly improves the completeness of 3D models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032200677X",
    "keywords": [
      "3D reconstruction",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Depth map",
      "Geography",
      "Image (mathematics)",
      "Matching (statistics)",
      "Mathematics",
      "Meteorology",
      "Motion (physics)",
      "Operating system",
      "Physics",
      "Process (computing)",
      "Quantum mechanics",
      "Regularization (linguistics)",
      "Statistics",
      "Structure from motion",
      "Visibility",
      "Volume (thermodynamics)"
    ],
    "authors": [
      {
        "surname": "Song",
        "given_name": "Soohwan"
      },
      {
        "surname": "Truong",
        "given_name": "Khang Giang"
      },
      {
        "surname": "Kim",
        "given_name": "Daekyum"
      },
      {
        "surname": "Jo",
        "given_name": "Sungho"
      }
    ]
  },
  {
    "title": "Multi-scale self-attention mixup for graph classification",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.03.013",
    "abstract": "Data augmentation can effectively improve the generalization performance of neural networks. However, data augmentation for the graph domain is challenging due to the fact of irregular nature of the special non-Euclidean structure. In this paper, we propose a novel graph data augmentation solution, Multi-Scale Self-Attention Mixup (MSSA-Mixup), which extends the training distribution by interpolating multi-scale graph representation with self-attention. The MSSA-Mixup improves the generalization ability of graph neural networks (GNNs) effectively. Extensive experiments illustrate that the proposed method yields consistent and robust performance boost across graph classification tasks on the frequently-used benchmark datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523000788",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Euclidean geometry",
      "External Data Representation",
      "Generalization",
      "Geometry",
      "Graph",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Theoretical computer science",
      "Training set"
    ],
    "authors": [
      {
        "surname": "Kong",
        "given_name": "Youyong"
      },
      {
        "surname": "Li",
        "given_name": "Jiaxing"
      },
      {
        "surname": "Zhang",
        "given_name": "Ke"
      },
      {
        "surname": "Wu",
        "given_name": "Jiasong"
      }
    ]
  },
  {
    "title": "Training Compact DNNs with ℓ 1 / 2 Regularization",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109206",
    "abstract": "Deep neural network(DNN) has achieved unprecedented success in many fields. However, its large model parameters which bring a great burden on storage and calculation hinder the development and application of DNNs. It is worthy of compressing the model to reduce the complexity of the DNN. Sparsity-inducing regularizer is one of the most common tools for compression. In this paper, we propose utilizing the ℓ 1 / 2 quasi-norm to zero out weights of neural networks and compressing the networks automatically during the learning process. To our knowledge, it is the first work applying the non-Lipschitz continuous regularizer for the compression of DNNs. The resulting sparse optimization problem is solved by stochastic proximal gradient algorithm. For further convenience of calculation, an approximation of the threshold-form solution to the proximal operator with ℓ 1 / 2 is given at the same time. Extensive experiments with various datasets and baselines demonstrate the advantages of our new method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322006859",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Deep neural networks",
      "Gradient descent",
      "Law",
      "Lipschitz continuity",
      "Mathematical analysis",
      "Mathematics",
      "Norm (philosophy)",
      "Operating system",
      "Political science",
      "Process (computing)"
    ],
    "authors": [
      {
        "surname": "Tang",
        "given_name": "Anda"
      },
      {
        "surname": "Niu",
        "given_name": "Lingfeng"
      },
      {
        "surname": "Miao",
        "given_name": "Jianyu"
      },
      {
        "surname": "Zhang",
        "given_name": "Peng"
      }
    ]
  },
  {
    "title": "Joint depth map super-resolution method via deep hybrid-cross guidance filter",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109260",
    "abstract": "Nowadays color-guided Depth map Super-Resolution (DSR) methods mainly have three thorny problems: (1) joint DSR methods have serious detail and structure loss at very high sampling rate; (2) existing DSR networks have high computational complexity; (3) color-depth inconsistency makes it hard to fuse dual-modality features. To resolve these problems, we propose a joint hybrid-cross guidance filter method to progressively recover the quality of degraded Low-Resolution (LR) depth maps by exploiting color-depth consistency from multiple perspectives. Specifically, the proposed method leverages pyramid structure to extract multi-scale features from High-Resolution (HR) color image. At each scale, hybrid side window filter block is proposed to achieve high-efficiency color feature extraction after each down-sampling for HR color image. This block is also used to extract depth features from the LR depth map. Meanwhile, we propose a multi-perspective cross-guided fusion filter block to progressively fuse high-quality multi-scale structure information of color image with corresponding enhanced depth features. In this filter block, two kinds of space-aware group-compensation modules are introduced to capture various spatial features from different perspectives. Meanwhile, color-depth cross-attention module is proposed to extract color-depth consistency features for impactful boundary preservation. Comprehensively qualitative and quantitative experimental results have demonstrated that our method can achieve superior performances against a lot of state-of-the-art depth SR approaches in terms of mean absolute deviation and root mean square error on Middlebury, NYU-v2 and RGB-D-D datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322007397",
    "keywords": [
      "Amplifier",
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Block (permutation group theory)",
      "Color space",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Depth map",
      "Distortion (music)",
      "Electrical engineering",
      "Engineering",
      "Feature (linguistics)",
      "Filter (signal processing)",
      "Fuse (electrical)",
      "Geometry",
      "Image (mathematics)",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pyramid (geometry)",
      "RGB color model"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Ke"
      },
      {
        "surname": "Zhao",
        "given_name": "Lijun"
      },
      {
        "surname": "Zhang",
        "given_name": "Jinjing"
      },
      {
        "surname": "Zhang",
        "given_name": "Jialong"
      },
      {
        "surname": "Wang",
        "given_name": "Anhong"
      },
      {
        "surname": "Bai",
        "given_name": "Huihui"
      }
    ]
  },
  {
    "title": "AuxBranch: Binarization residual-aware network design via auxiliary branch search",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109263",
    "abstract": "While network binarization is a promising method in memory saving and speedup on hardware, it inevitably leads to binarization residual of intermediate features, resulting in performance capability degradation. To alleviate the above issue, we focus on the network topology design scheme to the more suitable network structure for the extreme-low-bit scenario. In this paper, we propose the baseline-auxiliary expanding network design method to compensate for the binarization residual of features via searching for auxiliary branches, denoted as AuxBranch. The intermediate feature maps are reasonably enhanced by combining baseline and auxiliary features, mimicking the corresponding feature output of the full-precision network. In addition, we devise a hybrid performance estimator (PE) with three elements of preliminary accuracy, feature similarity, and computational complexity. The PE jointly performs an efficient architecture search for binarization baseline and enables automatic computation complexity adjustment under diverse constraints. Extensive experiments show that our approach is superior in terms of accuracy and computational performance, and is plug-and-play for different network backbones and binarization policies. Our code is available at https://github.com/VipaiLab/AuxBranch.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322007427",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Computation",
      "Computational complexity theory",
      "Computer science",
      "Computer security",
      "Estimator",
      "Feature (linguistics)",
      "Focus (optics)",
      "Geometry",
      "Image (mathematics)",
      "Linguistics",
      "Mathematics",
      "Network architecture",
      "Optics",
      "Parallel computing",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Residual",
      "Similarity (geometry)",
      "Speedup",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Fu",
        "given_name": "Siming"
      },
      {
        "surname": "Chu",
        "given_name": "Huanpeng"
      },
      {
        "surname": "Yu",
        "given_name": "Lu"
      },
      {
        "surname": "Peng",
        "given_name": "Bo"
      },
      {
        "surname": "Li",
        "given_name": "Zheyang"
      },
      {
        "surname": "Tan",
        "given_name": "Wenming"
      },
      {
        "surname": "Hu",
        "given_name": "Haoji"
      }
    ]
  },
  {
    "title": "Region-wise loss for biomedical image segmentation",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109208",
    "abstract": "We propose Region-wise (RW) loss for biomedical image segmentation. Region-wise loss is versatile, can simultaneously account for class imbalance and pixel importance, and it can be easily implemented as the pixel-wise multiplication between the softmax output and a RW map. We show that, under the proposed RW loss framework, certain loss functions, such as Active Contour and Boundary loss, can be reformulated similarly with appropriate RW maps, thus revealing their underlying similarities and a new perspective to understand these loss functions. We investigate the observed optimization instability caused by certain RW maps, such as Boundary loss distance maps, and we introduce a mathematically-grounded principle to avoid such instability. This principle provides excellent adaptability to any dataset and practically ensures convergence without extra regularization terms or optimization tricks. Following this principle, we propose a simple version of boundary distance maps called rectified Region-wise (RRW) maps that, as we demonstrate in our experiments, achieve state-of-the-art performance with similar or better Dice coefficients and Hausdorff distances than Dice, Focal, weighted Cross entropy, and Boundary losses in three distinct segmentation tasks. We quantify the optimization instability provided by Boundary loss distance maps, and we empirically show that our RRW maps are stable to optimize. The code to run all our experiments is publicly available at: https://github.com/jmlipman/RegionWiseLoss.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322006872",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Boundary (topology)",
      "Computer science",
      "Deep learning",
      "Image segmentation",
      "Instability",
      "Mathematical analysis",
      "Mathematics",
      "Mechanics",
      "Pattern recognition (psychology)",
      "Physics",
      "Pixel",
      "Programming language",
      "Rippling",
      "Segmentation",
      "Softmax function"
    ],
    "authors": [
      {
        "surname": "Valverde",
        "given_name": "Juan Miguel"
      },
      {
        "surname": "Tohka",
        "given_name": "Jussi"
      }
    ]
  },
  {
    "title": "An effective CNN and Transformer complementary network for medical image segmentation",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109228",
    "abstract": "The Transformer network was originally proposed for natural language processing. Due to its powerful representation ability for long-range dependency, it has been extended for vision tasks in recent years. To fully utilize the advantages of Transformers and Convolutional Neural Networks (CNNs), we propose a CNN and Transformer Complementary Network (CTCNet) for medical image segmentation. We first design two encoders by Swin Transformers and Residual CNNs to produce complementary features in Transformer and CNN domains, respectively. Then we cross-wisely concatenate these complementary features to propose a Cross-domain Fusion Block (CFB) for effectively blending them. In addition, we compute the correlation between features from the CNN and Transformer domains, and apply channel attention to the self-attention features by Transformers for capturing dual attention information. We incorporate cross-domain fusion, feature correlation and dual attention together to propose a Feature Complementary Module (FCM) for improving the representation ability of features. Finally, we design a Swin Transformer decoder to further improve the representation ability of long-range dependencies, and propose to use skip connections between the Transformer decoded features and the complementary features for extracting spatial details, contextual semantics and long-range information. Skip connections are performed in different levels for enhancing multi-scale invariance. Experimental results show that our CTCNet significantly surpasses the state-of-the-art image segmentation models based on CNNs, Transformers, and even Transformer and CNN combined models designed for medical image segmentation. It achieves superior performance on different medical applications, including multi-organ segmentation and cardiac segmentation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322007075",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Electrical engineering",
      "Encoder",
      "Engineering",
      "Image segmentation",
      "Operating system",
      "Pattern recognition (psychology)",
      "Segmentation",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Yuan",
        "given_name": "Feiniu"
      },
      {
        "surname": "Zhang",
        "given_name": "Zhengxiao"
      },
      {
        "surname": "Fang",
        "given_name": "Zhijun"
      }
    ]
  },
  {
    "title": "An efficient region expansion algorithm for regular triangulated meshes",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.02.014",
    "abstract": "Region expansion—the growth of regions to include all points within a certain distance of their perimeters—is a basic, widely applicable operation, but is expensive to perform exactly. It has been shown that, if the solution is approximated by relaxing the distance metric to the L ∞ -norm, efficiency can be greatly improved using properties of quadtrees. The method as described, however, requires the quadtrees to be square, both for the metric and the particular details of the algorithm. In some cases, such as spherical surface approximation, it is desirable for the quadtree nodes to be triangular instead. In this work, we thus describe an adaptation of the L ∞ -norm metric and the previously described algorithm to allow efficient approximation of region expansion in images represented as regular triangulated meshes. Like the original method for square quadtrees, our algorithm achieves sublinear time with respect to expansion radius.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523000466",
    "keywords": [
      "Algorithm",
      "Computer science",
      "Discrete mathematics",
      "Economics",
      "Geometry",
      "Law",
      "Mathematics",
      "Metric (unit)",
      "Norm (philosophy)",
      "Operations management",
      "Political science",
      "Polygon mesh",
      "Quadtree",
      "Square (algebra)",
      "Sublinear function"
    ],
    "authors": [
      {
        "surname": "Ondov",
        "given_name": "Brian"
      },
      {
        "surname": "Samet",
        "given_name": "Hanan"
      }
    ]
  },
  {
    "title": "Relation-aware attention for video captioning via graph learning",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109204",
    "abstract": "Video captioning often uses an attentive encoder-decoder as the baseline model. However, the conventional attention mechanism still remains two problems. First, the attended visual feature is often irrelevant to the target word state, because the attention process only uses the unidirectional flow from vision to linguistics, while lacking the reverse flow. Second, each attention result is independent, because it is computed only based on the previous word states while not considering the attention information from the past and future. This does not suit the attention habits of human beings. In this paper, we improve the conventional attention mechanism to a relation-aware attention mechanism. To this end, we propose two kinds of graph learning strategies, namely the linguistics-to-vision heterogeneous graph (HTG) and the vision-to-vision homogeneous graph (HMG). The HTG aims to enhance the inter-relation of attention by reversely modeling the relation of each word with respect to every attended visual feature, supporting proper semantic alignment in between. The HMG aims to enhance the intra-relation of attention by capturing the relations among all of the attended visual features, which can leverage the attention information from the past and future to guide the current attention process. Extensive experiments on two public datasets show that our proposed method not only significantly improves the baseline model, but also outperforms state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322006835",
    "keywords": [
      "Artificial intelligence",
      "Closed captioning",
      "Computer science",
      "Data mining",
      "Encoder",
      "Feature (linguistics)",
      "Graph",
      "Image (mathematics)",
      "Leverage (statistics)",
      "Linguistics",
      "Machine learning",
      "Natural language processing",
      "Operating system",
      "Philosophy",
      "Relation (database)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Tu",
        "given_name": "Yunbin"
      },
      {
        "surname": "Zhou",
        "given_name": "Chang"
      },
      {
        "surname": "Guo",
        "given_name": "Junjun"
      },
      {
        "surname": "Li",
        "given_name": "Huafeng"
      },
      {
        "surname": "Gao",
        "given_name": "Shengxiang"
      },
      {
        "surname": "Yu",
        "given_name": "Zhengtao"
      }
    ]
  },
  {
    "title": "The Dahu graph-cut for interactive segmentation on 2D/3D images",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109207",
    "abstract": "Interactive image segmentation is an important application in computer vision for selecting objects of interest in images. Several interactive segmentation methods are based on distance transform algorithms. However, the most known distance transform, geodesic distance, is sensitive to noise in the image and to seed placement. Recently, the Dahu pseudo-distance, a continuous version of the minimum barrier distance (MBD), is proved to be more powerful than the geodesic distance in noisy and blurred images. This paper presents a method for combining the Dahu pseudo-distance with edge information in a graph-cut optimization framework and leveraging each’s complementary strengths. Our method works efficiently on both 2D/3D images and videos. Results show that our method achieves better performance than other distance-based and graph-cut methods, thereby reducing the user’s efforts.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322006860",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Cut",
      "Distance transform",
      "Geodesic",
      "Graph",
      "Image (mathematics)",
      "Image segmentation",
      "Mathematical analysis",
      "Mathematics",
      "Segmentation",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Ôn Vũ Ngọc",
        "given_name": "Minh"
      },
      {
        "surname": "Carlinet",
        "given_name": "Edwin"
      },
      {
        "surname": "Fabrizio",
        "given_name": "Jonathan"
      },
      {
        "surname": "Géraud",
        "given_name": "Thierry"
      }
    ]
  },
  {
    "title": "SpatioTemporal focus for skeleton-based action recognition",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109231",
    "abstract": "Graph convolutional networks (GCNs) are widely adopted in skeleton-based action recognition due to their powerful ability to model data topology. We argue that the performance of recent proposed skeleton-based action recognition methods is limited by the following factors. First, the predefined graph structures are shared throughout the network, lacking the flexibility and capacity to model the multi-grain semantic information. Second, the relations among the global joints are not fully exploited by the graph local convolution, which may lose the implicit joint relevance. For instance, actions such as running and waving are performed by the co-movement of body parts and joints, e.g., legs and arms, however, they are located far away in physical connection. Inspired by the recent attention mechanism, we propose a multi-grain contextual focus module, termed MCF, to capture the action associated relation information from the body joints and parts. As a result, more explainable representations for different skeleton action sequences can be obtained by MCF. In this study, we follow the common practice that the dense sample strategy of the input skeleton sequences is adopted and this brings much redundancy since number of instances has nothing to do with actions. To reduce the redundancy, a temporal discrimination focus module, termed TDF, is developed to capture the local sensitive points of the temporal dynamics. MCF and TDF are integrated into the standard GCN network to form a unified architecture, named STF-Net. It is noted that STF-Net provides the capability to capture robust movement patterns from these skeleton topology structures, based on multi-grain context aggregation and temporal dependency. Extensive experimental results show that our STF-Net significantly achieves state-of-the-art results on three challenging benchmarks NTU-RGB+D 60, NTU-RGB+D 120, and Kinetics-Skeleton.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322007105",
    "keywords": [
      "Action recognition",
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Focus (optics)",
      "Graph",
      "Human skeleton",
      "Operating system",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Redundancy (engineering)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Liyu"
      },
      {
        "surname": "Zhang",
        "given_name": "Can"
      },
      {
        "surname": "Zou",
        "given_name": "Yuexian"
      }
    ]
  },
  {
    "title": "MSFA-Net: A convolutional neural network based on multispectral filter arrays for texture feature extraction",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.03.004",
    "abstract": "Multispectral snapshot cameras fitted with a multispectral filter array (MSFA) acquire several spectral bands in one shot and provide a raw mosaic image in which a single channel value is available at each pixel. Texture features are classically extracted from fully-defined images that are estimated by demosaicing. Such an estimation may however cause spatio-spectral artifacts. Moreover, texture feature extraction becomes computationally inefficient and yields to high-dimensional features as the number of bands increases. In this paper, we propose an original approach based on a convolutional neural network called MSFA-Net to capture spatio-spectral interactions in raw images at reduced computation costs. Experiments of multispectral image classification and outdoor image segmentation show that the proposed approach outperforms several hand-crafted and deep learning-based feature extractors.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523000703",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Feature (linguistics)",
      "Feature extraction",
      "Filter (signal processing)",
      "Linguistics",
      "Multispectral image",
      "Multispectral pattern recognition",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pixel",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Amziane",
        "given_name": "Anis"
      },
      {
        "surname": "Losson",
        "given_name": "Olivier"
      },
      {
        "surname": "Mathon",
        "given_name": "Benjamin"
      },
      {
        "surname": "Macaire",
        "given_name": "Ludovic"
      }
    ]
  },
  {
    "title": "Hyper-sausage coverage function neuron model and learning algorithm for image classification",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109216",
    "abstract": "Recently, deep neural networks (DNNs) promote mainly by network architectures and loss functions; however, the development of neuron models has been quite limited. In this study, inspired by the mechanism of human cognition, a hyper-sausage coverage function (HSCF) neuron model possessing a high flexible plasticity. Then, a novel cross-entropy and volume-coverage (CE_VC) loss is defined, which compresses the volume of the hyper-sausage to the hilt, and helps alleviate confusion among different classes, thus ensuring the intra-class compactness of the samples. Finally, a divisive iteration method is introduced, which considers each neuron model as a weak classifier, and iteratively increases the number of weak classifiers. Thus, the optimal number of the HSCF neuron is adaptively determined and an end-to-end learning framework is constructed. In particular, to improve the classification performance, the HSCF neuron can be applied to classical DNNs. Comprehensive experiments on eight datasets in several domains demonstrate the effectiveness of the proposed method. The proposed method exhibits the feasibility of boosting DNNs with neuron plasticity and provides a novel perspective for further developments in DNNs. The source code is available at https://github.com/Tough2011/HSCFNet.git .",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322006951",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Biological neuron model",
      "Classifier (UML)",
      "Computer science",
      "Contextual image classification",
      "Image (mathematics)",
      "Machine learning",
      "Operating system",
      "Pattern recognition (psychology)",
      "Source code"
    ],
    "authors": [
      {
        "surname": "Ning",
        "given_name": "Xin"
      },
      {
        "surname": "Tian",
        "given_name": "Weijuan"
      },
      {
        "surname": "He",
        "given_name": "Feng"
      },
      {
        "surname": "Bai",
        "given_name": "Xiao"
      },
      {
        "surname": "Sun",
        "given_name": "Le"
      },
      {
        "surname": "Li",
        "given_name": "Weijun"
      }
    ]
  },
  {
    "title": "Improving Parkinson’s disease recognition through voice analysis using deep learning",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.03.011",
    "abstract": "Parkinson’s disease (PD) corresponds to one of the most common neurological diseases in the world, which is mainly manifested by motor, cognitive, and language disorders. The change in the patient’s voice is one of the most striking clinical signs and proves to be an element of interest to support the diagnosis and the assessment of PD. In this research paper, a new approach based on speech signal analysis is set forward to automatically detect Parkinson’s disease. The approach evaluates two learning techniques, namely Support Vector Machines (SVM) and Convolutional Neural Networks (CNN), to classify data obtained from speech tasks. Two input data,i.e., the raw speech signal’s values and the i-vector features of dimensions 100, 200 and 300 are extracted in this study. Eventually, an evaluation step is undertaken through the use of five evaluation metrics which are accuracy, precision, recall/sensitivity, specificity and f -score. The most pertinent obtained results for a test dataset composed of 28 participants are recorded as follows: an accuracy of 100%, precision of 0.99, recall/sensitivity of 0.98, specificity of 0.96 and f -score of 0.98. These findings corroborate that our approach can help in terms of PD detection.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523000764",
    "keywords": [
      "Artificial intelligence",
      "Cognitive psychology",
      "Computer science",
      "Convolutional neural network",
      "Data set",
      "Electronic engineering",
      "Engineering",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Precision and recall",
      "Programming language",
      "Psychology",
      "Recall",
      "SIGNAL (programming language)",
      "Sensitivity (control systems)",
      "Set (abstract data type)",
      "Speech recognition",
      "Support vector machine",
      "Test set"
    ],
    "authors": [
      {
        "surname": "Khaskhoussy",
        "given_name": "Rania"
      },
      {
        "surname": "Ayed",
        "given_name": "Yassine Ben"
      }
    ]
  },
  {
    "title": "DIODE: Dilatable Incremental Object Detection",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109244",
    "abstract": "To accommodate rapid changes in the real world, the cognition system of humans is capable of continually learning concepts. On the contrary, conventional deep learning models lack this capability of preserving previously learned knowledge. When a neural network is fine-tuned to learn new tasks, its performance on previously trained tasks will significantly deteriorate. Many recent works on incremental object detection tackle this problem by introducing advanced regularization. Although these methods have shown promising results, the benefits are often short-lived after the first incremental step. Under multi-step incremental learning, the trade-off between old knowledge preserving and new task learning becomes progressively more severe. Thus, the performance of regularization-based incremental object detectors gradually decays for subsequent learning steps. In this paper, we aim to alleviate this performance decay on multi-step incremental detection tasks by proposing a dilatable incremental object detector (DIODE). For the task-shared parameters, our method adaptively penalizes the changes of important weights for previous tasks. At the same time, the structure of the model is dilated or expanded by a limited number of task-specific parameters to promote new task learning. Extensive experiments on PASCAL VOC and COCO datasets demonstrate substantial improvements over the state-of-the-art methods. Notably, compared with the state-of-the-art methods, our method achieves up to 6.0% performance improvement by increasing the number of parameters by just 1.2% for each newly learned task.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322007233",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Detector",
      "Economics",
      "Incremental learning",
      "Machine learning",
      "Management",
      "Multi-task learning",
      "Object detection",
      "Pascal (unit)",
      "Pattern recognition (psychology)",
      "Programming language",
      "Regularization (linguistics)",
      "Task (project management)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Peng",
        "given_name": "Can"
      },
      {
        "surname": "Zhao",
        "given_name": "Kun"
      },
      {
        "surname": "Maksoud",
        "given_name": "Sam"
      },
      {
        "surname": "Wang",
        "given_name": "Tianren"
      },
      {
        "surname": "Lovell",
        "given_name": "Brian C."
      }
    ]
  },
  {
    "title": "A binaural heterophasic adaptive beamformer and its deep learning assisted implementation",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.02.025",
    "abstract": "Beamforming is one of the most effective approaches to distant sound acquisition in complex acoustic environments, where noise, reverberation, and interference coexist; as a result, a significant number of efforts have been devoted to it over the last few decades. However, conventional beamformers produce a monaural output or colinear outputs, which are not optimal from the perception perspective. To take advantage of the human binaural hearing properties, a new type of fixed beamforming methods were developed recently, which attempt not only to attenuate noise but also render the signal of interest and residual noise into different perceptual regions, thereby achieving higher speech intelligibility. This work extends the principle of fixed binaural beamforming and develops a binaural heterophasic minimum variance distortionless response (MVDR) beamformer. A deep neural network (DNN) based noise estimation method is used to assist the implementation of this heterophasic MVDR beamformer, which is advantageous over the traditional one as it renders the desired source signal and residual noise to different perceptual regions, thereby yielding higher intelligibility. In comparison with the fixed binaural heterophasic beamformers, it can take advantage of the statistics of the noise to achieve better array performance. Results of simulations and listening tests validate the properties of the proposed technique.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523000557",
    "keywords": [
      "Acoustics",
      "Adaptive beamformer",
      "Artificial intelligence",
      "Beamforming",
      "Binaural recording",
      "Computer science",
      "Epistemology",
      "Image (mathematics)",
      "Intelligibility (philosophy)",
      "Monaural",
      "Noise (video)",
      "Philosophy",
      "Physics",
      "Reverberation",
      "Speech recognition",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Jin",
        "given_name": "Jilu"
      },
      {
        "surname": "Pan",
        "given_name": "Ningning"
      },
      {
        "surname": "Chen",
        "given_name": "Jingdong"
      },
      {
        "surname": "Benesty",
        "given_name": "Jacob"
      },
      {
        "surname": "Yang",
        "given_name": "Yiqian"
      }
    ]
  },
  {
    "title": "Progressive generation of 3D point clouds with hierarchical consistency",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109200",
    "abstract": "Generating 3D point cloud directly from latent prior (e.g., Gaussian distribution) plays a vital role in the representation learning and data augmentation in 3D vision tasks. Since point cloud is formed by irregular points, the generation process of point cloud requires rich semantic information, yet few studies are devoted to it. In this paper, we recast this generation task as a progressive learning problem to model the two-level hierarchy of distributions and address it by proposing a novel model called hierarchical consistency variational autoencoder (HC-VAE). This framework introduces a hierarchical consistent mechanism (HCM) to model the shape consistency and the pointwise representation consistency in a complementary manner. Specifically, we propose a stackable encoder-decoder framework and constrain the generation quality progressively to ensure that the underlying shape and fine-grained parts can be reconstructed with high fidelity. Additionally, given the progressively generated intermediate point cloud instances, a hierarchical-positive contrastive loss is introduced to learn the point-distribution-free instance representations to avoid explicitly parametrizing the distribution of points in a shape. In this way, our model suffices to generate diverse, high-resolution, and uniform point cloud instances. Extensive experimental results demonstrate that the proposed method achieves state-of-the-art performance in point cloud generation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322006793",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Autoencoder",
      "Computer science",
      "Consistency (knowledge bases)",
      "Deep learning",
      "Encoder",
      "Geometry",
      "Law",
      "Mathematical analysis",
      "Mathematics",
      "Operating system",
      "Point (geometry)",
      "Point cloud",
      "Point process",
      "Pointwise",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Statistics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Peipei"
      },
      {
        "surname": "Liu",
        "given_name": "Xiyan"
      },
      {
        "surname": "Huang",
        "given_name": "Jizhou"
      },
      {
        "surname": "Xia",
        "given_name": "Deguo"
      },
      {
        "surname": "Yang",
        "given_name": "Jianzhong"
      },
      {
        "surname": "Lu",
        "given_name": "Zhen"
      }
    ]
  },
  {
    "title": "Compact network embedding for fast node classification",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109236",
    "abstract": "Network embedding has shown promising performance in real-world applications. The network embedding typically lies in a continuous vector space, where storage and computation costs are high, especially in large-scale applications. This paper proposes more compact representation to fulfill the gap. The proposed discrete network embedding (DNE) leverages hash code to represent node in Hamming space. The Hamming similarity between hash codes approximates the ground-truth similarity. The embedding and classifier are jointly learned to improve compactness and discrimination. The proposed multi-class classifier is further constrained to be discrete to expedite classification. In addition, this paper further extends DNE and proposes deep discrete attributed network embedding (DDANE) to learn compact deep embedding from more informative attributed network. From the perspective of generalized signal smoothing, the proposed DDANE trains an improved graph convolutional network autoencoder to effectively leverage node attribute and network structure. Extensive experiments on node classification demonstrate the proposed methods exhibit lower storage and computational complexity than state-of-the-art network embedding methods, and achieve satisfactory accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322007154",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Autoencoder",
      "Block code",
      "Classifier (UML)",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Convolutional neural network",
      "Decoding methods",
      "Deep learning",
      "Embedding",
      "Hamming code",
      "Hamming space",
      "Hash function",
      "Pattern recognition (psychology)",
      "Smoothing",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Shen",
        "given_name": "Xiaobo"
      },
      {
        "surname": "Ong",
        "given_name": "Yew-Soon"
      },
      {
        "surname": "Mao",
        "given_name": "Zheng"
      },
      {
        "surname": "Pan",
        "given_name": "Shirui"
      },
      {
        "surname": "Liu",
        "given_name": "Weiwei"
      },
      {
        "surname": "Zheng",
        "given_name": "Yuhui"
      }
    ]
  },
  {
    "title": "Cross-modal hierarchical interaction network for RGB-D salient object detection",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109194",
    "abstract": "How to effectively exchange and aggregate the information of multiple modalities (e.g. RGB image and depth map) is a big challenge in the RGB-D salient object detection community. To address this problem, in this paper, we propose a cross-modal Hierarchical Interaction Network (HINet), which boosts the salient object detection by excavating the cross-modal feature interaction and progressively multi-level feature fusion. To achieve it, we design two modules: cross-modal information exchange (CIE) module and multi-level information progressively guided fusion (PGF) module. Specifically, the CIE module is proposed to exchange the cross-modal features for learning the shared representations, as well as the beneficial feedback to facilitate the discriminative feature learning of different modalities. Besides, the PGF module is designed to aggregate the hierarchical features progressively with the reverse guidance mechanism, which employs the high-level feature fusion to guide the low-level feature fusion and thus improve the saliency detection performance. Extensive experiments show that our proposed model significantly outperforms the existing nine state-of-the-art models on five challenging benchmark datasets. Codes and results are available at: https://github.com/RanwanWu/HINet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322006732",
    "keywords": [
      "Aggregate (composite)",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Chemistry",
      "Composite material",
      "Computer science",
      "Computer vision",
      "Discriminative model",
      "Feature (linguistics)",
      "Feature learning",
      "Geodesy",
      "Geography",
      "Linguistics",
      "Materials science",
      "Modal",
      "Modalities",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Polymer chemistry",
      "RGB color model",
      "Salient",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Bi",
        "given_name": "Hongbo"
      },
      {
        "surname": "Wu",
        "given_name": "Ranwan"
      },
      {
        "surname": "Liu",
        "given_name": "Ziqi"
      },
      {
        "surname": "Zhu",
        "given_name": "Huihui"
      },
      {
        "surname": "Zhang",
        "given_name": "Cong"
      },
      {
        "surname": "Xiang",
        "given_name": "Tian-Zhu"
      }
    ]
  },
  {
    "title": "This looks More Like that: Enhancing Self-Explaining Models by Prototypical Relevance Propagation",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109172",
    "abstract": "Current machine learning models have shown high efficiency in solving a wide variety of real-world problems. However, their black box character poses a major challenge for the comprehensibility and traceability of the underlying decision-making strategies. As a remedy, numerous post-hoc and self-explanation methods have been developed to interpret the models’ behavior. Those methods, in addition, enable the identification of artifacts that, inherent in the training data, can be erroneously learned by the model as class-relevant features. In this work, we provide a detailed case study of a representative for the state-of-the-art self-explaining network, ProtoPNet, in the presence of a spectrum of artifacts. Accordingly, we identify the main drawbacks of ProtoPNet, especially its coarse and spatially imprecise explanations. We address these limitations by introducing Prototypical Relevance Propagation (PRP), a novel method for generating more precise model-aware explanations. Furthermore, in order to obtain a clean, artifact-free dataset, we propose to use multi-view clustering strategies for segregating the artifact images using the PRP explanations, thereby suppressing the potential artifact learning in the models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322006513",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Epistemology",
      "Law",
      "Philosophy",
      "Political science",
      "Psychology",
      "Relevance (law)"
    ],
    "authors": [
      {
        "surname": "Gautam",
        "given_name": "Srishti"
      },
      {
        "surname": "Höhne",
        "given_name": "Marina M.-C."
      },
      {
        "surname": "Hansen",
        "given_name": "Stine"
      },
      {
        "surname": "Jenssen",
        "given_name": "Robert"
      },
      {
        "surname": "Kampffmeyer",
        "given_name": "Michael"
      }
    ]
  },
  {
    "title": "Neurodynamics-driven supervised feature selection",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109254",
    "abstract": "Feature selection is an important dimensionality reduction technique in machine learning, pattern recognition, image processing, and data mining. Most existing feature selection methods are greedy in nature thus are prone to sub-optimality. Though some feature selection methods based on global optimization of unsupervised redundancy may potentiate performance improvements, they may or may not be relevant to classification as the information on pairwise features with class labels is missing. In this paper, based on a supervised similarity measure, a biconvex optimization problem is formulated for holistic feature section with a quadratically weighted objective function subject to linear equality and nonnegativity constraints. In addition, an iteratively reweighted convex quadratic program is reformulated. A two-timescale duplex neurodynamic system is applied to solve the formulated biconvex optimization problem and a projection neural network is customized to solve the iteratively reweighted convex optimization problem. Experimental results of the proposed neurodynamics-based supervised feature selection are elaborated in comparison with several existing feature selection methods based on twenty benchmark datasets to substantiate the efficacy and superiority of the neurodynamics-based method for selecting informative features in classification.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322007336",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Dimensionality reduction",
      "Feature (linguistics)",
      "Feature selection",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Optimization problem",
      "Pairwise comparison",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Yadi"
      },
      {
        "surname": "Wang",
        "given_name": "Jun"
      },
      {
        "surname": "Tao",
        "given_name": "Dacheng"
      }
    ]
  },
  {
    "title": "Neural operator search",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109215",
    "abstract": "Existing neural architecture search (NAS) methods usually explore a limited feature-transformation-only search space, ignoring other advanced feature operations such as feature self-calibration by attention and dynamic convolutions. This disables the NAS algorithms to discover more advanced network architectures. We address this limitation by additionally exploiting feature self-calibration operations, resulting in a heterogeneous search space. To solve the challenges of operation heterogeneity and significantly larger search space, we formulate a neural operator search (NOS) method. NOS presents a novel heterogeneous residual block for integrating the heterogeneous operations in a unified structure, and an attention guided search strategy for facilitating the search process over a vast space. Extensive experiments show that NOS can search novel cell architectures with highly competitive performance on the CIFAR and ImageNet benchmarks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032200694X",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Biology",
      "Computer science",
      "Gene",
      "Operator (biology)",
      "Repressor",
      "Transcription factor"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Wei"
      },
      {
        "surname": "Gong",
        "given_name": "Shaogang"
      },
      {
        "surname": "Zhu",
        "given_name": "Xiatian"
      }
    ]
  },
  {
    "title": "Semi-supervised cross-modal hashing via modality-specific and cross-modal graph convolutional networks",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109211",
    "abstract": "Cross-modal hashing maps heterogeneous multimedia data into Hamming space for retrieving relevant samples across modalities, which has received great research interests due to its rapid retrieval and low storage cost. In real-world applications, due to high manual annotation cost of multi-media data, we can only make use of limited number of labeled data with rich unlabeled data. In recent years, several semi-supervised cross-modal hashing (SCH) methods have been presented. However, how to fully explore and jointly utilize the modality-specific (complementarity) and modality-shared (correlation) information for retrieval has not been well studied for existing SCH works. In this paper, we propose a novel SCH approach named Modality-specific and Cross-modal Graph Convolutional Networks (MCGCN). The network architecture contains two modality-specific channels and a cross-modal channel to learn modality-specific and shared representations for each modality, respectively. Graph convolutional network (GCN) is leveraged in these three channels to explore intra-modal and inter-modal similarity, and perform semantic information propagation from labeled data to unlabeled data. Modality-specific and shared representations for each modality are fused with attention scheme. To further reduce the modality gap, a discriminative model is designed, learning to classify the modality of representations, and network training is guided by adversarial scheme. Experiments on two widely used multi-modal datasets demonstrate MCGCN outperforms state-of-the-art semi-supervised/supervised cross-modal hashing methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322006902",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Computer security",
      "Graph",
      "Hash function",
      "Machine learning",
      "Modal",
      "Modality (human–computer interaction)",
      "Pattern recognition (psychology)",
      "Polymer chemistry",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Fei"
      },
      {
        "surname": "Li",
        "given_name": "Shuaishuai"
      },
      {
        "surname": "Gao",
        "given_name": "Guangwei"
      },
      {
        "surname": "Ji",
        "given_name": "Yimu"
      },
      {
        "surname": "Jing",
        "given_name": "Xiao-Yuan"
      },
      {
        "surname": "Wan",
        "given_name": "Zhiguo"
      }
    ]
  },
  {
    "title": "Interpreting denoising autoencoders with complex perturbation approach",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109212",
    "abstract": "The goal of this study is to interpret denoising autoencoders by quantifying the importance of input pixel features for image reconstruction. The importance of pixel features is evaluated using the attributions of the pixel features to the latent variables of a denoising autoencoder used for image reconstruction. Pixel attributions are computed using a highly accurate and automatable perturbation approach and are plotted as saliency maps. Saliency maps highlight the contribution of the pixels for image reconstruction. The proposed approach produces more meaningful and understandable explanations than guided backpropagation and layer wise propagation methods. Three sanity checks are introduced to verify the fidelity of the generated saliency maps and also to elucidate the influence of inputs on the latent variables. The classification accuracy of images is significantly lowered when the most important pixel regions highlighted by the saliency maps are corrupted validating the proposed approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322006914",
    "keywords": [
      "Artificial intelligence",
      "Autoencoder",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Fidelity",
      "Image denoising",
      "Noise reduction",
      "Non-local means",
      "Pattern recognition (psychology)",
      "Pixel",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Arumugam",
        "given_name": "Dharanidharan"
      },
      {
        "surname": "Kiran",
        "given_name": "Ravi"
      }
    ]
  },
  {
    "title": "Pitfalls of assessing extracted hierarchies for multi-class classification",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109225",
    "abstract": "Using hierarchies of classes is one of the standard methods to solve multi-class classification problems. In the literature, selecting the right hierarchy is considered to play a key role in improving classification performance. Although different methods have been proposed, there is still a lack of understanding of what makes a hierarchy good and what makes a method to extract hierarchies perform better or worse. To this effect, we analyze and compare some of the most popular approaches to extracting hierarchies. We identify some common pitfalls that may lead practitioners to make misleading conclusions about their methods. To address some of these problems, we demonstrate that using random hierarchies is an appropriate benchmark to assess how the hierarchy’s quality affects the classification performance. In particular, we show how the hierarchy’s quality can become irrelevant depending on the experimental setup: when using powerful enough classifiers, the final performance is not affected by the quality of the hierarchy. We also show how comparing the effect of the hierarchies against non-hierarchical approaches might incorrectly indicate their superiority. Our results confirm that datasets with a high number of classes generally present complex structures in how these classes relate to each other. In these datasets, the right hierarchy can dramatically improve classification performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032200704X",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Class (philosophy)",
      "Class hierarchy",
      "Computer science",
      "Computer security",
      "Data mining",
      "Economics",
      "Epistemology",
      "Geodesy",
      "Geography",
      "Hierarchy",
      "Key (lock)",
      "Machine learning",
      "Market economy",
      "Object-oriented programming",
      "Philosophy",
      "Programming language",
      "Quality (philosophy)"
    ],
    "authors": [
      {
        "surname": "del Moral",
        "given_name": "Pablo"
      },
      {
        "surname": "Nowaczyk",
        "given_name": "Sławomir"
      },
      {
        "surname": "Sant’Anna",
        "given_name": "Anita"
      },
      {
        "surname": "Pashami",
        "given_name": "Sepideh"
      }
    ]
  },
  {
    "title": "Efficient graph edit distance computation using isomorphic vertices",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.03.002",
    "abstract": "In this paper, we study the problem of graph edit distance (GED) computation. We empirically observe that real graph data contain many isomorphic substructures, which incur redundant computation. Based on this observation, we aim at reducing the cost of GED computation by avoiding redundant computation caused by isomorphic substructures. To detect isomorphic substructures, we precisely define a notion of vertex isomorphism and propose a dynamic programming algorithm that identifies isomorphic vertices in a graph. By taking advantage of isomorphic vertices, we develop an efficient GED computation algorithm. In experiments, we show that isomorphic vertices are effectively used in reducing the search space of GED computation, and as a result our approach improves the performance of GED computation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523000685",
    "keywords": [
      "Algorithm",
      "Chemistry",
      "Combinatorics",
      "Computation",
      "Computer science",
      "Crystal structure",
      "Crystallography",
      "Graph",
      "Graph isomorphism",
      "Isomorphism (crystallography)",
      "Line graph",
      "Mathematics",
      "Theoretical computer science",
      "Theory of computation",
      "Vertex (graph theory)"
    ],
    "authors": [
      {
        "surname": "Kim",
        "given_name": "Jongik"
      }
    ]
  },
  {
    "title": "Learning generalized visual odometry using position-aware optical flow and geometric bundle adjustment",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109262",
    "abstract": "Recent visual odometry (VO) methods incorporating geometric algorithm into deep-learning architecture have shown outstanding performance on the challenging monocular VO task. Despite encouraging results are shown, previous methods ignore the requirement of generalization capability under noisy environment and various scenes. To address this challenging issue, this work first proposes a novel optical flow network (PANet). Compared with previous methods that predict optical flow as a direct regression task, our PANet computes optical flow by predicting it into the discrete position space with optical flow probability volume, and then converting it to optical flow. Next, we improve the bundle adjustment module to fit the self-supervised training pipeline by introducing multiple sampling, ego-motion initialization, dynamic damping factor adjustment, and Jacobi matrix weighting. In addition, a novel normalized photometric loss function is advanced to improve the depth estimation accuracy. The experiments show that the proposed system not only achieves comparable performance with other state-of-the-art self-supervised learning-based methods on the KITTI dataset, but also significantly improves the generalization capability compared with geometry-based, learning-based and hybrid VO systems on the noisy KITTI and the challenging outdoor (KAIST) scenes.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322007415",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Generalization",
      "Image (mathematics)",
      "Initialization",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Medicine",
      "Optical flow",
      "Pattern recognition (psychology)",
      "Programming language",
      "Radiology",
      "Robot",
      "Visual odometry",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Cao",
        "given_name": "Yi-Jun"
      },
      {
        "surname": "Zhang",
        "given_name": "Xian-Shi"
      },
      {
        "surname": "Luo",
        "given_name": "Fu-Ya"
      },
      {
        "surname": "Peng",
        "given_name": "Peng"
      },
      {
        "surname": "Lin",
        "given_name": "Chuan"
      },
      {
        "surname": "Yang",
        "given_name": "Kai-Fu"
      },
      {
        "surname": "Li",
        "given_name": "Yong-Jie"
      }
    ]
  },
  {
    "title": "Re-ranking and TOPSIS-based ensemble feature selection with multi-stage aggregation for text categorization",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.02.027",
    "abstract": "Aiming at reducing data dimensionality, feature selection (FS) could improve the accuracy and reduce computational cost of machine learning model, especially those with high-dimensional text datasets. To improve the robustness, ensemble feature selection (EFS) has been developed with considerable attention recently where different aggregation methods are applied. This paper proposed a four-stage EFS method called re-ranking and TOPSIS-based ensemble feature selection (RTEFS). In the first stage of RTEFS, features are extracted from the text corpus. The second one is to construct a union subset yielded by six filter-based FS methods out of preprocessing feature vectors. Then a re-ranking stage is applied to evaluate those features from such subset. The TOPSIS method is used to aggregate the ranking lists ranked by two FS groups in the ensemble feature ranking stage. In the final stage, the two fused rankings are ensembled via a multi-objective genetic algorithm NSGA-III. To demonstrate the superiority of the proposed method, experiments are performed using 20-Newsgroups and Reuters-21,578 datasets with Support Vector Machine and K-Nearest Neighbors classifiers. Results show that RTEFS produces higher accuracy and F-measure scores over the base counterparts.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523000570",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Data mining",
      "Dimensionality reduction",
      "Ensemble learning",
      "Feature selection",
      "Gene",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Preprocessor",
      "Ranking (information retrieval)",
      "Robustness (evolution)",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Fu",
        "given_name": "Guanghua"
      },
      {
        "surname": "Li",
        "given_name": "Bencheng"
      },
      {
        "surname": "Yang",
        "given_name": "Yongsheng"
      },
      {
        "surname": "Li",
        "given_name": "Chaofeng"
      }
    ]
  },
  {
    "title": "Fast geometrical extraction of nearest neighbors from multi-dimensional data",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109183",
    "abstract": "K-Nearest Neighbor (KNN) algorithm plays a significant role in various fields of data science and machine learning. Most variants of the KNN algorithm involve distance computations and a parameter (K) that represents the required number of neighbors. The recent research regarding distance computations and finding the optimal value of K have made neighborhood extraction a slow process. This research presents a fast geometrical approach for neighborhood extraction from multi-dimensional data. Instead of distance computations, the proposed algorithm creates a geometrical shape based on the number of features of data. This geometrical shape encompasses the reference data point and the neighboring points. The proposed algorithm's efficiency of time, classification, and hashing are evaluated and compared with existing state-of-the-art algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322006628",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computation",
      "Computer science",
      "Computer security",
      "Data mining",
      "Data point",
      "Geometry",
      "Hash function",
      "Mathematics",
      "Nearest neighbor search",
      "Operating system",
      "Pattern recognition (psychology)",
      "Point (geometry)",
      "Point cloud",
      "Process (computing)",
      "k-nearest neighbors algorithm"
    ],
    "authors": [
      {
        "surname": "Aziz",
        "given_name": "Yasir"
      },
      {
        "surname": "Memon",
        "given_name": "Kashif Hussain"
      }
    ]
  },
  {
    "title": "Lower bound estimation of recommendation error through user uncertainty modeling",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109171",
    "abstract": "In machine learning, the Bayesian error is the lower bound of the prediction error induced by data distribution. In recommender systems, this is also known as the magic barrier (MGBR). MGBR estimation is an important issue because the recommended data frequently contain considerable uncertainties that are difficult to quantify. It is possible to determine the extent to which the recommendation algorithm can be optimized by obtaining the MGBR for a given dataset. MGBR estimation generally requires real user ratings that are not affected by external factors such as human emotions and living environment, which can be extremely difficult or even impossible to gather. Existing theoretical approaches based on simple models, such as Gaussian distributions, have limited estimation capabilities. In this paper, we propose a more sophisticated mixture of exponential power (MoEP) model, which enables adaptive parameter selection for intricate uncertainty. To fit the distribution of the real data, we constructed a flexible learning model that automatically adjusts super- or sub-Gaussian uncertainties using the MoEP components. To select parameters adaptively, we employed an expectation-maximization algorithm to infer the parameters of the components. To estimate the MGBR, we explored an approach for calculating the lower bound of the prediction error under the guidance of a probability model. Experiments on the four datasets validated the rationality of the proposed method. The results show that the MGBR estimated using the new model is marginally lower than the prediction error of state-of-the-art algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322006501",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Estimation theory",
      "Expectation–maximization algorithm",
      "Gaussian",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Maximum likelihood",
      "Physics",
      "Quantum mechanics",
      "Recommender system",
      "Statistics",
      "Upper and lower bounds"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Heng-Ru"
      },
      {
        "surname": "Qiu",
        "given_name": "Ying"
      },
      {
        "surname": "Zhu",
        "given_name": "Ke-Lin"
      },
      {
        "surname": "Min",
        "given_name": "Fan"
      }
    ]
  },
  {
    "title": "FP-DARTS: Fast parallel differentiable neural architecture search for image classification",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109193",
    "abstract": "Neural Architecture Search (NAS) has made remarkable progress in automatic machine learning. However, it still suffers massive computing overheads limiting its wide applications. In this paper, we present an efficient search method, Fast Parallel Differential Neural Architecture Search (FP-DARTS). The proposed method is carefully designed from three levels to construct and train the super-network. Firstly, at the operation-level, to reduce the computational burden, different from the standard DARTS search space (8 operations), we decompose the operation set into two non-overlapping operator sub-sets (4 operations for each). Adopting these two reduced search spaces, two over-parameterized sub-networks are constructed. Secondly, at the channel-level, the partially-connected strategy is adopted, where each sub-network only adopts partial channels. Then these two sub-networks construct a two-parallel-path super-network by addition. Thirdly, at the training-level, the binary gate is introduced to control whether a path participates in the super-network training. It may suffer an unfair issue when using softmax to select the best input for intermediate nodes across two operator sub-sets. To tackle this problem, the sigmoid function is introduced, which measures the performance of operations without compression. Extensive experiments demonstrate the effectiveness of the proposed algorithm. Specifically, FP-DARTS achieves 2.50% test error with only 0.08 GPU-days on CIFAR10, and a state-of-the-art top-1 error rate of 23.7% on ImageNet using only 2.44 GPU-days for search.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322006720",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer security",
      "Construct (python library)",
      "Network architecture",
      "Parameterized complexity",
      "Path (computing)",
      "Pipeline (software)",
      "Programming language",
      "Set (abstract data type)",
      "Sigmoid function",
      "Softmax function"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Wenna"
      },
      {
        "surname": "Zhang",
        "given_name": "Xiuwei"
      },
      {
        "surname": "Cui",
        "given_name": "Hengfei"
      },
      {
        "surname": "Yin",
        "given_name": "Hanlin"
      },
      {
        "surname": "Zhang",
        "given_name": "Yannnig"
      }
    ]
  },
  {
    "title": "Counterfactual attention alignment for visible-infrared cross-modality person re-identification",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.03.008",
    "abstract": "Visible-infrared person re-identification (VI-ReID) copes with cross-modality matching between the daytime visible and night-time infrared images. Existing methods try to use attention modules to enhance multi-modality feature representations, but ignore measures of attention quality and lack direct and effective supervision of the attention learning process. To solve these problems, we propose a counterfactual attention alignment (CAA) strategy by mining intra-modality attention information with counterfactual causality and aligning the cross-modality attentions. Specifically, a self-weighted part attention module is designed to extract the pairwise attention information in local parts. The counterfactual attention alignment strategy obtains the learning results of the attention module through counterfactual intervention, and aligns the attention maps of the two modalities to find better shared cross-modality attention regions. Then the effect of the aligned attention on network prediction is used as a supervision signal to directly guide the attention module to learn more effective attention information. Extensive experimental results demonstrate that the proposed approach outperforms other state-of-the-art methods on two standard benchmarks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523000739",
    "keywords": [
      "Artificial intelligence",
      "Attention network",
      "Biology",
      "Botany",
      "Computer science",
      "Counterfactual thinking",
      "Feature (linguistics)",
      "Identification (biology)",
      "Linguistics",
      "Machine learning",
      "Matching (statistics)",
      "Mathematics",
      "Modalities",
      "Modality (human–computer interaction)",
      "Operating system",
      "Pairwise comparison",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Process (computing)",
      "Programming language",
      "Psychology",
      "SIGNAL (programming language)",
      "Social psychology",
      "Social science",
      "Sociology",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Sun",
        "given_name": "Zongzhe"
      },
      {
        "surname": "Zhao",
        "given_name": "Feng"
      }
    ]
  },
  {
    "title": "Improving Tail-Class Representation with Centroid Contrastive Learning",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.03.010",
    "abstract": "In vision domain, large-scale natural datasets typically exhibit long-tailed distribution which has large class imbalance between head and tail classes. This distribution poses difficulty in learning good representations for tail classes. Recent developments have shown good long-tailed model can be learnt by decoupling the training into representation learning and classifier balancing. However, these works pay insufficient consideration on the long-tailed effect on representation learning. In this work, we propose interpolative centroid contrastive learning (ICCL) to improve long-tailed representation learning. ICCL interpolates two images from a class-agnostic sampler and a class-aware sampler, and trains the model such that the representation of the interpolative image can be used to retrieve the centroids for both source classes. We demonstrate the effectiveness of our approach on multiple long-tailed image classification benchmarks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523000752",
    "keywords": [
      "Artificial intelligence",
      "Centroid",
      "Class (philosophy)",
      "Classifier (UML)",
      "Computer science",
      "Feature learning",
      "Law",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Tiong",
        "given_name": "Anthony Meng Huat"
      },
      {
        "surname": "Li",
        "given_name": "Junnan"
      },
      {
        "surname": "Lin",
        "given_name": "Guosheng"
      },
      {
        "surname": "Li",
        "given_name": "Boyang"
      },
      {
        "surname": "Xiong",
        "given_name": "Caiming"
      },
      {
        "surname": "Hoi",
        "given_name": "Steven C.H."
      }
    ]
  },
  {
    "title": "HAMIL: Hierarchical aggregation-based multi-instance learning for microscopy image classification",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109245",
    "abstract": "Multi-instance learning is common for computer vision tasks, especially in biomedical image processing. Traditional methods for multi-instance learning focus on designing feature aggregation methods and multi-instance classifiers, where the aggregation operation is performed either in the feature extraction or learning phase. As deep neural networks (DNNs) achieve great success in image processing via automatic feature learning, certain feature aggregation mechanisms need to be incorporated into common DNN architecture for multi-instance learning. Moreover, flexibility and reliability are crucial considerations to deal with varying quality and number of instances. In this study, we propose a hierarchical aggregation network for multi-instance learning, called HAMIL. The hierarchical aggregation protocol enables feature fusion in a defined order, and the simple convolutional aggregation units lead to an efficient and flexible architecture. We assess the model performance on two microscopy image classification tasks, namely protein subcellular localization using immunofluorescence images and gene annotation using spatial gene expression images. The experimental results show that HAMIL outperforms the state-of-the-art feature aggregation methods and the existing models for addressing these two tasks. The visualization analyses also demonstrate the ability of HAMIL to focus on high-quality instances.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322007245",
    "keywords": [
      "Artificial intelligence",
      "Automatic image annotation",
      "Computer science",
      "Contextual image classification",
      "Convolutional neural network",
      "Deep learning",
      "Feature (linguistics)",
      "Feature extraction",
      "Feature learning",
      "Focus (optics)",
      "Image (mathematics)",
      "Image processing",
      "Linguistics",
      "Machine learning",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Yang"
      },
      {
        "surname": "Tu",
        "given_name": "Yanlun"
      },
      {
        "surname": "Lei",
        "given_name": "Houchao"
      },
      {
        "surname": "Long",
        "given_name": "Wei"
      }
    ]
  },
  {
    "title": "Real time iris segmentation quality evaluation using medoids",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109290",
    "abstract": "Integrating an efficient and robust iris segmentation-quality estimation module in iris biometric systems will undoubtedly enhance its performance and competitive advantage. It proffers a real time detection of segmentation errors to forestall their propagation to the subsequent modules. Hence, we propose a novel automatic iris segmentation-quality estimation model using medoids. The performance of the proposed model was empirically evaluated with reference to three published models, using three benchmarked iris datasets and our novel proprietary iris dataset – Biometrics Vision and Computing Iris dataset. The proposed medoids based model was experimentally demonstrated to be effective, robust and relatively efficient in estimating iris segmentation-quality. Specifically, the proposed model recorded the best classification accuracy rate of 95.27% on one of the datasets. Also, it consistently recorded the least classification error rate across several iris datasets with diverse segmentation-errors, which suggest that the medoids based model is relatively more robust than the examined counterparts.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322007695",
    "keywords": [
      "Artificial intelligence",
      "Biometrics",
      "Cluster analysis",
      "Computer science",
      "Computer vision",
      "Data mining",
      "IRIS (biosensor)",
      "Iris recognition",
      "Medoid",
      "Pattern recognition (psychology)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Ejiogu",
        "given_name": "Ugochi U.C."
      },
      {
        "surname": "Iloanusi",
        "given_name": "Ogechukwu N."
      }
    ]
  },
  {
    "title": "CrossRectify: Leveraging disagreement for semi-supervised object detection",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109280",
    "abstract": "Semi-supervised object detection has recently achieved substantial progress. As a mainstream solution, the self-labeling-based methods train the detector on both labeled data and unlabeled data with pseudo labels predicted by the detector itself, but their performances are always limited. Through experimental analysis, we reveal the underlying reason is that the detector is misguided by the incorrect pseudo labels predicted by itself (dubbed self-errors). These self-errors can hurt performance even worse than random-errors, and can be neither discerned nor rectified during the self-labeling process. In this paper, we propose an effective detection framework named CrossRectify, to obtain accurate pseudo labels by simultaneously training two detectors with different initial parameters. Specifically, the proposed approach leverages the disagreements between detectors to discern the self-errors and refines the pseudo label quality by the proposed cross-rectifying mechanism. Extensive experiments show that CrossRectify achieves outperforming performances over various detector structures on 2D and 3D detection benchmarks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322007592",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Detector",
      "Labeled data",
      "Machine learning",
      "Object (grammar)",
      "Object detection",
      "Operating system",
      "Pattern recognition (psychology)",
      "Physics",
      "Process (computing)",
      "Quality (philosophy)",
      "Quantum mechanics",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Ma",
        "given_name": "Chengcheng"
      },
      {
        "surname": "Pan",
        "given_name": "Xingjia"
      },
      {
        "surname": "Ye",
        "given_name": "Qixiang"
      },
      {
        "surname": "Tang",
        "given_name": "Fan"
      },
      {
        "surname": "Dong",
        "given_name": "Weiming"
      },
      {
        "surname": "Xu",
        "given_name": "Changsheng"
      }
    ]
  },
  {
    "title": "ProbSAP: A comprehensive and high-performance system for student academic performance prediction",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109309",
    "abstract": "The student academic performance prediction is becoming an indispensable service in the computer supported intelligent education system. But conventional machine learning-based methods can only exploit the sparse discriminative features of student behaviors in imbalanced academic datasets to predict student academic performance (SAP). Furthermore, there is a lack of imbalanced data processing mechanisms that can efficiently capture student characteristics and achievement. Therefore, we propose a comprehensive and high-performance prediction framework to probe SAP characteristics (ProbSAP) on massive educational data, which can resolve imbalanced data issue and improve academic prediction performance for making course final mark prediction. It consists of three main components: collaborative data processing module for enhancing the data quality, scalable metadata clustering module for alleviating the imbalance of academic features, and XGBoost-enhanced SAP prediction module for academic performance forecasting. The collaborative data processing module integrates multi-dimensional academic data, which sustains a good supply for clustering and modeling in the ProbSAP framework. The comparative evaluation results demonstrate that ProbSAP delivers superior accuracy and efficiency improvement for the course final mark prediction of college students over other state-of-the-art methods such as CNN, SVR, RFR, XGBoost, Catboost-SHAP, and AS-SAN. On average, ProbSAP reduces the mean absolute error (MAE) by 84.76%, 72.11%, and 66.49% compared with XGBoost, Catboost-SHAP, and AS-SAN, respectively. It also leads to a better out-sample fit that minimizes prediction errors between 1% and 9% with over 98% of actual samples.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323000109",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Computer security",
      "Data mining",
      "Database",
      "Discriminative model",
      "Engineering",
      "Exploit",
      "Machine learning",
      "Metadata",
      "Operating system",
      "Operations management",
      "Performance improvement",
      "Performance prediction",
      "Scalability",
      "Simulation",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Xinning"
      },
      {
        "surname": "Zhao",
        "given_name": "Yuben"
      },
      {
        "surname": "Li",
        "given_name": "Chong"
      },
      {
        "surname": "Ren",
        "given_name": "Peng"
      }
    ]
  },
  {
    "title": "Detail enhancement-based vehicle re-identification with orientation-guided re-ranking",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109304",
    "abstract": "Vehicle re-identification (re-ID) has attracted extensive attention in the field of computer vision owing to the development of smart cities. Two issues remain in the task of vehicle re-ID: minor inter-class differences and extreme orientation variations. To address them, we propose a detailed enhancement-based vehicle re-ID method with orientation-guided re-ranking. In the proposed method, a novel enhancement module using stripe-based partition, is presented to avoid the negative influence of minor inter-class difference. The stripe-based partition subdivides the feature map of middle layers in the network into two stripes, and then the module explores more detailed information for vehicle re-ID. Furthermore, a multilayer feature fusion module is proposed to enhance the feature representation of a vehicle. To address the problem of extreme orientation variation in vehicle re-ID, we propose an orientation-guided re-ranking strategy to re-rank the retrieval result based on the orientation information and query expansion algorithm. This strategy can optimize the ranking of vehicles whose orientation is not similar to the query images in the post-processing stage. Extensive experiments on three public datasets confirm the effectiveness of our method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323000055",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Feature (linguistics)",
      "Geometry",
      "Identification (biology)",
      "Law",
      "Linguistics",
      "Mathematics",
      "Orientation (vector space)",
      "Partition (number theory)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Ranking (information retrieval)",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Sun",
        "given_name": "Ziruo"
      },
      {
        "surname": "Nie",
        "given_name": "Xiushan"
      },
      {
        "surname": "Bi",
        "given_name": "Xiaopeng"
      },
      {
        "surname": "Wang",
        "given_name": "Shaohua"
      },
      {
        "surname": "Yin",
        "given_name": "Yilong"
      }
    ]
  },
  {
    "title": "Hybrid routing transformer for zero-shot learning",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109270",
    "abstract": "Zero-shot learning (ZSL) aims to learn models that can recognize unseen image semantics based on the training of data with seen semantics. Recent studies either leverage the global image features or mine discriminative local patch features to associate the extracted visual features to the semantic attributes. However, due to the lack of the necessary top-down guidance and semantic alignment for ensuring the model attend to the real attribute-correlation regions, these methods still encounter a significant semantic gap between the visual modality and the attribute modality, which makes their prediction on unseen semantics unreliable. To solve this problem, this paper establishes a novel transformer encoder-decoder model, called hybrid routing transformer (HRT). In HRT encoder, we embed an active attention, which is constructed by both the bottom-up and the top-down dynamic routing pathways to generate the attribute-aligned visual feature. While in HRT decoder, we use static routing to calculate the correlation among the attribute-aligned visual features, the corresponding attribute semantics, and the class attribute vectors to generate the final class label predictions. This design makes the presented transformer model a hybrid of 1) top-down and bottom-up attention pathways and 2) dynamic and static routing pathways. Comprehensive experiments on three widely-used benchmark datasets, namely CUB, SUN, and AWA2, are conducted. The obtained experimental results demonstrate the effectiveness of the proposed method. Our code is released in https://github.com/KORIYN/HRT.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032200749X",
    "keywords": [
      "Artificial intelligence",
      "Computer network",
      "Computer science",
      "Discriminative model",
      "Encoder",
      "Leverage (statistics)",
      "Machine learning",
      "Operating system",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Testbed",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Cheng",
        "given_name": "De"
      },
      {
        "surname": "Wang",
        "given_name": "Gerong"
      },
      {
        "surname": "Wang",
        "given_name": "Bo"
      },
      {
        "surname": "Zhang",
        "given_name": "Qiang"
      },
      {
        "surname": "Han",
        "given_name": "Jungong"
      },
      {
        "surname": "Zhang",
        "given_name": "Dingwen"
      }
    ]
  },
  {
    "title": "Semi-supervised node classification via fine-grained graph auxiliary augmentation learning",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109301",
    "abstract": "Node classification has become an important research topic in recent years. Since there are always a few training samples, researchers improve the performance by properly leveraging the predictions of unlabeled nodes during training. However, suffering from the model degradation resulted from the accumulative error of pseudo-labels, there is limited improvement. In this paper we present fine-grained Graph Auxiliary aUgmentation (GAU). It trains the primary task together with an automatically created auxiliary task which is a fine-grained node classification task. And an auxiliary augmentation strategy is designed to enlarge the labeled set for the auxiliary task by utilizing the pseudo-labels of the primary task. Comprehensive experiments show that GAU alleviates the sensitivity of the model to the pseudo-label quality, so more unlabeled nodes can participate in the training. From the perspective of co-training, the fine-grained auxiliary task which is trained by much more unlabeled nodes helps to learn better node representations from a different view, thereby boosting the final performance. Extensive experiments verify the superior performance of the GAU on different GNN architectures when compared with other state-of-the-art approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032300002X",
    "keywords": [
      "Artificial intelligence",
      "Boosting (machine learning)",
      "Computer science",
      "Engineering",
      "Graph",
      "Machine learning",
      "Node (physics)",
      "Pattern recognition (psychology)",
      "Programming language",
      "Set (abstract data type)",
      "Structural engineering",
      "Systems engineering",
      "Task (project management)",
      "Theoretical computer science",
      "Training set"
    ],
    "authors": [
      {
        "surname": "Lv",
        "given_name": "Jia"
      },
      {
        "surname": "Song",
        "given_name": "Kaikai"
      },
      {
        "surname": "Ye",
        "given_name": "Qiang"
      },
      {
        "surname": "Tian",
        "given_name": "Guangjian"
      }
    ]
  },
  {
    "title": "Sparse norm regularized attribute selection for graph neural networks",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109265",
    "abstract": "Graph Neural Networks (GNNs) have been widely used for graph learning tasks. The main aspect of GNN’s layer-wise message passing is conducting attribute/feature propagation on graph. Most existing GNNs generally conduct feature propagation across all feature dimensions. However, in many real applications, attributes usually contain irrelevant and redundant noise. In this case, attribute/feature selection is desired to extract meaningful features and eliminate noisy ones for GNN’s layer-wise propagation. Based on this observation, in this paper, we combine ℓ 2 , 1 / ℓ 1 -norm regularized attribute selection and GNNs together and propose a novel Attribute selection guided GNNs (AsGNNs) for graph data representation. AsGNNs aim to adaptively select some desired meaningful features/attributes that best serve GNNs. Moreover, an effective optimization framework has also been derived to train the proposed AsGNNs. The proposed AsGNNs provide a general framework which can incorporate any GNNs to conduct feature selection for layer-wise propagation. In this paper, we implement AsGNNs on both graph convolutional network (GCN) and graph attention network (GAT) and develop AsGCN and AsGAT for graph learning. Experimental results on several benchmark datasets demonstrate the effectiveness of the proposed AsGNNs (AsGCN, AsGAT) on semi-supervised learning tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322007440",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Data mining",
      "Feature learning",
      "Feature selection",
      "Graph",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Jiang",
        "given_name": "Bo"
      },
      {
        "surname": "Wang",
        "given_name": "Beibei"
      },
      {
        "surname": "Luo",
        "given_name": "Bin"
      }
    ]
  },
  {
    "title": "Deep continual hashing with gradient-aware memory for cross-modal retrieval",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109276",
    "abstract": "Cross-modal hashing (CMH) has become widely used for large-scale multimedia retrieval. However, most current CMH methods focus on the closed retrieval scenario, not the real-world environments, i.e., complex and changing semantics. When data containing new class objects emerge, the current CMH has to retrain the model on all history training data, not the new data, to accommodate new semantics, but the never-stop upload of data on the Internet makes this impractical. In this paper, we devise a deep hashing method called Continual Cross-Modal Hashing with Gradient Aware Memory (CCMH-GAM) for learning binary codes of multi-label cross-modal data with increasing categories. CCMH-GAM is a two-step hashing architecture, one hashing network learns to hash the increasing semantics of data, i.e., label, into the semantic codes, and other modality-specific hashing networks learn to map data into the corresponding semantic codes. Specifically, to keep the encoding ability for old semantics, a regularization based on accumulating low-storage label-code pairs is designed for the former network. For the modality-specific networks, we propose a memory construction method via approximating the full episodic gradients of all data by some exemplars and derive its fast implementation with the upper bound of approximation error. Based on this memory, we propose a gradient projection method to theoretically improve the probability of old data’s code being unchanged after updating the model. Extensive experiments on three datasets demonstrate that CCMH-GAM can continually learn hash functions and yield state-of-the-art retrieval performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322007555",
    "keywords": [
      "Arithmetic",
      "Artificial intelligence",
      "Binary code",
      "Binary number",
      "Computer science",
      "Computer security",
      "Double hashing",
      "Dynamic perfect hashing",
      "Hash function",
      "Hash table",
      "Mathematics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Song",
        "given_name": "Ge"
      },
      {
        "surname": "Tan",
        "given_name": "Xiaoyang"
      },
      {
        "surname": "Yang",
        "given_name": "Ming"
      }
    ]
  },
  {
    "title": "Pose-driven attention-guided image generation for person re-Identification",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109246",
    "abstract": "Person re-identification (re-ID) concerns the matching of subject images across different camera views in a multi camera surveillance system. One of the major challenges in person re-ID is pose variations across the camera network, which significantly affects the appearance of a person. Existing development data lack adequate pose variations to carry out effective training of person re-ID systems. To solve this issue, in this paper we propose an end-to-end pose-driven attention-guided generative adversarial network, to generate multiple poses of a person. We propose to attentively learn and transfer the subject pose through an attention mechanism. A semantic-consistency loss is proposed to preserve the semantic information of the person during pose transfer. To ensure fine image details are realistic after pose translation, an appearance discriminator is used while a pose discriminator is used to ensure the pose of the transferred images will exactly be the same as the target pose. We show that by incorporating the proposed approach in a person re-identification framework, realistic pose transferred images and state-of-the-art re-identification results can be achieved.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322007257",
    "keywords": [
      "3D pose estimation",
      "Articulated body pose estimation",
      "Artificial intelligence",
      "Biochemistry",
      "Biology",
      "Botany",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Consistency (knowledge bases)",
      "Detector",
      "Discriminator",
      "Gene",
      "Generative adversarial network",
      "Identification (biology)",
      "Image (mathematics)",
      "Image translation",
      "Matching (statistics)",
      "Mathematics",
      "Messenger RNA",
      "Pattern recognition (psychology)",
      "Pose",
      "Statistics",
      "Telecommunications",
      "Translation (biology)"
    ],
    "authors": [
      {
        "surname": "Khatun",
        "given_name": "Amena"
      },
      {
        "surname": "Denman",
        "given_name": "Simon"
      },
      {
        "surname": "Sridharan",
        "given_name": "Sridha"
      },
      {
        "surname": "Fookes",
        "given_name": "Clinton"
      }
    ]
  },
  {
    "title": "Entropy regularization for weakly supervised object localization",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.03.018",
    "abstract": "The goal of weakly-supervised object localization (WSOL) is to train a localization model without the location information of the object(s). Recently, most existing WSOL methods capture the object with an attention map extracted from a classification network. However, it has been observed that we need to sacrifice classification performances to achieve the best WSOL score. We conjecture that this is because the objective of classification training, minimizing entropy between one-hot ground truth and predicted class probability, is not entirely consistent with that of localization. In this paper, we investigate how the entropy of predicted class probability affects localization performances, where we conclude that there is a sweet spot for localization with respect to entropy. Hence, we propose a new training strategy that adopts entropy regularization for finding the optimal point effectively. Specifically, we add an additional term to the loss function, which minimizes the entropy between a uniform distribution and the predicted class probability vector. The proposed method is easy to implement since we do not need to modify the architecture or data pipeline. In addition, our method is efficient in that almost zero additional resources are required. Most importantly, our method improves WSOL scores significantly, which has been shown through extensive experiments.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523000831",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Binary entropy function",
      "Computer science",
      "Cross entropy",
      "Differential entropy",
      "Entropy (arrow of time)",
      "Mathematics",
      "Maximum entropy probability distribution",
      "Pattern recognition (psychology)",
      "Physics",
      "Principle of maximum entropy",
      "Probability distribution",
      "Quantum mechanics",
      "Regularization (linguistics)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Hwang",
        "given_name": "Dongjun"
      },
      {
        "surname": "Ha",
        "given_name": "Jung-Woo"
      },
      {
        "surname": "Shim",
        "given_name": "Hyunjung"
      },
      {
        "surname": "Choe",
        "given_name": "Junsuk"
      }
    ]
  },
  {
    "title": "Uncertainty-aware semi-supervised few shot segmentation",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109292",
    "abstract": "Few shot segmentation (FSS) aims to learn pixel-level classification of a target object in a query image using only a few annotated support samples. This is challenging as it requires modeling appearance variations of target objects and the diverse visual cues between query and support images with limited information. To address this problem, we propose a semi-supervised FSS strategy that leverages additional prototypes from unlabeled images with uncertainty guided pseudo label refinement. To obtain reliable prototypes from unlabeled images, we meta-train a neural network to jointly predict segmentation and estimate the uncertainty of predictions. We employ the uncertainty estimates to exclude predictions with high degrees of uncertainty for pseudo label construction to obtain additional prototypes from the refined pseudo labels. During inference, query segmentation is predicted using prototypes from both support and unlabeled images including low-level features of the query images. Our approach can easily supplement existing approaches without the requirement of additional training when employing unlabeled samples. Extensive experiments on PASCAL- 5 i and COCO- 20 i demonstrate that our model can effectively remove unreliable predictions to refine pseudo labels and significantly improve upon baseline performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322007713",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Image segmentation",
      "Inference",
      "Machine learning",
      "Pascal (unit)",
      "Pattern recognition (psychology)",
      "Programming language",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Kim",
        "given_name": "Soopil"
      },
      {
        "surname": "Chikontwe",
        "given_name": "Philip"
      },
      {
        "surname": "An",
        "given_name": "Sion"
      },
      {
        "surname": "Park",
        "given_name": "Sang Hyun"
      }
    ]
  },
  {
    "title": "Lightweight Semi-supervised Network for Single Image Rain Removal",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109277",
    "abstract": "Deep learning technologies have shown their advantages in Single Image Rain Removal (SIRR) tasks. However, the derained results of most methods are limited to some challenges. First, due to the lack of real-world rainy/clean image pairs, many methods seriously rely on the labeled synthetic training images and will not effectively remove complex rain streaks in real-world scenarios. Second, most existing SIRR models require high computing power, which considerably limits their real-world applications. To address these issues, we propose a Lightweight Semi-supervised Network (LSNet) for SIRR. Our LSNet utilizes a compact semi-supervised framework to improve generalization ability in real-world rainy images removal. Meanwhile, in our semi-supervised framework, we also design a cascaded sub-network, which progressively removes complex rain streaks via a multi-stage manner. Specially, the multi-stage manner is based on a series of cascaded blocks, where we conduct recursive learning strategy to reduce model parameters. Extensive experimental results demonstrate that our method achieves comparable performance to the state-of-the-arts while has fewer parameters.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322007567",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Deep learning",
      "Generalization",
      "Image (mathematics)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Supervised learning"
    ],
    "authors": [
      {
        "surname": "Jiang",
        "given_name": "Nanfeng"
      },
      {
        "surname": "Luo",
        "given_name": "Jiawei"
      },
      {
        "surname": "Lin",
        "given_name": "Junhong"
      },
      {
        "surname": "Chen",
        "given_name": "Weiling"
      },
      {
        "surname": "Zhao",
        "given_name": "Tiesong"
      }
    ]
  },
  {
    "title": "MSdocTr-Lite: A lite transformer for full page multi-script handwriting recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.03.020",
    "abstract": "The Transformer has quickly become the dominant architecture for various pattern recognition tasks due to its capacity for long-range representation. However, transformers are data-hungry models and need large datasets for training. In Handwritten Text Recognition (HTR), collecting a massive amount of labeled data is a complicated and expensive task. In this paper, we propose a lite transformer architecture for full-page multi-script handwriting recognition. The proposed model comes with three advantages: First, to solve the common problem of data scarcity, we propose a lite transformer model that can be trained on a reasonable amount of data, which is the case of most HTR public datasets, without the need for external data. Second, it can learn the reading order at page-level thanks to a curriculum learning strategy, allowing it to avoid line segmentation errors, exploit a larger context and reduce the need for costly segmentation annotations. Third, it can be easily adapted to other scripts by applying a simple transfer-learning process using only page-level labeled images. Extensive experiments on different datasets with different scripts (French, English, Spanish, and Arabic) show the effectiveness of the proposed model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523000855",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Exploit",
      "Feature extraction",
      "Handwriting",
      "Handwriting recognition",
      "Labeled data",
      "Machine learning",
      "Natural language processing",
      "Operating system",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Scripting language",
      "Segmentation",
      "Speech recognition",
      "Transfer of learning",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Dhiaf",
        "given_name": "Marwa"
      },
      {
        "surname": "Rouhou",
        "given_name": "Ahmed Cheikh"
      },
      {
        "surname": "Kessentini",
        "given_name": "Yousri"
      },
      {
        "surname": "Salem",
        "given_name": "Sinda Ben"
      }
    ]
  },
  {
    "title": "Learning depth via leveraging semantics: Self-supervised monocular depth estimation with both implicit and explicit semantic guidance",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109297",
    "abstract": "Self-supervised monocular depth estimation has shown great success in learning depth using only images for supervision. In this paper, we propose to enhance self-supervised depth estimation with semantics and propose a novel learning scheme, which incorporates both implicit and explicit semantic guidances. Specifically, we propose to relate depth distributions to the semantic category information by proposing a Semantic-aware Spatial Feature Modulation (SSFM) scheme, which implicitly modulates the semantic and depth features in a joint learning framework. The modulation parameters are generated from semantic labels to acquire category-level guidance. Meanwhile, a semantic-guided ranking loss is proposed to explicitly constrain the estimated depth borders using the corresponding segmentation labels. To avoid the impact brought by erroneous segmentation labels, both robust sampling strategy and prediction uncertainty weighting are proposed for the ranking loss. Extensive experimental results show that our method produces high-quality depth maps with semantically consistent depth distributions and accurate depth edges, outperforming the state-of-the-art methods by significant margins.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322007762",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Depth map",
      "Feature (linguistics)",
      "Image (mathematics)",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Medicine",
      "Monocular",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Radiology",
      "Ranking (information retrieval)",
      "Scheme (mathematics)",
      "Segmentation",
      "Semantic feature",
      "Semantics (computer science)",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Rui"
      },
      {
        "surname": "Xue",
        "given_name": "Danna"
      },
      {
        "surname": "Su",
        "given_name": "Shaolin"
      },
      {
        "surname": "He",
        "given_name": "Xiantuo"
      },
      {
        "surname": "Mao",
        "given_name": "Qing"
      },
      {
        "surname": "Zhu",
        "given_name": "Yu"
      },
      {
        "surname": "Sun",
        "given_name": "Jinqiu"
      },
      {
        "surname": "Zhang",
        "given_name": "Yanning"
      }
    ]
  },
  {
    "title": "Characters as graphs: Interpretable handwritten Chinese character recognition via Pyramid Graph Transformer",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109317",
    "abstract": "It is meaningful but challenging to teach machines to recognize handwritten Chinese characters. However, conventional approaches typically view handwritten Chinese characters as either static images or temporal trajectories, which may ignore the inherent geometric semantics of characters. Instead, here we first propose to represent handwritten characters as skeleton graphs, explicitly considering the natural characteristics of characters (i.e., characters as graphs). Furthermore, we propose a novel Pyramid Graph Transformer (PyGT) to specifically process the graph-structured characters, which fully integrates the advantages of Transformers and graph convolutional networks. Specifically, our PyGT can learn better graph features through (i) capturing the global information from all nodes with graph attention mechanism and (ii) modelling the explicit local adjacency structures of nodes with graph convolutions. Furthermore, the PyGT learns the multi-resolution features by constructing a progressive shrinking pyramid. Compared with existing approaches, it is more interpretable to recognize characters as geometric graphs. Moreover, the proposed method is generic for both online and offline handwritten Chinese character recognition (HCCR), and it also can be feasibly extended to handwritten text recognition. Extensive experiments empirically demonstrate the superiority of PyGT over the prevalent approaches including 2D-CNN, RNN/1D-CNN, and Vision Transformer (ViT) for HCCR. The code is available at https://github.com/ganji15/PyGT-HCCR.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323000183",
    "keywords": [
      "Adjacency list",
      "Algorithm",
      "Artificial intelligence",
      "Chinese characters",
      "Computer science",
      "Graph",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Theoretical computer science",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Gan",
        "given_name": "Ji"
      },
      {
        "surname": "Chen",
        "given_name": "Yuyan"
      },
      {
        "surname": "Hu",
        "given_name": "Bo"
      },
      {
        "surname": "Leng",
        "given_name": "Jiaxu"
      },
      {
        "surname": "Wang",
        "given_name": "Weiqiang"
      },
      {
        "surname": "Gao",
        "given_name": "Xinbo"
      }
    ]
  },
  {
    "title": "Object detection based on cortex hierarchical activation in border sensitive mechanism and classification-GIou joint representation",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109278",
    "abstract": "By imitating the brain neurons for object perception, the deep networks enable a comprehensive feature characterization in the task of object detection. Considering such a perceptual ability is usually bounded in a box area for feature extraction, the balance of dimension reduction and feature information retaining has been taken into account in more recent studies, especially for the information preservation in the border areas. Motivated by the mechanism of neuron cortex activation, in this work, a novel function based on cortex hierarchical activation is proposed to achieve more effective border sensitive mechanism by joint pooling in backbone networks. In order to avoid the parameter solidification, this strategy is also capable to benefit the feature extraction on the border without unnecessary model re-training. Furthermore, by replacing the square kernel with a designed band shape kernel, more adequate feature description can be obtained on the border via the combination of the strip hierarchical pooling and strip max pooling. With an extension of the proposed activation function on classification-GIoU joint representation, the overall detection accuracy has been further improved. Experimental evaluations on the COCO benchmark datasets have shown that the proposed work has a superior performance in comparison to other state-of-the-art detection approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322007579",
    "keywords": [
      "Activation function",
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Cognitive neuroscience of visual object recognition",
      "Combinatorics",
      "Computer science",
      "Feature (linguistics)",
      "Feature extraction",
      "Geodesy",
      "Geography",
      "Kernel (algebra)",
      "Law",
      "Linguistics",
      "Mathematics",
      "Object detection",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Pooling",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Song",
        "given_name": "Yaoye"
      },
      {
        "surname": "Zhang",
        "given_name": "Peng"
      },
      {
        "surname": "Huang",
        "given_name": "Wei"
      },
      {
        "surname": "Zha",
        "given_name": "Yufei"
      },
      {
        "surname": "You",
        "given_name": "Tao"
      },
      {
        "surname": "Zhang",
        "given_name": "Yanning"
      }
    ]
  },
  {
    "title": "Arbitrary Order Total Variation for Deformable Image Registration",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109318",
    "abstract": "In this work, we investigate image registration in a variational framework and focus on regularization generality and solver efficiency. We first propose a variational model combining the state-of-the-art sum of absolute differences (SAD) and a new arbitrary order total variation regularization term. The main advantage is that this variational model preserves discontinuities in the resultant deformation while being robust to outlier noise. It is however non-trivial to optimize the model due to its non-convexity, non-differentiabilities, and generality in the derivative order. To tackle these, we propose to first apply linearization to the model to formulate a convex objective function and then break down the resultant convex optimization into several point-wise, closed-form subproblems using a fast, over-relaxed alternating direction method of multipliers (ADMM). With this proposed algorithm, we show that solving higher-order variational formulations is similar to solving their lower-order counterparts. Extensive experiments show that our ADMM is significantly more efficient than both the subgradient and primal-dual algorithms particularly when higher-order derivatives are used, and that our new models outperform state-of-the-art methods based on deep learning and free-form deformation. Our code implemented in both Matlab and Pytorch is publicly available at https://github.com/j-duan/AOTV.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323000195",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Convexity",
      "Economics",
      "Financial economics",
      "Image (mathematics)",
      "Linearization",
      "Mathematical optimization",
      "Mathematics",
      "Nonlinear system",
      "Outlier",
      "Physics",
      "Quantum mechanics",
      "Regularization (linguistics)",
      "Solver",
      "Subgradient method",
      "Total variation denoising"
    ],
    "authors": [
      {
        "surname": "Duan",
        "given_name": "Jinming"
      },
      {
        "surname": "Jia",
        "given_name": "Xi"
      },
      {
        "surname": "Bartlett",
        "given_name": "Joseph"
      },
      {
        "surname": "Lu",
        "given_name": "Wenqi"
      },
      {
        "surname": "Qiu",
        "given_name": "Zhaowen"
      }
    ]
  },
  {
    "title": "Region contrastive camera localization",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.03.030",
    "abstract": "Visual camera localization is a well-studied computer vision problem and has many applications. Recently, deep convolutional neural networks have begun to be utilized to solve six-degree-of-freedom (6-DoF) camera pose estimation via scene coordinate regression from a single RGB image and they outperform the traditional methods. However, recent works do not consider scene variations such as viewpoint, light, scale, etc due to the camera motion. In this work, we propose a region contrastive representation learning approach to alleviate these problems. The proposed approach maps image features from different camera views of the same 3D region to nearby points in the learned feature space. In contrast, it pushes visual features of other regions to distant points. Our method improves the existing camera localization methods and achieves state-of-the-art results on indoor 7-Scenes and outdoor Cambridge Landmarks datasets. Experimental results show that the proposed approach reduces the pose and angle errors and increases the average accuracy from 84.8% to 85.62% on the state-of-the-art baseline model. In addition, we perform an ablation study on a baseline network with different settings to demonstrate the efficiency of the proposed region contrastive camera localization method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523000983",
    "keywords": [
      "Artificial intelligence",
      "Camera auto-calibration",
      "Camera resectioning",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Feature (linguistics)",
      "Law",
      "Linguistics",
      "Philosophy",
      "Political science",
      "Politics",
      "RGB color model",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Sarıgül",
        "given_name": "Mehmet"
      },
      {
        "surname": "Karacan",
        "given_name": "Levent"
      }
    ]
  },
  {
    "title": "A local tangent plane distance-based approach to 3D point cloud segmentation via clustering",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109307",
    "abstract": "This paper proposes an effective measure for the planar segmentation problem based on the clustering method. It uses the distance from a point to the local plane as a metric to characterize the relationship between data. As a result, the data points of the coplanar have a high similarity to distinguish each plane. A dissimilarity matrix of the input point cloud can be evaluated, and multidimensional scaling analysis is performed to reconstruct the correlation information between data points in the 3D Euclidean space. The obtained reconstructed point cloud shows the separation between different planes. An adaptive DBSCAN clustering method based on density stratification is developed to perform cluster segmentation on the reconstructed point cloud. Experimental results show that the proposed method can effectively solve the over-segmentation problem, and at the same time provide high segmentation accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323000080",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "CURE data clustering algorithm",
      "Cluster analysis",
      "Computer science",
      "DBSCAN",
      "Euclidean distance",
      "Fuzzy clustering",
      "Geometry",
      "Hausdorff distance",
      "Image (mathematics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Point cloud",
      "Segmentation",
      "Similarity (geometry)",
      "Tangent space"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Hui"
      },
      {
        "surname": "Xie",
        "given_name": "Tingting"
      },
      {
        "surname": "Liang",
        "given_name": "Man"
      },
      {
        "surname": "Liu",
        "given_name": "Wanquan"
      },
      {
        "surname": "Liu",
        "given_name": "Peter Xiaoping"
      }
    ]
  },
  {
    "title": "Multi-scale attention guided pose transfer",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109315",
    "abstract": "Pose transfer refers to the probabilistic image generation of a person with a previously unseen novel pose from another image of that person having a different pose. Due to potential academic and commercial applications, pose transfer has been extensively studied in recent years. Among the various approaches to the problem, attention guided progressive generation is shown to produce state-of-the-art results in most cases. This paper presents an improved network architecture for pose transfer by introducing attention links at every resolution level of the encoder and decoder. By utilizing such dense multi-scale attention guided approach, we are able to achieve significant improvement over the existing methods both visually and analytically. We conclude our findings with extensive qualitative and quantitative comparisons against several existing methods on the DeepFashion dataset. We also show the generality of the proposed network architecture by extending it to multiple application domains, such as semantic reconstruction, virtual try-on and style manipulation. 1 1 For reproducibility, the code implementation and the pre-trained models are available at https://github.com/prasunroy/pose-transfer.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032300016X",
    "keywords": [
      "Architecture",
      "Art",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Encoder",
      "Generality",
      "Image (mathematics)",
      "Machine learning",
      "Operating system",
      "Pattern recognition (psychology)",
      "Physics",
      "Pose",
      "Probabilistic logic",
      "Psychology",
      "Psychotherapist",
      "Quantum mechanics",
      "Scale (ratio)",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Roy",
        "given_name": "Prasun"
      },
      {
        "surname": "Bhattacharya",
        "given_name": "Saumik"
      },
      {
        "surname": "Ghosh",
        "given_name": "Subhankar"
      },
      {
        "surname": "Pal",
        "given_name": "Umapada"
      }
    ]
  },
  {
    "title": "Poincaré Fréchet mean",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109302",
    "abstract": "Generalizing the Fréchet mean from the Euclidean metric is not able to properly capture the geometric characteristics of many non-trivial operations, such as the non-dot inner product and non-Euclidean gradients defined on the manifold. One effective solution is to derive its hyperbolic representations in the Poincaré or hyperboloid model. Our goodness-of-fit testing shows that the Poincaré Fréchet mean achieves much lower χ 2 power than that of the hyperboloid and typical non-linear kernels with regard to parameter perturbations. However, recent advanced optimization solvers, such as Riemannian gradient descent and minimizing upper bound, may result in imprecise convergences. This paper presents an ( 1 − ϵ )-approximation approach to search a core-set on the Poincaré model, reducing deviations of the Poincaré Fréchet mean to its optimum. A hierarchical splitting algorithm that implicitly explores the hyperbolic representations for an arbitrary manifold is then presented. Experiments show that the ( 1 − ϵ ) Poincaré Fréchet mean adopted in hierarchical splitting, achieves better representations than Euclidean, kernel, and Lorentzian Fréchet means in graph and image data.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323000031",
    "keywords": [
      "Applied mathematics",
      "Combinatorics",
      "Engineering",
      "Euclidean geometry",
      "Geometry",
      "Graph",
      "Hyperboloid",
      "Manifold (fluid mechanics)",
      "Mathematics",
      "Mechanical engineering"
    ],
    "authors": [
      {
        "surname": "Cao",
        "given_name": "Xiaofeng"
      }
    ]
  },
  {
    "title": "Feature weighting in DBSCAN using reverse nearest neighbours",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109314",
    "abstract": "DBSCAN is arguably the most popular density-based clustering algorithm, and it is capable of recovering non-spherical clusters. One of its main weaknesses is that it treats all features equally. In this paper, we propose a density-based clustering algorithm capable of calculating feature weights representing the degree of relevance of each feature, which takes the density structure of the data into account. First, we improve DBSCAN and introduce a new algorithm called DBSCANR. DBSCANR reduces the number of parameters of DBSCAN to one. Then, a new step is introduced to the clustering process of DBSCANR to iteratively update feature weights based on the current partition of data. The feature weights produced by the weighted version of the new clustering algorithm, W-DBSCANR, measure the relevance of variables in a clustering and can be used in feature selection in data mining applications where large and complex real-world data are often involved. Experimental results on both artificial and real-world data have shown that the new algorithms outperformed various DBSCAN type algorithms in recovering clusters in data.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323000158",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "CURE data clustering algorithm",
      "Cluster analysis",
      "Clustering high-dimensional data",
      "Computer science",
      "Correlation clustering",
      "DBSCAN",
      "Data mining",
      "Feature (linguistics)",
      "Linguistics",
      "Medicine",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Radiology",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Chowdhury",
        "given_name": "Stiphen"
      },
      {
        "surname": "Helian",
        "given_name": "Na"
      },
      {
        "surname": "Cordeiro de Amorim",
        "given_name": "Renato"
      }
    ]
  },
  {
    "title": "Efficient anisotropic scaling and translation invariants of Tchebichef moments using image normalization",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.03.015",
    "abstract": "Anisotropic scaling and translation invariants of Tchebichef moments are commonly used in image analysis to address issues arising from patterns distorted by non-uniform scaling and translation. In this paper, we formulate a new fast computational algorithm and a new normalization scheme based on the properties of Tchebichef moments. The proposed model can correctly resolve the mirror reflection ambiguities caused by the scaling transformations and gives smaller size deviations for various patterns so that the canonical form can easily fit within the normalized space. An empirical study shows that the proposed method significantly improves numerical computation and classification accuracy under non-noisy and noisy conditions when compared with existing methods. The main contribution of this paper is a novel fast computational algorithm for anisotropic scaling and translation invariants of Tchebichef moments and a new normalization scheme that produces features with higher discriminative power. The proposed algorithm is useful for recognizing objects with non-uniform size and displacement deformations. It is also a key component in formulating a better affine invariant algorithm for the image analysis community.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523000806",
    "keywords": [
      "Affine transformation",
      "Algorithm",
      "Anthropology",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computation",
      "Computer science",
      "Discriminative model",
      "Gene",
      "Geometry",
      "Invariant (physics)",
      "Mathematical physics",
      "Mathematics",
      "Messenger RNA",
      "Multidimensional scaling",
      "Normalization (sociology)",
      "Pattern recognition (psychology)",
      "Scale invariance",
      "Scaling",
      "Sociology",
      "Statistics",
      "Translation (biology)"
    ],
    "authors": [
      {
        "surname": "Pee",
        "given_name": "Chih-Yang"
      },
      {
        "surname": "Ong",
        "given_name": "S.H."
      },
      {
        "surname": "Raveendran",
        "given_name": "P."
      },
      {
        "surname": "Wong",
        "given_name": "L.K."
      }
    ]
  },
  {
    "title": "DeepSIR: Deep semantic iterative registration for LiDAR point clouds",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109306",
    "abstract": "This paper proposes DeepSIR, a novel learning-based iterative registration framework for real-world 3D LiDAR point clouds. Specifically, a front-end semantic feature extraction (Semantic-feat) model is designed to fully explore semantic information in LiDAR data. To highlight the recognized objects of interest, we propose a novel point score that uses semantic and geometric information. To effectively integrate the extracted semantic features, geometric features, and point scores, we introduce an aggregation module to learn a hybrid feature on each point. Meanwhile, our method dynamically explores feature descriptions and optimizes poses through an iterative pipeline. Extensive experiments on outdoor driving datasets demonstrate that our DeepSIR achieves comparable performance to state-of-the-art methods and runs at a much faster speed. The source code will be made publicly available. 1 1 https://github.com/LeoQLi/DeepSIR",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323000079",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Feature extraction",
      "Geography",
      "Geometry",
      "Iterative closest point",
      "Iterative method",
      "Lidar",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pipeline (software)",
      "Point (geometry)",
      "Point cloud",
      "Programming language",
      "Remote sensing",
      "Semantic feature",
      "Semantics (computer science)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Qing"
      },
      {
        "surname": "Wang",
        "given_name": "Cheng"
      },
      {
        "surname": "Wen",
        "given_name": "Chenglu"
      },
      {
        "surname": "Li",
        "given_name": "Xin"
      }
    ]
  },
  {
    "title": "Riemannian representation learning for multi-source domain adaptation",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109271",
    "abstract": "Multi-Source Domain Adaptation (MSDA) aims at training a classification model that achieves small target error, by leveraging labeled data from multiple source domains and unlabeled data from a target domain. The source and target domains are described by related but different joint distributions, which lie on a Riemannian manifold named the statistical manifold. In this paper, we characterize the joint distribution difference by the Hellinger distance, which bears strong connection to the Riemannian metric defined on the statistical manifold. We show that the target error of a neural network classification model is upper bounded by the average source error of the model and the average Hellinger distance, i.e., the average of multiple Hellinger distances between the source and target joint distributions in the network representation space. Motivated by the error bound, we introduce Riemannian Representation Learning (RRL): An approach that trains the network model by minimizing (i) the average empirical Hellinger distance with respect to the representation function, and (ii) the average empirical source error with respect to the network model. Specifically, we derive the average empirical Hellinger distance by constructing and solving unconstrained convex optimization problems whose global optimal solutions are easy to find. With the network model trained, we expect it to achieve small error in the target domain. Our experimental results on several image datasets demonstrate that the proposed RRL approach is statistically better than the comparison methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322007506",
    "keywords": [
      "Algorithm",
      "Applied mathematics",
      "Artificial intelligence",
      "Bounded function",
      "Computer science",
      "Curvature",
      "Domain (mathematical analysis)",
      "Economics",
      "Engineering",
      "Geometry",
      "Hellinger distance",
      "Information geometry",
      "Joint probability distribution",
      "Law",
      "Manifold (fluid mechanics)",
      "Mathematical analysis",
      "Mathematics",
      "Mechanical engineering",
      "Metric (unit)",
      "Operations management",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Riemannian manifold",
      "Scalar curvature",
      "Statistical manifold",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Sentao"
      },
      {
        "surname": "Zheng",
        "given_name": "Lin"
      },
      {
        "surname": "Wu",
        "given_name": "Hanrui"
      }
    ]
  },
  {
    "title": "How to Use K-means for Big Data Clustering?",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109269",
    "abstract": "K-means plays a vital role in data mining and is the simplest and most widely used algorithm under the Euclidean Minimum Sum-of-Squares Clustering (MSSC) model. However, its performance drastically drops when applied to vast amounts of data. Therefore, it is crucial to improve K-means by scaling it to big data using as few of the following computational resources as possible: data, time, and algorithmic ingredients. We propose a new parallel scheme of using K-means and K-means++ algorithms for big data clustering that satisfies the properties of a “true big data” algorithm and outperforms the classical and recent state-of-the-art MSSC approaches in terms of solution quality and runtime. The new approach naturally implements global search by decomposing the MSSC problem without using additional metaheuristics. This work shows that data decomposition is the basic approach to solve the big data clustering problem. The empirical success of the new algorithm allowed us to challenge the common belief that more data is required to obtain a good clustering solution. Moreover, the present work questions the established trend that more sophisticated hybrid approaches and algorithms are required to obtain a better clustering solution.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322007488",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Big data",
      "CURE data clustering algorithm",
      "Canopy clustering algorithm",
      "Cluster analysis",
      "Computer science",
      "Correlation clustering",
      "Data mining",
      "Data stream clustering"
    ],
    "authors": [
      {
        "surname": "Mussabayev",
        "given_name": "Rustam"
      },
      {
        "surname": "Mladenovic",
        "given_name": "Nenad"
      },
      {
        "surname": "Jarboui",
        "given_name": "Bassem"
      },
      {
        "surname": "Mussabayev",
        "given_name": "Ravil"
      }
    ]
  },
  {
    "title": "Self-supervised semi-supervised nonnegative matrix factorization for data clustering",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109282",
    "abstract": "Semi-supervised nonnegative matrix factorization exploits the strengths of matrix factorization in successfully learning part-based representation and is also able to achieve high learning performance when facing a scarcity of labeled data and a large amount of unlabeled data. Its major challenge lies in how to learn more discriminative representations from limited labeled data. Furthermore, self-supervised learning has been proven very effective at learning representations from unlabeled data in various learning tasks. Recent research works focus on utilizing the capacity of self-supervised learning to enhance semi-supervised learning. In this paper, we design an effective Self-Supervised Semi-Supervised Nonnegative Matrix Factorization (S4NMF) in a semi-supervised clustering setting. The S4NMF directly extracts a consensus result from ensembled NMFs with similarity and dissimilarity regularizations. In an iterative process, this self-supervisory information will be fed back to the proposed model to boost semi-supervised learning and form more distinct clusters. The proposed iterative algorithm is used to solve the given problem, which is defined as an optimization problem with a well-formulated objective function. In addition, the theoretical and empirical analyses investigate the convergence of the proposed optimization algorithm. To demonstrate the effectiveness of the proposed model in semi-supervised clustering, we conduct extensive experiments on standard benchmark datasets. The source code for reproducing our results can be found at https://github.com/ChavoshiNejad/S4NMF.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322007610",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Cluster analysis",
      "Computer science",
      "Discriminative model",
      "Eigenvalues and eigenvectors",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Machine learning",
      "Matrix decomposition",
      "Non-negative matrix factorization",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Semi-supervised learning",
      "Similarity (geometry)",
      "Similarity learning",
      "Supervised learning"
    ],
    "authors": [
      {
        "surname": "Chavoshinejad",
        "given_name": "Jovan"
      },
      {
        "surname": "Seyedi",
        "given_name": "Seyed Amjad"
      },
      {
        "surname": "Akhlaghian Tab",
        "given_name": "Fardin"
      },
      {
        "surname": "Salahian",
        "given_name": "Navid"
      }
    ]
  },
  {
    "title": "Frequency learning attention networks based on deep learning for automatic modulation classification in wireless communication",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109345",
    "abstract": "Deep neural networks have been recently applied in automatic modulation classification task and achieved remarkable success. However, Existing neural networks mainly focus on the purely data-driven architecture design, and fail to explore the hand-crafted feature mechanisms which are particularly significant for radio signal presentation in wireless communication. Inspired by digital signal processing theories, we propose frequency learning attention networks (FLANs) to analyze the radio spectral bias from frequency perspective, based on a multi-spectral attention mechanism for learning-based frequency components selection. FLANs are the general case of classical global average pooling and leverage identical structures of the popular neural networks. Extensive experiments have been conducted to validate the superiority of FLANs for automatic modulation classification over a wide variety of state-of-the-art methods on RADIOML 2018.01A dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323000468",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Deep learning",
      "Leverage (statistics)",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Pooling"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Duona"
      },
      {
        "surname": "Lu",
        "given_name": "Yuanyao"
      },
      {
        "surname": "Li",
        "given_name": "Yundong"
      },
      {
        "surname": "Ding",
        "given_name": "Wenrui"
      },
      {
        "surname": "Zhang",
        "given_name": "Baochang"
      },
      {
        "surname": "Xiao",
        "given_name": "Jing"
      }
    ]
  },
  {
    "title": "Non-residual unrestricted pruned ultra-faster line detection for edge devices",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109321",
    "abstract": "Line detection with deep learning is a popular visual task that focuses mostly on lane detection. It requires quicker inference speed and lower consumption, especially for high-speed edge device applications. Based on the UFAST, we propose the Non-Residual Unrestricted Pruned Ultra-faster (NRUPU) line detection via a novel model compression method including non-interference structural reconstruction (NISR), shallow channel priority reservation (SCPR) pruning and non-residual equivalent transformation (NRET). NISR is a structure reconstruction scheme allocating residual branches into each layer to solve the cross-layer channel interference in ResNet-18. SCPR pruning directly uses the factors of BN layers to build channel importance evaluation for backbone and designs channel selection method for head based on data distribution consistency, reducing the parameters of each layer independently. Then NRET losslessly converts the multi-branch model to a single-branch one containing only convolution, linear, and relu, which reduces implementation complexity on edge devices. These designs follow the theoretical foundations: the data distribution transformation trend and effect of gradient back-propagation on model learning ability. Compared with previous pruning methods, our method optimizes not only the parameters of the model but also the structure of the model. We train NRUPU in RTX2080Ti and deploy tests on edge devices NVIDIA Jetson Xavier NX (NJXN) and Atlas 200 DK (A2DK). Extensive experiments are conducted on the dataset TuSimple, CULane and our belt dataset with 11,894 data. Results show that NRUPU achieves over 96% speed increase and over 66% parameter reduction on all datasets within 0.7% accuracy loss. The FPS can reach 749, 665 and 783 on RTX2080Ti, 133, 117 and 143 on NJNX, 178, 161 and 183 on A2DK respectively. The code is released at https://anonymous.4open.science/r/NRUPU-29B0.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323000225",
    "keywords": [
      "Agronomy",
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Biology",
      "Channel (broadcasting)",
      "Chemistry",
      "Computer network",
      "Computer science",
      "Enhanced Data Rates for GSM Evolution",
      "Gene",
      "Geometry",
      "Interference (communication)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pruning",
      "Reduction (mathematics)",
      "Residual",
      "Transformation (genetics)"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Pengpeng"
      },
      {
        "surname": "Liu",
        "given_name": "Dongjingdian"
      },
      {
        "surname": "Gao",
        "given_name": "Shouwan"
      }
    ]
  },
  {
    "title": "Theoretical guarantee for crowdsourcing learning with unsure option",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109316",
    "abstract": "Crowdsourcing learning, in which labels are collected from multiple workers through crowdsourcing platforms, has attracted much attention during the past decade. This learning paradigm would reduce the labeling cost since crowdsourcing workers may be non-expert and hence less costly. On the other hand, crowdsourcing learning algorithms also suffer from being misled by incorrect labels introduced by imperfect workers. To control such risks, recently, it has been suggested to provide workers an additional unsure option during the labeling process. Although the benefits of the unsure option have been empirically demonstrated, theoretical analysis is still limited. In this article, a theoretical analysis of crowdsourcing learning with the unsure option is presented. Specifically, an upper bound of minimally sufficient number of crowd labels required for learning a probably approximately correct (PAC) classification model with and without the unsure option are given respectively. Next, a condition under which providing (or not providing) an unsure option to workers is derived. Then, the theoretical results are extended to guide non-identical label options (with or without unsure options) to different workers. Last, several useful applications are proposed based on theoretical results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323000171",
    "keywords": [
      "Artificial intelligence",
      "Business",
      "Computer science",
      "Crowdsourcing",
      "Data science",
      "Imperfect",
      "Linguistics",
      "Machine learning",
      "Operating system",
      "Philosophy",
      "Process (computing)",
      "Risk analysis (engineering)",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Pan",
        "given_name": "Yigong"
      },
      {
        "surname": "Tang",
        "given_name": "Ke"
      },
      {
        "surname": "Sun",
        "given_name": "Guangzhong"
      }
    ]
  },
  {
    "title": "Multi-modal unsupervised domain adaptation for semantic image segmentation",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109299",
    "abstract": "We propose a novel multi-modal-based Unsupervised Domain Adaptation (UDA) method for semantic segmentation. Recently, depth has proven to be a relevent property for providing geometric cues to enhance the RGB representation. However, existing UDA methods solely process RGB images or additionally cultivate depth-awareness with an auxiliary depth estimation task. We argue that geometric cues that are crucial to semantic segmentation, such as local shape and relative position, are challenging to recover from an auxiliary depth estimation task with mere color (RGB) information. In this paper, we propose a novel multi-modal UDA method named MMADT, which relies on both RGB and depth images as input. In particular, we design a Depth Fusion Block (DFB) to recalibrate depth information and leverage Depth Adversarial Training (DAT) to bridge the depth discrepancy between the source and target domain. Besides, we propose a self-supervised multi-modal depth estimation assistant network named Geo-Assistant (GA) to align the feature space of RGB and depth and shape the sensitivity of our MMADT to depth information. We experimentally observed significant performance improvement in multiple synthetic to real adaptation benchmarks, i.e., SYNTHIA-to-Cityscapes, GTA5-to-Cityscapes and SELMA-to-Cityscapes. Additionally, our multi-modal UDA scheme is easy to port to other UDA methods with a consistent performance boost.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322007786",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Leverage (statistics)",
      "Modal",
      "Pattern recognition (psychology)",
      "Polymer chemistry",
      "RGB color model",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Hu",
        "given_name": "Sijie"
      },
      {
        "surname": "Bonardi",
        "given_name": "Fabien"
      },
      {
        "surname": "Bouchafa",
        "given_name": "Samia"
      },
      {
        "surname": "Sidibé",
        "given_name": "Désiré"
      }
    ]
  },
  {
    "title": "Communication-efficient and Byzantine-robust distributed learning with statistical guarantee",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109312",
    "abstract": "Communication efficiency and robustness are two major issues in modern distributed learning frameworks. This is due to the practical situations where some computing nodes may have limited communication power or may behave adversarial behaviors. To address the two issues simultaneously, this paper develops two communication-efficient and robust distributed learning algorithms for convex problems. Our motivation is based on surrogate likelihood framework and the median and trimmed mean operations. Particularly, the proposed algorithms are provably robust against Byzantine failures, and also achieve optimal statistical rates for strong convex losses and convex (non-smooth) penalties. For typical statistical models such as generalized linear models, our results show that statistical errors dominate optimization errors in finite iterations. Simulated and real data experiments are conducted to demonstrate the numerical performance of our algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323000134",
    "keywords": [
      "Adversarial system",
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Communications system",
      "Computer science",
      "Convex optimization",
      "Distributed learning",
      "Gene",
      "Geometry",
      "Mathematical optimization",
      "Mathematics",
      "Pedagogy",
      "Psychology",
      "Regular polygon",
      "Robustness (evolution)",
      "Statistical model",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Xingcai"
      },
      {
        "surname": "Chang",
        "given_name": "Le"
      },
      {
        "surname": "Xu",
        "given_name": "Pengfei"
      },
      {
        "surname": "Lv",
        "given_name": "Shaogao"
      }
    ]
  },
  {
    "title": "Amercing: An intuitive and effective constraint for dynamic time warping",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109333",
    "abstract": "Dynamic Time Warping (DTW) is a time series distance measure that allows non-linear alignments between series. Constraints on the alignments in the form of windows and weights have been introduced because unconstrained DTW is too permissive in its alignments. However, windowing introduces a crude step function, allowing unconstrained flexibility within the window, and none beyond it. While not entailing a step function, a multiplicative weight is relative to the distances between aligned points along a warped path, rather than being a direct function of the amount of warping that is introduced. In this paper, we introduce Amerced Dynamic Time Warping (ADTW), a new, intuitive, DTW variant that penalizes the act of warping by a fixed additive cost. Like windowing and weighting, ADTW constrains the amount of warping. However, it avoids both abrupt discontinuities in the amount of warping allowed and the limitations of a multiplicative penalty. We formally introduce ADTW, prove some of its properties, and discuss its parameterization. We show on a simple example how it can be parameterized to achieve an intuitive outcome, and demonstrate its usefulness on a standard time series classification benchmark. We provide a demonstration application in C++ Herrmann(2021)[1].",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323000341",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Classification of discontinuities",
      "Computer science",
      "Constraint (computer-aided design)",
      "Data mining",
      "Dynamic time warping",
      "Evolutionary biology",
      "Function (biology)",
      "Geodesy",
      "Geography",
      "Geometry",
      "Image warping",
      "Mathematical analysis",
      "Mathematics",
      "Measure (data warehouse)",
      "Medicine",
      "Multiplicative function",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Radiology",
      "Series (stratigraphy)",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Herrmann",
        "given_name": "Matthieu"
      },
      {
        "surname": "Webb",
        "given_name": "Geoffrey I."
      }
    ]
  },
  {
    "title": "Reducing bi-level feature redundancy for unsupervised domain adaptation",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109319",
    "abstract": "Unsupervised domain adaptation (UDA) deals with the problem of transferring knowledge from a labeled source domain to an unlabeled target domain when the two domains have distinct data distributions. Therefore, the purpose of domain adaptation is to mitigate the distribution divergence between the two domains. Many existing UDA methods only use the traditional batch normalization layer, but this may lead to a large number of feature redundancy and lead performance degradation. In this paper, we introduce a novel deep learning paradigm called feature redundancy in UDA to enhance adaptation ability. Specifically, we first show that feature redundancy also exists on unsupervised domain adaptation (UDA), which has been ignored by most previous efforts. We utilize feature similarity as a metric to measure feature redundancy and then analyze the relationship between uniform feature spectrum and minimal feature similarity. Based on this relationship, we intend to reduce cross-domain feature redundancy for UDA by making the distribution of feature spectrum uniforms in a bi-level way. For the first level, we propose a cross-domain batch normalization with the whitening module (xBN) to ensure compact domain-specific features and learn domain-invariant features at the same time. With the domain-specific features from the first level that paves a way, on the second level, we suggest an alternative orthogonal regularizer (OR) that can make the distribution of the feature spectrum more uniform, thus domain-invariant feature redundancy is mitigated. Such a bi-level mechanism greatly reduces the feature redundancy for UDA. To evaluate the efficacy of the proposed bi-level mechanism, we plug those two novel modules (i.e., xBN and OR) into convolutional neural networks (CNNs) to form our UDA model and also conduct the corresponding empirical evaluations on five cross-domain object recognition benchmarks including both classical and large-scale image datasets. Experimental results show that the proposed UDA model could achieve state-of-the-art performance both in quantity and quality. Our source codes will be released after publication.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323000201",
    "keywords": [
      "Anthropology",
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Domain adaptation",
      "Feature (linguistics)",
      "Feature selection",
      "Linguistics",
      "Minimum redundancy feature selection",
      "Normalization (sociology)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Redundancy (engineering)",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Mengzhu"
      },
      {
        "surname": "Wang",
        "given_name": "Shanshan"
      },
      {
        "surname": "Wang",
        "given_name": "Wei"
      },
      {
        "surname": "Shen",
        "given_name": "Li"
      },
      {
        "surname": "Zhang",
        "given_name": "Xiang"
      },
      {
        "surname": "Lan",
        "given_name": "Long"
      },
      {
        "surname": "Luo",
        "given_name": "Zhigang"
      }
    ]
  },
  {
    "title": "FlexFormer: Flexible Transformer for efficient visual recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.03.028",
    "abstract": "Vision Transformers have shown overwhelming superiority in computer vision communities compared with convolutional neural networks. Nevertheless, the understanding of multi-head self attentions, as the de facto ingredient of Transformers, is still limited, which leads to surging interest in explaining its core ideology. A notable theory interprets that, unlike high-frequency sensitive convolutions, self-attention behaves like a generalized spatial smoothing and blurs the high spatial-frequency signals with depth increasing. In this paper, we design a Conv-MSA structure to extract efficient local contextual information and remedy the inherent drawback of self-attention. Accordingly, a flexible transformer structure named FlexFormer , with linear computational complexity on input image size, is proposed. Experimental results on several visual recognition benchmarks show that our FlexFormer achieved the state-of-the-art results on visual recognition tasks with fewer parameters and higher computational efficiency.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552300096X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computational complexity theory",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "De facto",
      "Electrical engineering",
      "Engineering",
      "Law",
      "Pattern recognition (psychology)",
      "Political science",
      "Smoothing",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Fan",
        "given_name": "Xinyi"
      },
      {
        "surname": "Liu",
        "given_name": "Huajun"
      }
    ]
  },
  {
    "title": "Knowledge aggregation networks for class incremental learning",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109310",
    "abstract": "Most existing class incremental learning methods rely on storing old exemplars to avoid catastrophic forgetting. However, these methods inevitably face the gradient conflict problem, the inherent conflict between new streaming knowledge and existing knowledge in the gradient direction. To alleviate gradient conflict, this paper reuses the previous knowledge and expands the branch to accommodate new concepts instead of fine-tuning the original models. Specifically, this paper designs a novel dual-branch network called Knowledge Aggregation Networks. The previously trained model is frozen as a branch to retain existing knowledge, and a consistent trainable network is constructed as the other branch to learn new concepts. An adaptive feature fusion module is adopted to dynamically balance the two branches’ information during training. Moreover, a model compression stage maintains the dual-branch structure. Extensive experiments on CIFAR-100, ImageNet-Sub, and ImageNet show that our method significantly outperforms the other methods and effectively balances stability and plasticity.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323000110",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Artificial neural network",
      "Class (philosophy)",
      "Computer science",
      "Dual (grammatical number)",
      "Feature (linguistics)",
      "Forgetting",
      "Generalization",
      "Incremental learning",
      "Linguistics",
      "Literature",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Philosophy",
      "Stability (learning theory)"
    ],
    "authors": [
      {
        "surname": "Fu",
        "given_name": "Zhiling"
      },
      {
        "surname": "Wang",
        "given_name": "Zhe"
      },
      {
        "surname": "Xu",
        "given_name": "Xinlei"
      },
      {
        "surname": "Li",
        "given_name": "Dongdong"
      },
      {
        "surname": "Yang",
        "given_name": "Hai"
      }
    ]
  },
  {
    "title": "An adaptive mutual K-nearest neighbors clustering algorithm based on maximizing mutual information",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109273",
    "abstract": "Clustering based on Mutual K-nearest Neighbors (CMNN) is a classical method of grouping data into different clusters. However, it has two well-known limitations: (1) the clustering results are very much dependent on the parameter k; (2) CMNN assumes that noise points correspond to clusters of small sizes according to the Mutual K-nearest Neighbors (MKNN) criterion, but some data points in small size clusters are wrongly identified as noises. To address these two issues, we propose an adaptive improved CMNN algorithm (AVCMNN), which consists of two parts: (1) improved CMNN algorithm (abbreviated as VCMNN) and (2) adaptive VCMNN algorithm (abbreviated as AVCMNN). Specifically, the first part is VCMNN algorithm, we first reassign the data points in some small-size clusters by a novel voting strategy because some of them are wrongly identified as noise points, and the clustering results are improved. Then, the second part is AVCMNN, we use maximizing mutual information to construct an objective function to optimize the parameters of the proposed method and finally obtain the better parameters values and clustering results. We conduct extensive experiments on twenty datasets, including six synthetic datasets, ten UCI datasets, and four image datasets. The experimental results show that VCMNN and AVCMNN outperforms three classical algorithms (i.e., CMNN, DPC, and DBSCAN) and six state-of-the-art (SOTA) clustering algorithms in most cases.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032200752X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "CURE data clustering algorithm",
      "Canopy clustering algorithm",
      "Cluster analysis",
      "Computer science",
      "Correlation clustering",
      "DBSCAN",
      "Data mining",
      "Data point",
      "Image (mathematics)",
      "Mathematics",
      "Mutual information",
      "Nearest-neighbor chain algorithm",
      "Noise (video)",
      "Pattern recognition (psychology)",
      "k-nearest neighbors algorithm"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Yizhang"
      },
      {
        "surname": "Pang",
        "given_name": "Wei"
      },
      {
        "surname": "Jiao",
        "given_name": "Zhixiang"
      }
    ]
  },
  {
    "title": "AMGB: Trajectory prediction using attention-based mechanism GCN-BiLSTM in IOV",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.03.006",
    "abstract": "Accurate and reliable prediction of vehicle trajectories is closely related to the path planning of intelligent vehicles and contributes to intelligent transportation safety, especially in dynamic and uncertain scenarios. However, most existing methods have difficulty in accurately capturing vehicle interactions and the dependencies between vehicle multimodal features in dynamic and uncertain driving environments. Thus, we propose a new Attention-based Mechanism GCN-BiLSTM trajectory prediction model (AMGB) which tackles trajectory prediction in dynamic environments from a new perspective of predicting vehicle motion direction and motion distance. Firstly, the Attention-based Time-Frequency domain Graph Convolutional Network (AT-GCN) module learns the dependencies between vehicle multimodal features and extracts coarse-grained features containing directions information of future trajectories. Then the Multi-structure based Bidirectional Long- Short Term Memory network (M-BiLSTM) module can acquire fine-grained features containing future trajectory distance from vehicle interaction information by the memory storage function of BiLSTM. Finally, we apply the attention mechanism to fuse the coarse and fine-grained features to establish the mapping relationship between vehicle multimodal features and interaction behaviors, and future trajectories. The proposed AMGB model is evaluated on the NGSIM dataset, the results confirm that our model outperforms other state-of-the-art models in both short-long term trajectory prediction.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523000715",
    "keywords": [
      "Artificial intelligence",
      "Astronomy",
      "Computer science",
      "Electrical engineering",
      "Engineering",
      "Epistemology",
      "Fuse (electrical)",
      "Graph",
      "Interaction information",
      "Machine learning",
      "Mathematics",
      "Mechanism (biology)",
      "Philosophy",
      "Physics",
      "Statistics",
      "Theoretical computer science",
      "Trajectory"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Ruonan"
      },
      {
        "surname": "Qin",
        "given_name": "Yang"
      },
      {
        "surname": "Wang",
        "given_name": "Jingbo"
      },
      {
        "surname": "Wang",
        "given_name": "Hongye"
      }
    ]
  },
  {
    "title": "Invariance encoding in sliced-Wasserstein space for image classification with limited training data",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109268",
    "abstract": "Deep convolutional neural networks (CNNs) are broadly considered to be state-of-the-art generic end-to-end image classification systems. However, they are known to underperform when training data are limited and thus require data augmentation strategies that render the method computationally expensive and not always effective. Rather than using a data augmentation strategy to encode invariances as typically done in machine learning, here we propose to mathematically augment a nearest subspace classification model in sliced-Wasserstein space by exploiting certain mathematical properties of the Radon Cumulative Distribution Transform (R-CDT), a recently introduced image transform. We demonstrate that for a particular type of learning problem, our mathematical solution has advantages over data augmentation with deep CNNs in terms of classification accuracy and computational complexity, and is particularly effective under a limited training data setting. The method is simple, effective, computationally efficient, non-iterative, and requires no parameters to be tuned. Python code implementing our method is available at https://github.com/rohdelab/mathematical_augmentation. Our method is integrated as a part of the software package PyTransKit, which is available at https://github.com/rohdelab/PyTransKit.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322007476",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Code (set theory)",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "ENCODE",
      "Encoding (memory)",
      "Gene",
      "Operating system",
      "Pattern recognition (psychology)",
      "Programming language",
      "Python (programming language)",
      "Set (abstract data type)",
      "Source code",
      "Subspace topology"
    ],
    "authors": [
      {
        "surname": "Shifat-E-Rabbi",
        "given_name": "Mohammad"
      },
      {
        "surname": "Zhuang",
        "given_name": "Yan"
      },
      {
        "surname": "Li",
        "given_name": "Shiying"
      },
      {
        "surname": "Rubaiyat",
        "given_name": "Abu Hasnat Mohammad"
      },
      {
        "surname": "Yin",
        "given_name": "Xuwang"
      },
      {
        "surname": "Rohde",
        "given_name": "Gustavo K."
      }
    ]
  },
  {
    "title": "Text-to-image synthesis with self-supervised bi-stage generative adversarial network",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.03.023",
    "abstract": "Text-to-image synthesis is challenging as generating images that are visually realistic and semantically consistent with the given text description involves multi-modal learning with text and image. To address the challenges, this paper presents a text-to-image synthesis model that utilizes self-supervision and bi-stage image distribution architecture, referred to as the Self-Supervised Bi-Stage Generative Adversarial Network (SSBi-GAN). The self-supervision diversifies the learned representation thus improving the quality of the synthesized images. Besides that, the bi-stage architecture with Residual network enables the generation of larger images with finer visual contents. Not only that, some enhancements including L1 distance, one-sided smoothing and feature matching are incorporated to enhance the visual realism and semantic consistency of the images as well as the training stability of the model. The empirical results on Oxford-102 and CUB datasets corroborate the ability of the proposed SSBi-GAN in generating visually realistic and semantically consistent images.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523000880",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Computer vision",
      "Consistency (knowledge bases)",
      "Feature (linguistics)",
      "Generative grammar",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Image translation",
      "Law",
      "Linguistics",
      "Machine learning",
      "Matching (statistics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Smoothing",
      "Stability (learning theory)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Tan",
        "given_name": "Yong Xuan"
      },
      {
        "surname": "Lee",
        "given_name": "Chin Poo"
      },
      {
        "surname": "Neo",
        "given_name": "Mai"
      },
      {
        "surname": "Lim",
        "given_name": "Kian Ming"
      },
      {
        "surname": "Lim",
        "given_name": "Jit Yan"
      }
    ]
  },
  {
    "title": "A Dual Self-Attention mechanism for vehicle re-Identification",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109258",
    "abstract": "Vehicle re-identification has attracted tremendous attention from computer vision communities for its extensive applications in intelligent transportation and public security, while the high inter-class similarity and the large intra-class difference between vehicles bring out great challenges for re-identification (re-ID). To tackle these challenges, we learn from the self-attention mechanism in Natural Language Processing and propose a dual self-attention module to learn different regional dependencies: static self-attention for selectively refining semantic features and dynamic self-attention (called cross-region attention) for enhancing the spatial awareness of local feature. The static self-attention refines attended pixels within the entire image and salient regions, while the cross-region attention creatively captures the position-related regional dependencies for pixels within the windscreen area. These attention modules capture long-range dependencies and relative position information between different pixels or regions for vehicle feature learning globally and locally, realizing an efficient vehicle feature embedding by concatenating these augmented features for vehicle re-ID. Extensive experiments demonstrate the effectiveness and promising performance of our approach against the state-of-the-arts.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322007373",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Class (philosophy)",
      "Computer science",
      "Computer vision",
      "Dual (grammatical number)",
      "Economics",
      "Embedding",
      "Epistemology",
      "Feature (linguistics)",
      "Finance",
      "Identification (biology)",
      "Linguistics",
      "Literature",
      "Mechanism (biology)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pixel",
      "Position (finance)",
      "Process (computing)",
      "Salient"
    ],
    "authors": [
      {
        "surname": "Zhu",
        "given_name": "Wenqian"
      },
      {
        "surname": "Wang",
        "given_name": "Zhongyuan"
      },
      {
        "surname": "Wang",
        "given_name": "Xiaochen"
      },
      {
        "surname": "Hu",
        "given_name": "Ruimin"
      },
      {
        "surname": "Liu",
        "given_name": "Huikai"
      },
      {
        "surname": "Liu",
        "given_name": "Cheng"
      },
      {
        "surname": "Wang",
        "given_name": "Chao"
      },
      {
        "surname": "Li",
        "given_name": "Dengshi"
      }
    ]
  },
  {
    "title": "Learning a bi-directional discriminative representation for deep clustering",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109237",
    "abstract": "Nowadays, deep clustering achieves superior performance by jointly performing representation learning and cluster assignment. Although numerous deep clustering algorithms have emerged, most of them have difficulty learning representations that fit the clustering distribution. To address this issue, we propose a bi-directional discriminative representation learning clustering (BDRC) framework in this paper. In our framework, a dual autoencoder network, a bi-directional mutual information maximization module and a self-supervised cluster prediction module are combined into a joint optimization framework. To learn more cluster-friendly representations, the bi-directional mutual information maximization module is executed on both samples and their nearest neighbors to explore the cluster relationships between samples. In order to improve the stability of the model, a self-supervised cluster prediction module is devised to predict clustering assignments to supervise the autoencoder using the KL-divergence. Moreover, the UMAP is used to find the manifold of the latent representations which can better preserve the global structure. Experiments on some benchmark datasets demonstrate the superiority of the proposed BDRC algorithm.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322007166",
    "keywords": [
      "Artificial intelligence",
      "Autoencoder",
      "Benchmark (surveying)",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Deep learning",
      "Discriminative model",
      "Feature learning",
      "Geodesy",
      "Geography",
      "Law",
      "Machine learning",
      "Margin (machine learning)",
      "Mathematical optimization",
      "Mathematics",
      "Maximization",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Stability (learning theory)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Yiming"
      },
      {
        "surname": "Chang",
        "given_name": "Dongxia"
      },
      {
        "surname": "Fu",
        "given_name": "Zhiqiang"
      },
      {
        "surname": "Zhao",
        "given_name": "Yao"
      }
    ]
  },
  {
    "title": "Efficient sampling using feature matching and variable minimal structure size",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109311",
    "abstract": "Greedy search-based guided sampling is a promising research field in model fitting to data with multiple structures in the presence of a large number of outliers. However, these greedy search-based guided sampling algorithms are sensitive to the fixed minimal (acceptable) structure size and the initial model hypothesis: when the fixed minimal structure size is too small, data subsets sampled by these algorithms are not representative. In contrast, when it is too large, data subsets might be contaminated by outliers. Furthermore, these algorithms may fail to obtain an accurate model hypothesis, if the initial model hypothesis is far from the true model. In this paper, we address the above-mentioned two issues by proposing two greedy search-based strategies: one aims to adaptively estimate minimal structure sizes and the other aims to generate effective initial model hypotheses. Specifically, on one hand, to avoid using the fixed minimal structure size, a strategy is proposed to adaptively estimate minimal structure sizes by using previously obtained ones. On the other hand, to reduce the impact of outliers, a strategy is proposed to filter out outliers to obtain a reduced data subset by using a feature matching algorithm. Then, this strategy generates promising initial model hypotheses by using a proximity sampling on the reduced data subset. Finally, an efficient sampling algorithm based on the two proposed greedy search-based strategies is applied to three vision tasks, i.e., fundamental matrix estimation, homography plane detection and 3D motion segmentation. Extensive experimental results demonstrate the effectiveness of the proposed sampling algorithm.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323000122",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Data structure",
      "Feature (linguistics)",
      "Filter (signal processing)",
      "Greedy algorithm",
      "Linguistics",
      "Matching (statistics)",
      "Mathematics",
      "Outlier",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Sampling (signal processing)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Lai",
        "given_name": "Taotao"
      },
      {
        "surname": "Sadri",
        "given_name": "Alireza"
      },
      {
        "surname": "Lin",
        "given_name": "Shuyuan"
      },
      {
        "surname": "Li",
        "given_name": "Zuoyong"
      },
      {
        "surname": "Chen",
        "given_name": "Riqing"
      },
      {
        "surname": "Wang",
        "given_name": "Hanzi"
      }
    ]
  },
  {
    "title": "DCSNE: Density-based Clustering using Graph Shared Neighbors and Entropy",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109341",
    "abstract": "Density-based clustering techniques identify arbitrary shaped clusters in the presence of outliers by capturing the intrinsic distribution of data and separating high and low-density regions based on the neighborhood information. They use global parameters to compute the density distribution of data points, the optimization of which is quite a challenging and time intensive task. The similarity graphs constructed from the data points can easily capture the topology of the density regions without using any user-defined parameters. Moreover, the concept of entropy can be useful to capture the randomness and disorderliness present in highly complex data distributions. Our proposed algorithm makes use of entropy in conjunction with the graph local neighborhood information to compute density distribution of data points. Then, actual clusters are identified using the density distribution and the shared neighbor information between the regions. The experimental results show that the proposed technique outperforms other comparable methods in terms of cluster quality in the presence of noise on diversified gene expression and real datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323000420",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "CURE data clustering algorithm",
      "Cluster analysis",
      "Computer science",
      "Correlation clustering",
      "Data mining",
      "Determining the number of clusters in a data set",
      "Entropy (arrow of time)",
      "Graph",
      "Mathematics",
      "Outlier",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Randomness",
      "Statistics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Maheshwari",
        "given_name": "Rashmi"
      },
      {
        "surname": "Mohanty",
        "given_name": "Sraban Kumar"
      },
      {
        "surname": "Mishra",
        "given_name": "Amaresh Chandra"
      }
    ]
  },
  {
    "title": "Exploiting mixing regularization for truly unsupervised font synthesis",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.03.019",
    "abstract": "Creating a novel font set requires domain expertise and is a laborious and time-consuming process, particularly for languages with a large number of characters and complicated structures. Existing deep learning based methods consider font generation (FG) as an image-to-image translation problem, mostly in a supervised setting, either in the form of pair images (paired data) or font labels (character or style labels), which requires extensive effort and is expensive to collect. Additionally, these supervised counter parts lack generalization for extending to other text image-related tasks, such as word image generation and font attribute control at inference time. We found that these drawbacks are mainly due to the supervised setting adopted by these existing methods for font generation. In this paper, we tackle the FG problem in a truly unsupervised fashion, where a complete font set can be generated by training the generator such that adjacent styles are not correlated and projecting the input glyph image into its corresponding font style latent space. To accomplish this, we propose the Font Mixing Generative Adversarial Network (FM-GAN), which employs mixing regularization to supervise the generator to localize the font styles, and a projection encoder to project an arbitrary glyph image into its corresponding semantic space that is compatible with the generator. In the experiments, we demonstrated that our unsupervised model synthesizes font images that are comparable to supervised state-of-the-art FG baselines. Furthermore, FM-GAN can be directly applied to other text image related tasks, such as multi-lingual font style transfer, word image generation, and font attribute control.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523000843",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Font",
      "Generator (circuit theory)",
      "Glyph (data visualization)",
      "Inference",
      "Pattern recognition (psychology)",
      "Physics",
      "Power (physics)",
      "Quantum mechanics",
      "Regularization (linguistics)",
      "Visualization"
    ],
    "authors": [
      {
        "surname": "Muhammad",
        "given_name": "Ammar Ul Hassan"
      },
      {
        "surname": "Lee",
        "given_name": "Hyunsoo"
      },
      {
        "surname": "Choi",
        "given_name": "Jaeyoung"
      }
    ]
  },
  {
    "title": "A comprehensive evaluation framework for deep model robustness",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109308",
    "abstract": "Deep neural networks (DNNs) have achieved remarkable performance across a wide range of applications, while they are vulnerable to adversarial examples, which motivates the evaluation and benchmark of model robustness. However, current evaluations usually use simple metrics to study the performance of defenses, which are far from understanding the limitation and weaknesses of these defense methods. Thus, most proposed defenses are quickly shown to be attacked successfully, which results in the “arm race” phenomenon between attack and defense. To mitigate this problem, we establish a model robustness evaluation framework containing 23 comprehensive and rigorous metrics, which consider two key perspectives of adversarial learning (i.e., data and model). Through neuron coverage and data imperceptibility, we use data-oriented metrics to measure the integrity of test examples; by delving into model structure and behavior, we exploit model-oriented metrics to further evaluate robustness in the adversarial setting. To fully demonstrate the effectiveness of our framework, we conduct large-scale experiments on multiple datasets including CIFAR-10, SVHN, and ImageNet using different models and defenses with our open-source platform. Overall, our paper provides a comprehensive evaluation framework, where researchers could conduct comprehensive and fast evaluations using the open-source toolkit, and the analytical results could inspire deeper understanding and further improvement to the model robustness.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323000092",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Benchmarking",
      "Biochemistry",
      "Business",
      "Chemistry",
      "Computer science",
      "Computer security",
      "Data mining",
      "Deep learning",
      "Deep neural networks",
      "Exploit",
      "Gene",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Marketing",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Guo",
        "given_name": "Jun"
      },
      {
        "surname": "Bao",
        "given_name": "Wei"
      },
      {
        "surname": "Wang",
        "given_name": "Jiakai"
      },
      {
        "surname": "Ma",
        "given_name": "Yuqing"
      },
      {
        "surname": "Gao",
        "given_name": "Xinghai"
      },
      {
        "surname": "Xiao",
        "given_name": "Gang"
      },
      {
        "surname": "Liu",
        "given_name": "Aishan"
      },
      {
        "surname": "Dong",
        "given_name": "Jian"
      },
      {
        "surname": "Liu",
        "given_name": "Xianglong"
      },
      {
        "surname": "Wu",
        "given_name": "Wenjun"
      }
    ]
  },
  {
    "title": "MACnet: Mask augmented counting network for class-agnostic counting",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.03.017",
    "abstract": "Class agnostic counting involves counting the instances of any user-defined class. It is also usually phrased as a matching problem wherein the model finds all the matching objects in a query image given exemplar patches containing the target object. Typically, users define exemplar patches by placing bounding boxes around the target object. However, defining exemplars using bounding boxes inevitably captures both the target object (foreground) and its surrounding background. This would unintentionally match patches similar to the background, leading to an inaccurate count. Moreover, objects poorly represented by a bounding box (e.g., non-axis aligned, irregular, or non-rectangular shapes) may capture a significantly disproportionate amount of background relative to the foreground within the exemplar patch, leading to poor matches. In this paper, we propose to utilize segmentation masks to separate target objects from their background. We derived these segmentation masks from extreme points, which requires no additional annotation effort from the user compared to annotating bounding boxes. Moreover, we designed a module that learns the mask features as residual to the object features, allowing the network to learn how to better incorporate the segmentation masks. Our model improves upon state-of-the-art methods by up to 3.7 MAE points on the FSC-147 benchmark dataset, showing the effectiveness of our masking approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552300082X",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Bounding overwatch",
      "Class (philosophy)",
      "Computer science",
      "Computer vision",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Masking (illustration)",
      "Matching (statistics)",
      "Mathematics",
      "Minimum bounding box",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Segmentation",
      "Statistics",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "McCarthy",
        "given_name": "Tadhg"
      },
      {
        "surname": "Virtusio",
        "given_name": "John Jethro"
      },
      {
        "surname": "Ople",
        "given_name": "Jose Jaena Mari"
      },
      {
        "surname": "Tan",
        "given_name": "Daniel Stanley"
      },
      {
        "surname": "Amalin",
        "given_name": "Divina"
      },
      {
        "surname": "Hua",
        "given_name": "Kai-Lung"
      }
    ]
  },
  {
    "title": "Sequential Likelihood-Free Inference with Neural Proposal",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.03.021",
    "abstract": "Bayesian inference without the likelihood evaluation, or likelihood-free inference, has been a key research topic in simulation studies for gaining quantitatively validated simulation models on real-world datasets. As the likelihood evaluation is inaccessible, previous papers train the amortized neural network to estimate the ground-truth posterior for the simulation of interest. Training the network and accumulating the dataset alternatively in a sequential manner could save the total simulation budget by orders of magnitude. In the data accumulation phase, the new simulation inputs are chosen within a portion of the total simulation budget to accumulate upon the collected dataset so far. This newly accumulated data degenerates because the set of simulation inputs is hardly mixed, and this degenerated data collection process ruins the posterior inference. This paper introduces a new sampling approach, called Neural Proposal (NP), of the simulation input that resolves the biased data collection as it guarantees the i.i.d. sampling. The experiments show the improved performance of our sampler, especially for the simulations with multi-modal posteriors.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523000867",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Bayesian inference",
      "Bayesian probability",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Data set",
      "Filter (signal processing)",
      "Inference",
      "Machine learning",
      "Operating system",
      "Posterior probability",
      "Process (computing)",
      "Programming language",
      "Sampling (signal processing)",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Kim",
        "given_name": "Dongjun"
      },
      {
        "surname": "Song",
        "given_name": "Kyungwoo"
      },
      {
        "surname": "Kim",
        "given_name": "Yoon-Yeong"
      },
      {
        "surname": "Shin",
        "given_name": "Yongjin"
      },
      {
        "surname": "Kang",
        "given_name": "Wanmo"
      },
      {
        "surname": "Moon",
        "given_name": "Il-Chul"
      },
      {
        "surname": "Joo",
        "given_name": "Weonyoung"
      }
    ]
  },
  {
    "title": "Calibrated reconstruction based adversarial autoencoder model for novelty detection",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.03.026",
    "abstract": "Novelty detection detects outliers located at any location, such as abnormalities (i.e., far distance outliers) and novel/unobserved patterns (i.e., close distance outliers). While many novelty detection approaches have been proposed in the literature, they generally focus on detecting one specific type of outlier, e.g., Multi-Class Open Set Recognition (MCOSR) and One-Class Novelty Detection (OCND) approaches are applied for far and close distance outlier detection, respectively. However, in practice, it is difficult to measure in advance whether the distance between outliers and inliers is far or close. Recent work on outlier detection at any location with a unified model has yielded mixed performance. In this paper, we propose a new unified model, named Calibrated Reconstruction Based Adversarial AutoEncoder (CRAAE), for location agnostic outlier detection. The key idea is to integrate implicit and explicit confidence calibration strategies into a reconstruction based model for building a more accurate decision boundary. We leverage the category information disentangled from feature space to calibrate the decision metric (i.e., reconstruction error) constructed in the original data space. CRAAE also adds Uniform or Dirichlet noise into the artificial outlier generation process to represent various outliers. Experimental results show that CRAAE can outperform state-of-the-art unified models (e.g., GPND) and achieve similar performance with OCND and MCOSR methods in close and far distance outlier detection, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523000995",
    "keywords": [
      "Anomaly detection",
      "Artificial intelligence",
      "Artificial neural network",
      "Autoencoder",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Decision boundary",
      "Economics",
      "Metric (unit)",
      "Novelty",
      "Novelty detection",
      "Operations management",
      "Outlier",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Theology"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Yi"
      },
      {
        "surname": "Li",
        "given_name": "Ying"
      },
      {
        "surname": "Jourjon",
        "given_name": "Guillaume"
      },
      {
        "surname": "Seneviratne",
        "given_name": "Suranga"
      },
      {
        "surname": "Thilakarathna",
        "given_name": "Kanchana"
      },
      {
        "surname": "Cheng",
        "given_name": "Adriel"
      },
      {
        "surname": "Webb",
        "given_name": "Darren"
      },
      {
        "surname": "Xu",
        "given_name": "Richard Yi Da"
      }
    ]
  },
  {
    "title": "Shallow decision trees for explainable k -means clustering",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109239",
    "abstract": "A number of recent works have employed decision trees for the construction of explainable partitions that aim to minimize the k -means cost function. These works, however, largely ignore metrics related to the depths of the leaves in the resulting tree, which is perhaps surprising considering how the explainability of a decision tree depends on these depths. To fill this gap in the literature, we propose an efficient algorithm with a penalty term in its loss function to favor the construction of shallow decision trees – i.e., trees whose leaves are not very deep, which translate to clusters that are defined by a small number of attributes and are therefore easier to explain. In experiments on 16 datasets, our algorithm yields better results than decision-tree clustering algorithms recently presented in the literature, typically achieving lower or equivalent costs with considerably shallower trees.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032200718X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Cluster analysis",
      "Combinatorics",
      "Computer science",
      "Data mining",
      "Decision tree",
      "Evolutionary biology",
      "Function (biology)",
      "Machine learning",
      "Mathematics",
      "Tree (set theory)"
    ],
    "authors": [
      {
        "surname": "Laber",
        "given_name": "Eduardo"
      },
      {
        "surname": "Murtinho",
        "given_name": "Lucas"
      },
      {
        "surname": "Oliveira",
        "given_name": "Felipe"
      }
    ]
  },
  {
    "title": "Enriching the dialogue state tracking model with a asyntactic discourse graph",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.03.024",
    "abstract": "Dialogue state tracking is a key component of a task-oriented dialogue system. Recently, pretrained language models are widely used to track a dialogue state by generating or extracting values from a dialogue. These models have a shortcoming that they can explicitly encode contextual information of a dialogue. In this paper, we present a Syntactically ENriched Discourse Dialogue State Tracking model (SEND-DST) that fully leverages the discourse and syntax information of a dialogue. Our model consists of two parts: a dialogue encoding module and a slot-value extraction module. To provide a model with syntactic structures of utterances and the discourse flow of a dialogue, we devise a Syntactic Discourse (SD) graph using the dependency trees of the utterances. Experimental results show that SEND-DST achieves state-of-the-art results on multiple datasets, such as MultiWOZ 2.1, demonstrating its effectiveness.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523000958",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Dependency (UML)",
      "ENCODE",
      "Economics",
      "Gene",
      "Graph",
      "Management",
      "Natural language processing",
      "Programming language",
      "State (computer science)",
      "Syntax",
      "Task (project management)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Yu",
        "given_name": "Haeun"
      },
      {
        "surname": "Ko",
        "given_name": "Youngjoong"
      }
    ]
  },
  {
    "title": "Neural network for ordinal classification of imbalanced data by minimizing a Bayesian cost",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109303",
    "abstract": "Ordinal classification of imbalanced data is a challenging problem that appears in many real world applications. The challenge is to simultaneously consider the order of the classes and the class imbalance, which can notably improve the performance metrics. The Bayesian formulation allows to deal with these two characteristics jointly: It takes into account the prior probability of each class and the decision costs, which can be used to include the imbalance and the ordinal information, respectively. We propose to use the Bayesian formulation to train neural networks, which have shown excellent results in many classification tasks. A loss function is proposed to train networks with a single neuron in the output layer and a threshold based decision rule. The loss is an estimate of the Bayesian classification cost, based on the Parzen windows estimator, which is fitted for a thresholded decision. Experiments with several real datasets show that the proposed method provides competitive results in different scenarios, due to its high flexibility to specify the relative importance of the errors in the classification of patterns of different classes, considering the order and independently of the probability of each class.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323000043",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Bayesian probability",
      "Class (philosophy)",
      "Computer science",
      "Data mining",
      "Estimator",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Lázaro",
        "given_name": "Marcelino"
      },
      {
        "surname": "Figueiras-Vidal",
        "given_name": "Aníbal R."
      }
    ]
  },
  {
    "title": "TCCFusion: An infrared and visible image fusion method based on transformer and cross correlation",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109295",
    "abstract": "Infrared and visible image fusion aims to obtain a synthetic image that can simultaneously exhibit salient objects and provide abundant texture details. However, existing deep learning-based methods generally depend on convolutional operations, which indeed have good local feature extraction ability, but the restricted receptive field limits its capability in modeling long-range dependencies. To conquer this dilemma, we propose an infrared and visible image fusion method based on Transformer and cross correlation, named TCCFusion. Specifically, we design a local feature extraction branch (LFEB) to preserve local complementary information, in which a dense-shape network is introduced to reuse the information that may be lost during the convolutional operation. To avoid the limitation of the receptive field and to fully extract the global significant information, a global feature extraction branch (GFEB) is devised that consists of three Transformer blocks for long-range relationship construction. In addition, LFEB and GFEB are arranged in a parallel fashion to maintain local and global useful information in a more effective way. Furthermore, we design a cross correlation loss to train the proposed fusion model in an unsupervised manner, with which the fusion result can obtain adequate thermal radiation information in an infrared image and ample texture details in a visible image. Massive experiments on two mainstream datasets illustrate that our TCCFusion outperforms state-of-the-art algorithms not only on visual quality but also on quantitative assessments. Ablation experiments on the network framework and objective function demonstrate the effectiveness of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322007749",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Feature extraction",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Tang",
        "given_name": "Wei"
      },
      {
        "surname": "He",
        "given_name": "Fazhi"
      },
      {
        "surname": "Liu",
        "given_name": "Yu"
      }
    ]
  },
  {
    "title": "Toward a blind image quality evaluator in the wild by learning beyond human opinion scores",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109296",
    "abstract": "Nowadays, most existing blind image quality assessment (BIQA) models i n t h e w i l d heavily rely on human ratings, which are extraordinarily labor-expensive to collect. Here, we propose an o p i n i o n − f r e e BIQA method that learns from multiple annotators to assess the perceptual quality of images captured in the wild. Specifically, we first synthesize distorted images based on the pristine counterparts. We then randomly assemble a set of image pairs from the synthetic images, and use a group of IQA models to assign pseudo-binary labels for each pair indicating which image has higher quality as the supervisory signal. Based on the newly established pseudo-labeled dataset, we train a deep neural network (DNN)-based BIQA model to rank the perceptual quality, optimized for consistency with the binary rank labels. Since there exists domain shift, e.g., distortion shift and content shift, between the synthetic and in-the-wild images, we leverage two ways to alleviate this issue. First, the simulated distortions should be similar to authentic distortions as much as possible. Second, an unsupervised domain adaptation (UDA) module is further applied to encourage learning domain-invariant features between two domains. Extensive experiments demonstrate the effectiveness of our proposed o p i n i o n − f r e e BIQA model, yielding SOTA performance in terms of correlation with human opinion scores, as well as gMAD competition. Our code is available at: https://github.com/wangzhihua520/OF_BIQA.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322007750",
    "keywords": [
      "Amplifier",
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Computer network",
      "Computer science",
      "Distortion (music)",
      "Image (mathematics)",
      "Image quality",
      "Leverage (statistics)",
      "Machine learning",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Perception",
      "Programming language",
      "Psychology",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Zhihua"
      },
      {
        "surname": "Tang",
        "given_name": "Zhi-Ri"
      },
      {
        "surname": "Zhang",
        "given_name": "Jianguo"
      },
      {
        "surname": "Fang",
        "given_name": "Yuming"
      }
    ]
  },
  {
    "title": "Attentive ExFeat based deep generative adversarial network for noise robust face super-resolution",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.03.025",
    "abstract": "A new noise robust face super-resolution model using an attentive ExFeat-based generative adversarial network is proposed in this paper. The proposed model introduces the Exigent Feature Attention Unit (ExFAU) which consists of an Exigent Feature (ExFeat) block with a spatial attention unit to enhance the visual quality of the generated face images. The ExFAU block assists the model in reducing the noise and extracting the micro and high-level facial features. Further, the ExFeat block is followed by a spatial attention unit to focus on specific facial features. This allows us to give more attention to key face attributes related features and less to the remaining features. The proposed model repeats the ExFAU block to focus on different facial components and enhance them to improve the overall quality of the resultant face images. Experimental outcomes exhibit that the proposed model gains state-of-the-art performance on the standard datasets, namely CelebAHQ, Helen, and LFW face.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523000971",
    "keywords": [
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Computer science",
      "Computer vision",
      "Face (sociological concept)",
      "Feature (linguistics)",
      "Focus (optics)",
      "Generative adversarial network",
      "Generative grammar",
      "Geometry",
      "Image (mathematics)",
      "Linguistics",
      "Mathematics",
      "Noise (video)",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Social science",
      "Sociology",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Tomar",
        "given_name": "Anurag Singh"
      },
      {
        "surname": "Arya",
        "given_name": "K.V."
      },
      {
        "surname": "Rajput",
        "given_name": "Shyam Singh"
      }
    ]
  },
  {
    "title": "Automatically weighted binary multi-view clustering via deep initialization (AW-BMVC)",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109281",
    "abstract": "Clustering is inherently a process of exploratory data analysis. It has attracted more attention recently because much real-world data consists of multiple representations or views. However, it becomes increasingly problematic when dealing with large and heterogeneous data. It is worth noting that several approaches have been developed to increase computational efficiency, although most of them have some drawbacks: (1) Most existing techniques consider equal or static weights to quantify importance across different views and samples, so common and complementary features cannot be used. (2) The clustering task is performed by arbitrary initialization without caring about the rich structure of the joint discrete representation, and thus poorly executed. In this paper, we propose a novel approach called “Auto-Weighted Binary Multi-View Clustering Via Deep Initialization” for large-scale multi-view clustering based on two main scenarios. First, we consider the distinction between different views based on the importance of samples, and therefore apply a dynamic learning strategy for the automatic weighting of views and samples. Second, in the context of initializing binary clustering, we develop a new CNN feature and use a low-dimensional binary embedding by exploiting the efficient capabilities of Fourier mapping. Moreover, our approach simultaneously learns a joint discrete representation and performs direct clustering using a constrained binary matrix factorization; the optimization problem is perfectly solved in a unified learning model. Experimental results conducted on several challenging datasets demonstrate the effectiveness and superiority of the proposed approach over state-of-the-art methods in terms of accuracy, normalized mutual information, and purity.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322007609",
    "keywords": [
      "Arithmetic",
      "Artificial intelligence",
      "Binary number",
      "Cluster analysis",
      "Computer science",
      "Initialization",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Programming language"
    ],
    "authors": [
      {
        "surname": "Houfar",
        "given_name": "Khamis"
      },
      {
        "surname": "Samai",
        "given_name": "Djamel"
      },
      {
        "surname": "Dornaika",
        "given_name": "Fadi"
      },
      {
        "surname": "Benlamoudi",
        "given_name": "Azeddine"
      },
      {
        "surname": "Bensid",
        "given_name": "Khaled"
      },
      {
        "surname": "Taleb-Ahmed",
        "given_name": "Abdelmalik"
      }
    ]
  },
  {
    "title": "Diverse image inpainting with disentangled uncertainty",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109243",
    "abstract": "Most existing inpainting methods repair a corrupted image to a single output, which gives people no choice to select the most satisfactory result. However, image inpainting is essentially a multi-modal problem because the inpainted results could have multiple possibilities. To generate both diverse and realistic inpainted results, we propose a diverse image inpainting framework with disentangled uncertainty. We disentangle the uncertainty of the missing region into two aspects: structure and appearance. Correspondingly, we divide the process of diverse image inpainting into two stages: diverse structure inpainting and diverse appearance inpainting. In the first stage, we restore the structure of the missing region, producing diverse complete edge maps. In the second stage, using a complete edge map as the guidance, we fill in diverse appearance information of the missing region. We also design a light-weighted disentangling subnetwork to disentangle structure information and appearance information. Besides, we propose a novel style-based masked residual block to better deal with the uncertainty. Experiments on CelebA-HQ, Paris Street View, and Places2 demonstrate that our method can repair the corrupted image with higher fidelity and diversity than other existing methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322007221",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Computer science",
      "Computer vision",
      "Enhanced Data Rates for GSM Evolution",
      "Geometry",
      "Image (mathematics)",
      "Image processing",
      "Image restoration",
      "Inpainting",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Process (computing)",
      "Residual"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Wentao"
      },
      {
        "surname": "He",
        "given_name": "Lu"
      },
      {
        "surname": "Niu",
        "given_name": "Li"
      },
      {
        "surname": "Zhang",
        "given_name": "Jianfu"
      },
      {
        "surname": "Liu",
        "given_name": "Yue"
      },
      {
        "surname": "Ling",
        "given_name": "Haoyu"
      },
      {
        "surname": "Zhang",
        "given_name": "Liqing"
      }
    ]
  },
  {
    "title": "A High Dynamic Range Imaging Method for Short Exposure Multiview Images",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109344",
    "abstract": "The restoration and enhancement of multiview low dynamic range (MVLDR) images captured in low lighting conditions is a great challenge. The disparity maps are hardly reliable in practical, real-world scenarios and suffers from holes and artifacts due to large baseline and angle deviation among multiple cameras in low lighting conditions. Furthermore, multiple images with some additional information (e.g., ISO/exposure time, etc.) are required for the radiance map and poses the additional challenges of deghosting to encounter motion artifacts. In this paper, we proposed a method to reconstruct multiview high dynamic range (MVHDR) images from MVLDR images without relying on disparity maps. We detect and accurately match the feature points among the involved input views and gather the brightness information from the neighboring viewpoints to optimize an image restoration function based on input exposure gain to finally generate MVHDR images. Our method is very reliable and suitable for a wide baseline among sparse cameras. The proposed method requires only one image per viewpoint without any additional information and outperforms others.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323000456",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Brightness",
      "Composite material",
      "Computer science",
      "Computer vision",
      "Dynamic range",
      "Feature (linguistics)",
      "Geography",
      "High dynamic range",
      "High-dynamic-range imaging",
      "Linguistics",
      "Materials science",
      "Optics",
      "Philosophy",
      "Physics",
      "Radiance",
      "Range (aeronautics)",
      "Remote sensing",
      "Viewpoints",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Khan",
        "given_name": "Rizwan"
      },
      {
        "surname": "Yang",
        "given_name": "You"
      },
      {
        "surname": "Wu",
        "given_name": "Kejun"
      },
      {
        "surname": "Mehmood",
        "given_name": "Atif"
      },
      {
        "surname": "Qaisar",
        "given_name": "Zahid Hussain"
      },
      {
        "surname": "Zheng",
        "given_name": "Zhonglong"
      }
    ]
  },
  {
    "title": "Recurrent wavelet structure-preserving residual network for single image deraining",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109294",
    "abstract": "The combination of deep learning and image prior has been widely used in single image deraining since 2017. Recent studies have demonstrated an excellent deraining effect on the high-frequency part of rain images, but less attention was paid to the low-frequency part of rain images. The rain streaks remain in the low-frequency part of rain images, thus limiting the deraining effect. Since the rain streaks in rain images are often mixed with object edges and background scenes, it is challenging to separate rain from them by directly learning the deraining function in the image domain. To solve these problems, we propose a novel Recurrent Wavelet Structure-preserving Residual Network (RWSRNet), which mainly preserves and introduces the low-frequency sub-images of each level into the low-frequency rain removal sub-networks that are greatly different from the state-of-the-art approaches introducing wavelet transform. In addition, we also share the low-frequency structure information to the high-frequency sub-networks through block connection, which further enriches the detailed information, facilitates convergence, and strengthens the ability of our network to remove rain streaks in high frequency. Finally, we fuse the derained low-frequency sub-images of each level through the proposed image weighted blending module and finally reconstruct the low- and high-frequency sub-images into clean images through inverse wavelet transform recursively. The experimental results indicate that the proposed method achieves an excellent deraining effect on both low- and high-frequency parts of rain images and has better performance in low-frequency preservation and high-frequency enhancement in comparison with the state-of-the-art approaches on synthetic and real image datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322007737",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Computer science",
      "Computer vision",
      "Convergence (economics)",
      "Economic growth",
      "Economics",
      "Electrical engineering",
      "Engineering",
      "Frequency domain",
      "Fuse (electrical)",
      "Geometry",
      "Image (mathematics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Residual",
      "Wavelet"
    ],
    "authors": [
      {
        "surname": "Hsu",
        "given_name": "Wei-Yen"
      },
      {
        "surname": "Chang",
        "given_name": "Wei-Chi"
      }
    ]
  },
  {
    "title": "A multi-strategy contrastive learning framework for weakly supervised semantic segmentation",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109298",
    "abstract": "Weakly supervised semantic segmentation (WSSS) has gained significant popularity as it relies only on weak labels such as image level annotations rather than the pixel level annotations required by supervised semantic segmentation (SSS) methods. Despite drastically reduced annotation costs, typical feature representations learned from WSSS are only representative of some salient parts of objects and less reliable compared to SSS due to the weak guidance during training. In this paper, we propose a novel Multi-Strategy Contrastive Learning (MuSCLe) framework to obtain enhanced feature representations and improve WSSS performance by exploiting similarity and dissimilarity of contrastive sample pairs at image, region, pixel and object boundary levels. Extensive experiments demonstrate the effectiveness of our method and show that MuSCLe outperforms current state-of-the-art methods on the widely used PASCAL VOC 2012 dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322007774",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Feature (linguistics)",
      "Linguistics",
      "Machine learning",
      "Natural language processing",
      "Pascal (unit)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pixel",
      "Programming language",
      "Salient",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Yuan",
        "given_name": "Kunhao"
      },
      {
        "surname": "Schaefer",
        "given_name": "Gerald"
      },
      {
        "surname": "Lai",
        "given_name": "Yu-Kun"
      },
      {
        "surname": "Wang",
        "given_name": "Yifan"
      },
      {
        "surname": "Liu",
        "given_name": "Xiyao"
      },
      {
        "surname": "Guan",
        "given_name": "Lin"
      },
      {
        "surname": "Fang",
        "given_name": "Hui"
      }
    ]
  },
  {
    "title": "Hypergraph based semi-supervised symmetric nonnegative matrix factorization for image clustering",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109274",
    "abstract": "Semi-supervised symmetric nonnegative matrix factorization (SNMF) has been shown to be a significant method for both linear and nonlinear data clustering applications. Nevertheless, existing SNMF-based methods only adopt a simple graph to construct the similarity matrix, and cannot fully use the limited supervised information for the construction of the similarity matrix. To overcome the drawbacks of previous SNMF-based methods, a new semi-supervised SNMF-based method called hypergraph based semi-supervised SNMF (HSSNMF), is proposed in this paper for image clustering. Specifically, HSSNMF adopts a predefined hypergraph to build a similarity matrix for capturing the high-order relationships of samples. By exploiting a new hypergraph based pairwise constraints propagation (HPCP) algorithm, HSSNMF propagates the pairwise constraints of the limited data points to the entire data points, which can make full use of the limited supervised information and construct a more informative similarity matrix. Using the multiplicative updating algorithm, a discriminative assignment matrix can then be obtained by solving the optimization problem of HSSNMF. Moreover, analyses of the convergence, supervisory information, and computational complexity of HSSNMF are presented. Finally, extensive clustering experiments have been conducted on six real-world image datasets, and the experimental results have demonstrated the superiority of HSSNMF while compared with several state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322007531",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Combinatorics",
      "Composite material",
      "Computer science",
      "Discriminative model",
      "Eigenvalues and eigenvectors",
      "Hypergraph",
      "Image (mathematics)",
      "Materials science",
      "Mathematics",
      "Matrix (chemical analysis)",
      "Matrix decomposition",
      "Non-negative matrix factorization",
      "Pairwise comparison",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Yin",
        "given_name": "Jingxing"
      },
      {
        "surname": "Peng",
        "given_name": "Siyuan"
      },
      {
        "surname": "Yang",
        "given_name": "Zhijing"
      },
      {
        "surname": "Chen",
        "given_name": "Badong"
      },
      {
        "surname": "Lin",
        "given_name": "Zhiping"
      }
    ]
  },
  {
    "title": "Low-rank tensor recovery via non-convex regularization, structured factorization and spatio-temporal characteristics",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109343",
    "abstract": "Recently, the convex low-rank 3rd-order tensor recovery has attracted considerable attention. However, there are some limitations to the convex relaxation approach, which may yield biased estimators. To overcome this disadvantage, we develop a novel non-convex tensor pseudo-norm to replace the weighted sum of the tensor nuclear norm as a tighter rank approximation. Then in tensor robust principle component analysis, we introduce the noise analysis to separate the spare foreground from the dynamic background more accurately. Furthermore, by introducing a spatio-temporal matrix, we can make better use of the inherent spatio-temporal characteristics of the low-rank static background and sparse foreground. Finally, we introduce an incoherent term to constrain the sparse foreground and the dynamic background to improve the separability. Some preliminary numerical examples of color image, video, and face image data sets are presented to illustrate the efficiency of our proposed methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323000444",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Eigenvalues and eigenvectors",
      "Estimator",
      "Gaussian",
      "Geometry",
      "Low-rank approximation",
      "Mathematical optimization",
      "Mathematics",
      "Matrix decomposition",
      "Matrix norm",
      "Physics",
      "Pure mathematics",
      "Quantum mechanics",
      "Rank (graph theory)",
      "Regular polygon",
      "Regularization (linguistics)",
      "Sparse matrix",
      "Statistics",
      "Tensor (intrinsic definition)"
    ],
    "authors": [
      {
        "surname": "Yu",
        "given_name": "Quan"
      },
      {
        "surname": "Yang",
        "given_name": "Ming"
      }
    ]
  },
  {
    "title": "Rectified Euler k -means and beyond",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109283",
    "abstract": "Euler k -means (EulerK) first maps data onto the unit hyper-sphere surface of equi-dimensional space via a complex mapping which induces the robust Euler kernel and next employs the popular k -means. Consequently, besides enjoying the virtues of k -means such as simplicity and scalability to large data sets, EulerK is also robust to noises and outliers. Although so, the centroids captured by EulerK deviate from the unit hyper-sphere surface and thus in strict distributional sense, actually are outliers. This weird phenomenon also occurs in some generic kernel clustering methods. Intuitively, using such outlier-like centroids should not be quite reasonable but it is still seldom attended. To eliminate the deviation, we propose two Rectified Euler k -means methods, i.e., REK1 and REK2, which retain the merits of EulerK while acquiring real centroids residing on the mapped space to better characterize the data structures. Specifically, REK1 rectifies EulerK by imposing the constraint on the centroids while REK2 views each centroid as the mapped image from a pre-image in the original space and optimizes these pre-images in Euler kernel induced space. Undoubtedly, our proposed REKs can methodologically be extended to solve problems of such a category. Finally, the experiments validate the effectiveness of REK1 and REK2.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322007622",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Centroid",
      "Cluster analysis",
      "Combinatorics",
      "Computer science",
      "Euler's formula",
      "Kernel (algebra)",
      "Mathematical analysis",
      "Mathematics",
      "Outlier",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Lin",
        "given_name": "Yunxia"
      },
      {
        "surname": "Chen",
        "given_name": "Songcan"
      }
    ]
  },
  {
    "title": "Task-balanced distillation for object detection",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109320",
    "abstract": "Mainstream object detectors are commonly constituted of two sub-tasks, including classification and regression tasks, implemented by two parallel heads. This classic design paradigm inevitably leads to inconsistent spatial distributions between classification score and localization quality (IOU). Therefore, this paper alleviates this misalignment in the view of knowledge distillation. First, we observe that the massive teacher achieves a higher proportion of harmonious predictions than the lightweight student. Based on this intriguing observation, a novel Harmony Score (HS) is devised to estimate the alignment of classification and regression qualities. HS models the relationship between two sub-tasks and is seen as prior knowledge to promote harmonious predictions for the student. Second, this spatial misalignment will result in inharmonious region selection when distilling features. To alleviate this problem, a novel Task-decoupled Feature Distillation (TFD) is proposed by flexibly balancing the contributions of classification and regression tasks. Eventually, HD and TFD constitute the proposed method, named Task-Balanced Distillation (TBD). Extensive experiments demonstrate the considerable potential and generalization of the proposed method. Notably, when equipped with TBD, the performances of RetinaNet-R18/RetinaNet-R50/Faster-RCNN-R18 can be boosted from 33.2/37.4/34.5 to 37.3/41.2/37.7, outperforming the recent KD-based methods like FRS, FGD, and MGD.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323000213",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Classifier (UML)",
      "Computer science",
      "Distillation",
      "Economics",
      "Generalization",
      "Machine learning",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Object (grammar)",
      "Object detection",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Regression",
      "Statistics",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Tang",
        "given_name": "Ruining"
      },
      {
        "surname": "Liu",
        "given_name": "Zhenyu"
      },
      {
        "surname": "Li",
        "given_name": "Yangguang"
      },
      {
        "surname": "Song",
        "given_name": "Yiguo"
      },
      {
        "surname": "Liu",
        "given_name": "Hui"
      },
      {
        "surname": "Wang",
        "given_name": "Qide"
      },
      {
        "surname": "Shao",
        "given_name": "Jing"
      },
      {
        "surname": "Duan",
        "given_name": "Guifang"
      },
      {
        "surname": "Tan",
        "given_name": "Jianrong"
      }
    ]
  },
  {
    "title": "A posteriori control densities: Imitation learning from partial observations",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.04.001",
    "abstract": "This paper treats a special case of the Imitation from Observations (IfO) problem. IfO is a generalisation of Imitation Learning from state-only demonstrations. Our treatment of IfO considers the case of feature-only demonstrations. This means that the full state is inaccessible for inference, and imitation must occur on the basis of a limited set of features. We refer to this setting as Imitation from Partial Observations (IfPO). This scenario has the advantage of allowing to address a wider variety of demonstrations, as well as solving the problem of heteromorphic student and teacher. We set out for policy learning methods that extract an executable state-feedback policy, directly from those features, which in the literature is known as Behavioural Cloning. In this theoretical work, we formalize the rational inference model of the student decision maker, devoted to imitation, as a controlled Hidden Markov Model. The IfPO problem is then reformulated as a Maximum Likelihood Estimation problem and treated using Expectation-Maximization. We name the resulting fixed point iterations A Posteriori Control Densities. We compare the presented approach to existing methods in the field and identify potential directions for further development, such as an extension to unknown transition and emission models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523001022",
    "keywords": [
      "A priori and a posteriori",
      "Artificial intelligence",
      "Computer science",
      "Continuation",
      "Epistemology",
      "Executable",
      "Feature (linguistics)",
      "Field (mathematics)",
      "Imitation",
      "Inference",
      "Linguistics",
      "Machine learning",
      "Markov chain",
      "Markov decision process",
      "Markov process",
      "Mathematical optimization",
      "Mathematics",
      "Maximization",
      "Maximum a posteriori estimation",
      "Maximum likelihood",
      "Operating system",
      "Philosophy",
      "Programming language",
      "Psychology",
      "Pure mathematics",
      "Set (abstract data type)",
      "Social psychology",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Lefebvre",
        "given_name": "Tom"
      },
      {
        "surname": "Crevecoeur",
        "given_name": "Guillaume"
      }
    ]
  },
  {
    "title": "Hierarchical nearest neighbor descent, in-tree, and clustering",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109300",
    "abstract": "Recently, we have proposed a physically-inspired graph-theoretical method, called the Nearest Descent (ND), which is capable of organizing a dataset into an in-tree graph structure. Due to some beautiful and effective features, the constructed in-tree proves well-suited for data clustering. Although there exist some undesired edges (i.e., the inter-cluster edges) in this in-tree, those edges are usually very distinguishable, in sharp contrast to the cases in the famous Minimal Spanning Tree (MST). Here, we propose another graph-theoretical method, called the Hierarchical Nearest Neighbor Descent (HNND). Like ND, HNND also organizes a dataset into an in-tree, but in a more efficient way. Consequently, HNND-based clustering (HNND-C) is more efficient than ND-based clustering (ND-C) as well. This is well proved by the experimental results on five high-dimensional and large-size mass cytometry datasets. The experimental results also show that HNND-C achieves overall better performance than some state-of-the-art clustering methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323000018",
    "keywords": [
      "Artificial intelligence",
      "CURE data clustering algorithm",
      "Canopy clustering algorithm",
      "Cluster analysis",
      "Combinatorics",
      "Computer science",
      "Correlation clustering",
      "Graph",
      "Hierarchical clustering",
      "Mathematics",
      "Nearest-neighbor chain algorithm",
      "Pattern recognition (psychology)",
      "Single-linkage clustering",
      "Theoretical computer science",
      "Tree (set theory)",
      "k-nearest neighbors algorithm"
    ],
    "authors": [
      {
        "surname": "Qiu",
        "given_name": "Teng"
      },
      {
        "surname": "Li",
        "given_name": "Yongjie"
      }
    ]
  },
  {
    "title": "A Comprehensive Survey of Image Augmentation Techniques for Deep Learning",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109347",
    "abstract": "Although deep learning has achieved satisfactory performance in computer vision, a large volume of images is required. However, collecting images is often expensive and challenging. Many image augmentation algorithms have been proposed to alleviate this issue. Understanding existing algorithms is, therefore, essential for finding suitable and developing novel methods for a given task. In this study, we perform a comprehensive survey of image augmentation for deep learning using a novel informative taxonomy. To examine the basic objective of image augmentation, we introduce challenges in computer vision tasks and vicinity distribution. The algorithms are then classified among three categories: model-free, model-based, and optimizing policy-based. The model-free category employs the methods from image processing, whereas the model-based approach leverages image generation models to synthesize images. In contrast, the optimizing policy-based approach aims to find an optimal combination of operations. Based on this analysis, we believe that our survey enhances the understanding necessary for choosing suitable methods and designing novel algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323000481",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Deep learning",
      "Economics",
      "Image (mathematics)",
      "Image processing",
      "Machine learning",
      "Management",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Mingle"
      },
      {
        "surname": "Yoon",
        "given_name": "Sook"
      },
      {
        "surname": "Fuentes",
        "given_name": "Alvaro"
      },
      {
        "surname": "Park",
        "given_name": "Dong Sun"
      }
    ]
  },
  {
    "title": "Geometric-aware dense matching network for 6D pose estimation of objects from RGB-D images",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109293",
    "abstract": "6D pose estimation for certain targets from RGB-D images is a fundamental problem in computer vision. Current methods emphasize learning the overall expression of the targets, which leads to poor performance under occlusion and truncation conditions. In this paper, we propose using a geometric-aware dense matching network to obtain visible dense correspondences between a RGB-D image and 3D model to address difficult predictions from unseen keypoints. Two geometrical structures are considered for dense matching. (1) The neighbor area of the correspondences is treated as suboptimal matches in addition to the correspondence to reduce the influence of the error caused by ground truth calibration. (2) The distance consistency of the correspondences is leveraged to eliminate the ambiguity from the symmetrical objects. Experiments on LM-O dataset (77.1% ADD(S)-0.1d) and YCB-V dataset (97.6% ADD(S)) show the effectiveness and advantages of our proposed method. 1 1 The source code will soon be available at https://github.com/Ray0089/geometric-aware-dense-matching.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322007725",
    "keywords": [
      "Ambiguity",
      "Artificial intelligence",
      "Calibration",
      "Computer science",
      "Computer vision",
      "Consistency (knowledge bases)",
      "Ground truth",
      "Image (mathematics)",
      "Matching (statistics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pose",
      "Programming language",
      "RGB color model",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Chenrui"
      },
      {
        "surname": "Chen",
        "given_name": "Long"
      },
      {
        "surname": "Wang",
        "given_name": "Shenglong"
      },
      {
        "surname": "Yang",
        "given_name": "Han"
      },
      {
        "surname": "Jiang",
        "given_name": "Junjie"
      }
    ]
  },
  {
    "title": "Improving visual-semantic embeddings by learning semantically-enhanced hard negatives for cross-modal information retrieval",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109272",
    "abstract": "Visual Semantic Embedding (VSE) networks aim to extract the semantics of images and their descriptions and embed them into the same latent space for cross-modal information retrieval. Most existing VSE networks are trained by adopting a hard negatives loss function which learns an objective margin between the similarity of relevant and irrelevant image–description embedding pairs. However, the objective margin in the hard negatives loss function is set as a fixed hyperparameter that ignores the semantic differences of the irrelevant image–description pairs. To address the challenge of measuring the optimal similarities between image–description pairs before obtaining the trained VSE networks, this paper presents a novel approach that comprises two main parts: (1) finds the underlying semantics of image descriptions; and (2) proposes a novel semantically-enhanced hard negatives loss function, where the learning objective is dynamically determined based on the optimal similarity scores between irrelevant image–description pairs. Extensive experiments were carried out by integrating the proposed methods into five state-of-the-art VSE networks that were applied to three benchmark datasets for cross-modal information retrieval tasks. The results revealed that the proposed methods achieved the best performance and can also be adopted by existing and future VSE networks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322007518",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Chemistry",
      "Computer science",
      "Data mining",
      "Embedding",
      "Evolutionary biology",
      "Function (biology)",
      "Geodesy",
      "Geography",
      "Hyperparameter",
      "Image (mathematics)",
      "Image retrieval",
      "Information retrieval",
      "Machine learning",
      "Margin (machine learning)",
      "Modal",
      "Pattern recognition (psychology)",
      "Polymer chemistry",
      "Programming language",
      "Semantics (computer science)",
      "Set (abstract data type)",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Gong",
        "given_name": "Yan"
      },
      {
        "surname": "Cosma",
        "given_name": "Georgina"
      }
    ]
  },
  {
    "title": "Hybrid feature enhancement network for few-shot semantic segmentation",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109291",
    "abstract": "Although few-shot semantic segmentation methods have been widely studied in computer vision field, it still has room for improvement. In this work, we propose to enrich the feature representation with texture information and assign adaptive weights to losses. Specially, we incorporate the texture information obtained by texture enhance module with layer's features on ResNet, and then get a series of hybrid features. The incorporation of texture information enhances the similarity calculation to make the support set guidance more effective. Besides, the proposed adaptive loss makes the network optimize in a better direction. The experiments testify that the proposed method achieves the better results than that of previous methods on few-shot segmentation dataset such as PASCAL-5i, COCO-20i and FSS-1000.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322007701",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Image (mathematics)",
      "Law",
      "Linguistics",
      "Organic chemistry",
      "Pascal (unit)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Programming language",
      "Representation (politics)",
      "Segmentation",
      "Shot (pellet)",
      "Texture (cosmology)"
    ],
    "authors": [
      {
        "surname": "Min",
        "given_name": "Hai"
      },
      {
        "surname": "Zhang",
        "given_name": "Yemao"
      },
      {
        "surname": "Zhao",
        "given_name": "Yang"
      },
      {
        "surname": "Jia",
        "given_name": "Wei"
      },
      {
        "surname": "Lei",
        "given_name": "Yingke"
      },
      {
        "surname": "Fan",
        "given_name": "Chunxiao"
      }
    ]
  },
  {
    "title": "A lightweight network for smoke semantic segmentation",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109289",
    "abstract": "To obtain real-time performance on computation limited devices, we propose a lightweight network for smoke segmentation. To enhance the ability of feature encoding, we first propose an Attention Encoding Module (AEM) by designing a Channel Split and Shuffle Attention Module (CSSAM), which can extract powerful features and reduce computations simultaneously. CSSAM adopts Channel split and shuffle to greatly reduce learnable parameters for improving computation speed, and uses attention mechanism to focus on salient objects to enhance the effectiveness of features. In addition, AEM repeatedly stacks CSSAM in different encoding stages to achieve scale invariance. For the middle-level features of encoding stages, we propose a Spatial Enhancement Module (SEM) to boost the representation ability of spatial details. SEM concatenates feature maps produced by average and maximum pooling to achieve dominant and global responses, which are then weighted by the activated output of global average pooling to generate attention features. In the highest level of encoding stages, we present a Channel Attention Module (CAM) to explicitly model interdependency between channels. By reshaping 2D features into 1D features, we use element-wise matrix multiplications to reduce computation complexity for extracting channel-related information. Finally, we design a Feature Fusion Module (FFM) and a Global Coefficient Path (GCP) to fuse the outputs of SEM and CAM in an attention way for further improving robustness of final features. Experiments show that our method is significantly superior to existing state-of-the-art algorithms in smoke datasets, and also obtains excellent results in both synthetic and real smoke datasets. However, our method has less than 1 M network parameters.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322007683",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Channel (broadcasting)",
      "Chemistry",
      "Computation",
      "Computer network",
      "Computer science",
      "Computer security",
      "Electrical engineering",
      "Encoding (memory)",
      "Engineering",
      "Feature (linguistics)",
      "Fuse (electrical)",
      "Gene",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pooling",
      "Robustness (evolution)",
      "Segmentation",
      "Subnetwork"
    ],
    "authors": [
      {
        "surname": "Yuan",
        "given_name": "Feiniu"
      },
      {
        "surname": "Li",
        "given_name": "Kang"
      },
      {
        "surname": "Wang",
        "given_name": "Chunmei"
      },
      {
        "surname": "Fang",
        "given_name": "Zhijun"
      }
    ]
  },
  {
    "title": "2D Image head pose estimation via latent space regression under occlusion settings",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109288",
    "abstract": "Head orientation is a challenging Computer Vision problem that has been extensively researched having a wide variety of applications. However, current state-of-the-art systems still underperform in the presence of occlusions and are unreliable for many task applications in such scenarios. This work proposes a novel deep learning approach for the problem of head pose estimation under occlusions. The strategy is based on latent space regression as a fundamental key to better structure the problem for occluded scenarios. Our model surpasses several state-of-the-art methodologies for occluded HPE, and achieves similar accuracy for non-occluded scenarios. We demonstrate the usefulness of the proposed approach with: (i) two synthetically occluded versions of the BIWI and AFLW2000 datasets, (ii) real-life occlusions of the Pandora dataset, and (iii) a real-life application to human-robot interaction scenarios where face occlusions often occur. Specifically, the autonomous feeding from a robotic arm.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322007671",
    "keywords": [
      "3D pose estimation",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Engineering",
      "Face (sociological concept)",
      "Geology",
      "Geometry",
      "Geomorphology",
      "Head (geology)",
      "Image (mathematics)",
      "Machine learning",
      "Mathematics",
      "Orientation (vector space)",
      "Pattern recognition (psychology)",
      "Pose",
      "Regression",
      "Robot",
      "Social science",
      "Sociology",
      "Statistics",
      "Systems engineering",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Celestino",
        "given_name": "José"
      },
      {
        "surname": "Marques",
        "given_name": "Manuel"
      },
      {
        "surname": "Nascimento",
        "given_name": "Jacinto C."
      },
      {
        "surname": "Costeira",
        "given_name": "João Paulo"
      }
    ]
  },
  {
    "title": "Overcoming weaknesses of density peak clustering using a data-dependent similarity measure",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109287",
    "abstract": "Density Peak Clustering (DPC) is a popular state-of-the-art clustering algorithm, which requires pairwise (dis)similarity of data objects to detect arbitrary shaped clusters. While it is shown to perform well for many applications, DPC remains: (i) not robust for datasets with clusters having different densities, and (ii) sensitive to the change in the units/scales used to represent data. These drawbacks are mainly due to the use of the data-independent similarity measure based on the Euclidean distance. In this paper, we address these issues by proposing an effective data-dependent similarity measure based on Probability Mass, which we call MP-Similarity, and by incorporating it in DPC to create MP-DPC, a data-dependent variant of DPC. We evaluate and compare MP-DPC against diverse baselines using several clustering metrics and datasets. Our experiments demonstrate that: (a) MP-DPC produces better clustering results than DPC using the Euclidean distance and existing data-dependent similarity measures; (b) MP-Similarity coupled with Shared-Nearest-Neighbor-based density metric in DPC further enhances the quality of clustering results; and (c) unlike DPC with existing data-independent and data-dependent similarity measures, MP-DPC is robust to the change in the units/scales used to represent data. Our findings suggest that MP-Similarity provides a more viable solution for DPC in datasets with unknown distribution or units/scales of features, which is often the case in many real-world applications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032200766X",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Distance measures",
      "Economics",
      "Euclidean distance",
      "Image (mathematics)",
      "Mathematics",
      "Measure (data warehouse)",
      "Metric (unit)",
      "Operations management",
      "Pairwise comparison",
      "Pattern recognition (psychology)",
      "Similarity (geometry)",
      "Similarity measure",
      "k-nearest neighbors algorithm"
    ],
    "authors": [
      {
        "surname": "Rasool",
        "given_name": "Zafaryab"
      },
      {
        "surname": "Aryal",
        "given_name": "Sunil"
      },
      {
        "surname": "Bouadjenek",
        "given_name": "Mohamed Reda"
      },
      {
        "surname": "Dazeley",
        "given_name": "Richard"
      }
    ]
  },
  {
    "title": "The Performance Index of Convolutional Neural Network-Based Classifiers in Class Imbalance Problem",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109284",
    "abstract": "Class imbalance is a common problem in many classification domains. This paper provides an evaluation index and one algorithm for this problem based on binary classification. The Model Performance Index (MPI) is proposed for assessing classifier performance as a new evaluation metric, considering class imbalance impacts. Based on MPI, we investigate algorithms to estimate ideal classifier performance with a fair distribution (1:1), referred to as the Ideal Model Performance Algorithm. Experimentally, compared with traditional metrics, MPI is more sensitive. Specifically, it can detect all types of changes in classifier performances, while others might remain at the same levels. Moreover, for the estimation of classifier performances, the algorithm reaches small differences between predictions and the values observed. Generally, for ideal performances, it achieved error rates of 0.060% - 1.3% for rare class in four experiments, showing a practical value on estimation and representation on the classifier performances.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322007634",
    "keywords": [
      "Algorithm",
      "Arithmetic",
      "Artificial intelligence",
      "Artificial neural network",
      "Binary classification",
      "Binary number",
      "Classifier (UML)",
      "Computer science",
      "Convolutional neural network",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Yanchen"
      },
      {
        "surname": "Lai",
        "given_name": "King Wai Chiu"
      }
    ]
  },
  {
    "title": "Multi-order similarity learning for multi-view spectral clustering",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109264",
    "abstract": "This paper explores the problem of multi-view spectral clustering (MVSC) based on multi-order similarity learning. Unlike the existing methods that focus on direct similarity of pairwise data points without considering the hidden multi-order similarity among different data points, a novel multi-order similarity learning model for MVSC (MOSL) is proposed. Specifically, the first-order similarity (FOS) and second-order similarity (SOS) are learned to excavate the local structure relation and adjacent structure relation of pairwise data points. Afterwards, the third-order similarity (TOS) based on low-rank tensor is learned to excavate the view-specific information and consensus information from multiple views. Moreover, a trace constraint on each affinity graph from multiple views is learned to ensure the strict block diagonal structure of each affinity graph. Extensive experiments on six commonly benchmark datasets show that the proposed method outperforms state-of-the-art methods in most scenarios and is capable of revealing a reliable affinity graph structure concealed in different data points.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322007439",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Graph",
      "Image (mathematics)",
      "Mathematics",
      "Pairwise comparison",
      "Pattern recognition (psychology)",
      "Similarity (geometry)",
      "Spectral clustering",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Mei",
        "given_name": "Yanying"
      },
      {
        "surname": "Ren",
        "given_name": "Zhenwen"
      },
      {
        "surname": "Wu",
        "given_name": "Bin"
      },
      {
        "surname": "Yang",
        "given_name": "Tao"
      },
      {
        "surname": "Shao",
        "given_name": "Yanhua"
      }
    ]
  },
  {
    "title": "Disentangled convolution for optimizing receptive field",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.03.029",
    "abstract": "Convolutional neural network (CNN) is one of the primary techniques for high-performance image recognition. The convolution operation with small-sized filters is a key ingredient of CNN and the receptive field of the whole CNN is enlarged by stacking lots of convolution layers. The convolution layer, however, is problematic in terms of the receptive field. It provides fixed small receptive field due to the fixed filter size in convolution, which requires us to manually control it in advance. Besides, the larger-sized convolution filters significantly increase computation cost. Thus, in this study, we propose a method to adaptively tune the receptive field of the convolution operation in an end-to-end manner as well as to enlarge the receptive field in a low computation cost. Based on the biological studies and scale-space theory, we can disentangle convolution operation into Gaussian envelope filtering for smoothing and derivative-related filtering, both of which are heterogeneously parameterized. Those two types of filters are jointly optimized in the end-to-end CNN training and the receptive field of the convolution is adequately optimized via learning the Gaussian envelope with a low extra computation cost. The experimental results on image classification tasks demonstrate that the proposed method effectively enlarges receptive field to improve performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523001009",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computation",
      "Computer science",
      "Computer vision",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Envelope (radar)",
      "Field (mathematics)",
      "Filter (signal processing)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pure mathematics",
      "Radar",
      "Receptive field",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Kobayashi",
        "given_name": "Takumi"
      }
    ]
  },
  {
    "title": "Granularity-aware distillation and structure modeling region proposal network for fine-grained image classification",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109305",
    "abstract": "Fine-grained visual classification (FGVC) aims to identify objects belonging to multiple sub-categories of the same super-category. The key to solving fine-grained classification problems is to learn discriminative visual feature representation with only subtle differences. Although previous work based on refined feature learning has made great progress, however, high-level semantic features often lack key information for fine-grained visual object nuances. How to efficiently integrate semantic information of different granularities from classification networks is a critical. In this paper, we propose Granularity-aware Distillation and Structure Modeling region Proposal Network(GDSMP-Net). Our solution integrates multi-granularity hierarchical information through a multi-granularity fusion learning strategy to enhance feature representation. In view of the inherent challenges of large intra-class differences in FGVC, a cross-layer self-distillation regularization is proposed to to strengthen the connection between high-level semantics and low-level semantics for robust multi-granularity feature learning. On this basis, we use a weakly supervised method to generate local branches, and the collaborative learning of discriminative semantics and structural semantics based on local regions, facilitating model to perceive contextual information to capture structural interactions between local semantics. Comprehensive experiments show that our method achieves state-of-the-art performance on four widely-used challenging datasets.(CUB-200-2011, Stanford Cars, FGVC-Aircraft and NA-birds).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323000067",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Discriminative model",
      "Feature (linguistics)",
      "Feature extraction",
      "Feature learning",
      "Granularity",
      "Key (lock)",
      "Law",
      "Linguistics",
      "Machine learning",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Programming language",
      "Representation (politics)",
      "Semantics (computer science)",
      "Visualization"
    ],
    "authors": [
      {
        "surname": "Ke",
        "given_name": "Xiao"
      },
      {
        "surname": "Cai",
        "given_name": "Yuhang"
      },
      {
        "surname": "Chen",
        "given_name": "Baitao"
      },
      {
        "surname": "Liu",
        "given_name": "Hao"
      },
      {
        "surname": "Guo",
        "given_name": "Wenzhong"
      }
    ]
  },
  {
    "title": "Topology-preserving transfer learning for weakly-supervised anomaly detection and segmentation",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.04.019",
    "abstract": "Models pre-trained on the ImageNet dataset are introduced to be exploited for knowledge transfer in numerous downstream computer vision tasks, including the weakly-supervised anomaly detection and segmentation area. Specifically, in anomaly segmentation, the former study shows that representing images with feature maps extracted by pre-trained models significantly improves over previous techniques. This kind of representation method requires both high-quality and task-specific features, but feature extractors obtained from ImageNet directly are very general. One intuition for obtaining stronger features is by transferring a pre-trained model to the target dataset. However, in this paper, we show that under weakly-supervised settings, naïve fine-tune techniques that typically work for supervised learning can lead to catastrophic feature space collapse and reduce performance greatly. Thus, we propose to apply a topology-preserving constraint during transferring. Our method preserves the topology graph to keep the feature space from collapsing under weakly-supervised settings. And then we combine the transferred model with a simple anomaly detection and segmentation baseline for performance evaluation. The experiments show that our method achieves competitive accuracy on several benchmarks meanwhile setting a new state-of-the-art for anomaly detection on CIFAR100/10 and BTAD datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523001265",
    "keywords": [
      "Anomaly detection",
      "Artificial intelligence",
      "Artificial neural network",
      "Combinatorics",
      "Computer science",
      "Epistemology",
      "Feature (linguistics)",
      "Feature learning",
      "Feature vector",
      "Graph",
      "Intuition",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Segmentation",
      "Supervised learning",
      "Theoretical computer science",
      "Topology (electrical circuits)"
    ],
    "authors": [
      {
        "surname": "Wei",
        "given_name": "Shenxing"
      },
      {
        "surname": "Wei",
        "given_name": "Xing"
      },
      {
        "surname": "Kurniawan",
        "given_name": "Muhammad Rifki"
      },
      {
        "surname": "Ma",
        "given_name": "Zhiheng"
      },
      {
        "surname": "Gong",
        "given_name": "Yihong"
      }
    ]
  },
  {
    "title": "Capturing the few-shot class distribution: Transductive distribution optimization",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109371",
    "abstract": "Few-shot learning is challenging since only a few labeled samples are available for training a learning model. To alleviate the data limitation problem in few-shot learning, several works try to generate samples or features by learning a model or distribution. But complex models and biased estimation of class distribution hamper their interpretability and generalization ability, respectively. In this work, we propose a generation based Transductive Distribution Optimization (TDO) method, which introduces neither extra parameters nor complex models. We use a few labeled samples and some high-confident unlabeled samples of the target set to capture the distributions of the few-shot classes, and then generate sufficient samples from them to augment the labeled inputs. Our method can work with most pre-trained feature extractors and outperforms state-of-the-art methods with a simple linear classifier. The visualization of the generated samples shows that our method can capture an accurate distribution even though the few labeled samples deviate from the ground-truth distribution.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323000729",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Classifier (UML)",
      "Computer science",
      "Distribution (mathematics)",
      "Feature (linguistics)",
      "Ground truth",
      "Interpretability",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Shot (pellet)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Xinyue"
      },
      {
        "surname": "Liu",
        "given_name": "Ligang"
      },
      {
        "surname": "Liu",
        "given_name": "Han"
      },
      {
        "surname": "Zhang",
        "given_name": "Xiaotong"
      }
    ]
  },
  {
    "title": "Transformer for multiple object tracking: Exploring locality to vision",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.04.016",
    "abstract": "Multi-object tracking (MOT) is a critical task in various domains, such as traffic analysis, surveillance, and autonomous vehicles. The joint-detection-and-tracking paradigm has been extensively researched, which is faster and more convenient for training and deploying over the classic tracking-by-detection paradigm while achieving state-of-the-art performance. This paper explores the possibilities of enhancing the MOT system by leveraging the prevailing convolutional neural network (CNN) and a novel vision transformer technique Locality. There are several deficiencies in the transformer adopted for computer vision tasks. While the transformers are good at modeling global information for a long embedding, the locality mechanism, which learns the local features, is missing. This could lead to negligence of small objects, which may cause security issues. We combine the TransTrack MOT system with the locality mechanism inspired by LocalViT and find that the locality-enhanced system outperforms the baseline TransTrack by 5.3% MOTA on the MOT17 dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523001241",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Electrical engineering",
      "Embedding",
      "Engineering",
      "Linguistics",
      "Locality",
      "Machine learning",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Transformer",
      "Video tracking",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Shan"
      },
      {
        "surname": "Hadachi",
        "given_name": "Amnir"
      },
      {
        "surname": "Lu",
        "given_name": "Chaoru"
      },
      {
        "surname": "Vivet",
        "given_name": "Damien"
      }
    ]
  },
  {
    "title": "Robust image clustering via context-aware contrastive graph learning",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109340",
    "abstract": "Graph convolution networks (GCN) have recently become popular for image clustering. However, existing GCN-based image clustering techniques focus on learning image neighbourhoods which leads to poor reasoning on the cluster boundaries. To address this challenge, we propose a supervised image clustering approach based on contrastive graph learning (CGL). Our method generates an influential graph view (IGV) and a topological graph view (TGV) for each class to represent its global context from different viewpoints. These generated graph views are used to reason the inter-cluster relationships and intra-cluster boundaries from the local context of each node in a contrastive manner. Our method considers each class as a fully connected graph to explore its characteristics and strategically generate directional graph views. This enhances the transferability of the proposed approach to handle data with a similar structure. We conduct extensive experiments on open datasets such as LFW, CASIA-WebFace, and CIFAR-10 and show that our method outperforms state-of-the-art including deep GRAph Contrastive rEpresentation learning (GRACE), GraphCL, and Graph Contrastive Clustering (GCC).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323000419",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Clustering coefficient",
      "Computer science",
      "Graph",
      "Pattern recognition (psychology)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Fang",
        "given_name": "Uno"
      },
      {
        "surname": "Li",
        "given_name": "Jianxin"
      },
      {
        "surname": "Lu",
        "given_name": "Xuequan"
      },
      {
        "surname": "Mian",
        "given_name": "Ajmal"
      },
      {
        "surname": "Gu",
        "given_name": "Zhaoquan"
      }
    ]
  },
  {
    "title": "NPDN-3D: A 3D neural partial differential network for spatiotemporal prediction",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109425",
    "abstract": "Neural network-based methods have been widely applied to spatiotemporal prediction tasks, such as video prediction and weather forecasting. However, most existing works are designed for prediction in 2D space, and 3D prediction has not been extensively studied. In this paper, we propose to leverage 3D partial differential equations (PDEs) for spatiotemporal prediction in 3D space, and further develop a novel 3D neural partial differential network. This is inspired by that 3D PDEs can model both horizontal and vertical information interactions by various partial derivatives. Moreover, they can also formulate physical knowledge by equations. To integrate 3D PDEs in neural networks, we first develop the theory of approximating 3D partial derivatives by 3D convolutions, and further present an effective strategy to utilize the theory in practice. Then based on the theory and strategy, we propose a novel 3D Neural Partial Differential Network for prediction, named NPDN-3D. Specifically, NPDN-3D consists of two pivotal modules: (1) a neural partial differential module for capturing low-order spatiotemporal dynamics. This module is the key for prediction, where the dynamics are formulated by commonly-used low-order 3D PDEs. (2) A residual module for capturing the remaining non-low-order dynamics. This module performs as an extensible plug-in to enhance the expressiveness of our model. Extensive experiments on two simulated datasets and two real datasets show that our method not only achieves better prediction but also learns the correct PDEs.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323001267",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Leverage (statistics)",
      "Mathematical analysis",
      "Mathematics",
      "Partial derivative",
      "Partial differential equation"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Xu"
      },
      {
        "surname": "Feng",
        "given_name": "Shanshan"
      },
      {
        "surname": "Ye",
        "given_name": "Yunming"
      },
      {
        "surname": "Li",
        "given_name": "Xutao"
      },
      {
        "surname": "Zhang",
        "given_name": "Bowen"
      },
      {
        "surname": "Chen",
        "given_name": "Shidong"
      }
    ]
  },
  {
    "title": "MFSJMI: Multi-label feature selection considering join mutual information and interaction weight",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109378",
    "abstract": "Multi-label feature selection captures a reliable and informative feature subset from high-dimensional multi-label data, which plays an important role in pattern recognition. In conventional information-theoretical based multi-label feature selection methods, the high-order feature relevance between feature and label set is evaluated using low-order mutual information. However, existing methods do not establish the theoretical basis for the low-order approximation. To fill this gap, we first identify two underlying assumptions based on high-order label distribution: Label Independence Assumption (LIA) and Paired-label Independence Assumption (PIA). Second, we systematically analyze the strengths and weaknesses of two assumptions and introduce joint mutual information to satisfy more realistic label distribution. Furthermore, by decomposing joint mutual information, an interaction weight is proposed to consider multiple label correlations. Finally, a new method considering join mutual information and interaction weight is proposed. Comprehensive experiments demonstrate the effectiveness of the proposed method on various evaluation metrics.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323000791",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Conditional mutual information",
      "Data mining",
      "Feature (linguistics)",
      "Feature selection",
      "Independence (probability theory)",
      "Interaction information",
      "Law",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Mutual information",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pointwise mutual information",
      "Political science",
      "Programming language",
      "Relevance (law)",
      "Selection (genetic algorithm)",
      "Set (abstract data type)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Ping"
      },
      {
        "surname": "Liu",
        "given_name": "Guixia"
      },
      {
        "surname": "Song",
        "given_name": "Jiazhi"
      }
    ]
  },
  {
    "title": "Mitigate the classification ambiguity via localization-classification sequence in object detection",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109418",
    "abstract": "In anchor-based detectors, the confidence scores and label-assignment results for the classification task are determined by the unrefined anchors rather than the final-refined boxes, which causes classification ambiguity due to the lack of correlation between the classification and localization tasks. In this paper, we investigate the classification ambiguity thoroughly via extensive experiments, and present the localization-classification sequence detector (LCSDet) that performs localization and classification in order, bridging the gap between them. To achieve this, the refinement-aware (RA) classification branch and RA assignment are proposed in LCSDet. In inference, the RA classification branch rectifies the feature misalignment and directly classifies the refined anchors. During training, the RA assignment tackles the training instability, narrows the location-quality gap and assigns the refined anchors to ground-truth objects. Comprehensive experiments indicate that the LCSDet can effectively mitigate the classification ambiguity and achieve stable improvement across different baselines.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032300119X",
    "keywords": [
      "Ambiguity",
      "Artificial intelligence",
      "Biology",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Engineering",
      "Genetics",
      "Inference",
      "Machine learning",
      "One-class classification",
      "Pattern recognition (psychology)",
      "Programming language",
      "Sequence (biology)",
      "Systems engineering",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Chang"
      },
      {
        "surname": "Xie",
        "given_name": "Shaorong"
      },
      {
        "surname": "Li",
        "given_name": "Xiaomao"
      },
      {
        "surname": "Gao",
        "given_name": "Jiantao"
      },
      {
        "surname": "Xiao",
        "given_name": "Weiping"
      },
      {
        "surname": "Fan",
        "given_name": "Baojie"
      },
      {
        "surname": "Peng",
        "given_name": "Yan"
      }
    ]
  },
  {
    "title": "Large-margin representation learning for texture classification",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.04.006",
    "abstract": "This paper presents a novel approach combining convolutional layers (CLs) and large-margin metric learning for training supervised models on small datasets for texture classification. The core of such an approach is a loss function that computes the distances between instances of interest and support vectors. The objective is to update the weights of CLs iteratively to learn a representation with a large margin between classes. Each iteration results in a large-margin discriminant model represented by support vectors based on such a representation. The advantage of the proposed approach w.r.t. convolutional neural networks (CNNs) is two-fold. First, it allows representation learning with a small amount of data due to the reduced number of parameters compared to an equivalent CNN. Second, it has a low training cost since the backpropagation considers only support vectors. The experimental results on texture and histopathologic image datasets show that the proposed approach achieves competitive accuracy with lower computational cost and faster convergence compared to equivalent CNNs.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523001071",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Backpropagation",
      "CLs upper limits",
      "Computer science",
      "Convolutional neural network",
      "Economics",
      "Feature learning",
      "Image (mathematics)",
      "Law",
      "Machine learning",
      "Margin (machine learning)",
      "Medicine",
      "Metric (unit)",
      "Operations management",
      "Optometry",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "de Matos",
        "given_name": "Jonathan"
      },
      {
        "surname": "de Oliveira",
        "given_name": "Luiz Eduardo Soares"
      },
      {
        "surname": "de Souza Britto Junior",
        "given_name": "Alceu"
      },
      {
        "surname": "Koerich",
        "given_name": "Alessandro Lameiras"
      }
    ]
  },
  {
    "title": "Wse-MF: A weighting-based student exercise matrix factorization model",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.109285",
    "abstract": "Students who have been taught new ideas need to develop their skills by carrying out further work in their own time. This often consists of a series of exercises which must be completed. While students can choose exercises themselves from online sources, they will learn more quickly and easily if the exercises are specifically tailored to their needs. A good teacher will always aim to do this, but with the large groups of students who typically take advantage of open online courses, it may not be possible. Exercise prediction, working with large-scale matrix data, is a better way to address this challenge, and a key stage within such prediction is to calculate the probability that a student will answer a given question correctly. Therefore, this paper presents a novel approach called Weighting-based Student Exercise Matrix Factorization (Wse-MF) which combines student learning ability and exercise difficulty as prior weights. In order to learn how to complete the matrix, we apply an iterative optimization method that makes the approach practical for large-scale educational deployment. Compared with eight models in cognitive diagnosis and matrix factorization, our research results suggest that Wse-MF significantly outperforms the state-of-the-art on a range of real-world datasets in both prediction quality and time complexity. Moreover, we find that there is an optimal value of the latent factor K (the inner dimension of the factorization) for each dataset, which is related to the relationship between skills and exercises in that dataset. Similarly, the optimal value of hyperparameter c 0 is linked to the ratio between exercises and students. Taken as a whole, we demonstrate improvements to matrix factorization within the context of educational data.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322007646",
    "keywords": [
      "Artificial intelligence",
      "Composite material",
      "Computer science",
      "Dimension (graph theory)",
      "Eigenvalues and eigenvectors",
      "Hyperparameter",
      "Machine learning",
      "Materials science",
      "Mathematics",
      "Matrix (chemical analysis)",
      "Matrix decomposition",
      "Medicine",
      "Physics",
      "Pure mathematics",
      "Quantum mechanics",
      "Radiology",
      "Scale (ratio)",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Sun",
        "given_name": "Xia"
      },
      {
        "surname": "Li",
        "given_name": "Bo"
      },
      {
        "surname": "Sutcliffe",
        "given_name": "Richard"
      },
      {
        "surname": "Gao",
        "given_name": "Zhizezhang"
      },
      {
        "surname": "Kang",
        "given_name": "Wenying"
      },
      {
        "surname": "Feng",
        "given_name": "Jun"
      }
    ]
  },
  {
    "title": "Accurate light field depth estimation under occlusion",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109415",
    "abstract": "Epipolar plane images (EPIs) have advantages in light field depth estimation, but the current EPI-based methods only use one aspect of the information related to the line or its surroundings in EPIs. Moreover, most current methods merely extract the depth map of the target view, ignoring the depth information from other views. To fully utilize the available information, we first introduce a novel data cost by comprehensively utilizing the characteristics of pixel consistency on the line and region difference around the line in EPIs to improve the robustness against occlusion and noise. Then, we put forward a multi-view depth integration strategy that copes with weak texture and occlusion areas. Finally, an edging preserving filter is applied to further refine the depth map. Experiments on synthetic and real light field datasets show that the proposed method outperforms the state-of-the-art light field depth estimation algorithms, especially in the presence of occluded pixels.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323001164",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Depth map",
      "Epipolar geometry",
      "Gene",
      "Image (mathematics)",
      "Light field",
      "Pixel",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Yuxuan"
      },
      {
        "surname": "Aleksandrov",
        "given_name": "Mitko"
      },
      {
        "surname": "Hu",
        "given_name": "Zhihua"
      },
      {
        "surname": "Meng",
        "given_name": "Yan"
      },
      {
        "surname": "Zhang",
        "given_name": "Li"
      },
      {
        "surname": "Zlatanova",
        "given_name": "Sisi"
      },
      {
        "surname": "Ai",
        "given_name": "Haibin"
      },
      {
        "surname": "Tao",
        "given_name": "Pengjie"
      }
    ]
  },
  {
    "title": "Meta-learning for dynamic tuning of active learning on stream classification",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109359",
    "abstract": "Supervised data stream learning depends on the incoming sample’s true label to update a classifier’s model. In real life, obtaining the ground truth for each instance is a challenging process; it is highly costly and time consuming. Active Learning has already bridged this gap by finding a reduced set of instances to support the creation of a reliable stream classifier. However, identifying a reduced number of informative instances to support a suitable classifier update and drift adaptation is very tricky. To better adapt to concept drifts using a reduced number of samples, we propose an online tuning of the Uncertainty Sampling threshold using a meta-learning approach. Our approach exploits statistical meta-features from adaptive windows to meta-recommend a suitable threshold to address the trade-off between the number of labelling queries and high accuracy. Experiments exposed that the proposed approach provides the best trade-off between accuracy and query reduction by dynamic tuning the uncertainty threshold using lightweight meta-features.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323000602",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Computer security",
      "Concept drift",
      "Data mining",
      "Data stream",
      "Data stream mining",
      "Exploit",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Semi-supervised learning",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Martins",
        "given_name": "Vinicius Eiji"
      },
      {
        "surname": "Cano",
        "given_name": "Alberto"
      },
      {
        "surname": "Barbon Junior",
        "given_name": "Sylvio"
      }
    ]
  },
  {
    "title": "Robust detectors of rotationally symmetric shapes based on novel semi-shape signatures",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109336",
    "abstract": "Efficient detectors of rotationally symmetric shapes are proposed by introducing a novel concept of semi-shape signatures to overcome the main problem of projection-based approaches for studying the rotationally symmetric properties of an arbitrary binary shape. Indeed, the fact that the projection cues in these conventional approaches are periodical with a period of π has restricted an applicable exploitation of rotational symmetry detection. To this end, we propose a new concept of the profile of semi-shapes as a shape signature together with a simple yet efficient technique so that the rotational symmetry of the binary shape can be determined by considering the correlation between this signature and its circular shift. Moreover, a new meaningful measure, ranging from 0 to 1, is also introduced to indicate how perfect the rotational symmetry would be. Experimental results of detecting on single/compound shapes have clearly corroborated the competence of our proposal.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323000377",
    "keywords": [
      "Algorithm",
      "Arithmetic",
      "Binary number",
      "Biochemistry",
      "Chemistry",
      "Circular symmetry",
      "Computer science",
      "Database",
      "Detector",
      "Epistemology",
      "Gene",
      "Geometry",
      "Mathematics",
      "Measure (data warehouse)",
      "Optics",
      "Philosophy",
      "Physics",
      "Projection (relational algebra)",
      "Robustness (evolution)",
      "Rotational symmetry",
      "Signature (topology)",
      "Simple (philosophy)",
      "Symmetry (geometry)"
    ],
    "authors": [
      {
        "surname": "Nguyen",
        "given_name": "Thanh Phuong"
      },
      {
        "surname": "Nguyen",
        "given_name": "Thanh Tuan"
      }
    ]
  },
  {
    "title": "Weakly-supervised butterfly detection based on saliency map",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109313",
    "abstract": "Given the actual needs for detecting multiple features of butterflies in natural ecosystems, this paper proposes a model of weakly-supervised butterfly detection based on a saliency map (WBD-SM) to enhance the accuracy of butterfly detection in the ecological environment as well as to overcome the difficulty of fine annotation. Our proposed model first extracts the features of different scales using the VGG16 without the fully connected layers as the backbone network. Next, the saliency maps of butterfly images are extracted using the deep supervision network with shortcut connections (DSS) used for the butterfly target location. The class activation maps of butterfly images are derived via the adversarial complementary learning (ACoL) network for butterfly target recognition. Then, the saliency and class activation maps are post-processed with conditional random fields, thereby obtaining the refined saliency maps of butterfly objects. Finally, the locations of the butterflies are acquired based on the saliency maps. Experimental results on the 20 categories of butterfly dataset collected in this paper indicate that the WBD-SM achieves a higher recognition accuracy than that of the VGG16 under different division ratios. At the same time, when the training set and test set are 8:2, our WBD-SM attains a 95.67% localization accuracy, which is 9.37% and 11.87% higher than the results of the DSS and ACoL, respectively. Compared with three state-of-the-art fully-supervised object detection networks, RefineDet, YOLOv3 and single-shot detection (SSD), the detection performance of our WBD-SM is better than RefineDet, and YOLOv3, and is almost the same as SSD.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323000146",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Butterfly",
      "Computer science",
      "Computer vision",
      "Conditional random field",
      "Ecology",
      "Object detection",
      "Pattern recognition (psychology)",
      "Programming language",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Ting"
      },
      {
        "surname": "Waqas",
        "given_name": "Muhammad"
      },
      {
        "surname": "Fang",
        "given_name": "Yu"
      },
      {
        "surname": "Liu",
        "given_name": "Zhaoying"
      },
      {
        "surname": "Halim",
        "given_name": "Zahid"
      },
      {
        "surname": "Li",
        "given_name": "Yujian"
      },
      {
        "surname": "Chen",
        "given_name": "Sheng"
      }
    ]
  },
  {
    "title": "Unsupervised class-to-class translation for domain variations",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109346",
    "abstract": "The majority of image-to-image translation models tend to struggle in varying domain settings. For one varying domain, samples vary significantly in shape and size and have no domain labels. This paper proposes an unsupervised class-to-class translation model based on conditional contrastive learning to tackle the domain variations problem. The initial hypothesis is that the latent modalities of two varying domains are categorizable by style differences of different samples and turn the image-to-image translation problem into class-to-class translation. Firstly, unsupervised semantic clustering is performed for each domain to divide them into multiple classes and then leverage the classification features of different classes to perform class-to-class translation. Two conditional contrastive learning loss functions for each domain are proposed to perform unsupervised semantic clustering and decompose it into multiple classes. Then in the class-to-class translation stage, the classification features of different classes are employed to learn the latent modalities. The proposed model outperforms state-of-the-art baseline methods by employing the latent modalities of different classes. The sample code is available at https://github.com/c1a1o1/ucct.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032300047X",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Class (philosophy)",
      "Cluster analysis",
      "Computer science",
      "Domain (mathematical analysis)",
      "Gene",
      "Image (mathematics)",
      "Image translation",
      "Latent class model",
      "Leverage (statistics)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Messenger RNA",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Probabilistic latent semantic analysis",
      "Translation (biology)"
    ],
    "authors": [
      {
        "surname": "Cao",
        "given_name": "Zhiyi"
      },
      {
        "surname": "Wang",
        "given_name": "Wei"
      },
      {
        "surname": "Huo",
        "given_name": "Lina"
      },
      {
        "surname": "Niu",
        "given_name": "Shaozhang"
      }
    ]
  },
  {
    "title": "Surface normal and Gaussian weight constraints for indoor depth structure completion",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109362",
    "abstract": "Raw depth maps captured by depth sensors generally contain missing contents due to glossy, transparent, and sparsity problems. Recent methods well completed flat regions of raw depth maps; however, ignored the accuracy of depth structures. In this paper, an effective depth structure completion method is developed to infer missing depth structures. First, a raw depth map is divided into flat regions and depth structures based on a structure prediction network. Second, two local features including surface normals and Gaussian weights are extracted from a reference RGB image to impose constraints on flat regions and depth structures, separately. Third, a kernel least-square module is adopted to handle the texture-copy artifacts problem. Finally, an iterative optimization model is developed by embedding the two constraints into a Markov random field. The cost function of the model comprises three terms, which limit data fidelity between completed depth map and raw depth map, smoothness of flat regions, and accuracy of depth structures, respectively. The proposed method is evaluated on four indoor datasets including Matterport3D, RealSense, ScanNet, and NYUv2, and compared with eight recent baselines. Quantitative results demonstrate that RMSE and MAE of completed depth maps are considerably reduced by 22.0% and 45.3%, respectively. Visual results show the superiority in completing depth structures and suppressing texture-copy artifacts. Generalization test verify the effectiveness on unseen datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323000638",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Depth map",
      "Embedding",
      "Gaussian",
      "Image (mathematics)",
      "Image segmentation",
      "Markov random field",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "RGB color model",
      "Smoothness"
    ],
    "authors": [
      {
        "surname": "Ren",
        "given_name": "Dongran"
      },
      {
        "surname": "Yang",
        "given_name": "Meng"
      },
      {
        "surname": "Wu",
        "given_name": "Jiangfan"
      },
      {
        "surname": "Zheng",
        "given_name": "Nanning"
      }
    ]
  },
  {
    "title": "How to Reduce Change Detection to Semantic Segmentation",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109384",
    "abstract": "Change detection (CD) aims to identify changes that occur in an image pair taken different times. Prior methods devise specific networks from scratch to predict change masks in pixel-level, and struggle with general segmentation problems. In this paper, we propose a new paradigm that reduces CD to semantic segmentation which means tailoring an existing and powerful semantic segmentation network to solve CD. This new paradigm conveniently enjoys the mainstream semantic segmentation techniques to deal with general segmentation problems in CD. Hence we can concentrate on studying how to detect changes. We propose a novel and importance insight that different change types exist in CD and they should be learned separately. Based on it, we devise a module named MTF to extract the change information and fuse temporal features. MTF enjoys high interpretability and reveals the essential characteristic of CD. And most segmentation networks can be adapted to solve the CD problems with our MTF module. Finally, we propose C-3PO, a network to detect changes at pixel-level. C-3PO achieves state-of-the-art performance without bells and whistles. It is simple but effective and can be considered as a new baseline in this field. Our code for C-3PO is available at https://github.com/DoctorKey/C-3PO.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323000857",
    "keywords": [
      "Artificial intelligence",
      "Change detection",
      "Computer science",
      "Computer vision",
      "Electrical engineering",
      "Engineering",
      "Fuse (electrical)",
      "Interpretability",
      "Pattern recognition (psychology)",
      "Pixel",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Guo-Hua"
      },
      {
        "surname": "Gao",
        "given_name": "Bin-Bin"
      },
      {
        "surname": "Wang",
        "given_name": "Chengjie"
      }
    ]
  },
  {
    "title": "Processing a large collection of historical tabular images",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.04.007",
    "abstract": "Processing automatically historical document images to allow the search of textual information requires the preparation of ground-truth data for training and evaluation. This process is an expensive and arduous task, especially when the historical document images contain specialized vocabulary and/or tabular information. In the latter case, relevant decisions have to be taken to annotate the tabular parts. This paper presents a complex collection of historical document images and the resulting database, which is called HisClima. In this database, half of the images are in tabular format and half as running text. Both types of images contain pre-printed and handwritten text. The textual information is plenty of abbreviations and specific vocabulary related to weather conditions and old ships. This database can be used to research technologies related to historical document image processing and analysis, both for tabular and running text recognition. Baseline results are presented for Document Layout Analysis, Text Recognition, and Probabilistic Indexing. Although these results are good, there is still room for improvement and some indications are provided in this direction.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523001083",
    "keywords": [
      "Artificial intelligence",
      "Automatic indexing",
      "Computer science",
      "Document processing",
      "Economics",
      "Historical document",
      "Image (mathematics)",
      "Image processing",
      "Information retrieval",
      "Linguistics",
      "Management",
      "Natural language processing",
      "Operating system",
      "Philosophy",
      "Process (computing)",
      "Search engine indexing",
      "Task (project management)",
      "Text processing",
      "Vocabulary"
    ],
    "authors": [
      {
        "surname": "Granell",
        "given_name": "Emilio"
      },
      {
        "surname": "Romero",
        "given_name": "Verónica"
      },
      {
        "surname": "Prieto",
        "given_name": "José Ramón"
      },
      {
        "surname": "Andrés",
        "given_name": "José"
      },
      {
        "surname": "Quirós",
        "given_name": "Lorenzo"
      },
      {
        "surname": "Sánchez",
        "given_name": "Joan Andreu"
      },
      {
        "surname": "Vidal",
        "given_name": "Enrique"
      }
    ]
  },
  {
    "title": "SATS: Self-attention transfer for continual semantic segmentation",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109383",
    "abstract": "Continually learning to segment more and more types of image regions is a desired capability for many intelligent systems. However, such continual semantic segmentation exhibits catastrophic forgetting issues similar to those of continual classification learning. Unlike the existing knowledge distillation strategies for alleviating this problem, transferring a new type of information, namely, the relationships between elements (e.g., pixels) within each image that can capture both within-class and between-class knowledge, is proposed in this study. Such information can be effectively obtained from self-attention maps in a Transformer-style segmentation model. Considering that pixels belonging to the same class in each image typically share similar visual properties, a class-specific region pooling operator is novelly applied to provide reliable relationship information for knowledge transfer. Extensive evaluations on multiple public benchmarks reveal that the proposed self-attention transfer method can effectively alleviate the catastrophic forgetting issue. Furthermore, flexible combinations of the proposed method with widely adopted strategies considerably outperform state-of-the-art solutions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323000845",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Forgetting",
      "Image segmentation",
      "Linguistics",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pixel",
      "Pooling",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Qiu",
        "given_name": "Yiqiao"
      },
      {
        "surname": "Shen",
        "given_name": "Yixing"
      },
      {
        "surname": "Sun",
        "given_name": "Zhuohao"
      },
      {
        "surname": "Zheng",
        "given_name": "Yanchong"
      },
      {
        "surname": "Chang",
        "given_name": "Xiaobin"
      },
      {
        "surname": "Zheng",
        "given_name": "Weishi"
      },
      {
        "surname": "Wang",
        "given_name": "Ruixuan"
      }
    ]
  },
  {
    "title": "BEST: Building evidences from scattered templates for accurate contactless palmprint recognition",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109422",
    "abstract": "Contactless palmprint identification offers significantly improved hygiene and user convenience, making it highly attractive for a range of civilian applications, especially during the current pandemic. However, the accurate recognition of contactless palmprint images can be highly challenging, attributed to the significant variations in the intra-class similarity and limitations of conventional palmprint feature descriptors under involuntary or contactless imaging variations. State-of-the-art completely contactless palmprint matching algorithms in the literature cannot adequately address these challenges and are not sufficiently accurate and fast enough for such real-world applications. This paper proposes a novel approach that adaptively locates the local palmprint regions with high similarities between their corresponding feature representations or templates to address these challenges. We consider spatial localization of such highly similar feature representations from multiple local regions and consolidate them to generate a more reliable match score. This paper presents reproducible and comparative experimental results, using within-database, cross-database, and cross-sensor performance evaluation, on four publicly available contactless palmprint datasets, including a sizeable contactless palmprint database from 600 different subjects. The proposed method achieves outperforming results compared with three state-of-the-art deep learning-based methods and five widely used conventional methods. In addition, the proposed method is also significantly faster than all state-of-the-art baseline methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323001231",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Feature extraction",
      "Identification (biology)",
      "Image (mathematics)",
      "Linguistics",
      "Matching (statistics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Similarity (geometry)",
      "Statistics",
      "Template"
    ],
    "authors": [
      {
        "surname": "Yulin",
        "given_name": "Feng"
      },
      {
        "surname": "Kumar",
        "given_name": "Ajay"
      }
    ]
  },
  {
    "title": "Lambertian-based adversarial attacks on deep-learning-based underwater side-scan sonar image classification",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109363",
    "abstract": "Deep convolutional neural networks (CNNs) are extensively applied to the classification tasks for Side-scan sonar (SSS) images. However, state-of-the-art neural networks are prone to be confused by adversarial attacks that generate a tiny modification of the images, threatening the security of SSS classification. The robustness of CNN to adversarial attacks can be improved by introducing adversarial examples through adversarial training. Practical adversarial examples are often generated from elaborate adversarial attackers. For the underwater scenario of sonar, a specially designed adversarial attack method to weaken SSS image classification can make the research community better understand the weakness of CNN in this scenario and improve the security measures in a well-directed way. Thus, exploring adversarial attack methods for SSS image classification is essential. Nevertheless, the existing adversarial attack methods are designed for optical images, reflecting no physical characteristics of sonar images. To fill this gap and investigate the adversarial attack related to real-world conditions, in this paper, we propose an adversarial attack method named Lambertian Adversarial Sonar Attack (LASA). It initially leverages the Lambertian model to simulate the formation of the SSS image, factoring the image to three parameters, then updates the parameters on the direction of gradients by the chain rule. Finally, the parameters regenerate the adversarial example to fool the classifier. To validate the performance of LASA, we constructed a diversified SSS image dataset containing three categories. On our dataset, LASA reduces the Top-1 accuracy of a well-trained ResNet-101 to 7.31 % ± 0.21 (one-shot version) and 0.00% (iterative version), the success rate of targeted attack reaches 97.03 ± 2.24 , far beyond the performance of the existing state-of-the-art adversarial attack methods. Meanwhile, we show that the adversarial training using examples generated from LASA makes the classifier more robust. We expect that our methods can be applied as a benchmark of adversarial attacks on SSS images, motivating future research to design novel neural networks or defensive methods to resist real-world adversarial attacks on SSS images.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032300064X",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Classifier (UML)",
      "Computer science",
      "Computer vision",
      "Contextual image classification",
      "Convolutional neural network",
      "Deep learning",
      "Gene",
      "Image (mathematics)",
      "Pattern recognition (psychology)",
      "Robustness (evolution)",
      "Side-scan sonar",
      "Sonar"
    ],
    "authors": [
      {
        "surname": "Ma",
        "given_name": "Qixiang"
      },
      {
        "surname": "Jiang",
        "given_name": "Longyu"
      },
      {
        "surname": "Yu",
        "given_name": "Wenxue"
      }
    ]
  },
  {
    "title": "Under the hood of transformer networks for trajectory forecasting",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109372",
    "abstract": "Transformer Networks have established themselves as the de-facto state-of-the-art for trajectory forecasting but there is currently no systematic study on their capability to model the motion patterns of people, without interactions with other individuals nor the social context. There is abundant literature on LSTMs, CNNs and GANs on this subject. However methods adopting Transformer techniques achieve great performances by complex models and a clear analysis of their adoption as plain sequence models is missing. This paper proposes the first in-depth study of Transformer Networks (TF) and the Bidirectional Transformers (BERT) for the forecasting of the individual motion of people, without bells and whistles. We conduct an exhaustive evaluation of the input/output representations, problem formulations and sequence modelling, including a novel analysis of their capability to predict multi-modal futures. Out of comparative evaluation on the ETH+UCY benchmark, both TF and BERT are top performers in predicting individual motions and remain within a narrow margin wrt more complex techniques, including both social interactions and scene contexts. Source code will be released for all conducted experiments.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323000730",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "De facto",
      "Economics",
      "Electrical engineering",
      "Engineering",
      "Financial economics",
      "Futures contract",
      "Law",
      "Machine learning",
      "Political science",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Franco",
        "given_name": "Luca"
      },
      {
        "surname": "Placidi",
        "given_name": "Leonardo"
      },
      {
        "surname": "Giuliari",
        "given_name": "Francesco"
      },
      {
        "surname": "Hasan",
        "given_name": "Irtiza"
      },
      {
        "surname": "Cristani",
        "given_name": "Marco"
      },
      {
        "surname": "Galasso",
        "given_name": "Fabio"
      }
    ]
  },
  {
    "title": "Human-related anomalous event detection via memory-augmented Wasserstein generative adversarial network with gradient penalty",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109398",
    "abstract": "Timely detection of human-related anomaly in surveillance videos is a challenging task. Generally, the irregular human motion and action patterns can be regarded as abnormal human-related events. In this paper, we utilize the skeleton trajectories to learn the regularities of human motion and action in videos for anomaly detection. The skeleton trajectories are decomposed into global and local feature sequences, which are utilized to provide human motion and action information, respectively. Then, the global and local sequences are modeled as two separate sub-processes with our proposed Memory-augmented Wasserstein Generative Adversarial Network with Gradient Penalty (MemWGAN-GP). In each sub-process, the pre-trained MemWGAN-GP is employed to predict future feature sequences from corresponding input past sequences and reconstruct the input sequences simultaneously. The predicted and reconstructed feature sequences are compared with their groundtruth to identify anomalous sequences. The MemWGAN-GP integrates the autoencoder with a WGAN model to boost the reconstruction and prediction ability of the autoencoder. Besides, a memory module is employed in MemWGAN-GP to overcome high capacity of the autoencoder for anomalies reconstruction and prediction. Experimental results on four challenging datasets demonstrate advantages of the proposed method over other state-of-the-art algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323000997",
    "keywords": [
      "Action (physics)",
      "Anomaly (physics)",
      "Anomaly detection",
      "Artificial intelligence",
      "Artificial neural network",
      "Autoencoder",
      "Computer science",
      "Condensed matter physics",
      "Economics",
      "Event (particle physics)",
      "Feature (linguistics)",
      "Linguistics",
      "Management",
      "Motion (physics)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Nanjun"
      },
      {
        "surname": "Chang",
        "given_name": "Faliang"
      },
      {
        "surname": "Liu",
        "given_name": "Chunsheng"
      }
    ]
  },
  {
    "title": "Detecting outliers from pairwise proximities: Proximity isolation forests",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109334",
    "abstract": "Because outliers are very different from the rest of the data, it is natural to represent outliers by their distances to other objects. Furthermore, there are many scenarios in which only pairwise distances are known, and feature-based outlier detection methods cannot directly be applied. Considering these observations, and given the success of Isolation Forests for (feature-based) outlier detection, we propose Proximity Isolation Forest, a proximity-based extension. The methodology only requires a set of pairwise distances to work, making it suitable for different types of data. Analogously to Isolation Forest, outliers are detected via their early isolation in the trees; to encode the isolation we design nine training strategies, both random and optimized. We thoroughly evaluate the proposed approach on fifteen datasets, successfully assessing its robustness and suitability for the task; additionally we compare favourably to alternative proximity-based methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323000353",
    "keywords": [
      "Anomaly detection",
      "Artificial intelligence",
      "Biochemistry",
      "Biology",
      "Computer science",
      "Data mining",
      "Gene",
      "Isolation (microbiology)",
      "Microbiology",
      "Outlier",
      "Pairwise comparison",
      "Pattern recognition (psychology)",
      "Random forest",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Mensi",
        "given_name": "Antonella"
      },
      {
        "surname": "Tax",
        "given_name": "David M.J."
      },
      {
        "surname": "Bicego",
        "given_name": "Manuele"
      }
    ]
  },
  {
    "title": "Multiview Jointly Sparse Discriminant Common Subspace Learning",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109342",
    "abstract": "Multiview data leads to the demand for classifying samples from various views, and the large gap between different views makes the classification task challenging. Recently, researchers have extended linear discriminant analysis (LDA) to multi-view scenarios. However, the extended methods are generally associated with the small-class problem, that is, the projection size is limited by the number of classes. In addition, they are sensitive to variations in images or outliers. To solve these problems, this study proposes a generalized robust multiview discriminant analysis (GRMDA) to obtain a linear transform for each view and for learning multiview jointly sparse discriminant common subspace. GRMDA aims to achieve both maximal between-class and minimal within-class variation for data from multiple views in a common space. Instead of formulating the ratio trace problem, we reformulate GRMDA inspired by maximum margin criterion (MMC) to address the small-class problem. Moreover, the proposed method achieves stronger robustness by reconstructing the within-class and between-class scatter terms from the definition of L 2 , 1 norm. Furthermore, GRMDA ensures joint sparsity using the L 2 , 1 norm-based regularization term. Additionally, we present an iterative algorithm, convergence proof, and complexity analysis. Experiments on six popular databases, that is, COIL100, USPS/MNIST, Extended Yale Face B, AR, BBCSport, and multiple feature datasets, were conducted to evaluate the performance of GRMDA against the state-of-the-art multiview methods. The experimental results demonstrate that the proposed method can achieve a significant performance with strong robustness and fast convergence.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323000432",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Discriminant",
      "Facial recognition system",
      "Gene",
      "Linear discriminant analysis",
      "MNIST database",
      "Machine learning",
      "Mathematics",
      "Outlier",
      "Pattern recognition (psychology)",
      "Robustness (evolution)",
      "Subspace topology"
    ],
    "authors": [
      {
        "surname": "Lin",
        "given_name": "Yiling"
      },
      {
        "surname": "Lai",
        "given_name": "Zhihui"
      },
      {
        "surname": "Zhou",
        "given_name": "Jie"
      },
      {
        "surname": "Wen",
        "given_name": "Jiajun"
      },
      {
        "surname": "Kong",
        "given_name": "Heng"
      }
    ]
  },
  {
    "title": "Computation-Efficient Knowledge Distillation via Uncertainty-Aware Mixup",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109338",
    "abstract": "Knowledge distillation (KD) has emerged as an essential technique not only for model compression, but also other learning tasks such as continual learning. Given the richer application spectrum and potential online usage of KD, knowledge distillation efficiency becomes a pivotal component. In this work, we study this little-explored but important topic. Unlike previous works that focus solely on the accuracy of student network, we attempt to achieve a harder goal – to obtain a performance comparable to conventional KD with a lower computation cost during the transfer. To this end, we present UNcertainty-aware mIXup (UNIX), an effective approach that can reduce transfer cost by 20% to 30% and yet maintain comparable or achieve even better student performance than conventional KD. This is made possible via effective uncertainty sampling and a novel adaptive mixup approach that select informative samples dynamically over ample data and compact knowledge in these samples. We show that our approach inherently performs hard sample mining. We demonstrate the applicability of our approach to improve various existing KD approaches by reducing their queries to a teacher network. Extensive experiments are performed on CIFAR100 and ImageNet. Code and model are available at https://github.com/xuguodong03/UNIXKD.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323000390",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Chemistry",
      "Chromatography",
      "Code (set theory)",
      "Component (thermodynamics)",
      "Computation",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Distillation",
      "Filter (signal processing)",
      "Focus (optics)",
      "Machine learning",
      "Optics",
      "Organic chemistry",
      "Physics",
      "Programming language",
      "Sample (material)",
      "Sampling (signal processing)",
      "Set (abstract data type)",
      "Software",
      "Thermodynamics",
      "Unix"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Guodong"
      },
      {
        "surname": "Liu",
        "given_name": "Ziwei"
      },
      {
        "surname": "Loy",
        "given_name": "Chen Change"
      }
    ]
  },
  {
    "title": "Time series clustering with an EM algorithm for mixtures of linear Gaussian state space models",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109375",
    "abstract": "In this paper, we consider the task of clustering a set of individual time series while modeling each cluster, that is, model-based time series clustering. The task requires a parametric model with sufficient flexibility to describe the dynamics in various time series. To address this problem, we propose a novel model-based time series clustering method with mixtures of linear Gaussian state space models, which have high flexibility. The proposed method uses a new expectation-maximization algorithm for the mixture model to estimate the model parameters, and determines the number of clusters using the Bayesian information criterion. Experiments on a simulated dataset demonstrate the effectiveness of the method in clustering, parameter estimation, and model selection. The method is applied to real datasets commonly used to evaluate time series clustering methods. Results showed that the proposed method produces clustering results that are as accurate or more accurate than those obtained using previous methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323000766",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Bayesian information criterion",
      "Biology",
      "CURE data clustering algorithm",
      "Cluster analysis",
      "Computer science",
      "Correlation clustering",
      "Data stream clustering",
      "Determining the number of clusters in a data set",
      "Expectation–maximization algorithm",
      "Mathematics",
      "Maximum likelihood",
      "Mixture model",
      "Model selection",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Series (stratigraphy)",
      "State space",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Umatani",
        "given_name": "Ryohei"
      },
      {
        "surname": "Imai",
        "given_name": "Takashi"
      },
      {
        "surname": "Kawamoto",
        "given_name": "Kaoru"
      },
      {
        "surname": "Kunimasa",
        "given_name": "Shutaro"
      }
    ]
  },
  {
    "title": "A generalized multi-aspect distance metric for mixed-type data clustering",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109353",
    "abstract": "Distance calculation is straightforward when working with pure categorical or pure numerical data sets. Defining a unified distance to improve the clustering performance for a mixed data set composed of nominal, ordinal, and numerical attributes is very challenging due to the attributes’ different natures. In this study, we proposed a new measure of distance for a mixed-type data set that regards inter-attribute information and intra-attribute information depending on the type of attributes. In this regard, entropy and Jensen–Shannon divergence concepts were used to exploit the inter-attribute information of categorical-categorical and categorical-numerical attributes, respectively. Also, a modified version of Mahalanobis distance was proposed to consider the intra- and inter-attribute information of numerical attributes. We also introduced a unified framework based on mutual information to control attributes’ contribution to distance measurement. The proposed distance in conjunction with spectral clustering was extensively evaluated concerning various categorical, numerical, and mixed-type benchmark data sets, and the results demonstrated the efficacy of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323000547",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Categorical variable",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Divergence (linguistics)",
      "Entropy (arrow of time)",
      "Kullback–Leibler divergence",
      "Linguistics",
      "Mahalanobis distance",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Mousavi",
        "given_name": "Elahe"
      },
      {
        "surname": "Sehhati",
        "given_name": "Mohammadreza"
      }
    ]
  },
  {
    "title": "Robust weighted co-clustering with global and local discrimination",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109405",
    "abstract": "In the past few decades, the clustering problem has made considerable progress, and co-clustering algorithms have attracted more attention. Compared with one-side clustering, co-clustering not only groups samples according to the distribution of features but also groups features according to the distribution of samples at the same time. This duality helps to explore the structural information of data, such as genes and texts. In this paper, a new co-clustering algorithm is proposed to simultaneously consider feature weights, data noise, local manifolds, and global scatter, named robust weighted co-clustering with global and local discrimination. Furthermore, an alternate update rule is put forward to optimize objective, theoretically proven to converge. Then, the algorithm’s duality, robustness, and effectiveness have been verified on synthetic, corrupted, and real datasets, respectively. The runtime and parameter sensitivity of the algorithm are also analyzed. Finally, sufficient experiments clarify the competitiveness of our algorithm compared to other ones.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323001061",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "CURE data clustering algorithm",
      "Canopy clustering algorithm",
      "Chemistry",
      "Cluster analysis",
      "Computer science",
      "Correlation clustering",
      "Data mining",
      "Data stream clustering",
      "Gene",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Lu",
        "given_name": "Zhoumin"
      },
      {
        "surname": "Wang",
        "given_name": "Shiping"
      },
      {
        "surname": "Liu",
        "given_name": "Genggeng"
      },
      {
        "surname": "Nie",
        "given_name": "Feiping"
      }
    ]
  },
  {
    "title": "Probability propagation for faster and efficient point cloud segmentation using a neural network",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.04.010",
    "abstract": "Neural networks (NN) have shown promising performance in point cloud segmentation (PCS). However, the measured points are too numerous to be used as model input at once. It results in a long inference time and high computational cost due to iterative sampling and inference. This study proposes Probability Propagation (PP) as a stochastic upsampling method. PP propagates the predicted probability of a sampled part of a point cloud into the other unpredicted points by considering proximity. By replacing the iterative inference of NN with PP, large point clouds can be dealt with quickly and efficiently. We investigated the effectiveness of PP using the ShapeNet benchmark on various settings: sampling methods (random, farthest point, and Poisson disk sampling) with sampling ratios (5%, 10%, 20%, 39%, and 78%) for NN and the stochastic mapping conditions (uniform, linear, cosine, Gaussian, and exponential distributions) for PP. Using NN with PP achieved higher performance and faster inference speed than when using NN alone. For the farthest point sampling method of 5% sampling ratio, NN+PP improved the instance mIoU by 2.457%p with 102 times faster speed compared to that when using NN alone. The result indicates that PP can significantly contribute to the improvement of performance and efficiency in PCS when used in edge AI systems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523001186",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Computer science",
      "Computer vision",
      "Filter (signal processing)",
      "Gaussian",
      "Geodesy",
      "Geography",
      "Importance sampling",
      "Inference",
      "Mathematics",
      "Monte Carlo method",
      "Physics",
      "Point cloud",
      "Poisson distribution",
      "Quantum mechanics",
      "Sampling (signal processing)",
      "Segmentation",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Seo",
        "given_name": "Hogeon"
      },
      {
        "surname": "Noh",
        "given_name": "Sangjun"
      },
      {
        "surname": "Shin",
        "given_name": "Sungho"
      },
      {
        "surname": "Lee",
        "given_name": "Kyoobin"
      }
    ]
  },
  {
    "title": "Deep metric learning for few-shot image classification: A Review of recent developments",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109381",
    "abstract": "Few-shot image classification is a challenging problem that aims to achieve the human level of recognition based only on a small number of training images. One main solution to few-shot image classification is deep metric learning. These methods, by classifying unseen samples according to their distances to few seen samples in an embedding space learned by powerful deep neural networks, can avoid overfitting to few training images in few-shot image classification and have achieved the state-of-the-art performance. In this paper, we provide an up-to-date review of deep metric learning methods for few-shot image classification from 2018 to 2022 and categorize them into three groups according to three stages of metric learning, namely learning feature embeddings, learning class representations, and learning distance measures. Under this taxonomy, we identify the trends of transitioning from learning task-agnostic features to task-specific features, from simple computation of prototypes to computing task-dependent prototypes or learning prototypes, from using analytical distance or similarity measures to learning similarities through convolutional or graph neural networks. Finally, we discuss the current challenges and future directions of few-shot deep metric learning from the perspectives of effectiveness, optimization and applicability, and summarize their applications to real-world computer vision tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323000821",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Categorization",
      "Computer science",
      "Contextual image classification",
      "Convolutional neural network",
      "Deep learning",
      "Economics",
      "Embedding",
      "Image (mathematics)",
      "Machine learning",
      "Metric (unit)",
      "Operations management",
      "Overfitting",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Xiaoxu"
      },
      {
        "surname": "Yang",
        "given_name": "Xiaochen"
      },
      {
        "surname": "Ma",
        "given_name": "Zhanyu"
      },
      {
        "surname": "Xue",
        "given_name": "Jing-Hao"
      }
    ]
  },
  {
    "title": "Texts as points: Scene text detection with point supervision",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.04.004",
    "abstract": "Scene text detection is challenging due to the diverse text appearance, the complex background, and the expensive labeling of training data. For detecting arbitrary-shaped texts, most existing methods require heavy data labeling efforts to produce polygon-level annotations for supervised training. In order to reduce the cost in data labeling, we propose to combine center point annotation into mixed-supervised scene text detection, in which the dataset comprises small number of fully annotated images and large number of weakly annotated images by center points. For better incorporating point supervision, we adopt self-training strategy based on a detector which locates texts by predicting their centers. Besides, in order to weight the pseudo labels generated during self-training, we also propose a novel regression uncertainty estimation module to measure the quality of detection results. Extensive experiments on five benchmark datasets (ICDAR2015, C-SVT, CTW1500, Total-Text and ICDAR-ArT) show that using small amount of polygon annotated data and large amount of center point annotated data, our detector can achieve competitive detection performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523001058",
    "keywords": [
      "Artificial intelligence",
      "Computer graphics (images)",
      "Computer science",
      "Computer vision",
      "Geometry",
      "Mathematics",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Point (geometry)"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Mengbiao"
      },
      {
        "surname": "Feng",
        "given_name": "Wei"
      },
      {
        "surname": "Yin",
        "given_name": "Fei"
      },
      {
        "surname": "Liu",
        "given_name": "Cheng-Lin"
      }
    ]
  },
  {
    "title": "Towards local visual modeling for image captioning",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109420",
    "abstract": "In this paper, we study the local visual modeling with grid features for image captioning, which is critical for generating accurate and detailed captions. To achieve this target, we propose a Locality-Sensitive Transformer Network (LSTNet) with two novel designs, namely Locality-Sensitive Attention (LSA) and Locality-Sensitive Fusion (LSF). LSA is deployed for the intra-layer interaction in Transformer via modeling the relationship between each grid and its neighbors. It reduces the difficulty of local object recognition during captioning. LSF is used for inter-layer information fusion, which aggregates the information of different encoder layers for cross-layer semantical complementarity. With these two novel designs, the proposed LSTNet can model the local visual information of grid features to improve the captioning quality. To validate LSTNet, we conduct extensive experiments on the competitive MS-COCO benchmark. The experimental results show that LSTNet is not only capable of local visual modeling, but also outperforms a bunch of state-of-the-art captioning models on offline and online testings, i.e., 134.8 CIDEr and 136.3 CIDEr, respectively. Besides, the generalization of LSTNet is also verified on the Flickr8k and Flickr30k datasets. The source code is available on GitHub: https://www.github.com/xmu-xiaoma666/LSTNet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323001218",
    "keywords": [
      "Artificial intelligence",
      "Closed captioning",
      "Computer science",
      "Database",
      "Encoder",
      "Geometry",
      "Grid",
      "Image (mathematics)",
      "Linguistics",
      "Locality",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Snapshot (computer storage)",
      "Source code",
      "Speech recognition",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Ma",
        "given_name": "Yiwei"
      },
      {
        "surname": "Ji",
        "given_name": "Jiayi"
      },
      {
        "surname": "Sun",
        "given_name": "Xiaoshuai"
      },
      {
        "surname": "Zhou",
        "given_name": "Yiyi"
      },
      {
        "surname": "Ji",
        "given_name": "Rongrong"
      }
    ]
  },
  {
    "title": "On a method for detecting periods and repeating patterns in time series data with autocorrelation and function approximation",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109355",
    "abstract": "Detecting recurrent patterns in time series data is an important capability. The reason is that repeating patterns on the one hand indicate well defined processes that can be further analyzed once detected and on the other hand are a reliable feature to predict future occurrences and adapt accordingly. The challenge in real data to define a period is that a time series is usually also influenced by non-periodic dynamics and noise. In this work, a mathematical framework is proved to define regular patterns. Their properties are used within a suggested algorithm based on the concept of autocorrelation and function approximation to fit a model capturing the periodic part of the time series. Based on that model and a corresponding autocorrelation, a new score is defined to evaluate how well a hypothesized period fits to the time series. This score is particularly useful in a big data scenario where decisions for periodicity are needed to be taken automatically, which is one of the main achievement of the presented work. The period analysis algorithm is applied to data from two different use cases. The first one is a data center scenario where the information of the periodic pattern is used to create a feature that improves a machine learning framework predicting future resource demands. The feature represents the phase of the repeating pattern. In a second scenario, expression data from mice liver cells are investigated concerning periodic rhythms. A Python implementation of the presented algorithm is provided via a github repository under https://github.com/LauritzR/period-detection.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323000560",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Autocorrelation",
      "Autoregressive integrated moving average",
      "Biology",
      "Computer science",
      "Data mining",
      "Evolutionary biology",
      "Feature (linguistics)",
      "Function (biology)",
      "Image (mathematics)",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Noise (video)",
      "Operating system",
      "Paleontology",
      "Partial autocorrelation function",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Python (programming language)",
      "Series (stratigraphy)",
      "Statistics",
      "Time series"
    ],
    "authors": [
      {
        "surname": "Breitenbach",
        "given_name": "Tim"
      },
      {
        "surname": "Wilkusz",
        "given_name": "Bartosz"
      },
      {
        "surname": "Rasbach",
        "given_name": "Lauritz"
      },
      {
        "surname": "Jahnke",
        "given_name": "Patrick"
      }
    ]
  },
  {
    "title": "Infrared and visible image fusion based on Multi-State contextual hidden Markov Model",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109431",
    "abstract": "In this paper, we propose a novel multi-state contextual hidden Markov model (MCHMM) in the non-subsampled Shearlet transform (NSST) domain for image fusion. The traditional two-state hidden Markov model divides the multi-scale coefficients only into large and small states, which can lead to an inaccurate statistical model and reduce the quality of the fusion result. Our method improves upon this by developing a multi-state model and a soft context variable to provide a fine-grained representation of the high-frequency subbands, resulting in improved fusion results. Additionally, the fusion of low-frequency subbands is performed on the difference of regional energy to ensure visual quality. Our experimental results on several datasets demonstrate that the proposed method outperforms other fusion methods in both subjective and objective evaluations.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323001322",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Computer science",
      "Context (archaeology)",
      "Domain (mathematical analysis)",
      "Fusion",
      "Geography",
      "Hidden Markov model",
      "Image (mathematics)",
      "Image fusion",
      "Law",
      "Linguistics",
      "Machine learning",
      "Markov chain",
      "Markov model",
      "Markov process",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Luo",
        "given_name": "Xiaoqing"
      },
      {
        "surname": "Jiang",
        "given_name": "Yuting"
      },
      {
        "surname": "Wang",
        "given_name": "Anqi"
      },
      {
        "surname": "Wang",
        "given_name": "Juan"
      },
      {
        "surname": "Zhang",
        "given_name": "Zhancheng"
      },
      {
        "surname": "Wu",
        "given_name": "Xiao-Jun"
      }
    ]
  },
  {
    "title": "On a linear fused Gromov-Wasserstein distance for graph structured data",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109351",
    "abstract": "We present a framework for embedding graph structured data into a vector space, taking into account node features and structures of graphs into the optimal transport (OT) problem. Then we propose a novel distance between two graphs, named LinearFGW, defined as the Euclidean distance between their embeddings. The advantages of the proposed distance are twofold: 1) it takes into account node features and structures of graphs for measuring the dissimilarity between graphs in a kernel-based framework, 2) it is more efficient for computing a kernel matrix than pairwise OT-based distances, particularly fused Gromov-Wasserstein [1], making it possible to deal with large-scale data sets. Our theoretical analysis and experimental results demonstrate that our proposed distance leads to an increase in performance compared to the existing state-of-the-art graph distances when evaluated on graph classification and clustering tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323000523",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Block graph",
      "Cluster analysis",
      "Clustering coefficient",
      "Combinatorics",
      "Computer science",
      "Distance matrix",
      "Embedding",
      "Euclidean distance",
      "Euclidean space",
      "Graph",
      "Graph embedding",
      "Line graph",
      "Mathematics",
      "Pairwise comparison",
      "Pathwidth",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Nguyen",
        "given_name": "Dai Hai"
      },
      {
        "surname": "Tsuda",
        "given_name": "Koji"
      }
    ]
  },
  {
    "title": "A single-stage point cloud cleaning network for outlier removal and denoising",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109366",
    "abstract": "As a simple, flexible and effective representation for objects, 3D point cloud has attracted more and more attention in recent years. However, raw point clouds obtained from 3D scanners or image-based reconstruction techniques are often contaminated with noise and outliers, which hinders downstream tasks such as object classification, surface reconstruction, and so on. Therefore, point cloud cleaning, i.e., removing noisy points and outliers from raw point cloud, is a prior step of most geometry processing workflows. The exiting techniques for point cloud cleaning usually include two stages, that is, discarding outliers at first, and then denoising the resulting point cloud. This two-stage process usually requires two different models, which is cumbersome to train and use. To solve this problem, a novel data driven method, named SSPCN (single-stage point cloud cleaning network), is proposed in this paper. SSPCN can simultaneously remove outliers and denoise a point cloud in a single model. Specifically, SSPCN is consisted of adaptive downsampling module, feature compensation module, upsampling module and coordinate reconstruction module. Given a raw point cloud as input, the downsampling module is first used to obtain a prefiltered point cloud subset and learn initial features of the subset. The feature compensation module is then utilized to learn accurate features from initial features. Next, the upsampling module upsamples the features to restore the original size of the point cloud. Last, the coordinate reconstruction module generates a cleaned point cloud from upsampled features. SSPCN is validated both on synthetic and real scanned data. Extensive experiments demonstrate that SSPCN outperforms state-of-the-art point cloud cleaning techniques in terms of quantitative metric and visual quality.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323000675",
    "keywords": [
      "Artificial intelligence",
      "Cloud computing",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Image (mathematics)",
      "Linguistics",
      "Noise (video)",
      "Operating system",
      "Outlier",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Point cloud",
      "Upsampling"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Ying"
      },
      {
        "surname": "Sheng",
        "given_name": "Huankun"
      }
    ]
  },
  {
    "title": "A graph model-based multiscale feature fitting method for unsupervised anomaly detection",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109373",
    "abstract": "Anomaly detection and localization without prior knowledge is a challenging problem in industrial manufacturing due to the complexity and variety of anomaly types. Most of the existing methods have achieved considerable anomaly detection performance based on the distance between normal features and abnormal features. However, when the defect area is hard to distinguish from the background or the defect area is small, the distance between normal and abnormal features will be too close to detect anomaly areas. In addition, existing methods do not consider the influences of features in different layers with different anomaly sizes. In this paper, a graph model-based multiscale feature fitting method is proposed for unsupervised anomaly detection. Specifically, we build a graph model based on the K nearest neighbors of an anchor image. The feature fitting and anomaly scores of the anchor images in the graph vertices are calculated next. Finally, a weighted multiscale anomaly map matching method is proposed to detect and locate the anomaly regions of test images. Compared with the state-of-the-art methods, our proposed method achieves competitive improvement in anomaly detection and localization on the MVTec AD dataset, the two KolektorSDD datasets, and the mSTC dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323000742",
    "keywords": [
      "Anomaly (physics)",
      "Anomaly detection",
      "Artificial intelligence",
      "Computer science",
      "Condensed matter physics",
      "Data mining",
      "Feature (linguistics)",
      "Graph",
      "Linguistics",
      "Matching (statistics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Statistics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Fanghui"
      },
      {
        "surname": "Kan",
        "given_name": "Shichao"
      },
      {
        "surname": "Zhang",
        "given_name": "Damin"
      },
      {
        "surname": "Cen",
        "given_name": "Yigang"
      },
      {
        "surname": "Zhang",
        "given_name": "Linna"
      },
      {
        "surname": "Mladenovic",
        "given_name": "Vladimir"
      }
    ]
  },
  {
    "title": "Learning efficient facial landmark model for human attractiveness analysis",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109370",
    "abstract": "Existing geometric features on facial attractiveness analysis only focus on the ratios and distances, which is incomplete to represent all the information of a face. In this paper, we introduce a new category of feature, i.e., the angle features, to describe the angle of different organs such as the chin and eyes, which help boost the analysis performance in experiment. In addition, existing facial beauty analysis papers usually apply existing landmark models and extract their own different geometric feature sets on the landmarks. On the one hand, the geometric features are quite chaotic between different papers. On the other hand, most of the landmarks in the existing landmark model are useless for geometric feature extraction which wastes a lot of computational resources. To tackle these issues, we suggest to define a common geometric feature set and learn a special landmark model for attractiveness analysis. Specially, we collect all the available geometric features from the previous jobs and introduce a genetic feature selection algorithm to select the most effective geometric features. Furthermore, we introduce a special landmark model which exactly covers all the extracted geometric features. The experiments show that our method with the introduced angle features and the common feature set can outperform state-of-art facial beauty estimation methods with geometric features.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323000717",
    "keywords": [
      "Artificial intelligence",
      "Attractiveness",
      "Computer science",
      "Computer vision",
      "Face (sociological concept)",
      "Feature (linguistics)",
      "Feature extraction",
      "Geometric modeling",
      "Geometric shape",
      "Geometry",
      "Landmark",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Procrustes analysis",
      "Programming language",
      "Psychoanalysis",
      "Psychology",
      "Set (abstract data type)",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Peng",
        "given_name": "Tianhao"
      },
      {
        "surname": "Li",
        "given_name": "Mu"
      },
      {
        "surname": "Chen",
        "given_name": "Fangmei"
      },
      {
        "surname": "Xu",
        "given_name": "Yong"
      },
      {
        "surname": "Zhang",
        "given_name": "David"
      }
    ]
  },
  {
    "title": "Poisson PCA for matrix count data",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109401",
    "abstract": "We develop a dimension reduction framework for data consisting of matrices of counts. Our model is based on the assumption of existence of a small amount of independent normal latent variables that drive the dependency structure of the observed data, and can be seen as the exact discrete analogue of a contaminated low-rank matrix normal model. We derive estimators for the model parameters and establish their limiting normality. An extension of a recent proposal from the literature is used to estimate the latent dimension of the model. The method is shown to outperform both its vectorization-based competitors and matrix methods assuming the continuity of the data distribution in analysing simulated data and real world abundance data.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323001024",
    "keywords": [
      "Algorithm",
      "Applied mathematics",
      "Artificial intelligence",
      "Asymptotic distribution",
      "Combinatorics",
      "Composite material",
      "Computer science",
      "Count data",
      "Dimension (graph theory)",
      "Dimensionality reduction",
      "Estimator",
      "Latent variable",
      "Latent variable model",
      "Materials science",
      "Mathematics",
      "Matrix (chemical analysis)",
      "Normality",
      "Poisson distribution",
      "Rank (graph theory)",
      "Statistics",
      "Truncation (statistics)"
    ],
    "authors": [
      {
        "surname": "Virta",
        "given_name": "Joni"
      },
      {
        "surname": "Artemiou",
        "given_name": "Andreas"
      }
    ]
  },
  {
    "title": "Positive-weighting feature enhancement for weakly supervised object localization",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.04.017",
    "abstract": "Weakly Supervised Object Localization (WSOL) techniques identify the object location by only using image-level labels, without any bonding box annotations. A restriction of these techniques is that traditionally a well-trained model only focuses on the most discriminative parts of an image, instead of the whole object. To overcome this problem, most of the WSOL methods dropped some or all crucial discriminative parts to force the model to learn from the secondary discriminative region as well. However, only a small number of methods are dedicated to improving the utilization of the weights within the deep neural network model. In this paper, we propose a plug-in module, Positive-weighting Feature Enhancement (PFE), to correctly utilize existing information embedded in positive weights. The proposed method is made up of two major components: 1) a complementary Noisy Feature Elimination loss (NFE) to minimize noisy features, and 2) a Dynamic Concealment Mask (DCM) to prevent secondary discriminative features from being eliminated while minimizing noisy features. Based on the experiments, our solution outperforms previous method without modifying the architecture on two single-object benchmark datasets, CUB-200-2011 and ILSVRC 2012, and also on one multi-objects dataset, VOC 2007.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552300123X",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Discriminative model",
      "Feature (linguistics)",
      "Feature extraction",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Linguistics",
      "Medicine",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Radiology",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Shieh",
        "given_name": "Jeng-Lun"
      },
      {
        "surname": "Yu",
        "given_name": "Sheng-Feng"
      },
      {
        "surname": "Ruan",
        "given_name": "Shanq-Jang"
      }
    ]
  },
  {
    "title": "Training circuit-based quantum classifiers through memetic algorithms",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.04.008",
    "abstract": "Among the ready-to-implement quantum algorithms, Variational Quantum Circuits (VQCs) play a key role in several applications, including machine learning. Their strength lies in the use of a parameterized quantum circuit that is trained by means of an optimization algorithm run on a classical computer. In such a scenario, there is a strong need to design appropriate classical optimization schemes that deal efficiently with VQCs and pave the way for quantum advantage in machine learning. Among possible optimization schemes, those based on evolutionary computation are finding increasing interest, given the unconventional and nonanalytical nature of the problem to be solved. This paper proposes to apply memetic algorithms to train VQCs used as quantum classifiers and shows the benefits of exploiting this evolutionary optimization technique through a comparative experimental session.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523001162",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computation",
      "Computer science",
      "Evolution strategy",
      "Evolutionary algorithm",
      "Evolutionary computation",
      "Machine learning",
      "Memetic algorithm",
      "Optimization problem",
      "Parameterized complexity",
      "Physics",
      "Quantum",
      "Quantum algorithm",
      "Quantum circuit",
      "Quantum computer",
      "Quantum machine learning",
      "Quantum mechanics",
      "Quantum network"
    ],
    "authors": [
      {
        "surname": "Acampora",
        "given_name": "Giovanni"
      },
      {
        "surname": "Chiatto",
        "given_name": "Angela"
      },
      {
        "surname": "Vitiello",
        "given_name": "Autilia"
      }
    ]
  },
  {
    "title": "Timid semi–supervised learning for face expression analysis",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109417",
    "abstract": "In the last years, semi–supervised learning has been proposed as a strategy with high potential for improving machine learning capabilities. Face expression recognition may highly benefit from such a technique, as accurate labeling is both difficult and costly, whereas millions of unlabeled images with human faces are available on the Internet, but without annotations. In this paper we evaluate the benefits of semi–supervised learning in the practical scenarios of face expression analysis. Our conclusion is that better performance is indeed achievable, but by methods that put a distinct emphasis on the diversity of exploring patterns in the unlabeled data domain. The evaluation is carried on multiple tasks such as detecting Action Units on EmotioNet, assessing Action Units intensity on the spontaneous DISFA database and, respectively, recognizing expressions on static images acquired in the wild, from the RAF-DB and FER+ databases. We show that, in these scenarios, a so–called timid semi–supervised learner is more robust and achieves higher performance than standard, confident semi–supervised learners.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323001188",
    "keywords": [
      "Action (physics)",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Domain (mathematical analysis)",
      "Expression (computer science)",
      "Face (sociological concept)",
      "Labeled data",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Programming language",
      "Quantum mechanics",
      "Semi-supervised learning",
      "Social science",
      "Sociology",
      "Supervised learning"
    ],
    "authors": [
      {
        "surname": "Badea",
        "given_name": "Mihai"
      },
      {
        "surname": "Florea",
        "given_name": "Corneliu"
      },
      {
        "surname": "Racoviţeanu",
        "given_name": "Andrei"
      },
      {
        "surname": "Florea",
        "given_name": "Laura"
      },
      {
        "surname": "Vertan",
        "given_name": "Constantin"
      }
    ]
  },
  {
    "title": "Beyond OCR + VQA: Towards end-to-end reading and reasoning for robust and accurate textvqa",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109337",
    "abstract": "Text-based visual question answering (TextVQA), which answers a visual question by considering both visual contents and scene texts, has attracted increasing attention recently. Most existing methods employ an optical character recognition (OCR) module as a pre-processor to read texts, then combine it with a visual question answering (VQA) framework. However, inaccurate OCR results may lead to cumulative error propagation, and the correlation between text reading and text-based reasoning is not fully exploited. In this work, we integrate OCR into the flow of TextVQA, targeting the mutual reinforcement of OCR and VQA tasks. Specifically, a visually enhanced text embedding module is proposed to predict semantic features from the visual information of texts, by which texts can be reasonably understood even without accurate recognition. Further, two elaborate schemes are developed to leverage contextual information in VQA to modify OCR results. The first scheme is a reading modification module that adaptively selects the answer results according to the contexts. Second, we propose an efficient end-to-end text reading and reasoning network, where the downstream VQA signal contributes to the optimization of text reading. Extensive experiments show that our method outperforms existing alternatives in terms of accuracy and robustness, whether ground truth OCR annotations are used or not.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323000389",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "End-to-end principle",
      "Gene",
      "Image (mathematics)",
      "Information retrieval",
      "Law",
      "Leverage (statistics)",
      "Machine learning",
      "Natural language processing",
      "Optical character recognition",
      "Political science",
      "Question answering",
      "Reading (process)",
      "Robustness (evolution)",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Zeng",
        "given_name": "Gangyan"
      },
      {
        "surname": "Zhang",
        "given_name": "Yuan"
      },
      {
        "surname": "Zhou",
        "given_name": "Yu"
      },
      {
        "surname": "Yang",
        "given_name": "Xiaomeng"
      },
      {
        "surname": "Jiang",
        "given_name": "Ning"
      },
      {
        "surname": "Zhao",
        "given_name": "Guoqing"
      },
      {
        "surname": "Wang",
        "given_name": "Weiping"
      },
      {
        "surname": "Yin",
        "given_name": "Xu-Cheng"
      }
    ]
  },
  {
    "title": "Expression snippet transformer for robust video-based facial expression recognition",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109368",
    "abstract": "Although Transformer can be powerful for modeling visual relations and describing complicated patterns, it could still perform unsatisfactorily for video-based facial expression recognition, since the expression movements in a video can be too small to reflect meaningful spatial-temporal relations. To this end, we propose to decompose the modeling of expression movements of a video into the modeling of a series of expression snippets, each of which contains a few frames, and then boost the Transformer’s ability for intra-snippet and inter-snippet visual modeling, respectively, obtaining the Expression snippet Transformer (EST). For intra-snippet modeling, we devise an attention-augmented snippet feature extractor to enhance the encoding of subtle facial movements of each snippet. For inter-snippet modeling, we introduce a shuffled snippet order prediction head and a corresponding loss to improve the modeling of subtle motion changes across subsequent snippets. The EST obtains state-of-the-art performance, demonstrating its superiority to other CNN-based methods. Our code and the trained model are available at https://github.com/DreamMr/EST",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323000699",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Expression (computer science)",
      "Facial expression",
      "Feature (linguistics)",
      "Information retrieval",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Programming language",
      "Quantum mechanics",
      "Snippet",
      "Speech recognition",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Yuanyuan"
      },
      {
        "surname": "Wang",
        "given_name": "Wenbin"
      },
      {
        "surname": "Feng",
        "given_name": "Chuanxu"
      },
      {
        "surname": "Zhang",
        "given_name": "Haoyu"
      },
      {
        "surname": "Chen",
        "given_name": "Zhe"
      },
      {
        "surname": "Zhan",
        "given_name": "Yibing"
      }
    ]
  },
  {
    "title": "Semi-supervised vector-valued learning: Improved bounds and algorithms",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109356",
    "abstract": "Vector-valued learning, where the output space admits a vector-valued structure, is an important problem that covers a broad family of important domains, e.g. multi-task learning and transfer learning. Using local Rademacher complexity and unlabeled data, we derive novel semi-supervised excess risk bounds for general vector-valued learning from both kernel perspective and linear perspective. The derived bounds are much sharper than existing ones and the convergence rates are improved from the square root of labeled sample size to the square root of total sample size or directly dependent on labeled sample size. Motivated by our theoretical analysis, we propose a general semi-supervised algorithm for efficiently learning vector-valued functions, incorporating both local Rademacher complexity and Laplacian regularization. Extensive experimental results illustrate the proposed algorithm significantly outperforms the compared methods, which coincides with our theoretical findings.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323000572",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Discrete mathematics",
      "Geometry",
      "Kernel (algebra)",
      "Kernel method",
      "Mathematics",
      "Perspective (graphical)",
      "Regularization (linguistics)",
      "Semi-supervised learning",
      "Square root",
      "Supervised learning",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Jian"
      },
      {
        "surname": "Liu",
        "given_name": "Yong"
      },
      {
        "surname": "Wang",
        "given_name": "Weiping"
      }
    ]
  },
  {
    "title": "RACL: A robust adaptive contrastive learning method for conversational satisfaction prediction",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109386",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323000870",
    "keywords": [
      "Artificial intelligence",
      "Communication",
      "Computer science",
      "Contrastive analysis",
      "Conversation",
      "Linguistics",
      "Machine learning",
      "Natural language processing",
      "Neuroscience",
      "Perception",
      "Philosophy",
      "Psychology",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Gang"
      },
      {
        "surname": "Li",
        "given_name": "Xiangge"
      },
      {
        "surname": "Xiao",
        "given_name": "Shuaiyong"
      },
      {
        "surname": "Zhang",
        "given_name": "Chenghong"
      },
      {
        "surname": "Lu",
        "given_name": "Xianghua"
      }
    ]
  },
  {
    "title": "MUNet: Motion uncertainty-aware semi-supervised video object segmentation",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109399",
    "abstract": "The task of semi-supervised video object segmentation (VOS) has been greatly advanced and state-of-the-art performance has been made by dense matching-based methods. The recent methods leverage space-time memory (STM) networks and learn to retrieve relevant information from all available sources, where the past frames with object masks form an external memory and the current frame as the query is segmented using the mask information in the memory. However, when forming the memory and performing matching, these methods only exploit the appearance information while ignoring the motion information. In this paper, we advocate for the return of the motion information and propose a motion uncertainty-aware framework (MUNet) for semi-supervised VOS. First, we propose an implicit method to learn the spatial correspondences between neighboring frames, building upon a correlation cost volume. To handle the challenging cases of occlusion and textureless regions during constructing dense correspondences, we incorporate the uncertainty in dense matching and achieve motion uncertainty-aware feature representation. Second, we introduce a motion-aware spatial attention module to effectively fuse the motion features with the semantic features. Comprehensive experiments on challenging benchmarks show that using a small amount of data and combining it with powerful motion information can bring a significant performance boost. We achieve 76.5 % J & F only using DAVIS17 for training 2 2 This result is initialized by the Mask-RCNN-ResNet50 weights pre-trained on COCO dataset. By initialization from ResNet50 pre-trained on ImageNet dataset, we can achieve 75.0 % J & F , which is still the SOTA performance. , which significantly outperforms the SOTA methods under the low-data protocol. The code and supplementary materials will be available at https://npucvr.github.io/MUNet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323001000",
    "keywords": [
      "Artificial intelligence",
      "Code (set theory)",
      "Computer science",
      "Computer vision",
      "Leverage (statistics)",
      "Matching (statistics)",
      "Mathematics",
      "Motion (physics)",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Programming language",
      "Segmentation",
      "Set (abstract data type)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Sun",
        "given_name": "Jiadai"
      },
      {
        "surname": "Mao",
        "given_name": "Yuxin"
      },
      {
        "surname": "Dai",
        "given_name": "Yuchao"
      },
      {
        "surname": "Zhong",
        "given_name": "Yiran"
      },
      {
        "surname": "Wang",
        "given_name": "Jianyuan"
      }
    ]
  },
  {
    "title": "MinEnt: Minimum entropy for self-supervised representation learning",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109364",
    "abstract": "Self-supervised representation learning is becoming more and more popular due to its superior performance. According to the information entropy theory, the smaller the information entropy of a feature, the more certain it is and the less redundant it is. Based on this, we propose a simple yet effective self-supervised representation learning method via Minimum Entropy (MinEnt). From the perspective of reducing information entropy, our MinEnt takes the output of the projector towards its nearest minimum entropy as the optimization target. The core of our MinEnt consists of three important steps: 1) normalize along the batch dimension to avoid model collapse, 2) compute the nearest minimum entropy to get the target, 3) compute the loss and backpropagate to optimize the network. Our MinEnt can learn efficient representations, even without the need for techniques such as negative sample pairs, predictors, momentum encoders, cross-correlation matrices, etc. Experimental results on four widely used datasets show that our method achieves competitive results in a simple manner.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323000651",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Entropy (arrow of time)",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Shuo"
      },
      {
        "surname": "Liu",
        "given_name": "Fang"
      },
      {
        "surname": "Hao",
        "given_name": "Zehua"
      },
      {
        "surname": "Jiao",
        "given_name": "Licheng"
      },
      {
        "surname": "Liu",
        "given_name": "Xu"
      },
      {
        "surname": "Guo",
        "given_name": "Yuwei"
      }
    ]
  },
  {
    "title": "Refined edge detection with cascaded and high-resolution convolutional network",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109361",
    "abstract": "Edge detection is represented as one of the most challenging tasks in computer vision, due to the complexity of detecting the edges or boundaries in real-world images that contains objects of different types and scales like trees, building as well as various backgrounds. Edge detection is represented also as a key task for many computer vision applications. Using a set of backbones as well as attention modules, deep-learning-based methods improved the detection of edges compared with traditional methods like Sobel or Canny. However, images of complex scenes still represent a challenge for these methods. Also, the detected edges using the existing approaches suffer from non-refined results with erroneous edges. In this paper, we attempted to overcome these challenges for refined edge detection using a cascaded and high-resolution network named (CHRNet). By maintaining the high resolution of edges during the training process, and conserving the resolution of the edge image during the network stage, sub-blocks are connected at every stage with the output of the previous layer. Also, after each layer, we use batch normalization layer with an active affine parameter as an erosion operation for the homogeneous region in the image. The proposed method is evaluated using the most challenging datasets including BSDS500, NYUD, and Multicue. The obtained results outperform the designed edge detection networks in terms of performance metrics and quality of output images.The code is available at: https://github.com/elharroussomar/chrnet/",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323000626",
    "keywords": [
      "Affine transformation",
      "Anthropology",
      "Artificial intelligence",
      "Canny edge detector",
      "Computer science",
      "Computer vision",
      "Edge detection",
      "Enhanced Data Rates for GSM Evolution",
      "Image (mathematics)",
      "Image processing",
      "Mathematics",
      "Normalization (sociology)",
      "Pattern recognition (psychology)",
      "Pure mathematics",
      "Sobel operator",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Elharrouss",
        "given_name": "Omar"
      },
      {
        "surname": "Hmamouche",
        "given_name": "Youssef"
      },
      {
        "surname": "Idrissi",
        "given_name": "Assia Kamal"
      },
      {
        "surname": "El Khamlichi",
        "given_name": "Btissam"
      },
      {
        "surname": "El Fallah-Seghrouchni",
        "given_name": "Amal"
      }
    ]
  },
  {
    "title": "Improving generalization of double low-rank representation using Schatten-p norm",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109352",
    "abstract": "Low-rank representation reveals a highly-informative entailment of sparse matrices, where double low-rank representation (DLRR) presents an effective solution by adopting nuclear norm. However, it is a special constraint of Schatten- p norm with p = 1 which equally treats all singular values, deviating from the optimal low-rank representation that considers p = 0 . Thus, this paper improves the DLRR generalization of DLRR by relaxing p = 1 into 0 < p ≤ 1 to tighten the low-rank constraint of the Schatten- p norm. With such a relaxation, low-rank optimization is then accelerated, resulting in a lower bound on the calculation complexity. Experiments on unsupervised feature extraction and subspace clustering demonstrate that our low-rank optimization taking 0 < p ≤ 1 achieves a superior performance against state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323000535",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Constraint (computer-aided design)",
      "Eigenvalues and eigenvectors",
      "Generalization",
      "Geometry",
      "Law",
      "Low-rank approximation",
      "Mathematical analysis",
      "Mathematics",
      "Matrix norm",
      "Norm (philosophy)",
      "Pattern recognition (psychology)",
      "Physics",
      "Political science",
      "Politics",
      "Pure mathematics",
      "Quantum mechanics",
      "Rank (graph theory)",
      "Representation (politics)",
      "Subspace topology",
      "Tensor (intrinsic definition)"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Jiaoyan"
      },
      {
        "surname": "Liang",
        "given_name": "Yongsheng"
      },
      {
        "surname": "Yi",
        "given_name": "Shuangyan"
      },
      {
        "surname": "Shen",
        "given_name": "Qiangqiang"
      },
      {
        "surname": "Cao",
        "given_name": "Xiaofeng"
      }
    ]
  },
  {
    "title": "PFGAN: Fast transformers for image synthesis",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.04.013",
    "abstract": "Recently, the Transformers have shown great potential in computer vision tasks, such as classification detection, segmentation, and image synthesis, etc. The success of Transformers has been long attributed to the attention-based token mixer. However, the computational complexity of the attention-based token mixer module is quadratic to the number of tokens to be mixed. Therefore, the attention-based token mixer module requires more parameters and will cause a very large amount of computation. As far as image synthesis task is concerned, the attention-based token mixer module increases the computation amount of generative adversarial networks (GANs) based on Transformers. To address this problem, we propose the PFGAN method. The motivation is based on our observation that the computational complexity of pooling is linear to the sequence length, without any other learnable parameters. Based on this observation, we use pooling rather than self-attention as the token mixer. Experimental results on CelebA, CIFAR-10 and LSUN datasets demonstrate that our proposed method has fewer parameters and fewer computational complexity.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523001204",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computation",
      "Computational complexity theory",
      "Computer science",
      "Computer security",
      "Electrical engineering",
      "Engineering",
      "Generative grammar",
      "Geometry",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pooling",
      "Quadratic equation",
      "Security token",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Tianguang"
      },
      {
        "surname": "Zhang",
        "given_name": "Wei"
      },
      {
        "surname": "Zhang",
        "given_name": "Zheng"
      },
      {
        "surname": "Gan",
        "given_name": "Yan"
      }
    ]
  },
  {
    "title": "Aggregated pyramid gating network for human pose estimation without pre-training",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109429",
    "abstract": "In this work, we propose a comprehensive aggregated residual gating structure, the Pyramid GAting Network (PGA-Net) for human pose estimation which can select, distill, and fuse semantic level and natural level information from multiple scales. In comparison, through utilizing multi-scale features, most existing state-of-the-art pose estimation methods are still limited in three aspects. First, multi-scale features contain massively redundant information, which is unfortunately not distilled by most existing approaches. Second, preferring deeper network structures to extract strong semantic features, the conventional methods often ignore original texture information fusion. Third, to attain a good parameter initialization, the current methods heavily rely on pre-training, which is very time-consuming or even unavailable. While better coping with the above problems, our proposed PGA-Net distills high-level semantic features and replenishes low-level original information to reinforce module representation capability. Meanwhile, PGA-Net demonstrates notable training stability and superior performance even without pre-training. Extensive experiments demonstrate that our method consistently outperforms previous approaches even without pre-training, enabling thus an end-to-end model training from scratch. In COCO benchmark, PGA-Net consistently achieves over 3% improvements than the baseline (without pre-training) under various model configurations. 1 1 The code is released at https://github.com/ssr0512/PGA-Net",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323001309",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Electrical engineering",
      "Engineering",
      "Fuse (electrical)",
      "Geodesy",
      "Geography",
      "Geometry",
      "Initialization",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Programming language",
      "Pyramid (geometry)",
      "Residual"
    ],
    "authors": [
      {
        "surname": "Jiang",
        "given_name": "Chenru"
      },
      {
        "surname": "Huang",
        "given_name": "Kaizhu"
      },
      {
        "surname": "Zhang",
        "given_name": "Shufei"
      },
      {
        "surname": "Wang",
        "given_name": "Xinheng"
      },
      {
        "surname": "Xiao",
        "given_name": "Jimin"
      },
      {
        "surname": "Goulermas",
        "given_name": "Yannis"
      }
    ]
  },
  {
    "title": "Elucidating robust learning with uncertainty-aware corruption pattern estimation",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109387",
    "abstract": "Robust learning methods aim to learn a clean target distribution from noisy and corrupted training data where a specific corruption pattern is often assumed a priori. Our proposed method can not only successfully learn the clean target distribution from a dirty dataset but also can estimate the underlying noise pattern. To this end, we leverage a mixture-of-experts model that can distinguish two different types of predictive uncertainty, aleatoric and epistemic uncertainty. We show that the ability to estimate the uncertainty plays a significant role in elucidating the corruption patterns as these two objectives are tightly intertwined. We also present a novel validation scheme for evaluating the performance of the corruption pattern estimation. Our proposed method is extensively assessed in terms of both robustness and corruption pattern estimation in the computer vision domain. Code has been made publicly available at https://github.com/jeongeun980906/Uncertainty-Aware-Robust-Learning.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323000882",
    "keywords": [
      "A priori and a posteriori",
      "Art",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Data mining",
      "Epistemology",
      "Gene",
      "Image (mathematics)",
      "Language change",
      "Leverage (statistics)",
      "Literature",
      "Machine learning",
      "Noise (video)",
      "Operating system",
      "Outlier",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Robust statistics",
      "Robustness (evolution)",
      "Source code"
    ],
    "authors": [
      {
        "surname": "Park",
        "given_name": "Jeongeun"
      },
      {
        "surname": "Shin",
        "given_name": "Seungyoun"
      },
      {
        "surname": "Hwang",
        "given_name": "Sangheum"
      },
      {
        "surname": "Choi",
        "given_name": "Sungjoon"
      }
    ]
  },
  {
    "title": "Fast computation of cluster validity measures for bregman divergences and benefits",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.05.001",
    "abstract": "Partitional clustering is one of the most relevant unsupervised learning and pattern recognition techniques. Unfortunately, one of the main drawbacks of these methodologies refer to the fact that the number of clusters is generally assumed to be known beforehand and automating its selection is not straightforward. On the same token, internal validity measures, such as the Silhouette index, Davies-Bouldin and Caliski-Harabasz measures have emerged as the standard techniques to be used when comparing the goodness of clustering results obtained via different clustering methods. These measures take into consideration both the inter and intra-cluster simmilarities and can be adapted to different metrics. Unfortunately, their used has been hindered due to their large computational complexities, which are commonly quadratic with respect to the number of instances of the data set. In this work, we show that the time complexity of computing the most popular internal validity measures can be utterly reduced by making used of the within-cluster errors and different properties of the Bregman divergences. This contribution ultimately allows us to massively speed-up the selection of an adequate number of clusters for a given data set as verified with extensive empirical comparisons.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523001290",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Cluster analysis",
      "Computation",
      "Computer science",
      "Computer security",
      "Data mining",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Programming language",
      "Security token",
      "Selection (genetic algorithm)",
      "Set (abstract data type)",
      "Silhouette",
      "Unsupervised learning"
    ],
    "authors": [
      {
        "surname": "Capó",
        "given_name": "Marco"
      },
      {
        "surname": "Pérez",
        "given_name": "Aritz"
      },
      {
        "surname": "Lozano",
        "given_name": "Jose A."
      }
    ]
  },
  {
    "title": "Self-supervised clustering with assistance from off-the-shelf classifier",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109350",
    "abstract": "Deep clustering outperforms conventional clustering by mutually promoting representation learning and cluster assignment. However, most existing deep clustering methods suffer from two major drawbacks. Firstly, most cluster assignment methods are highly dependent on the intermediate target distribution generated by a handcrafted nonlinear mapping function. Secondly, the clustering results can be easily guided towards wrong direction by the misassigned samples in each cluster. The existing deep clustering methods are incapable of discriminating such samples. These facts largely limit the possible performance that deep clustering methods can reach. To address these issues, a novel Self-Supervised Clustering (SSC) framework is constructed, which boosts the clustering performance by classification in an unsupervised manner. Fuzzy theory is used to score the membership of each sample to the clusters in terms of probability in each training epoch, which evaluates the intermediate clustering result certainty of each sample. The most reliable samples can be selected with the help of a sample selection method according to the membership and enhanced by data augmentation method. These augmented data are employed to fine-tune an off-the-shelf deep network classifier with the labels provided by the clustering in a self-supervised way. The classification results of the original dataset are used as the target distribution to guide the training process of the deep clustering model. The proposed framework can efficiently discriminate sample outliers and generate better target distribution with the assistance of the powerful classifier. Extensive experiments indicate that the proposed framework remarkably outperforms state-of-the-art deep clustering methods on four benchmark datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323000511",
    "keywords": [
      "Artificial intelligence",
      "CURE data clustering algorithm",
      "Classifier (UML)",
      "Cluster analysis",
      "Clustering high-dimensional data",
      "Computer science",
      "Correlation clustering",
      "Data mining",
      "Data stream clustering",
      "Fuzzy clustering",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Single-linkage clustering"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Hanxuan"
      },
      {
        "surname": "Lu",
        "given_name": "Na"
      },
      {
        "surname": "Luo",
        "given_name": "Huan"
      },
      {
        "surname": "Liu",
        "given_name": "Qinyang"
      }
    ]
  },
  {
    "title": "Dual-stream correlation exploration for face anti-Spoofing",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.03.022",
    "abstract": "Face anti-spoofing (FAS) is an important technology to ensure the security of face recognition system. Previous methods generally focus on the representation of the spoof patterns. However, the hidden correlations between living and spoofing faces are ignored, making the generation and discrimination of face anti-spoofing less effective. In this paper, we propose a novel Dual-Stream Correlation Exploration method (DSCE) to simultaneously model the correlation between content and liveness features for the performance improvement. Specifically, DSCE devises two novel modules: the spoof cue generation module and the source face reconstruction module, at two streams respectively. The former one introduces pseudo negative feature to extend the diversity of attack types, and adopts a metric learning strategy to learn the correlation among liveness features. The latter one integrates liveness and content features to explores the potential relationship between the both features through source face reconstruction. Last, DSCE adaptively combines spoof cues and reconstructed faces to comprehensively consider the importance of different correlations for face anti-spoofing. Comprehensive experimental results on both intra-dataset testing and cross-dataset testing clearly demonstrate the high discrimination and generalization of DSCE.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523000879",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Correlation",
      "Dual (grammatical number)",
      "Economics",
      "Face (sociological concept)",
      "Feature (linguistics)",
      "Generalization",
      "Geometry",
      "IP address spoofing",
      "Internet Protocol",
      "Law",
      "Linguistics",
      "Literature",
      "Liveness",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Metric (unit)",
      "Network address translation",
      "Operations management",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Social science",
      "Sociology",
      "Spoofing attack",
      "Subspace topology",
      "The Internet",
      "Theoretical computer science",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Yongluo"
      },
      {
        "surname": "Wu",
        "given_name": "Lifang"
      },
      {
        "surname": "Li",
        "given_name": "Zun"
      },
      {
        "surname": "Wang",
        "given_name": "Zhuming"
      }
    ]
  },
  {
    "title": "Dynamic graph structure learning for multivariate time series forecasting",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109423",
    "abstract": "Multivariate time series forecasting is a challenging task because the dynamic spatio-temporal dependencies between variables are a combination of multiple unknown association patterns. Existing graph neural networks typically model multivariate relationships with a predefined spatial graph or a learned fixed adjacency graph, which fails to handle the aforementioned challenges. In this study, we decompose association patterns into stable long-term and dynamic short-term patterns and propose a novel framework, named the static and dynamic graph learning network (SDGL), for modeling unknown patterns. Our approach infers two types of graph structures, from the data simultaneously: static and dynamic graphs. A static graph is developed to capture the fixed long-term pattern via node embedding, and we leverage graph regularity to control its learning direction. Dynamic graphs, which are time-varying matrices based on changing node-level features, are used to model dynamic dependencies over the short term. To effectively capture local dynamic patterns, we integrate the learned long-term pattern as an inductive bias. Experiments on six benchmark datasets show the state-of-the-art performance of our method. Analysis of the learned graphs reveals that the model succeeds in modeling dynamic spatio-temporal dependencies.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323001243",
    "keywords": [
      "Adjacency list",
      "Adjacency matrix",
      "Algorithm",
      "Artificial intelligence",
      "Computer network",
      "Computer science",
      "Data mining",
      "Dynamic network analysis",
      "Embedding",
      "Graph",
      "Leverage (statistics)",
      "Machine learning",
      "Multivariate statistics",
      "Theoretical computer science",
      "Time series"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Zhuo Lin"
      },
      {
        "surname": "Zhang",
        "given_name": "Gao Wei"
      },
      {
        "surname": "Yu",
        "given_name": "Jie"
      },
      {
        "surname": "Xu",
        "given_name": "Ling Yu"
      }
    ]
  },
  {
    "title": "Independent vector analysis: Model, applications, challenges",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109376",
    "abstract": "This paper overviews an appealing unsupervised learning method named independent vector analysis (IVA) for its promising applications, such as in audio/speech signal separation, medical signal processing, remote sensing, video/image processing, wireless communication processing, and so on. As a useful data-driven technique in blind source separation (BSS) field, IVA has played an increasingly vital role in dealing with the problems of convolutive mixture separation, multivariate latent variable analysis and multivariate data fusion. IVA extends the conventional independent component analysis (ICA) to multidimensional components, which can result in more available information utilization. Compared with ICA mechanism, IVA is not only to utilize the statistical independence of multivariate signals but also the statistical inner dependency of each multivariate signal. With this generalization, IVA can manipulate some prominent ill-pose issues faced in sensor receiving models and has the advantage to overcome the inherent random permutation ambiguity problem in joint BSS. Motivated by the flexible and versatile technology superiorities of IVA, this paper concentrates on reviewing the IVA model in details, associated methods briefly, and its potential applications as well as prospects. Moreover, some significant open problems about IVA challenges are also discussed in this paper.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323000778",
    "keywords": [
      "Acoustics",
      "Artificial intelligence",
      "Blind signal separation",
      "Channel (broadcasting)",
      "Computer hardware",
      "Computer network",
      "Computer science",
      "Data mining",
      "Dependency (UML)",
      "Digital signal processing",
      "Independence (probability theory)",
      "Independent component analysis",
      "Machine learning",
      "Mathematics",
      "Multivariate statistics",
      "Pattern recognition (psychology)",
      "Permutation (music)",
      "Physics",
      "Programming language",
      "SIGNAL (programming language)",
      "Signal processing",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Luo",
        "given_name": "Zhongqiang"
      }
    ]
  },
  {
    "title": "Unsupervised person re-identification via multi-domain joint learning",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109369",
    "abstract": "Deep learning techniques have achieved impressive progress in the task of person re-identification. However, how to generalize a learned model from the source domain to the target domain remains a long-standing challenge. Inspired by the fact that the enrichment of data diversity and the utilization of miscellaneous semantic features can lead to better generalization ability, we design a model that integrates a novel data augmentation method with a multi-label assignment strategy to achieve semantic features decoupling in the source domain. The pre-trained model is employed to extract several kinds of semantic features from the target dataset, and each kind of semantic features is regarded as a specific domain. We then cluster features of each domain and exploit the connection between different clustering results to perform self-distillation for generating more reliable pseudo labels. Finally, the obtained pseudo labels are used to fine-tune the pre-trained model to achieve model transfer from the source domain to the target one. Extensive experiments demonstrate that our approach outperforms some state-of-the-art methods by a clear margin and even surpass some supervised methods. Source code is available at: https://www.github.com/flychen321/MDJL.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323000705",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Cluster analysis",
      "Computer science",
      "Computer security",
      "Domain (mathematical analysis)",
      "Exploit",
      "Generalization",
      "Identification (biology)",
      "Identifier",
      "Machine learning",
      "Margin (machine learning)",
      "Mathematical analysis",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Programming language",
      "Source code"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Feng"
      },
      {
        "surname": "Wang",
        "given_name": "Nian"
      },
      {
        "surname": "Tang",
        "given_name": "Jun"
      },
      {
        "surname": "Yan",
        "given_name": "Pu"
      },
      {
        "surname": "Yu",
        "given_name": "Jun"
      }
    ]
  },
  {
    "title": "Cycle-object consistency for image-to-image domain adaptation",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109416",
    "abstract": "Recent advances in generative adversarial networks (GANs) have been proven effective in performing domain adaptation for object detectors through data augmentation. While GANs are exceptionally successful, those methods that can preserve objects well in the image-to-image translation task usually require an auxiliary task, such as semantic segmentation to prevent the image content from being too distorted. However, pixel-level annotations are difficult to obtain in practice. Alternatively, instance-aware image-translation model treats object instances and background separately. Yet, it requires object detectors at test time, assuming that off-the-shelf detectors work well in both domains. In this work, we present AugGAN-Det, which introduces Cycle-object Consistency (CoCo) loss to generate instance-aware translated images across complex domains. The object detector of the target domain is directly leveraged in generator training and guides the preserved objects in the translated images to carry target-domain appearances. Compared to previous models, which e.g., require pixel-level semantic segmentation to force the latent distribution to be object-preserving, this work only needs bounding box annotations which are significantly easier to acquire. Next, as to the instance-aware GAN models, our model, AugGAN-Det, internalizes global and object style-transfer without explicitly aligning the instance features. Most importantly, a detector is not required at test time. Experimental results demonstrate that our model outperforms recent object-preserving and instance-level models and achieves state-of-the-art detection accuracy and visual perceptual quality.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323001176",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Consistency (knowledge bases)",
      "Domain (mathematical analysis)",
      "Economics",
      "Gene",
      "Generator (circuit theory)",
      "Image (mathematics)",
      "Image processing",
      "Image translation",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Messenger RNA",
      "Minimum bounding box",
      "Object (grammar)",
      "Object detection",
      "Physics",
      "Power (physics)",
      "Quantum mechanics",
      "Segmentation",
      "Standard test image",
      "Task (project management)",
      "Translation (biology)"
    ],
    "authors": [
      {
        "surname": "Lin",
        "given_name": "Che-Tsung"
      },
      {
        "surname": "Kew",
        "given_name": "Jie-Long"
      },
      {
        "surname": "Chan",
        "given_name": "Chee Seng"
      },
      {
        "surname": "Lai",
        "given_name": "Shang-Hong"
      },
      {
        "surname": "Zach",
        "given_name": "Christopher"
      }
    ]
  },
  {
    "title": "Efficient large-scale oblique image matching based on cascade hashing and match data scheduling",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109442",
    "abstract": "In this paper, we design an efficient large-scale oblique image matching method. First, to reduce the number of redundant transmissions of match data, we propose a novel three-level buffer data scheduling (TLBDS) algorithm that considers the adjacency between images for match data scheduling from disk to graphics memory. Second, we adopt the epipolar constraint to filter the initial candidate points of cascade hashing matching, thereby significantly increasing the robustness of matching feature points. Comprehensive experiments are conducted on three oblique image datasets to test the efficiency and effectiveness of the proposed method. The experimental results show that our method can complete a match pair within 2.50 ∼ 2.64 ms, which not only is much faster than two open benchmark pipelines (i.e., OpenMVG and COLMAP) by 20.4 ∼ 97.0 times but also have higher efficiency than two state-of-the-art commercial software (i.e., Agisoft Metashape and Pix4Dmapper) by 10.4 ∼ 50.0 times.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323001425",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Cascade",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Computer security",
      "Gene",
      "Hash function",
      "Linguistics",
      "Oblique case",
      "Philosophy",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Qiyuan"
      },
      {
        "surname": "Zheng",
        "given_name": "Shunyi"
      },
      {
        "surname": "Zhang",
        "given_name": "Ce"
      },
      {
        "surname": "Wang",
        "given_name": "Xiqi"
      },
      {
        "surname": "Li",
        "given_name": "Rui"
      }
    ]
  },
  {
    "title": "Human-centered deep compositional model for handling occlusions",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109397",
    "abstract": "Despite their powerful discriminative abilities, Convolutional Neural Networks (CNNs) lack the properties of generative models. This leads to a decreased performance in environments where objects are poorly visible. Solving such a problem by adding more training samples can quickly lead to a combinatorial explosion, therefore the underlying architecture has to be changed instead. This work proposes a Human-Centered Deep Compositional model (HCDC) that combines low-level visual discrimination of a CNN and the high-level reasoning of a Hierarchical Compositional model (HCM). Defined as a transparent model, it can be optimized to real-world environments by adding compactly encoded domain knowledge from human studies and physical laws. The new FridgeNetv2 dataset and a mixture of publicly available datasets are used as a benchmark. The experimental results show the proposed model is explainable, has higher discriminative and generative power, and better handles the occlusion than the current state-of-the-art Mask-RCNN in instance segmentation tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323000985",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Discriminative model",
      "Domain (mathematical analysis)",
      "Generative grammar",
      "Generative model",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Koporec",
        "given_name": "Gregor"
      },
      {
        "surname": "Perš",
        "given_name": "Janez"
      }
    ]
  },
  {
    "title": "Multi-dimensional multi-label classification: Towards encompassing heterogeneous label spaces and multi-label annotations",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109357",
    "abstract": "In traditional classification framework, the semantics of each object is usually characterized by annotating a single class label from one homogeneous label space. Nonetheless, objects with rich semantics naturally arise in real-world applications whose properties need to be characterized in a more sophisticated manner. In this paper, a new classification framework named Multi-Dimensional Multi-Label (MDML) classification is investigated which models objects with rich semantics by encompassing heterogeneous label spaces and multi-label annotations. Specifically, MDML generalizes the traditional classification framework by assuming a number of heterogeneous label spaces to characterize semantics from different dimensions, where each object is further annotated with multiple class labels from each heterogeneous label space. To learn from MDML training examples, a first attempt named CLIM is proposed based on an augmented stacking strategy. Firstly, CLIM induces a base multi-label predictive model w.r.t. each label space by maximizing the likelihood of the observed multiple class labels. Secondly, the thresholding predictions from all base models are used to augment the original feature space to yield stacked multi-label predictive models. The two-level models are refined alternately via empirical threshold tuning. Experiments on four real-world MDML data sets validate the effectiveness of CLIM in learning from training examples with heterogeneous label spaces and multi-label annotations.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323000584",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Feature (linguistics)",
      "Feature vector",
      "Image (mathematics)",
      "Linguistics",
      "Machine learning",
      "Multi-label classification",
      "Object (grammar)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Semantics (computer science)",
      "Space (punctuation)",
      "Thresholding"
    ],
    "authors": [
      {
        "surname": "Jia",
        "given_name": "Bin-Bin"
      },
      {
        "surname": "Zhang",
        "given_name": "Min-Ling"
      }
    ]
  },
  {
    "title": "From anomaly detection to open set recognition: Bridging the gap",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109385",
    "abstract": "The classifiers that return compact acceptance regions are crucial for the success in anomaly detection and open set recognition settings since we have to determine and reject the anomalies and samples coming from the unknown classes. This paper introduces novel methods that approximate the class acceptance regions with compact hypersphere models for anomaly detection and open set recognition. As opposed to the other deep hypersphere classifiers, we treat the hypersphere centers as learnable parameters and update them based on the changing deep feature representations. In addition, we propose novel loss terms that are more robust to the noisy labels within the outlier exposure and background datasets. The proposed methods bear similarity to the deep distance metric learning classifiers using the triplet loss function with the exception that the anchors are set to the hypersphere centers which are updated dynamically. The experimental results show that the proposed methods achieve the state-of-the-art accuracies on the majority of the tested datasets in the context of anomaly detection and open set recognition.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323000869",
    "keywords": [
      "Anomaly detection",
      "Archaeology",
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Context (archaeology)",
      "Discrete mathematics",
      "Engineering",
      "Geography",
      "Hypersphere",
      "Machine learning",
      "Mathematics",
      "Metric (unit)",
      "One-class classification",
      "Open set",
      "Operations management",
      "Outlier",
      "Pattern recognition (psychology)",
      "Programming language",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Cevikalp",
        "given_name": "Hakan"
      },
      {
        "surname": "Uzun",
        "given_name": "Bedirhan"
      },
      {
        "surname": "Salk",
        "given_name": "Yusuf"
      },
      {
        "surname": "Saribas",
        "given_name": "Hasan"
      },
      {
        "surname": "Köpüklü",
        "given_name": "Okan"
      }
    ]
  },
  {
    "title": "Synwmd: Syntax-aware word Mover’s distance for sentence similarity evaluation",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.04.012",
    "abstract": "Word Mover’s Distance (WMD) computes the distance between words and models text similarity with the moving cost between words in two text sequences. Yet, it does not offer good performance in sentence similarity evaluation since it does not incorporate word importance and fails to take inherent contextual and structural information in a sentence into account. An improved WMD method using the syntactic parse tree, called Syntax-aware Word Mover’s Distance (SynWMD), is proposed to address these two shortcomings in this work. First, a weighted graph is built upon the word co-occurrence statistics extracted from the syntactic parse trees of sentences. The importance of each word is inferred from graph connectivities. Second, the local syntactic parsing structure of words is considered in computing the distance between words. To demonstrate the effectiveness of the proposed SynWMD, we conduct experiments on 6 textual semantic similarity (STS) datasets and 4 sentence classification datasets. Experimental results show that SynWMD achieves state-of-the-art performance on STS tasks. It also outperforms other WMD-based methods on sentence classification tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523001174",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Graph",
      "Image (mathematics)",
      "Linguistics",
      "Natural language processing",
      "Parse tree",
      "Parsing",
      "Philosophy",
      "Semantic similarity",
      "Sentence",
      "Similarity (geometry)",
      "Syntax",
      "Theoretical computer science",
      "Word (group theory)"
    ],
    "authors": [
      {
        "surname": "Wei",
        "given_name": "Chengwei"
      },
      {
        "surname": "Wang",
        "given_name": "Bin"
      },
      {
        "surname": "Jay Kuo",
        "given_name": "C.-C."
      }
    ]
  },
  {
    "title": "Defense Against Adversarial Attacks with Efficient Frequency-Adaptive Compression and Reconstruction",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109382",
    "abstract": "The increasing use of deep neural networks exposes themselves to adversarial attacks in the real world drawn from closed-set and open-set, which poses great threats to their application in safety-critical systems. Since adversarial attacks tend to mislead an original model by adding small perturbations into clean images, an intuitive idea of defensing adversarial attacks is eliminating perturbations as much as possible to mitigate attacking effects. However, such elimination-based strategies unfortunately fail to achieve satisfactory robustness. Aiming to investigate the intrinsic reasons for this phenomenon, systematic experiments are carried out in this paper to indicate that even a 20% residual perturbation can still preserve and exhibit attacking effects as strong as a full one. Our study also indicates that there are strong correlations between perturbations and legitimate images. Thus, breaking the correlation across multiple bands is more effective in mitigating attacking effects. Based on these findings, this paper proposes an efficient defense strategy called “Frequency-Adaptive Compression and rEconstruction (FACE)” to improve the robustness of the model to adversarial attacks. Specifically, low-frequency bands containing semantic information are compressed by a down-sampling operation, while the channel width of high-frequency bands is squeezed and further compressed by adding noise before the Tanh activating function. Meanwhile, attachment spaces of perturbations are also squeezed to the extent as much as possible. Finally, a clean output is obtained by upsampling together with expanded reconstruction. Experiments are extensively conducted on widely used datasets to demonstrate the effectiveness of the proposed method. For closed-set attacks, FACE outperforms the STOA elimination-based methods on ImageNet, achieving a 27.9% improvement. For the MNIST open-set attacks, it not only reduces the success rate of targeted attack by a large margin (from 100% to 24.7%), but also mitigates attacking effects with an FPR-95 value of 0.3.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323000833",
    "keywords": [
      "Adversarial system",
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Deep neural networks",
      "Gene",
      "Programming language",
      "Residual",
      "Robustness (evolution)",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Niu",
        "given_name": "Zhong-Han"
      },
      {
        "surname": "Yang",
        "given_name": "Yu-Bin"
      }
    ]
  },
  {
    "title": "Boosting transferability of physical attack against detectors by redistributing separable attention",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109435",
    "abstract": "The research on attack transferability is of great importance as it can guide how to conduct an adversarial attack without knowing any information about target models. However, it remains challenging for adversarial examples to maintain a good attack transferability performance, especially for the black-box attack implemented in the physical world. To enhance black-box transferability of physical attacks on object detectors, we present a novel adversarial learning method to produce adversarial patches by redistributing separable attention maps. Concretely, we first develop smoothed multilayer attention maps by introducing serial composite transformations, which could suppress model-specific noise on the one hand, and cover objects to be concealed at various resolutions on the other hand. Besides, our method resorts to a scalable mask to separate object attention from the background and adjust their distribution with a novel loss function. Extensive experiments show that our approach outperforms state-of-the-art methods in both the digital space and the physical world. Our code is available at https://github.com/zhangyu13a/transPhyAtt.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032300136X",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Boosting (machine learning)",
      "Computer science",
      "Data mining",
      "Database",
      "Logit",
      "Machine learning",
      "Object (grammar)",
      "Programming language",
      "Scalability",
      "Source code",
      "Transferability"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Yu"
      },
      {
        "surname": "Gong",
        "given_name": "Zhiqiang"
      },
      {
        "surname": "Zhang",
        "given_name": "Yichuang"
      },
      {
        "surname": "Bin",
        "given_name": "Kangcheng"
      },
      {
        "surname": "Li",
        "given_name": "Yongqian"
      },
      {
        "surname": "Qi",
        "given_name": "Jiahao"
      },
      {
        "surname": "Wen",
        "given_name": "Hao"
      },
      {
        "surname": "Zhong",
        "given_name": "Ping"
      }
    ]
  },
  {
    "title": "Robust spherical principal curves",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109380",
    "abstract": "Principal curves are a nonlinear generalization of principal components and go through the mean of data lying in Euclidean space. In this paper, we propose L 1 -type and Huber-type principal curves through the median of data to robustify the principal curves for a dataset that may contain outliers. We further investigate the stationarity of the proposed robust principal curves on S 2 . Results from numerical experiments on S 2 and S 4 , including real data analysis, manifest promising empirical features of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032300081X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Curvature",
      "Euclidean distance",
      "Generalization",
      "Geometry",
      "Mathematical analysis",
      "Mathematics",
      "Mean curvature",
      "Operating system",
      "Outlier",
      "Pattern recognition (psychology)",
      "Principal (computer security)",
      "Principal component analysis",
      "Principal curvature"
    ],
    "authors": [
      {
        "surname": "Lee",
        "given_name": "Jongmin"
      },
      {
        "surname": "Oh",
        "given_name": "Hee-Seok"
      }
    ]
  },
  {
    "title": "K-Means for noise-insensitive multi-dimensional feature learning",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.04.009",
    "abstract": "Many measurement modalities which perform imaging by probing an object pixel-by-pixel, such as via Photoacoustic Microscopy, produce a multi-dimensional feature (typically a time-domain signal) at each pixel. In principle, the many degrees of freedom in the time-domain signal would admit the possibility of significant multi-modal information being implicitly present, much more than a single scalar “brightness”, regarding the underlying targets being observed. However, the measured signal is neither a weighted-sum of basis functions (such as principal components) nor one of a set of prototypes (K-means), which has motivated the novel clustering method proposed here. Signals are clustered based on their shape, but not amplitude, via angular distance, and centroids are calculated as the direction of maximal intra-cluster variance, resulting in a clustering algorithm capable of learning centroids (signal shapes) that are related to the underlying, albeit unknown, target characteristics in a scalable and noise-robust manner.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523001150",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Centroid",
      "Cluster analysis",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Image (mathematics)",
      "Linguistics",
      "Mathematics",
      "Noise (video)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pixel",
      "Programming language",
      "SIGNAL (programming language)"
    ],
    "authors": [
      {
        "surname": "Pellegrino",
        "given_name": "Nicholas"
      },
      {
        "surname": "Fieguth",
        "given_name": "Paul W."
      },
      {
        "surname": "Haji Reza",
        "given_name": "Parsin"
      }
    ]
  },
  {
    "title": "Learning from multiple annotators for medical image segmentation",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109400",
    "abstract": "Supervised machine learning methods have been widely developed for segmentation tasks in recent years. However, the quality of labels has high impact on the predictive performance of these algorithms. This issue is particularly acute in the medical image domain, where both the cost of annotation and the inter-observer variability are high. Different human experts contribute estimates of the ”actual” segmentation labels in a typical label acquisition process, influenced by their personal biases and competency levels. The performance of automatic segmentation algorithms is limited when these noisy labels are used as the expert consensus label. In this work, we use two coupled CNNs to jointly learn, from purely noisy observations alone, the reliability of individual annotators and the expert consensus label distributions. The separation of the two is achieved by maximally describing the annotator’s “unreliable behavior” (we call it “maximally unreliable”) while achieving high fidelity with the noisy training data. We first create a toy segmentation dataset using MNIST and investigate the properties of the proposed algorithm. We then use three public medical imaging segmentation datasets to demonstrate our method’s efficacy, including both simulated (where necessary) and real-world annotations: 1) ISBI2015 (multiple-sclerosis lesions); 2) BraTS (brain tumors); 3) LIDC-IDRI (lung abnormalities). Finally, we create a real-world multiple sclerosis lesion dataset (QSMSC at UCL: Queen Square Multiple Sclerosis Center at UCL, UK) with manual segmentations from 4 different annotators (3 radiologists with different level skills and 1 expert to generate the expert consensus label). In all datasets, our method consistently outperforms competing methods and relevant baselines, especially when the number of annotations is small and the amount of disagreement is large. The studies also reveal that the system is capable of capturing the complicated spatial characteristics of annotators’ mistakes.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323001012",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Ground truth",
      "Image segmentation",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Le"
      },
      {
        "surname": "Tanno",
        "given_name": "Ryutaro"
      },
      {
        "surname": "Xu",
        "given_name": "Moucheng"
      },
      {
        "surname": "Huang",
        "given_name": "Yawen"
      },
      {
        "surname": "Bronik",
        "given_name": "Kevin"
      },
      {
        "surname": "Jin",
        "given_name": "Chen"
      },
      {
        "surname": "Jacob",
        "given_name": "Joseph"
      },
      {
        "surname": "Zheng",
        "given_name": "Yefeng"
      },
      {
        "surname": "Shao",
        "given_name": "Ling"
      },
      {
        "surname": "Ciccarelli",
        "given_name": "Olga"
      },
      {
        "surname": "Barkhof",
        "given_name": "Frederik"
      },
      {
        "surname": "Alexander",
        "given_name": "Daniel C."
      }
    ]
  },
  {
    "title": "ProUDA: Progressive unsupervised data augmentation for semi-Supervised 3D object detection on point cloud",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.04.002",
    "abstract": "Unsupervised data augmentation (UDA) is a simple and general semi-supervised learning (SSL) framework. However, for the task of semi-supervised 3D object detection (SSOD-3D), due to the impact of object occlusion and point cloud resolution, the quality of pseudo labels is uncertain so that the performance of UDA is limited. In this paper, We propose an efficient and novel SSL framework progressive unsupervised data augmentation (ProUDA). At first, to minimize the overfitting risk of the inaccurate pseudo labels, we sort the unlabeled samples by prediction complexity, and present a progressive consistency loss to adjust the usage ratio of the unlabeled samples. After that, we employ a strategy of iteration check to select the best learning result using a few but representative validation dataset annotated from the unlabeled samples. It ensures the safe model weight updating. Extensive experiments are conducted on both the public indoor and outdoor 3D object detection datasets. Results demonstrate that ProUDA has better 3D average precision than UDA and the proposed method benefits to 3D object detector training.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523001034",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Consistency (knowledge bases)",
      "Data mining",
      "Database",
      "Machine learning",
      "Object (grammar)",
      "Object detection",
      "Overfitting",
      "Pattern recognition (psychology)",
      "Point cloud",
      "Supervised learning",
      "Unsupervised learning",
      "sort"
    ],
    "authors": [
      {
        "surname": "An",
        "given_name": "Pei"
      },
      {
        "surname": "Liang",
        "given_name": "Junxiong"
      },
      {
        "surname": "Ma",
        "given_name": "Tao"
      },
      {
        "surname": "Chen",
        "given_name": "Yanfei"
      },
      {
        "surname": "Wang",
        "given_name": "Liheng"
      },
      {
        "surname": "Ma",
        "given_name": "Jie"
      }
    ]
  },
  {
    "title": "Incorporating multi-stage spatial visual cues and active localization offset for pancreas segmentation",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.05.004",
    "abstract": "Accurately segmenting pancreas or pancreatic tumor from limited computed tomography (CT) scans plays an essential role in making a precise diagnosis and planning the surgical procedure for clinicians. Although deep convolutional neural networks (DCNNs) have greatly advanced in automatic organ segmentation, there are still many challenges in solving the pancreas segmentation problem with small region and complex background. Many researchers have developed a coarse-to-fine scheme, which employ prediction from the coarse stage as a smaller input region for the fine stage. Despite this scheme effectiveness, most existing approaches handle two stages individually, and fail to identify the reliability of coarse stage predictions. In this work, we present a novel coarse-to-fine framework based on spatial contextual cues and active localization offset. The novelty lies in carefully designed two modules: Spacial Visual Cues Fusion (SVCF) and Active Localization OffseT (ALOT). The SVCF combines the correlations between all pixels in an image to optimize the rough and uncertain pixel prediction at the coarse stage, while ALOT dynamically adjusts the localization as the coarse stage iteration. These two modules work together to optimize the coarse stage results and provide high-quality input for the fine stage, thereby achieving inspiring target segmentation. Empirical results on NIH pancreas segmentation and MSD pancreatic tumor segmentation dataset show that our framework yields state-of-the-art results. The code will make available at https://github.com/PinkGhost0812/SANet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523001319",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Novelty",
      "Offset (computer science)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pixel",
      "Programming language",
      "Segmentation",
      "Theology"
    ],
    "authors": [
      {
        "surname": "Ju",
        "given_name": "Jianguo"
      },
      {
        "surname": "Li",
        "given_name": "Jiaming"
      },
      {
        "surname": "Chang",
        "given_name": "Zhengqi"
      },
      {
        "surname": "Liang",
        "given_name": "Ying"
      },
      {
        "surname": "Guan",
        "given_name": "Ziyu"
      },
      {
        "surname": "Xu",
        "given_name": "Pengfei"
      },
      {
        "surname": "Xie",
        "given_name": "Fei"
      },
      {
        "surname": "Wang",
        "given_name": "Hexu"
      }
    ]
  },
  {
    "title": "Question-aware dynamic scene graph of local semantic representation learning for visual question answering",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.04.014",
    "abstract": "In visual question answering task, it is vital to learn the semantic interactions between the question and target objects in the input image. Existing scene graph-based methods generally extract global features from the image and then perform feature fusion with the question representation. However, the scene graph constructed by these methods only obtains the abstract semantic features from the image, but does not consider the influence of the positional words and semantic information in question. In this paper, we propose a Question-aware Dynamic Scene Graph (QDSG) method. Firstly, we adopt a scene graph of the initial state based on the local attribute features of the image target. Then we design a dynamic scene graph adaptive to different questions based on the initial scene graph, which is used a word-level co-attention mechanism to refine node features and edge features. Finally, iterative reasoning is performed on the refined scene graph and the correct answer is predicted by using the graph attention network model. The proposed method is sufficient to learn the semantic local features to generate the interactive scene graph between the image and question, which is beneficial to the logistic reasoning depending on the adaptive graph refinement. The proposed method outperforms the comparative performance when compared with state-of-the-art models on the GQA dataset and its semantic and structural type datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523001216",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Graph",
      "Pattern recognition (psychology)",
      "Rendering (computer graphics)",
      "Scene graph",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Jinmeng"
      },
      {
        "surname": "Ge",
        "given_name": "Fulin"
      },
      {
        "surname": "Hong",
        "given_name": "Hanyu"
      },
      {
        "surname": "Shi",
        "given_name": "Yu"
      },
      {
        "surname": "Hao",
        "given_name": "Yanbin"
      },
      {
        "surname": "Ma",
        "given_name": "Lei"
      }
    ]
  },
  {
    "title": "Learning visual question answering on controlled semantic noisy labels",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109339",
    "abstract": "Visual Question Answering (VQA) has made great progress recently due to the increasing ability to understand and encode multi-modal inputs based on deep learning. However, existing VQA models are usually based on assumptions of clean labels, and it is contradictory to real scenarios where labels are expensive and inevitably contain noises. In this paper, we take the lead in addressing this issue by establishing the first benchmark of controlled semantic noisy labels for VQA task, evaluating existing methods, and coming up with corresponding solutions. Specifically, through analyzing human labels of existing VQA datasets, we first design a controlled semantic label noise by imitating human mislabeling behavior, which is more reasonable than conventional random noise. Then, we evaluate several popular VQA models on these new benchmark datasets and show that their performance degrades significantly compared to the original setting. To this end, we propose a Semantic Noisy Label Correction (SNLC) to mitigate impacts of noisy labels, including a Semantic Cross-Entropy (SCE) loss and a Semantic Embedding Contrastive (SEC) loss. Extensive experiments demonstrate the effectiveness of the proposed method SNLC. The proposed approach achieves a stable improvement on several existing models. The source code is available at https://github.com/zchoi/SNLC.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323000407",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Deep learning",
      "ENCODE",
      "Economics",
      "Embedding",
      "Gene",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Machine learning",
      "Management",
      "Natural language processing",
      "Noise (video)",
      "Question answering",
      "Semantic similarity",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Haonan"
      },
      {
        "surname": "Zeng",
        "given_name": "Pengpeng"
      },
      {
        "surname": "Hu",
        "given_name": "Yuxuan"
      },
      {
        "surname": "Qian",
        "given_name": "Jin"
      },
      {
        "surname": "Song",
        "given_name": "Jingkuan"
      },
      {
        "surname": "Gao",
        "given_name": "Lianli"
      }
    ]
  },
  {
    "title": "Self-taught Multi-view Spectral Clustering",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109349",
    "abstract": "By integrating multiple views, i.e., multi-view learning (ML), we can discover the underlying data structures so that the performance of learning tasks can improve. As a basic and important branch of ML, multi-view clustering has achieved great success recently in pattern recognition and machine learning communities. Most existing multi-view spectral clustering methods heavily adopt the relax-and-discretize strategy to obtain discrete cluster labels (clustering results), i.e., using predefined similarity graphs to learn a consensus Laplacian embedding shared by all views for K -means clustering. However, the above clustering strategy may significantly affect clustering performance since there is information loss between independent steps. In this paper, we establish a novel Self-taught Multi-view Spectral Clustering (SMSC) framework to address the above issue. As the main contributions of this paper, we provide two versions of SMSC based on convex combination and centroid graph fusion schemes. Specifically, a self-taught mechanism is introduced in SMSC, which can effectively feedback the manifold structure induced by Laplacian embedding and the cluster information hidden in the discrete indicator matrix to learn an optimal consensus similarity graph for graph partitioning. The effectiveness of the proposed methods has been evaluated on real-world multi-view datasets, and experimental results show that our methods outperform other state-of-the-art baselines.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032300050X",
    "keywords": [
      "Artificial intelligence",
      "CURE data clustering algorithm",
      "Centroid",
      "Cluster analysis",
      "Computer science",
      "Consensus clustering",
      "Correlation clustering",
      "Data mining",
      "Embedding",
      "Graph",
      "Image (mathematics)",
      "Laplacian matrix",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Similarity (geometry)",
      "Spectral clustering",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Zhong",
        "given_name": "Guo"
      },
      {
        "surname": "Pun",
        "given_name": "Chi-Man"
      }
    ]
  },
  {
    "title": "ThumbDet: One thumbnail image is enough for object detection",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109424",
    "abstract": "Computer vision fields have witnessed great success thanks to deep convolutional neural networks (CNNs). However, state-of-the-art methods often benefit from large models and datasets, which introduce heavy parameters and computational requirements. Deploying such large models in real-world applications is very difficult because of the limited computing resources. Although many researchers focus on designing efficient block structures to compress model parameters, they ignore that the role of large-scale input images is also an important factor for algorithm efficiency. Reducing input resolution is a useful method to boost runtime efficiency, however, traditional interpolation methods assume a fixed degradation criterion that greatly hurts performance. To solve the above problems, in this paper, we propose a novel framework named ThumbDet for reducing model computation while maintaining detection accuracy. In our framework, we first design an image down-sampling module to learn a small-scale image that looks realistic and contains discriminative properties. Furthermore, we propose a distillation-boost supervision strategy to maintain the detection performance of small-scaled images as the original-size inputs. Extensive experiments conducted on a standard object detection dataset MS COCO demonstrate the effectiveness of the proposed method when using very low-resolution images (i.e. 4 × down-sampling) as inputs. In particular, ThumbDet achieves satisfactory detection performance (i.e. 32.3% in mAP) while drastically reducing computation and memory requirements (i.e. speed up of 1.26 × ), outperforming the traditional interpolation methods (e.g. bicubic) by +3.2% absolutely in terms of mAP.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323001255",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Bicubic interpolation",
      "Block (permutation group theory)",
      "Computation",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Data mining",
      "Discriminative model",
      "Geometry",
      "Image (mathematics)",
      "Interpolation (computer graphics)",
      "Linear interpolation",
      "Mathematics",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Yongqiang"
      },
      {
        "surname": "Zhang",
        "given_name": "Yin"
      },
      {
        "surname": "Tian",
        "given_name": "Rui"
      },
      {
        "surname": "Zhang",
        "given_name": "Zian"
      },
      {
        "surname": "Bai",
        "given_name": "Yancheng"
      },
      {
        "surname": "Zuo",
        "given_name": "Wangmeng"
      },
      {
        "surname": "Ding",
        "given_name": "Mingli"
      }
    ]
  },
  {
    "title": "AFI-GAN: Improving feature interpolation of feature pyramid networks via adversarial training for object detection",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109365",
    "abstract": "Recent convolutional detectors learn strong semantic features by generating and combining multi-scale features via feature interpolation. However, simple interpolation incurs often noisy and blurred features. To resolve this, we propose a novel adversarially-trained interpolator which can substitute for the traditional interpolation effortlessly. In specific, we design AFI-GAN consisting of an AF interpolator and a feature patch discriminator. In addition, we present a progressive adversarial learning and AFI-GAN losses to generate multi-scale features for downstream detection tasks. However, we can also finetune the proposed AFI-GAN with the recent multi-scale detectors without the adversarial learning once a pre-trained AF interpolator is provided. We prove the effectiveness and flexibility of our AF interpolator, and achieve the better box and mask APs by 2.2% and 1.6% on average compared to using other interpolation. Moreover, we achieve an impressive detection score of 57.3% mAP on the MSCOCO dataset. Code is available at https://github.com/inhavl-shlee/AFI-GAN.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323000663",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Code (set theory)",
      "Computer science",
      "Computer vision",
      "Detector",
      "Discriminator",
      "Feature (linguistics)",
      "Geometry",
      "Image (mathematics)",
      "Interpolation (computer graphics)",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Pyramid (geometry)",
      "Set (abstract data type)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Lee",
        "given_name": "Seong-Ho"
      },
      {
        "surname": "Bae",
        "given_name": "Seung-Hwan"
      }
    ]
  },
  {
    "title": "Geometric algebra-based multiview interaction networks for 3D human motion prediction",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109427",
    "abstract": "3D skeleton-based human motion prediction is an essential and challenging task for human-machine interactions, which aims to forecasts future poses given a history of their previous motions. Recent works based on Graph Neural Networks (GCNs) show promising performance for motion prediction due to the powerful ability of feature aggregation of GCNs. However, with the deep and multi-stage GCN model deployment, its feature extraction mechanism tends to result in feature similarity over all joints, and degrade the prediction performance. In addition, such a graph structure in recent works was still insufficient to process the high dimensional structural data in Euclidean space when inference through multi-layer networks. To solve the problem, we propose a novel Geometric Algebra-based Multi-view Interaction network (GA-MIN), which captures and aggregates motion features from two interactions: 1) global-interaction, which refactors various spectrum dependencies using geometric algebra-based structure, and 2) self-interaction, which leverage self-attention mechanism to capture compact representations. Extensive experiments are conducted on three public datasets: Human3.6M, CMU Mocap, and 3DPW, which prove that the proposed GA-MIN outperforms state-of-the-art methods on 3D Mean Per Joint Position Error (MPJPE) and Mean Angle Error (MAE) on average.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323001280",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Euclidean space",
      "Graph",
      "Inference",
      "Leverage (statistics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pure mathematics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Zhong",
        "given_name": "Jianqi"
      },
      {
        "surname": "Cao",
        "given_name": "Wenming"
      }
    ]
  },
  {
    "title": "Sparse possibilistic c-means clustering with Lasso",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109348",
    "abstract": "Krishnapuram and Keller first proposed possibilistic c-means (PCM) clustering in 1993. Afterward, PCM was widely studied with various extensions. The PCM algorithm and its extensions always treat feature components under equal importance, but, in real applications, different features may better have different weights. Recently, Yang and Benjamin in 2021 proposed a feature-weighted PCM clustering with feature reduction. Although Yang and Benjamin (2021) can reduce feature dimensions, it still encounters the curse of dimensionality for high dimensional data. One possible way to address this problem is to conduct a sparse clustering technique. In this paper, we further study the PCM clustering by incorporating the idea of sparsity with different feature weights. We propose two approaches that use the PCM clustering with the least absolute shrinkage and selection operator (Lasso). The first one is the sparse PCM subject to a Lasso constraint of feature weights, called S-PCM1. The second is the sparse PCM by adding a Lasso penalty term of feature weights in the objective function, called S-PCM2. We show that S-PCM1 and S-PCM2 are theoretically the same, and both can induce sparsity in features, but they use different procedures in algorithms. Synthetic and real data sets are used to compare S-PCM1 and S-PCM2 with some existing sparsity clustering algorithms. Experimental results and comparisons demonstrate the effectiveness and usefulness of the proposed S-PCM1 and S-PCM2 clustering algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323000493",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Cluster analysis",
      "Clustering high-dimensional data",
      "Computer science",
      "Curse of dimensionality",
      "Feature (linguistics)",
      "Feature selection",
      "Lasso (programming language)",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Miin-Shen"
      },
      {
        "surname": "Benjamin",
        "given_name": "Josephine B.M."
      }
    ]
  },
  {
    "title": "Versatile recurrent neural network for wide types of video restoration",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109360",
    "abstract": "Video shooting of natural scenes often suffers from various serious degradation, such as motion blur, impact of atmospheric turbulence, random noise and resolution reduction, etc. Different from the maturity of image restoration research, video restoration is much more complicated so that it lacks effective general method. Here, we present a versatile recurrent neural network (VRNN) to handle wide types of video degradation and generate stable videos with ideal clarity. We complete the design of VRNN through deducing a general video restoration paradigm that reveals the importance of simultaneously utilizing past and future information for restoring current frame. Specifically, we propose a novel RNN cell in which hidden state flows in bidirections, enriching temporal information contained in the extracted features. Furthermore, a feature fusion module involves temporal and spatial attention processing is designed to refine features of neighbouring frames and help reconstruct current frame. Extensive experiments on well-known public datasets (including four different kinds of video restoration tasks, with a total of 35,666 videos and 515,774 frames) show that the proposed VRNN achieves 1–4 dB of PSNR increasing or several times less of computational complexity in all tasks against state-of-the-art methods, manifesting the versatile and efficient ability of proposed VRNN in wide types of video restoration.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323000614",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Frame (networking)",
      "Image (mathematics)",
      "Image processing",
      "Image restoration",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Telecommunications",
      "Video processing"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Yadong"
      },
      {
        "surname": "Bai",
        "given_name": "Xiangzhi"
      }
    ]
  },
  {
    "title": "Global and local structure preserving nonnegative subspace clustering",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109388",
    "abstract": "Most subspace clustering methods construct the similarity matrix based on self-expressive property and apply the spectral relaxation on the similarity matrix to get the final clusters. Despite the advantages of this framework, it has two limitations that are easily ignored. Firstly, the original self-expressive model only considers the global structure of data, and the ubiquitous local structure among data is not paid enough attention. Secondly, spectral relaxation is naturally suitable for 2-way clustering tasks, but when dealing with multi-way clustering tasks, the assignment of cluster members becomes indirect and requires additional steps. To overcome these problems, this paper proposes a global and local structure preserving nonnegative subspace clustering method, which learns data similarities and cluster indicators in a mutually enhanced way within a unified framework. Besides, the model is extended to kernel space to strengthen its capability of dealing with nonlinear data structures. For optimizing the objective function of the method, multiplicative updating rules based on nonnegative Lagrangian relaxation are developed, and the convergence is guaranteed in theory. Abundant experiments have shown that the proposed model is better than many advanced clustering methods in most cases.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323000894",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "CURE data clustering algorithm",
      "Cluster analysis",
      "Combinatorics",
      "Computer science",
      "Constrained clustering",
      "Data mining",
      "Fuzzy clustering",
      "Image (mathematics)",
      "Kernel (algebra)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Psychology",
      "Relaxation (psychology)",
      "Similarity (geometry)",
      "Social psychology",
      "Spectral clustering",
      "Subspace topology"
    ],
    "authors": [
      {
        "surname": "Jia",
        "given_name": "Hongjie"
      },
      {
        "surname": "Zhu",
        "given_name": "Dongxia"
      },
      {
        "surname": "Huang",
        "given_name": "Longxia"
      },
      {
        "surname": "Mao",
        "given_name": "Qirong"
      },
      {
        "surname": "Wang",
        "given_name": "Liangjun"
      },
      {
        "surname": "Song",
        "given_name": "Heping"
      }
    ]
  },
  {
    "title": "Cascaded feature fusion with multi-level self-attention mechanism for object detection",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109377",
    "abstract": "Object detection has been a challenging task due to the complexity and diversity of objects. The emergence of self-attention mechanism provides a new clue for feature fusion in object detection task. Most existing self-attention mechanisms focus on extracting the correlation between global and local information in space or among channels, however it remains problematic issues of how to effectively fuse all those features. To address the above problems, we propose a Pooling and Global feature Fusion Self-attention Mechanism (PGFSM) to capture multi-level correlations among a variety of features, so as to perform cascaded aggregations upon them. PGFSM consists of three parts: Spatial Self-attention Pooling Fusion Module (SSPFM), Channel Self-attention Pooling Fusion Module (CSPFM), and Spatial and Channel Global Self-attention Fusion Module (SCGSFM). SSPFM and CSPFM respectively carried out in space and channel, extract the global maximum pooling and global average pooling self-attention features; SCGSFM extracts the spatial and channel fused characteristic relationship in the global. Finally, the three fused feature relations are added on the original feature to achieve an enhanced trait representation. In test, our PGFSM is embedded into YOLOv4, YOLOv5, and EfficientDet network respectively, and evaluated in PASCAL VOC and MS COCO datasets. The experiment results show that the feature fusion self-attention mechanism improves the performance of object detection compared to each original framework and also the state-of-the-art modules, which proves the effectiveness of our method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032300078X",
    "keywords": [
      "Artificial intelligence",
      "Channel (broadcasting)",
      "Computer network",
      "Computer science",
      "Feature (linguistics)",
      "Fusion",
      "Fusion mechanism",
      "Linguistics",
      "Lipid bilayer fusion",
      "Object detection",
      "Pascal (unit)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pooling",
      "Programming language"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Chuanxu"
      },
      {
        "surname": "Wang",
        "given_name": "Huiru"
      }
    ]
  },
  {
    "title": "Elaborate multi-task subspace learning with discrete group constraint",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109515",
    "abstract": "In multi-task learning (MTL), multiple related tasks can be learned simultaneously under the shared information to improve the generalization performance. However, most of MTL methods assume that all the learning tasks are related indeed and appropriate for joint learning. In some real situations, this assumption may not hold and further lead to the problem of negative transfer. Therefore, in this paper, we not only focus on researching the problem of robustly learning the common feature structure shared by tasks, but also discriminate with which tasks one task should share. By combining with the idea of subspace learning, we propose an elaborate multi-task subspace learning model (EMTSL) with discrete group structure constraint, which can cluster the learned tasks into a set of groups. By introducing the Schatten p -norm instead of trace norm, our model EMTSL can better approximate the low-rank constraint and also avoid the trivial solution. Furthermore, we design an efficient algorithm based on the re-weighted method to solve the proposed model. In addition, the convergence analysis of our algorithm is given in this paper. Experimental results on both synthetic and real-world datasets demonstrate the superiority of our method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323002157",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Constraint (computer-aided design)",
      "Convergence (economics)",
      "Economic growth",
      "Economics",
      "Generalization",
      "Geometry",
      "Law",
      "Machine learning",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Multi-task learning",
      "Norm (philosophy)",
      "Political science",
      "Subspace topology",
      "Task (project management)",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Chang",
        "given_name": "Wei"
      },
      {
        "surname": "Nie",
        "given_name": "Feiping"
      },
      {
        "surname": "Wang",
        "given_name": "Rong"
      },
      {
        "surname": "Li",
        "given_name": "Xuelong"
      }
    ]
  },
  {
    "title": "Skeleton estimation of directed acyclic graphs using partial least squares from correlated data",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109460",
    "abstract": "Directed acyclic graphs (DAGs) are directed graphical models that are well known for discovering causal relationships between variables in a high-dimensional setting. When the DAG is not identifiable due to the lack of interventional data, the skeleton can be estimated using observational data, which is formed by removing the direction of the edges in a DAG. In real data analyses, variables are often highly correlated due to some form of clustered sampling, and ignoring this correlation will inflate the standard errors of the parameter estimates in the regression-based DAG structure learning framework. In this work, we propose a two-stage DAG skeleton estimation approach for highly correlated data. First, we propose a novel neighborhood selection method based on sparse partial least squares (PLS) regression, and a cluster-weighted adaptive penalty is imposed on the PLS weight vectors to exploit the local information. In the second stage, the DAG skeleton is estimated by evaluating a set of conditional independence hypotheses. Simulation studies are presented to demonstrate the effectiveness of the proposed method. The algorithm is also tested on publicly available datasets, and we show that our algorithm obtains higher sensitivity with comparable false discovery rates for high-dimensional data under different network structures.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323001607",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Conditional independence",
      "Data mining",
      "Data set",
      "Directed acyclic graph",
      "Independence (probability theory)",
      "Mathematics",
      "Partial least squares regression",
      "Pattern recognition (psychology)",
      "Programming language",
      "Regression",
      "Set (abstract data type)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Xiaokang"
      },
      {
        "surname": "Lu",
        "given_name": "Shan"
      },
      {
        "surname": "Zhou",
        "given_name": "Rui"
      },
      {
        "surname": "Wang",
        "given_name": "Huiwen"
      }
    ]
  },
  {
    "title": "Deep embedded clustering with distribution consistency preservation for attributed networks",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109469",
    "abstract": "Many complex systems in the real world can be characterized as attributed networks. To mine the potential information in these networks, deep embedded clustering, which obtains node representations and clusters simultaneously, has been given much attention in recent years. Under the assumption of consistency for data in different views, the cluster structure of network topology and that of node attributes should be consistent for an attributed network. However, many existing methods ignore this property, even though they separately encode node representations from network topology and node attributes and cluster nodes on representation vectors learned from one of the views. Therefore, in this study, we propose an end-to-end deep embedded clustering model for attributed networks. It utilizes graph autoencoder and node attribute autoencoder to learn node representations and cluster assignments. In addition, a distribution consistency constraint is introduced to maintain the latent consistency of cluster distributions in two views. Extensive experiments on several datasets demonstrate that the proposed model achieves significantly better or competitive performance compared with the state-of-the-art methods. The source code can be found at https://github.com/Zhengymm/DCP.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323001693",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Autoencoder",
      "Biochemistry",
      "Chemistry",
      "Cluster (spacecraft)",
      "Cluster analysis",
      "Clustering coefficient",
      "Combinatorics",
      "Computer network",
      "Computer science",
      "Consistency (knowledge bases)",
      "Constraint (computer-aided design)",
      "Data mining",
      "ENCODE",
      "Engineering",
      "Gene",
      "Geometry",
      "Graph",
      "Law",
      "Mathematics",
      "Node (physics)",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Structural engineering",
      "Theoretical computer science",
      "Topology (electrical circuits)"
    ],
    "authors": [
      {
        "surname": "Zheng",
        "given_name": "Yimei"
      },
      {
        "surname": "Jia",
        "given_name": "Caiyan"
      },
      {
        "surname": "Yu",
        "given_name": "Jian"
      },
      {
        "surname": "Li",
        "given_name": "Xuanya"
      }
    ]
  },
  {
    "title": "Dual-decoder transformer network for answer grounding in visual question answering",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.04.003",
    "abstract": "Visual Question Answering (VQA) have made stunning advances by exploiting Transformer architecture and large-scale visual-linguistic pretraining. State-of-the-art methods generally require large amounts of data and devices to predict textualized answers and fail to provide visualized evidence of the answers. To mitigate these limitations, we propose a novel dual-decoder Transformer network (DDTN) for predicting the language answer and corresponding vision instance. Specifically, the linguistic features are first embedded by Long Short-Term Memory (LSTM) block and Transformer encoder, which are shared between the Transformer dual-decoder. Then, we introduce object detector to obtain vision region features and grid features for reducing the size and cost of DDTN. These visual features are combined with the linguistic features and are respectively fed into two decoders. Moreover, we design an instance query to guide the fused visual-linguistic features for outputting the instance mask or bounding box. The classification layers aggregate results from decoders and predict answer as well as corresponding instance coordinates at last. Without bells and whistles, DDTN achieves state-of-the-art performance and even competitive to pretraining models on VizWizGround and GQA dataset. The code is available at https://github.com/zlj63501/DDTN.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523001046",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Decoding methods",
      "Detector",
      "Encoder",
      "Language model",
      "Natural language processing",
      "Operating system",
      "Physics",
      "Quantum mechanics",
      "Question answering",
      "Telecommunications",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Zhu",
        "given_name": "Liangjun"
      },
      {
        "surname": "Peng",
        "given_name": "Li"
      },
      {
        "surname": "Zhou",
        "given_name": "Weinan"
      },
      {
        "surname": "Yang",
        "given_name": "Jielong"
      }
    ]
  },
  {
    "title": "HDGN: Heat diffusion graph network for few-shot learning",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.04.005",
    "abstract": "A heat diffusion graph network (HDGN) is proposed in this paper, which retains more similar graph signals in the spectral domain, for few-shot learning. Convolution on the graph is essentially the filtering of the graph signal. Most existing graph-network-based few-shot learning methods process graph signals with high-pass filters to get the difference in information. However, the low-frequency similar information is usually more valuable in the few-shot tasks. A joint low-pass filter is constructed to filter low-frequency graph signals, that is, heat kernel convolution aggregates similar information from neighboring nodes. The obtained low-frequency similarity information is utilized to update the representations of nodes on the graph. In addition, a more robust mixed metric is designed to dynamically update the edge feature of the graph. Predicting Unknown Nodes on Graphs by Alternating Updates of Node Representation and Edge Matrix. The experimental results also demonstrate that HDGN achieves better performance for the few-shot classification task.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552300106X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Graph",
      "Graph kernel",
      "Kernel method",
      "Line graph",
      "Pattern recognition (psychology)",
      "Radial basis function kernel",
      "Strength of a graph",
      "Support vector machine",
      "Theoretical computer science",
      "Voltage graph"
    ],
    "authors": [
      {
        "surname": "Tan",
        "given_name": "Qi"
      },
      {
        "surname": "Wu",
        "given_name": "Zongze"
      },
      {
        "surname": "Lai",
        "given_name": "Jialun"
      },
      {
        "surname": "Liang",
        "given_name": "Zexiao"
      },
      {
        "surname": "Ren",
        "given_name": "Zhigang"
      }
    ]
  },
  {
    "title": "Non-contact PPG signal and heart rate estimation with multi-hierarchical convolutional network",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109421",
    "abstract": "Heartbeat rhythm and heart rate (HR) are important physiological parameters of the human body. This study presents an efficient multi-hierarchical spatio-temporal convolutional network that can quickly estimate remote physiological (rPPG) signal and HR from face video clips. First, the facial color distribution characteristics are extracted using a low-level face feature generation (LFFG) module. Then, the three-dimensional (3D) spatio-temporal stack convolution module (STSC) and multi-hierarchical feature fusion module (MHFF) are used to strengthen the spatio-temporal correlation of multi-channel features. In the MHFF, sparse optical flow is used to capture the tiny motion information of faces between frames and generate a self-adaptive region of interest (ROI) skin mask. Finally, the signal prediction module (SP) is used to extract the estimated rPPG signal. The heart rate estimation results show that the proposed network overperforms the state-of-the-art methods on three datasets, 1) UBFC-RPPG, 2) COHFACE, 3) our dataset, with the mean absolute error (MAE) of 2.15, 5.57, 1.75 beats per minute (bpm) respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032300122X",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Channel (broadcasting)",
      "Computer network",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Face (sociological concept)",
      "Feature (linguistics)",
      "Heartbeat",
      "Image (mathematics)",
      "Linguistics",
      "Optical flow",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "SIGNAL (programming language)",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Bin"
      },
      {
        "surname": "Zhang",
        "given_name": "Panpan"
      },
      {
        "surname": "Peng",
        "given_name": "Jinye"
      },
      {
        "surname": "Fu",
        "given_name": "Hong"
      }
    ]
  },
  {
    "title": "An Effective and Adaptable K-means Algorithm for Big Data Cluster Analysis",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109404",
    "abstract": "Tradition K -means clustering algorithm is easy to fall into local optimum, poor clustering effect on large capacity data and uneven distribution of clustering centroids. To solve these problems, a novel k -means clustering algorithm based on Lévy flight trajectory (Lk-means) is proposed in the paper. In the iterative process of LK-means algorithm, Lévy flight is used to search new positions to avoid premature convergence in clustering. It is also applied to increase the diversity of the cluster, strengthen the global search ability of K -means algorithm, and avoid falling into the local optimal value too early. Nevertheless, the complexity of hybrid algorithm is not increased in the process of Lévy flight optimization. To verify the data clustering effect of LK-means algorithm, experiments are conducted to compare it with the k -means algorithm, XK-means algorithm, DDKmeans algorithm and Canopyk-means algorithm on 10 open source data sets. The results show that LK-means algorithm has better search results and more evenly distributed cluster centroids, which greatly improves the global search ability, big data processing ability and uneven distribution centroids of cluster of K -means algorithm.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032300105X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "CURE data clustering algorithm",
      "Canopy clustering algorithm",
      "Centroid",
      "Cluster (spacecraft)",
      "Cluster analysis",
      "Computer science",
      "Convergence (economics)",
      "Correlation clustering",
      "Data mining",
      "Determining the number of clusters in a data set",
      "Economic growth",
      "Economics",
      "Local optimum",
      "Operating system",
      "Process (computing)",
      "Programming language",
      "k-medians clustering"
    ],
    "authors": [
      {
        "surname": "Hu",
        "given_name": "Haize"
      },
      {
        "surname": "Liu",
        "given_name": "Jianxun"
      },
      {
        "surname": "Zhang",
        "given_name": "Xiangping"
      },
      {
        "surname": "Fang",
        "given_name": "Mengge"
      }
    ]
  },
  {
    "title": "Hybrid token transformer for deep face recognition",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109443",
    "abstract": "Although Convolutional Neural Networks have achieved remarkable successes in face recognition, they still suffer a critical limitation on capturing long range relations among facial regions. The recent vision transformers can naturally alleviate this problem, by learning global token dependencies. However, They are insufficient to discover high-level facial semantics since tokens in these transformers are based on small and fixed regions. To tackle such difficulty, we propose a novel Hybrid tOken Transformer (HOTformer) module to identify key facial semantics for effective recognition with cooperation of atomic and holistic tokens. Specifically, atomic tokens are generated from small fixed-size regions that can learn fine-grained core representation. Alternatively, holistic tokens are constructed from big adaptively-learned regions that can capture coarse-grained contextual representation. Furthermore, our HOTformer is a plug-and-play module. By hierarchically inserting it into convolutional networks, we can build a concise HOTformer-Net that achieves a preferable computation like CNN while boosting accuracy like transformer.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323001437",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computation",
      "Computer science",
      "Computer security",
      "Convolutional neural network",
      "Deep learning",
      "Electrical engineering",
      "Engineering",
      "Feature learning",
      "Pattern recognition (psychology)",
      "Security token",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Su",
        "given_name": "Weicong"
      },
      {
        "surname": "Wang",
        "given_name": "Yali"
      },
      {
        "surname": "Li",
        "given_name": "Kunchang"
      },
      {
        "surname": "Gao",
        "given_name": "Peng"
      },
      {
        "surname": "Qiao",
        "given_name": "Yu"
      }
    ]
  },
  {
    "title": "Deep neural network with deformable convolution and side window convolution for image denoising",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.05.015",
    "abstract": "A noval neural networks with irregular convolution block is proposed for image denoising. In the field of image processing, convolutional neural networks have shown great advantages compared with traditional approaches, however, it is found that standard convolution does not work well on image edge, and it has some drawbacks when dealing with variable noised images. In this paper, we numerically illustrate that the irregular convolution, including deformable convolutional kernel and side window filtering technique, is beneficial for finding effective receptive field and improving image edge. Quantitative and qualitative experimental results are demonstrated, which outperforms classical convolution neural networks in denoising tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552300140X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Block (permutation group theory)",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Convolution (computer science)",
      "Convolution theorem",
      "Convolutional neural network",
      "Enhanced Data Rates for GSM Evolution",
      "Fourier analysis",
      "Fourier transform",
      "Fractional Fourier transform",
      "Geometry",
      "Image (mathematics)",
      "Kernel (algebra)",
      "Mathematical analysis",
      "Mathematics",
      "Noise reduction",
      "Operating system",
      "Pattern recognition (psychology)",
      "Window (computing)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Chang"
      },
      {
        "surname": "Hu",
        "given_name": "Xianliang"
      }
    ]
  },
  {
    "title": "Atmospheric turbulence removal with complex-valued convolutional neural network",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.05.017",
    "abstract": "Atmospheric turbulence distorts visual imagery and is always problematic for information interpretation by both human and machine. Most well-developed approaches to remove atmospheric turbulence distortion are model-based. However, these methods require high computation and large memory making real-time operation infeasible. Deep learning-based approaches have hence gained more attention but currently work efficiently only on static scenes. This paper presents a novel learning-based framework offering short temporal spanning to support dynamic scenes. We exploit complex-valued convolutions as phase information, altered by atmospheric turbulence, is captured better than using ordinary real-valued convolutions. Two concatenated modules are proposed. The first module aims to remove geometric distortions and, if enough memory, the second module is applied to refine micro details of the videos. Experimental results show that our proposed framework efficiently mitigates the atmospheric turbulence distortion and significantly outperforms existing methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523001447",
    "keywords": [
      "Algorithm",
      "Amplifier",
      "Artificial intelligence",
      "Atmospheric turbulence",
      "Bandwidth (computing)",
      "Computation",
      "Computer network",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Convolutional neural network",
      "Distortion (music)",
      "Exploit",
      "Image (mathematics)",
      "Meteorology",
      "Physics",
      "Turbulence"
    ],
    "authors": [
      {
        "surname": "Anantrasirichai",
        "given_name": "Nantheera"
      }
    ]
  },
  {
    "title": "A gradient optimization and manifold preserving based binary neural network for point cloud",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109445",
    "abstract": "With significant progress of deep learning on 3D point cloud, the demand for deployment of point cloud neural network on the edge devices is growing. Binary neural network, a type of quantization compression method, with extreme low bit and fast inference speed, attracts more attention. It is more challenging, but has greater potentiality. Most of the researches on binary networks focus on images rather than point cloud. Considering the particularity of point cloud neural network, this paper presents a novel binarization framework, which includes two main contributions. Firstly, a gradient optimization method is proposed to overcome the shortcomings of Straight Through Estimator (STE) commonly used in the back propagation of binary network training. Secondly, based on the analysis of manifold distortion caused by the binary convolution and pooling operations, we propose an optimized scaling recovery method to restore manifold for the convoluted feature, and also, a pooling correction method to improve the pooled feature's fidelity. Manifold distortion leads to the severe feature homogeneity problem, which brings trouble in generating features with sufficient discrimination for classification and segmentation. The manifold preserving optimizations are designed to introduce minimum extra parameters to balance the accuracy with the computation and storage consumption. Experiments show that the proposed method outperforms state-of-the-art in accuracy with ignored overhead, and also has good scalability.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323001450",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Convolutional neural network",
      "Pattern recognition (psychology)",
      "Point cloud"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Zhi"
      },
      {
        "surname": "Xu",
        "given_name": "Ke"
      },
      {
        "surname": "Ma",
        "given_name": "Yanxin"
      },
      {
        "surname": "Wan",
        "given_name": "Jianwei"
      }
    ]
  },
  {
    "title": "Dynamic graph convolutional networks by semi-supervised contrastive learning",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109486",
    "abstract": "The traditional graph convolutional network(GCN) and its variants usually only propagate node information through the topology given by the dataset. However, the given topology can only represent a certain relationship and ignore some correlative feature information between nodes, which may make the graph convolutional networks unable to fully utilize the data information. To address the above issue, a novel model named Dynamic Graph Convolutional Networks by Semi-Supervised Contrastive Learning (DGSCL) is proposed in this paper. First, a feature graph is dynamically constructed from the input node features to exploit the potential correlative feature information between nodes. Then, to ensure a high-quality feature graph, a semi-supervised contrastive learning method is designed to learn discriminative node embeddings, which can iteratively refine the constructed feature graph with the learned node embeddings. Finally, we fuse the node embeddings obtained from the given topology and the dynamic feature graph by two co-attention modules to produce more informative embeddings for the classification task. Through a series of experiments, we demonstrate the competitive performance of our model on seven node classification benchmarks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323001863",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Convolutional neural network",
      "Correlative",
      "Discriminative model",
      "Engineering",
      "Exploit",
      "Feature (linguistics)",
      "Feature learning",
      "Graph",
      "Line graph",
      "Linguistics",
      "Node (physics)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Structural engineering",
      "Theoretical computer science",
      "Topological graph theory",
      "Voltage graph"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Guolin"
      },
      {
        "surname": "Hu",
        "given_name": "Zehui"
      },
      {
        "surname": "Wen",
        "given_name": "Guoqiu"
      },
      {
        "surname": "Ma",
        "given_name": "Junbo"
      },
      {
        "surname": "Zhu",
        "given_name": "Xiaofeng"
      }
    ]
  },
  {
    "title": "Patch loss: A generic multi-scale perceptual loss for single image super-resolution",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109510",
    "abstract": "In single image super-resolution (SISR), although PSNR is a key metric for signal fidelity, images with high PSNR do not necessarily render high visual quality. As a result, current perception-driven SISR methods employ perceptual metrics close to the human eye to measure the quality of the generated images. Unfortunately, the perceptual loss and adversarial loss, widely used by the perception-driven SISR methods, still underperform on these non-differentiable perceptual metrics. To this end, we propose a generic multi-scale perceptual loss, i.e., the patch loss, which can be easily plugged into off-the-shelf SISR methods to improve a broad range of perceptual metrics. Specifically, the proposed patch loss minimizes the multi-scale similarity of image patches and enhances the restoration of regions with complex textures and sharp edges via parameter-free adaptive patch-wise attention. Our proposed patch loss introduces more realistic details compared to the perceptual loss and fewer artifacts compared to the adversarial loss.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323002108",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer vision",
      "Economics",
      "Electrical engineering",
      "Engineering",
      "Fidelity",
      "High fidelity",
      "Image (mathematics)",
      "Metric (unit)",
      "Neuroscience",
      "Operations management",
      "Pattern recognition (psychology)",
      "Perception",
      "Physics",
      "Quantum mechanics",
      "Scale (ratio)",
      "Similarity (geometry)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "An",
        "given_name": "Tai"
      },
      {
        "surname": "Mao",
        "given_name": "Binjie"
      },
      {
        "surname": "Xue",
        "given_name": "Bin"
      },
      {
        "surname": "Huo",
        "given_name": "Chunlei"
      },
      {
        "surname": "Xiang",
        "given_name": "Shiming"
      },
      {
        "surname": "Pan",
        "given_name": "Chunhong"
      }
    ]
  },
  {
    "title": "Self-paced deep clustering with learning loss",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.05.003",
    "abstract": "Deep clustering has become a popular technique in various fields due to its superior performance over conventional clustering. However, it can be challenging to classify data located near the decision boundary, and difficult or noisy samples can confuse or mislead the training process of deep neural networks, ultimately impacting clustering performance. To address this issue, we propose a novel self-paced deep clustering method that gradually increases the number of samples input into the network from easy to difficult. Our approach involves attaching a loss prediction module to the convolutional neural network to judge the difficulty of samples. The module is robust as it relies on the input contents, not statistical estimates of uncertainty from outputs. Moreover, this module selects the most informative data for the training model, thus enabling the network to converge stably and reach a good optimal solution. Finally, we consider the reconstruction error, clustering loss, and loss prediction error to construct the loss of the model. Experimental results on four image datasets show that our method outperforms state-of-the-art works. The main code can be found at https://github.com/clustering106/SDCLL.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523001289",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Classifier (UML)",
      "Cluster analysis",
      "Code (set theory)",
      "Computer science",
      "Construct (python library)",
      "Convolutional neural network",
      "Data mining",
      "Decision boundary",
      "Deep learning",
      "Image (mathematics)",
      "Machine learning",
      "Operating system",
      "Pattern recognition (psychology)",
      "Process (computing)",
      "Programming language",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Kai"
      },
      {
        "surname": "Song",
        "given_name": "Chengyun"
      },
      {
        "surname": "Qiu",
        "given_name": "Lianpeng"
      }
    ]
  },
  {
    "title": "Doubly contrastive representation learning for federated image recognition",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109507",
    "abstract": "This paper focuses on the problem of personalized federated learning (FL) with the schema of contrastive learning (CL), which is to implement collaborative pattern classification by many clients. The traditional FL frameworks mostly facilitate the global model for the server and the local models for the clients to be similar, often ignoring the data heterogeneity of the clients. Aiming at achieving better performance in clients, this study introduces a personalized federated contrastive learning model, dubbed PerFCL, by proposing a new approach to doubly contrastive representation learning (DCL). Concretely, PerFCL borrows the DCL scheme, where one CL loss compares the shared parts of local models with the global model and the other CL loss compares the personalized parts of local models with the global model. To encourage the difference between the two parts, we created a double optimization problem composed of maximizing the comparison agreement for the former and minimizing the comparison agreement for the latter. We evaluated the proposed model on three publicly available data sets for federated image classification. Experiment results show that PerFCL benefits from the proposed DCL strategy and performs better than the state-of-the-art federated-learning models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323002078",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Feature learning",
      "Federated learning",
      "Law",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Schema (genetic algorithms)",
      "Scheme (mathematics)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Yupei"
      },
      {
        "surname": "Xu",
        "given_name": "Yunan"
      },
      {
        "surname": "Wei",
        "given_name": "Shuangshuang"
      },
      {
        "surname": "Wang",
        "given_name": "Yifei"
      },
      {
        "surname": "Li",
        "given_name": "Yuxin"
      },
      {
        "surname": "Shang",
        "given_name": "Xuequn"
      }
    ]
  },
  {
    "title": "Learning non-parametric kernel via matrix decomposition for logistic regression",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.05.018",
    "abstract": "Kernel logistic regression is a widely used method in machine learning applications. However, the common kernels are not flexible or representative enough due to their simplistic parametric formulation. Non-parametric learning on kernels can enlarge the representation capability of the kernel, but the low-rank property is required to suppress the model complexity. To improve the flexibility of kernels, we perform low-rank decomposition on an adjust matrix, which can explicitly control the model complexity without using complicated rank penalty terms. In this paper, we also extend our method to learn an indefinite kernel by a different decomposition. Experimental results demonstrate that the proposed learning method for non-parametric kernels outperforms other representative algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523001459",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Combinatorics",
      "Computer science",
      "Decomposition",
      "Discrete mathematics",
      "Ecology",
      "Flexibility (engineering)",
      "Kernel (algebra)",
      "Kernel embedding of distributions",
      "Kernel method",
      "Kernel principal component analysis",
      "Kernel regression",
      "Logistic regression",
      "Machine learning",
      "Mathematics",
      "Nonparametric statistics",
      "Parametric statistics",
      "Polynomial kernel",
      "Radial basis function kernel",
      "Rank (graph theory)",
      "Statistics",
      "Support vector machine",
      "Tree kernel",
      "Variable kernel density estimation"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Kaijie"
      },
      {
        "surname": "He",
        "given_name": "Fan"
      },
      {
        "surname": "He",
        "given_name": "Mingzhen"
      },
      {
        "surname": "Huang",
        "given_name": "Xiaolin"
      }
    ]
  },
  {
    "title": "Joint spatial and scale attention network for multi-view facial expression recognition",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109496",
    "abstract": "Multi-view facial expression recognition (FER) is a challenging task because the appearance of an expression varies greatly due to poses. To alleviate the influences of poses, recently developed methods perform pose normalization, learn pose-invariant features, or learn pose-specific FER classifiers. However, these methods usually rely on a prerequisite pose estimator or expressive region detector that is independent of the subsequent expression analysis. Different from existing methods, we propose a joint spatial and scale attention network (SSA-Net) to localize proper regions for simultaneous head pose estimation (HPE) and FER. Specifically, SSA-Net discovers the regions most relevant to the facial expression at hierarchical scales by a spatial attention mechanism, and the most informative scales are selected in a scale attention learning manner to learn the joint pose-invariant and expression-discriminative representations. Then, we employ a dynamically constrained multi-task learning mechanism with a delicately designed constrain regulation to properly and adaptively train the network to optimize the representations, thus achieving accurate multi-view FER. The effectiveness of the proposed SSA-Net is validated on three multi-view datasets (BU-3DFE, Multi-PIE, and KDEF) and three in-the-wild FER datasets (AffectNet, SFEW, and FER2013). Extensive experiments demonstrate that the proposed framework outperforms existing state-of-the-art methods under both within-dataset and cross-dataset settings, with relative accuracy gains of 2.36%, 1.33%, 3.11%, 2.84%, 15.7%, and 7.57%, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323001966",
    "keywords": [
      "Anthropology",
      "Artificial intelligence",
      "Computer science",
      "Discriminative model",
      "Estimator",
      "Expression (computer science)",
      "Facial expression",
      "Invariant (physics)",
      "Machine learning",
      "Mathematical physics",
      "Mathematics",
      "Normalization (sociology)",
      "Pattern recognition (psychology)",
      "Pose",
      "Programming language",
      "Sociology",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Yuanyuan"
      },
      {
        "surname": "Peng",
        "given_name": "Jiyao"
      },
      {
        "surname": "Dai",
        "given_name": "Wei"
      },
      {
        "surname": "Zeng",
        "given_name": "Jiabei"
      },
      {
        "surname": "Shan",
        "given_name": "Shiguang"
      }
    ]
  },
  {
    "title": "A Novel Improved Binary Harris Hawks Optimization For High dimensionality Feature Selection",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.05.007",
    "abstract": "Harris Hawks Optimization (HHO) is a relatively new meta-heuristic algorithm that has shown promise in solving various optimization problems. However, HHO suffers from some limitations that can affect its performance. The two main drawbacks of HHO are population diversity and local optima. To overcome these limitations and adapt it to solve feature selection issues, a new meta-heuristic, Chaotic Opposition Harris Hawks Optimization with Simulated Annealing (COHHS), is proposed in this paper. Two main improvements are proposed for the HHO. The first one, the chaotic opposition, is applied at the initialization phase of HHO to improve the population diversity of the search agents. The second one, simulated annealing, is applied to find the optimal solution during each iteration to improve HHO exploitation. The proposed method is a dynamic structure that was originally designed to solve problems with continuous optimization. In this paper, we propose a binary COHHS (BCOHHS) using X-shaped functions to boost the efficiency of feature selection. The KNN classifier is used to evaluate the classification accuracy. The performance of the proposed method is evaluated on nine high-dimensional medical datasets and compared with other optimization methods. The experimental results confirm the superiority of the BCOHHS method over the other methods on the majority of the datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523001344",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Chaotic",
      "Classifier (UML)",
      "Computer science",
      "Curse of dimensionality",
      "Demography",
      "Feature selection",
      "Heuristic",
      "Initialization",
      "Local optimum",
      "Mathematical optimization",
      "Mathematics",
      "Optimization problem",
      "Pattern recognition (psychology)",
      "Population",
      "Programming language",
      "Simulated annealing",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Lahmar",
        "given_name": "Ines"
      },
      {
        "surname": "Zaier",
        "given_name": "Aida"
      },
      {
        "surname": "Yahia",
        "given_name": "Mohamed"
      },
      {
        "surname": "Boaullegue",
        "given_name": "Ridha"
      }
    ]
  },
  {
    "title": "Semantic-based conditional generative adversarial hashing with pairwise labels",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109452",
    "abstract": "Hashing has been widely exploited in recent years due to the rapid growth of image and video data on the web. Benefiting from recent advances in deep learning, deep hashing methods have achieved promising results with supervised information. However, it is usually expensive to collect the supervised information. In order to utilize both labeled and unlabeled data samples, many semi-supervised hashing methods based on Generative Adversarial Networks (GANs) have been proposed. Most of them still need the conditional information, which is usually generated by the pre-trained neural networks or leveraging random binary vectors. One natural question about these methods is that how can we generate a better conditional information given the semantic similarity information? In this paper, we propose a general two-stage conditional GANs hashing framework based on the pairwise label information. Both the labeled and unlabeled data samples are exploited to learn hash codes under our framework. In the first stage, the conditional information is generated via a general Bayesian approach, which has a much lower dimensional representation and maintains the semantic information of original data samples. In the second stage, a semi-supervised approach is presented to learn hash codes based on the conditional information. Both pairwise based cross entropy loss and adversarial loss are introduced to make full use of labeled and unlabeled data samples. Extensive experiments have shown that the propose algorithm outperforms current state-of-the-art methods on three benchmark image datasets, which demonstrates the effectiveness of our method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323001528",
    "keywords": [
      "Arithmetic",
      "Artificial intelligence",
      "Binary code",
      "Binary number",
      "Computer science",
      "Computer security",
      "Data mining",
      "Hash function",
      "Machine learning",
      "Mathematics",
      "Pairwise comparison",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Qi"
      },
      {
        "surname": "Wang",
        "given_name": "Weining"
      },
      {
        "surname": "Tang",
        "given_name": "Yuanyan"
      },
      {
        "surname": "Xu",
        "given_name": "Chengzhong"
      },
      {
        "surname": "Sun",
        "given_name": "Zhenan"
      }
    ]
  },
  {
    "title": "Intensity mixture and band-adaptive detail fusion for pansharpening",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109434",
    "abstract": "Pansharpening aims to sharpen a low-resolution multispectral (MS) image through a high-resolution single-channel panchromatic (PAN) image to obtain a high-resolution multi-spectral (HRMS) image. However, low correlation between the PAN and MS images, as well as the inaccurate detail injection for each band of MS image are the key problems causing spectral and spatial distortions in pansharpening. To address these issues, a new pansharpening method based on the intensity mixture and band-adaptive detail fusion is proposed. To obtain a mixed-intensity image (T) that has a high correlation with the MS image and maintain the gradient information of the PAN image, the intensity mixture model is constructed by establishing the intensity and gradient constraints between T and the source images. As it is hard to obtain a proper degradation filter in the model, a filter estimation algorithm is designed by the distribution alignment. To inject the details that match the point spread function of the sensor, a band-adaptive detail fusion algorithm is presented to fuse the details extracted from T with those from the MS image for each band. Furthermore, as there are far fewer details in the MS image than in T, a detail enhancement algorithm is proposed to enhance the details proportionally. The final HRMS image is obtained by injecting the fused details into the upsampled MS image. Extensive experiments show that the proposed method can efficiently achieve the best results in fusion quality compared to state-of-the-art methods. The code is availabe at https://github.com/yotick/IMBD.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323001358",
    "keywords": [
      "Artificial intelligence",
      "Channel (broadcasting)",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Feature detection (computer vision)",
      "Filter (signal processing)",
      "Image (mathematics)",
      "Image fusion",
      "Image gradient",
      "Image processing",
      "Image resolution",
      "Intensity (physics)",
      "Multispectral image",
      "Optics",
      "Panchromatic film",
      "Physics",
      "Pixel"
    ],
    "authors": [
      {
        "surname": "Lu",
        "given_name": "Hangyuan"
      },
      {
        "surname": "Yang",
        "given_name": "Yong"
      },
      {
        "surname": "Huang",
        "given_name": "Shuying"
      },
      {
        "surname": "Chen",
        "given_name": "Xiaolong"
      },
      {
        "surname": "Su",
        "given_name": "Hongfu"
      },
      {
        "surname": "Tu",
        "given_name": "Wei"
      }
    ]
  },
  {
    "title": "BabyNet: Reconstructing 3D faces of babies from uncalibrated photographs",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109367",
    "abstract": "We present a 3D face reconstruction system that aims at recovering the 3D facial geometry of babies from uncalibrated photographs, BabyNet. Since the 3D facial geometry of babies differs substantially from that of adults, baby-specific facial reconstruction systems are needed. BabyNet consists of two stages: 1) a 3D graph convolutional autoencoder learns a latent space of the baby 3D facial shape; and 2) a 2D encoder that maps photographs to the 3D latent space based on representative features extracted using transfer learning. In this way, using the pre-trained 3D decoder, we can recover a 3D face from 2D images. We evaluate BabyNet and show that 1) methods based on adult datasets cannot model the 3D facial geometry of babies, which proves the need for a baby-specific method, and 2) BabyNet outperforms classical model-fitting methods even when a baby-specific 3D morphable model, such as BabyFM, is used.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323000687",
    "keywords": [
      "3D reconstruction",
      "3d model",
      "Artificial intelligence",
      "Autoencoder",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Face (sociological concept)",
      "Pattern recognition (psychology)",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Morales",
        "given_name": "Araceli"
      },
      {
        "surname": "Alomar",
        "given_name": "Antonia"
      },
      {
        "surname": "Porras",
        "given_name": "Antonio R."
      },
      {
        "surname": "Linguraru",
        "given_name": "Marius George"
      },
      {
        "surname": "Piella",
        "given_name": "Gemma"
      },
      {
        "surname": "Sukno",
        "given_name": "Federico M."
      }
    ]
  },
  {
    "title": "Seeing the unseen: Wifi-based 2D human pose estimation via an evolving attentive spatial-Frequency network",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.04.020",
    "abstract": "Camera-based human pose estimation has become popular due to its wide applications and easy implementation. However, it is not applicable under several circumstances such as poor illumination, occlusion, and private protection. In this work, we utilize Wifi signals to estimate 2D human poses, which is challenging because Wifi signals are abstract and contain limited information. To address these challenges, we develop an evolving attentive spatial-frequency network to discover the relationship between signal variation and body movement for Wifi-based 2D human pose estimation. By first taking dilated CSI sequences as inputs, a spatial-frequency encoder is then introduced to effectively integrate static spatial information and dynamic frequency information from CSI signals. Finally, we design an evolving attention module to enable our model to attend to certain channels of features. Due to a lack of benchmarks, we propose two Wifi-based human pose estimation datasets, the General Pose Estimation dataset (GPE) and Specific Pose Estimation dataset (SPE), which have been released as a public download at project page. Extensive experiments on the proposed datasets show that our model outperforms the state-of-the-art method by at least 16% in terms of P C K @ 20 (percentage of correct keypoints).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523001277",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Economics",
      "Encoder",
      "Estimation",
      "Machine learning",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Pose",
      "Programming language",
      "SIGNAL (programming language)",
      "Scheme (mathematics)"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Yi-Chung"
      },
      {
        "surname": "Huang",
        "given_name": "Zhi-Kai"
      },
      {
        "surname": "Pang",
        "given_name": "Lu"
      },
      {
        "surname": "Jiang-Lin",
        "given_name": "Jian-Yu"
      },
      {
        "surname": "Kuo",
        "given_name": "Chia-Han"
      },
      {
        "surname": "Shuai",
        "given_name": "Hong-Han"
      },
      {
        "surname": "Cheng",
        "given_name": "Wen-Huang"
      }
    ]
  },
  {
    "title": "A vehicle classification model based on deep active learning",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.05.009",
    "abstract": "Vehicle classification is widely used in intelligent transportation and smart cities. However, the good performance of vehicle classification often depends on a large number of labeling data, which leads to the high cost of manual labeling. How to use a small number of labeled samples with rich features to achieve good classification performance is a challenge in vehicle classification. To solve these problems, we propose a deep active learning framework called Feature Fusion Spatial Pyramid Pooling and Re-parameterization Visual Geometry Group (FFSPP-RepVGG). The framework is mainly divided into two parts: query strategy and feature extraction module. Specifically, the query strategy uses Feature Fusion (FF) to calculate the loss, thus defining uncertainty and being able to select more valuable samples for labeling training. The feature extraction module uses Spatial Pyramid Pooling and Re-parameterization Visual Geometry Group (SPP-RepVGG) model, which can extract more reliable image features. It can reduce the cost of data labeling and have better performance at the same time. The experimental results on the BIT-Vehicle data set and the car-10 data set show that the FFSPP-RepVGG framework has superior performance than that of the comparison models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523001368",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Contextual image classification",
      "Data mining",
      "Data set",
      "Feature (linguistics)",
      "Feature extraction",
      "Geometry",
      "Image (mathematics)",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pooling",
      "Programming language",
      "Pyramid (geometry)",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Xuanhong"
      },
      {
        "surname": "Yang",
        "given_name": "Shiyu"
      },
      {
        "surname": "Xiao",
        "given_name": "Yun"
      },
      {
        "surname": "Zheng",
        "given_name": "Xia"
      },
      {
        "surname": "Gao",
        "given_name": "Shuai"
      },
      {
        "surname": "Zhou",
        "given_name": "Jincheng"
      }
    ]
  },
  {
    "title": "Density peaks clustering algorithm based on fuzzy and weighted shared neighbor for uneven density datasets",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109406",
    "abstract": "Uneven density data refers to data with a certain difference in sample density between clusters. The local density of density peaks clustering algorithm (DPC) does not consider the effect of sample density difference between clusters of uneven density data, which may lead to wrong selection of cluster centers; the algorithm allocation strategy makes it easy to incorrectly allocate samples originally belonging to sparse clusters to dense clusters, which reduces clustering efficiency. In this study, we proposed the density peaks clustering algorithm based on fuzzy and weighted shared neighbor for uneven density datasets (DPC-FWSN). First, a nearest neighbor fuzzy kernel function is obtained by combining K-nearest neighbor and fuzzy neighborhood. Then, local density is redefined by the nearest neighbor fuzzy kernel function. The local density can better characterize the distribution characteristics of the sample by balancing the contribution of sample density in dense and sparse areas, in order to avoid the situation that the sparse cluster does not have a cluster center. Finally, the allocation strategy for weighted shared neighbor similarity is proposed to optimize the sample allocation at the boundary of the sparse cluster. Experiments are performed on IDPC-FA, FKNN-DPC, FNDPC, DPCSA and DPC for uneven density datasets, complex morphologies datasets and real datasets. The clustering results demonstrate that DPC-FWSN effectively handles datasets with uneven density distribution.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323001073",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "CURE data clustering algorithm",
      "Canopy clustering algorithm",
      "Cluster (spacecraft)",
      "Cluster analysis",
      "Combinatorics",
      "Computer science",
      "Data mining",
      "Density estimation",
      "Estimator",
      "FLAME clustering",
      "Fuzzy clustering",
      "Fuzzy logic",
      "Kernel (algebra)",
      "Kernel density estimation",
      "Mathematics",
      "Nearest-neighbor chain algorithm",
      "Pattern recognition (psychology)",
      "Physics",
      "Probability density function",
      "Programming language",
      "Sample (material)",
      "Statistics",
      "Thermodynamics",
      "k-nearest neighbors algorithm"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Jia"
      },
      {
        "surname": "Wang",
        "given_name": "Gang"
      },
      {
        "surname": "Pan",
        "given_name": "Jeng-Shyang"
      },
      {
        "surname": "Fan",
        "given_name": "Tanghuai"
      },
      {
        "surname": "Lee",
        "given_name": "Ivan"
      }
    ]
  },
  {
    "title": "Novel features to detect gender from handwritten documents",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2022.08.016",
    "abstract": "Gender detection from handwritten documents is a crucial research area in many disciplines such as psychology, pyelography, graphology, and forensic analysis. Furthermore, this task is challenging due to the high similarity and overlap between individuals’ handwriting. The performance of the document recognition and analysis systems, depends on the extracted features from handwritten documents, which can be a challenging task as this depends on extracting the most relevant information from row text. In this paper, a set of gender-related features suggested by a graphologist, to detect the gender of the writers, have been proposed. These features include margins, space between words, pen-pressure and handwriting irregularity. Both SVM and ANN classifiers have been used to train, validate and test the proposed approach on two different data sets: our data set FSHS and ICDAR2013 dataset. The proposed method has achieved high classification rates of 94.7% and 97.1% using SVM and ANN respectively. Meanwhile, our method outperformed state-of-arts methods when applied to the ICDAR2013 dataset with classification rates of 91.4% and 92.5% using SVM and ANN respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522002616",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data set",
      "Economics",
      "Feature extraction",
      "Handwriting",
      "Handwriting recognition",
      "Image (mathematics)",
      "Management",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Programming language",
      "Set (abstract data type)",
      "Similarity (geometry)",
      "Speech recognition",
      "Support vector machine",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "AL-Qawasmeh",
        "given_name": "Najla"
      },
      {
        "surname": "Khayyat",
        "given_name": "Muna"
      },
      {
        "surname": "Suen",
        "given_name": "Ching Y."
      }
    ]
  },
  {
    "title": "An individual fairness based outlier detection ensemble",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.05.010",
    "abstract": "Outlier detection ensembles usually outperform individual detectors. However, the result of the ensemble could still be biased toward a group or individual data. To omit such biases, the notion of fairness was introduced. Individual fairness, in particular, aims at treating similar instances similarly. This paper proposes an individual fairness-based member selection approach (iFselect) for an outlier detection ensemble. We represent individual fairness with iFscore, which considers the relative similarity of the instances in the dataset with their outlier scores. Our method picks up members according to their iFscore from a pool of diverse detectors with varying parameters, giving a balance for performance-fairness trade-off. This enhances its suitability for the unsupervised problem of outlier detection. Moreover, our method avoids the selection of sensitive attributes which otherwise have an associated cost. Experiments on benchmark datasets suggest the superiority of iFselect in fairness, with overall performance being relatively comparable to its state-of-the-art counterparts.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523001411",
    "keywords": [
      "Anomaly detection",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Data mining",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Machine learning",
      "Outlier",
      "Pattern recognition (psychology)",
      "Selection (genetic algorithm)",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Mishra",
        "given_name": "Gargi"
      },
      {
        "surname": "Kumar",
        "given_name": "Rajeev"
      }
    ]
  },
  {
    "title": "Sparse feature selection via fast embedding spectral analysis",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109472",
    "abstract": "Feature selection has been a research hotspot in many fields. Models based on graph learning are currently the most popular approaches. However, the sparsity of most models is not strong, and graph learning for pair-sample evaluation takes a lot of time. ℓ 2 , 1 -norm regularization is the sparsity strategy adopted in most sparse models at present since the convex function is easy to solve. Nevertheless, the sparsity of ℓ 2 , 1 -norm is insufficient, and there exist parameter adjustment problems. ℓ 2 , 0 -norm is a better choice, which can strengthen the sparse constraints of the subspace. In this paper, the Sparse feature selection via Fast Embedding Spectral Analysis (SFESA) is proposed. Firstly, an adaptive anchor nearest neighbor graph is constructed to avoid the high time cost of learning pairwise nearest neighbor graphs to a certain extent. The low-dimensional embedding of data manifold structure is maintained by performing spectral analysis for the constructed graph. Secondly, the projected data is approximated to the low-dimensional embedding structure via a regularization term. Finally, ℓ 2 , 0 -norm is employed to constrain the projection matrix to enhance the subspace sparsity. Furthermore, a fast iterative algorithm is presented to solve this non-convex optimization problem. Extensive experiments on multiple public datasets show that SFESA can obtain excellent performance in less time.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323001723",
    "keywords": [
      "1-planar graph",
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Dense graph",
      "Eigenvalues and eigenvectors",
      "Embedding",
      "Feature selection",
      "Geometry",
      "Graph",
      "Graph embedding",
      "Line graph",
      "Mathematics",
      "Matrix norm",
      "Pairwise comparison",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Regular polygon",
      "Regularization (linguistics)",
      "Subspace topology",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Jingyu"
      },
      {
        "surname": "Wang",
        "given_name": "Hongmei"
      },
      {
        "surname": "Nie",
        "given_name": "Feiping"
      },
      {
        "surname": "Li",
        "given_name": "Xuelong"
      }
    ]
  },
  {
    "title": "A Composite Network Model for Face Super-Resolution with Multi-Order Head Attention Facial Priors",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109503",
    "abstract": "Face super-resolution (FSR) aims to reconstruct high-resolution face images from low-resolution (LR) ones. Despite the progress made by deep convolutional neural networks (DCNNs) on FSR, convolutions struggle to relate spatially distant concepts and what is more, all image pixels and prior information (e.g., landmarks and facial component heatmaps) are treated equally regardless of importance, causing inaccuracy and decreasing the quality of face image recovery. To address these issues, in this paper we propose a composite network model for FSR with multi-order head attention facial priors. The proposed model contains a face hallucination transformer (FHT)-based network and a multi-order head attention (MOHA)-based DCNN. The FHT-based network can capture long-range dependencies and gradually increase resolution to achieve efficient and effective inference, while the MOHA-based DCNN exploits detailed and two-dimensional information of LR face images. Moreover, the novel generic submodule of the MOHA-based DCNN, namely Multi-Order Head Attention Network, can accurately model the relationship of facial components between spatial and channel dimensions. The proposed composite network model seamlessly integrates the advantages of DCNNs and transformers to super-resolve LR face images. When compared with state-of-the-art FSR methods on public benchmark datasets, the proposed model shows competitive recovery performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323002030",
    "keywords": [
      "Artificial intelligence",
      "Bayesian probability",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Face (sociological concept)",
      "Face detection",
      "Face hallucination",
      "Facial recognition system",
      "Image resolution",
      "Inference",
      "Pattern recognition (psychology)",
      "Pixel",
      "Prior probability",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Wei",
        "given_name": "Feng"
      },
      {
        "surname": "Wang",
        "given_name": "Song"
      },
      {
        "surname": "Yang",
        "given_name": "Jucheng"
      },
      {
        "surname": "Sun",
        "given_name": "Xiao"
      },
      {
        "surname": "Wang",
        "given_name": "Yuan"
      },
      {
        "surname": "Chen",
        "given_name": "Yarui"
      }
    ]
  },
  {
    "title": "YOGA: Deep object detection in the wild with lightweight feature learning and multiscale attention",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109451",
    "abstract": "We introduce YOGA, a deep learning based yet lightweight object detection model that can operate on low-end edge devices while still achieving competitive accuracy. The YOGA architecture consists of a two-phase feature learning pipeline with a cheap linear transformation, which learns feature maps using only half of the convolution filters required by conventional convolutional neural networks. In addition, it performs multi-scale feature fusion in its neck using an attention mechanism instead of the naive concatenation used by conventional detectors. YOGA is a flexible model that can be easily scaled up or down by several orders of magnitude to fit a broad range of hardware constraints. We evaluate YOGA on COCO-val and COCO-testdev datasets with over 10 state-of-the-art object detectors. The results show that YOGA strikes the best trade-off between model size and accuracy (up to 22% increase of AP and 23–34% reduction of parameters and FLOPs), making it an ideal choice for deployment in the wild on low-end edge devices. This is further affirmed by our hardware implementation and evaluation on NVIDIA Jetson Nano.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323001516",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Cloud computing",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Concatenation (mathematics)",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Deep learning",
      "Edge device",
      "Feature (linguistics)",
      "Feature learning",
      "Gene",
      "Linguistics",
      "Mathematics",
      "Object detection",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pipeline (software)",
      "Programming language",
      "Transformation (genetics)"
    ],
    "authors": [
      {
        "surname": "Sunkara",
        "given_name": "Raja"
      },
      {
        "surname": "Luo",
        "given_name": "Tie"
      }
    ]
  },
  {
    "title": "Triple-attention interaction network for breast tumor classification based on multi-modality images",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109526",
    "abstract": "Breast cancer can be diagnosed using medical imaging. Classification performance of medical imaging can be improved by multi-modality image fusion. However, existing fusion algorithm fail to consider the importance of modality interactions and cannot fully utilize multi-modality information. Attention mechanisms can effectively explore and combine multi-modality information. Thus, we propose a novel triple-attention interaction network for breast tumor classification based on diffusion-weighted imaging (DWI) and apparent dispersion coefficient (ADC) images. A triple inter-modality interaction mechanism is proposed to fully fuse the multi-modality information. Three modal interactions were performed through the developed inter-modality relation module, channel interaction module, and multi-level attention fusion module to explore the correlation, complementary, and discriminative information, respectively. Additionally, we introduce a novel dual parallel-attention module for the incorporation of spatial and channel attention to improve the discriminative ability of single-modality features. Using these mechanisms, the proposed algorithm can mine and explore useful multi-modality information fully, to improve classification performance. Experimental results demonstrate that our algorithm outperforms other multi-modality fusion algorithm, and extensive ablation studies were conducted to verify the advantages of our algorithm. The area under the receiver operating characteristic curve, accuracy, specificity, and sensitivity were 90.5%, 89.0%, 85.6%, and 92.4%, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323002261",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Discriminative model",
      "Machine learning",
      "Modality (human–computer interaction)",
      "Pattern recognition (psychology)",
      "Receiver operating characteristic"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Xiao"
      },
      {
        "surname": "Xi",
        "given_name": "Xiaoming"
      },
      {
        "surname": "Wang",
        "given_name": "Kesong"
      },
      {
        "surname": "Sun",
        "given_name": "Liangyun"
      },
      {
        "surname": "Meng",
        "given_name": "Lingzhao"
      },
      {
        "surname": "Nie",
        "given_name": "Xiushan"
      },
      {
        "surname": "Qiao",
        "given_name": "Lishan"
      },
      {
        "surname": "Yin",
        "given_name": "Yilong"
      }
    ]
  },
  {
    "title": "Fuzzy granular recurrence plot and quantification analysis: A novel method for classification",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109456",
    "abstract": "Recently, recurrence plot (RP) and its quantification techniques have become an important research tool in nonlinear analysis. In the existing researches, an RP is directly established on a time series ignoring the influence of noise on data, which will affect our judgement on the dynamic properties of a system. To tackle the problem there, this paper proposes a novel recurrence plot, namely fuzzy granular recurrence plot (FGRP). An FGRP of a time series is built not directly on the time series itself but on its corresponding granular time series which is composed of fuzzy information granules. With specific capability, fuzzy information granules are used as building blocks of an FGRP to achieve high-level, compact and understandable signal models. In order to apply the FGRP method to time series classification tasks, an FGRP based classification model is designed in this paper. Subsequent experiments show that the FGRP of a time series can reduce the effect of noise, and the FGRP based classification model can improve the classification performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323001565",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Data mining",
      "Fuzzy logic",
      "Image (mathematics)",
      "Machine learning",
      "Mathematics",
      "Noise (video)",
      "Nonlinear system",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Physics",
      "Plot (graphics)",
      "Quantum mechanics",
      "Recurrence plot",
      "Recurrence quantification analysis",
      "Series (stratigraphy)",
      "Statistics",
      "Time series"
    ],
    "authors": [
      {
        "surname": "He",
        "given_name": "Qian"
      },
      {
        "surname": "Yu",
        "given_name": "Fusheng"
      },
      {
        "surname": "Chang",
        "given_name": "Jiaqi"
      },
      {
        "surname": "Ouyang",
        "given_name": "Chenxi"
      }
    ]
  },
  {
    "title": "K-sets and k-swaps algorithms for clustering sets",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109454",
    "abstract": "We present two new clustering algorithms called k-sets and k-swaps for data where each object is a set. First, we define the mean of the sets in a cluster, and the distance between a set and the mean. We then derive the k-sets algorithm from the principles of classical k-means so that it repeats the assignment and update steps until convergence. To the best of our knowledge, the proposed algorithm is the first k-means based algorithm for this kind of data. We adopt the idea also into random swap algorithm, which is a wrapper around the k-means that avoids local minima. This variant is called k-swaps. We show by experiments that this algorithm provides more accurate clustering results than k-medoids and other competitive methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323001541",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Canopy clustering algorithm",
      "Cluster analysis",
      "Computer science",
      "Convergence (economics)",
      "Correlation clustering",
      "Economic growth",
      "Economics",
      "Finance",
      "Mathematical analysis",
      "Mathematics",
      "Maxima and minima",
      "Medoid",
      "Programming language",
      "Set (abstract data type)",
      "Swap (finance)",
      "k-means clustering",
      "k-medoids"
    ],
    "authors": [
      {
        "surname": "Rezaei",
        "given_name": "Mohammad"
      },
      {
        "surname": "Fränti",
        "given_name": "Pasi"
      }
    ]
  },
  {
    "title": "Towards realistic fingerprint presentation attacks: The ScreenSpoof method",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2022.09.002",
    "abstract": "Using mobile devices, people leave latent fingerprints on the screen that can be photographed and used as a mold to fabricate high-quality spoofs. In this paper, we analysed the threat level of realistic attacks using snapshot pictures of latent fingerprints against presentation attack detection systems. Our evaluation compares a semi-consensual acquisition, given by a voluntary pressure of the finger on the screen, with a completely non-consensual one, given by the normal use of the device.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522002653",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Database",
      "Fingerprint (computing)",
      "Fingerprint recognition",
      "Medicine",
      "Mobile device",
      "Pattern recognition (psychology)",
      "Presentation (obstetrics)",
      "Radiology",
      "Snapshot (computer storage)",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Casula",
        "given_name": "Roberto"
      },
      {
        "surname": "Micheletto",
        "given_name": "Marco"
      },
      {
        "surname": "Orrú",
        "given_name": "Giulia"
      },
      {
        "surname": "Marcialis",
        "given_name": "Gian Luca"
      },
      {
        "surname": "Roli",
        "given_name": "Fabio"
      }
    ]
  },
  {
    "title": "Fourier-based augmentation with applications to domain generalization",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109474",
    "abstract": "When deployed on a new domain different from the training set, deep learning often suffers from severe performance degradation. To combat domain shift, domain adaptation and domain generalization are proposed, where the former aims at transferring knowledge from related source domains to a known target domain, while the latter is more challenging by requiring the model to generalize to unknown target domains. This paper focuses on domain generalization and introduce a novel Fourier-based perspective for it. The main idea comes from the fact that Fourier amplitude component contains low-level statistics while phase component preserves high-level semantics. We thus propose a novel Fourier-based data augmentation strategy called AmpMix by linearly interpolating the amplitudes of two images while keeping their phases unchanged, to highlight the generalizable semantics contained in phase. To make full use of Fourier-augmented samples, we further incorporate consistency training between different augmentation views and devise a Fourier-based framework for three different domain generalization settings. Extensive experiments demonstrate the effectiveness of our Fourier-based method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323001747",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Domain (mathematical analysis)",
      "Fourier domain",
      "Fourier series",
      "Fourier transform",
      "Frequency domain",
      "Generalization",
      "Mathematical analysis",
      "Mathematics",
      "Programming language",
      "Semantics (computer science)"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Qinwei"
      },
      {
        "surname": "Zhang",
        "given_name": "Ruipeng"
      },
      {
        "surname": "Fan",
        "given_name": "Ziqing"
      },
      {
        "surname": "Wang",
        "given_name": "Yanfeng"
      },
      {
        "surname": "Wu",
        "given_name": "Yi-Yan"
      },
      {
        "surname": "Zhang",
        "given_name": "Ya"
      }
    ]
  },
  {
    "title": "Complementary adversarial mechanisms for weakly-supervised temporal action localization",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109426",
    "abstract": "Weakly supervised Temporal Action Localization (WTAL) aims to locate the start and end boundaries of action instances and recognize their corresponding categories. Classical methods mainly rely on random erasure mechanisms, attention mechanisms, or imposing loss constraints. Despite their great progress, there are still two challenges of incomplete positioning and context confusion. Therefore, we propose a framework with complementary adversarial mechanisms to address these issues. In the adversarial learning stage, for an input snippet, we roughly determine its proper duration, by matching it with the specified multi-scaled anchors based on CAS score loss; then, it undergoes a frame-level iterative regression to precisely figure out its boundary, which can reject none closely related frames merged in, and ensures no overlapping between different action proposals. Subsequently, the GCN module explicitly enhances the feature representation for this fine localized snippet, aiming to strengthen the exclusiveness between different action snippets. Afterward, our complementary learning module calculates the similarity between the original input video V g and the video V r reconstructed with the above localization refined snippets, aiming to ensure no closely relevant frames missing, this checking mechanism works as feedback to guide the regression module for more accurate localization regression. Finally, each refined snippet undergoes multi-instance learning to obtain its classification score, and the top-k strategy is used to aggregate temporally adjacent snippets based on their content similarity, which can avoid fragmentation of an action proposal. This method is tested on datasets of THUMOS14 and ActivityNet1.2, and their average accuracy is 64.68% and 32.94% respectively, its comparisons with other articles prove its effectiveness.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323001279",
    "keywords": [
      "Action (physics)",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Frame (networking)",
      "Image (mathematics)",
      "Information retrieval",
      "Law",
      "Machine learning",
      "Matching (statistics)",
      "Mathematics",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Physics",
      "Political science",
      "Politics",
      "Quantum mechanics",
      "Regression",
      "Representation (politics)",
      "Similarity (geometry)",
      "Snippet",
      "Statistics",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Chuanxu"
      },
      {
        "surname": "Wang",
        "given_name": "Jing"
      },
      {
        "surname": "Liu",
        "given_name": "Peng"
      }
    ]
  },
  {
    "title": "Dual-channel graph contrastive learning for self-supervised graph-level representation learning",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109448",
    "abstract": "Self-supervised graph-level representation learning aims to learn discriminative representations for subgraphs or entire graphs without human-curated labels. Recently, graph contrastive learning (GCL) methods have revolutionized this field and achieved state-of-the-art results in various downstream tasks. Nonetheless, current GCL models are mostly based on simple node-level information aggregation operations and fail to reveal various substructures from input graphs. Moreover, to perform graph-graph contrastive training, they often involve well-designed graph augmentation, which is expensive and requires extensive expert efforts. Here, we propose a novel GCL framework, namely DualGCL, for self-supervised graph-level representation learning. For fine-grained local information incorporation, we first present an adaptive hierarchical aggregation process with a differentiable Transformer-based aggregator. Then, to efficiently learn graph-level discriminative representations, we introduce a dual-channel contrastive learning process in a multi-granularity and augmentation-free contrasting mode. When tested empirically on six popular graph classification benchmarks, our DualGCL achieves better or comparable performance than various strong baselines.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323001486",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Discriminative model",
      "Feature learning",
      "Graph",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Semi-supervised learning",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Luo",
        "given_name": "Zhenfei"
      },
      {
        "surname": "Dong",
        "given_name": "Yixiang"
      },
      {
        "surname": "Zheng",
        "given_name": "Qinghua"
      },
      {
        "surname": "Liu",
        "given_name": "Huan"
      },
      {
        "surname": "Luo",
        "given_name": "Minnan"
      }
    ]
  },
  {
    "title": "Relation-mining self-attention network for skeleton-based human action recognition",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109455",
    "abstract": "Modeling spatiotemporal global dependencies and dynamics of body joints are crucial to recognizing actions from 3D skeleton sequences. We propose a Relation-mining Self-Attention Network (RSA-Net) for skeleton-based human action recognition. The proposed RSA-Net is motivated by two important observations: (1) body joint relationships can be modeled independently as pairwise and unary to reduce the difficulty of action feature learning. (2) Computing action semantics and position information independently removes noisy correlations over heterogeneous embedding. The proposed RSA-Net contains pairwise self-attention, unary self-attention, and position embedding attention modules. The pairwise self-attention captures the relationship between every two body joints. The unary self-attention learns a general correlation features among one key joint over all other query joints. The position embedding attention module computes the correlation between action semantics and position information independently with separate projection matrices. Extensive evaluations are performed in the NTU-60, NTU-120, and UESTC datasets with CS, CV, CSet, and A-view evaluation benchmarks. The proposed RSA-Net outperforms existing transformer-based approaches and comparable results with state-of-the-art graph ConvNet methods. The source code is available in Github 1 1 https://github.com/GedamuA/RSA-Net. .",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323001553",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Data mining",
      "Embedding",
      "Geometry",
      "Mathematics",
      "Net (polyhedron)",
      "Pairwise comparison",
      "Pattern recognition (psychology)",
      "Programming language",
      "Relation (database)",
      "Semantics (computer science)",
      "Theoretical computer science",
      "Unary operation"
    ],
    "authors": [
      {
        "surname": "Gedamu",
        "given_name": "Kumie"
      },
      {
        "surname": "Ji",
        "given_name": "Yanli"
      },
      {
        "surname": "Gao",
        "given_name": "LingLing"
      },
      {
        "surname": "Yang",
        "given_name": "Yang"
      },
      {
        "surname": "Shen",
        "given_name": "Heng Tao"
      }
    ]
  },
  {
    "title": "Weakly-supervised pre-training for 3D human pose estimation via perspective knowledge",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109497",
    "abstract": "Modern deep learning-based 3D pose estimation approaches require plenty of 3D pose annotations. However, existing 3D datasets lack diversity, which limits the performance of current methods and their generalization ability. Although existing methods utilize 2D pose annotations to help 3D pose estimation, they mainly focus on extracting 2D structural constraints from 2D poses, ignoring the 3D information hidden in the images. In this paper, we propose a novel method to extract weak 3D information directly from 2D images without 3D pose supervision. Firstly, we utilize 2D pose annotations and perspective prior knowledge to generate the relative depth of human joints. Then, we collect a 2D pose dataset (MCPC) and generate relative depth labels. Based on MCPC, we propose a weakly-supervised pre-training (WSP) strategy to distinguish the depth relationship between two points in an image. WSP enables the learning of the relative depth of two keypoints on lots of in-the-wild images, which is more capable of predicting depth and generalization ability for 3D human pose estimation. After fine-tuning the pose model on 3D pose datasets, WSP achieves state-of-the-art results on two widely-used benchmarks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323001978",
    "keywords": [
      "3D pose estimation",
      "Articulated body pose estimation",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Focus (optics)",
      "Generalization",
      "Image (mathematics)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Optics",
      "Pattern recognition (psychology)",
      "Perspective (graphical)",
      "Physics",
      "Pose"
    ],
    "authors": [
      {
        "surname": "Qiu",
        "given_name": "Zhongwei"
      },
      {
        "surname": "Qiu",
        "given_name": "Kai"
      },
      {
        "surname": "Fu",
        "given_name": "Jianlong"
      },
      {
        "surname": "Fu",
        "given_name": "Dongmei"
      }
    ]
  },
  {
    "title": "Localized curvature-based combinatorial subgraph sampling for large-scale graphs",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109475",
    "abstract": "This paper introduces a subgraph sampling method based on curvature to train large-scale graphs via mini-batch training. Owing to the difficulty in sampling globally optimal subgraphs from large graphs, we sample the subgraphs to minimize the distributional metric with combinatorial sampling. In particular, we define a combinatorial metric that distributionally measures the similarity between an original graph and all possible node and edge combinations of the subgraphs. Further, we prove that the subgraphs sampled using the probability model proportional to the discrete Ricci curvature (i.e., Ollivier-Ricci curvatures) of the edges can minimize the proposed metric. Moreover, as accurate calculation of the curvature on a large graph is challenging, we propose to use a localized curvature considering only 3-cycles on the graph, suggesting that this is a sufficiently approximated curvature on a sparse graph. We also show that the probability models of conventional sampling methods are related to coarsely approximated curvatures with no cycles, implying that the curvature is closely related to subgraph sampling. The experimental results confirm the feasibility of integrating the proposed curvature-based sampling method into existing graph neural networks to improve performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323001759",
    "keywords": [
      "Algorithm",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Curvature",
      "Discrete mathematics",
      "Filter (signal processing)",
      "Geometry",
      "Graph",
      "Induced subgraph isomorphism problem",
      "Line graph",
      "Mathematical optimization",
      "Mathematics",
      "Ricci curvature",
      "Sampling (signal processing)",
      "Voltage graph"
    ],
    "authors": [
      {
        "surname": "Shu",
        "given_name": "Dong Wook"
      },
      {
        "surname": "Kim",
        "given_name": "Youjin"
      },
      {
        "surname": "Kwon",
        "given_name": "Junseok"
      }
    ]
  },
  {
    "title": "ConCoNet: Class-agnostic counting with positive and negative exemplars",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.04.018",
    "abstract": "Class-agnostic counting is usually phrased as a matching problem between a user-defined exemplar patch and a query image. The count is derived based on the number of objects similar to the exemplar patch. However, defining a target class using only positive exemplar patches inevitably miscounts unintended objects that are visually alike to the exemplar. In this paper, we propose to include negative exemplars that define what not to count. This allows the model to calibrate its notion of what is similar based on both positive and negative exemplars. It effectively disentangles visually similar negatives, leading to a more discriminative definition of the target object. We designed our method such that it can be incorporated with other class-agnostic counting models. Moreover, application-wise, our model can be used into a semi-automatic labeling tool to simplify the job of the annotator",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523001253",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Computer vision",
      "Discriminative model",
      "Image (mathematics)",
      "Machine learning",
      "Matching (statistics)",
      "Mathematics",
      "Natural language processing",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Soliven",
        "given_name": "Adrienne Francesca O."
      },
      {
        "surname": "Virtusio",
        "given_name": "John Jethro"
      },
      {
        "surname": "Ople",
        "given_name": "Jose Jaena Mari"
      },
      {
        "surname": "Tan",
        "given_name": "Daniel Stanley"
      },
      {
        "surname": "Amalin",
        "given_name": "Divina"
      },
      {
        "surname": "Hua",
        "given_name": "Kai-Lung"
      }
    ]
  },
  {
    "title": "MEQA: Manifold embedding quality assessment via anisotropic scaling and Kolmogorov-Smirnov test",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109447",
    "abstract": "Manifold learning methods unfold the manifold structures and embed them in a lower-dimensional space. The quality of such an embedding should be measured both qualitatively and quantitatively. The proposed manifold embedding quality assessment (MEQA) method does so by taking into account of local and global structure preservation as both are important traits of an embedding. To measure the local structure preservation MEQA uses two transformations. Initially anisotropic scaling, rotation and translation are incorporated to measure the closeness between the original and the embedded data points. In the next stage, rigid transformation is incorporated to quantify the previous transformation which involved anisotropic scaling. For quantifying the global structure preservation, the Kolmogorov-Smirnov test is applied in a distributed manner over each dimension and averaged over them. To establish the superiority of MEQA we conducted several studies over standard synthetic and real-life datasets across separate existing feature extraction techniques.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323001474",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Closeness",
      "Combinatorics",
      "Computer science",
      "Data mining",
      "Dimension (graph theory)",
      "Dimensionality reduction",
      "Embedding",
      "Engineering",
      "Gene",
      "Geometry",
      "Manifold (fluid mechanics)",
      "Mathematical analysis",
      "Mathematics",
      "Measure (data warehouse)",
      "Mechanical engineering",
      "Messenger RNA",
      "Nonlinear dimensionality reduction",
      "Pure mathematics",
      "Rotation (mathematics)",
      "Scaling",
      "Topology (electrical circuits)",
      "Transformation (genetics)",
      "Translation (biology)"
    ],
    "authors": [
      {
        "surname": "Boral",
        "given_name": "Subhadip"
      },
      {
        "surname": "Sarkar",
        "given_name": "Mainak"
      },
      {
        "surname": "Ghosh",
        "given_name": "Ashish"
      }
    ]
  },
  {
    "title": "Generalized self-supervised contrastive learning with bregman divergence for image recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.05.020",
    "abstract": "Contrastive learning techniques continue to receive a lot of attention in the self-supervised learning area. Specifically, the learned distance features can be further utilized to capture the distance between latent features in the embedding space and improve the performance of both supervised and unsupervised learning tasks. However, most contrastive learning efforts are focused on learning the geometric distance between the latent features, while the underlying probability distribution is usually ignored. To address this challenge, we propose a novel generalized contrastive loss for self-supervised learning using the Bregman divergence by investigating the hidden relationship between the contrastive loss and the Bregman divergence. Our method considers the hybrid divergence that leverages the Euclidean-based distance and probabilistic divergence, which improves the quality of self-supervised learned feature representation. Besides theory, extensive experimental results demonstrate the effectiveness of our method compared to other state-of-the-art self-supervised methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523001472",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Bregman divergence",
      "Computer science",
      "Divergence (linguistics)",
      "Embedding",
      "Euclidean distance",
      "Feature (linguistics)",
      "Feature learning",
      "Feature vector",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Probabilistic logic",
      "Semi-supervised learning",
      "Statistics",
      "Supervised learning",
      "Unsupervised learning"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Zhiyuan"
      },
      {
        "surname": "Ralescu",
        "given_name": "Anca"
      }
    ]
  },
  {
    "title": "Bottom-up 2D pose estimation via dual anatomical centers for small-scale persons",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109403",
    "abstract": "In multi-person 2D pose estimation, the bottom-up methods simultaneously predict poses for all persons, and unlike the top-down methods, do not rely on human detection. However, the SOTA bottom-up methods’ accuracy is still inferior compared to the existing top-down methods. This is due to the predicted human poses being regressed based on the inconsistent human bounding box center and the lack of human-scale normalization, leading to the predicted human poses being inaccurate and small-scale persons being missed. To push the envelope of the bottom-up pose estimation, we firstly propose multi-scale training to enhance the network to handle scale variation with single-scale testing, particularly for small-scale persons. Secondly, we introduce dual anatomical centers (i.e., head and body), where we can predict the human poses more accurately and reliably, especially for small-scale persons. Moreover, existing bottom-up methods use multi-scale testing to boost the accuracy of pose estimation at the price of multiple additional forward passes, which weakens the efficiency of bottom-up methods, the core strength compared to top-down methods. By contrast, our multi-scale training enables the model to predict high-quality poses in a single forward pass (i.e., single-scale testing). Our method achieves 38.4% improvement on bounding box precision and 39.1% improvement on bounding box recall over the state of the art (SOTA) on the challenging small-scale persons subset of COCO. For the human pose AP evaluation, we achieve a new SOTA (71.0 AP) on the COCO test-dev set with the single-scale testing. We also achieve the top performance (40.3 AP) on the OCHuman dataset in cross-dataset evaluation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323001048",
    "keywords": [
      "Anthropology",
      "Artificial intelligence",
      "Bounding overwatch",
      "Cartography",
      "Computer science",
      "Geography",
      "Image (mathematics)",
      "Machine learning",
      "Minimum bounding box",
      "Normalization (sociology)",
      "Pattern recognition (psychology)",
      "Pose",
      "Scale (ratio)",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Cheng",
        "given_name": "Yu"
      },
      {
        "surname": "Ai",
        "given_name": "Yihao"
      },
      {
        "surname": "Wang",
        "given_name": "Bo"
      },
      {
        "surname": "Wang",
        "given_name": "Xinchao"
      },
      {
        "surname": "Tan",
        "given_name": "Robby T."
      }
    ]
  },
  {
    "title": "Deep debiased contrastive hashing",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109483",
    "abstract": "Hashing has achieved great success in multimedia retrieval due to its high computing efficiency and low storage cost. Recently, contrastive-learning-based hashing methods have achieved decent retrieval performance in label-free scenarios by learning distortion-invariant representations with Siamese networks. Their learning principle, i.e., instance discrimination, maximizes the correlation between self-augmented views and treats all others as negative samples. However, it may learn with false negative samples that are naturally similar, resulting in biased hash learning. To bridge this flaw, we reveal the between-instance similarity of naturally similar samples by exploring the latent structure of the training data. As a result, we propose the Deep Debiased Contrastive Hashing (DDCH) algorithm, using the neighborhood discovery module to explore the intrinsic similarity relationship that can help contrastive hashing reduce false negatives for superior discriminatory ability. Furthermore, we elucidate the rationale for incorporating the module into the contrastive hashing framework and explain our hashing process from an Expectation-Maximization (EM) perspective. Extensive experimental results on three benchmark image datasets demonstrate that DDCH significantly outperforms the state-of-the-art unsupervised hashing methods for image retrieval.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323001838",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Computer security",
      "Double hashing",
      "Dynamic perfect hashing",
      "Feature hashing",
      "Geodesy",
      "Geography",
      "Hash function",
      "Hash table",
      "Image (mathematics)",
      "Image retrieval",
      "Locality-sensitive hashing",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Universal hashing"
    ],
    "authors": [
      {
        "surname": "Wei",
        "given_name": "Rukai"
      },
      {
        "surname": "Liu",
        "given_name": "Yu"
      },
      {
        "surname": "Song",
        "given_name": "Jingkuan"
      },
      {
        "surname": "Xie",
        "given_name": "Yanzhao"
      },
      {
        "surname": "Zhou",
        "given_name": "Ke"
      }
    ]
  },
  {
    "title": "Where you edit is what you get: Text-guided image editing with region-based attention",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109458",
    "abstract": "Leveraging the abundant knowledge learned from pre-trained multi-modal models like CLIP has recently proved to be effective for text-guided image editing. Though convincing results have been made when combining the image generator StyleGAN with CLIP, most methods need to train separate models for different prompts, and irrelevant regions are often changed after editing due to the lack of spatial disentanglement. We propose a novel framework that can edit different images according to different prompts in one model. Besides, an innovative region-based spatial attention mechanism is adopted to explicitly guarantee the locality of editing. Experiments mainly in the face domain verify the feasibility of our framework and show that when multi-text editing and local editing are accomplishable, our method can complete practical applications like sequential editing and regional style transfer.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323001589",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Domain (mathematical analysis)",
      "Face (sociological concept)",
      "Generator (circuit theory)",
      "Image (mathematics)",
      "Image editing",
      "Linguistics",
      "Locality",
      "Mathematical analysis",
      "Mathematics",
      "Philosophy",
      "Physics",
      "Power (physics)",
      "Quantum mechanics",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Xiao",
        "given_name": "Changming"
      },
      {
        "surname": "Yang",
        "given_name": "Qi"
      },
      {
        "surname": "Xu",
        "given_name": "Xiaoqiang"
      },
      {
        "surname": "Zhang",
        "given_name": "Jianwei"
      },
      {
        "surname": "Zhou",
        "given_name": "Feng"
      },
      {
        "surname": "Zhang",
        "given_name": "Changshui"
      }
    ]
  },
  {
    "title": "A survey on machine learning from few samples",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109480",
    "abstract": "The capability of learning and generalizing from very few samples successfully is a noticeable demarcation separating artificial intelligence and human intelligence. Despite the long history dated back to the early 2000s and the widespread attention in recent years with booming deep learning, few surveys for few sample learning (FSL) are available. We extensively study almost all papers of FSL spanning from the 2000s to now and provide a timely and comprehensive survey for FSL. In this survey, we review the evolution history and current progress on FSL, categorize FSL approaches into the generative model based and discriminative model based kinds in principle, and emphasize particularly on the meta learning based FSL approaches. We also summarize several recently emerging extensional topics of FSL and review their latest advances. Furthermore, we highlight the important FSL applications covering many research hotspots in computer vision, natural language processing, audio and speech, reinforcement learning and robotic, data analysis, etc. Finally, we conclude the survey with a discussion on promising trends in the hope of providing guidance and insights to follow-up researches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323001802",
    "keywords": [
      "Artificial intelligence",
      "Categorization",
      "Computer science",
      "Data science",
      "Deep learning",
      "Generative grammar",
      "Machine learning"
    ],
    "authors": [
      {
        "surname": "Lu",
        "given_name": "Jiang"
      },
      {
        "surname": "Gong",
        "given_name": "Pinghua"
      },
      {
        "surname": "Ye",
        "given_name": "Jieping"
      },
      {
        "surname": "Zhang",
        "given_name": "Jianwei"
      },
      {
        "surname": "Zhang",
        "given_name": "Changshui"
      }
    ]
  },
  {
    "title": "SANet-SI: A new Self-Attention-Network for Script Identification in scene images",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.04.015",
    "abstract": "Developing an automatic method for identifying scripts in natural scene text images is of great importance for improving performance of multilingual OCR. This paper presents a new Self-Attention Network (SANet-SI) for script identification in natural scene text images. The rationale behind proposing SANet-SI is that each script exhibits its own pattern because of different characteristics of scripts. To extract such observations, we explore self-attention-based CNN with a multi-scale feature extraction approach. The proposed multi-scale feature extraction involves local, global features extraction and fusion of both the features. Furthermore, to extract dominant features from the pool of features that contribute more for script identification, we explore Style-based Recalibration Module (SRM) in a new way. In addition, to improve the performance of the identification and reduce the model size, the proposed model uses the Global Average Pooling (GAP) layer, instead of Fully Connected(FC) layers in this work. The proposed model is evaluated on standard datasets, namely, RRC-MLT2017, SIW-13, and CVSI2015 to show effectiveness over state-of-the-art methods in terms of confusion matrix and classification rate. In addition, we also conducted experiments for Cross Dataset Validation to show that the proposed model is independent of the number of scripts and different datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523001228",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Computer science",
      "Confusion matrix",
      "Feature (linguistics)",
      "Feature extraction",
      "Identification (biology)",
      "Linguistics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Pooling",
      "Quantum mechanics",
      "Scale (ratio)",
      "Scripting language"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Xiaomeng"
      },
      {
        "surname": "Zhan",
        "given_name": "Hongjian"
      },
      {
        "surname": "Shivakumara",
        "given_name": "Palaiahnakote"
      },
      {
        "surname": "Pal",
        "given_name": "Umapada"
      },
      {
        "surname": "Lu",
        "given_name": "Yue"
      }
    ]
  },
  {
    "title": "Multi-stage stacked temporal convolution neural networks (MS-S-TCNs) for biosignal segmentation and anomaly localization",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109440",
    "abstract": "In the computer vision domain, temporal convolution networks (TCN) have gained traction due to their lightweight, robust architectures for sequence-to-sequence prediction tasks. With that insight, in this study, we propose a novel deep learning architecture for biosignal segmentation and anomaly localization based on TCNs, named the multi-stage stacked TCN, which employs multiple TCN modules with varying dilation factors. More precisely, for each stage, our architecture uses TCN modules with multiple dilation factors, and we use convolution-based fusion to combine predictions returned from each stage. Furthermore, aiming smoothed predictions, we introduce a novel loss function based on the first-order derivative. To demonstrate the robustness of our architecture, we evaluate our model on five different tasks related to three 1D biosignal modalities (heart sounds, lung sounds and electrocardiogram). Our proposed framework achieves state-of-the-art performance for all tasks, significantly outperforming the respective state-of-the-art models having F1 score gains up to ≈ 9 %. Furthermore, the framework demonstrates competitive performance gains compared to traditional multi-stage TCN models with similar configurations yielding F1 score gains up to ≈ 5 %. Our model is also interpretable. Using neural conductance, we demonstrate the effectiveness of having TCNs with varying dilation factors. Our visualizations show that the model benefits from feature maps captured at multiple dilation factors, and the information is effectively propagated through the network such that the final stage produces the most accurate result.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323001413",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Biosignal",
      "Chemistry",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Deep learning",
      "Dilation (metric space)",
      "Filter (signal processing)",
      "Gene",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Robustness (evolution)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Dissanayake",
        "given_name": "Theekshana"
      },
      {
        "surname": "Fernando",
        "given_name": "Tharindu"
      },
      {
        "surname": "Denman",
        "given_name": "Simon"
      },
      {
        "surname": "Sridharan",
        "given_name": "Sridha"
      },
      {
        "surname": "Fookes",
        "given_name": "Clinton"
      }
    ]
  },
  {
    "title": "DeepActsNet: A deep ensemble framework combining features from face, hands, and body for action recognition",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109484",
    "abstract": "Human action recognition from videos has gained substantial focus due to its wide applications in the field of video understanding. Most of the existing approaches extract human skeleton data from videos to encode actions because of the invariance nature of the skeleton information with respect to lightning conditions and background changes. Despite their success in achieving high recognition accuracy, methods based on limited body joints fail to capture the nuances of subtle body parts which are highly relevant for discriminating similar actions. In this paper, we overcome this limitation by presenting a holistic framework for combining spatial and motion features from the body, face, and hands to develop a novel data representation termed “Deep Actions Stamps (DeepActs)” for video-based action recognition. Compared to the skeleton sequences based on limited body joints, DeepActs encode more effective spatio-temporal features that provide robustness against pose estimation noises and improve action recognition accuracy. We also present “DeepActsNet”, a deep learning based ensemble model which learns convolutional and structural features from Deep Action Stamps for highly accurate action recognition. Experiments on three challenging action recognition datasets (NTU60, NTU120, and SYSU) show that the proposed model produces significant improvements in the action recognition accuracy with less computational cost compared to the state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032300184X",
    "keywords": [
      "Action (physics)",
      "Action recognition",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Class (philosophy)",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Deep learning",
      "ENCODE",
      "Face (sociological concept)",
      "Field (mathematics)",
      "Gene",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Pose",
      "Pure mathematics",
      "Quantum mechanics",
      "Robustness (evolution)",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Asif",
        "given_name": "Umar"
      },
      {
        "surname": "Mehta",
        "given_name": "Deval"
      },
      {
        "surname": "Von Cavallar",
        "given_name": "Stefan"
      },
      {
        "surname": "Tang",
        "given_name": "Jianbin"
      },
      {
        "surname": "Harrer",
        "given_name": "Stefan"
      }
    ]
  },
  {
    "title": "An Interpretable Channelwise Attention Mechanism based on Asymmetric and Skewed Gaussian Distribution",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109467",
    "abstract": "Channelwise attention mechanisms have recently been demonstrated to boost the performance of deep convolutional neural networks (CNNs). The hypothesis on the negative correlation between channelwise responses and their importance levels has been verified. Therefore, according to the shifted means and unbalanced responses that are observed in attention distribution, two empirical hypotheses are proposed in this paper. Then, as an interpretable attention module, bilateral asymmetric skewed Gaussian attention (bi-SGA) is utilized to combine skewed and asymmetric properties into the Gaussian context transformer (GCT), which is the state-of-the-art. Finally, extensive experiments on the different benchmarks validate the rationality of the hypotheses. The proposed bi-SGA improves the overall performance of GCT attention mechanisms with only two extra parameters to be learned. Moreover, our attention mechanism also provides a perspective on analyzing the channelwise importance levels in deep neural networks in an interpretable and logical manner.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032300167X",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Convolutional neural network",
      "Deep neural networks",
      "Epistemology",
      "Gaussian",
      "Gaussian process",
      "Machine learning",
      "Mechanism (biology)",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Perspective (graphical)",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Cheng"
      },
      {
        "surname": "Li",
        "given_name": "Bo"
      }
    ]
  },
  {
    "title": "Distilling EEG representations via capsules for affective computing",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.05.011",
    "abstract": "Affective computing with Electroencephalogram (EEG) is a challenging task that requires cumbersome models to effectively learn the information contained in large-scale EEG signals, causing difficulties for real-time smart-device deployment. In this paper, we propose a novel knowledge distillation pipeline to distill EEG representations via capsule-based architectures for both classification and regression tasks. Our goal is to distill information from a heavy model to a lightweight model for subject-specific tasks. To this end, we first pre-train a large model (teacher network) on large number of training samples. Then, we employ the teacher network to learn the discriminative features embedded in capsules by adopting a lightweight model (student network) to mimic the teacher using the privileged knowledge. Such privileged information learned by the teacher contain similarities among capsules and are only available during the training stage of the student network. We evaluate the proposed architecture on two large-scale public EEG datasets, showing that our framework consistently enables student networks with different compression ratios to effectively learn from the teacher, even when provided with limited training samples. Lastly, our method achieves state-of-the-art results on one of the two datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523001423",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Discriminative model",
      "Economics",
      "Electroencephalography",
      "Machine learning",
      "Management",
      "Operating system",
      "Pipeline (software)",
      "Programming language",
      "Psychiatry",
      "Psychology",
      "Software deployment",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Guangyi"
      },
      {
        "surname": "Etemad",
        "given_name": "Ali"
      }
    ]
  },
  {
    "title": "Multi-scale foreground-background separation for light field depth estimation with deep convolutional networks",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.05.014",
    "abstract": "There are three viewpoints to divide the depth from light field (DFLF) methods: light field (LF) representation, signal processing, and learning viewpoints. In the signal processing viewpoint in details, the DFLF methods can be divided into three groups: cost-based, foreground-background separation (FBS)-based, and depth model-based methods. Although the deep learning techniques are widely implemented in the cost-based and depth model-based methods for the DFLF, they are not implemented in the FBS-based method, yet. In this paper, we propose a DFLF method based on the FBS with the deep learning techniques. Based on an observation that the angular aliasing artifacts are reduced in the resulting disparity maps obtained from spatially low-scale LF images, multi-scale (MS) FBS score maps are used in the proposed methods. To combine the MS FBS score maps into a single FBS score maps, four directional LF gradients, features from the 4-D LF, and features from the center view images are utilized. Based on the sensitivity and specificity loss, the gradient matching loss between the predicted and ground truth disparity maps is used to improve the performance around the occlusion boundary in the resulting disparity maps. The experimental results show that the proposed method performs reasonably in both the synthetic and real LF datasets. Especially, in the texture-less region in the real dataset, the resulting disparity maps obtained by the proposed method show that the artifacts are reduced compared to those by the existing DFLF methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552300137X",
    "keywords": [
      "Aliasing",
      "Artificial intelligence",
      "Cartography",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Deep learning",
      "Field (mathematics)",
      "Geography",
      "Ground truth",
      "Matching (statistics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pure mathematics",
      "Scale (ratio)",
      "Statistics",
      "Undersampling"
    ],
    "authors": [
      {
        "surname": "Lee",
        "given_name": "Jae Young"
      },
      {
        "surname": "Hur",
        "given_name": "Jiwan"
      },
      {
        "surname": "Choi",
        "given_name": "Jaehyun"
      },
      {
        "surname": "Park",
        "given_name": "Rae-Hong"
      },
      {
        "surname": "Kim",
        "given_name": "Junmo"
      }
    ]
  },
  {
    "title": "Attention reweighted sparse subspace clustering",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109438",
    "abstract": "Subspace clustering has attracted much attention in many applications of computer vision and pattern recognition. Spectral clustering based methods, such as sparse subspace clustering (SSC) and low-rank representation (LRR), have become popular due to their theoretical guarantees and impressive performance. However, many state-of-the-art subspace clustering methods specify the mean square error (MSE) criterion as the loss function, which is sensitive to outliers and complex noises in reality. These methods have poor performance when the data are corrupted by complex noise. In this paper, we propose a robust sparse subspace clustering method, termed Attention Reweighted SSC (ARSSC), by paying less attention to the corrupted entries (adaptively assigning small weights to the corrupted entries in each data point). To reduce the extra bias in estimation introduced by ℓ 1 regularization, we also utilize non-convex penalties to overcome the overpenalized problem. In addition, we provide theoretical guarantees for ARSSC and theoretically show that our method gives a subspace-preserving affinity matrix under appropriate conditions. To solve the ARSSC optimization problem, we devise an optimization algorithm using an Alternating Direction Method of Multipliers (ADMM) method. Experiments on real-world databases validate the effectiveness of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323001395",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Cluster analysis",
      "Combinatorics",
      "Computer science",
      "Image (mathematics)",
      "Mathematics",
      "Noise (video)",
      "Outlier",
      "Pattern recognition (psychology)",
      "Rank (graph theory)",
      "Regularization (linguistics)",
      "Spectral clustering",
      "Subspace topology"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Libin"
      },
      {
        "surname": "Wang",
        "given_name": "Yulong"
      },
      {
        "surname": "Deng",
        "given_name": "Hao"
      },
      {
        "surname": "Chen",
        "given_name": "Hong"
      }
    ]
  },
  {
    "title": "Class-incremental object detection",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109488",
    "abstract": "Deep learning architectures have shown remarkable results in the object detection task. However, they experience a critical performance drop when they are required to learn new classes incrementally without forgetting old ones. This catastrophic forgetting phenomenon impedes the deployment of artificial intelligence in real word scenarios where systems need to learn new and different representations over time. Recently, many incremental learning methods have been proposed to avoid the catastrophic forgetting problem. However, current state-of-the-art class-incremental learning strategies aim at preserving the knowledge of old classes while learning new ones sequentially, which would encounter other problems as follows: (1) In the process of preserving information of old classes, only a small portion of data in the previous tasks are kept and replayed during training, which inevitably incurs bias that is favorable for the new classes but malicious to the old classes. (2) With the knowledge of previous classes distilled into the new model, a sub-optimal solution for the new task is obtained since the preserving process of previous classes sabotages the training of new classes. To address these issues, termed as Information Asymmetry (IA), we propose a double-head framework which preserves the knowledge of old classes and learns the knowledge of new classes separately. Specifically, we transfer the knowledge of the previous model to the current learned one for overcoming the catastrophic forgetting problem. Furthermore, considering that IA would introduce impacts on the training of the new model, we propose a Non-Affection mask to distill the knowledge of the interested regions at the feature level. Comprehensive experimental results demonstrate that our proposed method significantly outperforms other state-of-the-art class-incremental object detection methods on PASCAL VOC and MS COCO datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323001887",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Economics",
      "Forgetting",
      "Linguistics",
      "Machine learning",
      "Management",
      "Object (grammar)",
      "Operating system",
      "Philosophy",
      "Process (computing)",
      "Software deployment",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Dong",
        "given_name": "Na"
      },
      {
        "surname": "Zhang",
        "given_name": "Yongqiang"
      },
      {
        "surname": "Ding",
        "given_name": "Mingli"
      },
      {
        "surname": "Bai",
        "given_name": "Yancheng"
      }
    ]
  },
  {
    "title": "Dynamic dual graph networks for textbook question answering",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109441",
    "abstract": "Textbook Question Answering (TQA) is a complex task oriented to multi-modal context, which requires reasoning on a diagram and a long essay to get the correct answer. There are mainly two related issues for the task. First, diagrams are mostly abstract expressions of real world and some constituents with similar appearance may have different semantics, which makes it difficult to understand them effectively. Secondly, a long essay contains abundant and useful information for question answering, which shows that it is vital to extract the relevant information from the abundant text and then perform reasoning on it. To address the two issues, we propose a new model, Dynamic Dual Graph Networks (DDGNet), which performs question-guided multi-step reasoning on the dynamic directed Diagram Graph Network (DGN) for the diagram and Textual Graph Network (TGN) for the most related paragraph extracted from a long essay. Specifically, DGN combines text features with positional features of text boxes in the diagram as the node feature to avoid the ambiguity of visual features for the abstract constituents and help express explicit semantics. TGN uses the representation of each sentence in the most related paragraph as the node feature to learn the contextualized interaction of the useful information in the graph reasoning process. For the reasoning strategy, we propose a question-guided multi-step graph reasoning method to update both DGN and TGN dynamically under the question guidance in every step. Experimental results show that our proposed model outperforms the baselines on the TQA dataset. Moreover, extensive ablation studies are also conducted to analyze the effectiveness of our proposed model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323000961",
    "keywords": [
      "Ambiguity",
      "Artificial intelligence",
      "Computer science",
      "Feature (linguistics)",
      "Graph",
      "Linguistics",
      "Natural language processing",
      "Paragraph",
      "Philosophy",
      "Programming language",
      "Question answering",
      "Sentence",
      "Theoretical computer science",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Yaxian"
      },
      {
        "surname": "Liu",
        "given_name": "Jun"
      },
      {
        "surname": "Ma",
        "given_name": "Jie"
      },
      {
        "surname": "Zeng",
        "given_name": "Hongwei"
      },
      {
        "surname": "Zhang",
        "given_name": "Lingling"
      },
      {
        "surname": "Li",
        "given_name": "Junjun"
      }
    ]
  },
  {
    "title": "FGPNet: A weakly supervised fine-grained 3D point clouds classification network",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109509",
    "abstract": "3D point clouds classification has been a hot research topic and received great progress in recent years. However, due to the similar data distributions and subtle differences among various sub-categories in a meta-category, the 3D point clouds classification at a fine-grained level is still very challenging, especially without the annotations of part locations or attributes. In this paper, we propose a novel weakly supervised network for fine-grained 3D point clouds classification, namely FGPNet. Different from the previous supervised fine-grained classification methods that use class labels and other manual annotation information, FGPNet develops a unified framework to address both local geometric details and global spatial structures only using the class labels as input. Specifically, FGPNet firstly employs a context-aware discriminative feature extraction (CDFE) module, which extract contextual contrasted information across differential receptive fields hierarchically, and further capture discriminative local details from point clouds. Subsequently, an SimAM-Capsule Aggregation (SCA) module is introduced to highlight the significant local features and capture their spatial relationships. Quantitative and qualitative experimental results on fine-grained dataset including three categories Airplane, Chair and Car demonstrate that FGPNet outperforms the state-of-the-art methods on fine-grained 3D point clouds classification tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323002091",
    "keywords": [
      "Annotation",
      "Archaeology",
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Context (archaeology)",
      "Discriminative model",
      "Feature (linguistics)",
      "Feature extraction",
      "Geography",
      "Geometry",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Point (geometry)",
      "Point cloud"
    ],
    "authors": [
      {
        "surname": "Shao",
        "given_name": "Huihui"
      },
      {
        "surname": "Bai",
        "given_name": "Jing"
      },
      {
        "surname": "Wu",
        "given_name": "Rusong"
      },
      {
        "surname": "Jiang",
        "given_name": "Jinzhe"
      },
      {
        "surname": "Liang",
        "given_name": "Hongbo"
      }
    ]
  },
  {
    "title": "Deep collaborative graph hashing for discriminative image retrieval",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109462",
    "abstract": "The most striking success of deep hashing for large-scale image retrieval benefits from its powerful discriminative representation of deep learning and the attractive computational efficiency of compact hash code learning. Most existing deep semantic-preserving hashing regard the available semantic labels as the ground truth for classification or transform them into prevalent pairwise similarities. However, such strategies fail to capture the interactive correlations between the visual semantics embedded in images and the given category-level labels. Moreover, they utilize the fixed piecewise or pairwise semantics as the optimization objectives, which suffers from the limited flexibility on semantic representation and adaptive knowledge communication in hash code learning. In this paper, we propose a novel Deep Collaborative Graph Hashing (DCGH), which collectively considers multi-level semantic embeddings, latent common space construction, and intrinsic structure mining in discriminative hash codes learning, for large-scale image retrieval. To the best of our knowledge, this is the first collaborative graph hashing for image retrieval. Specifically, instead of using the conventional single-flow visual network architecture, we design a dual-stream feature encoding network to jointly explore the multi-level semantic information across visual and semantic features. Moreover, a well-established shared latent space is constructed based on space reconstruction to explore the concurrent information and bridge the semantic gap between visual and semantic space. Furthermore, a graph convolutional network is introduced to preserve the latent structural relations in the optimal pairwise similarity-preserving hash codes. The whole learning framework is optimized in an end-to-end fashion. Extensive experiments on different datasets demonstrate that our DCGH can achieve superb image retrieval performance against state-of-the-art supervised hashing methods. The source codes of the proposed DCGH are available at https://github.com/JalinWang/DCGH .",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323001620",
    "keywords": [
      "Artificial intelligence",
      "Automatic image annotation",
      "Computer science",
      "Computer security",
      "Convolutional neural network",
      "Deep learning",
      "Discriminative model",
      "Double hashing",
      "Feature hashing",
      "Feature learning",
      "Hash function",
      "Hash table",
      "Image (mathematics)",
      "Image retrieval",
      "Pairwise comparison",
      "Pattern recognition (psychology)",
      "Semantic gap",
      "Semantic similarity",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Zheng"
      },
      {
        "surname": "Wang",
        "given_name": "Jianning"
      },
      {
        "surname": "Zhu",
        "given_name": "Lei"
      },
      {
        "surname": "Luo",
        "given_name": "Yadan"
      },
      {
        "surname": "Lu",
        "given_name": "Guangming"
      }
    ]
  },
  {
    "title": "TreeNet: Structure preserving multi-class 3D point cloud completion",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109476",
    "abstract": "Generating the missing data of 3D object point clouds from partial observations is a challenging task. Existing state-of-the-art learning-based 3D point cloud completion methods tend to use a limited number of categories/classes of training data and regenerate the entire point cloud based on the training datasets. As a result, output 3D point clouds generated by such methods may lose details (i.e. sharp edges and topology changes) due to the lack of multi-class training. These methods also lose the structural and spatial details of partial inputs due to the models do not separate the reconstructed partial input from missing points in the output. In this paper, we propose a novel deep learning network - TreeNet for 3D point cloud completion. TreeNet has two networks in hierarchical tree-based structures: TreeNet-multiclass focuses on multi-class training with a specific class of the completion task on each sub-tree to improve the quality of point cloud output; TreeNet-binary focuses on generating points in missing areas and fully preserving the original partial input. TreeNet-multiclass and TreeNet-binary are both network decoders and can be trained independently. TreeNet decoder is the combination of TreeNet-multiclass and TreeNet-binary and is trained with an encoder from existing methods (i.e. PointNet encoder). We compare the proposed TreeNet with five state-of-the-art learning-based methods on fifty classes of the public Shapenet dataset and unknown classes, which shows that TreeNet provides a significant improvement in the overall quality and exhibits strong generalization to unknown classes that are not trained.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323001760",
    "keywords": [
      "Algorithm",
      "Arithmetic",
      "Artificial intelligence",
      "Binary number",
      "Binary tree",
      "Class (philosophy)",
      "Computer science",
      "Data mining",
      "Economics",
      "Encoder",
      "Generalization",
      "Geometry",
      "Machine learning",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Object (grammar)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Point (geometry)",
      "Point cloud",
      "Task (project management)",
      "Tree (set theory)"
    ],
    "authors": [
      {
        "surname": "Xi",
        "given_name": "Long"
      },
      {
        "surname": "Tang",
        "given_name": "Wen"
      },
      {
        "surname": "Wan",
        "given_name": "TaoRuan"
      }
    ]
  },
  {
    "title": "Incremental estimation of low-density separating hyperplanes for clustering large data sets",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109471",
    "abstract": "An efficient unsupervised method for obtaining low-density hyperplane separators is proposed. The method is based on a modified stochastic gradient descent applied on a convolution of the empirical distribution function with a smoothing kernel. Low-density hyperplanes are motivated by the fact that they avoid intersecting high density regions, and so tend to pass between high density clusters, thus separating them from one another, while keeping the individual clusters intact. Multiple hyperplanes can be combined in a hierarchical model to obtain a complete clustering solution. A simple post-processing of solutions induced by large collections of hyperplanes yields an efficient and accurate clustering method, capable of automatically selecting the number of clusters. Experiments show that the proposed method is highly competitive in terms of both speed and accuracy when compared with relevant benchmarks. Code is available in the form of an R package at https://github.com/DavidHofmeyr/iMDH.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323001711",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Cluster analysis",
      "Code (set theory)",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Density estimation",
      "Estimator",
      "Hyperplane",
      "Kernel (algebra)",
      "Mathematics",
      "Programming language",
      "Set (abstract data type)",
      "Smoothing",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Hofmeyr",
        "given_name": "David P."
      }
    ]
  },
  {
    "title": "Quantifying the preferential direction of the model gradient in adversarial training with projected gradient descent",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109430",
    "abstract": "Adversarial training, especially projected gradient descent (PGD), has proven to be a successful approach for improving robustness against adversarial attacks. After adversarial training, gradients of models with respect to their inputs have a preferential direction. However, the direction of alignment is not mathematically well established, making it difficult to evaluate quantitatively. We propose a novel definition of this direction as the direction of the vector pointing toward the closest point of the support of the closest inaccurate class in decision space. To evaluate the alignment with this direction after adversarial training, we apply a metric that uses generative adversarial networks to produce the smallest residual needed to change the class present in the image. We show that PGD-trained models have a higher alignment than the baseline according to our definition, that our metric presents higher alignment values than a competing metric formulation, and that enforcing this alignment increases the robustness of models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323001310",
    "keywords": [
      "Adversarial system",
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Economics",
      "Gene",
      "Generative grammar",
      "Geometry",
      "Gradient descent",
      "Mathematical optimization",
      "Mathematics",
      "Metric (unit)",
      "Operations management",
      "Point (geometry)",
      "Residual",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Bigolin Lanfredi",
        "given_name": "Ricardo"
      },
      {
        "surname": "Schroeder",
        "given_name": "Joyce D."
      },
      {
        "surname": "Tasdizen",
        "given_name": "Tolga"
      }
    ]
  },
  {
    "title": "Multi-level dynamic error coding for face recognition with a contaminated single sample per person",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.05.002",
    "abstract": "Single sample per person (SSPP) has always been a significantly challenging and practical problem for face recognition owing to limited information and facial variations. Although the existing holistic and local methods have achieved great success in face recognition with SSPP, their performances suffer a serious degradation when SSPP is accompanied by a contaminated gallery database. In this situation, the gallery set is usually disturbed by facial variations such as illumination, expression, and disguise. To solve this problem, we propose a novel method called multi-level dynamic error coding. First, a multi-level pyramid structure is constructed for holistic and local sparse representation, where gallery dictionary patches are extracted to build a local gallery dictionary and a variation dictionary is built by extracting the generic dataset patches to depict potential facial variations. Second, we further propose a scheme of dynamic error coding by constructing an error function at different levels to reduce the negative impact of variations. At last, the corrected holistic and local errors are fused to perform the classification. Experimental results on various benchmark data sets have demonstrated that our method has strong generalization ability to dictionary and is more robust against facial variations under SSPP.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523001307",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Coding (social sciences)",
      "Computer science",
      "Face (sociological concept)",
      "Facial expression",
      "Facial recognition system",
      "Geodesy",
      "Geography",
      "Mathematics",
      "Neural coding",
      "Pattern recognition (psychology)",
      "Social science",
      "Sociology",
      "Sparse approximation",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Luan",
        "given_name": "Xiao"
      },
      {
        "surname": "Wang",
        "given_name": "Xin"
      },
      {
        "surname": "Liu",
        "given_name": "Linghui"
      },
      {
        "surname": "Li",
        "given_name": "Weisheng"
      }
    ]
  },
  {
    "title": "Hyperspectral subpixel target detection based on interaction subspace model",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109464",
    "abstract": "In this paper, we examine the problem of detecting subpixel targets in hyperspectral images. The so-called subpixel target refers to a target that only occupies a part of a pixel due to the low spatial resolution of hyperspectral sensors. Considering that the subpixel target spectrum is not always reliable (e.g., due to spectral variability), an interaction subspace model is designed to deal with this problem. In this subspace model, the second-order interaction terms are introduced to better describe the spectral variability, thereby improving the robustness. Specifically, the subspace model uses a hyperplane in a high-dimensional space to model spectral variability, while traditional models (e.g., the additive model and the replacement model) use a line in the high-dimensional space to model spectral variability. Obviously, the stronger description ability of the hyperplane makes the subspace model more tolerant to the mismatch of the target spectrum. Based on this interaction subspace model, we derive adaptive detectors according to the one-step generalized likelihood ratio test and its two-step variant. Experiments conducted on hyperspectral data demonstrate that the proposed two-step detector exhibits the strongest robustness in cases where the target spectrum is not very reliable.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323001644",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Detector",
      "Gene",
      "Geometry",
      "Hyperplane",
      "Hyperspectral imaging",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pixel",
      "Robustness (evolution)",
      "Subpixel rendering",
      "Subspace topology",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Sun",
        "given_name": "Shengyin"
      },
      {
        "surname": "Liu",
        "given_name": "Jun"
      },
      {
        "surname": "Sun",
        "given_name": "Siyu"
      }
    ]
  },
  {
    "title": "Object-centric Contour-aware Data Augmentation Using Superpixels of Varying Granularity",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109481",
    "abstract": "Regional dropout strategies have demonstrated to be very effective in improving both the performance and the generalization capability of deep learning models. However, when such strategies are performed in a totally random manner, the background noise and label mismatch problems arise. To tackle such problems, existing approaches typically focus on regions with the highest distinctiveness. Yet, there are two main drawbacks of existing approaches: (I) Many existing region-based augmentation methods can only use rectangular regions, resulting in the loss of object contour information; (II) Deterministic selection of the most discriminative regions leads to poor diversification in data augmentation. In fact, a trade-off is needed between diversification and concentration, which can decrease the undesirable noise. In this paper, we propose a novel object-centric contour-aware CutMix data augmentation strategy with arbitrary- shape and size superpixel supports, which is hereafter referred to as OcCaMix for short. It not only captures the most discriminative regions, but also effectively preserves the contour details of the objects. Moreover, it enables the search of natural object parts of different sizes. Extensive experiments on a large number of benchmark datasets show that OcCaMix significantly outperforms state-of-the-art CutMix based data augmentation methods in classification tasks. The source codes and trained models are available at https://github.com/DanielaPlusPlus/OcCaMix.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323001814",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Data mining",
      "Discriminative model",
      "Dropout (neural networks)",
      "Generalization",
      "Geodesy",
      "Geography",
      "Granularity",
      "Image (mathematics)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Noise (video)",
      "Object (grammar)",
      "Operating system",
      "Optimal distinctiveness theory",
      "Pattern recognition (psychology)",
      "Psychology",
      "Psychotherapist"
    ],
    "authors": [
      {
        "surname": "Dornaika",
        "given_name": "F."
      },
      {
        "surname": "Sun",
        "given_name": "D."
      },
      {
        "surname": "Hammoudi",
        "given_name": "K."
      },
      {
        "surname": "Charafeddine",
        "given_name": "J."
      },
      {
        "surname": "Cabani",
        "given_name": "A."
      },
      {
        "surname": "Zhang",
        "given_name": "C."
      }
    ]
  },
  {
    "title": "Autoencoders with exponential deviation loss for weakly supervised anomaly detection",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.05.016",
    "abstract": "Weakly supervised anomaly detection aims to detect anomalies using a small number of labeled anomalies and a large amount of unlabeled data. However, existing methods have limitations: unsuitable setting of the anomaly threshold, using an inefficient loss function, and being vulnerable to contaminated data. To address these limitations, this paper proposes a novel framework called the Exponential Deviation Autoencoder (EDAE), which consists of two stages. In the first stage, EDAE pre-trains an autoencoder (AE) to learn a compressed representation of the input data and estimates the anomaly score distribution of the training data to determine an appropriate anomaly threshold. In the second stage, EDAE fine-tunes the AE with a novel Exponential Deviation Loss (EDL) function that provides continuous and nonlinear penalties according to anomaly scores and enables more effective training using labeled anomalies. EDAE also uses batch sampling based on empirical distribution to create batches of data that are more robust to contaminated data. We conduct extensive experiments on various datasets and show that EDAE outperforms state-of-the-art weakly supervised methods with up to a 26% improvement in accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523001435",
    "keywords": [
      "Anomaly (physics)",
      "Anomaly detection",
      "Artificial intelligence",
      "Artificial neural network",
      "Autoencoder",
      "Computer science",
      "Condensed matter physics",
      "Data mining",
      "Exponential function",
      "Law",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Political science",
      "Politics",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Kwon",
        "given_name": "Min-Seong"
      },
      {
        "surname": "Moon",
        "given_name": "Yong-Geun"
      },
      {
        "surname": "Lee",
        "given_name": "Byungju"
      },
      {
        "surname": "Noh",
        "given_name": "Jung-Hoon"
      }
    ]
  },
  {
    "title": "Spatio-temporal hard attention learning for skeleton-based activity recognition",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109428",
    "abstract": "The use of skeleton data for activity recognition has become prevalent due to its advantages over RGB data. A skeleton video includes frames showing two- or three-dimensional coordinates of human body joints. For recognizing an activity, not all the video frames are informative, and only a few key frames can well represent an activity. Moreover, not all joints participate in every activity; i.e., the key joints may vary across frames and activities. In this paper, we propose a novel framework for finding temporal and spatial attentions in a cooperative manner for activity recognition. The proposed method, which is called STH-DRL, consists of a temporal agent and a spatial agent. The temporal agent is responsible for finding the key frames, i.e., temporal hard attention finding, and the spatial agent attempts to find the key joints, i.e., spatial hard attention finding. We formulate the search problems as Markov decision processes and train both agents through interacting with each other using deep reinforcement learning. Experimental results on three widely used activity recognition benchmark datasets demonstrate the effectiveness of our proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323001292",
    "keywords": [
      "Activity recognition",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Geodesy",
      "Geography",
      "Hidden Markov model",
      "Key (lock)",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Programming language",
      "RGB color model",
      "Reinforcement learning",
      "Skeleton (computer programming)"
    ],
    "authors": [
      {
        "surname": "Nikpour",
        "given_name": "Bahareh"
      },
      {
        "surname": "Armanfard",
        "given_name": "Narges"
      }
    ]
  },
  {
    "title": "Deep generative image priors for semantic face manipulation",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109477",
    "abstract": "Previous works on generative adversarial networks (GANs) mainly focus on how to synthesize high-fidelity images. In this paper, we present a framework to leverage the knowledge learned by GANs for semantic face manipulation. In particular, we propose to control the semantics of synthesized faces by adapting the latent codes with an attribute prediction model. Moreover, in order to achieve a more accurate estimation of different facial attributes, we propose to pretrain the attribute prediction model by inverting the synthesized face images back to the GAN latent space. As a result, our method explicitly considers the semantics encoded in the latent space of a pretrained GAN and is able to faithfully edit various attributes like eyeglasses, smiling, bald, age, mustache and gender for high-resolution face images. Extensive experiments show that our method has superior performance compared to state of the art for both face attribute prediction and semantic face manipulation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323001772",
    "keywords": [
      "Artificial intelligence",
      "Bayesian probability",
      "Computer science",
      "Computer vision",
      "Face (sociological concept)",
      "Focus (optics)",
      "Generative grammar",
      "Generative model",
      "Image (mathematics)",
      "Leverage (statistics)",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Prior probability",
      "Programming language",
      "Semantics (computer science)",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Hou",
        "given_name": "Xianxu"
      },
      {
        "surname": "Shen",
        "given_name": "Linlin"
      },
      {
        "surname": "Ming",
        "given_name": "Zhong"
      },
      {
        "surname": "Qiu",
        "given_name": "Guoping"
      }
    ]
  },
  {
    "title": "Exploring the diversity and invariance in yourself for visual pre-training task",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109437",
    "abstract": "Recently, self-supervised learning methods have achieved remarkable success in the visual pre-training task. By simply pulling the different augmented views of each image together or other novel mechanisms, they can learn much unsupervised knowledge and significantly improve the transfer performance of pre-training models. However, these works still cannot avoid the representation collapse problem, i.e., they only focus on limited regions or the extracted features on totally different regions inside each image are nearly the same. Generally, this problem makes the pre-training models cannot sufficiently describe the multi-grained information inside images, which further limits the upper bound of their transfer performance. To alleviate this issue, this paper introduces a simple but effective mechanism, called Exploring the Diversity and Invariance in Yourself (E-DIY). By simply pushing the most different regions inside each augmented view away, E-DIY can preserve the diversity of extracted region-level features. By pulling the most similar regions from different augmented views of the same image together, E-DIY can ensure the robustness of extracted region-level features. Benefiting from the above diversity and invariance exploring mechanism, E-DIY better extracts the multi-grained visual information inside each image compared with previous self-supervised learning approaches. Extensive experiments on various downstream tasks have demonstrated the superiority of our method, e.g., there is 1.9 % improvement (compared with the recent state-of-the-art method, BYOL) on A P 50 metric of detection task on VOC.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323001383",
    "keywords": [
      "Anthropology",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Diversity (politics)",
      "Economics",
      "Gene",
      "Image (mathematics)",
      "Law",
      "Machine learning",
      "Management",
      "Metric (unit)",
      "Operations management",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Robustness (evolution)",
      "Sociology",
      "Task (project management)",
      "Training set"
    ],
    "authors": [
      {
        "surname": "Wei",
        "given_name": "Longhui"
      },
      {
        "surname": "Xie",
        "given_name": "Lingxi"
      },
      {
        "surname": "Zhou",
        "given_name": "Wengang"
      },
      {
        "surname": "Li",
        "given_name": "Houqiang"
      },
      {
        "surname": "Tian",
        "given_name": "Qi"
      }
    ]
  },
  {
    "title": "Adversarial pan-sharpening attacks for object detection in remote sensing",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109466",
    "abstract": "Pan-sharpening, as one of the most commonly used techniques in remote sensing systems, aims to fuse texture-rich PAN images and multi-spectral MS images to obtain texture-rich MS images. With the development of deep learning, CNN based Pan-sharpening methods have received more and more attention in recent years. Since Pan-sharpening technique can integrate the complementary information of Pan and MS images, researchers usually apply object detectors on these pan-sharpened images to achieve reliable detection results. However, recent studies have shown that Deep Learning-based object detection methods are vulnerable to adversarial examples, i.e., adding imperceptible noise to clean images can fool well-trained deep neural networks. It is interesting to combine the pan-sharpening technique with adversarial examples to attack object detectors in remote sensing. In this paper, we propose a framework to generate adversarial pan-sharpened images. Specifically, we propose a two-stream network to generate the pan-sharpened images, and then utilize the shape loss and label loss to perform the attack task. To guarantee the quality of pan-sharpened images, a perceptual loss is utilized to balance spectral preservation and attacking performance. Experimental results demonstrate that the proposed method can generate effective adversarial pan-sharpened images that maintain a high success rate for white-box attacks and achieve transferability for black-box attacks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323001668",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Deep neural networks",
      "Electrical engineering",
      "Engineering",
      "Fuse (electrical)",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Sharpening"
    ],
    "authors": [
      {
        "surname": "Wei",
        "given_name": "Xingxing"
      },
      {
        "surname": "Yuan",
        "given_name": "Maoxun"
      }
    ]
  },
  {
    "title": "Multi-dimensional Graph Neural Network for Sequential Recommendation",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109504",
    "abstract": "Graph neural networks (GNNs) technology has been widely used in recommendation systems because most information in recommendation systems has a graph structure in nature, and GNNs have advantages in graph representation learning. In sequential recommendation, the relationships between interacting items can be constructed as an isomorphic graph, and (GNNs) can capture high-order information between graph nodes. Many models have used graph-based methods for sequential recommendation, and achieved great success. However, the existing research only considers the number of interactions between items when constructing the item graph. As such, revisions are needed to capture the multi-dimensional transformation relationships between items. Hence, we emphasize the importance of multi-dimensional information, and we propose a Category and Time information integrated Graph Neural Network (CTGNN), which combines the item category and interaction time information with a multi-layer graph convolution network to form multi-dimensional fine-grained item representations. In addition, we design a temporal self-attention network to model the dynamic user preference and make the next-item recommendation. Finally, we conduct extensive experiments on three real-world datasets, and the results demonstrate the excellent performance of the proposed model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323002042",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Graph",
      "Machine learning",
      "Recommender system",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Hao",
        "given_name": "Yongjing"
      },
      {
        "surname": "Ma",
        "given_name": "Jun"
      },
      {
        "surname": "Zhao",
        "given_name": "Pengpeng"
      },
      {
        "surname": "Liu",
        "given_name": "Guanfeng"
      },
      {
        "surname": "Xian",
        "given_name": "Xuefeng"
      },
      {
        "surname": "Zhao",
        "given_name": "Lei"
      },
      {
        "surname": "Sheng",
        "given_name": "Victor S."
      }
    ]
  },
  {
    "title": "Discriminative subspace learning via optimization on Riemannian manifold",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109450",
    "abstract": "Discriminative subspace learning is an important problem in machine learning, which aims to find the maximum separable decision subspace. Traditional Euclidean-based methods usually use Fisher discriminant criterion for finding an optimal linear mapping from a high-dimensional data space to a lower-dimensional subspace, which hardly guarantee a quadratic rate of global convergence and suffers from the singularity problem. Here, we propose the manifold optimization-based discriminant analysis (MODA) which is constructed by using the latent subspace alignment and the geometry of objective function with orthogonality constraint. MODA is solved by using Riemannian version of trust-region algorithm. Experimental results on various image datasets and electroencephalogram (EEG) datasets show that MODA achieves the best separability and is significantly superior to the competing algorithms. Especially for the time series of EEG signals, the accuracy of MODA is 20–30% higher than existing algorithms. The code for MODA is available at https://github.com/ncclabsustech/MODA-algorithm.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323001504",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Constraint (computer-aided design)",
      "Discriminative model",
      "Engineering",
      "Geometry",
      "Linear discriminant analysis",
      "Manifold (fluid mechanics)",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Mechanical engineering",
      "Orthogonality",
      "Pattern recognition (psychology)",
      "Riemannian manifold",
      "Subspace topology"
    ],
    "authors": [
      {
        "surname": "Yin",
        "given_name": "Wanguang"
      },
      {
        "surname": "Ma",
        "given_name": "Zhengming"
      },
      {
        "surname": "Liu",
        "given_name": "Quanying"
      }
    ]
  },
  {
    "title": "Underwater object detection algorithm based on feature enhancement and progressive dynamic aggregation strategy",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109511",
    "abstract": "To solve the problems that the conventional object detector is hard to extract features and miss detection of small objects when detecting underwater objects due to the noise of underwater environment and the scale change of objects, this paper designs a novel feature enhancement & progressive dynamic aggregation strategy, and proposes a new underwater object detector based on YOLOv5s. Firstly, a feature enhancement gating module is designed to selectively suppress or enhance multi-level features and reduce the interference of underwater complex environment noise on feature fusion. Then, the adjacent feature fusion mechanism and dynamic fusion module are designed to dynamically learn fusion weights and perform multi-level feature fusion progressively, so as to suppress the conflict information in multi-scale feature fusion and prevent small objects from being submerged by the conflict information. At last, a spatial pyramid pool structure (FMSPP) based on the same size quickly mixed pool layer is proposed, which can make the network obtain stronger description ability of texture and contour features, reduce the parameters, and further improve the generalization ability and classification accuracy. The ablation experiments and multi-method comparison experiments on URPC and DUT-USEG data sets prove the effectiveness of the proposed strategy. Compared with the current mainstream detectors, our detector achieves obvious advantages in detection performance and efficiency.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032300211X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Channel (broadcasting)",
      "Computer science",
      "Computer vision",
      "Detector",
      "Feature (linguistics)",
      "Geodesy",
      "Geography",
      "Geology",
      "Image (mathematics)",
      "Interference (communication)",
      "Linguistics",
      "Noise (video)",
      "Object detection",
      "Oceanography",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Telecommunications",
      "Underwater"
    ],
    "authors": [
      {
        "surname": "Hua",
        "given_name": "Xia"
      },
      {
        "surname": "Cui",
        "given_name": "Xiaopeng"
      },
      {
        "surname": "Xu",
        "given_name": "Xinghua"
      },
      {
        "surname": "Qiu",
        "given_name": "Shaohua"
      },
      {
        "surname": "Liang",
        "given_name": "Yingjie"
      },
      {
        "surname": "Bao",
        "given_name": "Xianqiang"
      },
      {
        "surname": "Li",
        "given_name": "Zhong"
      }
    ]
  },
  {
    "title": "Fast support vector machine training via three-term conjugate-like SMO algorithm",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109478",
    "abstract": "Support vector machine (SVM) is an important class of methods in pattern recognition, and the sequential minimal optimization (SMO) algorithm is one of the most popular methods for training SVM at present. Based on the conjugate sequential minimal optimization algorithm (CSMO), we propose a novel three-term conjugate-like sequential minimal optimization algorithm (TCSMO) for classification and regression tasks. Compared with the CSMO, although the three-term conjugate-like SMO slightly increases the amount of arithmetic operations in each iteration, it significantly reduces the number of iterations required to converge to the specified accuracy and shortens the training time of the SVM. Additionally, we give a convergence proof of the three-term conjugate-like SMO algorithm and four new conjugate parameters. Numerical experiments show that the three-term conjugate-like SMO algorithm performs better numerically in both classification and regression tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323001784",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Conjugate",
      "Conjugate gradient method",
      "Conjugate residual method",
      "Convergence (economics)",
      "Economic growth",
      "Economics",
      "Gradient descent",
      "Mathematical analysis",
      "Mathematics",
      "Physics",
      "Quantum mechanics",
      "Sequential minimal optimization",
      "Support vector machine",
      "Term (time)"
    ],
    "authors": [
      {
        "surname": "Yu",
        "given_name": "Lang"
      },
      {
        "surname": "Li",
        "given_name": "Shengjie"
      },
      {
        "surname": "Liu",
        "given_name": "Siyi"
      }
    ]
  },
  {
    "title": "A bi-level metric learning framework via self-paced learning weighting",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109446",
    "abstract": "Distance metric learning (DML) has achieved great success in many real-world applications. However, most existing DML models characterize the quality of tuples on the tuple level while ignoring the anchor level. Therefore, the models are less accurate to portray the quality of tuples and tend to be over-fitting when anchors are noisy samples. In this paper, we devise a bi-level metric learning framework (BMLF), which characterizes the quality of tuples more finely on both levels, enhancing the generalization performance of the DML model. Furthermore, we present an implementation of BMLF based on a self-paced learning regular term and design the corresponding optimization algorithm. By weighing tuples on the anchor level and training the model using tuples with higher weights preferentially, the side effect of low-quality noisy samples will be alleviated. We empirically demonstrate that the effectiveness and robustness of the proposed method outperform the state-of-the-art methods on several benchmark datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323001462",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Discrete mathematics",
      "Engineering",
      "Gene",
      "Generalization",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Medicine",
      "Metric (unit)",
      "Operations management",
      "Radiology",
      "Robustness (evolution)",
      "Tuple",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Yan",
        "given_name": "Jing"
      },
      {
        "surname": "Wei",
        "given_name": "Wei"
      },
      {
        "surname": "Guo",
        "given_name": "Xinyao"
      },
      {
        "surname": "Dang",
        "given_name": "Chuangyin"
      },
      {
        "surname": "Liang",
        "given_name": "Jiye"
      }
    ]
  },
  {
    "title": "SNRCN2: Steganalysis noise residuals based CNN for source social network identification of digital images",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.05.019",
    "abstract": "Identifying the processing history of digital images related to source acquisition device, image manipulations, and Source Social Media Network (SSMN) identification is important for a forensic analyst to verify image source, trustworthiness, and integrity. Nowadays, social media networks have become a major medium of image sharing and identifying the SSMN of digital images has allured attention in the image forensic scientific community. With the precedence of deep Convolutional Neural Networks (CNNs) in the domain of image forensics, we propose a novel Steganalysis Noise Residuals based CNN (SNRCN2) for digital images SSMN identification. Inspired by the fact that image content can heavily obscures the artifacts of post-processing operations, the proposed CNN utilizes the features from steganalysis-based noise residuals to highlight the artifacts introduced by social media networks. Therefore, the given input image is high-pass filtered using well-known 30 Spatial Rich Model (SRM) filters to obtain noise residuals. Afterward, these noise residuals are passed to an efficient CNN for the extraction of high-level features related to social media networks. Lastly, a fully-connected layer is used for classification purposes. The experimental results show that the proposed model outperforms the existing techniques by providing an image-level accuracy of 99.53 % and 100 % on the VISION and Forchheim datasets, respectively. To further evaluate the robustness of the proposed model, we create a combined dataset that includes the images of common classes from both datasets. The results of the combined dataset further confirm the efficacy of the proposed model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523001393",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Biology",
      "Botany",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Data mining",
      "Digital image",
      "Gene",
      "Identification (biology)",
      "Image (mathematics)",
      "Image processing",
      "Noise (video)",
      "Pattern recognition (psychology)",
      "Robustness (evolution)",
      "Steganalysis",
      "Steganography"
    ],
    "authors": [
      {
        "surname": "Rana",
        "given_name": "Kapil"
      },
      {
        "surname": "Singh",
        "given_name": "Gurinder"
      },
      {
        "surname": "Goyal",
        "given_name": "Puneet"
      }
    ]
  },
  {
    "title": "Segmentation of 3D Point Cloud Data Representing Full Human Body Geometry: A Review",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109444",
    "abstract": "This article aims to present a review of the segmentation techniques of the 3D data representing human body in the form of point clouds. The techniques discussed are divided into three subgroups: 2D contour approaches, topological techniques, and machine learning heuristics. These subgroups are then reviewed regarding the following aspects: computation time, accuracy, reliability, dependency on human pose, and segment count. The authors emphasize an analysis of these algorithms with respect to their exploitation in the segmentation of 3D data varying in time, as well as further improvement of the applications in anthropometry. The conclusion reached is that machine learning approach tends to be the most suitable solution for future 4D applications. Another foreseeable direction of development in the field of segmentation algorithms is the classification of the points on the borders between segments and maintaining fluent and consistent edges of the segments between the subsequent frames.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323001449",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computation",
      "Computer science",
      "Computer vision",
      "Dependency (UML)",
      "Field (mathematics)",
      "Geometry",
      "Heuristics",
      "Image segmentation",
      "Machine learning",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Physics",
      "Point (geometry)",
      "Point cloud",
      "Power (physics)",
      "Pure mathematics",
      "Quantum mechanics",
      "Reliability (semiconductor)",
      "Scale-space segmentation",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Krawczyk",
        "given_name": "Damian"
      },
      {
        "surname": "Sitnik",
        "given_name": "Robert"
      }
    ]
  },
  {
    "title": "Adaptive spatial-temporal surrounding-aware correlation filter tracking via ensemble learning",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109457",
    "abstract": "With the advancement of computer vision, object trackers based on discriminative correlation filters (DCF) have demonstrated superior performance and accuracy compared to traditional trackers. However, most existing DCF-based trackers are easily affected by various factors, such as cluttered background, illumination variations, occlusions, rotations etc. Therefore, in order to accurately track the target, further investigation into the characteristics of the correlation filter is required. In this study, we propose an adaptive spatial-temporal surrounding-aware correlation filter tracker via the ensemble learning (ASTSAELT) technique. Specifically, the adaptive spatial-temporal regularized correlation filter to remove the boundary effects and temporal degradation is presented. And then, a method of extracting surrounding samples according to the size and shape of the tracking object, designed to preserve the integrity of the object, is proposed. Moreover, our tracker utilizes a multi-expert tracking framework to improve its performance by integrating both handcrafted features and deep convolutional layer features. And then, the update strategy is proposed to measure the reliability of the current tracking result and mitigate model corruption. Finally, numerous experiments on visual tracking benchmarks including OTB2013, OTB2015, TempleColor128, UAV123, UAVDT and DTB70 are implemented to verify the developed method achieves superior performance compared with several state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323001577",
    "keywords": [
      "Artificial intelligence",
      "BitTorrent tracker",
      "Computer science",
      "Computer vision",
      "Correlation",
      "Discriminative model",
      "Eye tracking",
      "Filter (signal processing)",
      "Geometry",
      "Mathematics",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Pedagogy",
      "Psychology",
      "Tracking (education)",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Moorthy",
        "given_name": "Sathishkumar"
      },
      {
        "surname": "Joo",
        "given_name": "Young Hoon"
      }
    ]
  },
  {
    "title": "Dense extreme inception network for edge detection",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109461",
    "abstract": "Edge detection is the basis of many computer vision applications. State of the art predominantly relies on deep learning with two decisive factors: dataset content and network architecture. Most of the publicly available datasets are not curated for edge detection tasks. Here, we address this limitation. First, we argue that edges, contours and boundaries, despite their overlaps, are three distinct visual features requiring separate benchmark datasets. To this end, we present a new dataset of edges. Second, we propose a novel architecture, termed Dense Extreme Inception Network for Edge Detection (DexiNed), that can be trained from scratch without any pre-trained weights. DexiNed outperforms other algorithms in the presented dataset. It also generalizes well to other datasets without any fine-tuning. The higher quality of DexiNed is also perceptually evident thanks to the sharper and finer edges it outputs.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323001619",
    "keywords": [
      "Architecture",
      "Art",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Computer security",
      "Edge detection",
      "Enhanced Data Rates for GSM Evolution",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Image processing",
      "Machine learning",
      "Network architecture",
      "Operating system",
      "Pattern recognition (psychology)",
      "Scratch",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Soria",
        "given_name": "Xavier"
      },
      {
        "surname": "Sappa",
        "given_name": "Angel"
      },
      {
        "surname": "Humanante",
        "given_name": "Patricio"
      },
      {
        "surname": "Akbarinia",
        "given_name": "Arash"
      }
    ]
  },
  {
    "title": "ECM-EFS: An ensemble feature selection based on enhanced co-association matrix",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109449",
    "abstract": "Currently, feature selection faces a huge challenge that no single feature selection method can effectively deal with various data sets for all real cases. Ensemble learning is a potential promising solution to address this problem. We propose an ensemble feature selection method based on enhanced co-association matrix (ECM-EFS). Positive-co-association matrix (PCM), negative-co-association matrix (NCM), and relative-co-association matrix (RCM) are first introduced to discover the relationship among features by ensembling the results in multiple feature selection methods. To further produce a more stable feature selection result, “Feature Kernel” is also introduced and used as a starting point for feature selection. Comparative experiments with four state-of-the-art methods have confirmed that the ECM-EFS can provide more robust results. Moreover, compared with traditional ensemble feature selection methods, our method can compensate information loss and reduce computational cost significantly.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323001498",
    "keywords": [
      "Artificial intelligence",
      "Association (psychology)",
      "Chemistry",
      "Chromatography",
      "Combinatorics",
      "Computer science",
      "Data mining",
      "Epistemology",
      "Feature (linguistics)",
      "Feature selection",
      "Kernel (algebra)",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Matrix (chemical analysis)",
      "Minimum redundancy feature selection",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Selection (genetic algorithm)"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Ting"
      },
      {
        "surname": "Hao",
        "given_name": "Yihang"
      },
      {
        "surname": "Yang",
        "given_name": "Bo"
      },
      {
        "surname": "Peng",
        "given_name": "Lizhi"
      }
    ]
  },
  {
    "title": "A simple and effective patch-Based method for frame-level face anti-spoofing",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.04.011",
    "abstract": "With the wide applications of face recognition, face anti-spoofing has become a major challenge for reliable face recognition. Thus, it is necessary to perform face liveness detection. Most existing methods rely on a whole face image for training and testing and are thus susceptible to the overfitting problem because of limited training samples; meanwhile, liveness information is not fully explored. To address these issues, we propose a simple and effective patch-based approach. There are two main contributions: 1) different patch sampling strategies are applied to a training set and a testing set to overcome the overfitting problem, and 2) an attention mechanism is applied to explore more significant information for liveness detection. We evaluate the proposed approach on four popular and challenging databases: the CASIA-SURF, OULU-NPU, CASIA-FASD and REPLAY-ATTACK databases. The proposed method could obtain very promising liveness detection performance. For example, the average classification error rate (ACER) on the CASIA-SURF database (using RGB images only) was 1.6%, which is the lowest reported error rate to the best of our knowledge.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523001198",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Epistemology",
      "Face (sociological concept)",
      "Facial recognition system",
      "Frame (networking)",
      "Liveness",
      "Machine learning",
      "Overfitting",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Set (abstract data type)",
      "Simple (philosophy)",
      "Social science",
      "Sociology",
      "Spoofing attack",
      "Telecommunications",
      "Word error rate"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Shengjie"
      },
      {
        "surname": "Wu",
        "given_name": "Gang"
      },
      {
        "surname": "Yang",
        "given_name": "Yujiu"
      },
      {
        "surname": "Guo",
        "given_name": "Zhenhua"
      }
    ]
  },
  {
    "title": "Approximating dynamic time warping with a convolutional neural network on EEG data",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.05.012",
    "abstract": "Dynamic Time Warping (DTW) is a widely used algorithm for measuring similarities between two time series. It is especially valuable in a wide variety of applications, such as clustering, anomaly detection, classification, or video segmentation, where the time series have different timescales, are irregularly sampled, or are shifted. However, it is not prone to be considered as a loss function in an end-to-end learning framework because of its non-differentiability and its quadratic temporal complexity. While differentiable variants of DTW have been introduced by the community, they still present some drawbacks: computing the distance is still expensive, and this similarity tends to blur some differences in the time series. In this paper, we propose a fast and differentiable approximation of DTW by comparing two architectures: the first one aims to learn an embedding in which the Euclidean distance mimics the DTW, and the second one to directly predict the DTW output value using regression. We build the former by training a siamese neural network to regress the DTW value between two time series. Depending on the nature of the activation function, this approximation naturally supports differentiation, and it is efficient to compute. We show, in a time series retrieval context on EEG datasets, that our methods achieve at least the same level of accuracy as other DTW main approximations with higher computational efficiency. We also show that it can be used to learn in an end-to-end setting on long time series by proposing generative models of EEGs.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523001460",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Cluster analysis",
      "Computer science",
      "Context (archaeology)",
      "Convolutional neural network",
      "Differentiable function",
      "Dynamic time warping",
      "Embedding",
      "Image (mathematics)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Series (stratigraphy)",
      "Similarity (geometry)",
      "Time series"
    ],
    "authors": [
      {
        "surname": "Lerogeron",
        "given_name": "Hugo"
      },
      {
        "surname": "Picot-Clémente",
        "given_name": "Romain"
      },
      {
        "surname": "Rakotomamonjy",
        "given_name": "Alain"
      },
      {
        "surname": "Heutte",
        "given_name": "Laurent"
      }
    ]
  },
  {
    "title": "Dual similarity pre-training and domain difference encouragement learning for vehicle re-identification in the wild",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109513",
    "abstract": "Existing unsupervised domain adaptation (UDA) tasks require extensive annotated wild data in the source domain to be generalized to the target domain. Additionally, the large gap between the source and target domains hinder the clustering performance. Our work concentrates on few-shot UDA task to train a robust Re-ID model from practical vehicle re-identification (Re-ID). That is to say, this task learns discriminative representations from a few labeled source data to unlabeled target data. In this paper, a novel progressive few-shot UDA learning framework for vehicle Re-ID is proposed, which consists of two branches. In source branch, the dual prior model is used to gain the color and IDs in unlabeled source data. A dual constraint label smoothing regularization (DCLSR) loss is designed to supervise extensive unlabeled source data during pre-training phase, which considers color and ID constraints to mine the similarity between unlabeled source data and a few labeled ones. The target branch develops a progressive domain difference encouragement learning method to optimize the cross-domain capability of Re-ID model. The domain difference penalty term (DDPT) is encoded by the feature variations before and after style transfer, which improves clustering results and refines the pseudo label. Comprehensive experimental results verify that the proposed approach outperforms other ones in the practical UDA task.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323002133",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Cluster analysis",
      "Computer science",
      "Computer vision",
      "Discriminative model",
      "Economics",
      "Identification (biology)",
      "Image (mathematics)",
      "Labeled data",
      "Machine learning",
      "Management",
      "Pattern recognition (psychology)",
      "Regularization (linguistics)",
      "Similarity (geometry)",
      "Smoothing",
      "Task (project management)",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Qi"
      },
      {
        "surname": "Zhong",
        "given_name": "Yuling"
      },
      {
        "surname": "Min",
        "given_name": "Weidong"
      },
      {
        "surname": "Zhao",
        "given_name": "Haoyu"
      },
      {
        "surname": "Gai",
        "given_name": "Di"
      },
      {
        "surname": "Han",
        "given_name": "Qing"
      }
    ]
  },
  {
    "title": "Bayesian asymmetric quantized neural networks",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109463",
    "abstract": "This paper develops a robust model compression for neural networks via parameter quantization. Traditionally, quantized neural networks (QNN) were constructed by binary or ternary weights where the weights were deterministic. This paper generalizes QNN in two directions. First, M-ary QNN is developed to adjust the balance between memory storage and model capacity. The representation values and the quantization partitions in M-ary quantization are mutually estimated to enhance the resolution of gradients in neural network training. A flexible quantization with asymmetric partitions is formulated. Second, the variational inference is incorporated to implement the Bayesian asymmetric QNN. The uncertainty of weights is faithfully represented to enhance the robustness of the trained model in presence of heterogeneous data. Importantly, the multiple spike-and-slab prior is proposed to represent the quantization levels in Bayesian asymmetric learning. M-ary quantization is then optimized by maximizing the evidence lower bound of classification network. An adaptive parameter space is built to implement Bayesian quantization and neural representation. The experiments on various image recognition tasks show that M-ary QNN achieves similar performance as the full-precision neural network (FPNN), but the memory cost and the test time are significantly reduced relative to FPNN. The merit of Bayesian M-ary QNN using multiple spike-and-slab prior is investigated.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323001632",
    "keywords": [
      "Algorithm",
      "Arithmetic",
      "Artificial intelligence",
      "Artificial neural network",
      "Bayesian probability",
      "Binary number",
      "Biochemistry",
      "Boltzmann machine",
      "Chemistry",
      "Computer science",
      "Gene",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Quantization (signal processing)",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Chien",
        "given_name": "Jen-Tzung"
      },
      {
        "surname": "Chang",
        "given_name": "Su-Ting"
      }
    ]
  },
  {
    "title": "Markov clustering regularized multi-hop graph neural network",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109518",
    "abstract": "Graph Neural Networks (GNNs) have shown great potential for graph data analysis. In this paper, we focus on multi-hop graph neural networks and aim to extend existing models to a high-order multi-hop form for graph-level representation learning. However, such a directly extending method suffers from two limitations, i.e., computational inefficiency and limited representation ability of the multi-hop neighbor. For the former limitation, we utilize an iteration approach to approximate the power of a complex adjacency matrix to achieve linear computational complexity. For the latter limitation, we introduce the Regularized Markov Clustering (R-MCL) to regularize the flow matrix, i.e., the adjacency matrix, in each iteration step. With these two strategies, we construct Markov Clustering Regularized Multi-hop Graph Neural Network (MCMGN) for graph-level representation learning tasks. Specifically, MCMGN consists of a multi-hop message passing phase and a readout phase, where the multi-hop message passing phase aims to learn multi-hop node embedding, and then the readout phase aggregates multi-hop node representations to generate graph embedding for graph-level representation learning tasks. Extensive experiments on eight graph benchmark datasets strongly demonstrate the effectiveness of Markov Clustering Regularized Multi-hop Graph Neural Network, leading to superior performance on graph classification.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323002182",
    "keywords": [
      "Adjacency matrix",
      "Algorithm",
      "Artificial intelligence",
      "Cluster analysis",
      "Clustering coefficient",
      "Computer science",
      "Graph",
      "Graph partition",
      "Machine learning",
      "Markov chain",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Fan",
        "given_name": "Xiaolong"
      },
      {
        "surname": "Gong",
        "given_name": "Maoguo"
      },
      {
        "surname": "Wu",
        "given_name": "Yue"
      }
    ]
  },
  {
    "title": "Meta-learning with topic-agnostic representations for zero-shot stance detection",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.05.006",
    "abstract": "Zero-shot stance detection (ZSSD) aims to classify stances on unseen topic data with the ability of a model to generalize in the range of topics in the real world. The key point of ZSSD is to prevent the model from overfitting on seen topic data and show robust stance recognition performance on unseen topic samples. In this paper, we propose a new ZSSD framework to generalize stance detection performance by alleviating seen data information from the encoder and focusing on improving stance classification ability in zero-shot conditions. The proposed framework involves two learning stages contributing to ZSSD: topic-agnostic text encoder learning and zero-shot meta-learning. Our framework achieves notable improvements on the three benchmark zero-shot stance detection datasets and zero-shot aspect target sentiment classification dataset showing the effectiveness of our method in the zero-shot settings.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523001332",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Linguistics",
      "Natural language processing",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Shot (pellet)",
      "Theoretical computer science",
      "Zero (linguistics)"
    ],
    "authors": [
      {
        "surname": "Choi",
        "given_name": "Kyuri"
      },
      {
        "surname": "Ko",
        "given_name": "Youngjoong"
      }
    ]
  },
  {
    "title": "A survey of human-computer interaction (HCI) & natural habits-based behavioural biometric modalities for user recognition schemes",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109453",
    "abstract": "The proliferation of Internet of Things (IoT) systems is having a profound impact across all aspects of life. Recognising and identifying particular users is central to delivering the personalised experience that citizens want to experience, and that organisations wish to deliver. This article presents a survey of human-computer interaction-based (HCI-based) and natural habits-based behavioural biometrics that can be acquired unobtrusively through smart devices or IoT sensors for user recognition purposes. Robust and usable user recognition is also a security requirement for emerging IoT ecosystems to protect them from adversaries. Typically, it can be specified as a fundamental building block for most types of human-to-things accountability principles and access-control methods. However, end-users are facing numerous security and usability challenges in using currently available knowledge- and token-based recognition (i.e., authentication and identification) schemes. To address the limitations of conventional recognition schemes, biometrics, naturally come as a first choice to supporting sophisticated user recognition solutions. We perform a comprehensive review of touch-stroke, swipe, touch signature, hand-movements, voice, gait and footstep behavioural biometrics modalities. This survey analyzes the recent state-of-the-art research of these behavioural biometrics with a goal to identify their attributes and features for generating unique identification signatures. Finally, we present security, privacy, and usability evaluations that can strengthen the designing of robust and usable user recognition schemes for IoT applications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032300153X",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Biometric data",
      "Biometrics",
      "Computer science",
      "Human–computer interaction",
      "Modalities",
      "Natural (archaeology)",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Gupta",
        "given_name": "Sandeep"
      },
      {
        "surname": "Maple",
        "given_name": "Carsten"
      },
      {
        "surname": "Crispo",
        "given_name": "Bruno"
      },
      {
        "surname": "Raja",
        "given_name": "Kiran"
      },
      {
        "surname": "Yautsiukhin",
        "given_name": "Artsiom"
      },
      {
        "surname": "Martinelli",
        "given_name": "Fabio"
      }
    ]
  },
  {
    "title": "Self-supervised learning of scene flow with occlusion handling through feature masking",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109487",
    "abstract": "In this work, we improve the optical flow and depth based on the important observation that they should share the geometric structure of the reference image. We initially propose a feature masking method to reduce the occlusion impact on the optical flow and depth by producing preliminary motion features that share the structure of the reference image. In addition, we propose a deformable decoder that learns the geometrical structure of the reference image in the form of a group of offsets and uses them to adapt the motion features of flow and depth maps, thereby preventing incorrect propagation in occluded regions and providing more structural details in the other regions. Furthermore, we recursively update the optical flow with self-supervised cues learned from the rigid flow and optical flow. Our method achieves a new state-of-the-art result for the optical flow on the KITTI 2015 benchmark with F1 = 11.17%.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323001875",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Flow (mathematics)",
      "Geodesy",
      "Geography",
      "Geometry",
      "Image (mathematics)",
      "Linguistics",
      "Masking (illustration)",
      "Mathematics",
      "Motion (physics)",
      "Optical flow",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Xiang",
        "given_name": "Xuezhi"
      },
      {
        "surname": "Abdein",
        "given_name": "Rokia"
      },
      {
        "surname": "Lv",
        "given_name": "Ning"
      },
      {
        "surname": "Yang",
        "given_name": "Jie"
      }
    ]
  },
  {
    "title": "VLCDoC: Vision-Language contrastive pre-training model for cross-Modal document classification",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109419",
    "abstract": "Multimodal learning from document data has achieved great success lately as it allows to pre-train semantically meaningful features as a prior into a learnable downstream task. In this paper, we approach the document classification problem by learning cross-modal representations through language and vision cues, considering intra- and inter-modality relationships. Instead of merging features from different modalities into a joint representation space, the proposed method exploits high-level interactions and learns relevant semantic information from effective attention flows within and across modalities. The proposed learning objective is devised between intra- and inter-modality alignment tasks, where the similarity distribution per task is computed by contracting positive sample pairs while simultaneously contrasting negative ones in the joint representation space. Extensive experiments on public benchmark datasets demonstrate the effectiveness and the generality of our model both on low-scale and large-scale datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323001206",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Chemistry",
      "Computer science",
      "Computer security",
      "Economics",
      "Exploit",
      "Generality",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Law",
      "Machine learning",
      "Management",
      "Modal",
      "Modalities",
      "Modality (human–computer interaction)",
      "Natural language processing",
      "Operating system",
      "Physics",
      "Political science",
      "Politics",
      "Polymer chemistry",
      "Psychology",
      "Psychotherapist",
      "Quantum mechanics",
      "Representation (politics)",
      "Scale (ratio)",
      "Similarity (geometry)",
      "Social science",
      "Sociology",
      "Space (punctuation)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Bakkali",
        "given_name": "Souhail"
      },
      {
        "surname": "Ming",
        "given_name": "Zuheng"
      },
      {
        "surname": "Coustaty",
        "given_name": "Mickael"
      },
      {
        "surname": "Rusiñol",
        "given_name": "Marçal"
      },
      {
        "surname": "Terrades",
        "given_name": "Oriol Ramos"
      }
    ]
  },
  {
    "title": "Conditional invertible image re-scaling",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109459",
    "abstract": "One of the key issue for image re-scaling is modeling the down-sampling loss. The loss can be approached by a prior distribution through an invertible network in recent works. However, this assumption of independence between images and down-sampling losses is not always satisfied in practice. To address the above problem, we design a module to learn a latent variable representing the down-scaling loss conditional on low-resolution(LR) images according to the information entropy theory. Unlike previous methods, the down-scaling loss is recovered through the proposed module whose inputs are both LR images and predefined distributions. The difference between the high-resolution(HR) image and the low-resolution(LR) image is represented with a latent variable through a lossless invertible neural network. In addition, a conditional entropy loss is proposed to train the invertible neural network to suppress the conditional entropy between LR images and latent variables. It helps to decrease the effect of the latent variable to generate the HR image from LR images during the up-scaling procedure. We evaluate the proposed method on widely used data sets, and the experimental results demonstrate that our proposed method performs favorably against SOTA methods in terms of PSNR and SSIM metrics.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323001590",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Conditional entropy",
      "Data compression",
      "Entropy (arrow of time)",
      "Geometry",
      "Invertible matrix",
      "Latent variable",
      "Lossless compression",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Principle of maximum entropy",
      "Pure mathematics",
      "Quantum mechanics",
      "Scaling"
    ],
    "authors": [
      {
        "surname": "Zha",
        "given_name": "Yufei"
      },
      {
        "surname": "Li",
        "given_name": "Fan"
      },
      {
        "surname": "Zhang",
        "given_name": "Peng"
      },
      {
        "surname": "Huang",
        "given_name": "Wei"
      }
    ]
  },
  {
    "title": "Multi-agent dueling Q-learning with mean field and value decomposition",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109436",
    "abstract": "A great deal of multi agent reinforcement learning(MARL) work has investigated how multiple agents effectively accomplish cooperative tasks utilizing value function decomposition methods. However, existing value decomposition methods can only handle cooperative tasks with shared reward, due to these methods factorize the value function from a global perspective. To tackle the competitive tasks and mixed cooperative-competitive tasks with differing individual reward setting, we design the Multi-agent Dueling Q-learning (MDQ) method based on mean-filed theory and individual value decomposition. Specifically, we integrate the mean-field theory with the value decomposition to factorize the value function at the individual level, which can deal with mixed cooperative-competitive tasks. Besides, we take a dueling network architecture to distinguish which states are valuable, eliminating the need to learn the impact of each action on each state, therefore enabling efficient learning and leading to better policy evaluation. The proposed method MDQ is applicable not only to cooperative tasks with shared rewards setting, but also to mixed cooperative-competitive tasks with individualized reward settings. In this end, it is flexible and generically applicable enough to most multi-agent tasks. Empirical experiments on various mixed cooperative-competitive tasks demonstrate that MDQ significantly outperforms existing multi agent reinforcement learning methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323001371",
    "keywords": [
      "Artificial intelligence",
      "Bellman equation",
      "Biology",
      "Competitive learning",
      "Computer science",
      "Decomposition",
      "Ecology",
      "Evolutionary biology",
      "Field (mathematics)",
      "Function (biology)",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Perspective (graphical)",
      "Pure mathematics",
      "Reinforcement learning",
      "Unsupervised learning",
      "Value (mathematics)"
    ],
    "authors": [
      {
        "surname": "Ding",
        "given_name": "Shifei"
      },
      {
        "surname": "Du",
        "given_name": "Wei"
      },
      {
        "surname": "Ding",
        "given_name": "Ling"
      },
      {
        "surname": "Guo",
        "given_name": "Lili"
      },
      {
        "surname": "Zhang",
        "given_name": "Jian"
      },
      {
        "surname": "An",
        "given_name": "Bo"
      }
    ]
  },
  {
    "title": "Editorial for special section ‘ICPR 2020’",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.05.023",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523001587",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Combinatorics",
      "Composite material",
      "Computer science",
      "Decomposition",
      "Ecology",
      "Epistemology",
      "Flexibility (engineering)",
      "Kernel (algebra)",
      "Kernel method",
      "Law",
      "Machine learning",
      "Materials science",
      "Mathematical optimization",
      "Mathematics",
      "Matrix (chemical analysis)",
      "Parametric statistics",
      "Philosophy",
      "Political science",
      "Politics",
      "Property (philosophy)",
      "Rank (graph theory)",
      "Representation (politics)",
      "Statistics",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Tistarelli",
        "given_name": "Massimo"
      }
    ]
  },
  {
    "title": "CanBiPT: Cancelable biometrics with physical template",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.06.016",
    "abstract": "Cancelable biometrics are designed to solve the problem where biological characteristics cannot be cancelled and reissued. At present, most of the cancelable biometrics are carried out in the digital domain, therefore we call them cancelable biometrics with soft-template. In this paper, we propose a method called Cancelable Biometrics with Physical-Template (CanBiPT). The physical-template is a printed sticker that can be worn on the selected region of the face. First, in the face image, we select a specific region referring to the entropy between the faces with and without the physical-template, i.e., the sticker. Second, we generate an image with individual features and this image is called the physical-template which can be printed as a sticker. And the sticker is worn on the selected region of the face in the first step and the face with the sticker is used for authentication, recognition and so on. Different physical-templates can be formed by changing the regions (forehead, nose or mouth) or appearance according to the parameters in the algorithm. At the same time, the image printed as the sticker can also be used for the soft-template. The experiments on the public datasets, i.e., LFW and MS-Celeb-1M datasets and our own dataset, i.e., CanBiPT dataset, demonstrate the feasibility and effectiveness of the proposed method with both physical-templates and soft-templates.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523001952",
    "keywords": [
      "Artificial intelligence",
      "Authentication (law)",
      "Biometrics",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Entropy (arrow of time)",
      "Face (sociological concept)",
      "Forehead",
      "Image (mathematics)",
      "Medicine",
      "Pattern recognition (psychology)",
      "Physics",
      "Programming language",
      "Quantum mechanics",
      "Social science",
      "Sociology",
      "Surgery",
      "Template"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Hao"
      },
      {
        "surname": "Gao",
        "given_name": "Youjun"
      },
      {
        "surname": "Liu",
        "given_name": "Chengcheng"
      },
      {
        "surname": "Sun",
        "given_name": "Jiande"
      },
      {
        "surname": "Guo",
        "given_name": "Xin"
      },
      {
        "surname": "Zhang",
        "given_name": "Huaxiang"
      },
      {
        "surname": "Wan",
        "given_name": "Wenbo"
      }
    ]
  },
  {
    "title": "Improved prototypical network for active few-shot learning",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.06.015",
    "abstract": "Different from deep learning with large scale supervision, few-shot learning aims to learn the samples’ characteristics from few labeled examples. Apparently, few-shot learning is more in line with the visual cognitive mechanism of the human brain. In recent years, few-shot learning has attracted more researchers’ attention. They all assume that each category only contains a few labeled samples and without unlabeled samples during the few-shot training. However, in reality, each visual categories also includes some unlabeled samples which store rich semantic information. Labeling all unlabeled samples is time-consuming and laborious. Therefore, we combine the few-shot learning and active learning together, and propose an active few-shot learning model (AC-FSL) based on prototype network. This model includes two core modules: few-shot classification module and loss prediction module. The former is used for classification task under few labeled samples, while the latter is applied to choose some unlabeled samples of high value for labeling, and then assist for the few-shot classification task. We conduct extensive experiments on two image benchmark datasets. The results show that the proposed model AC-FSL can effectively improve the classification performance of current methods on few-shot setting.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523001940",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Chemistry",
      "Computer science",
      "Economics",
      "Engineering",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Management",
      "Mechanical engineering",
      "One shot",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Shot (pellet)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Yaqiang"
      },
      {
        "surname": "Li",
        "given_name": "Yifei"
      },
      {
        "surname": "Zhao",
        "given_name": "Tianzhe"
      },
      {
        "surname": "Zhang",
        "given_name": "Lingling"
      },
      {
        "surname": "Wei",
        "given_name": "Bifan"
      },
      {
        "surname": "Liu",
        "given_name": "Jun"
      },
      {
        "surname": "Zheng",
        "given_name": "Qinghua"
      }
    ]
  },
  {
    "title": "Cloud-VAE: Variational autoencoder with concepts embedded",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109530",
    "abstract": "Variational Autoencoder (VAE) has been widely and successfully used in learning coherent latent representation of data. However, the lack of interpretability in the latent space constructed by the VAE under the prior distribution is still an urgent problem. This paper proposes a VAE with understandable concept embedding named Cloud-VAE, which constructs interpretable latent space by disentangling the latent variables and considering their uncertainty based on cloud model. Firstly, cloud model-based clustering algorithm cast initial constraint of latent space into a prior distribution of concept which can be embedded into the latent space of the VAE to disentangle the latent variables. Secondly, reparameterization trick based on forward cloud transformation algorithm is designed to estimate the latent space concept by increasing the randomness of latent variables. Furthermore, variational lower bound of Cloud-VAE is derived to guide the training process to construct concepts of latent space, realizing the mutual mapping between latent space and concept space. Finally, experimental results on 6 benchmark datasets show that Cloud-VAE has good clustering and reconstruction performance, which can explicitly explain the aggregation process of the model and discover more interpretable disentangled representations.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323002303",
    "keywords": [
      "Artificial intelligence",
      "Autoencoder",
      "Cloud computing",
      "Cluster analysis",
      "Computer science",
      "Deep learning",
      "Embedding",
      "Interpretability",
      "Latent variable",
      "Latent variable model",
      "Law",
      "Machine learning",
      "Operating system",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Space (punctuation)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Yue"
      },
      {
        "surname": "Liu",
        "given_name": "Zitu"
      },
      {
        "surname": "Li",
        "given_name": "Shuang"
      },
      {
        "surname": "Yu",
        "given_name": "Zhenyao"
      },
      {
        "surname": "Guo",
        "given_name": "Yike"
      },
      {
        "surname": "Liu",
        "given_name": "Qun"
      },
      {
        "surname": "Wang",
        "given_name": "Guoyin"
      }
    ]
  },
  {
    "title": "Crowdmeta: Crowdsourcing truth inference with meta-Knowledge transfer",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109525",
    "abstract": "Crowdsourcing provides a fast and low-cost solution to collect annotations for training data in computer vision. However, there are two challenges in crowdsourced image annotation: First, when crowdsourced workers perform annotation tasks in an unfamiliar domain, their accuracy will dramatically decline due to the lack of expertise; Second, the difficulties of tasks may be different due to the noises in images, which is only related to the features of images themselves and will affect the judgment of workers. It is well known that transferring knowledge from relevant domains can form a better representation for training samples, which benefits the estimation of workers’ expertise in truth inference models. However, the existing knowledge transfer processes for crowdsourcing require a considerable number of well-collected samples in source domains. Comprehensively considering the above issues, this paper proposes a novel probabilistic model for crowdsourcing truth inference, which fuses few-shot meta-learning and transfer learning. The proposed model transfers meta-knowledge from the source domain to form better high-level representations of the instances in the target domain. Simultaneously utilizing both high-level representations and instance features, the quality of workers and the difficulty of instances can be better modeled and inferred. Experimental results on a number of datasets show that the proposed model not only outperforms the state-of-the-art models but also significantly reduces the number of instances required in the source domain.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032300225X",
    "keywords": [
      "Annotation",
      "Artificial intelligence",
      "Computer science",
      "Crowdsourcing",
      "Domain (mathematical analysis)",
      "Domain knowledge",
      "Epistemology",
      "Ground truth",
      "Inference",
      "Knowledge management",
      "Knowledge transfer",
      "Law",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Philosophy",
      "Political science",
      "Politics",
      "Probabilistic logic",
      "Quality (philosophy)",
      "Representation (politics)",
      "Transfer of learning",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Jing"
      },
      {
        "surname": "Xu",
        "given_name": "Sunyue"
      },
      {
        "surname": "Sheng",
        "given_name": "Victor S."
      }
    ]
  },
  {
    "title": "Sparse fooling images: Fooling machine perception through unrecognizable images",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.07.003",
    "abstract": "Fooling images are potential threats to deep neural networks (DNNs). These images cannot be recognized by humans as natural objects, e.g., dogs and cats. However, they are misclassified by DNNs as natural object classes with high confidence scores. Despite their original design concept, existing fooling images, if closely examined, can be seen to retain some features that are characteristic of the target objects. Hence, DNNs can react to these features. In this study, we evaluate whether fooling images with no characteristic pattern of natural objects, either locally or globally, can exist. As a minimal case, we introduce single-color images with a few pixels altered, called sparse fooling images (SFIs). We first prove that SFIs always exist under mild conditions for linear and nonlinear models and reveal that complex models are more likely to be vulnerable to SFI attacks. Using two SFI generation methods, we demonstrate that in deeper layers, SFIs have features similar to those of natural images. Therefore, they fool DNNs successfully. Among the other layers, we discover that the max-pooling layer causes vulnerability to SFIs. The defense against SFIs and transferability are also discussed. This study highlights a new vulnerability of DNNs by introducing a novel class of images that are distributed extremely far from natural images.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523002003",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Image (mathematics)",
      "Nonlinear system",
      "Pattern recognition (psychology)",
      "Physics",
      "Pixel",
      "Pooling",
      "Quantum mechanics",
      "Vulnerability (computing)"
    ],
    "authors": [
      {
        "surname": "Kumano",
        "given_name": "Soichiro"
      },
      {
        "surname": "Kera",
        "given_name": "Hiroshi"
      },
      {
        "surname": "Yamasaki",
        "given_name": "Toshihiko"
      }
    ]
  },
  {
    "title": "Learning smooth dendrite morphological neurons for pattern classification using linkage trees and evolutionary-based hyperparameter tuning",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.05.024",
    "abstract": "The current learning approach for smooth dendrite morphological neurons (DMNs) determines dendrite parameters using k -means clustering, which is non-reproducible due to its stochastic nature, risking falling into local minima. To overcome this issue, we introduce a DMN learning approach based on a deterministic hierarchical clustering method, which builds a linkage tree for each class of patterns. In addition, a micro genetic algorithm automatically tunes the cut-off points in the linkage trees hierarchy to create suitable clusters of dendrites. The classification experiments consider 40 real-world datasets. The proposed approach outperforms three DMN models in classification performance and is quite competitive with a hybrid morphological-linear perceptron, multilayer perceptron, random forest, and support vector machine. Therefore, the proposed method is a suitable alternative for pattern classification applications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523001605",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Cluster analysis",
      "Computer science",
      "Evolutionary algorithm",
      "Hyperparameter",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Maxima and minima",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Soma",
      "Tree (set theory)"
    ],
    "authors": [
      {
        "surname": "Tovias-Alanis",
        "given_name": "Samuel Omar"
      },
      {
        "surname": "Sossa",
        "given_name": "Humberto"
      },
      {
        "surname": "Gómez-Flores",
        "given_name": "Wilfrido"
      }
    ]
  },
  {
    "title": "Dense light field reconstruction based on epipolar focus spectrum",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109551",
    "abstract": "Existing light field (LF) representations, such as epipolar plane image (EPI) and sub-aperture images, do not consider the structural characteristics across the views, so they usually require additional disparity and spatial structure cues for follow-up tasks. Besides, they have difficulties dealing with occlusions or large disparity scenes. To this end, this paper proposes a novel Epipolar Focus Spectrum (EFS) representation by rearranging the EPI spectrum. Different from the classical EPI representation where an EPI line corresponds to a specific depth, there is a one-to-one mapping from the EFS line to the view. By exploring the EFS sampling task, the analytical function is derived for constructing a non-aliasing EFS. To demonstrate its effectiveness, we develop a trainable EFS-based pipeline for light field reconstruction, where a dense light field can be reconstructed by compensating the missing EFS lines given a sparse light field, yielding promising results with cross-view consistency, especially in the presence of severe occlusion and large disparity. Experimental results on both synthetic and real-world datasets demonstrate the validity and superiority of the proposed method over SOTA methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323002510",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Epipolar geometry",
      "Field (mathematics)",
      "Focus (optics)",
      "Image (mathematics)",
      "Law",
      "Light field",
      "Mathematics",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Pipeline (software)",
      "Political science",
      "Politics",
      "Programming language",
      "Pure mathematics",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Yaning"
      },
      {
        "surname": "Wang",
        "given_name": "Xue"
      },
      {
        "surname": "Zhu",
        "given_name": "Hao"
      },
      {
        "surname": "Zhou",
        "given_name": "Guoqing"
      },
      {
        "surname": "Wang",
        "given_name": "Qing"
      }
    ]
  },
  {
    "title": "Hierarchical disentangling network for object representation learning",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109539",
    "abstract": "An object can be described as the combination of primary visual attributes. Disentangling such underlying primitives is the long-term objective of representation learning. It is observed that categories have natural hierarchical characteristics, i.e., any two objects can share some common primitives at a particular category level while possess unique traits at another. However, previous works usually operate in a flat manner (i.e., at a particular level) to disentangle the representations of objects. Even though they may obtain the primitives to constitute objects as the categories at that level, their results are obviously not efficient and complete. In this paper, we propose a Hierarchical Disentangling Network (HDN) to exploit the rich hierarchical characteristics among categories to divide the disentangling process in a coarse-to-fine manner (i.e., level-wise), such that each level only focuses on learning the specific representations and finally the common and unique representations at all levels jointly constitute the raw object. Specifically, HDN is designed based on an encoder-decoder architecture. To simultaneously ensure the level-wise disentanglement and interpretability of the encoded representations, a novel hierarchical Generative Adversarial Network (GAN) is introduced. Quantitative and qualitative evaluations on popular object datasets validate the effectiveness of our method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032300239X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Exploit",
      "Feature (linguistics)",
      "Generative grammar",
      "Interpretability",
      "Law",
      "Linguistics",
      "Machine learning",
      "Object (grammar)",
      "Operating system",
      "Philosophy",
      "Political science",
      "Politics",
      "Process (computing)",
      "Representation (politics)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Qiao",
        "given_name": "Shishi"
      },
      {
        "surname": "Wang",
        "given_name": "Ruiping"
      },
      {
        "surname": "Shan",
        "given_name": "Shiguang"
      },
      {
        "surname": "Chen",
        "given_name": "Xilin"
      }
    ]
  },
  {
    "title": "Rigorous non-disjoint discretization for naive Bayes",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109554",
    "abstract": "Naive Bayes is a classical machine learning algorithm for which discretization is commonly used to transform quantitative attributes into qualitative attributes. Of numerous discretization methods, Non-Disjoint Discretization (NDD) proposes a novel perspective by forming overlapping intervals and always locating a value toward the middle of an interval. However, existing approaches to NDD fail to adequately consider the effect of multiple occurrences of a single value — a commonly occurring circumstance in practice. By necessity, all occurrences of a single value fall within the same interval. As a result, it is often not possible to discretize an attribute into intervals containing equal numbers of training instances. Current methods address this issue in an ad hoc manner, reducing the specificity of the resulting atomic intervals. In this study, we propose a non-disjoint discretization method for NB, called Rigorous Non-Disjoint Discretization (RNDD), that handles multiple occurrences of a single value in a systematic manner. Our extensive experimental results suggest that RNDD significantly outperforms NDD along with all other existing state-of-the-art competitors.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323002546",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Discrete mathematics",
      "Discretization",
      "Discretization error",
      "Discretization of continuous features",
      "Disjoint sets",
      "Interval (graph theory)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Value (mathematics)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Huan"
      },
      {
        "surname": "Jiang",
        "given_name": "Liangxiao"
      },
      {
        "surname": "Webb",
        "given_name": "Geoffrey I."
      }
    ]
  },
  {
    "title": "FSConv: Flexible and separable convolution for convolutional neural networks compression",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109589",
    "abstract": "Because of limited computation resources, convolutional neural networks (CNNs) are difficult to deploy on mobile devices. To overcome this issue, many methods have successively reduced parameters in CNNs with the idea of removing redundancy among feature maps. We observe similarities between feature maps at the same layer but not complete consistency. Intuitively, the difference between similar feature maps is an essential ingredient for the success of CNNs. Therefore, we propose a flexible and separable convolution (FSConv) in a different perspective to embrace redundancy while requiring less computation, which can implicitly cluster feature maps into different clusters without introducing similarity measurements. Our proposed model extracts intrinsic information from the representative part through ordinary convolution in each cluster and reveals tiny hidden details from the redundant part through groupwise/depthwise convolution. Experimental results demonstrate that FSConv-equipped networks always perform better than previous state-of-the-art CNNs compression algorithms. Code is available at https://github.com/Clarkxielf/FSConv-Flexible-and-Separable-Convolution-for-Convolutional-Neural-Networks-Compression.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032300290X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computation",
      "Computer science",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Feature (linguistics)",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Redundancy (engineering)",
      "Separable space",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Zhu",
        "given_name": "Yangyang"
      },
      {
        "surname": "Xie",
        "given_name": "Luofeng"
      },
      {
        "surname": "Xie",
        "given_name": "Zhengfeng"
      },
      {
        "surname": "Yin",
        "given_name": "Ming"
      },
      {
        "surname": "Yin",
        "given_name": "Guofu"
      }
    ]
  },
  {
    "title": "RareAnom: A Benchmark Video Dataset for Rare Type Anomalies",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109567",
    "abstract": "Existing video anomaly detection methods and datasets suffer from restricted anomaly categories containing single-source (CCTV) videos recorded in controlled environment, inadequate annotations, and lack of adequate supervision. To mitigate these problems, we introduce a new dataset (RareAnom) containing 17 rare types of real-world anomalies (2200 videos) recorded using multiple sources (e.g., CCTV, handheld cameras, dash-cams, and mobile phones) with rich temporal annotations. A new fully unsupervised anomaly detection and classification method has been proposed. It has three stages: training of a 3D Convolution Autoencoder using pseudo-labelled video segments, anomaly detection using latent features, and classification. Unlike the existing datasets, we have benchmarked RareAnom using three levels of supervision: fully, weakly, and unsupervised. It has been compared with UCF-Crime and XD-Violence datasets. The proposed anomaly detection and classification method beats the latest unsupervised methods by 4.49%, 8.66%, and 6.77% on RareAnom, UCF-Crime, and XD-violence datasets, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323002674",
    "keywords": [
      "Anomaly (physics)",
      "Anomaly detection",
      "Artificial intelligence",
      "Artificial neural network",
      "Autoencoder",
      "Benchmark (surveying)",
      "Cartography",
      "Computer science",
      "Condensed matter physics",
      "Convolution (computer science)",
      "Deep learning",
      "Geography",
      "Mobile device",
      "Operating system",
      "Pattern recognition (psychology)",
      "Physics"
    ],
    "authors": [
      {
        "surname": "Thakare",
        "given_name": "Kamalakar Vijay"
      },
      {
        "surname": "Dogra",
        "given_name": "Debi Prosad"
      },
      {
        "surname": "Choi",
        "given_name": "Heeseung"
      },
      {
        "surname": "Kim",
        "given_name": "Haksub"
      },
      {
        "surname": "Kim",
        "given_name": "Ig-Jae"
      }
    ]
  },
  {
    "title": "Global spatio-temporal synergistic topology learning for skeleton-based action recognition",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109540",
    "abstract": "Compared to RGB video-based action recognition, skeleton-based action recognition algorithm has attracted much more attention due to being more lightweight, better generalization and robustness. The extraction of temporal and spatial features is a crucial factor for skeleton-based action recognition. However, existing feature extraction methods suffer from two limitations: (1) the isolated extraction of temporal and spatial feature cannot capture temporal feature connections among non-adjacent joints and (2) convolution-limited perceptual fields cannot capture global temporal features of joints effectively. In this work, we propose a global spatio-temporal synergistic feature learning module (GSTL), which generates global spatio-temporal synergistic topology of joints by spatio-temporal feature fusion. By further combining the GSTL with a temporal modeling unit, we develop a powerful global spatio-temporal synergistic topology learning network (GSTLN), and it achieves competitive performance with fewer parameters on three challenge datasets: NTU RGB + D, NTU RGB + D 120, and NW-UCLA.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323002406",
    "keywords": [
      "Action recognition",
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Class (philosophy)",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Feature (linguistics)",
      "Feature extraction",
      "Feature learning",
      "Gene",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "RGB color model",
      "Robustness (evolution)",
      "Topology (electrical circuits)"
    ],
    "authors": [
      {
        "surname": "Dai",
        "given_name": "Meng"
      },
      {
        "surname": "Sun",
        "given_name": "Zhonghua"
      },
      {
        "surname": "Wang",
        "given_name": "Tianyi"
      },
      {
        "surname": "Feng",
        "given_name": "Jinchao"
      },
      {
        "surname": "Jia",
        "given_name": "Kebin"
      }
    ]
  },
  {
    "title": "Line graph contrastive learning for link prediction",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109537",
    "abstract": "Link prediction tasks focus on predicting possible future connections. Most existing researches measure the likelihood of links by different similarity scores on node pairs and predict links between nodes. However, the similarity-based approaches have some challenges in information loss on nodes and generalization ability on similarity indexes. To address the above issues, we propose a Line Graph Contrastive Learning (LGCL) method to obtain rich information with multiple perspectives. LGCL obtains a subgraph view by h -hop subgraph sampling with target node pairs. After transforming the sampled subgraph into a line graph, the link prediction task is converted into a node classification task, which graph convolution progress can learn edge embeddings from graphs more effectively. Then we design a novel cross-scale contrastive learning framework on the line graph and the subgraph to maximize the mutual information of them, so that fuses the structure and feature information. The experimental results demonstrate that the proposed LGCL outperforms the state-of-the-art methods and has better performance on generalization and robustness.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323002376",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Gene",
      "Graph",
      "Pattern recognition (psychology)",
      "Robustness (evolution)",
      "Subgraph isomorphism problem",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Zehua"
      },
      {
        "surname": "Sun",
        "given_name": "Shilin"
      },
      {
        "surname": "Ma",
        "given_name": "Guixiang"
      },
      {
        "surname": "Zhong",
        "given_name": "Caiming"
      }
    ]
  },
  {
    "title": "A handwritten ancient text detector based on improved feature pyramid network",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.06.013",
    "abstract": "Text detection is the primary task for digitization of ancient books. Different from the common scene text detection tasks (ICDAR, TotalText, etc.), the texts in handwritten ancient documents are more densely distributed and generally small objects; at the same time, the layout structure is also more complex, with problems such as mixed arrangement of pictures and texts and high background noise, all of which pose challenges for detection. According to the characteristics of ancient book images, this paper proposes a new fusion structure based on Feature Pyramid Networks, and takes FCOS as the baseline model to form a new detector (named RFCOS). We enhance the detection capability for dense and small text instances by adding bottom-up fusion paths, cross-layer connections and weighted fusion. Meanwhile, the loss of high-level feature maps during fusion is reduced by new upsampling method and lateral connections. We verified the effectiveness of our RFCOS on the HWAD (Handwritten Ancient Books Dataset), a dataset containing samples in four languages - Yi, Chinese, Tibetan and Tangut, and verify the generalization of RFCOS on another public dataset MTHv2. The results show that RFCOS outperformed most of the existing text detectors in terms of precision, recall and F-measure.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523001927",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Detector",
      "Digitization",
      "Economics",
      "Feature (linguistics)",
      "Generalization",
      "Geometry",
      "Image (mathematics)",
      "Layer (electronics)",
      "Linguistics",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Natural language processing",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pooling",
      "Pyramid (geometry)",
      "Task (project management)",
      "Telecommunications",
      "Upsampling"
    ],
    "authors": [
      {
        "surname": "Feng",
        "given_name": "Ruiqi"
      },
      {
        "surname": "Zhao",
        "given_name": "Fujia"
      },
      {
        "surname": "Chen",
        "given_name": "Shanxiong"
      },
      {
        "surname": "Zhang",
        "given_name": "Shixue"
      },
      {
        "surname": "Zhu",
        "given_name": "Shiyu"
      }
    ]
  },
  {
    "title": "The reversibility of cancelable biometric templates based on iterative perturbation stochastic approximation strategy",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.06.014",
    "abstract": "Researchers have been proposing different matching frameworks for cancelable template design to improve the matching performance and security of biometric authentication systems. Despite the advantages provided by these systems, they are often vulnerable to different attacks due to their templates' invertible nature. This study utilized an iterative perturbation stochastic approximation procedure to analyze the irreversible order-based encoding approach used in cancelable biometric template transformation. The strategy begins by exploiting the scores corresponding to the encoded words, then a mixed variable-based iterative perturbation is used to generate independent random elements and consequently the corresponding template from the scores. The strategy exploits the vulnerability of three different cancelable systems it has tested, demonstrating that other techniques of similar nature fall short of the security standard for attack mitigation that cancelable biometric systems require. After analyzing the reasons that exploit the system's vulnerability, we use a parameterized thresholding non-uniform quantization as a countermeasure to boost the system's robustness while maintaining a balanced performance-security trade-off. As a result, the system can evade attacks without significantly hindering its matching performance. Finally, generality and computational complexity evaluations on different modalities and strategies validate the attack's efficacy and realism, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523001939",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biometrics",
      "Computer science",
      "Computer security",
      "Data mining",
      "Exploit",
      "Generality",
      "Mathematical optimization",
      "Mathematics",
      "Psychology",
      "Psychotherapist",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Abdullahi",
        "given_name": "Sani M."
      },
      {
        "surname": "Sun",
        "given_name": "Shuifa"
      },
      {
        "surname": "Wang",
        "given_name": "Hongxia"
      },
      {
        "surname": "Wang",
        "given_name": "Beng"
      }
    ]
  },
  {
    "title": "RA-YOLOX: Re-parameterization align decoupled head and novel label assignment scheme based on YOLOX",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109579",
    "abstract": "YOLOX is a state-of-the-art one-stage object detection model for real-time applications that employs a decoupled head and advanced label assignment. Despite its impressive performance, YOLOX has limitations that prevent it from achieving optimal accuracy in real-time settings. To improve these limitations, we propose a new approach called re-parameterization align YOLOX (RA-YOLOX). Our approach employs a novel re-parameterization align decoupled head to align the classification and regression tasks, enhancing the learning of connection information between classification and regression. In addition, we propose a novel label assignment(LA) scheme that effectively defines positive and negative samples and precisely designs loss weight function. Our LA scheme enables the detector to focus on high-quality positive samples and filter out low-quality positive samples during training. We provide three sizes of lite models, namely RA-YOLOX-s, RA-YOLOX-tiny, and RA-YOLOX-nano, all of which outperform YOLOX models of similar size by an average precision of 2.3%, 1.5%, and 1.7%, respectively, on the MS COCO-2017 validation set, demonstrating the efficacy of our approach. Our code is available at github.com/hcmyhc/RA-YOLOX.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323002790",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Code (set theory)",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Epistemology",
      "Evolutionary biology",
      "Filter (signal processing)",
      "Focus (optics)",
      "Function (biology)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Programming language",
      "Quality (philosophy)",
      "Scheme (mathematics)",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Zuopeng"
      },
      {
        "surname": "He",
        "given_name": "Chen"
      },
      {
        "surname": "Zhao",
        "given_name": "Guangming"
      },
      {
        "surname": "Zhou",
        "given_name": "Jie"
      },
      {
        "surname": "Hao",
        "given_name": "Kai"
      }
    ]
  },
  {
    "title": "GSAL: Geometric structure adversarial learning for robust medical image segmentation",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109596",
    "abstract": "Automatic medical image segmentation plays a crucial role in clinical diagnosis and treatment. However, it is still a challenging task due to the complex interior characteristics (e.g., inconsistent intensity, low contrast, texture heterogeneity) and ambiguous external boundary structures. In this paper, we introduce a novel geometric structure learning mechanism (GSLM) to overcome the limitations of existing segmentation models that lack learning ”focus, path, and difficulty.” The geometric structure in this mechanism is jointly characterized by the skeleton-like structure extracted by the mask distance transform (MDT) and the boundary structure extracted by the mask distance inverse transform (MDIT). Among them, the skeleton-like and boundary pay attention to the trend of interior characteristics consistency and external structure continuity, respectively. With this idea, we design GSAL, a novel end-to-end geometric structure adversarial learning for robust medical image segmentation. GSAL has four components: a geometric structure generator, which yields the geometric structure to learn the most discriminative features that preserve interior characteristics consistency and external boundary structure continuity, skeleton-like and boundary structure discriminators, which enhance and correct the characterization of internal and external geometry to mutually promote the capture of global contextual dependencies, and a geometric structure fusion sub-network, which fuses the two complementary and refined skeleton-like and boundary structures to generate the high-quality segmentation results. The proposed approach has been successfully applied to three different challenging medical image segmentation tasks, including polyp segmentation, COVID-19 lung infection segmentation, and lung nodule segmentation. Extensive experimental results demonstrate that the proposed GSAL achieves favorably against most state-of-the-art methods under different evaluation metrics. The code is available at: https://github.com/DLWK/GSAL.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323002972",
    "keywords": [
      "Artificial intelligence",
      "Boundary (topology)",
      "Computer science",
      "Computer vision",
      "Discriminative model",
      "Image segmentation",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Scale-space segmentation",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Kun"
      },
      {
        "surname": "Zhang",
        "given_name": "Xiaohong"
      },
      {
        "surname": "Lu",
        "given_name": "Yuting"
      },
      {
        "surname": "Zhang",
        "given_name": "Wei"
      },
      {
        "surname": "Huang",
        "given_name": "Sheng"
      },
      {
        "surname": "Yang",
        "given_name": "Dan"
      }
    ]
  },
  {
    "title": "Federated learning with ℓ 1 regularization",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.05.030",
    "abstract": "Federated Learning (FL) is a widely adopted deep learning method that does not require the collection of raw training data and solves specific learning tasks by federating distributed devices. Due to the heterogeneous distribution of data across clients, the clients will drift toward the local optimal solutions during local training and result in different local models. The global model after aggregating these different local models may keep away from the global optimal solution. This phenomenon is known as client drift, which often hinders the performance of FL. Parameter regularization methods address this challenge of client drift by controlling the update direction of each client. They consider the global model as both the starting point and a reference for the induction bias in the penalty. However, the existing regularization approaches produce dense solutions so that all parameters need to be updated during local model training. At the same time, we note that some studies on deep learning have found that it is unnecessary to update all parameters at each round. Therefore, in this work, we design a novel FL training approach called Fed ℓ 1 which can alleviate the performance degradation of FL by updating only part of the parameters at each round. ℓ 1 regularization is utilized to control the update direction of each client and avoid unnecessary parameter updates at the same time. To our knowledge, our study is the first to introduce sparse regularization term to correct the local training of individual clients in FL. We design a stochastic subgradient descent algorithm to train the ℓ 1 -regularized nonsmooth model. The comparison experiments with state-of-the-art baselines verify the superiority of the proposed approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523001630",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Machine learning",
      "Regularization (linguistics)"
    ],
    "authors": [
      {
        "surname": "Shi",
        "given_name": "Yong"
      },
      {
        "surname": "Zhang",
        "given_name": "Yuanying"
      },
      {
        "surname": "Zhang",
        "given_name": "Peng"
      },
      {
        "surname": "Xiao",
        "given_name": "Yang"
      },
      {
        "surname": "Niu",
        "given_name": "Lingfeng"
      }
    ]
  },
  {
    "title": "Online and real-time mask-guided multi-person tracking and segmentation",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.06.001",
    "abstract": "Applications of multi-object tracking and segmentation (MOTS) include autonomous driving and video surveillance systems. In these applications, the tracking system must be able to track in real-time and online. Recent studies to improve the tracking performance of MOTS have been actively conducted; however, most studies fail to consider online and real-time performance. This paper proposes a MOTS system that operates in real time using a deep learning–based model with a light backbone, and can be tracked online. Additionally, state-of-the-art trackers use an object’s bounding box to extract re-identification (ReID) features, but include unnecessary background. Additionally, this causes confusion in accurately expressing the appearance features of an object, resulting in difficulty in matching. To solve this difficulty, instead of providing the features of the bounding box as the input of the ReID branch, we focus on expressing the object by providing the mask features of the object as an input. As a result of the mask-based ReID experiment, the association accuracy performance was higher than that of the existing bounding box–based ReID model, and among the KITTI MOTS benchmarks, it ranked second among models that can operate online. Our experiments show that background information causes ambiguous ReID matching in MOTS systems, and that object mask information is important for avoiding ambiguous matching.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523001733",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "BitTorrent tracker",
      "Botany",
      "Bounding overwatch",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Eye tracking",
      "Focus (optics)",
      "Identification (biology)",
      "Image (mathematics)",
      "Matching (statistics)",
      "Mathematics",
      "Minimum bounding box",
      "Object (grammar)",
      "Object detection",
      "Optics",
      "Pedagogy",
      "Physics",
      "Psychology",
      "Segmentation",
      "Statistics",
      "Tracking (education)",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Seong",
        "given_name": "Jin"
      }
    ]
  },
  {
    "title": "Special section: Best papers of the 14th Mexican conference on pattern recognition (MCPR) 2022",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.05.027",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523001629",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Encoder",
      "Epistemology",
      "Image (mathematics)",
      "Latent variable",
      "Machine learning",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Quality (philosophy)",
      "Section (typography)",
      "Space (punctuation)"
    ],
    "authors": [
      {
        "surname": "Vergara Villegas",
        "given_name": "Osslan Osiris"
      },
      {
        "surname": "Cruz Sánchez",
        "given_name": "Vianey Guadalupe"
      },
      {
        "surname": "Sossa Azuela",
        "given_name": "Juan Humberto"
      },
      {
        "surname": "Carrasco Ochoa",
        "given_name": "Jesús Ariel"
      },
      {
        "surname": "Martínez Trinidad",
        "given_name": "José Francisco"
      },
      {
        "surname": "Olvera López",
        "given_name": "José Arturo"
      }
    ]
  },
  {
    "title": "RBDL: Robust block-Structured dictionary learning for block sparse representation",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.06.009",
    "abstract": "Dictionary learning methods have been extensively used in different types of image and signal processing tasks. In a number of applications, the collected data/signal may have a multi-subspace structure and be perturbed with outliers. These motivate the use of robust and block-sparse signal representations. In this paper, a new algorithm for learning a block-structured dictionary in the presence of outliers is proposed. It is based on α − divergence and has the advantage of tolerating the presence of outliers. A block coordinate descent approach is adopted to obtain simple closed-form solutions for both the sparse coding and dictionary update stages. Finally, experimental results illustrating the superiority of the proposed method over some state-of-the-art dictionary learning methods, are provided.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523001885",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Computer science",
      "Coordinate descent",
      "Dictionary learning",
      "Geometry",
      "K-SVD",
      "Mathematics",
      "Neural coding",
      "Outlier",
      "Pattern recognition (psychology)",
      "Sparse approximation",
      "Subspace topology"
    ],
    "authors": [
      {
        "surname": "Seghouane",
        "given_name": "Abd-Krim"
      },
      {
        "surname": "Iqbal",
        "given_name": "Asif"
      },
      {
        "surname": "Miri Rekavandi",
        "given_name": "Aref"
      }
    ]
  },
  {
    "title": "FETNet: Feature erasing and transferring network for scene text removal",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109531",
    "abstract": "The scene text removal (STR) task aims to remove text regions and recover the background smoothly in images for private information protection. Most existing STR methods adopt encoder-decoder-based CNNs, with direct copies of the features in the skip connections. However, the encoded features contain both text texture and structure information. The insufficient utilization of text features hampers the performance of background reconstruction in text removal regions. To tackle these problems, we propose a novel Feature Erasing and Transferring (FET) mechanism to reconfigure the encoded features for STR in this paper. In FET, a Feature Erasing Module (FEM) is designed to erase text features. An attention module is responsible for generating the feature similarity guidance. The Feature Transferring Module (FTM) is introduced to transfer the corresponding features in different layers based on the attention guidance. With this mechanism, a one-stage, end-to-end trainable network called FETNet is constructed for scene text removal. In addition, to facilitate research on both scene text removal and segmentation tasks, we introduce a novel dataset, Flickr-ST, with multi-category annotations. A sufficient number of experiments and ablation studies are conducted on the public datasets and Flickr-ST. Our proposed method achieves state-of-the-art performance using most metrics, with remarkably higher quality scene text removal results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323002315",
    "keywords": [
      "Artificial intelligence",
      "Code (set theory)",
      "Computer science",
      "Economics",
      "Encoder",
      "Feature (linguistics)",
      "Feature extraction",
      "Image (mathematics)",
      "Information retrieval",
      "Linguistics",
      "Management",
      "Natural language processing",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Segmentation",
      "Set (abstract data type)",
      "Similarity (geometry)",
      "Source code",
      "Task (project management)",
      "Text detection"
    ],
    "authors": [
      {
        "surname": "Lyu",
        "given_name": "Guangtao"
      },
      {
        "surname": "Liu",
        "given_name": "Kun"
      },
      {
        "surname": "Zhu",
        "given_name": "Anna"
      },
      {
        "surname": "Uchida",
        "given_name": "Seiichi"
      },
      {
        "surname": "Iwana",
        "given_name": "Brian Kenji"
      }
    ]
  },
  {
    "title": "Regression by Re-Ranking",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109577",
    "abstract": "Several approaches based on regression have been developed in the past few years with the goal of improving prediction results, including the use of ranking strategies. Re-ranking has been exploited and successfully employed in several applications, improving rankings by encoding the manifold structure and redefining distances among elements from a dataset. Despite the promising results observed, re-ranking has not been evaluated in regressions tasks. This paper proposes a novel, generic, and customizable framework entitled Regression by Re-ranking (RbR), which explores the ability of re-ranking algorithms in determining relevant rankings of objects in prediction tasks. The framework relies on the integration of a base regressor, unsupervised re-ranking learning techniques, and predictions associated with nearest neighbours weighted according to their ranking positions. The RbR framework was evaluated under a rigorous experimental protocol and presented significant results in improving the prediction when compared to state-of-the-art approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323002777",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Learning to rank",
      "Machine learning",
      "Mathematics",
      "Ranking (information retrieval)",
      "Ranking SVM",
      "Regression",
      "Regression analysis",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Gonçalves",
        "given_name": "Filipe Marcel Fernandes"
      },
      {
        "surname": "Pedronette",
        "given_name": "Daniel Carlos Guimarães"
      },
      {
        "surname": "da Silva Torres",
        "given_name": "Ricardo"
      }
    ]
  },
  {
    "title": "Towards Automatic Model Compression via a Unified Two-Stage Framework",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109527",
    "abstract": "Deep Neural Networks have become ubiquitous in various domains. Meanwhile, the problems of massive storage and computation costs have hindered the deployment of these models to real-world applications. This paper proposes a novel and unified two-stage framework for automatic model compression. To determine the compression ratio of each layer, we improve the optimization from two aspects. First, to predict the performance of each compression policy, we propose Dynamic BN, which improves the correlation significantly with little computation overhead. Second, to search for the compression ratio allocation, we propose an efficient and hyperparameter-free solving algorithm based on the proposed Hessian matrix approximation and Knapsack problem reformulation. Moreover, comprehensive experiments and analyses are conducted on the CIFAR-100&ImageNet datasets and various network architectures to demonstrate its performance advantages over existing model compression methods under the quantization-only, pruning-only, and pruning-quantization settings.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323002273",
    "keywords": [
      "Agronomy",
      "Algorithm",
      "Applied mathematics",
      "Artificial intelligence",
      "Artificial neural network",
      "Automotive engineering",
      "Biology",
      "Compression ratio",
      "Computation",
      "Computer science",
      "Data compression",
      "Data compression ratio",
      "Engineering",
      "Hessian matrix",
      "Image (mathematics)",
      "Image compression",
      "Image processing",
      "Internal combustion engine",
      "Knapsack problem",
      "Lossless compression",
      "Machine learning",
      "Mathematics",
      "Operating system",
      "Overhead (engineering)",
      "Pruning",
      "Quantization (signal processing)"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Weihan"
      },
      {
        "surname": "Wang",
        "given_name": "Peisong"
      },
      {
        "surname": "Cheng",
        "given_name": "Jian"
      }
    ]
  },
  {
    "title": "Residual 3D convolutional neural network to enhance sinograms from small-animal positron emission tomography images",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.05.005",
    "abstract": "Positron emission tomography (PET) has been widely used in nuclear medicine to diagnose cancer. PET images suffer from degradation because of the scanner's physical limitations, the radiotracer's reduced dose, and the acquisition time. In this work, we propose a residual three-dimensional (3D) and convolutional neural network (CNN) to enhance sinograms acquired from a small-animal PET scanner. The network comprises three convolutional layers created with 3D filters of sizes 9, 5, and 5, respectively. For training, we extracted 15250 3D patches from low- and high-count sinograms to build the low- and high-resolution pairs. After training and prediction, the image was reconstructed from the enhanced sinogram using the ordered subset expectation maximization (OSEM) algorithm. The results revealed that the proposed network improves the spillover ratio by up to 4.5% and the uniformity by 55% compared to the U-Net. The NEMA phantom data were obtained in a simulation environment. The network was tested on acquired real data from a mouse. The reconstructed images and the profiles of maximum intensity projection show that the proposed method visually yields sharper images. 2023 Elsevier Ltd. All rights reserved.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523001320",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Image resolution",
      "Imaging phantom",
      "Iterative reconstruction",
      "Medicine",
      "Nuclear medicine",
      "Pattern recognition (psychology)",
      "Positron emission tomography",
      "Projection (relational algebra)",
      "Residual",
      "Scanner"
    ],
    "authors": [
      {
        "surname": "Hernández",
        "given_name": "Leandro José Rodríguez"
      },
      {
        "surname": "Domínguez",
        "given_name": "Humberto de Jesús Ochoa"
      },
      {
        "surname": "Villegas",
        "given_name": "Osslan Osiris Vergara"
      },
      {
        "surname": "Sánchez",
        "given_name": "Vianey Guadalupe Cruz"
      },
      {
        "surname": "Azuela",
        "given_name": "Juan Humberto Sossa"
      },
      {
        "surname": "González",
        "given_name": "Javier Polanco"
      }
    ]
  },
  {
    "title": "A uniform transformer-based structure for feature fusion and enhancement for RGB-D saliency detection",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109516",
    "abstract": "RGB-D saliency detection integrates information from both RGB images and depth maps to improve the prediction of salient regions under challenging conditions. The key to RGB-D saliency detection is to fully mine and fuse information at multiple scales across the two modalities. Previous approaches tend to apply the multi-scale and multi-modal fusion separately via local operations, which fails to capture long-range dependencies. Here we propose a transformer-based structure to address this issue. The proposed architecture is composed of two modules: an Intra-modality Feature Enhancement Module (IFEM) and an Inter-modality Feature Fusion Module (IFFM). IFFM conducts a sufficient feature fusion by integrating features from multiple scales and two modalities over all positions simultaneously. IFEM enhances feature on each scale by selecting and integrating complementary information from other scales within the same modality before IFFM. We show that transformer is a uniform operation which presents great efficacy in both feature fusion and feature enhancement, and simplifies the model design. Extensive experimental results on five benchmark datasets demonstrate that our proposed network performs favorably against most state-of-the-art RGB-D saliency detection methods. Furthermore, our model is efficient for having relatively smaller FLOPs and model size compared with other methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323002169",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Computer vision",
      "Electrical engineering",
      "Engineering",
      "Feature (linguistics)",
      "Fusion",
      "Geodesy",
      "Geography",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "RGB color model",
      "Salient",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Yue"
      },
      {
        "surname": "Jia",
        "given_name": "Xu"
      },
      {
        "surname": "Zhang",
        "given_name": "Lu"
      },
      {
        "surname": "Li",
        "given_name": "Yuke"
      },
      {
        "surname": "Elder",
        "given_name": "James H."
      },
      {
        "surname": "Lu",
        "given_name": "Huchuan"
      }
    ]
  },
  {
    "title": "Towards better long-tailed oracle character recognition with adversarial data augmentation",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109534",
    "abstract": "Deciphering oracle bone script is of great significance to the study of ancient Chinese culture as well as archaeology. Although recent studies on oracle character recognition have made substantial progress, they still suffer from the long-tailed data situation that results in a noticeable performance drop on the tail classes. To mitigate this issue, we propose a generative adversarial framework to augment oracle characters in the problematic classes. In this framework, the generator produces synthetic data through convex combinations of all the available samples in the corresponding classes, and is further optimized through adversarial learning with the classifier and simultaneously the discriminator. Meanwhile, we introduce Repatch to generalize samples in the generator. Since tail classes do not have sufficient data for convex combinations, we propose the TailMix mechanism to generate suitable tail class samples from other classes. Experimental results show that our proposed algorithm obtains remarkable performance in oracle character recognition and achieves new state-of-the-art average (total) accuracy with 86.03% (89.46%), 86.54% (93.86%), 95.22% (96.17%) on the three datasets Oracle-AYNU, OBC306 and Oracle-20K, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323002340",
    "keywords": [
      "Artificial intelligence",
      "Character (mathematics)",
      "Classifier (UML)",
      "Computer science",
      "Detector",
      "Discriminator",
      "Generative grammar",
      "Generator (circuit theory)",
      "Geometry",
      "Machine learning",
      "Mathematics",
      "Oracle",
      "Pattern recognition (psychology)",
      "Physics",
      "Power (physics)",
      "Quantum mechanics",
      "Software engineering",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Jing"
      },
      {
        "surname": "Wang",
        "given_name": "Qiu-Feng"
      },
      {
        "surname": "Huang",
        "given_name": "Kaizhu"
      },
      {
        "surname": "Yang",
        "given_name": "Xi"
      },
      {
        "surname": "Zhang",
        "given_name": "Rui"
      },
      {
        "surname": "Goulermas",
        "given_name": "John Y."
      }
    ]
  },
  {
    "title": "Evaluating synthetic pre-Training for handwriting processing tasks",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.06.003",
    "abstract": "In this work, we explore massive pre-training on synthetic word images for enhancing the performance on four benchmark downstream handwriting analysis tasks. To this end, we build a large synthetic dataset of word images rendered in several handwriting fonts, which offers a complete supervision signal. We use it to train a simple convolutional neural network (ConvNet) with a fully supervised objective. The vector representations of the images obtained from the pre-trained ConvNet can then be considered as encodings of the handwriting style. We exploit such representations for Writer Retrieval, Writer Identification, Writer Verification, and Writer Classification and demonstrate that our pre-training strategy allows extracting rich representations of the writers’ style that enable the aforementioned tasks with competitive results with respect to task-specific State-of-the-Art approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523001800",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Botany",
      "Computer science",
      "Computer security",
      "Convolutional neural network",
      "Economics",
      "Exploit",
      "Feature extraction",
      "Geodesy",
      "Geography",
      "Handwriting",
      "Handwriting recognition",
      "Identification (biology)",
      "Linguistics",
      "Management",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Speech recognition",
      "Task (project management)",
      "Word (group theory)"
    ],
    "authors": [
      {
        "surname": "Pippi",
        "given_name": "Vittorio"
      },
      {
        "surname": "Cascianelli",
        "given_name": "Silvia"
      },
      {
        "surname": "Baraldi",
        "given_name": "Lorenzo"
      },
      {
        "surname": "Cucchiara",
        "given_name": "Rita"
      }
    ]
  },
  {
    "title": "TMO-Det: Deep tone-mapping optimized with and for object detection",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.06.017",
    "abstract": "Detecting objects in challenging illumination conditions is critical for autonomous driving. Existing solutions detect objects with standard or tone-mapped Low Dynamic Range (LDR) images. In this paper, we propose a novel adversarial approach that jointly optimizes tone-mapping (mapping High Dynamic Range (HDR) to LDR) and object detection. We analyze different ways to combine the feedback from tone-mapping quality and object detection quality for training such an adversarial network. We show that our deep tone-mapping operator jointly trained with an object detector achieves the best tone-mapping quality as well as detection quality compared to alternative approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523001964",
    "keywords": [
      "Aerospace engineering",
      "Art",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Detector",
      "Dynamic range",
      "Engineering",
      "Epistemology",
      "High dynamic range",
      "Literature",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Quality (philosophy)",
      "Range (aeronautics)",
      "Telecommunications",
      "Tone (literature)",
      "Tone mapping"
    ],
    "authors": [
      {
        "surname": "Kocdemir",
        "given_name": "Ismail Hakki"
      },
      {
        "surname": "Koz",
        "given_name": "Alper"
      },
      {
        "surname": "Akyuz",
        "given_name": "Ahmet Oguz"
      },
      {
        "surname": "Chalmers",
        "given_name": "Alan"
      },
      {
        "surname": "Alatan",
        "given_name": "Aydin"
      },
      {
        "surname": "Kalkan",
        "given_name": "Sinan"
      }
    ]
  },
  {
    "title": "CANet: Contextual Information and Spatial Attention Based Network for Detecting Small Defects in Manufacturing Industry",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109558",
    "abstract": "Despite the promising development of Automatic Visual Inspection (AVI) in the manufacturing industry, detecting small-sized defects with fewer pixels coverage remains a challenging problem due to its insufficient attention and lack of semantic information. Most exsiting convolutional inspection methods overlook the long-range dependence of context and lack adaptive fusion strategies to exploit heterogeneous features. To address these issues in AVI, this paper proposes a novel contextual information and spatial attention based network (CANet), which consists of two steps, namely CAblock and LaplacianFPN, for effective perception and exploitation of small defect features. Specifically, CAblock extracts semantic information with rich context by encoding spatial long-range dependence and decoding contextual information as channel-specific bias through a Spatial Attention Encoder (SAE) and a Context Block Decoder (CBD), respectively. LaplacianFPN further performs adaptive feature fusion considering both feature consistency and heterogeneity via two parallel branches. As a benchmark, a self-built Engine Surface Defects (ESD) dataset collected in real industry containing 89.70% small defects is constructed. Experimental results show that CANet achieves mAP-50 improvements of 1.5% and 4.3% compared to state-of-the-art methods on NEU-DET and ESD, which demonstrates the effectiveness of the proposed method. The code is now available at https://github.com/xiuqhou/CANet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323002583",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Block (permutation group theory)",
      "Computer science",
      "Computer security",
      "Consistency (knowledge bases)",
      "Context (archaeology)",
      "Data mining",
      "Encoder",
      "Encoding (memory)",
      "Exploit",
      "Feature (linguistics)",
      "Geodesy",
      "Geography",
      "Geology",
      "Geometry",
      "Linguistics",
      "Mathematics",
      "Operating system",
      "Paleontology",
      "Philosophy",
      "Remote sensing",
      "Spatial analysis",
      "Spatial contextual awareness"
    ],
    "authors": [
      {
        "surname": "Hou",
        "given_name": "Xiuquan"
      },
      {
        "surname": "Liu",
        "given_name": "Meiqin"
      },
      {
        "surname": "Zhang",
        "given_name": "Senlin"
      },
      {
        "surname": "Wei",
        "given_name": "Ping"
      },
      {
        "surname": "Chen",
        "given_name": "Badong"
      }
    ]
  },
  {
    "title": "Multi-task semi-supervised crowd counting via global to local self-correction",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109506",
    "abstract": "In this paper, we propose a novel multi-task semi-supervised method. To sufficiently exploit massive unlabeled data, multi-task pseudo-labels and global to local self-correction strategy are proposed. Specifically, labeled images and massive amounts of unlabeled images with proposed multi-task pseudo-labels are leveraged for model optimization. The density level of the whole image is predicted in classification task. The density is estimated in density regression task. The crowd area is segmented out in segmentation task. To suppress incorrect predictions caused by the inevitable noises from some unlabeled data misleading the model, the counting relationship between classification task and density task is exploited to propose the global self-correction strategy, and the semantic consistency between density task and segmentation task is mined to propose the local self-correction strategy. The classification task and segmentation task contribute in generating the final highly refined density map from the density task. Extensive experiments on six benchmark datasets indicate the superiority of our method over the SOTA methods in semi-supervised paradigm.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323002066",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Consistency (knowledge bases)",
      "Economics",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Management",
      "Pattern recognition (psychology)",
      "Segmentation",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Jiwei"
      },
      {
        "surname": "Wang",
        "given_name": "Zengfu"
      }
    ]
  },
  {
    "title": "Hierarchical supervisions with two-stream network for Deepfake detection",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.05.029",
    "abstract": "Recently, the quality of face generation and manipulation has reached impressive levels, making it difficult even for humans to distinguish real and fake faces. At the same time, methods to distinguish fake faces from reals came out, such as Deepfake detection. However, the task of Deepfake detection remains challenging, especially the low-quality fake images circulating on the Internet and the diversity of face generation methods. In this work, we propose a new Deepfake detection network that could effectively distinguish both high-quality and low-quality faces generated by various generation methods. First, we design a two-stream framework that incorporates a regular spatial stream and a frequency stream to handle the low-quality problem since we find that the frequency domain artifacts of low-quality images will be preserved. Second, we introduce hierarchical supervisions in a coarse-to-fine manner, which consists of a coarse binary classification branch to classify reals and fakes and a five-category classification branch to classify reals and four different types of fakes. Extensive experiments have proved the effectiveness of our framework on several widely used datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523001678",
    "keywords": [
      "Arithmetic",
      "Artificial intelligence",
      "Binary number",
      "Computer science",
      "Data mining",
      "Domain (mathematical analysis)",
      "Economics",
      "Epistemology",
      "Face (sociological concept)",
      "Machine learning",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Quality (philosophy)",
      "Social science",
      "Sociology",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Liang",
        "given_name": "Yufei"
      },
      {
        "surname": "Wang",
        "given_name": "Mengmeng"
      },
      {
        "surname": "Jin",
        "given_name": "Yining"
      },
      {
        "surname": "Pan",
        "given_name": "Shuwen"
      },
      {
        "surname": "Liu",
        "given_name": "Yong"
      }
    ]
  },
  {
    "title": "Topic-aware video summarization using multimodal transformer",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109578",
    "abstract": "Video summarization aims to generate a short and compact summary to represent the original video. Existing methods mainly focus on how to extract a general objective synopsis that precisely summaries the video content. However, in real scenarios, a video usually contains rich content with multiple topics and people may cast diverse interests on the visual contents even for the same video. In this paper, we propose a novel topic-aware video summarization task that generates multiple video summaries with different topics. To support the study of this new task, we first build a video benchmark dataset by collecting videos from various types of movies and annotate them with topic labels and frame-level importance scores. Then we propose a multimodal Transformer model for the topic-aware video summarization, which simultaneously predicts topic labels and generates topic-related summaries by adaptively fusing multimodal features extracted from the video. Experimental results show the effectiveness of our method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323002789",
    "keywords": [
      "Artificial intelligence",
      "Automatic summarization",
      "Benchmark (surveying)",
      "Computer science",
      "Economics",
      "Focus (optics)",
      "Frame (networking)",
      "Geodesy",
      "Geography",
      "Information retrieval",
      "Management",
      "Multimedia",
      "Optics",
      "Physics",
      "Quantum mechanics",
      "Task (project management)",
      "Telecommunications",
      "Transformer",
      "Video processing",
      "Video retrieval",
      "Video tracking",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Zhu",
        "given_name": "Yubo"
      },
      {
        "surname": "Zhao",
        "given_name": "Wentian"
      },
      {
        "surname": "Hua",
        "given_name": "Rui"
      },
      {
        "surname": "Wu",
        "given_name": "Xinxiao"
      }
    ]
  },
  {
    "title": "MMA-Net: Multi-view mixed attention mechanism for facial action unit detection",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.06.004",
    "abstract": "Facial action units (AU) have strong mutual correlation. How to explore fine-grained AU regional features from different dimensions while adding inter-AU correlational information is the key to accurate AU detection. In this paper, we propose a novel AU detection framework called MMA-Net based on multi-view mixed attention, combining AU-regional information, co-occurrence correlational information and spatially correlational information by adopting a new AU partitioning scheme. Specifically, the proposed multi-view AU partitioning scheme first applies in both the AU co-occurrence correlational view and the facial ROI view to define the co-occurrence and spatially correlational information of AUs. Then, mixed attention, consisting of regional, channel-wise, and spatial attention, is incorporated into the encoder of MMA-Net to extract features from different dimensions. Finally, a pixel-level cross-view contrastive loss is proposed for feature enhancement by differing cross-view features for complement. Experimental results on two widely-used benchmark datasets, namely DISFA and BP4D, demonstrate the superior performance of MMA-Net against the state-of-the-art methods for AU detection.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523001721",
    "keywords": [
      "Action (physics)",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biochemistry",
      "Channel (broadcasting)",
      "Chemistry",
      "Complement (music)",
      "Complementation",
      "Computer network",
      "Computer science",
      "Computer security",
      "Correlation",
      "Encoder",
      "Face (sociological concept)",
      "Feature (linguistics)",
      "Gene",
      "Geodesy",
      "Geography",
      "Geometry",
      "Image (mathematics)",
      "Key (lock)",
      "Linguistics",
      "Mathematics",
      "Net (polyhedron)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Phenotype",
      "Philosophy",
      "Physics",
      "Pixel",
      "Quantum mechanics",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Shang",
        "given_name": "Ziqiao"
      },
      {
        "surname": "Du",
        "given_name": "Congju"
      },
      {
        "surname": "Li",
        "given_name": "Bingyin"
      },
      {
        "surname": "Yan",
        "given_name": "Zengqiang"
      },
      {
        "surname": "Yu",
        "given_name": "Li"
      }
    ]
  },
  {
    "title": "Task weighting based on particle filter in deep multi-task learning with a view to uncertainty and performance",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109587",
    "abstract": "Recently multi-task learning (MTL) has been widely used in different applications to build more robust models by sharing knowledge across several related tasks. However, one challenge that arises is the variability in the learning pace of different tasks causing the inefficiency of naively training all tasks. Therefore, it is of great importance to consider some coefficients to balance tasks in the process of learning, but, due to the large search space and the significance of setting them properly, conventional search methods such as grid or random search are no longer effective. In this paper, we propose a learning mechanism for these coefficients based on the high efficiency of the particle filter (PF) algorithm to deal with nonlinear search problems. PF considers each state of the tasks’ coefficients as a particle and recursively converges coefficients to an optimum point. While in most previous works coefficients were evaluated to only increase performance, to address the recent concerns related to applying AI in real-world applications, we also incorporate uncertainty alongside our method to prevent learning coefficients leading to unstable outcomes. This mechanism is independent of the models main learning process and can be easily added to every learning system without changing its training algorithm. Extensive experiments on real-world data sets demonstrate the superiority of the proposed method over the state-of-the-art methods on both performance and uncertainty. We also proved the acceptable performance of the method using Cramer Rao lower bound theory.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323002881",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Economics",
      "Filter (signal processing)",
      "Geodesy",
      "Geography",
      "Geometry",
      "Grid",
      "Hyperparameter optimization",
      "Kalman filter",
      "Machine learning",
      "Management",
      "Mathematics",
      "Medicine",
      "Operating system",
      "Pace",
      "Particle filter",
      "Process (computing)",
      "Radiology",
      "Support vector machine",
      "Task (project management)",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Aghajanzadeh",
        "given_name": "Emad"
      },
      {
        "surname": "Bahraini",
        "given_name": "Tahereh"
      },
      {
        "surname": "Mehrizi",
        "given_name": "Amir Hossein"
      },
      {
        "surname": "Yazdi",
        "given_name": "Hadi Sadoghi"
      }
    ]
  },
  {
    "title": "The IBEM dataset: A large printed scientific image dataset for indexing and searching mathematical expressions",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.05.033",
    "abstract": "Searching for information in printed scientific documents is a challenging problem that has recently received special attention from the Pattern Recognition research community. Mathematical expressions are complex elements that appear in scientific documents, and developing techniques for locating and recognizing them requires the preparation of datasets that can be used as benchmarks. Most current techniques for dealing with mathematical expressions are based on Machine Learning techniques which require a large amount of annotated data. These datasets must be prepared with ground-truth information for automatic training and testing. However, preparing large datasets with ground-truth is a very expensive and time-consuming task. This paper introduces the IBEM dataset, consisting of scientific documents that have been prepared for mathematical expression recognition and searching. This dataset consists of 600 documents, more than 8 200 page images with more than 160 000 mathematical expressions. It has been automatically generated from the Image 1 version of the documents and can be enlarged easily. The ground-truth includes the position at the page level and the Image 1 transcript for mathematical expressions both embedded in the text and displayed. This paper also reports a baseline classification experiment with mathematical symbols and a baseline experiment of Mathematical Expression Recognition performed on the IBEM dataset. These experiments aim to provide some benchmarks for comparison purposes so that future users of the IBEM dataset can have a baseline framework.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552300168X",
    "keywords": [
      "Artificial intelligence",
      "Baseline (sea)",
      "Computer science",
      "Data mining",
      "Economics",
      "Expression (computer science)",
      "Geology",
      "Ground truth",
      "Information retrieval",
      "Machine learning",
      "Management",
      "Natural language processing",
      "Oceanography",
      "Pattern recognition (psychology)",
      "Programming language",
      "Search engine indexing",
      "Task (project management)",
      "Visualization"
    ],
    "authors": [
      {
        "surname": "Anitei",
        "given_name": "Dan"
      },
      {
        "surname": "Sánchez",
        "given_name": "Joan Andreu"
      },
      {
        "surname": "Benedí",
        "given_name": "José Miguel"
      },
      {
        "surname": "Noya",
        "given_name": "Ernesto"
      }
    ]
  },
  {
    "title": "Prediction of evoked expression from videos with temporal position fusion",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.07.002",
    "abstract": "This paper introduces an approach for estimating evoked categories expression from videos with the temporal position fusion. Pre-trained models on large-scale datasets in computer vision and audio signals were used to extract the deep representation for timestamps in the video. A temporal convolution network, rather than an RNN-like architecture, was applied to explore temporal relationships due to its advantage in memory consumption and parallelism. Furthermore, to address the noise labels, the temporal position was fused with the deep learned feature to ensure the network differentiates the time steps when noise labels were removed from the training set. This technique helps the system gain a considerable improvement compared to other methods. We conducted experiments on EEV, a large-scale dataset for evoked expression from videos, and achieved a score of 0.054 in terms of Pearson correlation coefficient as a state-of-the-art result. Further experiments on a sub set of LIRIS-ACCEDE dataset - MediaEval 2018 benchmark, also demonstrated the effectiveness of our approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552300199X",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Convolution (computer science)",
      "Economics",
      "Encoding (memory)",
      "Expression (computer science)",
      "Finance",
      "Law",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Position (finance)",
      "Programming language",
      "Real-time computing",
      "Representation (politics)",
      "Speech recognition",
      "Timestamp"
    ],
    "authors": [
      {
        "surname": "Huynh",
        "given_name": "Van Thong"
      },
      {
        "surname": "Yang",
        "given_name": "Hyung-Jeong"
      },
      {
        "surname": "Lee",
        "given_name": "Guee-Sang"
      },
      {
        "surname": "Kim",
        "given_name": "Soo-Hyung"
      }
    ]
  },
  {
    "title": "Learn from each other to Classify better: Cross-layer mutual attention learning for fine-grained visual classification",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109550",
    "abstract": "Fine-grained visual classification (FGVC) is valuable yet challenging. The difficulty of FGVC mainly lies in its intrinsic inter-class similarity, intra-class variation, and limited training data. Moreover, with the popularity of deep convolutional neural networks, researchers have mainly used deep, abstract, semantic information for FGVC, while shallow, detailed information has been neglected. This work proposes a cross-layer mutual attention learning network (CMAL-Net) to solve the above problems. Specifically, this work views the shallow to deep layers of CNNs as “experts” knowledgeable about different perspectives. We let each expert give a category prediction and an attention region indicating the found clues. Attention regions are treated as information carriers among experts, bringing three benefits: ( i ) helping the model focus on discriminative regions; ( i i ) providing more training data; ( i i i ) allowing experts to learn from each other to improve the overall performance. CMAL-Net achieves state-of-the-art performance on three competitive datasets: FGVC-Aircraft, Stanford Cars, and Food-11. The source code is available at https://github.com/Dichao-Liu/CMAL",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323002509",
    "keywords": [
      "Artificial intelligence",
      "Astrophysics",
      "Chemistry",
      "Class (philosophy)",
      "Computer science",
      "Convolutional neural network",
      "Data science",
      "Deep learning",
      "Discriminative model",
      "Economics",
      "Focus (optics)",
      "Image (mathematics)",
      "Layer (electronics)",
      "Machine learning",
      "Metric (unit)",
      "Operations management",
      "Optics",
      "Organic chemistry",
      "Physics",
      "Popularity",
      "Psychology",
      "Similarity (geometry)",
      "Social psychology",
      "Variation (astronomy)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Dichao"
      },
      {
        "surname": "Zhao",
        "given_name": "Longjiao"
      },
      {
        "surname": "Wang",
        "given_name": "Yu"
      },
      {
        "surname": "Kato",
        "given_name": "Jien"
      }
    ]
  },
  {
    "title": "Improve Temporal Action Proposals using Hierarchical Context",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109560",
    "abstract": "Temporal action proposal (TAP) aims to generate accurate candidates of action instances in an untrimmed video. It has been proved that contexts are critically important to this task. In this paper, we propose a novel hierarchical context network (HCN) to further explore the snippet-level and proposal-level contexts, which are used to improve the representations of snippets and proposals, respectively. First, we pinpoint that different scales of snippet-level contexts are not equally important for different action instances. To this end, we incorporate a novel gating mechanism into the U-Net structure to capture the content-adaptive snippet-level contexts. Second, to exploit the proposal-level contexts, we propose a task-specific self-attention model with high efficiency. By stacking multiple attention models, we can deeply explore the proposal-level contexts in a wide range. Finally, to leverage both levels of context, we equip HCN with three branches to evaluate proposals from local to global perspectives. Our experiments on the ActivityNet-1.3 and THUMOS14 datasets show that HCN significantly outperforms previous TAP methods. Additionally, further experiments demonstrate that our method can substantially improve the state-of-the-art action detection performance when combined with existing action classifiers.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323002601",
    "keywords": [
      "Action (physics)",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer security",
      "Context (archaeology)",
      "Economics",
      "Exploit",
      "Information retrieval",
      "Leverage (statistics)",
      "Machine learning",
      "Management",
      "Paleontology",
      "Physics",
      "Quantum mechanics",
      "Snippet",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Qinying"
      },
      {
        "surname": "Wang",
        "given_name": "Zilei"
      },
      {
        "surname": "Rong",
        "given_name": "Shenghai"
      }
    ]
  },
  {
    "title": "Motif Entropy Graph Kernel",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109544",
    "abstract": "Graph kernels have achieved excellent performance in graph classification tasks. In this paper, we propose a novel deep motif entropy graph kernel for the purpose of graph classification. For better capturing the differences between substructures, we gauge detailed information through a family of K -layer expansion motifs rooted at each node and combine the Weisfeiler-Lehman algorithm to subdivide motifs, which is further enhanced by motif entropy. Experiments on eight graph-structured datasets demonstrate that our method is able to outperform the state-of-the-art kernel methods for the tasks of graph classification.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323002443",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Graph",
      "Graph kernel",
      "Kernel method",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Polynomial kernel",
      "Support vector machine",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Liang"
      },
      {
        "surname": "Yi",
        "given_name": "Longqiang"
      },
      {
        "surname": "Liu",
        "given_name": "Yu"
      },
      {
        "surname": "Wang",
        "given_name": "Cheng"
      },
      {
        "surname": "Zhou",
        "given_name": "Da"
      }
    ]
  },
  {
    "title": "Scatter matrix decomposition for jointly sparse learning",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109485",
    "abstract": "Orthogonal Linear Discriminant Analysis (OLDA) based on generalized Eigen-equation is widely used in the field of computer vision and pattern recognition. However, the performance of OLDA for feature extraction and classification needs to be improved as it lacks sparsity for better interpretation of the features. Moreover, computing the orthogonal sparse projections based on LDA is very difficult and is still unsolved. To solve these problems, in this paper, we propose a method called Jointly Sparse Orthogonal Linear Discriminant Analysis (JSOLDA). Different from the existing OLDA, JSOLDA is proposed from a novel viewpoint of scatter matrix decomposition. Theoretical analysis shows that OLDA can be derived by the constrained scatter matrix decomposition. In addition, by imposing L2,1-norm on the penalty term, the proposed JSOLDA can obtain the jointly sparse orthogonal projections to perform feature extraction. We also design an iterative algorithm to obtain the optimal solution. Systematic theoretical analysis between the OLDA and JSOLDA are uncovered. Both of convergence and computational complexity are also discussed. Experimental results on four data sets (i.e., COIL100, USPS, ICADAR2003 and CMU PIE) indicate that JSOLDA outperforms several well-known LDA-based and L2,1-norm based methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323001851",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Composite material",
      "Computational complexity theory",
      "Computer science",
      "Eigenvalues and eigenvectors",
      "Gaussian",
      "Linear discriminant analysis",
      "Materials science",
      "Matrix (chemical analysis)",
      "Matrix decomposition",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Sparse matrix"
    ],
    "authors": [
      {
        "surname": "Mo",
        "given_name": "Dongmei"
      },
      {
        "surname": "Lai",
        "given_name": "Zhihui"
      },
      {
        "surname": "Zhou",
        "given_name": "Jie"
      },
      {
        "surname": "Qinghua",
        "given_name": "Hu"
      }
    ]
  },
  {
    "title": "Attribute disentanglement with gradient reversal for interactive fashion retrieval",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.06.018",
    "abstract": "Interactive fashion search is gaining more and more interest thanks to the rapid diffusion of online retailers. It allows users to browse fashion items and perform attribute manipulations, modifying parts or details of given garments. To successfully model and analyze garments at such a fine-grained level, it is necessary to obtain attribute-wise representations, separating information relative to different characteristics. In this work we propose an attribute disentanglement method based on attribute classifiers and the usage of gradient reversal layers. This combination allows us to learn attribute-specific features, removing unwanted details from each representation. We test the effectiveness of our learned features in a fashion attribute manipulation task, obtaining state of the art results. Furthermore, to favor training stability we present a novel loss balancing approach, preventing reversed losses to diverge during the optimization process.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523001976",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Clothing",
      "Computer science",
      "Data mining",
      "Economics",
      "History",
      "Law",
      "Machine learning",
      "Management",
      "Operating system",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Process (computing)",
      "Representation (politics)",
      "Stability (learning theory)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Scaramuzzino",
        "given_name": "Giovanna"
      },
      {
        "surname": "Becattini",
        "given_name": "Federico"
      },
      {
        "surname": "Del Bimbo",
        "given_name": "Alberto"
      }
    ]
  },
  {
    "title": "HeadPose-Softmax: Head pose adaptive curriculum learning loss for deep face recognition",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109552",
    "abstract": "Face recognition has been one of the most popular applications in the field of target detection. Currently, frontal faces can be easily detected, but multi-view face detection remains a difficult task because of various factors such as illumination, various poses, occlusions, and facial expressions. Margin-based loss functions are used to increase the feature margins between different classes, thus enhancing the discriminability of face recognition models, but the performance in face detection in complex scenes (e.g., high pitch angle face detection in surveillance environments) can be significantly degraded. Recently, the idea of a mining-based strategy to emphasize hard samples has been used to achieve good results in multi-view face detection. However, most of the existing methods do not explicitly emphasize samples based on their importance, resulting in the underutilization of hard samples. In this paper, we propose a curriculum learning loss function (HeadPose-Softmax) to classify the difficulty of a sample based on its facial pose, and embed the concept of curriculum learning into the loss function to implement a novel training strategy for deep face recognition. The loss function explicitly emphasizes the importance of the samples according to the different difficulty of each sample, which allows the model to make fuller use of hard samples, focus on learning pose invariant features, and improve the accuracy of the model in multi-view face detection tasks. Specifically, our HeadPose-Softmax dynamically adjusts the relative importance of the hard samples according to the pose angle of the face in the hard samples during the training phase. At each stage, different samples are assigned different importance according to their corresponding difficulty. Extensive experimental results under popular benchmarks show that our HeadPose-Softmax can enhance the accuracy of the model in multi-view face detection and outperform the state-of-the-art competitors.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323002522",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Evolutionary biology",
      "Face (sociological concept)",
      "Face detection",
      "Facial recognition system",
      "Feature (linguistics)",
      "Function (biology)",
      "Linguistics",
      "Machine learning",
      "Margin (machine learning)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Social science",
      "Sociology",
      "Softmax function",
      "Three-dimensional face recognition"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Jifan"
      },
      {
        "surname": "Wang",
        "given_name": "Zhongyuan"
      },
      {
        "surname": "Huang",
        "given_name": "Baojin"
      },
      {
        "surname": "Xiao",
        "given_name": "Jinsheng"
      },
      {
        "surname": "Liang",
        "given_name": "Chao"
      },
      {
        "surname": "Han",
        "given_name": "Zhen"
      },
      {
        "surname": "Zou",
        "given_name": "Hua"
      }
    ]
  },
  {
    "title": "Self-information of radicals: A new clue for zero-shot Chinese character recognition",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109598",
    "abstract": "Zero-shot Chinese character recognition (ZSCCR) is an important research topic in Chinese character recognition as it attempts to recognize unseen Chinese characters. As basic components and mid-level representations, radicals are significant for ZSCCR. However, previous methods treat the importance of radicals equally, ignoring the different contributions of radicals in distinguishing characters. In this paper, we propose the self-information of radicals (SIR) to measure the importance of radicals in recognizing Chinese characters. The proposed SIR can be easily adopted by two commonly used radical-based ZSCCR frameworks, i.e., sequence matching based and attribute embedding based. For sequence matching based ZSCCR, we propose a novel Chinese character uncertainty elimination (CUE) framework to alleviate the radical sequence mismatch problem. For attribute embedding based ZSCCR, we propose a novel radical information embedding (RIE) method that can highlight the importance of indispensable radicals and weaken the influence of some unnecessary radicals. We conducted comprehensive experiments on the CASIA-HWDB, ICDAR2013, CTW datasets, and AHCDB datasets to evaluate the proposed method. Experiments show that our proposed methods can achieve superior performance to the state-of-the-art methods, which demonstrate the effectiveness and the high extensibility of the proposed SIR.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323002996",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Character (mathematics)",
      "Chemistry",
      "Chinese characters",
      "Computer science",
      "Data mining",
      "Embedding",
      "Geometry",
      "Linguistics",
      "Matching (statistics)",
      "Mathematics",
      "Measure (data warehouse)",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Radical",
      "Sequence (biology)",
      "Statistics",
      "Zero (linguistics)"
    ],
    "authors": [
      {
        "surname": "Luo",
        "given_name": "Guo-Feng"
      },
      {
        "surname": "Wang",
        "given_name": "Da-Han"
      },
      {
        "surname": "Du",
        "given_name": "Xia"
      },
      {
        "surname": "Yin",
        "given_name": "Hua-Yi"
      },
      {
        "surname": "Zhang",
        "given_name": "Xu-Yao"
      },
      {
        "surname": "Zhu",
        "given_name": "Shunzhi"
      }
    ]
  },
  {
    "title": "Disentangling high-level factors and their features with conditional vector quantized VAEs",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.05.028",
    "abstract": "Two recent works have shown the benefit of modeling both high-level factors and their related features to learn disentangled representations with variational autoencoders (VAE). We propose here a novel VAE-based approach that follows this principle. Inspired by conditional VAE, the features are no longer treated as random variables over which integration must be performed. Instead, they are deterministically computed from the input data using a neural network whose parameters can be estimated jointly with those of the decoder and of the encoder. Moreover, the quality of the generated images has been improved by using discrete latent variables and a two-step learning procedure, which makes it possible to increase the size of the latent space without altering the disentanglement properties of the model. Results obtained on two different datasets validate the proposed approach that achieves better performance than the two aforementioned works in terms of disentanglement, while providing higher quality images.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523001642",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Encoder",
      "Epistemology",
      "Geometry",
      "Latent variable",
      "Machine learning",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Quality (philosophy)",
      "Space (punctuation)",
      "Vector space"
    ],
    "authors": [
      {
        "surname": "Zou",
        "given_name": "Kaifeng"
      },
      {
        "surname": "Faisan",
        "given_name": "Sylvain"
      },
      {
        "surname": "Heitz",
        "given_name": "Fabrice"
      },
      {
        "surname": "Valette",
        "given_name": "Sébastien"
      }
    ]
  },
  {
    "title": "Keep an eye on faces: Robust face detection with heatmap-Assisted spatial attention and scale-Aware layer attention",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109553",
    "abstract": "Modern anchor-based face detectors learn discriminative features using large-capacity networks and extensive anchor settings. In spite of their promising results, they are not without problems. First, most anchors extract redundant features from the background. As a consequence, the performance improvements are achieved at the expense of a disproportionate computational complexity. Second, the predicted face boxes are only distinguished by a classifier supervised by pre-defined positive, negative and ignored anchors. This strategy may ignore potential contributions from cohorts of anchors labeled negative/ignored during inference simply because of their inferior initialisation, although they can regress well to a target. In other words, true positives and representative features may get filtered out by unreliable confidence scores. To deal with the first concern and achieve more efficient face detection, we propose a Heatmap-assisted Spatial Attention (HSA) module and a Scale-aware Layer Attention (SLA) module to extract informative features using lower computational costs. To be specific, SLA incorporates the information from all the feature pyramid layers, weighted adaptively to remove redundant layers. HSA predicts a reshaped Gaussian heatmap and employs it to facilitate a spatial feature selection by better highlighting facial areas. For more reliable decision-making, we merge the predicted heatmap scores and classification results by voting. Since our heatmap scores are based on the distance to the face centres, they are able to retain all the well-regressed anchors. The experiments obtained on several well-known benchmarks demonstrate the merits of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323002534",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Discriminative model",
      "Facial recognition system",
      "False positive paradox",
      "Feature (linguistics)",
      "Feature selection",
      "Inference",
      "Law",
      "Linguistics",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Pooling",
      "Voting"
    ],
    "authors": [
      {
        "surname": "Ju",
        "given_name": "Lei"
      },
      {
        "surname": "Kittler",
        "given_name": "Josef"
      },
      {
        "surname": "Rana",
        "given_name": "Muhammad Awais"
      },
      {
        "surname": "Yang",
        "given_name": "Wankou"
      },
      {
        "surname": "Feng",
        "given_name": "Zhenhua"
      }
    ]
  },
  {
    "title": "Automatic classification of company’s document stream: Comparison of two solutions",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.06.012",
    "abstract": "Documents are essential nowadays and present everywhere. In order to manage the vast amount of documents managed by companies, a first step consists in automatically determining the type of the document (its class). Even if automatic classification has been widely studied in the state of the art, the strongly imbalanced context and industrial constraints bring new challenges which were not studied till now: how to classify as many documents as possible with the highest precision, in an imbalanced context and with some classes missing during training? To this end, this paper proposes to study two different solutions to address these issues. The first is a multimodal neural network reinforced by an attention model and an adapted loss function that is able to classify a great variety of documents. The second is a combination method that uses a cascade of systems to offer a gradual solution for each issue. These two options provide good results as well in ideal context than in imbalanced context. This comparison outlines the limitations and the future challenges.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523001915",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Class (philosophy)",
      "Computer science",
      "Context (archaeology)",
      "Data mining",
      "Data science",
      "Document classification",
      "Economics",
      "Epistemology",
      "Evolutionary biology",
      "Finance",
      "Function (biology)",
      "Ideal (ethics)",
      "Information retrieval",
      "Machine learning",
      "Order (exchange)",
      "Paleontology",
      "Philosophy",
      "Variety (cybernetics)"
    ],
    "authors": [
      {
        "surname": "Voerman",
        "given_name": "Joris"
      },
      {
        "surname": "Souleiman Mahamoud",
        "given_name": "Ibrahim"
      },
      {
        "surname": "Coustaty",
        "given_name": "Mickael"
      },
      {
        "surname": "Joseph",
        "given_name": "Aurélie"
      },
      {
        "surname": "Poulain d’Andecy",
        "given_name": "Vincent"
      },
      {
        "surname": "Ogier",
        "given_name": "Jean-Marc"
      }
    ]
  },
  {
    "title": "Underestimation modification for intrinsic dimension estimation",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109580",
    "abstract": "The intrinsic dimension is the dimension of the low-dimensional manifold where the high-dimensional data is located. Accurately estimating the intrinsic dimension of the data set is helpful for data-dimensionality reduction and preprocessing. Due to the unknown spatial distribution of data and the limited sample size of a dataset, estimation methods which only use distance information tend to underestimate the intrinsic dimension of dataset. To reduce the estimation complexity and improve the accuracy, two estimation algorithms based on ID( κ ) are proposed, where κ is the scaling ratio of the neighborhood radius of the sample point. First, according to the selection criteria of parameter κ , an improved algorithm for selecting the optimal scaling ratio κ is proposed, which reduces the computational complexity and improves the stability of estimation. Second, using simulation datasets with the same sample size and known intrinsic dimensions, the relationship between the estimated dimension and the true intrinsic dimension is obtained, and an underestimation modification method for intrinsic dimension estimation is proposed. Results of comparative experiments on simulation and real datasets indicate that the underestimation modification algorithm has high estimation accuracy and robustness.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323002807",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Curse of dimensionality",
      "Dimension (graph theory)",
      "Dimensionality reduction",
      "Gene",
      "Geometry",
      "Intrinsic dimension",
      "Machine learning",
      "Mathematics",
      "Preprocessor",
      "Pure mathematics",
      "Robustness (evolution)",
      "Sample size determination",
      "Scaling",
      "Stability (learning theory)",
      "Statistics",
      "Sufficient dimension reduction"
    ],
    "authors": [
      {
        "surname": "Qiu",
        "given_name": "Haiquan"
      },
      {
        "surname": "Yang",
        "given_name": "Youlong"
      },
      {
        "surname": "Pan",
        "given_name": "Hua"
      }
    ]
  },
  {
    "title": "Deterministic sampling in heterogeneous graph neural networks",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.05.022",
    "abstract": "Graphs are typically used to model datasets where any given data point is correlated with only a small number of other data points in the set, i.e. localized correlations. In some datasets, the data points can be of different types, and this requires the use of heterogeneous graphs. Learning methods underpinned by graphs are used for analysis tasks such as node classification and link prediction. To exploit localized correlations in the learning process, sampling the neigbourhood of a candidate root node is typically required. The data from the sampled set of nodes can then be embedded and aggregated for use in an end-to-end neural network architecture. Previous approaches to sampling are stochastic in nature, e.g. random walk with restart. In this work, we propose a new approach to sampling that is deterministic in nature. The deterministic approach is based on the notion of node importance in relation to a root node. The factors that contribute to the importance are: (i) distance (number of edges) from root node; and (ii) centrality measure of the node. In this study, we adopt the Katz measure as the centrality measure. By devising an efficient sampling method together with node embedding and aggregation methods, we propose a Deterministic Heterogeneous Graph Neural Network (D-HetGNN). The application of D-HetGNN to three datasets is presented, and an extensive experimental evaluation demonstrates the superiority of the proposed sampling.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523001496",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Centrality",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Embedding",
      "Engineering",
      "Filter (signal processing)",
      "Graph",
      "Mathematics",
      "Measure (data warehouse)",
      "Node (physics)",
      "Programming language",
      "Sampling (signal processing)",
      "Set (abstract data type)",
      "Structural engineering",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Ansarizadeh",
        "given_name": "Fatemeh"
      },
      {
        "surname": "Tay",
        "given_name": "David B."
      },
      {
        "surname": "Thiruvady",
        "given_name": "Dhananjay"
      },
      {
        "surname": "Robles-kelly",
        "given_name": "Antonio"
      }
    ]
  },
  {
    "title": "Factorized multi-Graph matching",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109597",
    "abstract": "In recent years, multi-graph matching has become a popular yet challenging task in graph theory. There exist two major problems in multi-graph matching, i.e., the cycle-consistency problem, and the high time and space complexity problem. On one hand, the pairwise-based multi-graph matching methods are of low time and space complexity, but in order to keep the cycle-consistency of the matching results, they need additional constraints. Besides, the accuracy of the pairwise-based multi-graph matching is highly dependent on the selected optimization algorithms. On the other hand, the tensor-based multi-graph matching methods can avoid the cycle-consistency problem, while their time and space complexity is extremely high. In this paper, we found the equivalence between the pairwise-based and the tensor-based multi-graph matching methods under some specific circumstances. Based on this finding, we proposed a new multi-graph matching method, which not only avoids the cycle-consistency problem, but also reduces the complexity. In addition, we further improved the proposed method by introducing a lossless factorization of the affinity matrix in the multi-graph matching methods. Synthetic and real data experiments demonstrate the superiority of our method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323002984",
    "keywords": [
      "3-dimensional matching",
      "Algorithm",
      "Artificial intelligence",
      "Bipartite graph",
      "Computer science",
      "Factor-critical graph",
      "Graph",
      "Graph factorization",
      "Line graph",
      "Matching (statistics)",
      "Mathematics",
      "Pairwise comparison",
      "Statistics",
      "Theoretical computer science",
      "Voltage graph"
    ],
    "authors": [
      {
        "surname": "Zhu",
        "given_name": "Liangliang"
      },
      {
        "surname": "Zhu",
        "given_name": "Xinwen"
      },
      {
        "surname": "Geng",
        "given_name": "Xiurui"
      }
    ]
  },
  {
    "title": "GFMRC: A machine reading comprehension model for named entity recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.06.011",
    "abstract": "Recent advances in natural language representation have enabled the internal state of an upstream trained model to migrate to downstream tasks such as named entity recognition (NER). To better utilize pretrained models to perform NER tasks, the latest approach implements NER using the machine reading comprehension (MRC) framework. However, existing MRC approaches do not consider the limited performance of reading comprehension models due to the absence of contextual information in a single sample. Moreover, only word-level features are employed in the feature extraction phase in existing approaches. In this paper, a novel MRC model named GFMRC is proposed to realize NER. GRMRC enhances the MRC model with contextual information and hybrid features. In the preprocessing stage, the samples of the initial MRC dataset are spliced with N-gram information. In the feature extraction stage, global features are extracted for each token using a CNN, and local features are extracted using LSTM. Experiments are carried out on both Chinese datasets and English datasets, and the results demonstrated the effectiveness of the proposed model. The improvements on the English CoNLL 2003, English OntoNotes 5.0, Chinese MSRA, and Chinese OntoNotes 4.0 datasets are 0.07%, 0.23%, 0.04%, and 0.26%, respectively, compared to BERT-MRC+DSC.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523001903",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Economics",
      "Feature (linguistics)",
      "Linguistics",
      "Management",
      "Named-entity recognition",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Preprocessor",
      "Security token",
      "Task (project management)",
      "Word (group theory)"
    ],
    "authors": [
      {
        "surname": "Fei",
        "given_name": "Yuefan"
      },
      {
        "surname": "Xu",
        "given_name": "Xiaolong"
      }
    ]
  },
  {
    "title": "Reciprocal normalization for domain adaptation",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109533",
    "abstract": "Batch normalization (BN) is widely used in modern deep neural networks, which has been shown to represent the domain-related knowledge, and thus is ineffective for cross-domain tasks like unsupervised domain adaptation (UDA). Existing BN variant methods aggregate source and target domain knowledge in the same channel in normalization module. However, the misalignment between the features of corresponding channels across domains often leads to a sub-optimal transferability. In this paper, we exploit the cross-domain relation and propose a novel normalization method, Reciprocal Normalization (RN). Specifically, RN first presents a Reciprocal Compensation (RC) module to acquire the compensatory for each channel in both domains based on the cross-domain channel-wise correlation. Then RN develops a Reciprocal Aggregation (RA) module to adaptively aggregate the feature with its cross-domain compensatory components. As an alternative to BN, RN is more suitable for UDA problems and can be easily integrated into popular domain adaptation methods. Experiments show that the proposed RN outperforms existing normalization counterparts by a large margin and helps state-of-the-art adaptation approaches achieve better results. The source code is available on https://github.com/Openning07/reciprocal-normalization-for-DA.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323002339",
    "keywords": [
      "Algorithm",
      "Anthropology",
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Computer security",
      "Domain adaptation",
      "Exploit",
      "Linguistics",
      "Normalization (sociology)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Reciprocal",
      "Sociology",
      "Source code"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Zhiyong"
      },
      {
        "surname": "Sheng",
        "given_name": "Kekai"
      },
      {
        "surname": "Li",
        "given_name": "Ke"
      },
      {
        "surname": "Liang",
        "given_name": "Jian"
      },
      {
        "surname": "Yao",
        "given_name": "Taiping"
      },
      {
        "surname": "Dong",
        "given_name": "Weiming"
      },
      {
        "surname": "Zhou",
        "given_name": "Dengwen"
      },
      {
        "surname": "Sun",
        "given_name": "Xing"
      }
    ]
  },
  {
    "title": "Boundary-constrained robust regularization for single image dehazing",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109522",
    "abstract": "Generally, for single image dehazing, regularization-based schemes improve the initial transmission map iteratively by using a guidance map as a structural prior. We conducted experiments on a large number of hazy images and observed that a constrained transmission map affects the quality of the recovered image. However, regularization-based methods do not constrain the transmission map to its physically valid range during the iterative process. It degrades its robustness to outliers, and consequently, deteriorates the quality of the recovered image. In addition, conventional methods fuse the structural information of the guidance and initial transmission map without considering any structural differences between them. To address these issues, in this paper, we present a robust regularization scheme that constraints the transmission map during its enhancement. In the proposed scheme, a nonconvex energy function is constructed that leverages the mutual structural information of the guidance and transmission map. The nonconvex problem is solved by a majorize-minimization algorithm, and the intermediate transmission maps are constrained through the appropriate lower and upper bounds. The retrieved transmission map has better edge-preserving properties, and ultimately, results in a high-quality haze-free image that has faithful colors and fine details. The proposed scheme is tested on benchmark datasets and results are evaluated through quantitative metrics. The comparative analysis has revealed the effectiveness of the proposed scheme.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323002224",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Gene",
      "Geodesy",
      "Geography",
      "Mathematical optimization",
      "Mathematics",
      "Outlier",
      "Regularization (linguistics)",
      "Robustness (evolution)",
      "Telecommunications",
      "Transmission (telecommunications)"
    ],
    "authors": [
      {
        "surname": "Ali",
        "given_name": "Usman"
      },
      {
        "surname": "Choi",
        "given_name": "Jeongdan"
      },
      {
        "surname": "Min",
        "given_name": "KyoungWook"
      },
      {
        "surname": "Choi",
        "given_name": "Young-Kyu"
      },
      {
        "surname": "Mahmood",
        "given_name": "Muhammad Tariq"
      }
    ]
  },
  {
    "title": "AA-trans: Core attention aggregating transformer with information entropy selector for fine-grained visual classification",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109547",
    "abstract": "The task of fine-grained visual classification (FGVC) is to distinguish targets from subordinate classifications. Since fine-grained images have the inherent characteristic of large inter-class variances and small intra-class variances, it is considered an extremely difficult task. Most existing approaches adopt CNN-based networks as feature extractors, which causes the extracted discriminative regions to contain most parts of the object in this way, thus failing to locate the really important parts. Recently, the vision transformer (ViT) has demonstrated its power on a wide range of image tasks, which uses an attention mechanism to capture global contextual information to establish a remote dependency on the target and thus extract more powerful features. Nevertheless, the ViT model still focuses more on global coarse-grained information rather than local fine-grained information, which may lead to its undesirable performance in fine-grained image classification. To this end, we redesigned an attention aggregating transformer (AA-Trans) to better capture minor differences among images by improving the ViT structure in this paper. In detail, we propose a core attention aggregator (CAA), which enables better information sharing between each transformer layer. Besides, we further propose an innovative information entropy selector (IES) to guide the network in acquiring discriminative parts of the image precisely. Extensive experiments show that our proposed model structure can achieve a new state-of-the-art performance on several mainstream datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323002479",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Discriminative model",
      "Entropy (arrow of time)",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Qi"
      },
      {
        "surname": "Wang",
        "given_name": "JianJun"
      },
      {
        "surname": "Deng",
        "given_name": "Hongyu"
      },
      {
        "surname": "Wu",
        "given_name": "Xue"
      },
      {
        "surname": "Wang",
        "given_name": "Yazhou"
      },
      {
        "surname": "Hao",
        "given_name": "Gefei"
      }
    ]
  },
  {
    "title": "Semantics-enhanced early action detection using dynamic dilated convolution",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109595",
    "abstract": "This paper proposes a new pipeline to perform early action detection from skeleton-based untrimmed videos. Our pipeline includes two new technical components. The first is a new Dynamic Dilated Convolutional Network (DDCN), which supports dynamic temporal sampling and makes feature learning more robust against temporal scale variance in action sequences. The second is a new semantic referencing module, which uses identified objects in the scene and their co-existence relationship with actions to adjust the probabilities of inferred actions. Such semantic guidance can help distinguish many ambiguous actions, which is a core challenge in the early detection of incomplete actions. Our pipeline achieves state-of-the-art performance in early action detection in two widely used skeleton-based untrimmed video benchmarks. The source codes are available at: https://github.com/Powercoder64/DDCN_SRM.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323002960",
    "keywords": [
      "Accounting",
      "Action (physics)",
      "Artificial intelligence",
      "Artificial neural network",
      "Business",
      "Computer science",
      "Computer vision",
      "Convolution (computer science)",
      "Feature (linguistics)",
      "Filter (signal processing)",
      "Linguistics",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Pipeline (software)",
      "Programming language",
      "Quantum mechanics",
      "Sampling (signal processing)",
      "Scale (ratio)",
      "Semantics (computer science)",
      "Variance (accounting)"
    ],
    "authors": [
      {
        "surname": "Korban",
        "given_name": "Matthew"
      },
      {
        "surname": "Li",
        "given_name": "Xin"
      }
    ]
  },
  {
    "title": "Bi-Attention enhanced representation learning for image-text matching",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109548",
    "abstract": "Image-text matching has become a research hotspot in recent years. The key point of image-text matching is to accurately measure the similarity between an image and a sentence. However, most existing methods either focus on the inter-modality similarities between regions in image and words in text or the intra-modality similarities within image regions or words, such that they cannot well exploit detailed correlations between images and texts. Furthermore, existing methods typically train their models using a triplet ranking loss, which relies on the similarity of randomly sampled triples. Since the weights of positive and negative samples are not adjusted, it cannot provide enough gradient information for training, resulting in slow convergence and limited performance. To address the above problems, we propose an image-text matching method named Bi-Attention Enhanced Representation Learning (BAERL). It builds a self-attention learning sub-network to exploit intra-modality correlations within image regions or words and a co-attention learning sub-network to exploit inter-modality correlations between image regions and words. Then, representations obtained by two sub-networks capture holistic correlations between images and texts. In addition, BAERL uses the self-similarity polynomial loss instead of triplet ranking loss to train the model. The self-similarity polynomial loss can adaptively assign appropriate weights to different pairs based on their similarity scores so as to further improve the retrieval performance. Experiments on two benchmark datasets demonstrate the superior performance of the proposed BAERL method over several state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323002480",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Exploit",
      "Image (mathematics)",
      "Image retrieval",
      "Law",
      "Machine learning",
      "Matching (statistics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Ranking (information retrieval)",
      "Representation (politics)",
      "Sentence",
      "Similarity (geometry)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Tian",
        "given_name": "Yumin"
      },
      {
        "surname": "Ding",
        "given_name": "Aqiang"
      },
      {
        "surname": "Wang",
        "given_name": "Di"
      },
      {
        "surname": "Luo",
        "given_name": "Xuemei"
      },
      {
        "surname": "Wan",
        "given_name": "Bo"
      },
      {
        "surname": "Wang",
        "given_name": "Yifeng"
      }
    ]
  },
  {
    "title": "A real-time semantic segmentation model using iteratively shared features in multiple sub-encoders",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109557",
    "abstract": "Recent studies show a significant growth in semantic segmentation. However, many semantic segmentation models still have a large number of parameters, making them unsuitable for resource-constrained embedded devices. To address this issue, we propose an efficient Shared Feature Reuse Segmentation (SFRSeg) model containing several novelties: a new yet effective shared-branch multiple sub-encoders design, a context mining module and a semantic aggregating module for better context granularity. In particular, our shared-branch approach improves the entire feature hierarchy by sharing the spatial and context knowledge in both shallow and deep branches. After every shared point in each sub-encoder, a proposed cascading context mining (CCM) module is deployed to filter out the noisy spatial details from the feature maps and provides a diverse size of receptive fields for capturing the latent context between multi-scale geometric shapes in the scene. To overcome the gradient vanishing issue at the early stage, we reduce the number of layers in the first sub-encoder and employ a unique multiple sub-encoders design which reprocesses the rich global feature maps through multiple sub-encoders for better feature refinement. Later, the rich semantic features generated by the efficient sub-encoders at different levels are fused by the proposed Hybrid Path Attention Semantic Aggregation (HPA-SA) module that effectively reduces the semantic gap between feature maps at different levels and alleviate the well-known boundary degeneration effect. To make it computationally efficient for resource-constrained embedded devices, a series of lightweight methods such as a lightweight encoder, a squeeze-and-excitation design, separable convolution filters, channel reduction (CR) are carefully exploited. With an exceptional performance on Cityscapes (70.6% test mIoU) and CamVid (74.7% test mIoU) data sets, the proposed model is shown to be superior over existing light real-time semantic segmentation models whilst having only 1.6 million parameters.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323002571",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Encoder",
      "Feature (linguistics)",
      "Linguistics",
      "Operating system",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Segmentation",
      "Semantic feature"
    ],
    "authors": [
      {
        "surname": "Singha",
        "given_name": "Tanmay"
      },
      {
        "surname": "Pham",
        "given_name": "Duc-Son"
      },
      {
        "surname": "Krishna",
        "given_name": "Aneesh"
      }
    ]
  },
  {
    "title": "SOLARNet: A single stage regression based framework for efficient and robust object recognition in aerial images",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.05.031",
    "abstract": "Object recognition and localization play a crucial role in aerial images and their applications. The aerial images are challenging due to the large aspect ratio, arbitrary orientation, variation in scales, and non-uniform and cluttered object distribution. To address these challenges, we propose an efficient and robust model called the Simultaneous Object Localization and Recognition Network (SOLARNet), which is a fusion network that integrates two different sub-networks: PixelAttentionDetector (PD) and RotationDetector (RD). The PD considers features from different scales and cluttered objects, while RD handles rotation invariance, giving horizontal and oriented object detection results. The state-of-the-art model fails to improve accuracy when images are adversarially attacked. SOLARNet is not only efficient in terms of accuracy but also robust concerning to Fast Gradient Sign Method (FGSM), Basic Iterative Method (BIM), Jacobian Based Saliency Map (JSM) adversarial attacks, which is the crucial factor for any mission-critical system. We have performed experiments and achieved the accuracy on the publicly available DOTA dataset (75.00% mAP, 66.40% mAP) and DIOR dataset (88.60% mAP, 81.50% mAP) for horizontal and oriented object recognition tasks respectively while having high inference speed. Qualitative and quantitative results reported in this paper substantiate the superiority of SOLARNet over other state-of-the-art methodologies.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523001666",
    "keywords": [
      "Aerial image",
      "Applied mathematics",
      "Artificial intelligence",
      "Cognitive neuroscience of visual object recognition",
      "Computer science",
      "Computer vision",
      "Geometry",
      "Image (mathematics)",
      "Jacobian matrix and determinant",
      "Mathematics",
      "Object (grammar)",
      "Orientation (vector space)",
      "Pattern recognition (psychology)",
      "Rotation (mathematics)"
    ],
    "authors": [
      {
        "surname": "Saini",
        "given_name": "Nandini"
      },
      {
        "surname": "Chattopadhyay",
        "given_name": "Chiranjoy"
      },
      {
        "surname": "Das",
        "given_name": "Debasis"
      }
    ]
  },
  {
    "title": "Information extraction in handwritten historical logbooks",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.06.008",
    "abstract": "Document Image Understanding is a demanding Pattern Recognition problem that requires complex recognition models. This problem is even more difficult for document images with complicated layouts like tables, where the reading order is often intrinsically ambiguous, and consequently, the context is generally ambiguous as well. In this paper, we compare two machine learning approaches for extracting information in pre-printed historical tables with handwritten information. We analyze the performance of each approach at each step of the extraction process over different corpora, up to a realistic scenario where documents with different table layouts written by different hands are used. The results are good in general and show that a model based on Multilayer Perceptrons yields better results on more homogeneous documents, while another model based on Graph Neural Networks generalizes better on heterogeneous corpora.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552300185X",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Data mining",
      "Document layout analysis",
      "Graph",
      "Image (mathematics)",
      "Information extraction",
      "Information retrieval",
      "Machine learning",
      "Natural language processing",
      "Operating system",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Process (computing)",
      "Table (database)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Prieto",
        "given_name": "Jose Ramón"
      },
      {
        "surname": "Andrés",
        "given_name": "José"
      },
      {
        "surname": "Granell",
        "given_name": "Emilio"
      },
      {
        "surname": "Sánchez",
        "given_name": "Joan Andreu"
      },
      {
        "surname": "Vidal",
        "given_name": "Enrique"
      }
    ]
  },
  {
    "title": "An improved checkerboard detection algorithm based on adaptive filters",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.05.032",
    "abstract": "Checkerboard corner extraction is a crucial step in camera calibration. However, most existing algorithms are not good enough if the lens distortion is too large. This study aims to propose a checkerboard corner detection algorithm based on adaptive filters to address this problem. First, adaptive filters based on local image features are used to generate response maps of checkerboard images. Next, a non-max suppression and a scoring system are applied for further screening. Finally, the whole checkerboard structure is restored via inertia growth. The algorithm proposed is subject to rigorous experimental validation using synthetic and real images. Compared with several state of the art methods, our algorithm has the best performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523001654",
    "keywords": [
      "Algorithm",
      "Amplifier",
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Checkerboard",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Distortion (music)",
      "Geometry",
      "Image (mathematics)",
      "Mathematics",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Sang",
        "given_name": "Qiang"
      },
      {
        "surname": "Huang",
        "given_name": "Tao"
      },
      {
        "surname": "Wang",
        "given_name": "Hongyi"
      }
    ]
  },
  {
    "title": "Learning relation-based features for fine-grained image retrieval",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109543",
    "abstract": "Fine-Grained Image Retrieval (FGIR) is a fundamental yet challenging task that has recently received considerable attention. However, two critical issues remain unresolved. On the one hand, convolutional neural networks (CNNs) trained with image-level labels tend to focus on the most discriminative image patches but overlook the implicit relation among them. On the other hand, existing large models developed for FGIR are computationally expensive and difficult to learn discriminative features. To address these issues without additional object-level annotations or localization sub-networks, we propose a novel unified framework for fine-grained image retrieval. Specifically, a novel Relation-based Convolutional Descriptor Aggregation (RCDA) method for extracting subtle yet discriminative features from fine-grained images is introduced. The RCDA method consists of a local feature generation network and a relation extraction (RE) module that models both explicit information and implicit relations. The explicit information is modeled by computing feature similarities, while the implicit relation is mined via an expectation-maximization algorithm. Moreover, we further leverage the knowledge distillation technique to optimize the parameters of the feature generation network and speed up the fine-tuning procedure by transferring knowledge from a large model to a smaller model. Experimental results on three benchmark datasets (CUB-200-2011, Stanford-Car and FGVC-Aircraft) demonstrate that the proposed method not only achieves a significant improvement over baseline models but also outperforms state-of-the-art methods by a large margin (6.4%, 1.3%, 23.2%, respectively).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323002431",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Convolutional neural network",
      "Data mining",
      "Discriminative model",
      "Feature (linguistics)",
      "Feature extraction",
      "Feature learning",
      "Focus (optics)",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Image retrieval",
      "Leverage (statistics)",
      "Linguistics",
      "Machine learning",
      "Margin (machine learning)",
      "Mathematical optimization",
      "Mathematics",
      "Maximization",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Relation (database)"
    ],
    "authors": [
      {
        "surname": "Zhu",
        "given_name": "Yingying"
      },
      {
        "surname": "Cao",
        "given_name": "Gang"
      },
      {
        "surname": "Yang",
        "given_name": "Zhanyuan"
      },
      {
        "surname": "Lu",
        "given_name": "Xiufan"
      }
    ]
  },
  {
    "title": "Local pseudo-attributes for long-tailed recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.05.035",
    "abstract": "Existing long-tailed recognition methods focus on learning global image representation by re-weighing, re-sampling, or global representation learning. However, we observe that solving real-world long-tailed recognition problems requires a fine-grained understanding of local parts within the image in order to avoid confusion among images with similar global configurations. We propose a novel self-supervised learning framework based on local pseudo-attributes (LPA) that are learned via clustering of local features without any human annotations. Such pseudo-attributes are often more balanced compared to image-level class labels. Our method outperforms the state-of-the-art on various long-tailed image classification datasets, such as CIFAR100-LT, iNaturalist, and ImageNet-LT.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552300171X",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Cluster analysis",
      "Computer science",
      "Feature learning",
      "Focus (optics)",
      "Image (mathematics)",
      "Law",
      "Machine learning",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Political science",
      "Politics",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Kim",
        "given_name": "Dong-Jin"
      },
      {
        "surname": "Ke",
        "given_name": "Tsung-Wei"
      },
      {
        "surname": "Yu",
        "given_name": "Stella X."
      }
    ]
  },
  {
    "title": "Exemplar-free class incremental learning via discriminative and comparable parallel one-class classifiers",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109561",
    "abstract": "The exemplar-free class incremental learning (IL) requires classification models to learn new-class knowledge incrementally without retaining any old samples. Recently, the IL framework based on parallel one-class classifiers (POC) has demonstrated promising performance. It trains a one-class classifier (OCC) for each category and thus is immune to the catastrophic forgetting problem. However, the single-class training strategy may incur weak discriminability and low comparability between different classifiers in POC. To meet this challenge, we propose a new IL framework, referred to as Discriminative and Comparable Parallel One-class Classifiers (DCPOC). Instead of ordinary OCCs (e.g., deep SVDD) used in other POC methods, DCPOC adopts variational auto-encoders (VAE) as OCCs because VAEs can be used not only to identify classes for given samples but also to generate pseudo samples for the trained classes. With this advantage, DCPOC trains a new-class VAE in contrast with the old-class VAEs, which benefits the new-class VAE to reconstruct better for new-class samples but worse for old-class pseudo samples, thus enhancing the comparability. Furthermore, DCPOC introduces a hinge reconstruction loss to reinforce the discriminability. We evaluate our method on MNIST, CIFAR10, CIFAR100, Tiny-ImageNet, and ImageNet. The experimental results show that DCPOC achieves state-of-the-art performance on these datasets. 1 1 The source code is publicly available at https://github.com/SunWenJu123/DCPOC",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323002613",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Classifier (UML)",
      "Combinatorics",
      "Comparability",
      "Computer science",
      "Deep learning",
      "Discriminative model",
      "MNIST database",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Sun",
        "given_name": "Wenju"
      },
      {
        "surname": "Li",
        "given_name": "Qingyong"
      },
      {
        "surname": "Zhang",
        "given_name": "Jing"
      },
      {
        "surname": "Wang",
        "given_name": "Danyu"
      },
      {
        "surname": "Wang",
        "given_name": "Wen"
      },
      {
        "surname": "Geng",
        "given_name": "YangLi-ao"
      }
    ]
  },
  {
    "title": "Identifying effective trajectory predictions under the guidance of trajectory anomaly detection model",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109559",
    "abstract": "Trajectory Prediction (TP) is an important research topic in computer vision and robotics fields. Recently, many stochastic TP models have been proposed to deal with this problem and have achieved better performance than the traditional models with deterministic trajectory outputs. However, these stochastic models can generate a number of future trajectories with different qualities. They are lack of self-evaluation ability, that is, to examine the rationality of their prediction results, thus failing to guide users to identify high-quality ones from their candidate results. This hinders them from playing their best in real applications. In this paper, we make up for this defect and propose TPAD, a novel TP evaluation method based on the trajectory Anomaly Detection (AD) technique. In TPAD, we firstly combine the Automated Machine Learning (AutoML) technique and the experience in the AD and TP field to automatically design an effective trajectory AD model. Then, we utilize the learned trajectory AD model to examine the rationality of the predicted trajectories, and screen out good TP results for users. Extensive experimental results demonstrate that TPAD can effectively identify near-optimal prediction results, improving stochastic TP models’ practical application effect.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323002595",
    "keywords": [
      "Anomaly detection",
      "Artificial intelligence",
      "Astronomy",
      "Computer science",
      "Data mining",
      "Field (mathematics)",
      "Law",
      "Machine learning",
      "Mathematics",
      "Physics",
      "Political science",
      "Pure mathematics",
      "Rationality",
      "Robot",
      "Robotics",
      "Trajectory"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Chunnan"
      },
      {
        "surname": "Liang",
        "given_name": "Chen"
      },
      {
        "surname": "Chen",
        "given_name": "Xiang"
      },
      {
        "surname": "Wang",
        "given_name": "Hongzhi"
      }
    ]
  },
  {
    "title": "Distributional and spatial-temporal robust representation learning for transportation activity recognition",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109568",
    "abstract": "Transportation activity recognition (TAR) provides valuable support for intelligent transportation applications, such as urban transportation planning, driving behavior analysis, and traffic prediction. There are many advantages of movable sensor-based TAR, and the key challenge is to capture salient features from segmented data for representing diverse patterns of activity. Although existing methods based on statistical information are efficient, they usually rely on domain knowledge to construct high-quality features manually. Likewise, the methods based on spatial-temporal relationships achieve good performance but fail to extract statistical features. The features extracted by these two methods have proven to be crucial for the classification of activity. How to combine them to acquire a more robust representation remains an open question. In this work, we introduce a novel parallel model named Distributional and Spatial-Temporal Robust Representation (DSTRR), which combines automatic learning of statistical, spatial, and temporal features into a unified framework. This model leads to three optimized subnets and thus obtains a robust representation specific to TAR. Extensive experiments performed on three public datasets show that DSTRR is a state-of-the-art method compared with the baseline methods. The results of ablation study and visualization not only demonstrate the effectiveness of each component in DSTRR, but also show the model remains robust to a wide range of parameter variations.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323002686",
    "keywords": [
      "Artificial intelligence",
      "Baseline (sea)",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Data mining",
      "Feature learning",
      "Gene",
      "Geology",
      "Law",
      "Machine learning",
      "Oceanography",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Robustness (evolution)",
      "Salient",
      "Visualization"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Jing"
      },
      {
        "surname": "Liu",
        "given_name": "Yang"
      },
      {
        "surname": "Zhu",
        "given_name": "Wei"
      },
      {
        "surname": "Zhu",
        "given_name": "Xiaoguang"
      },
      {
        "surname": "Song",
        "given_name": "Liang"
      }
    ]
  },
  {
    "title": "MoCA: Incorporating domain pretraining and cross attention for textbook question answering",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109588",
    "abstract": "Textbook Question Answering (TQA) is a complex multimodal task to infer answers given large context descriptions and abundant diagrams. Compared with Visual Question Answering (VQA), TQA contains a large number of uncommon terminologies and various diagram inputs. It brings new challenges to the representation capability of language model for domain-specific spans. Also, it requires the model to take fully advantage of the complementary information of different diagram types, which pushes the multimodal fusion task to a more complex level. To tackle the above issues, we propose a novel model named MoCA, which incorporates Multi-stage domain pretraining and Cross-guided multimodal Attention for the TQA task. Firstly, we introduce a multi-stage domain pretraining module to conduct unsupervised post-pretraining with a span mask strategy and supervised pre-finetune. Especially for domain post-pretraining, we propose a heuristic generation algorithm to employ the terminology corpus. Secondly, to fully consider the rich inputs of context and diagrams, we propose a cross-guided multimodal attention mechanism to update the features of text, question diagram and instructional diagram based on a progressive strategy. Further, a dual gating mechanism is adopted to improve the model ensemble of three background retrievals. The experimental results show the superiority of our model, which outperforms the state-of-the-art methods on the validation and test split respectively. Also, ablation and comparison experiments verify the effectiveness of each module proposed in our model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323002893",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Domain (mathematical analysis)",
      "Economics",
      "Heuristic",
      "Linguistics",
      "Machine learning",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Natural language processing",
      "Paleontology",
      "Philosophy",
      "Question answering",
      "Task (project management)",
      "Terminology"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Fangzhi"
      },
      {
        "surname": "Lin",
        "given_name": "Qika"
      },
      {
        "surname": "Liu",
        "given_name": "Jun"
      },
      {
        "surname": "Zhang",
        "given_name": "Lingling"
      },
      {
        "surname": "Zhao",
        "given_name": "Tianzhe"
      },
      {
        "surname": "Chai",
        "given_name": "Qi"
      },
      {
        "surname": "Pan",
        "given_name": "Yudai"
      },
      {
        "surname": "Huang",
        "given_name": "Yi"
      },
      {
        "surname": "Wang",
        "given_name": "Qianying"
      }
    ]
  },
  {
    "title": "MEP-3M: A large-scale multi-modal E-commerce product dataset",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109519",
    "abstract": "The product categories are vital for the E-commerce platforms due to the core applications on automatic product category assignment, personalized product recommendations, etc. In this paper, we construct a large-scale Multi-modal E-commerce Products classification dataset MEP-3M, which is large-scale, hierarchical-categorized, multi-modal, fine-grained, and long-tailed. Statistically, MEP-3M consists of over 3 million products, thus achieves the largest data scale in comparison to the existing E-commerce product datasets. The products in MEP-3M are represented in three modalities: image, textual description, and OCR text, and labeled with tree-like labels. The third level labels are extremely fine-grained. In addition, we exploit four novel practical tasks on this dataset, Product classification, Hierarchical Product Classification, Fine-grained Product Classification, and Product Representation Learning. For each task, we present some image-only, text-only, and multi-modal baseline performances for further researches. The MEP-3M dataset will be released at https://github.com/ChenDelong1999/MEP-3M.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323002194",
    "keywords": [
      "Artificial intelligence",
      "Cartography",
      "Chemistry",
      "Computer science",
      "Construct (python library)",
      "Data mining",
      "E-commerce",
      "Engineering",
      "Geography",
      "Geometry",
      "Information retrieval",
      "Law",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Modal",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Polymer chemistry",
      "Product (mathematics)",
      "Programming language",
      "Representation (politics)",
      "Scale (ratio)",
      "Systems engineering",
      "Task (project management)",
      "Tree (set theory)",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Fan"
      },
      {
        "surname": "Chen",
        "given_name": "Delong"
      },
      {
        "surname": "Du",
        "given_name": "Xiaoyu"
      },
      {
        "surname": "Gao",
        "given_name": "Ruizhuo"
      },
      {
        "surname": "Xu",
        "given_name": "Feng"
      }
    ]
  },
  {
    "title": "Unsupervised generalizable multi-source person re-identification: A Domain-specific adaptive framework",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109546",
    "abstract": "Domain generalization (DG) has attracted much attention in person re-identification (ReID) recently. It aims to make a model trained on multiple source domains generalize to an unseen target domain. Although achieving promising progress, existing methods usually need the source domains to be labeled, which could be a significant burden for practical ReID tasks. In this paper, we turn to investigate “unsupervised” domain generalization for ReID, by assuming that no label is available for any source domains. To address this challenging setting, we propose a simple and efficient domain-specific adaptive framework, and realize it with an adaptive normalization module designed upon the batch and instance normalization techniques. In doing so, we successfully yield reliable pseudo-labels to implement training and also enhance the domain generalization capability of the model as required. In addition, we show that our framework can even be applied to improve person ReID under the settings of supervised domain generalization and unsupervised domain adaptation, demonstrating competitive performance with respect to relevant methods. Extensive experimental study on benchmark datasets is conducted to validate the proposed framework. A significance of our work lies in that it shows the potential of unsupervised domain generalization for person ReID and sets a strong baseline for the further research on this topic. The code is available at https://github.com/Qi5Lei/DSAF.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323002467",
    "keywords": [
      "Anthropology",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Botany",
      "Computer science",
      "Domain (mathematical analysis)",
      "Generalization",
      "Geodesy",
      "Geography",
      "Identification (biology)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Normalization (sociology)",
      "Operating system",
      "Sociology",
      "Source code"
    ],
    "authors": [
      {
        "surname": "Qi",
        "given_name": "Lei"
      },
      {
        "surname": "Liu",
        "given_name": "Jiaqi"
      },
      {
        "surname": "Wang",
        "given_name": "Lei"
      },
      {
        "surname": "Shi",
        "given_name": "Yinghuan"
      },
      {
        "surname": "Geng",
        "given_name": "Xin"
      }
    ]
  },
  {
    "title": "A novel relation aware wrapper method for feature selection",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109566",
    "abstract": "Feature selection, aiming at eliminating irrelevant and redundant features, is an important data preprocessing technology for downstream tasks, e.g., classification. With the explosive growth of data in various fields, some data are high-dimensional and contain critical and complex hidden relationships, which brings new challenges to feature selection: i) How to find out the underlying available relationships from the data, and ii) how to use the learned relations to better select features? To deal with these challenges, we propose a novel wrapper feature selection method named Relation Aware Feature Selection Method (ERASE), which can learn and use the underlying sample relations and feature relations for feature selection. Different from existing methods, our method jointly learns sample relationships and feature relationships through a graph of samples and trees of features. Furthermore, it uses the relations to select the optimal feature subset according to the new proposed Relation-based Sequence Floating Selection Strategy. Extensive experimental results on nine datasets from different domains demonstrate that our method achieves the best performance in most cases compared with other feature selection methods, including state-of-the-art wrapper methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323002662",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Data pre-processing",
      "Feature (linguistics)",
      "Feature selection",
      "Linguistics",
      "Machine learning",
      "Minimum redundancy feature selection",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Preprocessor",
      "Relation (database)",
      "Selection (genetic algorithm)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Zhaogeng"
      },
      {
        "surname": "Yang",
        "given_name": "Jielong"
      },
      {
        "surname": "Wang",
        "given_name": "Li"
      },
      {
        "surname": "Chang",
        "given_name": "Yi"
      }
    ]
  },
  {
    "title": "Pairwise learning for the partial label ranking problem",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109590",
    "abstract": "The partial label ranking problem is a particular preference learning scenario that focuses on learning preference models from data, such that they predict a complete ranking with ties defined over the values of the class variable for a given input instance. This work proposes to transform the rankings into preference relations among pairs of class labels and to learn a standard classifier for each of them. This classifier is then used to estimate the probability of each event from the preference relation between the two compared class labels. Finally, the probabilities obtained for each preference comparison are used to compute a preference matrix utilized to solve the corresponding rank aggregation problem and so obtain the ranking among all the class labels. The experimental evaluation shows that the proposed method is ranked ahead of competing algorithms in accuracy while obtaining similar CPU time results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323002911",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Classifier (UML)",
      "Computer science",
      "Learning to rank",
      "Machine learning",
      "Mathematics",
      "Pairwise comparison",
      "Preference",
      "Preference learning",
      "Ranking (information retrieval)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Alfaro",
        "given_name": "Juan C."
      },
      {
        "surname": "Aledo",
        "given_name": "Juan A."
      },
      {
        "surname": "Gámez",
        "given_name": "José A."
      }
    ]
  },
  {
    "title": "Transfer learning on stratified data: joint estimation transferred from strata",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109535",
    "abstract": "This paper studies the target model with the help of auxiliary models from different but possibly related groups. Inspired by transfer learning, we propose a method called joint estimation transferred from strata (JETS). To obtain a sparse solution, JETS constructs a penalized framework combining a term that penalizes the target model and an additional term that penalizes the differences between auxiliary models and the target model. In this way, JETS overcomes the challenge caused by the limited samples in high-dimensional study, and obtains stable and accurate estimates regardless of whether auxiliary samples contain noisy information. We demonstrate that this method enjoys the computational advantage of the traditional methods such as the lasso. During simulations and applications, the proposed method is compared with several existing methods and JETS outperforms others.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323002352",
    "keywords": [
      "Algorithm",
      "Architectural engineering",
      "Artificial intelligence",
      "Computer science",
      "Engineering",
      "Joint (building)",
      "Lasso (programming language)",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Term (time)",
      "Transfer of learning",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Gao",
        "given_name": "Yimiao"
      },
      {
        "surname": "Yang",
        "given_name": "Yuehan"
      }
    ]
  },
  {
    "title": "Continual spatio-temporal graph convolutional networks",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109528",
    "abstract": "Graph-based reasoning over skeleton data has emerged as a promising approach for human action recognition. However, the application of prior graph-based methods, which predominantly employ whole temporal sequences as their input, to the setting of online inference entails considerable computational redundancy. In this paper, we tackle this issue by reformulating the Spatio-Temporal Graph Convolutional Neural Network as a Continual Inference Network, which can perform step-by-step predictions in time without repeat frame processing. To evaluate our method, we create a continual version of ST-GCN, CoST-GCN, alongside two derived methods with different self-attention mechanisms, CoAGCN and CoS-TR. We investigate weight transfer strategies and architectural modifications for inference acceleration, and perform experiments on the NTU RGB+D 60, NTU RGB+D 120, and Kinetics Skeleton 400 datasets. Retaining similar predictive accuracy, we observe up to 109 × reduction in time complexity, on-hardware accelerations of 26 × , and reductions in maximum allocated memory of 52% during online inference.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323002285",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Graph",
      "Pattern recognition (psychology)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Hedegaard",
        "given_name": "Lukas"
      },
      {
        "surname": "Heidari",
        "given_name": "Negar"
      },
      {
        "surname": "Iosifidis",
        "given_name": "Alexandros"
      }
    ]
  },
  {
    "title": "Data preprocessing and feature selection techniques in gait recognition: A comparative study of machine learning and deep learning approaches",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.05.021",
    "abstract": "The study of gait recognition, a biometric application that identifies individuals based on their unique walking patterns, is an evolving field. In this paper, we conduct a literature review to compare the performance of machine learning and deep learning approaches in covariate conditions, focusing on the specific aspects of deep learning pipelines in gait recognition. We highlight commonly used strategies and open problems in identification based on behavioral traits and propose future perspectives for researchers in this field. Through our investigation, we aim to provide insights that will aid researchers in developing informed decisions when to take which data preprocessing technique in designing gait recognition systems. Our paper provides a comprehensive exposition of machine learning versus deep learning architectures and pipelines for biometric applications using human gait and serves as a valuable resource for researchers in this area.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523001484",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Biometrics",
      "Botany",
      "Computer network",
      "Computer science",
      "Data pre-processing",
      "Deep learning",
      "Feature selection",
      "Field (mathematics)",
      "Gait",
      "Gait analysis",
      "Identification (biology)",
      "Machine learning",
      "Mathematics",
      "Medicine",
      "Physical medicine and rehabilitation",
      "Preprocessor",
      "Pure mathematics",
      "Resource (disambiguation)"
    ],
    "authors": [
      {
        "surname": "Parashar",
        "given_name": "Anubha"
      },
      {
        "surname": "Parashar",
        "given_name": "Apoorva"
      },
      {
        "surname": "Ding",
        "given_name": "Weiping"
      },
      {
        "surname": "Shabaz",
        "given_name": "Mohammad"
      },
      {
        "surname": "Rida",
        "given_name": "Imad"
      }
    ]
  },
  {
    "title": "Robust semi-supervised multi-view graph learning with sharable and individual structure",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109565",
    "abstract": "The construction of a high-quality multi-view consensus graph is key to graph-based semi-supervised multi-view learning (GSSMvL) methods. However, most existing GSSMvL methods explore sample relationships in the original multi-view feature space, which obtains a contaminated graph that cannot reveal the underlying manifold structure of the samples. Moreover, traditional GSSMvL methods fail to explore the diverse structures of multi-view features, which may lose their complementary information and lead to a suboptimal graph. In this paper, we propose a novel unified robust semi-supervised multi-view graph learning framework based on the sharable and individual structure (RSSMvSI), which can eliminate the influence of noise and exploit the knowledge of multi-view data in a reasonable manner. Specifically, we first learn clean data by manipulating sparse noise with l 2 , 1 norm. We then simultaneously explore the sharable and individual self-representation subspace on the learned clean multi-view data. The key point is that noisy data does not participate in subspace learning, which improves the robustness of the proposed method. By constructing the optimal consensus graph with the learned sharable and individual subspace, RSSMvSI can better utilize the complementary information of multi-view data and approximate the manifold structure of samples. To the best of our knowledge, this is the first attempt to learn the self-representation subspace on recovered multi-view clean data. Extensive experiments on various real-world multi-view datasets demonstrate the superiority and robustness against state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323002650",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer security",
      "Data mining",
      "Exploit",
      "Feature learning",
      "Gene",
      "Graph",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Robustness (evolution)",
      "Semi-supervised learning",
      "Subspace topology",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Guo",
        "given_name": "Wei"
      },
      {
        "surname": "Wang",
        "given_name": "Zhe"
      },
      {
        "surname": "Du",
        "given_name": "Wenli"
      }
    ]
  },
  {
    "title": "Ambiguity-aware robust teacher (ART): Enhanced self-knowledge distillation framework with pruned teacher network",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109541",
    "abstract": "Self-knowledge distillation (self-KD) methods, which use a student model itself as the teacher model instead of a large and complex teacher model, are currently a subject of active study. Since most previous self-KD approaches relied on the knowledge of a single teacher model, if the teacher model incorrectly predicted confusing samples, poor-quality knowledge was transferred to the student model. Unfortunately, natural images are often ambiguous for teacher models due to multiple objects, mislabeling, or low quality. In this paper, we propose a novel knowledge distillation framework named ambiguity-aware robust teacher knowledge distillation (ART-KD) that provides refined knowledge, that reflects the ambiguity of the samples with network pruning. Since the pruned teacher model is simply obtained by copying and pruning the teacher model, re-training process is unnecessary in ART-KD. The key insight of ART-KD lies in the predictions of a teacher model and pruned teacher model for ambiguous samples providing different distributions with low similarity. From these two distributions, we obtain a joint distribution considering the ambiguity of the samples as teacher’s knowledge for distillation. We comprehensively evaluate our method on public classification benchmarks, as well as more challenging benchmarks for fine-grained visual recognition (FGVR), achieving much superior performance to state-of-the-art counterparts.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323002418",
    "keywords": [
      "Agronomy",
      "Ambiguity",
      "Artificial intelligence",
      "Biology",
      "Chemistry",
      "Computer science",
      "Computer security",
      "Copying",
      "Distillation",
      "Epistemology",
      "Key (lock)",
      "Law",
      "Machine learning",
      "Operating system",
      "Organic chemistry",
      "Philosophy",
      "Political science",
      "Process (computing)",
      "Programming language",
      "Pruning",
      "Quality (philosophy)"
    ],
    "authors": [
      {
        "surname": "Cho",
        "given_name": "Yucheol"
      },
      {
        "surname": "Ham",
        "given_name": "Gyeongdo"
      },
      {
        "surname": "Lee",
        "given_name": "Jae-Hyeok"
      },
      {
        "surname": "Kim",
        "given_name": "Daeshik"
      }
    ]
  },
  {
    "title": "Feature Selection as a Hedonic Coalition Formation Game for Arabic Topic Detection",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.06.002",
    "abstract": "Arabic topic detection is a Natural Language Processing (NLP) task that aims to assign a topic or a set of topics to a new document based on its content and selected features. Feature selection is an essential topic detection technique used to capture discriminative and descriptive features in documents to enhance the performance of topic detection. This paper proposes a novel feature selection approach formulated as a Hedonic Coalition Formation Game. We use the proposed method to describe each topic with a unique Topic Oriented Vocabulary (TOV) vector. We inspect the performances of the vocabularies obtained by solving the Hedonic game in topic detection over several Arabic corpora. In addition, the proposed feature selection method is compared with a Mutual Information (MI) based vocabulary vector generation method. The reported results show the effectiveness of the proposed feature selection approach by generating new vocabularies with highly discriminating features and reduced dimensions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523001812",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Discriminative model",
      "Economics",
      "Feature (linguistics)",
      "Feature selection",
      "Linguistics",
      "Machine learning",
      "Management",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Selection (genetic algorithm)",
      "Set (abstract data type)",
      "Task (project management)",
      "Vocabulary"
    ],
    "authors": [
      {
        "surname": "Koulali",
        "given_name": "Rim"
      },
      {
        "surname": "Koulali",
        "given_name": "Mohammed-Amine"
      }
    ]
  },
  {
    "title": "A black-box reversible adversarial example for authorizable recognition to shared images",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109549",
    "abstract": "Shared images on the Internet are easily collected, classified, and analyzed by unauthorized commercial companies through Deep Neural Networks (DNNs). The illegal use of these data damages the rights and interests of authorized companies and individuals. How to ensure that network-shared data is legally used by authorized users and not used by unauthorized DNNs has become an urgent problem. Reversible Adversarial Example (RAE) provides an effective solution, which can mislead the classification of unauthorized DNNs and does not affect the authorized users. The existing RAE schemes assumed that we could know the parameters of the target model and thus generate reversible adversarial examples. However, model parameters are often protected to avoid leakage, increasing the difficulty of generating accurate RAEs. In this paper, we first propose a Black-box Reversible Adversarial Example (B-RAE) scheme to generate robust reversible adversarial examples. We aim to protect image privacy while maintaining data usability in real scenarios. Experimental results and analysis have demonstrated that the proposed B-RAE is more effective and robust compared with the existing schemes.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323002492",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Artificial neural network",
      "Black box",
      "Computer science",
      "Computer security",
      "Damages",
      "Deep neural networks",
      "Human–computer interaction",
      "Image (mathematics)",
      "Internet privacy",
      "Law",
      "Mathematical analysis",
      "Mathematics",
      "Political science",
      "Scheme (mathematics)",
      "The Internet",
      "Usability",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Xiong",
        "given_name": "Lizhi"
      },
      {
        "surname": "Wu",
        "given_name": "Yue"
      },
      {
        "surname": "Yu",
        "given_name": "Peipeng"
      },
      {
        "surname": "Zheng",
        "given_name": "Yuhui"
      }
    ]
  },
  {
    "title": "Feature clustering-Assisted feature selection with differential evolution",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109523",
    "abstract": "Modern data collection technologies may produce thousands of or even more features in a single dataset. The high dimensionality of data poses a barrier to determining discriminating features due to the curse of dimensionality. Thanks to the global search ability, many population-based feature selection approaches have been proposed. However, very few studies pay attention on that a feature selection task has multiple optimal feature subsets. To search for multiple optimal feature subsets, we propose a feature clustering-assisted feature selection method. The proposed method employs the knowledge of correlation measures to group features. And, this correlation knowledge is embedded into the encoding method and the search process. A niching-based mutation operator is also used to explore the vicinity of a target individual. The aim is to find different feature subsets with very similar or the same classification performance. In addition, a modification operator is proposed aiming to increase the population diversity to improve the feature selection performance. The experiments on 16 datasets show that the proposed algorithm outperforms other popular feature selection methods in terms of classification accuracy and feature subset size.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323002236",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Curse of dimensionality",
      "Data mining",
      "Demography",
      "Dimensionality reduction",
      "Feature (linguistics)",
      "Feature selection",
      "Linguistics",
      "Minimum redundancy feature selection",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Population",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Peng"
      },
      {
        "surname": "Xue",
        "given_name": "Bing"
      },
      {
        "surname": "Liang",
        "given_name": "Jing"
      },
      {
        "surname": "Zhang",
        "given_name": "Mengjie"
      }
    ]
  },
  {
    "title": "Learning modality-invariant binary descriptor for crossing palmprint to palm-vein recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.05.026",
    "abstract": "Palmprint has shown promising potential for biometrics recognition due to its excellent contactless and hygienic properties. However, most existing methods focus only on feature learning of unimodal palmprint images leading to the challenge of palmprint recognition crossing different modalities. In this paper, we propose a modality-invariant binary features learning (MIBFL) method for crossing palmprint to palm-vein recognition, where palm images are captured under visible and invisible near-infrared illumination, respectively. We first map the multiple modal palm images into their high-dimensional alignment representation to reduce the impact of misalignment and noise between different image modalities. Then, we simultaneously learn the discriminative features from different modalities of alignment images via matrix factorization by enforcing the orthogonal and balanced constraints. Lastly, we jointly learn a pair of encoding functions to project multi-modal palm features into the common binary feature descriptor for crossing palmprint to palm-vein recognition. Experimental results on the widely used PolyU multi-spectral palmprint database are presented to demonstrate the effectiveness of the proposed MIBFL method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523001617",
    "keywords": [
      "Arithmetic",
      "Artificial intelligence",
      "Binary number",
      "Biometrics",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Discriminative model",
      "Feature (linguistics)",
      "Feature extraction",
      "Feature learning",
      "Focus (optics)",
      "Histogram",
      "Image (mathematics)",
      "Invariant (physics)",
      "Linguistics",
      "Local binary patterns",
      "Mathematical physics",
      "Mathematics",
      "Modal",
      "Modality (human–computer interaction)",
      "Optics",
      "Palm print",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Polymer chemistry"
    ],
    "authors": [
      {
        "surname": "Su",
        "given_name": "Le"
      },
      {
        "surname": "Fei",
        "given_name": "Lunke"
      },
      {
        "surname": "Zhao",
        "given_name": "Shuping"
      },
      {
        "surname": "Wen",
        "given_name": "Jie"
      },
      {
        "surname": "Zhu",
        "given_name": "Jian"
      },
      {
        "surname": "Teng",
        "given_name": "Shaohua"
      }
    ]
  },
  {
    "title": "RMAML: Riemannian meta-learning with orthogonality constraints",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109563",
    "abstract": "Meta-learning is the core capability that enables intelligent systems to rapidly generalize their prior experience to learn new tasks. In general, the optimization-based methods formalize the meta-learning as a bi-level optimization problem, that is a nested optimization framework, in which meta-parameters are optimized (or learned) at the outer-level, while the inner-level optimizes the task-specific parameters. In this paper, we introduce RMAML, a meta-learning method that enforces orthogonality constraints to the bi-level optimization problem. We develop a geometry aware framework that generalizes the bi-level optimization problem to the Riemannian (constrained) setting. Using the Riemannian operations such as orthogonal projection, retraction and parallel transport, the bi-level optimization is reformulated so that it respects the Riemannian geometry. Moreover, we observe that a superior stable optimization and an improved generalization ability can be achieved when the parameters and meta-parameters of the method are modeled using a Stiefel Manifold. We empirically show that RMAML can easily reach competitive performances against several state of the art algorithms for few-shot classification and consistently outperforms its Euclidean counterpart, MAML. For example, by using the geometry of the Stiefel manifold to structure the fully-connected layers in a deep neural network, a 7% increase in single-domain few-shot classification accuracy is achieved. For the cross-domain few-shot learning, RMAML outperforms MAML by up to 9% of accuracy. Our ablation study also demonstrates the effectiveness of RMAML over MAML in terms of higher accuracy with a reduced number of tasks and (or) inner-level updates.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323002637",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Domain (mathematical analysis)",
      "Engineering",
      "Generalization",
      "Geometry",
      "Manifold (fluid mechanics)",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Mechanical engineering",
      "Optimization problem",
      "Orthogonality",
      "Projection (relational algebra)",
      "Pure mathematics",
      "Riemannian geometry",
      "Riemannian manifold",
      "Stiefel manifold"
    ],
    "authors": [
      {
        "surname": "Tabealhojeh",
        "given_name": "Hadi"
      },
      {
        "surname": "Adibi",
        "given_name": "Peyman"
      },
      {
        "surname": "Karshenas",
        "given_name": "Hossein"
      },
      {
        "surname": "Roy",
        "given_name": "Soumava Kumar"
      },
      {
        "surname": "Harandi",
        "given_name": "Mehrtash"
      }
    ]
  },
  {
    "title": "MES-Loss: Mutually equidistant separation metric learning loss function",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.06.005",
    "abstract": "Deep metric learning has attracted much attention in recent years due to its extensive applications, such as clustering and image retrieval. Thanks to the success of deep learning (DL), many deep metric learning (DML) methods have been proposed. Neural networks (NNs) utilize DML loss functions to learn a mapping function that maps samples into a highly discriminative low-dimensional feature space, facilitating measuring similarities between pairs of samples in such a manifold. Most existing methods usually try to boost the discriminatory power of NN by enhancing intra-class compactness in the high-level feature space. However, they do not explicitly imply constraints to improve inter-class separation. We propose in this paper a new composite DML loss function that, in addition to the intra-class compactness, explicitly implies regulations to enforce the best inter-class separation by mutually equidistantly distributing the centers of the classes. The proposed DML loss function achieved state-of-the-art results for clustering and image-retrieval tasks on two real-world data sets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523001824",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Class (philosophy)",
      "Cluster analysis",
      "Combinatorics",
      "Compact space",
      "Computer science",
      "Deep learning",
      "Dimensionality reduction",
      "Discrete mathematics",
      "Discriminative model",
      "Economics",
      "Equidistant",
      "Evolutionary biology",
      "Feature (linguistics)",
      "Feature learning",
      "Feature vector",
      "Function (biology)",
      "Geometry",
      "Linguistics",
      "Mathematics",
      "Metric (unit)",
      "Metric space",
      "Nonlinear dimensionality reduction",
      "Operations management",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pure mathematics",
      "Topology (electrical circuits)"
    ],
    "authors": [
      {
        "surname": "Boutaleb",
        "given_name": "Yasser"
      },
      {
        "surname": "Soladie",
        "given_name": "Catherine"
      },
      {
        "surname": "Duong",
        "given_name": "Nam-Duong"
      },
      {
        "surname": "Kacete",
        "given_name": "Amine"
      },
      {
        "surname": "Royan",
        "given_name": "Jérôme"
      },
      {
        "surname": "Seguier",
        "given_name": "Renaud"
      }
    ]
  },
  {
    "title": "Semi-automatic muscle segmentation in MR images using deep registration-based label propagation",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109529",
    "abstract": "Fully automated approaches based on convolutional neural networks have shown promising performances on muscle segmentation from magnetic resonance (MR) images, but still rely on an extensive amount of training data to achieve valuable results. Muscle segmentation for pediatric and rare diseases cohorts is therefore still often done manually. Producing dense delineations over 3D volumes remains a time-consuming and tedious task, with significant redundancy between successive slices. In this work, we propose a segmentation method relying on registration-based label propagation, which provides 3D muscle delineations from a limited number of annotated 2D slices. Based on an unsupervised deep registration scheme, our approach ensures the preservation of anatomical structures by penalizing deformation compositions that do not produce consistent segmentation from one annotated slice to another. Evaluation is performed on MR data from lower leg and shoulder joints. Results demonstrate that the proposed semi-automatic multi-label segmentation model outperforms state-of-the-art techniques.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323002297",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Deep learning",
      "Operating system",
      "Pattern recognition (psychology)",
      "Redundancy (engineering)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Decaux",
        "given_name": "Nathan"
      },
      {
        "surname": "Conze",
        "given_name": "Pierre-Henri"
      },
      {
        "surname": "Ropars",
        "given_name": "Juliette"
      },
      {
        "surname": "He",
        "given_name": "Xinyan"
      },
      {
        "surname": "Sheehan",
        "given_name": "Frances T."
      },
      {
        "surname": "Pons",
        "given_name": "Christelle"
      },
      {
        "surname": "Ben Salem",
        "given_name": "Douraied"
      },
      {
        "surname": "Brochard",
        "given_name": "Sylvain"
      },
      {
        "surname": "Rousseau",
        "given_name": "François"
      }
    ]
  },
  {
    "title": "An enhanced vision transformer with wavelet position embedding for histopathological image classification",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109532",
    "abstract": "Histopathological image classification is a fundamental task in pathological diagnosis workflow. It remains a huge challenge due to the complexity of histopathological images. Recently, hybrid methods combining convolutional neural networks(CNN) with vision transformers(ViT) are proposed to this field. These methods can well represent the global and local contextual information and achieve excellent classification performances. However, the downsampling operation like max-pooling which ignores the sampling theorem transmits the jagged artifacts into transformer, which would lead to an aliasing phenomenon. It makes the subsequent feature maps focus on the incorrect regions and influences the final classification results. In this work, we propose an enhanced vision transformer with wavelet position embedding to tackle this challenge. In particular, a wavelet position embedding module, which introduces the wave transform into position embedding, is employed to enhance the smoothness of discontinuous feature information by decomposing sequences into amplitude and phase in pathological feature maps. In addition, an external multi-head attention is proposed to replace self-attention in the transformer block with two linear layers. It reduces the cost of computation and excavates potential correlations between different samples. We evaluate the proposed method on three public histopathological classification challenging datasets, and perform a quantitative comparison with previous state-of-the-art methods. The results empirically demonstrate that our method achieves the best accuracy. Furthermore, it has the least parameters and a very low FLOPs. In conclusion, the enhanced vision transformer shows high classification performances and demonstrates significant potential for assisting pathologists in pathological diagnosis.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323002327",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Electrical engineering",
      "Embedding",
      "Engineering",
      "Pattern recognition (psychology)",
      "Transformer",
      "Voltage",
      "Wavelet",
      "Wavelet transform"
    ],
    "authors": [
      {
        "surname": "Ding",
        "given_name": "Meidan"
      },
      {
        "surname": "Qu",
        "given_name": "Aiping"
      },
      {
        "surname": "Zhong",
        "given_name": "Haiqin"
      },
      {
        "surname": "Lai",
        "given_name": "Zhihui"
      },
      {
        "surname": "Xiao",
        "given_name": "Shuomin"
      },
      {
        "surname": "He",
        "given_name": "Penghui"
      }
    ]
  },
  {
    "title": "Cross-level Feature Aggregation Network for Polyp Segmentation",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109555",
    "abstract": "Accurate segmentation of polyps from colonoscopy images plays a critical role in the diagnosis and cure of colorectal cancer. Although effectiveness has been achieved in the field of polyp segmentation, there are still several challenges. Polyps often have a diversity of size and shape and have no sharp boundary between polyps and their surrounding. To address these challenges, we propose a novel Cross-level Feature Aggregation Network (CFA-Net) for polyp segmentation. Specifically, we first propose a boundary prediction network to generate boundary-aware features, which are incorporated into the segmentation network using a layer-wise strategy. In particular, we design a two-stream structure based segmentation network, to exploit hierarchical semantic information from cross-level features. Furthermore, a Cross-level Feature Fusion (CFF) module is proposed to integrate the adjacent features from different levels, which can characterize the cross-level and multi-scale information to handle scale variations of polyps. Further, a Boundary Aggregated Module (BAM) is proposed to incorporate boundary information into the segmentation network, which enhances these hierarchical features to generate finer segmentation maps. Quantitative and qualitative experiments on five public datasets demonstrate the effectiveness of our CFA-Net against other state-of-the-art polyp segmentation methods. The source code and segmentation maps will be released at https://github.com/taozh2017/CFANet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323002558",
    "keywords": [
      "Artificial intelligence",
      "Boundary (topology)",
      "Computer science",
      "Feature (linguistics)",
      "Image segmentation",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Scale-space segmentation",
      "Segmentation",
      "Segmentation-based object categorization"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Tao"
      },
      {
        "surname": "Zhou",
        "given_name": "Yi"
      },
      {
        "surname": "He",
        "given_name": "Kelei"
      },
      {
        "surname": "Gong",
        "given_name": "Chen"
      },
      {
        "surname": "Yang",
        "given_name": "Jian"
      },
      {
        "surname": "Fu",
        "given_name": "Huazhu"
      },
      {
        "surname": "Shen",
        "given_name": "Dinggang"
      }
    ]
  },
  {
    "title": "Open set classification of untranscribed handwritten text image documents",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.06.006",
    "abstract": "Content-based classification of manuscripts is an important task that is generally carried out by expert archivists. Nevertheless, many historical manuscript collections are so vast that in most cases this task is hardly feasible, even for large, well staffed archives. Nowadays, manuscripts are generally preserved in the form of sets of digital images. Therefore, the technical problem we are interested in is automatic classification of “‘image documents”, each consisting of a set of untranscribed handwritten text images, by the textual contents of the images. The traditional Pattern Recognition classification paradigm does provide the basic tools to deal with this problem. However, in practice, the set of relevant classes of a large documental series is seldom known in advance. Therefore, a classifier trained with a predefined set of classes will systematically fail when new image documents arrive which do not belong to any of the classes assumed in training. Here we adopt the “Open Set Classification” framework to extend and consolidate our previous work on image document classification in order to adequately handle new documents from unknown classes. The proposed approaches are based on a relatively novel technology for text image representation known as “probabilistic indexing”, which proves very effective to characterise the intrinsic word-level uncertainty exhibited by historical handwritten text images. We assess the performance of this approach on a moderately sized but representative dataset extracted from a huge series of complex notarial manuscripts from the Spanish Archivo Histórico Provincial de Cádiz, with good results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523001848",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Contextual image classification",
      "Economics",
      "Image (mathematics)",
      "Information retrieval",
      "Law",
      "Linguistics",
      "Management",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Probabilistic logic",
      "Programming language",
      "Representation (politics)",
      "Search engine indexing",
      "Set (abstract data type)",
      "Task (project management)",
      "Word (group theory)"
    ],
    "authors": [
      {
        "surname": "Prieto",
        "given_name": "Jose Ramón"
      },
      {
        "surname": "Flores",
        "given_name": "Juan José"
      },
      {
        "surname": "Vidal",
        "given_name": "Enrique"
      },
      {
        "surname": "Toselli",
        "given_name": "Alejandro Hector"
      }
    ]
  },
  {
    "title": "Diluted binary neural network",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109556",
    "abstract": "Binary neural networks (BNNs) are promising on resource-constrained devices because they reduce memory consumption and accelerate inference effectively. However, they are still potential on performance improvement. Prior studies attribute performance degradation of BNNs to limited representation ability and gradient mismatch. In this paper, we find that it also results from the mandatory representation of small full-precision auxiliary weights to large values. To tackle with this issue, we propose an approach dubbed as Diluted Binary Neural Network (DBNN). Besides avoiding mandatory representation effectively, the proposed DBNN also alleviates sign flip problem to a large extent. For activations, we jointly minimize quantization error and maximize information entropy to develop the binarization scheme. Compared with existing sparsity-binarization approaches, DBNN trains network from scratch without other procedures and achieves larger sparsity. Experiments on several datasets with various networks demonstrate the superiority of our approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032300256X",
    "keywords": [
      "Algorithm",
      "Arithmetic",
      "Artificial intelligence",
      "Artificial neural network",
      "Binary number",
      "Computer science",
      "Data mining",
      "Deep neural networks",
      "Entropy (arrow of time)",
      "Inference",
      "Law",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Political science",
      "Politics",
      "Quantization (signal processing)",
      "Quantum mechanics",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Lin",
        "given_name": "Yuhan"
      },
      {
        "surname": "Niu",
        "given_name": "Lingfeng"
      },
      {
        "surname": "Xiao",
        "given_name": "Yang"
      },
      {
        "surname": "Zhou",
        "given_name": "Ruizhi"
      }
    ]
  },
  {
    "title": "A Noising-Denoising Framework for Point Cloud Upsampling via Normalizing Flows",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109569",
    "abstract": "Point cloud upsampling aims to generate dense and uniform point cloud from the sparse input point cloud. One challenge is how to flexibly upsample the sparse point cloud in arbitrary ratios, even without the given supervised high resolution point cloud. To address this challenge, we propose a noising-denoising framework, dubbed ND-PUFlow, for arbitrary 3D point cloud upsampling (3DPU) in supervised and self-supervised settings. It consists of two stages, i.e., dense noisy points generation and noisy points denoising via continuous normalizing flows (CNFs). In the first stage, noisy points are generated by adding noise to the input points. In the second stage, CNFs move each noisy point to the underlying surface, forming a dense and clean point cloud. Extensive experiments show that our method is competitive in both supervised and self-supervised settings, and in most cases, it achieves the best performance on benchmark datasets for 3DPU.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323002698",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Cloud computing",
      "Computer science",
      "Computer vision",
      "Geodesy",
      "Geography",
      "Geometry",
      "Image (mathematics)",
      "Mathematics",
      "Noise (video)",
      "Noise reduction",
      "Operating system",
      "Pattern recognition (psychology)",
      "Point (geometry)",
      "Point cloud",
      "Upsampling"
    ],
    "authors": [
      {
        "surname": "Hu",
        "given_name": "Xin"
      },
      {
        "surname": "Wei",
        "given_name": "Xin"
      },
      {
        "surname": "Sun",
        "given_name": "Jian"
      }
    ]
  },
  {
    "title": "CDText: Scene text detector based on context-aware deformable transformer",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.05.025",
    "abstract": "Scene text detection task aims to precisely locate text regions in natural scenes. However, the existing methods still face challenges in detecting arbitrary-shaped text, due to their limited feature representation capability. To alleviate this problem, we propose a scene text detector, i.e., CDText, based on structure of context-aware deformable transformer. Specifically, CDText firstly adopts different convolution kernel designs for feature extraction, which designs receptive fields with different size for multi-scale feature perception and fusion. Meanwhile, multi-head self-attention mechanism is used to strengthen the reasoning ability of CDText in a global sense, thus enhancing feature maps with abundant context information by extracting implicit relationship between multi-scale text features. Moreover, CDText designs a segmentation head to segment text instances of arbitrary shapes from rectangular detection boxes. Experiments show that CDText is superior to comparative methods in detection accuracy, achieving F-scores of 92.7, 81.9, and 82.9 on ICDAR2013, Total Text, and CTW-1500 datasets, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523001599",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Detector",
      "Feature (linguistics)",
      "Feature extraction",
      "Image (mathematics)",
      "Kernel (algebra)",
      "Law",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Political science",
      "Politics",
      "Quantum mechanics",
      "Representation (politics)",
      "Segmentation",
      "Telecommunications",
      "Text detection",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Yirui"
      },
      {
        "surname": "Kong",
        "given_name": "Qiran"
      },
      {
        "surname": "Lai",
        "given_name": "Yong"
      },
      {
        "surname": "Narducci",
        "given_name": "Fabio"
      },
      {
        "surname": "Wan",
        "given_name": "Shaohua"
      }
    ]
  },
  {
    "title": "Low-rank learning for feature selection in multi-label classification",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.05.036",
    "abstract": "Considering several research areas such as data mining, pattern recognition, and machine learning, it is difficult to handle high-dimensional data sets because of time consumption, large memory, and inefficient algorithms. Therefore, the feature selection method is widely used to address these problems. Several conventional feature selection methods have selected features by calculating the relationship between features and labels using some measures such as a mutual information. However, it is difficult to calculate mutual information because it requires to joint probability from a high-dimensional data set. Therefore, only few features can be considered, and a global search cannot be used. Moreover, calculating the correlation among labels using mutual information is limited because of insufficient label sets. In this study, we propose a feature selection method for multi-label classification based on low-rank learning. Unlike conventional methods, which use restricted search space, the proposed method considers a low-rank space from the entire feature space provided and the relationship between features and labels simultaneously. Therefore, we design a regression-based objective function using a nuclear norm and propose an efficient algorithm based on the gradient descent method to solve the optimization problem. The proposed method shows a better performance than the existing feature selection methods. This is based on the results of the multi-label classification experiments with five data sets and five multi-label classification performances.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523001708",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Data mining",
      "Feature (linguistics)",
      "Feature selection",
      "Feature vector",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Multi-label classification",
      "Mutual information",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Rank (graph theory)"
    ],
    "authors": [
      {
        "surname": "Lim",
        "given_name": "Hyunki"
      }
    ]
  },
  {
    "title": "Sensitivity pruner: Filter-Level compression algorithm for deep neural networks",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109508",
    "abstract": "As neural networks get deeper for better performance, the demand for deployable models on resource-constrained devices also grows. In this work, we propose eliminating less sensitive filters to compress models. The previous method evaluates neuron importance using the connection matrix gradient in a single shot. To mitigate the sampling bias, we integrate this measure into the previously proposed “pruning while fine-tuning” framework. Besides classification errors, we introduce the difference between the learned and the single-shot strategy as the second loss component with a self-adjustive hyper-parameter that balances the training goal between improving accuracy and pruning more filters. Our Sensitivity Pruner (SP) adapts the unstructured pruning saliency metric to structured pruning tasks and enables the strategy to be derived sequentially to accommodate the updating sparsity. Experimental results demonstrate that SP significantly reduces the computational cost and the pruned models give comparable or better performance on CIFAR10, CIFAR100, and ILSVRC-12 datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032300208X",
    "keywords": [
      "Agronomy",
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Composite material",
      "Computer science",
      "Computer vision",
      "Economics",
      "Electronic engineering",
      "Engineering",
      "Filter (signal processing)",
      "Materials science",
      "Matrix (chemical analysis)",
      "Metric (unit)",
      "Operations management",
      "Pattern recognition (psychology)",
      "Pruning",
      "Sensitivity (control systems)"
    ],
    "authors": [
      {
        "surname": "Guo",
        "given_name": "Suhan"
      },
      {
        "surname": "Lai",
        "given_name": "Bilan"
      },
      {
        "surname": "Yang",
        "given_name": "Suorong"
      },
      {
        "surname": "Zhao",
        "given_name": "Jian"
      },
      {
        "surname": "Shen",
        "given_name": "Furao"
      }
    ]
  },
  {
    "title": "AEA-Net:Affinity-supervised entanglement attentive network for person re-identification",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.07.006",
    "abstract": "Most existing person re-identification algorithms prioritize the extraction of effective and salient local features while neglecting the affinity between local and adjacent features. This phenomenon will cause misidentification when different people have similar local features. To address this issue, we introduce the AEA-Net, which emphasizes the affinity between the local features of a single image. Specifically, three important components are proposed. The affinity-supervised attention module (ASA) centers on adjacent features and utilizes the affinity between global and adjacent features to supervise the learning of attention. The affinity relationship module (AR) focuses on constructing relationship features between local and adjacent features to enhance the closeness between local features. The tangle hybrid loss (THL) makes the final predictions have a distinct weight profile. Extensive experiments quantitatively and qualitatively demonstrate that our method outperforms the state-of-the-art approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523002039",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Closeness",
      "Computer science",
      "Geometry",
      "Identification (biology)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Net (polyhedron)",
      "Pattern recognition (psychology)",
      "Salient"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Dengwen"
      },
      {
        "surname": "Chen",
        "given_name": "Yanbing"
      },
      {
        "surname": "Tao",
        "given_name": "Lingbing"
      },
      {
        "surname": "Hu",
        "given_name": "Chentao"
      },
      {
        "surname": "Tie",
        "given_name": "Zhixin"
      },
      {
        "surname": "Ke",
        "given_name": "Wei"
      }
    ]
  },
  {
    "title": "Multicycle disassembly-based decomposition algorithm to train multiclass support vector machines",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109479",
    "abstract": "Employing the classic optimization solver to train a multiclass support vector machine (SVM) requires prohibitive training time as the sample size and number of categories increase. It has been proposed to develop the corresponding decomposition algorithm (DA) as it is efficient for training SVMs. However, the dual problem of multiclass SVM comprises complex constraints that complicate DA design, so no corresponding DA has yet been developed. We propose a multicycle disassembly-based DA (MCD-DA) to efficiently solve the training problem of multiclass SVM. First, a graph model is constructed to re-express the constraints in multiclass SVM. Then, the original complex feasible region is partitioned into several simple sub-feasible regions, and multiple cycle-based disassembly strategies are designed to update the working variables analytically within each specific sub-feasible region. We mathematically verify that MCD-DA can stop within a finite number of cycle disassemblies and reach the τ -optimal solution satisfying relaxed Karush–Kuhn–Tucker conditions. Remarkably, MCD-DA as a universal decomposition algorithm can be used to solve many other SVM variants, including C-SVM, v-SVM, and one-class SVM. Experimental results using six UCI datasets demonstrate that MCD-DA outperforms typical optimization algorithms for more sample cases.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323001796",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Decomposition",
      "Ecology",
      "Multiclass classification",
      "Pattern recognition (psychology)",
      "Programming language",
      "Solver",
      "Structured support vector machine",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Gao",
        "given_name": "Tong"
      },
      {
        "surname": "Chen",
        "given_name": "Hao"
      }
    ]
  },
  {
    "title": "Discriminative semi-supervised learning via deep and dictionary representation for image classification",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109521",
    "abstract": "Supervised dictionary learning and deep learning have achieved promising performance in the classification task. However, in many real-world applications there usually exist very limited labeled training samples, although abundant unlabeled data is relatively easy to collect. How to effectively exploit the discrimination of unlabeled data is still an open question, hence semi-supervised learning has attracted much attention from wide fields. Semi-supervised deep feature learning has well exploited the feature discrimination from only the discriminative viewpoint, while dictionary representation-based classification has also been applied to semi-supervised learning but with shallow features. In this paper, we propose a novel discriminative semi-supervised learning via deep and dictionary representation (DSSLDDR), which jointly utilizes the discrimination of dictionary representation for data reconstruction and the distinguishing feature of each sample. To exploit the powerful discrimination of dictionary representation, class-specific dictionaries are required to discriminatively reconstruct a sample, with the reconstruction error to predict the sample’s class label. To exploit the semantic information, the deep neural network extracts discriminative features by using multiple nonlinear transformations to generate the powerful descriptor. Then the class-specific dictionary learning and deep network learning are integrated together to conduct more accurate class estimation for unlabeled data and learn a more discriminative classifier, where an entropy regularization is designed to balance and control the class estimation of unlabeled data. Furthermore, we propose the DSSLDDR++, the extension model of DSSLDDR based on consistency/contrastive learning to further improve the accuracy of class estimation for unlabeled data, making a more powerful semi-supervised learning classifier. Extensive experiments on benchmark datasets show the effectiveness of the proposed methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323002212",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Classifier (UML)",
      "Computer science",
      "Computer security",
      "Deep learning",
      "Discriminative model",
      "Exploit",
      "Feature learning",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Semi-supervised learning",
      "Supervised learning"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Meng"
      },
      {
        "surname": "Ling",
        "given_name": "Jie"
      },
      {
        "surname": "Chen",
        "given_name": "Jiaming"
      },
      {
        "surname": "Feng",
        "given_name": "Mao"
      },
      {
        "surname": "Yang",
        "given_name": "Jian"
      }
    ]
  },
  {
    "title": "Low-rank kernel regression with preserved locality for multi-class analysis",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109601",
    "abstract": "Kernel ridge regression (KRR) is a kind of efficient supervised algorithm for multi-class analysis. However, limited by the implicit kernel space, current KRR methods have weak abilities to deal with redundant features and hidden local structures. Thus, they may get indifferent results when applied to analyze the data with complicated components. To overcome this weakness and obtain better multi-class regression performance, we propose a new method named low-rank kernel regression with preserved locality (RLRKRR). In this method, data are mapped into an explicit feature space by using the random Fourier feature technique to discover the non-linear relationship between data samples. In addition, during the training of the regression coefficient matrix, the low-rank components of this explicit feature space are simultaneously extracted for reducing the effect of the redundancy. Moreover, the graph regularization is performed on the extracted low-rank components to preserve local structures. Furthermore, the l 2 , p norm is imposed on the regression error term for relieving the impact of outliers. Based on these strategies, RLRKRR is capable to achieve rewarding results in complicated multi-class data analysis. In the comprehensive experiments conducted on various types of datasets, RLRKRR outperforms several state-of-the-art regression methods in terms of classification accuracy (CA).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323003023",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Kernel (algebra)",
      "Kernel method",
      "Linguistics",
      "Locality",
      "Mathematics",
      "Outlier",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Regression",
      "Statistics",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Yingxu"
      },
      {
        "surname": "Chen",
        "given_name": "Long"
      },
      {
        "surname": "Zhou",
        "given_name": "Jin"
      },
      {
        "surname": "Li",
        "given_name": "Tianjun"
      },
      {
        "surname": "Yu",
        "given_name": "Yufeng"
      }
    ]
  },
  {
    "title": "AIDA: Analytic isolation and distance-based anomaly detection algorithm",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109607",
    "abstract": "Many unsupervised anomaly detection algorithms rely on the concept of nearest neighbours to compute the anomaly scores. Such algorithms are popular because there are no assumptions about the data, making them a robust choice for unstructured datasets. However, the number ( k ) of nearest neighbours, which critically affects the model performance, cannot be tuned in an unsupervised setting. Hence, we propose the new and parameter-free Analytic Isolation and Distance-based Anomaly (AIDA) detection algorithm, that combines the metrics of distance with isolation. Based on AIDA, we also introduce the Tempered Isolation-based eXplanation (TIX) algorithm, which identifies the most relevant features characterizing an outlier, even in large multi-dimensional datasets, improving the overall explainability of the detection mechanism. Both AIDA and TIX are thoroughly tested and compared with state-of-the-art alternatives, proving to be useful additions to the existing set of tools in anomaly detection.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323003084",
    "keywords": [
      "Algorithm",
      "Anomaly (physics)",
      "Anomaly detection",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Condensed matter physics",
      "Data mining",
      "Isolation (microbiology)",
      "Microbiology",
      "Outlier",
      "Pattern recognition (psychology)",
      "Physics",
      "Programming language",
      "Set (abstract data type)",
      "k-nearest neighbors algorithm"
    ],
    "authors": [
      {
        "surname": "Souto Arias",
        "given_name": "Luis Antonio"
      },
      {
        "surname": "Oosterlee",
        "given_name": "Cornelis W."
      },
      {
        "surname": "Cirillo",
        "given_name": "Pasquale"
      }
    ]
  },
  {
    "title": "BDNet: A BERT-based dual-path network for text-to-image cross-modal person re-identification",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109636",
    "abstract": "Text-to-image person re-identification (TI-ReID) aims to provide a descriptive sentence to find a specific person in the gallery. The task is very challenging due to the huge feature differences between both image and text descriptions. Currently, most approaches use the idea of combining global and local features to get more fine-grained features. However, these methods usually acquire local features with the help of human pose or segmentation models, which makes it difficult to use in realistic scenarios due to the introduction of additional models or complex training evaluation strategies. To facilitate practical applications, we propose a BERT-based framework for dual-path TI-ReID. Without the help of additional models, our approach directly employs visual attention in the global feature extraction network to allow the network to adaptively learn to focus on salient local features in image and text descriptions, which enhances the network’s attention to local information through a visual attention mechanism, thus strengthening the global feature representation and effectively improving the global feature representation. In addition, to learn text and image modality invariant feature representations, we propose a convolutional shared network (CSN) to learn image and text features together. To optimize cross-modal feature distances more effectively, we propose a global hybrid modal triplet global metric loss. In addition to combining local metric learning and global metric learning, we also introduce the CMPM loss and CMPC loss to jointly optimize the proposed model. Extensive experiments on the CUHK-PEDES dataset show that the proposed method performs significantly better than the current research results, achieving a Rank-1/mAP accuracy of 66.27%/ 57.04%.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323003370",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Computer science",
      "Economics",
      "Feature (linguistics)",
      "Feature extraction",
      "Feature learning",
      "Identification (biology)",
      "Law",
      "Linguistics",
      "Machine learning",
      "Metric (unit)",
      "Operations management",
      "Path (computing)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Pooling",
      "Programming language",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Qiang"
      },
      {
        "surname": "He",
        "given_name": "Xiaohai"
      },
      {
        "surname": "Teng",
        "given_name": "Qizhi"
      },
      {
        "surname": "Qing",
        "given_name": "Linbo"
      },
      {
        "surname": "Chen",
        "given_name": "Honggang"
      }
    ]
  },
  {
    "title": "Presumably correct decision sets",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109640",
    "abstract": "The paper presents the presumably correct decision sets as a tool to analyze uncertainty in the form of inconsistency in decision systems. As a first step, problem instances are gathered into three regions containing weak members, borderline members, and strong members. This is accomplished by using the membership degrees of instances to their neighborhoods while neglecting their actual labels. As a second step, we derive the presumably correct and incorrect sets by contrasting the decision classes determined by a neighborhood function with the actual decision classes. We extract these sets from either the regions containing strong members or the whole universe, which defines the strict and relaxed versions of our theoretical formalism. These sets allow isolating the instances difficult to handle by machine learning algorithms as they are responsible for inconsistent patterns. The simulations using synthetic and real-world datasets illustrate the advantages of our model compared to rough sets, which is deemed a solid state-of-the-art approach to cope with inconsistency. In particular, it is shown that we can increase the accuracy of selected classifiers up to 36% by weighting the presumably correct and incorrect instances during the training process.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323003412",
    "keywords": [
      "Artificial intelligence",
      "Business",
      "Computer science",
      "Data mining",
      "Decision process",
      "Decision tree",
      "Machine learning",
      "Mathematics",
      "Medicine",
      "Process management",
      "Radiology",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Nápoles",
        "given_name": "Gonzalo"
      },
      {
        "surname": "Grau",
        "given_name": "Isel"
      },
      {
        "surname": "Jastrzębska",
        "given_name": "Agnieszka"
      },
      {
        "surname": "Salgueiro",
        "given_name": "Yamisleydi"
      }
    ]
  },
  {
    "title": "An incremental facility location clustering with a new hybrid constrained pseudometric",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109520",
    "abstract": "The Euclidean metric, one of the classical similarity measures applied in clustering algorithms, has drawbacks when applied to spatial clustering. The resulting clusters are spherical and similarly sized, and the edges of objects are considerably smoothed. This paper proposes a novel hybrid constrained pseudometric formed by the linear combination of the Euclidean metric and a pseudometric plus penalty. The pseudometric is used in a new deterministic incremental heuristic facility location algorithm (IHFL). Our method generates larger, isotropic, and partially overlapping clusters of different sizes and spatial densities, better adapting to the surface complexity than the classical non-deterministic clustering. Cluster properties are used to derive new features for supervised/unsupervised learning. Possible applications are the classification of point clouds, their simplification, detection, filtering, and extraction of different structural patterns or sampled objects. Experiments were run on point clouds derived from laser scanning and images.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323002200",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Canopy clustering algorithm",
      "Cluster analysis",
      "Computer science",
      "Economics",
      "Euclidean distance",
      "Euclidean geometry",
      "Fuzzy clustering",
      "Geometry",
      "Heuristic",
      "Image (mathematics)",
      "Mathematical optimization",
      "Mathematics",
      "Metric (unit)",
      "Operations management",
      "Pattern recognition (psychology)",
      "Point (geometry)",
      "Point cloud",
      "Similarity (geometry)",
      "Statistics",
      "k-medians clustering"
    ],
    "authors": [
      {
        "surname": "Bayer",
        "given_name": "Tomáš"
      },
      {
        "surname": "Kolingerová",
        "given_name": "Ivana"
      },
      {
        "surname": "Potůčková",
        "given_name": "Markéta"
      },
      {
        "surname": "Čábelka",
        "given_name": "Miroslav"
      },
      {
        "surname": "Štefanová",
        "given_name": "Eva"
      }
    ]
  },
  {
    "title": "Avoiding dominance of speaker features in speech-based depression detection",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.07.016",
    "abstract": "The performance of speech-based depression detectors is limited by the scarcity and imbalance in depression data. We found that depression detectors could be strongly biased toward speaker features when the number of training speakers is insufficient. To address this issue, we propose a speaker-invariant depression detector (SIDD) that minimizes speaker information in the latent space. The SIDD consists of an autoencoder, a depression classifier, and a speaker-embedding projector. By incorporating speaker-embedding vectors into the autoencoder’s latent vectors, speaker information is effectively eliminated for the depression classifier. Experimental results demonstrate significant improvements achieved by minimizing speaker information, and our proposed method generally outperforms previous approaches for depression detection on the DAIC-WOZ dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523002192",
    "keywords": [
      "Artificial intelligence",
      "Autoencoder",
      "Classifier (UML)",
      "Computer science",
      "Deep learning",
      "Detector",
      "Embedding",
      "Pattern recognition (psychology)",
      "Speaker diarisation",
      "Speaker recognition",
      "Speaker verification",
      "Speech recognition",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Zuo",
        "given_name": "Lishi"
      },
      {
        "surname": "Mak",
        "given_name": "Man-Wai"
      }
    ]
  },
  {
    "title": "Towards desirable decision boundary by Moderate-Margin Adversarial Training",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.07.009",
    "abstract": "The previous adversarial training methods tended to use a larger uniform perturbation budget to obtain an inclusive decision boundary, which improved robustness. However, this large uniform perturbation budget will bring an unnecessary increase in the margin along adversarial directions, causing heavy cross-over between natural and adversarial examples. It is not conducive to balancing the trade-off between robustness and natural accuracy. In this paper, we propose a novel adversarial training scheme, namely Moderate-Margin Adversarial Training (MMAT), to achieve a better trade-off. Specifically, we generate finer-grained adversarial examples to mitigate the cross-over between them and natural examples of neighboring classes. Meanwhile, we design a hybrid loss to learn adversarial examples and natural examples respectively to further obtain a moderate decision boundary. Extensive experiments show MMAT achieves high natural accuracy and robustness under both black-box and white-box attacks. Especially, state-of-the-art robustness and natural accuracy are achieved on SVHN.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523002076",
    "keywords": [
      "Adversarial system",
      "Adversary",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer security",
      "Decision boundary",
      "Gene",
      "Machine learning",
      "Margin (machine learning)",
      "Mathematical optimization",
      "Mathematics",
      "Perturbation (astronomy)",
      "Physics",
      "Quantum mechanics",
      "Robustness (evolution)",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Liang",
        "given_name": "Xiaoyu"
      },
      {
        "surname": "Qian",
        "given_name": "Yaguan"
      },
      {
        "surname": "Huang",
        "given_name": "Jianchang"
      },
      {
        "surname": "Ling",
        "given_name": "Xiang"
      },
      {
        "surname": "Wang",
        "given_name": "Bin"
      },
      {
        "surname": "Wu",
        "given_name": "Chunming"
      },
      {
        "surname": "Swaileh",
        "given_name": "Wassim"
      }
    ]
  },
  {
    "title": "Holistic transformer: A joint neural network for trajectory prediction and decision-making of autonomous vehicles",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109592",
    "abstract": "Trajectory prediction and behavioral decision-making are two important tasks for autonomous vehicles that require a good understanding of the environmental context. Notably, behavioral decisions are better made by referring to the outputs of trajectory predictions. However, most current solutions perform these tasks separately. Therefore, this paper proposes a new joint holistic transformer network that combines multiple cues to predict trajectories and make behavioral decisions simultaneously. To better explore the intrinsic relationships among cues, the network uses existing knowledge and adopts three kinds of attention mechanisms: the sparse multi-head type for reducing noise impact, feature selection sparse type for optimally using partial prior knowledge, and multi-head with sigmoid activation type for optimally using posteriori knowledge. Compared with other trajectory prediction models, the proposed model has a better comprehensive performance and good interpretability. Perceptual noise robustness experiments demonstrate that the proposed model has good noise robustness. Thus, simultaneous trajectory prediction and behavioral decision-making combining multiple cues are accomplished, which reduces computational costs and enhances semantic relationships between scenes and agents.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323002935",
    "keywords": [
      "A priori and a posteriori",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Epistemology",
      "Gene",
      "Interpretability",
      "Machine learning",
      "Philosophy",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Hu",
        "given_name": "Hongyu"
      },
      {
        "surname": "Wang",
        "given_name": "Qi"
      },
      {
        "surname": "Zhang",
        "given_name": "Zhengguang"
      },
      {
        "surname": "Li",
        "given_name": "Zhengyi"
      },
      {
        "surname": "Gao",
        "given_name": "Zhenhai"
      }
    ]
  },
  {
    "title": "Local multi-scale feature aggregation network for real-time image dehazing",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109599",
    "abstract": "Haze causes visual degradation and obscures image information, which gravely affects the reliability of computer vision tasks in real-time systems. Leveraging an enormous number of learning parameters as the restoration costs, learning-based methods have gained significant success, but they are runtime intensive or memory inefficient. In this paper, we propose a local multi-scale feature aggregation network, called LMFA-Net, which has a lightweight model structure and can be used for real-time dehazing. By learning the local mapping relationship between the clean value of a haze image at a certain point and its surrounding local region, LMFA-Net can directly restore the final haze-free image. In particular, we adopt a novel multi-scale feature extraction sub-network (M-Net) to extract features from different scales. As a lightweight network, LMFA-Net can achieve fast and efficient dehazing. Extensive experiments demonstrate that our proposed LMFA-Net surpasses previous state-of-the-art lightweight dehazing methods in both quantitatively and qualitatively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032300300X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Feature extraction",
      "Geometry",
      "Haze",
      "Image (mathematics)",
      "Linguistics",
      "Mathematics",
      "Meteorology",
      "Net (polyhedron)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Power (physics)",
      "Quantum mechanics",
      "Reliability (semiconductor)",
      "Scale (ratio)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Yong"
      },
      {
        "surname": "Hou",
        "given_name": "Xiaorong"
      }
    ]
  },
  {
    "title": "CPSS-FAT: A consistent positive sample selection for object detection with full adaptive threshold",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109627",
    "abstract": "Recent CNN-based methods for object detection largely focus on training the backbone of the object detector, neglecting the key part of selecting positive/negative samples. Instead, based on our analyses about the limitations and inconsistency of existing positive sample selection, we propose a novel Consistent Positive Sample Selection (CPSS) method to select the positive samples automatically, which joints Box-IoU and normalized central distance of object-anchor by mutual weighting. Meanwhile, we employ a consistent approach to location quality evaluation for suppressing the false-positive predicted box when inferencing. Furthermore, an auxiliary Full Adaptive Threshold (FAT) post-processing is also adopted according to the objects’ occlusion level to improve the recall ratio. We implement the proposed CPSS-FAT detector using MS COCO 2017 and CityScapes datasets, the comparing results indicate that our approaches are effective and robust to the different objects in an open world. Especially, we achieve 52.2% A P and 43.1% A R S , outperforming most existing detectors.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032300328X",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Computer vision",
      "Detector",
      "Medicine",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Radiology",
      "Sample (material)",
      "Selection (genetic algorithm)",
      "Telecommunications",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Xiaobao"
      },
      {
        "surname": "Wu",
        "given_name": "Junsheng"
      },
      {
        "surname": "He",
        "given_name": "Lang"
      },
      {
        "surname": "Ma",
        "given_name": "Sugang"
      },
      {
        "surname": "Hou",
        "given_name": "Zhiqiang"
      },
      {
        "surname": "Sun",
        "given_name": "Wei"
      }
    ]
  },
  {
    "title": "Evaluating participating methods in image analysis challenges: Lessons from MoNuSAC 2020",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109600",
    "abstract": "Biomedical image analysis competitions often rank the participants based on a single metric that combines assessments of different aspects of the task at hand. While this is useful for declaring a single winner for a competition, it makes it difficult to assess the strengths and weaknesses of participating algorithms. By involving multiple capabilities (detection, segmentation and classification) and releasing the prediction masks provided by several teams, the MoNuSAC 2020 challenge provides an interesting opportunity to look at what information may be lost by using entangled metrics. We analyse the challenge results based on the “Panoptic Quality” (PQ) used by the organizers, as well as on disentangled metrics that assess the detection, classification and segmentation abilities of the algorithms separately. We show that the PQ hides interesting aspects of the results, and that its sensitivity to small changes in the prediction masks makes it hard to interpret these results and to draw useful insights from them. Our results also demonstrate the necessity to have access, as much as possible, to the raw predictions provided by the participating teams so that challenge results can be more easily analysed and thus more useful to the research community.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323003011",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Data mining",
      "Data science",
      "Economics",
      "Electronic engineering",
      "Engineering",
      "Epistemology",
      "Image (mathematics)",
      "Machine learning",
      "Management",
      "Mathematics",
      "Metric (unit)",
      "Operations management",
      "Philosophy",
      "Quality (philosophy)",
      "Rank (graph theory)",
      "Segmentation",
      "Sensitivity (control systems)",
      "Strengths and weaknesses",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Foucart",
        "given_name": "Adrien"
      },
      {
        "surname": "Debeir",
        "given_name": "Olivier"
      },
      {
        "surname": "Decaestecker",
        "given_name": "Christine"
      }
    ]
  },
  {
    "title": "SiamRank: A siamese based visual tracking network with ranking strategy",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109630",
    "abstract": "Visual tracking is one of the most fundamental and active research topics in the field of computer vision with industrial applications. It has to solve 2 core problems, namely classification and state estimation. Most of the existing trackers utilize deep networks to extract the features of the object. Especially, Siamese based approaches have prevailed in tracking tasks, which generate labels for both positive and negative samples. However, these approaches introduce ambiguities and inaccurate semantic information at the same time, which may cause failure in classification. To address this problem, we present SiamRank by adding the sequential information of different samples in one image. We apply the proposed network to 2 backbones (AlexNet and GoogLeNet) to testify its general performance. Extensive experiments have been carried out on 7 popular benchmarks, including OTB100, LaSOT, GOT-10 K, TrackingNet, NFS, UAV123 and VOT2019, and our tracker achieves state-of-the-art results. Specifically, on both large-scale TrackingNet dataset and long-time LaSOT dataset, SiamRank surpasses the previous approaches with a relative gain of 10%, while running at 65 FPS.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032300331X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "BitTorrent tracker",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Eye tracking",
      "Field (mathematics)",
      "Image (mathematics)",
      "Machine learning",
      "Mathematics",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Pedagogy",
      "Physics",
      "Psychology",
      "Pure mathematics",
      "Quantum mechanics",
      "Rank (graph theory)",
      "Ranking (information retrieval)",
      "Scale (ratio)",
      "State (computer science)",
      "Tracking (education)"
    ],
    "authors": [
      {
        "surname": "Meng",
        "given_name": "Feiyu"
      },
      {
        "surname": "Gong",
        "given_name": "Xiaomei"
      },
      {
        "surname": "Zhang",
        "given_name": "Yi"
      }
    ]
  },
  {
    "title": "Tensor train factorization under noisy and incomplete data with automatic rank estimation",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109650",
    "abstract": "As a powerful tool in analyzing multi-dimensional data, tensor train (TT) decomposition shows superior performance compared to other tensor decomposition formats. Existing TT decomposition methods, however, either easily overfit with noise, or require substantial fine-tuning to strike a balance between recovery accuracy and model complexity. To avoid the above shortcomings, this paper treats the TT decomposition in a fully Bayesian perspective, which includes automatic TT rank determination and noise power estimation. Theoretical justification on adopting the Gaussian-product-Gamma priors for inducing sparsity on the slices of the TT cores is provided, thus allowing the model complexity to be automatically determined even when the observed tensor data is noisy and contains many missing values. Furthermore, using the variational inference framework, an effective learning algorithm on the probabilistic model parameters is derived. Simulations on synthetic data demonstrate that the proposed algorithm accurately recovers the underlying TT structure from incomplete noisy observations. Further experiments on image and video data also show its superior performance to other existing TT decomposition algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323003515",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Combinatorics",
      "Computer science",
      "Eigenvalues and eigenvectors",
      "Image (mathematics)",
      "Inference",
      "Machine learning",
      "Mathematics",
      "Matrix decomposition",
      "Noise (video)",
      "Overfitting",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Rank (graph theory)"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Le"
      },
      {
        "surname": "Cheng",
        "given_name": "Lei"
      },
      {
        "surname": "Wong",
        "given_name": "Ngai"
      },
      {
        "surname": "Wu",
        "given_name": "Yik-Chung"
      }
    ]
  },
  {
    "title": "Semantic-guided de-attention with sharpened triplet marginal loss for visual place recognition",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109645",
    "abstract": "Thanks to Earth-level Street View images from Google Maps, a visual image geo-localization can estimate the coarse location of a query image with a visual place recognition process. However, this can get very challenging when non-static objects change with time, severely degrading image retrieval accuracy. We address the problem of city-scale visual place recognition in complex urban environments crowded with non-static clutters. To this end, we first analyze what clutters degrade similarity matching between the query and database images. Second, we design a self-supervised trainable de-attention module that prevents the network from focusing on non-static objects in an input image. In addition, we propose a novel triplet marginal loss called sharpened triplet marginal loss to make feature descriptors more discriminative. Lastly, due to the lack of geo-tagged public datasets with a high density of non-static objects, we propose a clutter augmentation method to evaluate our approach. The experimental results show that our model has notably improved over the existing attention methods in geo-localization tasks on the public benchmark datasets and on their augmented versions with high population and traffic. Our code is available at https://github.com/ccsmm78/deattention_with_stml_for_vpr.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323003461",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Clutter",
      "Code (set theory)",
      "Computer science",
      "Computer vision",
      "Discriminative model",
      "Feature (linguistics)",
      "Feature extraction",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Image retrieval",
      "Linguistics",
      "Matching (statistics)",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Process (computing)",
      "Programming language",
      "Radar",
      "Set (abstract data type)",
      "Similarity (geometry)",
      "Statistics",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Choi",
        "given_name": "Seung-Min"
      },
      {
        "surname": "Lee",
        "given_name": "Seung-Ik"
      },
      {
        "surname": "Lee",
        "given_name": "Jae-Yeong"
      },
      {
        "surname": "Kweon",
        "given_name": "In So"
      }
    ]
  },
  {
    "title": "Margin-aware rectified augmentation for long-tailed recognition",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109608",
    "abstract": "The long-tailed data distribution is prevalent in real world and it poses great challenge on deep neural network training. In this paper, we propose Margin-aware Rectified Augmentation (MRA) to tackle this problem. Specifically, the MRA consists of two parts. From the data perspective, we analyze that data imbalance will cause the decision boundary be biased, and we propose a novel Margin-aware Rectified mixup (MR-mixup) that adaptively rectifies the biased decision boundary. Furthermore, from the model perspective, we analyze that the imbalance will also lead to consistent ‘gradient suppression’ on minority class logits. Then we propose Reweighted Mutual Learning (RML) that provides extra ‘soft target’ as supervision signal and augments the ‘encouraging gradients’ on the minority classes. We conduct extensive experiments on benchmark datasets CIFAR-LT, ImageNet-LT and iNaturalist18. The results demonstrate that the proposed MRA not only achieves state-of-the-art performance, but also yields a better-calibrated prediction.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323003096",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Boundary (topology)",
      "Class (philosophy)",
      "Computer science",
      "Decision boundary",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Margin (machine learning)",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Perspective (graphical)",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Xiang",
        "given_name": "Liuyu"
      },
      {
        "surname": "Han",
        "given_name": "Jungong"
      },
      {
        "surname": "Ding",
        "given_name": "Guiguang"
      }
    ]
  },
  {
    "title": "Multi-hypothesis representation learning for transformer-based 3D human pose estimation",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109631",
    "abstract": "Despite significant progress, estimating 3D human poses from monocular videos remains a challenging task due to depth ambiguity and self-occlusion. Most existing works attempt to solve both issues by exploiting spatial and temporal relationships. However, those works ignore the fact that it is an inverse problem where multiple feasible solutions (i.e., hypotheses) exist. To relieve this limitation, we propose a Multi-Hypothesis Transformer that learns spatio-temporal representations of multiple plausible pose hypotheses. In order to effectively model multi-hypothesis dependencies and build strong relationships across hypothesis features, we introduce a one-to-many-to-one three-stage framework: (i) Generate multiple initial hypothesis representations; (ii) Model self-hypothesis communication, merge multiple hypotheses into a single converged representation and then partition it into several diverged hypotheses; (iii) Learn cross-hypothesis communication and aggregate the multi-hypothesis features to synthesize the final 3D pose. Through the above processes, the final representation is enhanced and the synthesized pose is much more accurate. Extensive experiments show that the proposed method achieves state-of-the-art results on two challenging datasets: Human3.6M and MPI-INF-3DHP. The code and models are available at https://github.com/Vegetebird/MHFormer.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323003321",
    "keywords": [
      "Ambiguity",
      "Artificial intelligence",
      "Computer science",
      "Information retrieval",
      "Law",
      "Machine learning",
      "Merge (version control)",
      "Pattern recognition (psychology)",
      "Physics",
      "Political science",
      "Politics",
      "Programming language",
      "Quantum mechanics",
      "Representation (politics)",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Wenhao"
      },
      {
        "surname": "Liu",
        "given_name": "Hong"
      },
      {
        "surname": "Tang",
        "given_name": "Hao"
      },
      {
        "surname": "Wang",
        "given_name": "Pichao"
      }
    ]
  },
  {
    "title": "Slide deep reinforcement learning networks: Application for left ventricle segmentation",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109667",
    "abstract": "Automatic segmentation of the left ventricle (LV) in four-chamber view images is critical for computer-aided cardiac disease diagnosis. The complex structure of the cardiac image and the encoder-decoder networks may cause coarse segmentation results. High-accuracy LV segmentation is still a challenge with existing automatic LV segmentation methods. In this paper, we propose a slide deep reinforcement learning segmentation network for pixelwise LV segmentation. The main architecture of the slide reinforcement learning networks consists of a slider item combined state, a group of morphology transforming actions and an agent network. The specifically designed reinforcement learning state comprises an image item and a slider item, which contains both original image information and network act information. The reinforcement learning actions proposed in this paper enable accurate and fast formulation of the binary segment result for each frame by controlling the length and location of the slider. Additionally, the confidence branch proposed in our experiment provides a continuous frame series environment, and the identification algorithm avoids losing the segmentation target. The segmentation result reveals that the proposed method outperforms FCN, SegNet, U-Net and TransUnet. The IoU improved by 23.01 % , 15.4 % , 11.24 % and 6.9 % . Additionally, we demonstrate how the proposed method can be used as a semisupervised method, which is more convenient for the image annotation process.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323003680",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Encoder",
      "Frame (networking)",
      "Image segmentation",
      "Operating system",
      "Pattern recognition (psychology)",
      "Process (computing)",
      "Reinforcement learning",
      "Segmentation",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Wanjun"
      },
      {
        "surname": "Ding",
        "given_name": "Xiaohua"
      },
      {
        "surname": "Liu",
        "given_name": "Yang"
      },
      {
        "surname": "Qiao",
        "given_name": "Baojun"
      }
    ]
  },
  {
    "title": "An ablation study on part-based face analysis using a Multi-input Convolutional Neural Network and Semantic Segmentation",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.07.010",
    "abstract": "Face-based recognition methods usually need the image of the whole face to perform, but in some situations, only a fraction of the face is visible, for example wearing sunglasses or recently with the COVID pandemic we had to wear facial masks. In this work, we propose a network architecture made up of four deep learning streams that process each one a different face element, namely: mouth, nose, eyes, and eyebrows, followed by a feature merge layer. Therefore, the face is segmented into the part of interest by means of ROI masks to keep the same input size for the four network streams. The aim is to assess the capacity of different combinations of face elements in recognizing the subject. The experiments were carried out on the Masked Face Recognition Database (M2FRED) which includes videos of 46 participants. The obtained results are 96% of recognition accuracy considering the four face elements; and 92%, 87%, and 63% of accuracy for the best combination of three, two, and one face elements respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523002064",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Face (sociological concept)",
      "Facial recognition system",
      "Feature extraction",
      "Information retrieval",
      "Merge (version control)",
      "Pattern recognition (psychology)",
      "Segmentation",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Abate",
        "given_name": "Andrea F."
      },
      {
        "surname": "Cimmino",
        "given_name": "Lucia"
      },
      {
        "surname": "Lorenzo-Navarro",
        "given_name": "Javier"
      }
    ]
  },
  {
    "title": "CSLT-AK: Convolutional-embedded transformer with an action tokenizer and keypoint emphasizer for sign language translation",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.08.009",
    "abstract": "Sign language translation is a complex task that involves generating spoken-language sentences from sign language (SL) videos, considering the signer's manual and nonmanual movements. We observed the following issues with existing SL translation (SLT) methods and datasets for improving performance. First, every SL video frame does not have gloss notation. Second, nonmanual components can be easily overlooked despite their importance because they occur in small areas of the image. Third, recent SLT models, based on the transformer, have numerous parameters and struggle to capture the local context of SL images comprehensively. To address these problems, we propose an action tokenizer that divides SL videos into semantic units. In addition, we design a keypoint emphasizer and convolutional-embedded SL transformer (CSLT) to understand noticeable manual and subtle nonmanual features effectively. By applying the proposed modules to Sign2 (Gloss+Text), we introduce CSLT with an action tokenizer and keypoint emphasizer (CSLT-AK), a simple yet efficient and effective SLT model based on domain knowledge. The experimental results on the RWTH-PHOENIX-Weather 2014 T reveal that CSLT-AK surpasses the baseline regarding performance and parameter reduction and demonstrates competitive performance without the need for regularization methods compared to other state-of-the-art models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523002283",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Linguistics",
      "Natural language processing",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Sign language",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Kim",
        "given_name": "Jungeun"
      },
      {
        "surname": "Kim",
        "given_name": "Ha Young"
      }
    ]
  },
  {
    "title": "Neural networks with divisive normalization for image segmentation",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.07.017",
    "abstract": "One of the key problems in computer vision is adaptation: models are too rigid to follow the variability of the inputs. The canonical computation that explains adaptation in sensory neuroscience is divisive normalization, and it has appealing effects on image manifolds. In this work we show that including divisive normalization in current deep networks makes them more invariant to non-informative changes in the images. In particular, we illustrate this concept in U-Net architectures for image segmentation. Experiments show that the inclusion of divisive normalization in the U-Net architecture leads to better segmentation results with respect to the conventional U-Net. The gain increases steadily when dealing with images acquired in bad weather conditions (from 3% of IoU increase in regular weather up to 20% on high fog). In addition to the positive results on the Cityscapes and Foggy Cityscapes datasets, we explain these advantages through the visualization of the responses: the equalization induced by the divisive normalization leads to more invariant features to local changes in contrast and illumination.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523002209",
    "keywords": [
      "Algorithm",
      "Anthropology",
      "Artificial intelligence",
      "Computation",
      "Computer science",
      "Computer vision",
      "Image segmentation",
      "Invariant (physics)",
      "Mathematical physics",
      "Mathematics",
      "Normalization (sociology)",
      "Pattern recognition (psychology)",
      "Segmentation",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Hernández-Cámara",
        "given_name": "Pablo"
      },
      {
        "surname": "Vila-Tomás",
        "given_name": "Jorge"
      },
      {
        "surname": "Laparra",
        "given_name": "Valero"
      },
      {
        "surname": "Malo",
        "given_name": "Jesús"
      }
    ]
  },
  {
    "title": "Orthonormal product quantization network for scalable face image retrieval",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109671",
    "abstract": "Existing deep quantization methods provided an efficient solution for large-scale image retrieval. However, the significant intra-class variations, like pose, illumination, and expressions in face images, still pose a challenge. In light of this, face image retrieval requires sufficiently powerful learning metrics, which are absent in current deep quantization works. Moreover, to tackle the growing unseen identities in the query stage, face image retrieval drives more demands regarding model generalization and scalability than general image retrieval tasks. This paper integrates product quantization with orthonormal constraints into an end-to-end deep learning framework to effectively retrieve face images. Specifically, we propose a novel scheme that uses predefined orthonormal vectors as codewords to enhance the quantization informativeness and reduce codewords’ redundancy. A tailored loss function maximizes discriminability among identities in each quantization subspace for both the quantized and original features. An entropy-based regularization term is imposed to reduce the quantization error. Experiments are conducted on four commonly-used face datasets under both seen and unseen identity retrieval settings. Our method outperforms all the compared state-of-the-art under both settings. The proposed orthonormal codewords consistently boost both models’ standard retrieval performance and generalization ability, demonstrating the superiority of our method for scalable face image retrieval.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323003722",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Database",
      "Facial recognition system",
      "Image (mathematics)",
      "Image retrieval",
      "Orthonormal basis",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantization (signal processing)",
      "Quantum mechanics",
      "Scalability"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Ming"
      },
      {
        "surname": "Zhe",
        "given_name": "Xuefei"
      },
      {
        "surname": "Yan",
        "given_name": "Hong"
      }
    ]
  },
  {
    "title": "Crowd counting from single images using recursive multi-pathway zooming and foreground enhancement",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109585",
    "abstract": "Crowd counting is a challenging task due to many challenges such as scale variations and noisy background. To handle these challenges, we propose a novel framework named Multi-Pathway Zooming Network (MZNet) in this paper. The proposed framework recursively optimizes multi-scale features using multiple zooming pathways and progressively enhances the foreground information to improve crowd counting performance. Each zooming pathway comprises two zooming directions, zooming in and zooming out. Convolutional features at different resolutions are propagated to optimize the context information at each specific level. By sequentially integrating and interacting multi-observation information, the optimized features are powerful in handling the scale variation issue, and thus the crowd counting performance can be enhanced. To address the noisy background in many scenarios, we also introduce a new scheme to enhance the foreground information by incorporating a masked input image into the network, which is formed by a mask that element-wise multiplies with the original image. Finally, the context information, incorporated with an output density map, is recursively finetuned in our network to boost the counting performance. Extensive experiments evaluated on challenging benchmark datasets show competitive performances for both crowded and sparse scenarios.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323002868",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Convolutional neural network",
      "Economics",
      "Engineering",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Lens (geology)",
      "Management",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Petroleum engineering",
      "Physics",
      "Quantum mechanics",
      "Scale (ratio)",
      "Task (project management)",
      "Zoom"
    ],
    "authors": [
      {
        "surname": "Ma",
        "given_name": "Junjie"
      },
      {
        "surname": "Dai",
        "given_name": "Yaping"
      },
      {
        "surname": "Jia",
        "given_name": "Zhiyang"
      },
      {
        "surname": "Sun",
        "given_name": "Fuchun"
      },
      {
        "surname": "Tan",
        "given_name": "Yap-Peng"
      },
      {
        "surname": "Liu",
        "given_name": "Jun"
      }
    ]
  },
  {
    "title": "Few-shot node classification on attributed networks based on deep metric learning for Cyber–Physical–Social Services",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.08.003",
    "abstract": "In Cyber–Physical–Social Systems (CPSS), the interactions among various entities form complex graphs. Many tasks can be formulated as instances of node classification. Node classification on graphs has attracted increasing research interest. However, the performance of existing graph neural networks for few-shot node classification has not achieved satisfactory results due to the limitation of the number of labeled instances. Therefore, we propose a graph-weighted prototype scaling network (GWPSN) based on deep metric learning for addressing the graph few-shot node classification problem. Specifically, we first extract node representations for the attributed graph via the simplifying graph convolutional network. At the same time, learning the importance of each node in the attributed graph is used to aggregate class prototypes. Finally, the class of the test node can be predicted by comparing the scaled metric distance between the test node and the class prototype. Experiments indicate that GWPSN can achieve superior performance on three real-world datasets and thus can provide enhanced few-shot classification services for CPSS.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523002234",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Convolutional neural network",
      "Data mining",
      "Deep learning",
      "Economics",
      "Engineering",
      "Geometry",
      "Graph",
      "Machine learning",
      "Mathematics",
      "Metric (unit)",
      "Node (physics)",
      "Operations management",
      "Scaling",
      "Structural engineering",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Guangming"
      },
      {
        "surname": "Zhao",
        "given_name": "Yaliang"
      },
      {
        "surname": "Wang",
        "given_name": "Jinke"
      }
    ]
  },
  {
    "title": "Augmented bilinear network for incremental multi-stock time-series classification",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109604",
    "abstract": "Deep Learning models have become dominant in tackling financial time-series analysis problems, overturning conventional machine learning and statistical methods. Most often, a model trained for one market or security cannot be directly applied to another market or security due to differences inherent in the market conditions. In addition, as the market evolves over time, it is necessary to update the existing models or train new ones when new data is made available. This scenario, which is inherent in most financial forecasting applications, naturally raises the following research question: How to efficiently adapt a pre-trained model to a new set of data while retaining performance on the old data, especially when the old data is not accessible? In this paper, we propose a method to efficiently retain the knowledge available in a neural network pre-trained on a set of securities and adapt it to achieve high performance in new ones. In our method, the prior knowledge encoded in a pre-trained neural network is maintained by keeping existing connections fixed, and this knowledge is adjusted for the new securities by a set of augmented connections, which are optimized using the new data. The auxiliary connections are constrained to be of low rank. This not only allows us to rapidly optimize for the new task but also reduces the storage and run-time complexity during the deployment phase. The efficiency of our approach is empirically validated in the stock mid-price movement prediction problem using a large-scale limit order book dataset. Experimental results show that our approach enhances prediction performance as well as reduces the overall number of network parameters.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323003059",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer science",
      "Data mining",
      "Economics",
      "Horse",
      "Machine learning",
      "Management",
      "Operating system",
      "Paleontology",
      "Programming language",
      "Set (abstract data type)",
      "Stock market",
      "Stock market prediction",
      "Task (project management)",
      "Time series",
      "Time to market"
    ],
    "authors": [
      {
        "surname": "Shabani",
        "given_name": "Mostafa"
      },
      {
        "surname": "Tran",
        "given_name": "Dat Thanh"
      },
      {
        "surname": "Kanniainen",
        "given_name": "Juho"
      },
      {
        "surname": "Iosifidis",
        "given_name": "Alexandros"
      }
    ]
  },
  {
    "title": "Few-shot classification with task-adaptive semantic feature learning",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109594",
    "abstract": "Few-shot classification aims to learn a classifier that categorizes objects of unseen classes with limited samples. One general approach is to mine as much information as possible from limited samples. This can be achieved by incorporating data from multiple modalities. However, existing multi-modality methods only use additional modality in support samples while adhering to a single modal in query samples. Such approach could lead to information imbalance between support and query samples, which confounds model generalization from support to query samples. Towards this problem, we propose a task-adaptive semantic feature learning mechanism to incorporate semantic features for both support and query samples. The semantic feature learner is trained episodic-wisely by regressing from the feature vectors of the support samples. It is utilized to predict semantic features for the query samples. Such method maintains a consistent training scheme between support and query samples and enables direct model transfer from support to query data, which significantly improves model generalization. We conduct extensive experiments on four benchmarks in both inductive and transductive settings. Results show that the proposed TasNet outperforms state-of-the-art methods with an improvement of 1% to 5% in classification accuracy, demonstrating the superiority of our method. The exhaustive ablation studies further validate the effectiveness of our framework. The code is available at: https://github.com/pmhDL/TasNet",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323002959",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Feature (linguistics)",
      "Linguistics",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Semantic feature",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Pan",
        "given_name": "Mei-Hong"
      },
      {
        "surname": "Xin",
        "given_name": "Hong-Yi"
      },
      {
        "surname": "Xia",
        "given_name": "Chun-Qiu"
      },
      {
        "surname": "Shen",
        "given_name": "Hong-Bin"
      }
    ]
  },
  {
    "title": "Corrigendum to “FETNet: Feature Erasing and Transferring Network for Scene Text Removal”: Pattern Recognition Volume 140 (2023) 109531",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109581",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323002820",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Volume (thermodynamics)"
    ],
    "authors": [
      {
        "surname": "Lyu",
        "given_name": "Guangtao"
      },
      {
        "surname": "Liu",
        "given_name": "Kun"
      },
      {
        "surname": "Zhu",
        "given_name": "Anna"
      },
      {
        "surname": "Uchida",
        "given_name": "Seiichi"
      },
      {
        "surname": "Iwana",
        "given_name": "Brian Kenji"
      }
    ]
  },
  {
    "title": "FontTransformer: Few-shot high-resolution Chinese glyph image synthesis via stacked transformers",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109593",
    "abstract": "Automatic generation of high-quality Chinese fonts from a few online training samples is a challenging task, especially when the amount of samples is very small. Existing few-shot font generation methods can only synthesize low-resolution glyph images that often possess incorrect topological structures or/and incomplete strokes. To address the problem, this paper proposes FontTransformer, a novel few-shot learning model, for high-resolution Chinese glyph image synthesis by using stacked Transformers. The key idea is to apply the parallel Transformer to avoid the accumulation of prediction errors and utilize the serial Transformer to enhance the quality of synthesized strokes. Meanwhile, we also design a novel encoding scheme to feed more glyph information and prior knowledge to our model, which further enables the generation of high-resolution and visually-pleasing glyph images. Both qualitative and quantitative experimental results demonstrate the superiority of our method compared to other existing approaches in few-shot Chinese font synthesis task.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323002947",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Glyph (data visualization)",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Transformer",
      "Visualization",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Yitian"
      },
      {
        "surname": "Lian",
        "given_name": "Zhouhui"
      }
    ]
  },
  {
    "title": "DyGAT: Dynamic stroke classification of online handwritten documents and sketches",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109564",
    "abstract": "Online handwriting is widely used in human-machine interface, education, office automation, and so on. Stroke classification for online handwritten documents and sketches aims to divide strokes into several semantic categories and is a necessary step for document recognition and understanding. Previous methods are essentially static in that they have to wait for the user to finish the whole sketch before making prediction. However, in practice, the more user-friendly way is to make real-time prediction as the user is writing. In this paper, we introduce Dynamic Graph ATtention network (DyGAT) to solve the dynamic stroke classification problem. The core of our method is to formalize a document/sketch into a multi-feature graph, in which nodes represent strokes, edges represent the relationships between strokes, and multiple nodes are applied to one stroke to control the information flow. The proposed method is general and is applicable to online handwritten data of many types. We conduct experiments on popular public datasets to perform sketch semantic segmentation, document layout analysis and diagram recognition, and experimental results show competitive performance. Particularly, the proposed method achieves stroke classification accuracies which are only slightly lower than those of static classification.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323002649",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Character recognition",
      "Computer science",
      "Control flow graph",
      "Feature (linguistics)",
      "Gesture",
      "Gesture recognition",
      "Graph",
      "Handwriting",
      "Image (mathematics)",
      "Information retrieval",
      "Intelligent character recognition",
      "Linguistics",
      "Machine learning",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Segmentation",
      "Sketch",
      "Sketch recognition",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Yu-Ting"
      },
      {
        "surname": "Zhang",
        "given_name": "Yan-Ming"
      },
      {
        "surname": "Yun",
        "given_name": "Xiao-Long"
      },
      {
        "surname": "Yin",
        "given_name": "Fei"
      },
      {
        "surname": "Liu",
        "given_name": "Cheng-Lin"
      }
    ]
  },
  {
    "title": "Multi-view prototype-based disambiguation for partial label learning",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109625",
    "abstract": "In this work, we study the multi-view partial label learning (MVPLL) problem, where each instance is depicted by different view features and associated with a set of candidate labels, among which a true label exists but is inaccessible in the training phase. Most existing PLL methods only consider single-view case, which learn view classifier independently and neglect the view correlations, thus can not be applied to solve MVPLL problem. Due to the non-deep framework, traditional MVPLL approach is weak in the representation ability, so its performance is still to be improved. To solve the MVPLL problem, a deep multi-view prototype-based disambiguation approach is proposed in this paper. Specifically, we innovatively employ the deep neural network for multi-view ambiguously-labeled image classification to enhance the representation ability, which makes use of the information fusion between multiple views. To improve the discriminative ability, we propose multi-view prototype-based label disambiguation algorithm. On theoretical aspect, an estimation error bound for view-risk estimator is established, which is shown to be larger than that for fuse-risk estimator. Experiments demonstrate the superiorities of our proposed method in terms of the prediction accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323003266",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Classifier (UML)",
      "Computer science",
      "Discriminative model",
      "Electrical engineering",
      "Engineering",
      "Estimator",
      "Fuse (electrical)",
      "Image (mathematics)",
      "Law",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Programming language",
      "Representation (politics)",
      "Set (abstract data type)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Sun",
        "given_name": "Shiding"
      },
      {
        "surname": "Yu",
        "given_name": "Xiaotong"
      },
      {
        "surname": "Tian",
        "given_name": "Yingjie"
      }
    ]
  },
  {
    "title": "Tensor completion via convolutional sparse coding with small samples-based training",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109624",
    "abstract": "Tensor data often suffer from missing value problems due to the complex high-dimensional structure while acquiring them. To complete the missing information, lots of Low-Rank Tensor Completion (LRTC) methods have been proposed, most of which depend on the low-rank property of tensor data. In this way, the low-rank component of the original data could be recovered roughly. However, the shortcoming is that the detailed information can not be fully restored, no matter the Sum of the Nuclear Norm (SNN) nor the Tensor Nuclear Norm (TNN) based methods. On the contrary, in the field of signal processing, Convolutional Sparse Coding (CSC) can provide a good representation of the high-frequency component of the image, which is generally associated with the detail component of the data. To this end, we propose two novel methods, LRTC-CSC-I and LRTC-CSC-II, which adopt CSC as a supplementary regularization for LRTC to capture the high-frequency components. Therefore, the LRTC-CSC methods can not only solve the missing value problem but also recover the details. Moreover, the regularizer CSC can be trained with small samples due to the sparsity characteristic. Extensive experiments show the effectiveness of LRTC-CSC methods, and quantitative evaluation indicates that the performance of our models are superior to state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323003254",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Coding (social sciences)",
      "Computer science",
      "Data mining",
      "Eigenvalues and eigenvectors",
      "Gaussian",
      "Machine learning",
      "Mathematics",
      "Matrix completion",
      "Matrix norm",
      "Missing data",
      "Neural coding",
      "Pattern recognition (psychology)",
      "Physics",
      "Pure mathematics",
      "Quantum mechanics",
      "Regularization (linguistics)",
      "Statistics",
      "Tensor (intrinsic definition)"
    ],
    "authors": [
      {
        "surname": "Liao",
        "given_name": "Tianchi"
      },
      {
        "surname": "Wu",
        "given_name": "Zhebin"
      },
      {
        "surname": "Chen",
        "given_name": "Chuan"
      },
      {
        "surname": "Zheng",
        "given_name": "Zibin"
      },
      {
        "surname": "Zhang",
        "given_name": "Xiongjun"
      }
    ]
  },
  {
    "title": "Multi-level progressive transfer learning for cervical cancer dose prediction",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109606",
    "abstract": "Recently, deep learning has accomplished the automation of radiation therapy planning, enhancing its quality and efficiency. However, such progress comes at the cost of a large amount of clinical data. For some low-incidence cancers, i.e., cervical cancer, with limited available data, current data-hungry deep models fail to achieve satisfactory performance. To address this, in this paper, considering that cervical cancer and rectum cancer share the same scanning area and organs at risk (OARs), we resort to transfer learning to transfer the knowledge acquired from rectum cancer (source domain) to cervical cancer (target domain) to perform dose map prediction task. To overcome the possible negative transferring problem, we design a two-phase paradigm to progressively transfer knowledge. In the first phase, we aggregate the data of the two domains by linear interpolation and pre-train an aggregated network with the aggregated data to perceive the target dose distribution beforehand. In the second phase, we elaborately design two modules, i.e., a Feature-level Transfer (FT) Module, and an Image-level Transfer (IT) Module, to selectively transfer knowledge in multi-level. Specifically, the FT module aims to preserve those filters that are more helpful while the IT module tries to highlight those samples with more target-specific knowledge. Extensive experiments proclaim the exemplary performance of our proposed method compared with other state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323003072",
    "keywords": [
      "Artificial intelligence",
      "Automation",
      "Cancer",
      "Cervical cancer",
      "Computer science",
      "Data mining",
      "Deep learning",
      "Domain (mathematical analysis)",
      "Engineering",
      "Feature (linguistics)",
      "Image (mathematics)",
      "Internal medicine",
      "Interpolation (computer graphics)",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Mechanical engineering",
      "Medicine",
      "Parallel computing",
      "Philosophy",
      "Transfer (computing)",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Wen",
        "given_name": "Lu"
      },
      {
        "surname": "Xiao",
        "given_name": "Jianghong"
      },
      {
        "surname": "Zeng",
        "given_name": "Jie"
      },
      {
        "surname": "Zu",
        "given_name": "Chen"
      },
      {
        "surname": "Wu",
        "given_name": "Xi"
      },
      {
        "surname": "Zhou",
        "given_name": "Jiliu"
      },
      {
        "surname": "Peng",
        "given_name": "Xingchen"
      },
      {
        "surname": "Wang",
        "given_name": "Yan"
      }
    ]
  },
  {
    "title": "Learning cross-domain semantic-visual relationships for transductive zero-shot learning",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109591",
    "abstract": "Zero-Shot Learning (ZSL) learns models for recognizing new classes. One of the main challenges in ZSL is the domain discrepancy caused by the category inconsistency between training and testing data. Domain adaptation is the most intuitive way to address this challenge. However, existing domain adaptation techniques cannot be directly applied into ZSL due to the disjoint label space between source and target domains. This work proposes the Transferrable Semantic-Visual Relation (TSVR) approach towards transductive ZSL. TSVR redefines image recognition as predicting the similarity/dissimilarity labels for semantic-visual fusions consisting of class attributes and visual features. After the above transformation, the source and target domains can have the same label space, which hence enables to quantify domain discrepancy. For the redefined problem, the number of similar semantic-visual pairs is significantly smaller than that of dissimilar ones. To this end, we further propose to use Domain-Specific Batch Normalization to align the domain discrepancy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323002923",
    "keywords": [
      "Anthropology",
      "Artificial intelligence",
      "Biochemistry",
      "Biology",
      "Chemistry",
      "Class (philosophy)",
      "Classifier (UML)",
      "Combinatorics",
      "Computer science",
      "Disjoint sets",
      "Domain (mathematical analysis)",
      "Domain adaptation",
      "Gene",
      "Image (mathematics)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Neuroscience",
      "Normalization (sociology)",
      "Pattern recognition (psychology)",
      "Perception",
      "Similarity (geometry)",
      "Sociology",
      "Transformation (genetics)",
      "Visual space"
    ],
    "authors": [
      {
        "surname": "Lv",
        "given_name": "Fengmao"
      },
      {
        "surname": "Zhang",
        "given_name": "Jianyang"
      },
      {
        "surname": "Yang",
        "given_name": "Guowu"
      },
      {
        "surname": "Feng",
        "given_name": "Lei"
      },
      {
        "surname": "Yu",
        "given_name": "Yufeng"
      },
      {
        "surname": "Duan",
        "given_name": "Lixin"
      }
    ]
  },
  {
    "title": "CGFNet: 3D Convolution Guided and Multi-scale Volume Fusion Network for fast and robust stereo matching",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.07.012",
    "abstract": "Nowadays, although significant progress has been made by convolutional neural network, it is still difficult to realize accurate and robust stereo matching in real time. In this article, we study how to achieve more accurate and robust disparity estimation based on real-time requirement. For this reason, a Multi-scale Volume Fusion (MVF) module was proposed and embedded to improve the matching accuracy. To achieve real-time performance, an innovative way to use 3D convolution is proposed. The 3D convolution is used during training for guidance and supervision, making the inference lightweight. Based on these two structures, we designed an end-to-end stereo matching method called 3D Convolution Guided and Multi-scale Cost Volume Fusion Network (CGFNet). Experimental results showed that our CGFNet has better generalization performance on cross-domain datasets, which achieves more accurate disparity estimation without additional fine tuning process in challenging regions. On KITTI benchmark, CGFNet reached D1-all=1.98% with substantial improvement among the State-Of-The-Art (SOTA) real-time models and runs a pair of images within 38 ms (26 fps). The results are notable when considering both matching accuracy and real-time performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552300209X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer vision",
      "Convolution (computer science)",
      "Fusion",
      "Linguistics",
      "Matching (statistics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Scale (ratio)",
      "Statistics",
      "Volume (thermodynamics)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Qingyu"
      },
      {
        "surname": "Xing",
        "given_name": "Hao"
      },
      {
        "surname": "Ying",
        "given_name": "Yibin"
      },
      {
        "surname": "Zhou",
        "given_name": "Mingchuan"
      }
    ]
  },
  {
    "title": "Hypercomplex context guided interaction modeling for scene graph generation",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109634",
    "abstract": "Intuitively, humans can consciously and subjectively attend to the interactions between objects, and thus infer reasonable visual relations. However, mainstream approaches of Scene Graph Generation (SGG) strive to alleviate the long-tailed distribution problem with various complicated re-weighting strategies, where a simple concatenation of the refined object features is treated as the final representation of visual relations. In spite of their remarkable progress, such an operation overlooks the importance of interaction on relation recognition. To tackle the problem, this work devises a hyperComplex- Context guided Interaction Modeling (CCIM for short) plug-in, which can be successfully assimilated by the existing methods for performance improvement. Specifically, we first extract the contextual relation feature determined by the constraint r e l a t i o n ≈ u n i o n ( h e a d , t a i l ) − h e a d o b j e c t − t a i l o b j e c t . Then, we encode the features of relations and objects into hypercomplex space, with three imaginary components, to learn more expressive representations for SGG. Next, guided by the context, we can capture the interaction between a head or tail object and their relation through the Hamilton product. We further reinforce the interaction between enhanced hypercomplex-valued representations of the two entities with Quaternion inner product. At last, the concatenation of all components from the learned hypercomplex feature is adopted as our final relation representation. Extensive experiments on the popular benchmark Visual Genome in various existing approaches demonstrate the effectiveness and generalization of our proposed model-agnostic method under comprehensive evaluation metrics.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323003357",
    "keywords": [
      "Arithmetic",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Concatenation (mathematics)",
      "Context (archaeology)",
      "Data mining",
      "Feature (linguistics)",
      "Geometry",
      "Graph",
      "Hypercomplex number",
      "Law",
      "Linguistics",
      "Mathematics",
      "Paleontology",
      "Philosophy",
      "Political science",
      "Politics",
      "Quaternion",
      "Relation (database)",
      "Rendering (computer graphics)",
      "Representation (politics)",
      "Scene graph",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Zheng"
      },
      {
        "surname": "Xu",
        "given_name": "Xing"
      },
      {
        "surname": "Luo",
        "given_name": "Yadan"
      },
      {
        "surname": "Wang",
        "given_name": "Guoqing"
      },
      {
        "surname": "Yang",
        "given_name": "Yang"
      }
    ]
  },
  {
    "title": "Action class relation detection and classification across multiple video datasets",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.08.002",
    "abstract": "The Meta Video Dataset (MetaVD) provides annotated relations between action classes in major datasets for human action recognition in videos. Although these annotated relations enable dataset augmentation, it is only applicable to those covered by MetaVD. For an external dataset to enjoy the same benefit, the relations between its action classes and those in MetaVD need to be determined. To address this issue, we consider two new machine learning tasks: action class relation detection and classification. We propose a unified model to predict relations between action classes, using language and visual information associated with classes. Experimental results show that (i) pre-trained recent neural network models for texts and videos contribute to high predictive performance, (ii) the relation prediction based on action label texts is more accurate than based on videos, and (iii) a blending approach that combines predictions by both modalities can further improve the predictive performance in some cases.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523002222",
    "keywords": [
      "Action (physics)",
      "Action recognition",
      "Artificial intelligence",
      "Artificial neural network",
      "Class (philosophy)",
      "Computer science",
      "Data mining",
      "Machine learning",
      "Modalities",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Relation (database)",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Yoshikawa",
        "given_name": "Yuya"
      },
      {
        "surname": "Shigeto",
        "given_name": "Yutaro"
      },
      {
        "surname": "Shimbo",
        "given_name": "Masashi"
      },
      {
        "surname": "Takeuchi",
        "given_name": "Akikazu"
      }
    ]
  },
  {
    "title": "Unsupervised image segmentation with robust virtual class contrast",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.07.004",
    "abstract": "Unsupervised Image Segmentation (UIS) is a challenging problem in computer vision that aims to classify pixels in an image into different semantic classes without using any labels. Among the various UIS methods, MaskContrast performs well when dealing with complex objects containing lots of detail. It utilizes prior knowledge from a pretrained saliency detection model to identify foreground pixels and leverages contrastive learning to learn the representations of those foreground pixels. However, this method is not designed to handle situations where foreground pixels belong to multiple semantic classes. To mitigate this problem, we propose a novel method for UIS called Robust Virtual Class Contrast (RVCC), which characterizes foreground pixels based on their “virtual classes” rather than just saliency masks. A virtual class can be viewed as the representation of some unknown object in the training data. Our method employs K-means clustering to find the pseudo virtual class label for every foreground pixel at the start of each training epoch. These pseudo-labels are then used to guide the learning of pixel representations. Additionally, to avoid the overconfidence of pseudo-label prediction, RVCC incorporates an additional regularization term that encourages consistency between the predictions under weak and strong augmentations. Our experiments demonstrate that RVCC outperforms existing baselines on the Pascal VOC2012 and MSRCv2 datasets, showcasing its capability for the UIS problem.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523002015",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Computer vision",
      "Contrast (vision)",
      "Image (mathematics)",
      "Image segmentation",
      "Pattern recognition (psychology)",
      "Scale-space segmentation",
      "Segmentation",
      "Segmentation-based object categorization"
    ],
    "authors": [
      {
        "surname": "Nguyen",
        "given_name": "Khang"
      },
      {
        "surname": "Do",
        "given_name": "Kien"
      },
      {
        "surname": "Vu",
        "given_name": "Truong"
      },
      {
        "surname": "Than",
        "given_name": "Khoat"
      }
    ]
  },
  {
    "title": "Learning Motion-Perceive Siamese network for robust visual object tracking",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.07.011",
    "abstract": "Siamese networks enable end-to-end training for visual tracking, achieving excellent performance in recent years. However, classical Siamese-based formulation relies on an offline-trained appearance model to perform tracking for each frame, ignoring the temporal variation at the online stage. As been verified that temporal motion is crucial for robust and accurate tracking, we propose a novel Motion-Perceive Siamese network (SiamMP) that explicitly predicts motion patterns, providing complementary clues for the appearance-only formulation. Specifically, successive historical frames are collected, with their appearance and potential trajectory being employed to predict the next state, achieving motion awareness correspondingly. Besides, an adaptive fusion module is dedicated to performing a decision-level negotiation between the tracking evidence of the appearance and the motion models. To verify the effectiveness and merit of our SiamMP, extensive experiments are conducted on several challenging benchmarks, including LaSOT, OTB100, GOT-10k, TC128, and DTB70. The comparison and analysis of the obtained results demonstrate the necessity and validity of involving the motion-perceive design in the Siamese framework.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523002088",
    "keywords": [
      "Artificial intelligence",
      "Astronomy",
      "Computer science",
      "Computer vision",
      "Eye tracking",
      "Frame (networking)",
      "Match moving",
      "Motion (physics)",
      "Object (grammar)",
      "Pedagogy",
      "Physics",
      "Psychology",
      "Telecommunications",
      "Tracking (education)",
      "Trajectory",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Kang",
        "given_name": "Ze"
      },
      {
        "surname": "Xu",
        "given_name": "Tianyang"
      },
      {
        "surname": "Zhu",
        "given_name": "Xue-Feng"
      },
      {
        "surname": "Wu",
        "given_name": "Xiao-Jun"
      }
    ]
  },
  {
    "title": "Coloring anime line art videos with transformation region enhancement network",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109562",
    "abstract": "Automatic colorization of anime line art videos aims to produce color frames given line art frames and reference color images, which is challenging due to various motions and geometric transformations across frame sequences. Existing methods usually utilize the feature maps of reference images directly and treat all the regions in an image equally. However, this may overlook the details of the regions undergoing geometric transformations. To emphasize the regions with significant transformations between the reference and target frames, we propose a Transformation Region Enhancement Network (TRE-Net) to exploit useful reference information and enhance the colorization of key transformation regions with Region Localization Module (RLM) and Feature Enhancement Module (FEM). Specifically, we propose Multi-scale Euclidean Distance Difference (Multi-scale EDD) Maps in RLM which effectively locate geometric transformation regions by contrasting the Euclidean Distance Maps of two line arts and aggregating representations at multiple scales of the network. In addition, FEM is devised to enhance feature learning in the regions with geometric transformation and to ensure proper color alignment. FEM learns locally enhanced features through an attention-gating operation at a low computational cost. With the well-represented key geometric transformation regions, our method exploits the multi-scale reference information well for color alignment, thus produces perceptually pleasing frames. Comprehensive experimental results show that our proposed method is superior to existing methods in terms of the overall quality of colorized anime line art videos.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323002625",
    "keywords": [
      "Anime",
      "Artificial intelligence",
      "Biochemistry",
      "Cartography",
      "Chemistry",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Euclidean distance",
      "Exploit",
      "Feature (linguistics)",
      "Frame (networking)",
      "Gene",
      "Geography",
      "Geometric transformation",
      "Geometry",
      "Image (mathematics)",
      "Key (lock)",
      "Line (geometry)",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Scale (ratio)",
      "Telecommunications",
      "Transformation (genetics)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Ning"
      },
      {
        "surname": "Niu",
        "given_name": "Muyao"
      },
      {
        "surname": "Dou",
        "given_name": "Zhi"
      },
      {
        "surname": "Wang",
        "given_name": "Zhihui"
      },
      {
        "surname": "Wang",
        "given_name": "Zhiyong"
      },
      {
        "surname": "Ming",
        "given_name": "Zhaoyan"
      },
      {
        "surname": "Liu",
        "given_name": "Bin"
      },
      {
        "surname": "Li",
        "given_name": "Haojie"
      }
    ]
  },
  {
    "title": "GGD-GAN: Gradient-Guided dual-Branch adversarial networks for relic sketch generation",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109586",
    "abstract": "Sketch reflects the main content and structure of the painted cultural relics, which help researcher understand the original drawing intention and painting skills. Although the existing automatic sketch extraction methods can improve efficiency, most of them are based on edge detection leading the limitations in incomplete details, serious disease noise and blurring. In this paper, a gradient guided dual-branch generative adversarial networks (GANs) is proposed for high-quality relic sketch generation. The sketch generation branch (SGB) and auxiliary gradient-image generation branch (GGB) were designed via two independent GANs for learning different and complement characteristics. We also designed a feature transmission module for transferring context features from SGB to GGB, and a fusion block to realize the gradient guidance from GGB to SGB, forcing SGB to pay more attention to shape, details, and noise suppression. Both branches not only learn different characteristics independently, but also affect and complement each other, which generates rich and clean high-quality sketches. In our experiments, we compared our method with eight state-of-the-art algorithms quantitatively and qualitatively, analysis the effects of the gradient guidance feature transfer, and the generalization of the proposed dual-branch GAN framework. Experiments show the proposed framework is promising in its ability to extract a clear, coherent, and complete sketch of painted cultural relics.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032300287X",
    "keywords": [
      "Adversarial system",
      "Algorithm",
      "Art",
      "Artificial intelligence",
      "Biochemistry",
      "Biology",
      "Chemistry",
      "Complement (music)",
      "Complementation",
      "Computer science",
      "Context (archaeology)",
      "Dual (grammatical number)",
      "Enhanced Data Rates for GSM Evolution",
      "Feature (linguistics)",
      "Gene",
      "Generalization",
      "Image (mathematics)",
      "Linguistics",
      "Literature",
      "Mathematical analysis",
      "Mathematics",
      "Noise (video)",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Phenotype",
      "Philosophy",
      "Sketch"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Jun"
      },
      {
        "surname": "Zhang",
        "given_name": "Erlei"
      },
      {
        "surname": "Cui",
        "given_name": "Shan"
      },
      {
        "surname": "Wang",
        "given_name": "Jiaxin"
      },
      {
        "surname": "Zhang",
        "given_name": "Qunxi"
      },
      {
        "surname": "Fan",
        "given_name": "Jianping"
      },
      {
        "surname": "Peng",
        "given_name": "Jinye"
      }
    ]
  },
  {
    "title": "Multi-dataset fusion for multi-task learning on face attribute recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.07.015",
    "abstract": "The goal of face attribute recognition(FAR) is to recognize the attributes of face images, such as gender, race, etc. Multi-dataset fusion aims to train a network with multiple datasets simultaneously, which has significant advantages in deep learning. For example, in FAR, the attribute labels differ with the datasets. When using multi-dataset fusion on FAR, training data and attribute labels can be increased so that the network performance can be improved and more attributes can be recognized simultaneously. Currently, most related researchers take the multi-task learning approach on FAR, where each attribute is treated as a recognition task. However, the existing multi-task learning framework cannot be applied when using multi-dataset fusion on FAR. That is because multi-task learning requires the training data to contain the labels of all tasks, but no data contains all the attribute labels of multiple datasets. A Multi-Dataset and Multi-Task Framework(MDMTF) that takes knowledge distillation as the core is proposed in this paper to deal with the problem above. Three distillation strategies are designed in the MDMTF according to the characteristics of multi-dataset fusion on FAR. Experimental results on CelebA and MAAD-Face datasets demonstrate the effectiveness of our framework and strategies. Compared to training two networks on both datasets, respectively, that is, single dataset training, our method has significant advantages in accuracy, parameters, and computational complexity.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523002180",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Economics",
      "Face (sociological concept)",
      "Facial recognition system",
      "Machine learning",
      "Management",
      "Multi-task learning",
      "Pattern recognition (psychology)",
      "Social science",
      "Sociology",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Lu",
        "given_name": "Hengjie"
      },
      {
        "surname": "Xu",
        "given_name": "Shugong"
      },
      {
        "surname": "Wang",
        "given_name": "Jiahao"
      }
    ]
  },
  {
    "title": "Multi-stage information diffusion for joint depth and surface normal estimation",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109660",
    "abstract": "Depth and surface normal estimations are important for 3D geometric perception, which has numerous applications including autonomous vehicles and robots. In this paper, we propose a lightweight Multi-stage Information Diffusion Network (MIDNet) for the simultaneous prediction of depth and surface normals from a single RGB image. To obtain semantic and detail-preserving features, we adopt a high-resolution network as our backbone to learn multi-scale features, which are then fused into shared features for the two tasks. To mutually boost each task, a Cross-Correlation Attention Module (CCAM) is proposed to adaptively integrate information for the prediction of the two tasks in multiple stages, including feature-level information interaction and task-level information interaction. Ablation studies show that the proposed multi-stage information diffusion strategy can boost the performance gain for the two tasks at different levels. Compared to current state-of-the-art methods on the NYU Depth V2, Stanford 2D-3D-Semantic and KITTI datasets, our method achieves superior performance for both monocular depth and surface normal estimation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323003618",
    "keywords": [
      "Architectural engineering",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer vision",
      "Depth perception",
      "Economics",
      "Engineering",
      "Feature (linguistics)",
      "Joint (building)",
      "Linguistics",
      "Management",
      "Monocular",
      "Multi-task learning",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Perception",
      "Philosophy",
      "RGB color model",
      "Robot",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Fu",
        "given_name": "Zhiheng"
      },
      {
        "surname": "Hong",
        "given_name": "Siyu"
      },
      {
        "surname": "Liu",
        "given_name": "Mengyi"
      },
      {
        "surname": "Laga",
        "given_name": "Hamid"
      },
      {
        "surname": "Bennamoun",
        "given_name": "Mohammed"
      },
      {
        "surname": "Boussaid",
        "given_name": "Farid"
      },
      {
        "surname": "Guo",
        "given_name": "Yulan"
      }
    ]
  },
  {
    "title": "Adaptive fusion affinity graph with noise-free online low-rank representation for natural image segmentation",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109611",
    "abstract": "Affinity graph-based segmentation methods have become a major trend in computer vision. The performance of these methods rely on the constructed affinity graph, with particular emphasis on the neighborhood topology and pairwise affinities among superpixels. However, these graph-based methods ignore the noisy data from images, that influence the accuracy of pairwise similarities. Multiscale combinatorial grouping and graph fusion also generate a higher computational complexity. In this paper, we propose an adaptive fusion affinity graph with noise-free low-rank representation in an online manner for natural image segmentation. An input image is first over-segmented into superpixels at different scales and then filtered by an improved kernel density estimation method. Moreover, we select global nodes of these superpixels on the basis of their subspace-preserving presentation, which reveals the feature distribution of superpixels exactly. To reduce time complexity while improving performance, a sparse representation of global nodes based on noise-free online low-rank representation is used to obtain a global graph at each scale. Experimental results on BSD300, BSD500, MSRC, SBD, and PASCAL VOC show the effectiveness of our method in comparison with the state-of-the-art approaches. The code is available at https://github.com/Yangzhangcst/AFA-graph.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323003126",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Graph",
      "Image segmentation",
      "Mathematics",
      "Pairwise comparison",
      "Pascal (unit)",
      "Pattern recognition (psychology)",
      "Programming language",
      "Segmentation",
      "Subspace topology",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Yang"
      },
      {
        "surname": "Liu",
        "given_name": "Moyun"
      },
      {
        "surname": "Zhang",
        "given_name": "Huiming"
      },
      {
        "surname": "Sun",
        "given_name": "Guodong"
      },
      {
        "surname": "He",
        "given_name": "Jingwu"
      }
    ]
  },
  {
    "title": "Mask-guided network for image captioning",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.07.013",
    "abstract": "Attention mechanisms have been widely adopted for image captioning because of their powerful performance. In this paper, we propose a Mask Captioning Network (MaC) consisting of an object layer and a background layer to capture the objects and scenes of an image to generate a sentence. To this end, we leverage the Mask RCNN to detect salient regions at the pixel level instead of a bounding box in the object layer. Meanwhile, in the background layer, a CNN model is used to encode the scene features. In addition, MaC is implemented in both LSTM-based and Transformer-based image captioning architectures. We introduce a mask-guided transformer encoder with additional features to enhance the model. Experimental results show that our model significantly outperforms (with a much richer sentence) baseline models and achieves comparable results with state-of-the-art methods on MSCOCO and Flickr30k datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523002167",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Closed captioning",
      "Computer science",
      "Computer vision",
      "ENCODE",
      "Encoder",
      "Gene",
      "Image (mathematics)",
      "Leverage (statistics)",
      "Minimum bounding box",
      "Operating system",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Sentence",
      "Speech recognition",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Lim",
        "given_name": "Jian Han"
      },
      {
        "surname": "Chan",
        "given_name": "Chee Seng"
      }
    ]
  },
  {
    "title": "Process-Oriented heterogeneous graph learning in GNN-Based ICS anomalous pattern recognition",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109661",
    "abstract": "Over the past few years, massive penetrations targeting an Industrial Control System (ICS) network intend to compromise its core industrial processes. So far, numerous advanced methods have been proposed to detect anomalous patterns in the numeric data streams with respect to the heterogeneous field devices involved in the industrial processes. These methods, despite reporting decent results, usually conduct system-wise detection instead of fine-grained anomalous pattern recognition at the device level. Furthermore, lacking explicit consideration of the exclusive process-related features with respect to each differentiated device, the fitness of their application in specified industrial processes is undermined. To tackle these issues, a GNN-based Attributed Heterogeneous Graph Analyzer (the AHGA) is designed to perform device-wise anomalous pattern detection via in-depth process-oriented associativity learning. The AHGA’s framework is constructed with four building blocks: a graph processor, a feature analyzer, a link inference decoder, and an anomaly detector. Its performance is assessed and compared against multiple link inference and anomaly detection baselines over 2 popular ICS datasets (SWaT and WADI). Comparative results demonstrate the AHGA’s reliability in capturing sophisticated process-oriented relations among heterogeneous devices as well as its effectiveness in boosting the performance of anomalous pattern recognition at device-level granularity.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032300362X",
    "keywords": [
      "Anomaly detection",
      "Artificial intelligence",
      "Associative property",
      "Bottleneck",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Embedded system",
      "Granularity",
      "Graph",
      "Inference",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Process (computing)",
      "Pure mathematics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "L(y)u",
        "given_name": "Shuaiyi"
      },
      {
        "surname": "Wang",
        "given_name": "Kai"
      },
      {
        "surname": "Zhang",
        "given_name": "Liren"
      },
      {
        "surname": "Wang",
        "given_name": "Bailing"
      }
    ]
  },
  {
    "title": "An improved differential evolution algorithm for quantifying fraudulent transactions",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109623",
    "abstract": "Identification of fraudulent credit card transactions is a complex problem mainly due to the following factors: 1) The relative behavior of customers and fraudsters may alter over time. 2) The ratio of legitimate to fraudulent transactions is highly imbalanced, and 3) Investigators examine a small segment of transactions in a reasonable time frame. Researchers have proposed various algorithms to identify potential fraud in a new incoming transaction. However, these approaches require significant human investigator effort and are sometimes misleading. To address this issue, this paper proposes an improved multiobjective differential evolution (DE) algorithm to estimate the distribution of fraudulent transactions in a set of new incoming transactions, referred to as quantifying fraudulent transactions. Our paper has three major novelties. First, we present the problem formulation of cost-based feature selection with maximum quantification ability. Second, we improve the DE by applying effective trial vector generation algorithms to the random control parameter settings to exploit the advantage of individual DE variants. Third, we develop the maximum-relevancy-minimum-redundancy-based Pareto refining operator to enhance the self-learning ability of individuals in Pareto solutions. We compare our approach against four other modifications of DE and five state-of-the-art evolutionary algorithms on real-time credit datasets in streaming and non-streaming frameworks using hyper-volume, two-set coverage, and spread performance metrics.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323003242",
    "keywords": [
      "Algorithm",
      "Biology",
      "Botany",
      "Computer science",
      "Computer security",
      "Data mining",
      "Database",
      "Database transaction",
      "Differential evolution",
      "Evolutionary algorithm",
      "Exploit",
      "Identification (biology)",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Operating system",
      "Pareto principle",
      "Programming language",
      "Redundancy (engineering)",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Rakesh",
        "given_name": "Deepak Kumar"
      },
      {
        "surname": "Jana",
        "given_name": "Prasanta K."
      }
    ]
  },
  {
    "title": "Deepfacelab: Integrated, flexible and extensible face-swapping framework",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109628",
    "abstract": "Face swapping has drawn a lot of attention for its compelling performance. However, current deepfake methods suffer the effects of obscure workflow and poor performance. To solve these problems, we present DeepFaceLab, the current dominant deepfake framework for practical face-swapping. It provides the necessary tools as well as an easy-to-use way to conduct high-quality face-swapping. It also offers a flexible and loose coupling structure for people who need to strengthen their pipeline with other features without writing complicated boilerplate code. We detail the principles that drive the implementation of DeepFaceLab and introduce its pipeline. DeepFaceLab could achieve cinema-level results with high fidelity as our supplemental video shows. We also demonstrate the advantage of our system by comparing our approach with other face-swapping methods. Deepfake defense not only requires the research of detection but also requires the efforts of generation methods. As for a popular and practical toolkit, we encourage users to promote harmless deepfake-entertainment content on social media, reminding the public of the existence of deepfake when they are looking for entertainment.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323003291",
    "keywords": [
      "Art",
      "Computer science",
      "Data science",
      "Database",
      "Entertainment",
      "Epistemology",
      "Extensibility",
      "Face (sociological concept)",
      "Fidelity",
      "Guard (computer science)",
      "Human–computer interaction",
      "Philosophy",
      "Pipeline (software)",
      "Programming language",
      "Quality (philosophy)",
      "Social media",
      "Social science",
      "Sociology",
      "Software engineering",
      "Telecommunications",
      "Visual arts",
      "Workflow",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Kunlin"
      },
      {
        "surname": "Perov",
        "given_name": "Ivan"
      },
      {
        "surname": "Gao",
        "given_name": "Daiheng"
      },
      {
        "surname": "Chervoniy",
        "given_name": "Nikolay"
      },
      {
        "surname": "Zhou",
        "given_name": "Wenbo"
      },
      {
        "surname": "Zhang",
        "given_name": "Weiming"
      }
    ]
  },
  {
    "title": "MAML-SR: Self-adaptive super-resolution networks via multi-scale optimized attention-aware meta-learning",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.08.004",
    "abstract": "The deep-learning-based super-resolution (SR) methods require an avalanche of training images. However, they do not adapt model parameters in test time to cope with the novel blur kernel scenarios. Even though the recent meta-learning-based SR techniques adapt trained model parameters leveraging a test image’s internal patch recurrence, they need heavy pre-training on an external dataset to initialize. Also, the shallow internal information exploration and failure to amplify the salient edges impacts blurry SR image generation. Besides, model inferencing gets delayed due to a threshold-dependent adaptation phase. In this paper, we present Multi-scale Optimized Attention-aware Meta-Learning framework for SR (MAML-SR) to explore the multi-scale hierarchical self-similarity of recurring patches in a test image. Precisely, without any pre-training, we directly meta-train our model with a second-order optimization having the first-order adapted parameters from the intermediate scales, which are again directly supervised with the ground-truth HR images. At each scale, non-local self-similarity is maximized along with the amplification of salient edges using a novel cross-scale spectro-spatial attention learning unit. Also, we drastically reduce the inference delay by putting a metric-dependent constraint on the gradient updates for a test image. We demonstrate our method’s superior super-resolving capability over four benchmark SR datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523002246",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Economics",
      "Image (mathematics)",
      "Machine learning",
      "Management",
      "Meta learning (computer science)",
      "Physics",
      "Quantum mechanics",
      "Resolution (logic)",
      "Scale (ratio)",
      "Superresolution",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Pal",
        "given_name": "Debabrata"
      },
      {
        "surname": "Bose",
        "given_name": "Shirsha"
      },
      {
        "surname": "More",
        "given_name": "Deeptej"
      },
      {
        "surname": "Jha",
        "given_name": "Ankit"
      },
      {
        "surname": "Banerjee",
        "given_name": "Biplab"
      },
      {
        "surname": "Jeppu",
        "given_name": "Yogananda"
      }
    ]
  },
  {
    "title": "Community Channel-Net: Efficient channel-wise interactions via community graph topology",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109536",
    "abstract": "The layer-wise structure of deep neural networks (DNNs) isolates the channel interactions in the same layer, which significantly impedes the efficient learning of DNNs. Several existing methods enable channel-wise information exchange via learning channel interdependence in a heuristic and empirical manner. Nevertheless, only informative channels are emphasized while other channels are suppressed in these approaches. This results in a low channel diversity, which impeds the generalization of DNNs. Our work aims to learn channel-wise interdependence and keep the channel diversity concurrently via designing optimal channel interaction patterns. We model the channel interaction pattern from a graph perspective, where the interactions can be regarded as information exchange on the channel graph. Based on this framework, we propose the Community Channel-Net (CC-Net), using a community-based graph topology for channel interaction. Each community contains channels with semantic commonalities, and the inter-community connections are activated among critical channels. With this structured and dynamic topology, the channels from the same community can learn channel interdependence, and those critical channels from distinct communities can gain more diverse features. CC-Net outperforms baselines on image classification tasks over various backbones with fewer computational costs.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323002364",
    "keywords": [
      "Artificial intelligence",
      "Channel (broadcasting)",
      "Combinatorics",
      "Community structure",
      "Computer network",
      "Computer science",
      "Graph",
      "Heuristic",
      "Information exchange",
      "Machine learning",
      "Mathematics",
      "Telecommunications",
      "Theoretical computer science",
      "Topology (electrical circuits)"
    ],
    "authors": [
      {
        "surname": "Feng",
        "given_name": "Fan"
      },
      {
        "surname": "Liu",
        "given_name": "Qi"
      },
      {
        "surname": "Peng",
        "given_name": "Zhanglin"
      },
      {
        "surname": "Zhang",
        "given_name": "Ruimao"
      },
      {
        "surname": "Chan",
        "given_name": "Rosa H.M."
      }
    ]
  },
  {
    "title": "KeyEncoder: A secure and usable EEG-based cryptographic key generation mechanism",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.07.008",
    "abstract": "Nowadays, a rapid, easy, and convenient access to our private information is essential to carry out both personal and professional activities. In most cases, this information is sensitive and can be stolen due to its importance and the lack of security protocols. In this study we propose a time–invariant cryptographic key generation mechanism based on electroencephalogram (EEG) signals. We employed Discrete Wavelet Transform and autoencoders to extract the biometric features from the EEG signals. Using these features, we construct a scheme to generate secure seeds that can be used as inputs for secure hash functions and obtain cryptographic keys. The mechanism proposed preserves the privacy of the user, as the cryptographic key is generated for each new EEG signal received, avoiding the need of storing the key, previous EEG signals or any other information. Results show that the proposed mechanism is secure against random attacks, as a 0% of False Acceptance Rate is reported, while generating seeds with an entropy of 0.968 in less than 500 ms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523002052",
    "keywords": [
      "Artificial intelligence",
      "Biometrics",
      "Computer science",
      "Computer security",
      "Cryptographic hash function",
      "Cryptographic primitive",
      "Cryptographic protocol",
      "Cryptography",
      "Electroencephalography",
      "Entropy (arrow of time)",
      "Hash function",
      "Key (lock)",
      "Key generation",
      "Pattern recognition (psychology)",
      "Physics",
      "Psychiatry",
      "Psychology",
      "Quantum mechanics",
      "Theoretical computer science",
      "USable",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Hernández-Álvarez",
        "given_name": "Luis"
      },
      {
        "surname": "Barbierato",
        "given_name": "Elena"
      },
      {
        "surname": "Caputo",
        "given_name": "Stefano"
      },
      {
        "surname": "de Fuentes",
        "given_name": "José María"
      },
      {
        "surname": "González-Manzano",
        "given_name": "Lorena"
      },
      {
        "surname": "Encinas",
        "given_name": "Luis Hernández"
      },
      {
        "surname": "Mucchi",
        "given_name": "Lorenzo"
      }
    ]
  },
  {
    "title": "SurroundNet: Towards effective low-light image enhancement",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109602",
    "abstract": "Although Convolution Neural Networks (CNNs) have made substantial progress in the low-light image enhancement task, one critical problem of CNNs is the paradox of model complexity and performance. This paper presents a novel SurroundNet that only involves less than 150 K parameters (about 80–98 percent size reduction compared to SOTAs) and achieves very competitive performance. The proposed network comprises several Adaptive Retinex Blocks (ARBlock), which can be viewed as a novel extension of Single Scale Retinex in feature space. The core of our ARBlock is an efficient illumination estimation function called Adaptive Surround Function (ASF). It can be regarded as a general form of surround functions and be implemented by convolution layers. In addition, we also introduce a Low-Exposure Denoiser (LED) to smooth the low-light image before the enhancement. We evaluate the proposed method on two real-world low-light datasets. Experimental results demonstrate the superiority of our submitted SurroundNet in both performance and network parameters against State-of-the-Art low-light image enhancement methods. The code is available at https://github.com/ouc-ocean-group/SurroundNet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323003035",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Code (set theory)",
      "Color constancy",
      "Computer science",
      "Computer vision",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Evolutionary biology",
      "Feature (linguistics)",
      "Function (biology)",
      "Geometry",
      "Image (mathematics)",
      "Image enhancement",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Reduction (mathematics)",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Fei"
      },
      {
        "surname": "Sun",
        "given_name": "Xin"
      },
      {
        "surname": "Dong",
        "given_name": "Junyu"
      },
      {
        "surname": "Zhu",
        "given_name": "Xiao Xiang"
      }
    ]
  },
  {
    "title": "Robust federated learning under statistical heterogeneity via Hessian spectral decomposition",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109635",
    "abstract": "Federated Learning (FL) is a collaborative machine learning paradigm in which a global model is learned via aggregating local ones. Although statistical heterogeneity of the local training data is necessary for the generalisability of the global model, it also introduces local model “drift” that slows down the convergence. Thus, how to optimally aggregate local models in FL remains an open problem. Recognising that training data lends varying evidential credence to different parts of a local model, we propose a novel approach to exploit such evidential asymmetry in FL aggregation in not independent and identically distributed (non-IID) data by applying a unique weight coefficient to each of the local parameter updates. To this end, we measure the parameter-level evidential credence making use of the eigenvalues of the Hessian of the local likelihood function, which are theoretically connected to the observed Fisher information. We employ these eigenpairs to propose a novel aggregation method, which we name FedHess. Our experiments show FedHess achieves smoother and faster convergence to a more accurate global model when compared with popular baselines such as Federated Average (FedAvg), FedProx, SCAFFOLD, Federated Curvature (FedCurv) and FedDF across different types of heterogeneous training data drawn from a number of benchmark datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323003369",
    "keywords": [
      "Aggregate (composite)",
      "Applied mathematics",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Composite material",
      "Computer science",
      "Convergence (economics)",
      "Credence",
      "Data mining",
      "Economic growth",
      "Economics",
      "Geodesy",
      "Geography",
      "Hessian matrix",
      "Independent and identically distributed random variables",
      "Machine learning",
      "Materials science",
      "Mathematical optimization",
      "Mathematics",
      "Measure (data warehouse)",
      "Random variable",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Ahmad",
        "given_name": "Adnan"
      },
      {
        "surname": "Luo",
        "given_name": "Wei"
      },
      {
        "surname": "Robles-Kelly",
        "given_name": "Antonio"
      }
    ]
  },
  {
    "title": "RPI-CapsuleGAN: Predicting RNA-protein interactions through an interpretable generative adversarial capsule network",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109626",
    "abstract": "RNA-protein interactions (RPI) play a crucial regulatory role in cellular physiological processes. The study and prediction of RPIs can be insightful for exploring disease mechanisms and drug target design. Traditional RPI prediction methods relied mainly on tedious and expensive biological experiments, and there is an increasing interest in developing more cost-effective computational methods to predict RPIs. This work proposes an interpretable RPI-CapsuleGAN method for RPI prediction based on a generative adversarial capsule network with a convolutional block attention module. First, RPI-CapsuleGAN extracts and fuses multiple features to characterize RNA and protein sequences. Subsequently, the elastic net feature selection method is used to retain features that are highly informative to RPI prediction. Finally, we introduce a convolutional attention mechanism into the generative adversarial capsule network for the first time in order to construct the RPI prediction framework, which is shown to improve the model feature learning of interpretable and expression ability, and effectively solves the problem of the disappearance of the model spatial structure hierarchy. Based on a five-fold cross-validation test, the prediction accuracy of the RPI-CapsuleGAN method reaches 97.1%, 88.8%, 92.5%, 97.3%, and 87.8% for datasets RPI488, RPI369, RPI2241, RPI1807, and RPI1446. The RPI-CapsuleGAN method has higher accuracy than state-of-the-art RPI prediction methods that use the same datasets. In the test dataset NPInter227 constructed in this paper, five groups of test sets are composed of positive samples and five groups of negative samples, the prediction accuracy reaches 97.38%, 96.48%, 97.38%, 97.81%, and 97.15%, respectively, outperforming other mainstream deep learning algorithms. In addition, RPI-CapsuleGAN obtained better results for the prediction of independent test datasets. Extensive experiments detailed here show that RPI-CapsuleGAN can provide an efficient, accurate, and stable method for RPI prediction.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323003278",
    "keywords": [
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Computer science",
      "Construct (python library)",
      "Data mining",
      "Deep learning",
      "Feature (linguistics)",
      "Feature selection",
      "Generative adversarial network",
      "Generative grammar",
      "Geometry",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Yifei"
      },
      {
        "surname": "Wang",
        "given_name": "Xue"
      },
      {
        "surname": "Chen",
        "given_name": "Cheng"
      },
      {
        "surname": "Gao",
        "given_name": "Hongli"
      },
      {
        "surname": "Salhi",
        "given_name": "Adil"
      },
      {
        "surname": "Gao",
        "given_name": "Xin"
      },
      {
        "surname": "Yu",
        "given_name": "Bin"
      }
    ]
  },
  {
    "title": "Improved robustness of vision transformers via prelayernorm in patch embedding",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109659",
    "abstract": "Vision Transformers (ViTs) have recently demonstrated state-of-the-art performance in various vision tasks, replacing convolutional neural networks (CNNs). However, because ViT has a different architectural design than CNN, it may behave differently. To investigate whether ViT has a different performance or robustness, we tested ViT and CNN under various imaging conditions in practical vision tasks. We confirmed that for most image transformations, ViT’s robustness was comparable or even better than that of CNN. However, for contrast enhancement, ViT performed particularly poorly. We show that this is because positional embedding in ViT’s patch embedding can work improperly when the color scale changes. We demonstrate that the use of PreLayerNorm, a modified patch embedding structure, ensures the consistent behavior of ViT. Results demonstrate that ViT with PreLayerNorm exhibited improved robustness in the contrast-varying environments.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323003606",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Electrical engineering",
      "Embedding",
      "Engineering",
      "Gene",
      "Pattern recognition (psychology)",
      "Robustness (evolution)",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Kim",
        "given_name": "Bum Jun"
      },
      {
        "surname": "Choi",
        "given_name": "Hyeyeon"
      },
      {
        "surname": "Jang",
        "given_name": "Hyeonah"
      },
      {
        "surname": "Lee",
        "given_name": "Dong Gu"
      },
      {
        "surname": "Jeong",
        "given_name": "Wonseok"
      },
      {
        "surname": "Kim",
        "given_name": "Sang Woo"
      }
    ]
  },
  {
    "title": "Generalization of deep learning models for natural gas indication in 2D seismic data",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109642",
    "abstract": "Methods based on Machine Learning and Deep Learning are increasingly popular to help interpret large volumes of data that belong to various areas and seek to fulfill multiple tasks. One of these areas studies seismic data in the search for hydrocarbon reserves, for which Deep Learning models are trained, showing acceptable results for low study data. However, these models present generalization problems. Their performance tends to decrease when used on seismic data from new exploration. This tendency is particularly true for 2D data, which have many features. This work presents a method to improve the generalization of the Deep Learning model for the indication of natural gas in 2D seismic data based on the recommendation of training data and hyperparameter operations of the model. The tests used a database of the Parnaíba basin in northeast Brazil. Experiments showed an increase in the correct indication of natural gas that varies according to the metric 8 % ≤ R e c a l l ≤ 37 % , with a fluctuation in the increase of false positives of − 2 % ≤ P r e c i s i o n ≤ 13 % . It is an improvement in the generalization of the Deep Learning model of up to 11% according to the F1 score metric or up to 10% according to the IoU metric.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323003436",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Deep learning",
      "Engineering",
      "Generalization",
      "Hyperparameter",
      "Hyperparameter optimization",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Metric (unit)",
      "Operations management",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Sepulveda",
        "given_name": "Luis Fernando Marin"
      },
      {
        "surname": "Gattass",
        "given_name": "Marcelo"
      },
      {
        "surname": "Silva",
        "given_name": "Aristofanes Correa"
      },
      {
        "surname": "Quevedo",
        "given_name": "Roberto"
      },
      {
        "surname": "Michelon",
        "given_name": "Diogo"
      },
      {
        "surname": "Siedschlag",
        "given_name": "Carlos"
      },
      {
        "surname": "Ribeiro",
        "given_name": "Roberto"
      }
    ]
  },
  {
    "title": "L1-norm discriminant analysis via Bhattacharyya error bounds under Laplace distributions",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109609",
    "abstract": "L1-norm discriminant analysis has been proposed to enhance the robustness of classical LDA in the presence of outliers. This paper develops L1-norm discriminant analysis by exploring Bhattacharyya error bounds under Laplace distributions. Unlike some previous models, we assume that the samples of each class in the projected space follow Laplace distributions. It is the first time that the Bhattacharyya error bound is derived under Laplace distributions. It is interesting to note that this bound has a closed-form expression in a one-dimensional projected space. We relax the Bhattacharyya error bound to achieve another bound that can facilitate the design of a tractable model. Since the parameters of Laplace distributions are estimated by the maximum likelihood estimation, this yields the problem of estimating class centroids in our model. We employ a simple yet effective strategy to estimate class centroids in the original space. To address the small-sample-size problem, we transform the original model into a difference criterion by introducing additional parameters. We achieve an alternative representation of our model and design an effective algorithm to solve it. In addition, we also extend our model to its kernel version. The experiments on a series of data sets are done to demonstrate the effectiveness of our method in dealing with contaminated data.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323003102",
    "keywords": [
      "Algorithm",
      "Applied mathematics",
      "Artificial intelligence",
      "Bhattacharyya distance",
      "Centroid",
      "Computer science",
      "Discriminant",
      "Facial recognition system",
      "Geometry",
      "Kernel Fisher discriminant analysis",
      "Laplace distribution",
      "Laplace transform",
      "Law",
      "Linear discriminant analysis",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Norm (philosophy)",
      "Outlier",
      "Pattern recognition (psychology)",
      "Political science",
      "Statistics",
      "Upper and lower bounds"
    ],
    "authors": [
      {
        "surname": "Liang",
        "given_name": "Zhizheng"
      },
      {
        "surname": "Zhang",
        "given_name": "Lei"
      }
    ]
  },
  {
    "title": "Faster OreFSDet: A lightweight and effective few-shot object detector for ore images",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109664",
    "abstract": "For the ore particle size detection, obtaining a sizable amount of high-quality ore labeled data is time-consuming and expensive. General object detection methods often suffer from severe over-fitting with scarce labeled data. Despite their ability to eliminate over-fitting, existing few-shot object detectors encounter drawbacks such as slow detection speed and high memory requirements, making them difficult to implement in a real-world deployment scenario. To this end, we propose a lightweight and effective few-shot detector to achieve competitive performance with general object detection with only a few samples for ore images. First, the proposed support feature mining block characterizes the importance of location information in support features. Next, the relationship guidance block makes full use of support features to guide the generation of accurate candidate proposals. Finally, the dual-scale semantic aggregation module retrieves detailed features at different resolutions to contribute with the prediction process. Experimental results show that our method consistently exceeds the few-shot detectors with an excellent performance gap on all metrics. Moreover, our method achieves the smallest model size of 19 MB as well as being competitive at 50 FPS detection speed compared with general object detectors.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323003655",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Detector",
      "Dual (grammatical number)",
      "Feature (linguistics)",
      "Geometry",
      "Linguistics",
      "Literature",
      "Mathematics",
      "Object (grammar)",
      "Object detection",
      "Operating system",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Process (computing)",
      "Shot (pellet)",
      "Speedup",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Yang"
      },
      {
        "surname": "Cheng",
        "given_name": "Le"
      },
      {
        "surname": "Peng",
        "given_name": "Yuting"
      },
      {
        "surname": "Xu",
        "given_name": "Chengming"
      },
      {
        "surname": "Fu",
        "given_name": "Yanwei"
      },
      {
        "surname": "Wu",
        "given_name": "Bo"
      },
      {
        "surname": "Sun",
        "given_name": "Guodong"
      }
    ]
  },
  {
    "title": "Do all roads lead to Rome? Studying distance measures in the context of machine learning",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109646",
    "abstract": "Many machine learning and data mining tasks are based on distance measures, so a large amount of literature addresses this aspect somehow. Due to the broad scope of the topic, this paper aims to provide an overview of the use of these measures in the most common machine learning problems, pointing out those aspects to consider to choose the most appropriate measure for a particular task. For this purpose, the most recent works addressing the subject were reviewed and seven of the most commonly used measures were analyzed, investigating in detail their main properties and applications. Different experiments were carried out to study their relationships and compare their performance. The degradation of the results in the presence of noise was also considered, as well as the execution time required by each measure.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323003473",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Data mining",
      "Data science",
      "Engineering",
      "Geology",
      "Geomorphology",
      "Image (mathematics)",
      "Lead (geology)",
      "Machine learning",
      "Measure (data warehouse)",
      "Noise (video)",
      "Paleontology",
      "Programming language",
      "Scope (computer science)",
      "Systems engineering",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Blanco-Mallo",
        "given_name": "Eva"
      },
      {
        "surname": "Morán-Fernández",
        "given_name": "Laura"
      },
      {
        "surname": "Remeseiro",
        "given_name": "Beatriz"
      },
      {
        "surname": "Bolón-Canedo",
        "given_name": "Verónica"
      }
    ]
  },
  {
    "title": "MTFD-Net: Left atrium segmentation in CT images through fractal dimension estimation",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.08.005",
    "abstract": "Multi-task learning proved to be an effective strategy to increase the performance of a dense prediction network on a segmentation task, by defining auxiliary tasks to reflect different aspects of the problem and concurrently learning them with the main task of segmentation. Up to now, previous studies defined their auxiliary tasks in the Euclidean space. However, for some segmentation tasks, the complexity and high variation in the texture of a region of interest may not follow the smoothness constraint in the Euclidean geometry. This paper addresses this issue by introducing a new multi-task network, MTFD-Net, which utilizes the fractal geometry to quantify texture complexity through self-similar patterns in an image. To this end, we propose to transform an image into a map of fractal dimensions and define its learning as an auxiliary task, which will provide auxiliary supervision to the main segmentation task, towards betterment of left atrium (LA) segmentation in computed tomography (CT) images. To the best of our knowledge, this is the first proposal of a dense prediction network that employs the fractal geometry to define an auxiliary task and learns it in parallel to the segmentation task in a multi-task learning framework. Our experiments revealed that the proposed MTFD-Net model led to more accurate LA segmentations compared to its counterparts.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523002258",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Economics",
      "Euclidean distance",
      "Euclidean geometry",
      "Fractal",
      "Fractal analysis",
      "Fractal dimension",
      "Geometry",
      "Image segmentation",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Scale-space segmentation",
      "Segmentation",
      "Smoothness",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Saber Jabdaragh",
        "given_name": "Aziza"
      },
      {
        "surname": "Firouznia",
        "given_name": "Marjan"
      },
      {
        "surname": "Faez",
        "given_name": "Karim"
      },
      {
        "surname": "Alikhani",
        "given_name": "Fariba"
      },
      {
        "surname": "Alikhani Koupaei",
        "given_name": "Javad"
      },
      {
        "surname": "Gunduz-Demir",
        "given_name": "Cigdem"
      }
    ]
  },
  {
    "title": "Knowledge transduction for cross-domain few-shot learning",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109652",
    "abstract": "Cross-Domain Few-Shot Learning (CDFSL) aims to classify new categories from new domains with few samples. It confronts a greater domain shift than Few-Shot Learning (FSL). Based on the transfer learning framework, we propose a Knowledge Transduction method (KT) to alleviate domain shift and achieve few-shot recognition. First, a feature adaptation module based on feed-forward attention is constructed to learn domain-adapted features. The feature adaptation module weakens domain shift by transducing knowledge from an auxiliary dataset to the new dataset. Second, a feature transduction module based on deep sparse representation is developed to gather class semantics from limited support images. The feature transduction module transduces knowledge from support images to query images for few-shot recognition. In addition, a stochastic image augmentation method is proposed for FSL to train a more generalized model through consistency representation learning. Our method achieves competitive accuracy on four CDFSL datasets and four FSL datasets compared to state-of-the-art methods. The source code is available at https://github.com/XDUpfLi/KT.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323003539",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Class (philosophy)",
      "Classifier (UML)",
      "Computer science",
      "Consistency (knowledge bases)",
      "Domain (mathematical analysis)",
      "Domain adaptation",
      "Domain knowledge",
      "Feature (linguistics)",
      "Feature learning",
      "Law",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Programming language",
      "Representation (politics)",
      "Semantics (computer science)",
      "Transduction (biophysics)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Pengfang"
      },
      {
        "surname": "Liu",
        "given_name": "Fang"
      },
      {
        "surname": "Jiao",
        "given_name": "Licheng"
      },
      {
        "surname": "Li",
        "given_name": "Shuo"
      },
      {
        "surname": "Li",
        "given_name": "Lingling"
      },
      {
        "surname": "Liu",
        "given_name": "Xu"
      },
      {
        "surname": "Huang",
        "given_name": "Xinyan"
      }
    ]
  },
  {
    "title": "Multi scale pixel attention and feature extraction based neural network for image denoising",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109603",
    "abstract": "In this paper, we propose a blind Gaussian denoising network that utilize the features of the input image and its negative for generating denoised output of the same. The proposed network is a dual path model which employs a multi-scale pixel attention (MSPA) block on one path and a multi-scale feature extraction (MSFE) block on another. The concept of using the features of a negative image (that it highlights the low contrast region) in blind Gaussian denoising network is, to the best of our knowledge, a first such attempt. The proposed MSPA and MSFE blocks are designed to focus on the features of the image at multiple scales. The MSPA block focuses on the important features of the negative of the input image whereas the MSFE block focuses on extracting features of the input noisy image. The features of both the images are then combined and a final residual noise is obtained, subtracting which from the input noisy image produces the final denoised result. The proposed network is lightweight and fast, due to the low number of convolutional layers involved, and produces superior results (both quantitatively and qualitatively) when compared with various traditional and learning based blind Gaussian denoising techniques. The code of this paper can be downloaded from https://github.com/RTSIR/NIFBGDNet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323003047",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Code (set theory)",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Feature (linguistics)",
      "Feature extraction",
      "Focus (optics)",
      "Gaussian",
      "Gaussian noise",
      "Geometry",
      "Image (mathematics)",
      "Linguistics",
      "Mathematics",
      "Noise (video)",
      "Noise reduction",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Pixel",
      "Programming language",
      "Quantum mechanics",
      "Residual",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Thakur",
        "given_name": "Ramesh Kumar"
      },
      {
        "surname": "Maji",
        "given_name": "Suman Kumar"
      }
    ]
  },
  {
    "title": "Neighborhood-based credibility anchor learning for universal domain adaptation",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109686",
    "abstract": "Universal domain adaptation (UniDA) aims to transfer knowledge from the source domain to the target domain in the presence of distribution shift and class mismatch. Most existing works design threshold-relied methods to reject target private classes by carefully-proposed uncertainty scoring functions which are very sensitive to thresholds. To overcome this problem, a few threshold-free methods are proposed but ignore the neighborhood structure information of the target domain, leading to poor performance. In this paper, we propose Neighborhood-based Credibility Anchor Learning (NCAL), a new threshold-free framework that fully mines the neighborhood structure information to explore better target representations. NCAL contains three key components: a class anchor learning module to learn target class distribution, a credibility-weighted conditional adversarial module to learn class-invariant features of common classes, and an open-set neighborhood clustering module to learn well-clustered features. Extensive experiments demonstrate that our method outperforms the state-of-the-art.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323003849",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Classifier (UML)",
      "Cluster analysis",
      "Computer science",
      "Credibility",
      "Data mining",
      "Domain (mathematical analysis)",
      "Domain adaptation",
      "Invariant (physics)",
      "Law",
      "Machine learning",
      "Mathematical analysis",
      "Mathematical physics",
      "Mathematics",
      "Political science",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Su",
        "given_name": "Wan"
      },
      {
        "surname": "Han",
        "given_name": "Zhongyi"
      },
      {
        "surname": "He",
        "given_name": "Rundong"
      },
      {
        "surname": "Wei",
        "given_name": "Benzheng"
      },
      {
        "surname": "He",
        "given_name": "Xueying"
      },
      {
        "surname": "Yin",
        "given_name": "Yilong"
      }
    ]
  },
  {
    "title": "Dual-channel embedding learning model for partially labeled attributed networks",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109644",
    "abstract": "Network embedding is an important fundamental work in many network application tasks, which encodes the input network from the high-dimensional and sparse topological space into a low-dimensional and dense vector space. Recently, there has been a growing interest in embedding learning on Partially Labeled Attributed Networks (PLANs) due to the increasing occurrence of node attributes and partially available category labels in real-world networks. Semi-supervised embedding learning is a standard approach employed in PLANs, utilizing category labels to supervise the learning process. However, the semi-supervised learning procedure can fail when labels are scarce, noisy, or unreliable. Additionally, most existing embedding algorithms have not successfully integrated heterogeneous information, such as labels, attributes, and structure. To address these issues, we develop a new model, the Dual-Channel Network Embedding (DcNE), which integrates different types of network information into embeddings from a mutual information (MI) perspective. Specifically, we construct a dual-channel information propagation framework to encode the input network in semi-supervised and self-supervised learning paradigms in parallel. Furthermore, a redundancy elimination module is implemented to capture and eliminate the redundant information between the two encoders. Finally, we propose a unified optimization model that integrates the two learning paradigms to collaborate effectively. In the experiments, we demonstrate the effectiveness of DcNE in various network analysis tasks using real-world datasets, establishing its superiority over state-of-the-art baselines.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032300345X",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Dual (grammatical number)",
      "ENCODE",
      "Embedding",
      "Engineering",
      "Gene",
      "Literature",
      "Machine learning",
      "Node (physics)",
      "Operating system",
      "Redundancy (engineering)",
      "Structural engineering",
      "Supervised learning",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Du",
        "given_name": "Hangyuan"
      },
      {
        "surname": "Wang",
        "given_name": "Wenjian"
      },
      {
        "surname": "Bai",
        "given_name": "Liang"
      }
    ]
  },
  {
    "title": "Distributed edge-event-triggered consensus of multi-agent system under DoS attack",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.08.015",
    "abstract": "This paper designs a consensus control protocol for multi-agent systems(MAS) based on the edge-event-triggering mechanism to achieve a leader-following consensus under Denial-of-service (DoS) attacks. First, the dynamics model of the multi-agent system is given, and the DoS attack is modeled. A compensator is designed for each follower separately to keep tracking the state of the leader. In this paper, agents are classified into leaders, informed followers, and uninformed followers. The event-triggering conditions are designed separately. Then, the controller of each agent is designed using the difference between the state of the compensator and the agent state. Using the Lyapunov stability theory, the effectiveness of the designed control strategy under the DoS attack is analyzed, and the constraints on the control parameters are given. Additionally, we proved that the proposed control strategy could avoid the Zeno behavior. Finally, the effectiveness of the proposed control strategy is verified through numerical simulations.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523002362",
    "keywords": [
      "Agronomy",
      "Algorithm",
      "Alternative medicine",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Consensus",
      "Control (management)",
      "Control theory (sociology)",
      "Controller (irrigation)",
      "Denial-of-service attack",
      "Enhanced Data Rates for GSM Evolution",
      "Lyapunov function",
      "Lyapunov stability",
      "Machine learning",
      "Medicine",
      "Multi-agent system",
      "Nonlinear system",
      "Pathology",
      "Physics",
      "Protocol (science)",
      "Quantum mechanics",
      "Stability (learning theory)",
      "State (computer science)",
      "The Internet",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Xue",
        "given_name": "Shuangsi"
      },
      {
        "surname": "Li",
        "given_name": "Huan"
      },
      {
        "surname": "Cao",
        "given_name": "Hui"
      },
      {
        "surname": "Tan",
        "given_name": "Junkai"
      }
    ]
  },
  {
    "title": "SSP-Net: Scalable sequential pyramid networks for real-Time 3D human pose regression",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109714",
    "abstract": "In this paper we propose a highly scalable convolutional neural networks, end-to-end trainable, for real-time 3D human pose regression from still RGB images. We call this approach Scalable Sequential Pyramid Networks (SSP-Net) as it is trained with refined supervision at multiple scales in a sequential manner. Our network requires a single training procedure and is capable of producing its best predictions at 120 frames per second (FPS), or acceptable predictions at more than 200 FPS when cut at test time. We show that the proposed regression approach is invariant to the size of feature maps, allowing our method to perform multi-resolution intermediate supervisions and reaching results comparable to the state-of-the-art with very low resolution feature maps. We demonstrate the accuracy and the effectiveness of our method by providing extensive experiments on two of the most important publicly available datasets for 3D pose estimation, Human3.6M and MPI-INF-3DHP. Additionally, we provide relevant insights about our decisions on the network architecture and show its flexibility to meet the best precision-speed compromise.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323004120",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer security",
      "Convolutional neural network",
      "Data mining",
      "Database",
      "Deep learning",
      "Feature (linguistics)",
      "Geometry",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Network architecture",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pose",
      "Pyramid (geometry)",
      "RGB color model",
      "Regression",
      "Scalability",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Carbonera Luvizon",
        "given_name": "Diogo"
      },
      {
        "surname": "Tabia",
        "given_name": "Hedi"
      },
      {
        "surname": "Picard",
        "given_name": "David"
      }
    ]
  },
  {
    "title": "Text line extraction strategy for palm leaf manuscripts",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.08.007",
    "abstract": "Text line segmentation is an important step in the historical document image analysis pipeline to supply useful information for recognition, keyword spotting and indexing. Many handcrafted-based and learning-based approaches have been developed to cope with text line extraction challenges. In this work we present a hybrid technique which combines a convolutional-based denoising task and heuristic Seam Carving framework. We propose the following changes to the original Seam Carving: (1) We applied adaptive slice to anticipate miss-extraction on a short text during medial seam computation. (2) We applied a triple smoothing to find the best local maxima of the smoothed horizontal projection profile which represents candidate medial seams. (3) We utilized five post-processing steps aimed at reconstructing a more precise medial seam. Three different palm leaf data sets: old Sundanese, old Balinese, and old Khmer, have been used to compare Convolutional Seam Carving (CSC) to several baseline methods, including the original Seam Carving. Experimental results show that the proposed method outperforms other current handcrafted-based baselines on all three palm leaf manuscript (PLM) data sets. On the old Sundanese data sets, CSC can produce a significant improvement of the performance rate compared to all other baseline approaches, and it also enhance the measurement on old Balinese and old Khmer datasets. In the ablation study, we discovered that a foreground extraction step is not only able to reduce noises and color degradation but also provide better separation of text region. Following that, an adaptive slice and triple smoothing approach contribute to solve the text length variation problem. Finally, post-processing steps were effective in connecting discontinuous medial seam. The code has been published in https://github.com/erickpaulus/text-line-segmentation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523002271",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Pattern recognition (psychology)",
      "Pipeline (software)",
      "Programming language",
      "Projection (relational algebra)",
      "Search engine indexing",
      "Segmentation",
      "Smoothing"
    ],
    "authors": [
      {
        "surname": "Paulus",
        "given_name": "Erick"
      },
      {
        "surname": "Burie",
        "given_name": "Jean-Christophe"
      },
      {
        "surname": "Verbeek",
        "given_name": "Fons J."
      }
    ]
  },
  {
    "title": "Robust rank-one matrix completion with rank estimation",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109637",
    "abstract": "Matrix completion aims at estimating the missing entries of a low-rank and incomplete data matrix. It frequently arises in many applications such as computer vision, pattern recognition, recommendation system, and data mining. Most of the existing methods face two problems. Firstly, the data matrix in real world is often disturbed by noise. Noise may change the date structure of the incomplete matrix, thereby degrade the performance of matrix completion algorithms. Secondly, some existing methods need to preset a reasonable rank as input, and the value of rank will affect the performance of the algorithms. Therefore, we proposed a robust rank-one matrix completion method with rank estimation in this paper. To mitigate the influence of noise, we divide the incomplete and noisy data matrix into two parts iteratively: low-rank and sparse parts. Besides, we use a weighted rank-one matrix pursuit algorithm to approximate the low-rank part of the data matrix, and the rank of the matrix can be estimated with the adaptive weight vector. The performance of the proposed method is demonstrated by experiments on both synthetic datasets and image datasets. The experimental results demonstrate the performance of the proposed method with incompleted matrices distrubed by sparse noise.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323003382",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Augmented matrix",
      "Biochemistry",
      "Chemistry",
      "Clade",
      "Combinatorics",
      "Composite material",
      "Computer science",
      "Data Matrix",
      "Data mining",
      "Eigenvalues and eigenvectors",
      "Eight-point algorithm",
      "Gaussian",
      "Gene",
      "Hankel matrix",
      "Image (mathematics)",
      "Low-rank approximation",
      "Materials science",
      "Mathematical analysis",
      "Mathematics",
      "Matrix (chemical analysis)",
      "Matrix completion",
      "Noise (video)",
      "Pattern recognition (psychology)",
      "Phylogenetic tree",
      "Physics",
      "Quantum mechanics",
      "Rank (graph theory)",
      "Sparse matrix",
      "State-transition matrix",
      "Symmetric matrix"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Ziheng"
      },
      {
        "surname": "Nie",
        "given_name": "Feiping"
      },
      {
        "surname": "Wang",
        "given_name": "Rong"
      },
      {
        "surname": "Li",
        "given_name": "Xuelong"
      }
    ]
  },
  {
    "title": "Generalization of the shortest path approach for superpixel segmentation of omnidirectional images",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109673",
    "abstract": "With the growing use of image capture devices using wide angles and the need for fast and accurate image analysis in computer vision, there is a demand for dedicated under-representation methods. Most decomposition methods segment an image into a small number of irregular homogeneous regions, called superpixels. Nevertheless, these approaches are generally designed to process natural 2D planar images, i.e., captured with a 90 o angle view without distortion. In this work, we present SphSPS, a new general decomposition method (for Spherical Shortest Path-based Superpixels) 1 1 Available code at: https://github.com/rgiraud/sphsps , that is dedicated to wide 360 o omnidirectional or spherical images. The produced superpixels respect both the geometry of the 3D spherical acquisition space, and the boundaries of image objects. To fastly extract relevant clustering features, we generalize the shortest path approach between a pixel and a superpixel center. We demonstrate that considering the geometry of the acquisition space to compute the shortest path enables to jointly improve the segmentation accuracy and the shape regularity of superpixels. To evaluate this regularity property, we propose a generalization of a standard 2D regularity metric to the spherical space, addressing the limitations of the only existing spherical compactness measure. Finally, SphSPS is validated on reference 360 o images from the PSD (Panorama Segmentation Dataset) and also on synthetic road omnidirectional images. Our method significantly outperforms both planar and spherical state-of-the-art approaches in terms of segmentation accuracy, robustness to noise and regularity, providing a very interesting tool for superpixel-based applications on 360o images.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323003746",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Gene",
      "Graph",
      "Image segmentation",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Robustness (evolution)",
      "Segmentation",
      "Shortest path problem",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Giraud",
        "given_name": "Rémi"
      },
      {
        "surname": "Borba Pinheiro",
        "given_name": "Rodrigo"
      },
      {
        "surname": "Berthoumieu",
        "given_name": "Yannick"
      }
    ]
  },
  {
    "title": "Deep learning-based intelligent system for fingerprint identification using decision-based median filter",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.08.006",
    "abstract": "Fingerprint recognition has emerged as one of the most reliable biometric authentication methods, owing to its uniqueness and permanence. However, the security and confidentiality of the user’s data are key considerations in modern biometric systems. In this study, we describe an intelligent computational technique for automatically validating fingerprints for identification and verification purposes. The feature vector is created by fusing Gabor filtering features with deep learning techniques like the faster region-based convolutional neural network (Faster R-CNN). This study uses linear and decision-based median filtering (DBMF) techniques to minimize visual impulse noise. Faster-R-CNN with DBMF was applied to the feature vectors to reduce overfitting problems while improving classification precision and reliability. For fingerprint matching, the Euclidean distance between the associated Harris-SURF feature vectors of two feature points is used to measure feature-matching similarity between two fingerprint images. Furthermore, for fine-tuned matching an iterative technique known as RANSAC (Random Sample Consensus) is used. The experimental results collected from the public-domain fingerprint databases FVC-2002 DB1 and FVC-2000 DB1 show that the proposed design is viable and performs well with an accuracy of 99.43%, MSE value of 43.321%, and an execution time of 3.102 ms which was more exact than existing models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552300226X",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biometrics",
      "Computer science",
      "Convolutional neural network",
      "Euclidean distance",
      "Feature (linguistics)",
      "Feature vector",
      "Fingerprint (computing)",
      "Linguistics",
      "Overfitting",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Kumar Jain",
        "given_name": "Deepak"
      },
      {
        "surname": "Neelakandan",
        "given_name": "S."
      },
      {
        "surname": "Vidyarthi",
        "given_name": "Ankit"
      },
      {
        "surname": "Gupta",
        "given_name": "Deepak"
      }
    ]
  },
  {
    "title": "Determining the trustworthiness of DNNs in classification tasks using generalized feature-based confidence metric",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109683",
    "abstract": "Determining the confidence of Deep Neural Networks in predictions is crucial for building reliable and robust systems. However, it has received minor attention among other areas related to Deep Learning. The confidence of DNNs in predictions is highly correlated with their ability in feature extraction. Consequently, a more robust feature extractor in DNNs leads to a more confident and trustworthy model. In this study, a method is designed in order to determine the trustworthiness of DNNs based on the quality of their feature extraction components. The concept of feature quality is defined based on the models’ confidence in predictions. In a situation where two DNNs have approximately the same accuracy, the superior model has more confidence in its predictions. Hence, it is less influenced by overfitting, making it more robust and reliable in unseen and noisy environments. Determining such a model is not always possible with the well-known accuracy metric. Accordingly, a novel metric named Generalized Feature-Based Confidence Metric is proposed, which is capable of profoundly evaluating the models’ confidence in predictions. It analyzes layer-by-layer feature vectors generated by DNNs and evaluates their quality. Altogether, these utilities boost assessing and comparing different models with varying widths and depths, improving them, and picking the best one. The practicality of the proposed method and metric is investigated through four significantly diverse case studies and empirically proved. Three of them are reputable benchmarking datasets, namely, CIFAR-10, CIFAR-100, and Fashion-MNIST. Moreover, a new high-quality dataset for the Hand Rubbing problem (made by the authors) is used to analyze the proposed method’s performance in a real-world application. Overall, the proposed metric is able to distinguish between different models from about 1% to 8% in terms of confidence in predictions where the models possess almost the same accuracy (0.5% difference or lower).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323003813",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmarking",
      "Business",
      "Computer science",
      "Data mining",
      "Economics",
      "Engineering",
      "Epistemology",
      "Feature (linguistics)",
      "Feature extraction",
      "Linguistics",
      "MNIST database",
      "Machine learning",
      "Management",
      "Marketing",
      "Metric (unit)",
      "Operations management",
      "Overfitting",
      "Pattern recognition (psychology)",
      "Performance metric",
      "Philosophy",
      "Quality (philosophy)"
    ],
    "authors": [
      {
        "surname": "Haghpanah",
        "given_name": "Mohammad Amin"
      },
      {
        "surname": "Tale Masouleh",
        "given_name": "Mehdi"
      },
      {
        "surname": "Kalhor",
        "given_name": "Ahmad"
      }
    ]
  },
  {
    "title": "Cross domain 2D-3D descriptor matching for unconstrained 6-DOF pose estimation",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109655",
    "abstract": "This paper presents a novel approach for cross-domain descriptor matching between 2D and 3D modalities. The 2D-3D matching is applied to localize 2D images in 3D point clouds. Direct cross-domain matching allows our technique to localize images in any type of 3D point cloud without any constraints on the nature or mechanism by which it is obtained. We propose a learning based framework, called Desc-Matcher, to directly match features between the two modalities. A dataset of 2D and 3D features with corresponding locations in images and point clouds is generated to train the Desc-Matcher. To estimate the pose of an image in any 3D cloud, keypoints and feature descriptors are extracted from the query image and the point cloud. The trained Desc-Matcher is then used to match the features from the image and the point cloud. A robust pose estimator is used to predict the location and orientation of the query image from the corresponding positions of the matched 2D and 3D features. We carried out an extensive evaluation of the proposed method for indoor and outdoor scenarios and with different types of point clouds to verify the feasibility of our approach. Experimental results show that the proposed approach can reliably estimate the 6-DOF poses of query cameras in any type of 3D point cloud with high precision. We achieved average median errors of 1.09 c m / 0 . 27 ∘ and 19 c m / 0 . 39 ∘ on the Stanford and Cambridge datasets, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323003564",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Domain (mathematical analysis)",
      "Estimator",
      "Feature (linguistics)",
      "Geometry",
      "Image (mathematics)",
      "Linguistics",
      "Matching (statistics)",
      "Mathematical analysis",
      "Mathematics",
      "Orientation (vector space)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Point (geometry)",
      "Point cloud",
      "Pose",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Nadeem",
        "given_name": "Uzair"
      },
      {
        "surname": "Bennamoun",
        "given_name": "Mohammed"
      },
      {
        "surname": "Togneri",
        "given_name": "Roberto"
      },
      {
        "surname": "Sohel",
        "given_name": "Ferdous"
      },
      {
        "surname": "Miri Rekavandi",
        "given_name": "Aref"
      },
      {
        "surname": "Boussaid",
        "given_name": "Farid"
      }
    ]
  },
  {
    "title": "Joint A-SNN: Joint training of artificial and spiking neural networks via self-Distillation and weight factorization",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109639",
    "abstract": "Emerged as a biology-inspired method, Spiking Neural Networks (SNNs) mimic the spiking nature of brain neurons and have received lots of research attention. SNNs deal with binary spikes as their activation and therefore derive extreme energy efficiency on hardware. However, it also leads to an intrinsic obstacle that training SNNs from scratch requires a re-definition of the firing function for computing gradient. Artificial Neural Networks (ANNs), however, are fully differentiable to be trained with gradient descent. In this paper, we propose a joint training framework of ANN and SNN, in which the ANN can guide the SNN’s optimization. This joint framework contains two parts: First, the knowledge inside ANN is distilled to SNN by using multiple branches from the networks. Second, we restrict the parameters of ANN and SNN, where they share partial parameters and learn different singular weights. Extensive experiments over several widely used network structures show that our method consistently outperforms many other state-of-the-art training methods. For example, on the CIFAR100 classification task, the spiking ResNet-18 model trained by our method can reach to 77.39% top-1 accuracy with only 4 time steps.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323003400",
    "keywords": [
      "Architectural engineering",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Engineering",
      "Gradient descent",
      "Joint (building)",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Spiking neural network"
    ],
    "authors": [
      {
        "surname": "Guo",
        "given_name": "Yufei"
      },
      {
        "surname": "Peng",
        "given_name": "Weihang"
      },
      {
        "surname": "Chen",
        "given_name": "Yuanpei"
      },
      {
        "surname": "Zhang",
        "given_name": "Liwen"
      },
      {
        "surname": "Liu",
        "given_name": "Xiaode"
      },
      {
        "surname": "Huang",
        "given_name": "Xuhui"
      },
      {
        "surname": "Ma",
        "given_name": "Zhe"
      }
    ]
  },
  {
    "title": "A k nearest neighbour ensemble via extended neighbourhood rule and feature subsets",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109641",
    "abstract": "k NN based ensemble methods minimise the effect of outliers by identifying a set of data points in the given feature space that are nearest to an unseen observation in order to predict its response by using majority voting. The ordinary ensembles based on k NN find out the k nearest observations in a region (bounded by a sphere) based on a predefined value of k . This scenario, however, might not work in situations where the test observation follows the pattern of the closest data points with the same class that lie on a certain path not contained in the given sphere. This paper proposes a k nearest neighbour ensemble where the neighbours are determined in k steps. Starting from the first nearest observation of the test point, the algorithm identifies a single observation that is closest to the observation at the previous step. At each base learner in the ensemble, this search is extended to k steps on a random bootstrap sample with a random subset of features selected from the feature space. The final predicted class of the test point is determined by using a majority vote in the predicted classes given by all base models. This new ensemble method is applied on 20 benchmark datasets and compared with other classical methods, including k NN based models, in terms of classification accuracy, kappa and Brier score as performance metrics. Boxplots are also utilised to illustrate the difference in the results given by the proposed and other state-of-the-art methods. The proposed method outperformed the considered classical methods in the majority of cases. The proposed method is further assessed through a detailed simulation study.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323003424",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Mathematics",
      "Outlier",
      "Pattern recognition (psychology)",
      "k-nearest neighbors algorithm"
    ],
    "authors": [
      {
        "surname": "Ali",
        "given_name": "Amjad"
      },
      {
        "surname": "Hamraz",
        "given_name": "Muhammad"
      },
      {
        "surname": "Gul",
        "given_name": "Naz"
      },
      {
        "surname": "Khan",
        "given_name": "Dost Muhammad"
      },
      {
        "surname": "Aldahmani",
        "given_name": "Saeed"
      },
      {
        "surname": "Khan",
        "given_name": "Zardad"
      }
    ]
  },
  {
    "title": "Multi-semantic hypergraph neural network for effective few-shot learning",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109677",
    "abstract": "Recently, Graph-based Few-Shot Learning (FSL) methods exhibit good generalization by mining relations among few samples with Graph Neural Networks. However, most Graph-based FSL methods consider only binary relations and ignore the multi-semantic information of the global context knowledge. We propose a framework of Multi-Semantic Hypergraph for FSL (MSH-FSL) to explore complex latent high-order multi-semantic relations among the few samples. By mining the complex relationship structure of multi-node and multi-semantics, more refined feature representation can be learned, which yields better classification robustness. Specifically, we first construct a novel Multi-Semantic Hypergraph by obtaining associated instances with different semantic features via orthogonal mapping. With the constructed hypergraph, we then develop the Hyergraph Neural Network along with a novel multi-generation hypergraph message passing so as to better leverage the complex latent semantic relations among samples. Finally, after a number of generations, the hyper-node representations embedded in the learned hypergraph become more accurate for obtaining few-shot prediction. In the 5-way 1-shot task of ResNet-12 on mini-Imagenet dataset, the multi-semantic hypergraph outperforms single-semantic graph by 3.1%, and with the proposed semantic-distribution message passing, the improvement can further reach 6.1%.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323003783",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Discrete mathematics",
      "Graph",
      "Hypergraph",
      "Leverage (statistics)",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Programming language",
      "Semantics (computer science)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Hao"
      },
      {
        "surname": "Li",
        "given_name": "Linyan"
      },
      {
        "surname": "Hu",
        "given_name": "Fuyuan"
      },
      {
        "surname": "Lyu",
        "given_name": "Fan"
      },
      {
        "surname": "Zhao",
        "given_name": "Liuqing"
      },
      {
        "surname": "Huang",
        "given_name": "Kaizhu"
      },
      {
        "surname": "Feng",
        "given_name": "Wei"
      },
      {
        "surname": "Xia",
        "given_name": "Zhenping"
      }
    ]
  },
  {
    "title": "Hybrid neural-like P systems with evolutionary channels for multiple brain metastases segmentation",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109651",
    "abstract": "Neural-like P systems are membrane computing models inspired by natural computing. Spiking neurral (SN) P systems, a kind of neural-like P systems, are viewed as third-generation neural network models. Although real neurons have complex structures, classical SN P systems simplify the structures and corresponding mechanisms to stationary two-dimensional graphs and lack related evolution mechanisms on spikes and channels, which limits the real applications of these models. In this paper, we propose a new hybrid SN P system with evolutionary channels (HN P systems), including three new types of rules for dynamically generating or removing one-one and one-many/many-one channels with related evolutions of spikes on the hybrid neuron structures. Two dynamic regulatory factors are also presented on rules to help guide the optimization of the HN P systems automatically. Based on the new P system, a multiple brain metastases (BMs) segmentation model is developed. The experimental results indicate that the proposed models outperform the state-of-the-art methods on the BMs, which have large variations in sizes, positions and shapes, and low contrast with their surroundings. Performances on the head and neck segmentation dataset also verifies the effectiveness of the HN P system.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323003527",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Complex system",
      "Computer science",
      "Deep neural networks",
      "Evolutionary algorithm",
      "Hybrid system",
      "Machine learning",
      "Membrane computing",
      "Neural system",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Xue",
        "given_name": "Jie"
      },
      {
        "surname": "Li",
        "given_name": "Qi"
      },
      {
        "surname": "Liu",
        "given_name": "Xiyu"
      },
      {
        "surname": "Guo",
        "given_name": "Yujie"
      },
      {
        "surname": "Lu",
        "given_name": "Jie"
      },
      {
        "surname": "Song",
        "given_name": "Bosheng"
      },
      {
        "surname": "Huang",
        "given_name": "Pu"
      },
      {
        "surname": "An",
        "given_name": "Qiong"
      },
      {
        "surname": "Gong",
        "given_name": "Guanzhong"
      },
      {
        "surname": "Li",
        "given_name": "Dengwang"
      }
    ]
  },
  {
    "title": "FBN: Federated Bert Network with client-server architecture for cross-lingual signature verification",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109681",
    "abstract": "Online signature verification has a great challenge due to the poor performance of deep learning techniques on cross-lingual datasets under privacy constraints. In this paper, we propose a novel Federated Bert Network (FBN) by embedding the Bidirectional Encoder Representations from Transformers (Bert) into a Federated Learning (FL) framework with client-server architecture. A new Length Alignment Algorithm is employed to unify the signature pairs’ sequence length, and the input representations are fed into the different clients to complete the independent learning of local-models. In addition, the server (coordinator) uses the improved Federated Average Algorithm with Reward-Punishment Mechanism (FedAvgRP) to aggregate these local-models and further generate a global-model. After multiple iterations, the optimal model can be obtained and cross-tested on four datasets (SVC 2004, MCYT-330, BioecurID, and Ours) with skilled forged (random forged) EERs of 7.65% (4.76%), 10.73% (8.46%), 10.09% (7.13%), and 8.28% (5.74%), respectively, far higher than that of the independent learning of state-of-the-art methods. Compared with the domain adaptation and improved FL models, our FBN model performs best in random and skilled forgery scenarios. Moreover, the FedAvgRP algorithm helps our model maintain high performance in the face of data attacks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032300376X",
    "keywords": [
      "Aggregate (composite)",
      "Architecture",
      "Art",
      "Artificial intelligence",
      "Artificial neural network",
      "Composite material",
      "Computer science",
      "Conditional random field",
      "Embedding",
      "Encoder",
      "Federated learning",
      "Geometry",
      "Machine learning",
      "Materials science",
      "Mathematics",
      "Operating system",
      "Random forest",
      "Signature (topology)",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Xie",
        "given_name": "Liyang"
      },
      {
        "surname": "Wu",
        "given_name": "Zhongcheng"
      },
      {
        "surname": "Zhang",
        "given_name": "Xian"
      },
      {
        "surname": "Li",
        "given_name": "Yong"
      }
    ]
  },
  {
    "title": "Robust unsupervised feature selection via data relationship learning",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109676",
    "abstract": "Unsupervised feature selection robust to many outliers is a challenging task. The crucial difficulty is learning a robust subspace, which preserves local structure. The most common solution is to reduce fitting error by applying different robust norms. However, there are three shortcomings. Firstly, they are not robust enough when outliers distributed both randomly and concentratedly are widely present. Secondly, outlier removal is not considered. Thirdly, it is not easy to understand and choose an euclidean distance threshold that decides a sample as an outlier in different scenarios. The first two shortcomings make previous methods fail to achieve their expected learning results, and the third one increases the application difficulty in different fields. To address these issues, a robust unsupervised feature selection via data relationship learning (RUFSDR) is proposed in this paper. Specifically, scores representing the data’s importance will be learned and assigned to each sample. Inliers will be given different positive scores. Outliers will be given 0 such that a subspace, which preserves the local structure better, can be learned without prior knowledge about the distance threshold. The experiments conducted on various datasets with several scenarios show the superiority of RUFSDR.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323003771",
    "keywords": [
      "Anomaly detection",
      "Artificial intelligence",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Data mining",
      "Economics",
      "Euclidean distance",
      "Feature (linguistics)",
      "Feature selection",
      "Linguistics",
      "Machine learning",
      "Management",
      "Outlier",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Sample (material)",
      "Selection (genetic algorithm)",
      "Subspace topology",
      "Task (project management)",
      "Unsupervised learning"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Pei"
      },
      {
        "surname": "Kong",
        "given_name": "Zhaoming"
      },
      {
        "surname": "Xie",
        "given_name": "Mengying"
      },
      {
        "surname": "Yang",
        "given_name": "Xiaowei"
      }
    ]
  },
  {
    "title": "Continuous cross-modal hashing",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109662",
    "abstract": "Generally, multimodal data with new classes arrive continuously in the real world. While advanced cross-modal hashing (CMH) focuses primarily on batch-based data with previously observed classes (ASCs), it disregards the effect of newly arriving classes (ANCs) on hash-code conflicts. In addition, class-level continuous hashing scenarios do not suit themselves well with the generic CMH configuration. To solve the aforementioned issues, we propose a novel framework, called CT-CMH, for the new task of continuous cross-modal hashing. For dealing with ANCs, CMH models require the ability of continuous learning, i.e. they can preserve the knowledge of previously observed data and, more crucially, they can be adapted to unseen data with ANCs. Specifically, we introduce the adaptive weight importance updating (AWIU) mechanism to alleviate the catastrophic forgetting problem of CMH and a new hash-code divergence (HCD) method to eliminate hash-code conflicts between ASCs and ANCs. When CT-CMH is equipped with both AWIU and HCD, it can consistently achieve high retrieval performance. The experiment results and visualization analyses validate the effectiveness of our approach. To the best of our knowledge, we are the first to introduce and implement the task of CCMH for ANCs.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323003631",
    "keywords": [
      "Code (set theory)",
      "Computer science",
      "Computer security",
      "Data mining",
      "Economics",
      "Forgetting",
      "Hash function",
      "Hash table",
      "Linguistics",
      "Management",
      "Philosophy",
      "Programming language",
      "Set (abstract data type)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Zheng",
        "given_name": "Hao"
      },
      {
        "surname": "Wang",
        "given_name": "Jinbao"
      },
      {
        "surname": "Zhen",
        "given_name": "Xiantong"
      },
      {
        "surname": "Song",
        "given_name": "Jingkuan"
      },
      {
        "surname": "Zheng",
        "given_name": "Feng"
      },
      {
        "surname": "Lu",
        "given_name": "Ke"
      },
      {
        "surname": "Qi",
        "given_name": "Guo-Jun"
      }
    ]
  },
  {
    "title": "Discrete analytical objects in the body-centered cubic grid",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109693",
    "abstract": "We propose a characterization of discrete analytical spheres, planes and lines in the body-centered cubic (BCC) grid, both in the Cartesian and in the recently proposed alternative compact coordinate system, in which each integer triplet addresses some voxel in the grid. We define spheres and planes through double Diophantine inequalities and investigate their relevant topological features, such as functionality or the interrelation between the thickness of the objects and their connectivity and separation properties. We define lines as the intersection of planes. The number of the planes (up to six) is equal to the number of the pairs of faces of a BCC voxel that are parallel to the line.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323003916",
    "keywords": [
      "Aerospace engineering",
      "Artificial intelligence",
      "Astronomy",
      "Cartesian coordinate system",
      "Combinatorics",
      "Computer science",
      "Diophantine equation",
      "Discrete geometry",
      "Engineering",
      "Geometry",
      "Grid",
      "Integer (computer science)",
      "Intersection (aeronautics)",
      "Line (geometry)",
      "Mathematics",
      "Physics",
      "Programming language",
      "Regular grid",
      "SPHERES",
      "Topology (electrical circuits)",
      "Voxel"
    ],
    "authors": [
      {
        "surname": "Čomić",
        "given_name": "Lidija"
      },
      {
        "surname": "Largeteau-Skapin",
        "given_name": "Gaëlle"
      },
      {
        "surname": "Zrour",
        "given_name": "Rita"
      },
      {
        "surname": "Biswas",
        "given_name": "Ranita"
      },
      {
        "surname": "Andres",
        "given_name": "Eric"
      }
    ]
  },
  {
    "title": "Information bottleneck disentanglement based sparse representation for fair classification",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.08.022",
    "abstract": "Unlike current state-of-the-art methods based on data augmentation and adversarial frameworks to solve the fair classification problem, this paper proposes the Information Bottleneck Disentanglement Based Sparse Representation algorithm to ensure the fairness of extracted features. The algorithm assumes that each sample is collaboratively generated by a categorical latent variable and a sensitive or category-irrelevant latent variable. To ensure that these two latent variables correctly capture their corresponding features, we impose Information Bottleneck Disentanglement constraints on them. Specifically, the Information Bottleneck is used to obtain more relevant information while compressing the amount of information. Simultaneously, a mutual information minimization constraint enhances their independence. Additionally, by employing the sparse structure, we can further disentangle the discriminative and sensitive information under supervision guidance. Therefore, the algorithm can improve the discrimination of the categorical latent variable as much as possible without extracting the sensitive data into it. Extensive experiments on five public and challenging benchmark datasets reveal the effectiveness of our proposed method, demonstrating that it can ensure fairness without sacrificing classification performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523002489",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Bottleneck",
      "Categorical variable",
      "Computer science",
      "Constraint (computer-aided design)",
      "Data mining",
      "Discriminative model",
      "Embedded system",
      "Geodesy",
      "Geography",
      "Geometry",
      "Independence (probability theory)",
      "Information bottleneck method",
      "Latent variable",
      "Law",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Mutual information",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Statistics",
      "Variable (mathematics)"
    ],
    "authors": [
      {
        "surname": "Lu",
        "given_name": "Xiongbo"
      },
      {
        "surname": "Rong",
        "given_name": "Yi"
      },
      {
        "surname": "Chen",
        "given_name": "Yaxiong"
      },
      {
        "surname": "Xiong",
        "given_name": "Shengwu"
      }
    ]
  },
  {
    "title": "Single-/Multi-Source Domain Adaptation via domain separation: A simple but effective method",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.09.005",
    "abstract": "Domain adaptation (DA) aims to reduce knowledge gap between domains and improve the prediction ability of models in the target domain. However, the representations learned from feature extraction network often contain redundant information, which is harmful to domain alignment. In addition, many methods only focus on either single-source DA task or multi-source DA task, which limits their real-world applications. In this paper, we propose a simple but effective method called Category and Domain Features Augmentation (CDFA), which consists of two components: Contrastive Classifier Network (CCN) and Domain-specific Learning Network (DSLN). CDFA can remove the specific representation at the feature extraction stage to alleviate transfer difficulty, where CCN is used to increase the probabilistic output of samples and avoid misclassification, and DSLN facilitates the separation of redundant information from all representations by learning domain-specific representations. Empirical evaluations on several cross-domain benchmarks under single-source and multi-source DA scenarios illustrate the competitive performance of CDFA with respect to the state-of-the-art.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523002556",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Domain (mathematical analysis)",
      "Domain adaptation",
      "Feature extraction",
      "Law",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Probabilistic logic",
      "Representation (politics)",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Cai",
        "given_name": "Ziyun"
      },
      {
        "surname": "Zhang",
        "given_name": "Dandan"
      },
      {
        "surname": "Zhang",
        "given_name": "Tengfei"
      },
      {
        "surname": "Hu",
        "given_name": "Changhui"
      },
      {
        "surname": "Jing",
        "given_name": "Xiao-Yuan"
      }
    ]
  },
  {
    "title": "Extending version-space theory to multi-label active learning with imbalanced data",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109690",
    "abstract": "Version space, defined as the subset of the hypothesis space consistent with the training samples, is an important concept in supervised learning. It has been successfully applied for evaluating the informativeness of unlabeled samples in traditional single-label active learning. Specifically, the most inconsistent samples among the version space members can reduce the size of the version space as fast as possible, these samples are given high priority for domain expert annotation, thereby the learner can construct a high-performance classifier by labeling as few samples as possible. We point out that the concept of version space has not been extended to multi-label environments yet, which hinders its application in multi-label active learning. This paper makes an attempt to extend the version space theory from single-label scenario to multi-label scenario, builds up a spatial structure for the multi-label version space, generalizes it from finite case to infinite case, puts forward a simplified representation for it and accordingly proposes a new multi-label active learning algorithm. Moreover, considering the imbalance issue in multi-label data, the algorithm is further improved by allocating different annotation numbers to the labels. Experimental comparisons verify the feasibility and effectiveness of the proposed methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323003886",
    "keywords": [
      "Active learning (machine learning)",
      "Annotation",
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Construct (python library)",
      "Geometry",
      "Law",
      "Machine learning",
      "Mathematics",
      "Multi-label classification",
      "Operating system",
      "Point (geometry)",
      "Political science",
      "Politics",
      "Programming language",
      "Representation (politics)",
      "Space (punctuation)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Ran"
      },
      {
        "surname": "Chen",
        "given_name": "Shuyue"
      },
      {
        "surname": "Yu",
        "given_name": "Yu"
      }
    ]
  },
  {
    "title": "AITST—Affective EEG-based person identification via interrelated temporal–spatial transformer",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.08.010",
    "abstract": "Compared with other biometrics, such as fingerprints and irises, EEG signals contain richer identity information due to their high subject dependence. In this study, we proposed an affective EEG-based person identification by using an affective interrelated temporal–spatial transformer (AITST) to overcome the influence of different emotional states in affective EEG-based person identification. The AITST contains an interrelated temporal–spatial attention that can preserve the interrelationships of temporal and spatial domains while extracting temporal and spatial features of the EEG signal. We evaluated the proposed AITST on the public emotional database DEAP. The experimental results show that our AITST can reach an average recognition accuracy of 98.94±0.04% in the low valence, low arousal state, 99.06±0.02% in low valence, high arousal state, 98.97±0.02% in high valence, low arousal state, and 99.21±0.03% in the high valence, high arousal state, significantly outperforming the other compared models and overcoming the influence of different emotional states. Furthermore, we explored the contribution of different frequency bands, different emotional states and different features of EEG for EEG-based person identification. The results showed that gamma band of EEG, power spectrum density (PSD), and high arousal state have the greatest contribution to affective EEG-based person identification. Source code is available at https://github.com/JerryKingQAQ/AITST.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523002301",
    "keywords": [
      "Arousal",
      "Artificial intelligence",
      "Biometrics",
      "Cognition",
      "Cognitive psychology",
      "Computer science",
      "Electroencephalography",
      "Emotional valence",
      "International Affective Picture System",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Physics",
      "Psychology",
      "Quantum mechanics",
      "Social psychology",
      "Speech recognition",
      "Valence (chemistry)"
    ],
    "authors": [
      {
        "surname": "Cai",
        "given_name": "Honghua"
      },
      {
        "surname": "Jin",
        "given_name": "Jiarui"
      },
      {
        "surname": "Wang",
        "given_name": "Haoyu"
      },
      {
        "surname": "Li",
        "given_name": "Liujiang"
      },
      {
        "surname": "Huang",
        "given_name": "Yucui"
      },
      {
        "surname": "Pan",
        "given_name": "Jiahui"
      }
    ]
  },
  {
    "title": "Features kept generative adversarial network data augmentation strategy for hyperspectral image classification",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109701",
    "abstract": "In recent years, significant breakthroughs have been achieved in hyperspectral image (HSI) processing using deep learning techniques, including classification, object detection, and anomaly detection. However, the practical application of deep learning in HSI processing is limited by challenges such as small-sample size and sample imbalance issues. To mitigate these limitations, we propose a novel data augmentation strategy called Feature-Preserving Generative Adversarial Network Data Augmentation (FPGANDA). What sets our data augmentation strategy apart from existing generative model-based approaches is that we preserve the main spectral bands of HSI data using a newly designed band selection method. Additionally, our proposed generative model generates synthetic spectral bands, which are combined with the real spectral bands using a mixture strategy to create augmented data. This approach ensures that the augmented data retain the main features of the original data while also incorporating diverse features from the generated data. We evaluate our method on three different HSI datasets, comparing it with state-of-the-art techniques. Experimental results demonstrate that our proposed method significantly improves classification performance in most scenes and exhibits remarkable compatibility.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323003990",
    "keywords": [
      "Anomaly detection",
      "Artificial intelligence",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Deep learning",
      "Generative adversarial network",
      "Generative grammar",
      "Generative model",
      "Hyperspectral imaging",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Sample (material)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Mingyang"
      },
      {
        "surname": "Wang",
        "given_name": "Zhaoyang"
      },
      {
        "surname": "Wang",
        "given_name": "Xiangyu"
      },
      {
        "surname": "Gong",
        "given_name": "Maoguo"
      },
      {
        "surname": "Wu",
        "given_name": "Yue"
      },
      {
        "surname": "Li",
        "given_name": "Hao"
      }
    ]
  },
  {
    "title": "Dataset agnostic document object detection",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109698",
    "abstract": "Localizing document objects such as tables, figures, and equations is a primary step for extracting information from document images. We propose a novel end-to-end trainable deep network, termed Document Object Localization Network (dolnet), for detecting various objects present in the document images. The proposed network is a multi-stage extension of Mask r-cnn with a dual backbone having deformable convolution for detecting document objects with high detection accuracy at a higher IoU threshold. We also empirically evaluate the proposed dolnet on the publicly available benchmark datasets. The proposed DOLNet achieves state-of-the-art performance for most of the bench-mark datasets under various existing experimental environments. Our solution has three important properties: (i) a single trained model dolnet ‡ that performs well across all the popular benchmark datasets, (ii) reports excellent performances across multiple, including with higher IoU thresholds, and (iii) consistently demonstrate the superior quantitative performance by following the same protocol of the recent works for each of the benchmarks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323003965",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Information retrieval",
      "Object (grammar)",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Mondal",
        "given_name": "Ajoy"
      },
      {
        "surname": "Agarwal",
        "given_name": "Madhav"
      },
      {
        "surname": "Jawahar",
        "given_name": "C.V."
      }
    ]
  },
  {
    "title": "Towards accurate dense pedestrian detection via occlusion-prediction aware label assignment and hierarchical-NMS",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.08.019",
    "abstract": "With the development of self-driving technology, pedestrian detection models are becoming more useful and important. Although object detection models based on Convolutional Neural Networks have achieved favorable detection results, due to the occlusion in crowded scenarios, dense pedestrian detection is still a challenging task compared with generic object detection. For a CNN-based object detector, an appropriate label assignment method is significant. In this paper, we propose an effective training sample allocation method called Occlusion-Prediction aware Label Assignment (OPLA) to improve the performance of the detector in crowded scenarios. For those Ground Truths (GTs) without severe occlusion, we directly assign prediction boxes with the best matching quality as the corresponding positive samples. In terms of GTs in heavily crowded areas, we fine-tune the matching quality based on visible ratio to enhance the training sample selection priority of severe occluded GTs. In addition, we reveal the deficiency of currently popular post-processing algorithms, Non-Maximum Suppression (NMS) and Soft-NMS for dense pedestrian detection, and propose Hierarchical-NMS (H-NMS) whose strategy diverges with different Intersection over Union (IoU) levels. Experimental results on Crowdhuman, the largest dense pedestrian detection dataset at present, show our approach optimizes the Average Precision (AP) and Miss Rate (MR) of Fully Convolutional One Stage Object Detection (FCOS) by 4.05% and 5.35% and outperforms many state-of-the-art detectors, respectively. Besides, our method does not bring any increase in calculation during inference.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523002398",
    "keywords": [
      "Aerospace engineering",
      "Artificial intelligence",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Detector",
      "Engineering",
      "Inference",
      "Intersection (aeronautics)",
      "Matching (statistics)",
      "Mathematics",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Pedestrian",
      "Pedestrian detection",
      "Sample (material)",
      "Statistics",
      "Systems engineering",
      "Task (project management)",
      "Telecommunications",
      "Transport engineering"
    ],
    "authors": [
      {
        "surname": "He",
        "given_name": "Haoyang"
      },
      {
        "surname": "Li",
        "given_name": "Zhishan"
      },
      {
        "surname": "Tian",
        "given_name": "Guanzhong"
      },
      {
        "surname": "Chen",
        "given_name": "Hongxu"
      },
      {
        "surname": "Xie",
        "given_name": "Lei"
      },
      {
        "surname": "Lu",
        "given_name": "Shan"
      },
      {
        "surname": "Su",
        "given_name": "Hongye"
      }
    ]
  },
  {
    "title": "Mobile image restoration via prior quantization",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.08.017",
    "abstract": "In the photograph of mobile terminal, image degradation is a multivariate problem, where the spectral of the scene, the lens imperfections, the sensor noise, and the field of view together contribute to the results. Besides eliminating it at the hardware level, the post-processing system, which utilizes various prior information, is significant for correction. However, due to the content differences among priors, the pipeline that directly aligns these factors shows limited efficiency and unoptimized restoration. Here, we propose a prior quantization model to correct the degradation introduced in the image formation pipeline. To integrate the multivariate messages, we encode various priors into a latent space and quantify them by the learnable codebooks. After quantization, the prior codes are fused with the image restoration branch to realize targeted optical degradation correction. Moreover, we propose a comprehensive synthetic flow to acquire data pairs in a relative low computational overhead. Comprehensive experiments demonstrate the flexibility of the proposed method and validate its potential to accomplish targeted restoration for mass-produced mobile terminals. Furthermore, our model promises to analyze the influence of various priors and the degradation of devices, which is helpful for joint soft-hardware design.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523002374",
    "keywords": [
      "Artificial intelligence",
      "Bayesian probability",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "ENCODE",
      "Gene",
      "Image (mathematics)",
      "Image processing",
      "Image restoration",
      "Operating system",
      "Overhead (engineering)",
      "Pipeline (software)",
      "Prior probability",
      "Programming language",
      "Quantization (signal processing)"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Shiqi"
      },
      {
        "surname": "Zhou",
        "given_name": "Jingwen"
      },
      {
        "surname": "Li",
        "given_name": "Menghao"
      },
      {
        "surname": "Chen",
        "given_name": "Yueting"
      },
      {
        "surname": "Jiang",
        "given_name": "Tingting"
      }
    ]
  },
  {
    "title": "Hyperspectral image denoising via spectral noise distribution bootstrap",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109699",
    "abstract": "Hyperspectral image (HSI) denoising is an ill-posed problem, leading to integrating proper prior knowledge about hyperspectral noise is critical to developing an efficient denoising method. Most existing methods share a common assumption that all bands have equal noise intensity. However, such assumption runs counter to the practical HSIs, leading to unpleasant denoising results. To tackle this, we intend to investigate the intrinsic properties of real HSI noise in the spectral dimension and construct a novel denoising framework bootstrapping by spectral noise distribution N ^ , termed N ^ -Net. On the one hand, we develop dense and sparse recurrent calculations, exploiting intrinsic properties of HSI noise (i.e., diversity, dense dependency, and global sparsity) to estimate spectral noise distribution. On the other hand, having the estimated spectral noise distribution, we develop a bootstrap mechanism with a repetitive emphasis on its guidance for subsequent spatial noise separation and clean HSI recovery, ensuring a more delicate denoising effect. In particular, we verify that the proposed denoising framework can achieve promising denoising performances due to the merit of spectral noise distribution bootstrapping, which also promotes new insights for future related research. The code is avaliable at https://github.com/EtPan/N-Net.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323003977",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Bootstrapping (finance)",
      "Computer science",
      "Econometrics",
      "Hyperspectral imaging",
      "Image (mathematics)",
      "Mathematics",
      "Noise (video)",
      "Noise reduction",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Pan",
        "given_name": "Erting"
      },
      {
        "surname": "Ma",
        "given_name": "Yong"
      },
      {
        "surname": "Mei",
        "given_name": "Xiaoguang"
      },
      {
        "surname": "Fan",
        "given_name": "Fan"
      },
      {
        "surname": "Ma",
        "given_name": "Jiayi"
      }
    ]
  },
  {
    "title": "Hypersphere guided embedding for masked face recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.08.014",
    "abstract": "Hypersphere Guided Embedding for Masked Face Recognition has been proposed to address the problem encountered in the Masked Face Recognition task, which arises due to non-biological information from occlusions. While some existing algorithms prefer to digesting the existence of masks by probing and covering, others aim to integrate face recognition and masked face recognition tasks into a unified solution domain. In this paper, we propose a framework to enable existing methods to accommodate multiple data distributions by orthogonal subspaces. Specifically, We introduce constraints on multiple hypersphere manifolds via Multi-Center Loss and employ a Spatial Split Strategy to ensure the orthogonality of base vectors associated with different hypersphere manifolds, corresponding to distinct distribution. Our method is extensively evaluated on publicly available datasets on face recognition, mask face recognition and occlusion, demonstrating promising performance. Our code is available on an anonymous website: https://github.com/CaptainKai/HE_MFR.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523002349",
    "keywords": [
      "Artificial intelligence",
      "Code (set theory)",
      "Computer science",
      "Domain (mathematical analysis)",
      "Embedding",
      "Face (sociological concept)",
      "Facial recognition system",
      "Geometry",
      "Hypersphere",
      "Linear subspace",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Orthogonality",
      "Pattern recognition (psychology)",
      "Programming language",
      "Set (abstract data type)",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Kai"
      },
      {
        "surname": "Zhu",
        "given_name": "Xiaobin"
      },
      {
        "surname": "Chen",
        "given_name": "Song-Lu"
      },
      {
        "surname": "Chen",
        "given_name": "Feng"
      },
      {
        "surname": "Yin",
        "given_name": "Xu-Cheng"
      },
      {
        "surname": "Chen",
        "given_name": "Lei"
      }
    ]
  },
  {
    "title": "Style transformation super-resolution GAN for extremely small infrared target image",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.08.013",
    "abstract": "With the development of generative adversarial networks, the super-resolution technique of reconstructing a high-resolution image from a low-resolution has achieved excellent resolution results. However, small, low-resolution images are widespread, such as images taken by a thermal camera or with a lens far from the target. Extremely small target image super-resolution is a challenging problem. The main reason is that the small infrared target has fewer pixels and weaker features. The current optimization methods for the tiny target are mainly based on multi-scale feature fusion or super-resolution enhancement. The low-resolution images characterizing small targets are usually obtained by downsampling with high-resolution images during training, which is different from the style of the tiny target in actual detection applications, resulting in poor resolution. In order to solve the problem, we propose a new resolution network: Style Transformation Super-Resolution Generative Adversarial Network (STSRGAN). It contains two sub-networks: one is style transformation GAN to convert the style of the image, and the other is super-resolution GAN. STSRGAN transforms a blurry infrared small target into a clear target with a distribution similar to the training set. Then the resolution can be increased to get a better enhancement effect. The discriminator distinguishes whether the input comes from the generator or the actual image to assist in generating a better super-resolution image. Meanwhile, we produced an infrared Unmanned Aerial Vehicle (UAV) small target dataset with target pixels below 16 × 16. Our method proves better resolution enhancement of small IR targets and shows superior performance over other methods through experiments.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523002337",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Detector",
      "Digital image processing",
      "Discriminator",
      "Feature (linguistics)",
      "Gene",
      "Image (mathematics)",
      "Image processing",
      "Image resolution",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pixel",
      "Resolution (logic)",
      "Sub-pixel resolution",
      "Telecommunications",
      "Transformation (genetics)",
      "Upsampling"
    ],
    "authors": [
      {
        "surname": "Lee",
        "given_name": "In Ho"
      },
      {
        "surname": "Chung",
        "given_name": "Won Young"
      },
      {
        "surname": "Park",
        "given_name": "Chan Gook"
      }
    ]
  },
  {
    "title": "Class-specific and self-learning local manifold structure for domain adaptation",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109654",
    "abstract": "Domain adaptation (DA) is a powerful technology that allows a classifier trained on a well-labeled source domain to be adapted to an unlabeled target domain with different distributions. Existing DA methods aim to enhance feature transferability by reducing distribution distance, and they often rely on preserving the global discriminative (GD) structure to boost feature discriminability. However, strict GD loss may degrade transferability, and poor discriminability may result from wrongly labeled samples. To address these issues, we propose a new approach called class-specific and self-learning local manifold structure (CSSL-LM), to extract more desirable features for better DA effects. Specifically, it draws two data points close with large weight if they are from the same class and close in the original feature space. This approach is more relaxed than GD and thus mitigates the negative effect on transferability. Moreover, CSSL-LM is more robust to wrongly labeled samples than GD since two data points that are wrongly labeled as the same classes have small weight and are not required to be close. Inspired by previous adaptive local manifold learning, we utilize a self-learning mechanism to model CSSL-LM more accurately and reliably, particularly for feature-corrupted samples. Extensive experiments on four DA benchmarks verify that CSSL-LM outperforms some state-of-the-art methods. We also construct DA datasets that are randomly wrongly labeled or feature-corrupted to further evaluate CSSL-LM’s robustness.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323003552",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Classifier (UML)",
      "Computer science",
      "Discriminative model",
      "Domain adaptation",
      "Feature (linguistics)",
      "Feature vector",
      "Gene",
      "Linguistics",
      "Logit",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Robustness (evolution)",
      "Transferability"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Wei"
      },
      {
        "surname": "Wang",
        "given_name": "Mengzhu"
      },
      {
        "surname": "Dong",
        "given_name": "Xiao"
      },
      {
        "surname": "Lan",
        "given_name": "Long"
      },
      {
        "surname": "Zu",
        "given_name": "Quannan"
      },
      {
        "surname": "Zhang",
        "given_name": "Xiang"
      },
      {
        "surname": "Wang",
        "given_name": "Cong"
      }
    ]
  },
  {
    "title": "Correlated and individual feature learning with contrast-enhanced MR for malignancy characterization of hepatocellular carcinoma",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109638",
    "abstract": "Malignancy characterization of hepatocellular carcinoma (HCC) is of great importance in patient management and prognosis prediction. In this study, we propose an end-to-end correlated and individual feature learning framework to characterize the malignancy of HCC from Contrast-enhanced MR. From the phases of pre-contrast, arterial and portal venous, our framework simultaneously and explicitly learns both the shareable and phase-specific features that are discriminative to malignancy grades. We evaluate our method on the Contrast enhanced MR of 112 consecutive patients with 117 histologically proven HCCs. Experimental results demonstrate that arterial phase yields better results than portal vein and pre-contrast phase. Furthermore, phase specific components show better discriminant ability than the shareable components. Finally, combining the extracted shareable and individual features components has yielded significantly better performance than traditional feature fusion methods. We also conduct t-SNE analysis and feature scoring analysis to qualitatively assess the effectiveness of the proposed method for malignancy characterization.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323003394",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Contrast (vision)",
      "Discriminative model",
      "Feature (linguistics)",
      "Hepatocellular carcinoma",
      "Internal medicine",
      "Linear discriminant analysis",
      "Linguistics",
      "Malignancy",
      "Medicine",
      "Pathology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Radiology"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Yunling"
      },
      {
        "surname": "Li",
        "given_name": "Shangxuan"
      },
      {
        "surname": "Ju",
        "given_name": "Hanqiu"
      },
      {
        "surname": "Harada",
        "given_name": "Tatsuya"
      },
      {
        "surname": "Zhang",
        "given_name": "Honglai"
      },
      {
        "surname": "Duan",
        "given_name": "Ting"
      },
      {
        "surname": "Wang",
        "given_name": "Guangyi"
      },
      {
        "surname": "Zhang",
        "given_name": "Lijuan"
      },
      {
        "surname": "Gu",
        "given_name": "Lin"
      },
      {
        "surname": "Zhou",
        "given_name": "Wu"
      }
    ]
  },
  {
    "title": "eX-ViT: A Novel explainable vision transformer for weakly supervised semantic segmentation",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109666",
    "abstract": "Recently vision transformer models have become prominent models for a multitude of vision tasks. These models, however, are usually opaque with weak feature interpretability, making their predictions inaccessible to the users. While there has been a surge of interest in the development of post-hoc solutions that explain model decisions, these methods can not be broadly applied to different transformer architectures, as rules for interpretability have to change accordingly based on the heterogeneity of data and model structures. Moreover, there is no method currently built for an intrinsically interpretable transformer, which is able to explain its reasoning process and provide a faithful explanation. To close these crucial gaps, we propose a novel vision transformer dubbed the eXplainable Vision Transformer (eX-ViT), an intrinsically interpretable transformer model that is able to jointly discover robust interpretable features and perform the prediction. Specifically, eX-ViT is composed of the Explainable Multi-Head Attention (E-MHA) module, the Attribute-guided Explainer (AttE) module with the self-supervised attribute-guided loss. The E-MHA tailors explainable attention weights that are able to learn semantically interpretable representations from tokens in terms of model decisions with noise robustness. Meanwhile, AttE is proposed to encode discriminative attribute features for the target object through diverse attribute discovery, which constitutes faithful evidence for the model predictions. Additionally, we have developed a self-supervised attribute-guided loss for our eX-ViT architecture, which utilizes both the attribute discriminability mechanism and the attribute diversity mechanism to enhance the quality of learned representations. As a result, the proposed eX-ViT model can produce faithful and robust interpretations with a variety of learned attributes. To verify and evaluate our method, we apply the eX-ViT to several weakly supervised semantic segmentation (WSSS) tasks, since these tasks typically rely on accurate visual explanations to extract object localization maps. Particularly, the explanation results obtained via eX-ViT are regarded as pseudo segmentation labels to train WSSS models. Comprehensive simulation results illustrate that our proposed eX-ViT model achieves comparable performance to supervised baselines, while surpassing the accuracy and interpretability of state-of-the-art black-box methods using only image-level labels.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323003679",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Discriminative model",
      "Interpretability",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Segmentation",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Yu",
        "given_name": "Lu"
      },
      {
        "surname": "Xiang",
        "given_name": "Wei"
      },
      {
        "surname": "Fang",
        "given_name": "Juan"
      },
      {
        "surname": "Chen",
        "given_name": "Yi-Ping Phoebe"
      },
      {
        "surname": "Chi",
        "given_name": "Lianhua"
      }
    ]
  },
  {
    "title": "Triplet teaching graph contrastive networks with self-evolving adaptive augmentation",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109687",
    "abstract": "Unsupervised graph contrastive learning has recently emerged as the solution to the crisis of label information scarcity for graph data in the real world. However, from the general paradigm of graph contrastive learning, most of the existing methods are still flawed in the design and use of augmented views and the design of contrastive targets. Therefore, the works on how to generate reasonable augmented views and utilize them canonically and how to construct efficient and comprehensive contrastive objectives are very meaningful. Based on the teaching concept, this paper proposes a new triplet teaching graph contrastive network with self-evolving adaptive augmentation. Firstly, after carefully analyzing the internal relationships between different augmented perspectives, we present a triple teaching graph neural network framework based on the improved triplet idea. It creates contrastive objectives depending on different contrastive angle levels, providing thorough guidance for graph encoders. Secondly, a self-evolving adaptive graph augmentation scheme based on topology and feature information is proposed. It is worth mentioning that with the continuous deepening of the training process, the scheme can utilize the learnable self-attention mechanism to constantly supply our network framework with an increasing number of reliable augmented views as input. Finally, when designing the contrastive objectives, we introduce a stochastic hybrid module to mine the unexploited information, which opportunely complements the contrastive sample space formed by our network framework. Furthermore, extensive experiments on multiple real-world node classification datasets demonstrate that our model can generate better-quality node embedding for downstream tasks. The implementation of this paper is available at https://github.com/PaperMiao/T-GCSA.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323003850",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Embedding",
      "Graph",
      "Machine learning",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Miao",
        "given_name": "Jiaxing"
      },
      {
        "surname": "Cao",
        "given_name": "Feilong"
      },
      {
        "surname": "Li",
        "given_name": "Ming"
      },
      {
        "surname": "Yang",
        "given_name": "Bing"
      },
      {
        "surname": "Ye",
        "given_name": "Hailiang"
      }
    ]
  },
  {
    "title": "GlobalAP: Global average precision optimization for person re-identification",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109682",
    "abstract": "Average Precision (AP) measures the overall performance on the Person Re-Identification (ReID) task. Optimizing AP using all instances in the training set is accordingly an excellent choice for learning a discriminative ReID model. However, exploiting this method directly is unacceptable in practice due to the high cost of computation on the entire dataset. To this end, this paper proposes an effective and easy-to-use approach called GlobalAP that optimizes AP globally at negligible computational cost. More specifically, GlobalAP adopts a memory module to acquire the embedding features of all instances in the training set. To reduce the required computational complexity, GlobalAP utilizes only a few instances with high similarities to the query, to compute AP; this is because we observe that only these instances significantly affect AP and model optimization. Moreover, we propose to gradually increase the difficulty of GlobalAP to further encourage intra-class compactness and inter-class separability. Ultimately, GlobalAP can globally optimize AP and dramatically boost the model performance at negligible computational cost. We evaluate GlobalAP on six large-scale ReID datasets. Experimental results show that GlobalAP exhibits obvious advantages in terms of both computational efficiency and ReID accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323003795",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Class (philosophy)",
      "Computation",
      "Computational complexity theory",
      "Computer science",
      "Discriminative model",
      "Economics",
      "Embedding",
      "Identification (biology)",
      "Machine learning",
      "Management",
      "Programming language",
      "Set (abstract data type)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Yifei"
      },
      {
        "surname": "Liang",
        "given_name": "Yaling"
      },
      {
        "surname": "Wang",
        "given_name": "Pengfei"
      },
      {
        "surname": "Chen",
        "given_name": "Ziheng"
      },
      {
        "surname": "Ding",
        "given_name": "Changxing"
      }
    ]
  },
  {
    "title": "Outlier detection: How to Select k for k -nearest-neighbors-based outlier detectors",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.08.020",
    "abstract": "Unsupervised k -nearest-neighbor-based outlier detectors play a vital role in data science research. However, the detectors’ performance relies on the choice of the parameter k . However, autonomous selection of the optimal k is poorly documented in literature as it is very challenging. Conventional methods prove ineffective and lack universality as they fail to account for both application and detector factors simultaneously. This article proposes neighborhood consistency, a new concept which tackles the existing issues of selecting the optimal k by considering both application and detector factors. This concept was used to develop a method termed k finder based on neighborhood consistency (KFC). KFC does not rely on any extra parameter and has linear time complexity. Simulations show that KFC outperformed baselines and had a good generality to different datasets and detectors. The implementation of the proposed methods can be found on www.OutlierNet.com for reproducibility.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523002404",
    "keywords": [
      "Algorithm",
      "Anomaly detection",
      "Artificial intelligence",
      "Computer science",
      "Consistency (knowledge bases)",
      "Data mining",
      "Detector",
      "Generality",
      "Outlier",
      "Physics",
      "Psychology",
      "Psychotherapist",
      "Quantum mechanics",
      "Telecommunications",
      "Universality (dynamical systems)"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Jiawei"
      },
      {
        "surname": "Tan",
        "given_name": "Xu"
      },
      {
        "surname": "Rahardja",
        "given_name": "Sylwan"
      }
    ]
  },
  {
    "title": "End-to-End page-Level assessment of handwritten text recognition",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109695",
    "abstract": "The evaluation of Handwritten Text Recognition (HTR) systems has traditionally used metrics based on the edit distance between HTR and ground truth (GT) transcripts, at both the character and word levels. This is very adequate when the experimental protocol assumes that both GT and HTR text lines are the same, which allows edit distances to be independently computed to each given line. Driven by recent advances in pattern recognition, HTR systems increasingly face the end-to-end page-level transcription of a document, where the precision of locating the different text lines and their corresponding reading order (RO) play a key role. In such a case, the standard metrics do not take into account the inconsistencies that might appear. In this paper, the problem of evaluating HTR systems at the page level is introduced in detail. We analyse the convenience of using a two-fold evaluation, where the transcription accuracy and the RO goodness are considered separately. Different alternatives are proposed, analysed and empirically compared both through partially simulated and through real, full end-to-end experiments. Results support the validity of the proposed two-fold evaluation approach. An important conclusion is that such an evaluation can be adequately achieved by just two simple and well-known metrics: the Word Error Rate (WER), that takes transcription sequentiality into account, and the here re-formulated Bag of Words Word Error Rate (bWER), that ignores order. While the latter directly and very accurately assess intrinsic word recognition errors, the difference between both metrics ( Δ WER) gracefully correlates with the Normalised Spearman’s Foot Rule Distance (NSFD), a metric which explicitly measures RO errors associated with layout analysis flaws. To arrive to these conclusions, we have introduced another metric called Hungarian Word Word Rate (hWER), based on a here proposed regularised version of the Hungarian Algorithm. This metric is shown to be always almost identical to bWER and both bWER and hWER are also almost identical to WER whenever HTR transcripts and GT references are guarantee to be in the same RO.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032300393X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Edit distance",
      "Geometry",
      "Ground truth",
      "Linguistics",
      "Mathematics",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Reading (process)",
      "Speech recognition",
      "Transcription (linguistics)",
      "Word (group theory)",
      "Word error rate",
      "Word recognition"
    ],
    "authors": [
      {
        "surname": "Vidal",
        "given_name": "Enrique"
      },
      {
        "surname": "Toselli",
        "given_name": "Alejandro H."
      },
      {
        "surname": "Ríos-Vila",
        "given_name": "Antonio"
      },
      {
        "surname": "Calvo-Zaragoza",
        "given_name": "Jorge"
      }
    ]
  },
  {
    "title": "A new methodology in constructing no-reference focus quality assessment metrics",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109688",
    "abstract": "This paper proposed a new methodology which converts a full-reference focus quality assessment metric into a no-reference one. The methodology consists of three hypotheses which describe the relationship in focus quality between the original image and its variants. Using the proposed methodology, two no-reference metrics were constructed. The first used Brenner Gradient and the second used a full-reference metric proposed by ourselves. Evaluation was conducted on a public dataset and our own proposed dataset. Comparing with other no-reference metrics, our second one exhibited best performance on both datasets, with calculation time comparable to some fastest metrics considered.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323003862",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Economics",
      "Epistemology",
      "Focus (optics)",
      "Metric (unit)",
      "Operations management",
      "Optics",
      "Philosophy",
      "Physics",
      "Quality (philosophy)",
      "Reference data",
      "Reference model",
      "Software engineering"
    ],
    "authors": [
      {
        "surname": "Song",
        "given_name": "Jie"
      },
      {
        "surname": "Liu",
        "given_name": "Mengjun"
      }
    ]
  },
  {
    "title": "OS-DS tracker: Orientation-variant Siamese 3D tracking with Detection based Sampling",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.08.021",
    "abstract": "LIDAR point clouds are less affected by the weather and possess more depth of field than 2D images. However, sparsity and disorder of point clouds bring challenges for 3D object tracking due to the difficulty in detecting and encoding. In addition, ineffective candidate proposals increase the phenomenon of the loss of target tracking or wrong target tracking. In this paper, we propose an Orientation-variant Siamese 3D object tracking network that utilizes a Detection based Sampling module to generate candidate proposals (OS-DS tracker). The Detection based Sampling module is used to cut the excessive and useless proposals. And the multivariate Gaussian sampling generates the candidate proposals when the objects are not detected. Concretely, we first pre-train PointRCNN Network to globally detect objects from LIDAR point clouds. Then the 3D detected objects are refined by Candidate Regions Sampling to generate candidate proposals. Meanwhile, to make the feature vectors more discriminative, we design an Orientation-variant Siamese Auto-encoder, i . e . , tracking loss regress to the intersection over union (IOU) of the template box and the candidate region boxes. Our method is tested on the KITTI dataset and the SHIP Tracking dataset. Compared with the previous state-of-the-arts, the proposed method outperforms in vessel tracking with 5.2%/2.1% improvement in Success and Precision.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523002416",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Discriminative model",
      "Eye tracking",
      "Feature (linguistics)",
      "Filter (signal processing)",
      "Geometry",
      "Linguistics",
      "Mathematics",
      "Object (grammar)",
      "Object detection",
      "Orientation (vector space)",
      "Pattern recognition (psychology)",
      "Pedagogy",
      "Philosophy",
      "Point cloud",
      "Psychology",
      "Sampling (signal processing)",
      "Tracking (education)",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Qiuyu"
      },
      {
        "surname": "Wang",
        "given_name": "Lipeng"
      },
      {
        "surname": "Li",
        "given_name": "Wanyi"
      },
      {
        "surname": "Meng",
        "given_name": "Hao"
      },
      {
        "surname": "Zhang",
        "given_name": "Wen"
      },
      {
        "surname": "Zhang",
        "given_name": "Zhi"
      },
      {
        "surname": "Wang",
        "given_name": "Peng"
      }
    ]
  },
  {
    "title": "Adversarial image generation by spatial transformation in perceptual colorspaces",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.09.003",
    "abstract": "Deep neural networks are known to be vulnerable to adversarial perturbations. The amount of these perturbations are generally quantified using L p metrics, such as L 0 , L 2 and L ∞ . However, even when the measured perturbations are small, they tend to be noticeable by human observers since L p distance metrics are not representative of human perception. On the other hand, humans are less sensitive to changes in colorspace. In addition, pixel shifts in a constrained neighborhood are hard to notice. Motivated by these observations, we propose a method that creates adversarial examples by applying spatial transformations, which creates adversarial examples by changing the pixel locations independently to chrominance channels of perceptual colorspaces such as Y C b C r and C I E L A B , instead of making an additive perturbation or manipulating pixel values directly. In a targeted white-box attack setting, the proposed method is able to obtain competitive fooling rates with very high confidence. The experimental evaluations show that the proposed method has favorable results in terms of approximate perceptual distance between benign and adversarially generated images. The source code is publicly available at https://github.com/ayberkydn/stadv-torch.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523002519",
    "keywords": [
      "Adversarial system",
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Chrominance",
      "Code (set theory)",
      "Computer science",
      "Computer vision",
      "Deep neural networks",
      "Gene",
      "Image (mathematics)",
      "Luminance",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Perception",
      "Pixel",
      "Programming language",
      "Psychology",
      "Set (abstract data type)",
      "Transformation (genetics)"
    ],
    "authors": [
      {
        "surname": "Aydin",
        "given_name": "Ayberk"
      },
      {
        "surname": "Temizel",
        "given_name": "Alptekin"
      }
    ]
  },
  {
    "title": "Interpreting vulnerabilities of multi-instance learning to adversarial perturbations",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109725",
    "abstract": "Multi-instance learning (MIL) is a recent machine learning paradigm which is immensely useful in various real-life applications, like image analysis, video anomaly detection, text classification, etc. It is well known that most of the existing machine learning classifiers are highly vulnerable to adversarial perturbations. Since MIL is a weakly supervised learning, where information is available for a set of instances, called bag and not for every instance, adversarial perturbations can be fatal. In this paper, we have proposed two adversarial perturbation methods to analyze the effect of adversarial perturbations to interpret the vulnerabilities of MIL methods. Out of the two algorithms, one can be customized for every bag, and the other is a universal one, which can affect all bags in a given data set and thus has some generalizability. Furthermore, through simulations, we have demonstrated the efficacy of the proposed algorithms in fooling state-of-the-art MIL approaches, such that these models make incorrect predictions regarding the label assigned to the bag. Finally, we have discussed, through experiments, about taking care of these kind of adversarial perturbations through a simple strategy. Source codes are available at https://github.com/InkiInki/MI-UAP.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323004235",
    "keywords": [
      "Adversarial machine learning",
      "Adversarial system",
      "Anomaly detection",
      "Artificial intelligence",
      "Computer science",
      "Generalizability theory",
      "Machine learning",
      "Mathematics",
      "Programming language",
      "Set (abstract data type)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Yu-Xuan"
      },
      {
        "surname": "Meng",
        "given_name": "Hua"
      },
      {
        "surname": "Cao",
        "given_name": "Xue-Mei"
      },
      {
        "surname": "Zhou",
        "given_name": "Zhengchun"
      },
      {
        "surname": "Yang",
        "given_name": "Mei"
      },
      {
        "surname": "Adhikary",
        "given_name": "Avik Ranjan"
      }
    ]
  },
  {
    "title": "Generalization capacity of multi-class SVM based on Markovian resampling",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109720",
    "abstract": "The generalization performance of “All-in-one” Multi-class SVM (AIO-MSVM) based on uniformly ergodic Markovian chain (u.e.M.c.) samples is considered. We establish the fast learning rate of AIO-MSVM algorithm with u.e.M.c. samples and prove that AIO-MSVM algorithm with u.e.M.c. samples is consistent. We also propose a novel AIO-MSVM algorithm based on q -times Markovian resampling (AIO-MSVM-MR), and show the numerical investigation on the learning performance of AIO-MSVM-MR based on public datasets. The experimental studies indicate that compared to the classical AIO-MSVM algorithm and other MSVM algorithms, the proposed AIO-MSVM-MR algorithm has not only smaller misclassification rate, but also less sampling and training total time. We present some discussions on the case of unbalanced training samples, the choices of q and two technical parameters, and present some explanations on the learning performance of the proposed algorithm.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323004181",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Generalization",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Resampling",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Dong",
        "given_name": "Zijie"
      },
      {
        "surname": "Xu",
        "given_name": "Chen"
      },
      {
        "surname": "Xu",
        "given_name": "Jie"
      },
      {
        "surname": "Zou",
        "given_name": "Bin"
      },
      {
        "surname": "Zeng",
        "given_name": "Jingjing"
      },
      {
        "surname": "Tang",
        "given_name": "Yuan Yan"
      }
    ]
  },
  {
    "title": "Person re-identification: A retrospective on domain specific open challenges and future trends",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109669",
    "abstract": "Person Re-Identification (Re-ID) is a critical aspect of visual surveillance systems, which aims to automatically recognize and locate individuals across a multi-camera network with non-overlapping fields-of-view. Despite significant progress in recent years through the use of deep learning-based approaches, there remain many vision-related challenges, such as occlusion, pose, background clutter, misalignment, scale, viewpoint, low resolution & illumination, and cross-domain generalization across camera modalities, that hinder the accurate identification of individuals. The majority of the proposed approaches directly or indirectly aim to solve one or multiple of these existing challenges. To further advance the development of Re-ID solutions, a comprehensive review of the current approaches is necessary. However, no focused review currently exists that analyses and highlights specific aspects for further development. To fill this gap, we present a systematic challenge-specific literature survey of about 300 papers published between 2015 and 2022, which reviews Re-ID approaches from a solution-oriented perspective. This survey is the first of its kind to provide an in-depth analysis of the different approaches used to address the various challenges in Re-ID. Furthermore, our review highlights several prominent and diverse research trends in the Re-ID domain. These trends offer a visionary perspective regarding ongoing person Re-ID research, and they may eventually lead to the development of practical real-world solutions. We highlighted the AI ethics that must be followed while developing a Re-ID solution, and recently being practiced as well. Another exciting future dimension of person Re-ID research is the long-term Re-ID, which is still under evolution. Overall, our survey aims to serve as a valuable resource for researchers and practitioners working in the field of Re-ID and to inspire the development of innovative and effective Re-ID solutions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323003709",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Computer science",
      "Data science",
      "Dimension (graph theory)",
      "Domain (mathematical analysis)",
      "Generalization",
      "Identification (biology)",
      "Mathematical analysis",
      "Mathematics",
      "Modalities",
      "Perspective (graphical)",
      "Physics",
      "Pure mathematics",
      "Quantum mechanics",
      "Scale (ratio)",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Zahra",
        "given_name": "Asmat"
      },
      {
        "surname": "Perwaiz",
        "given_name": "Nazia"
      },
      {
        "surname": "Shahzad",
        "given_name": "Muhammad"
      },
      {
        "surname": "Fraz",
        "given_name": "Muhammad Moazam"
      }
    ]
  },
  {
    "title": "GARNet: Global-aware multi-view 3D reconstruction network and the cost-performance tradeoff",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109674",
    "abstract": "Deep learning technology has made great progress in multi-view 3D reconstruction tasks. At present, the mainstream solutions adopt different ways to fusion the features from several views. Among them, attention-based aggregation function performs relatively well and stably, however, it still has an obvious shortcoming the strong independence of each view during predicting the weights for merging leads to a lack of adaption of the global state. In this paper, we propose a global-aware attention-based fusion approach that builds the correlation between each branch and the global feature to provide a comprehensive foundation for weights inference. On the basis of this, we design a complete reconstruction algorithm. Experiments on ShapeNet verify that our method outperforms existing SOTA methods. Furthermore, we propose a view-reduction method based on maximizing diversity and discuss the cost-performance tradeoff of our model to achieve a better performance when facing a heavy input amount and limited computational cost.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323003758",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Deep learning",
      "Evolutionary biology",
      "Feature (linguistics)",
      "Function (biology)",
      "Fusion",
      "Geometry",
      "Independence (probability theory)",
      "Inference",
      "Linguistics",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Reduction (mathematics)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Zhu",
        "given_name": "Zhenwei"
      },
      {
        "surname": "Yang",
        "given_name": "Liying"
      },
      {
        "surname": "Lin",
        "given_name": "Xuxin"
      },
      {
        "surname": "Yang",
        "given_name": "Lin"
      },
      {
        "surname": "Liang",
        "given_name": "Yanyan"
      }
    ]
  },
  {
    "title": "Truncated attention-aware proposal networks with multi-scale dilation for temporal action detection",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109684",
    "abstract": "Detecting actions temporally in untrimmed videos is very challenging, and it accomplishes action classification and localization simultaneously. Capturing the relations among action proposals (i.e., candidate video segments) is of vital importance. While there have been several attempts to encode such relations, they neglect the adverse effects of those irrelevant or negative relations among proposals. Besides, there is a crucial fact that action durations are flexible in videos, which has not been well explored. For the former, we develop a truncated attention mechanism that learns positive proposal relations by dynamically adjusting edge weights of proposal nodes in a graph, and construct the proposal network model using graph convolution networks to suppress disadvantageous relations of proposal pairs by truncating negative attention scores. For the latter, we devise a light multi-scale dilation module shared by all proposals to handle different action durations by enlarging temporal receptive field, thus capturing temporal context to increase the representation capacity of proposals. Unifying these considerations, we present the Multi-scale Dilation based Truncated Attention Proposal Network (MD-TAPN) model for temporal action detection. Our model achieves state-of-the-art performances of detecting actions on two benchmark databases, and especially it outperforms the most competitive method by a significant gain of 3.6% mAP at tIoU0.5 on THUMOS14.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323003825",
    "keywords": [
      "Action (physics)",
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Biochemistry",
      "Biology",
      "Chemistry",
      "Combinatorics",
      "Computer science",
      "Construct (python library)",
      "Context (archaeology)",
      "Convolution (computer science)",
      "Dilation (metric space)",
      "Discriminative model",
      "ENCODE",
      "Expressive power",
      "Gene",
      "Geodesy",
      "Geography",
      "Graph",
      "Law",
      "Mathematics",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Physics",
      "Political science",
      "Politics",
      "Programming language",
      "Quantum mechanics",
      "Representation (politics)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Ping"
      },
      {
        "surname": "Cao",
        "given_name": "Jiachen"
      },
      {
        "surname": "Yuan",
        "given_name": "Li"
      },
      {
        "surname": "Ye",
        "given_name": "Qinghao"
      },
      {
        "surname": "Xu",
        "given_name": "Xianghua"
      }
    ]
  },
  {
    "title": "Region-aware RGB and near-infrared image fusion",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109717",
    "abstract": "This paper proposes a region-aware fusion method, called RaIF, for RGB and near-infrared (NIR) outdoor scenery image fusion. The method is motivated by the observation that current fusion approaches produce gray appearance in overexposed sky regions and distortion in vegetation regions. RaIF generates the region probability maps by exploiting their specific characteristics in the visible and NIR spectra. It recovers the overexposed sky regions by employing the intrinsic channel correlation between RGB and NIR images, and enhances the vegetation regions in an adjustable manner. RaIF formulates image fusion problem as a gradient-domain optimization problem with luminance and chromaticity regularizations. Experimental results validate the superiority of RaIF that produces fused images with improved appearance in the sky and vegetation regions, and achieves the state-of-the-art performance quantitatively and qualitatively. Furthermore, RaIF can act as a refinement module that improves the fusion results of current deep learning based approaches. It is also capable of recovering specular highlight regions other than sky overexposure.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323004156",
    "keywords": [
      "Amplifier",
      "Artificial intelligence",
      "Astrophysics",
      "Bandwidth (computing)",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Distortion (music)",
      "Fusion",
      "Geology",
      "Image (mathematics)",
      "Image fusion",
      "Linguistics",
      "Optics",
      "Philosophy",
      "Physics",
      "Pixel",
      "RGB color model",
      "Remote sensing",
      "Sky",
      "Specular reflection"
    ],
    "authors": [
      {
        "surname": "Ying",
        "given_name": "Jiacheng"
      },
      {
        "surname": "Tong",
        "given_name": "Can"
      },
      {
        "surname": "Sheng",
        "given_name": "Zehua"
      },
      {
        "surname": "Yao",
        "given_name": "Bowen"
      },
      {
        "surname": "Cao",
        "given_name": "Si-Yuan"
      },
      {
        "surname": "Yu",
        "given_name": "Heng"
      },
      {
        "surname": "Shen",
        "given_name": "Hui-Liang"
      }
    ]
  },
  {
    "title": "Attentional prototype inference for few-shot segmentation",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109726",
    "abstract": "This paper aims to address few-shot segmentation. While existing prototype-based methods have achieved considerable success, they suffer from uncertainty and ambiguity caused by limited labeled examples. In this work, we propose attentional prototype inference (API), a probabilistic latent variable framework for few-shot segmentation. We define a global latent variable to represent the prototype of each object category, which we model as a probabilistic distribution. The probabilistic modeling of the prototype enhances the model’s generalization ability by handling the inherent uncertainty caused by limited data and intra-class variations of objects. To further enhance the model, we introduce a local latent variable to represent the attention map of each query image, which enables the model to attend to foreground objects while suppressing the background. The optimization of the proposed model is formulated as a variational Bayesian inference problem, which is established by amortized inference networks. We conduct extensive experiments on four benchmarks, where our proposal obtains at least competitive and often better performance than state-of-the-art prototype-based methods. We also provide comprehensive analyses and ablation studies to gain insight into the effectiveness of our method for few-shot segmentation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323004247",
    "keywords": [
      "Ambiguity",
      "Artificial intelligence",
      "Bayesian inference",
      "Bayesian probability",
      "Computer science",
      "Generalization",
      "Inference",
      "Latent variable",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Probabilistic logic",
      "Programming language",
      "Segmentation",
      "Variable (mathematics)"
    ],
    "authors": [
      {
        "surname": "Sun",
        "given_name": "Haoliang"
      },
      {
        "surname": "Lu",
        "given_name": "Xiankai"
      },
      {
        "surname": "Wang",
        "given_name": "Haochen"
      },
      {
        "surname": "Yin",
        "given_name": "Yilong"
      },
      {
        "surname": "Zhen",
        "given_name": "Xiantong"
      },
      {
        "surname": "Snoek",
        "given_name": "Cees G.M."
      },
      {
        "surname": "Shao",
        "given_name": "Ling"
      }
    ]
  },
  {
    "title": "Lightweight 3D hand pose estimation by cascading CNNs with reinforcement learning",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.09.004",
    "abstract": "This paper proposes a novel strategy for lightweight 3D hand pose estimation. The strategy decomposes the estimation process into feature extraction and feature exploitation, where feature extraction performs dimension reduction on the original input and outputs feature vectors. Feature exploitation is further analyzed and considered as a path optimization problem, and reinforcement learning (RL) is proved to be capable of tackling the problem accurately. A framework cascading convolutional neural networks (CNNs) and RL is next introduced to validate the effectiveness of the proposed strategy, where two different backbones are used to extract features, and RL is extended into continuous space to enhance accuracy. Ablation studies and experiments are carried out on NYU and ICVL datasets using the proposed strategy with continuous RL. The results show that the accuracy of continuous RL exceeds discrete RL, and the rapidity and accuracy leads the backbones. Comparative studies show the strategy achieves leading rapidity and accuracy in single-view depth-based methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523002520",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Dimension (graph theory)",
      "Dimensionality reduction",
      "Feature (linguistics)",
      "Feature extraction",
      "Linguistics",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Process (computing)",
      "Pure mathematics",
      "Reinforcement learning"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Mingqi"
      },
      {
        "surname": "Li",
        "given_name": "Shaodong"
      },
      {
        "surname": "Shuang",
        "given_name": "Feng"
      },
      {
        "surname": "Liu",
        "given_name": "Xi"
      },
      {
        "surname": "Luo",
        "given_name": "Kai"
      },
      {
        "surname": "He",
        "given_name": "Wenbo"
      }
    ]
  },
  {
    "title": "Face recognition system with hybrid template protection scheme for Cyber–Physical-Social Services",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.08.011",
    "abstract": "This paper presents a secure face recognition system with advanced template protection schemes for Cyber–Physical–Social Services (CPSS). The implementation of the proposed system consists of five components. The initial step performs image preprocessing, where it detects the facial region from the captured image using the Tree-Structured Part Model (TSPM). The second phase involves feature extraction, where it utilizes the Scale Invariant Feature Transform (SIFT) descriptor to extract features from small patches of the preprocessed images, forming a collection of feature descriptors. The collection of feature descriptors is then clustered using the K-means clustering algorithm, returning the centers of K-clusters that serve as the vocabulary of a dictionary. Finally, a histogram is generated using the vocabularies and frequencies, referred to as the “Bag of Visual Words (BoVW)”. Using this dictionary and a feature learning technique called Sparse Representation Coding (SRC), followed by Spatial Pyramid Mapping (SPM), the system generates feature vectors from training/testing image samples. In the third component, the modified FaceHashing technique is applied to the original feature vectors, generating cancelable feature vectors. The fourth component employs a Bio-Cryptographic technique to preserve the cancelable feature vectors in a database. Lastly, the fifth component utilizes a multi-class linear SVM classifier on the decrypted and query-cancellable feature vector to classify users. The system evaluates its performance using FERET and CASIA-FaceV5 benchmark databases, providing 100% identification accuracy for 200-dimensional cancelable feature vectors. The performance and security comparisons demonstrate the superiority of the proposed system over existing methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523002313",
    "keywords": [
      "Artificial intelligence",
      "Bag-of-words model",
      "Bag-of-words model in computer vision",
      "Cluster analysis",
      "Computer science",
      "Feature (linguistics)",
      "Feature extraction",
      "Feature vector",
      "Histogram",
      "Histogram of oriented gradients",
      "Image (mathematics)",
      "Image retrieval",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Preprocessor",
      "Scale-invariant feature transform",
      "Visual Word"
    ],
    "authors": [
      {
        "surname": "Sardar",
        "given_name": "Alamgir"
      },
      {
        "surname": "Umer",
        "given_name": "Saiyed"
      },
      {
        "surname": "Rout",
        "given_name": "Ranjeet Kumar"
      },
      {
        "surname": "Pero",
        "given_name": "Chiara"
      }
    ]
  },
  {
    "title": "Random forest clustering for discrete sequences",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.09.001",
    "abstract": "As one of the most popular machine learning methods, random forests have been successfully applied to different data analysis tasks such as classification, regression and cluster analysis. Recently, the random forest clustering method has received much attention due to its simplicity, accuracy and robustness. However, we cannot directly employ the random forest clustering algorithm to solve the discrete sequence clustering problem because of the lack of explicit features and “negative” sequences. In this paper, we propose a new random forest clustering algorithm for discrete sequences. The proposed method firstly injects a set of decoy sequences and then constructs the random forest in a supervised and adaptive manner by generating features on the fly. Experimental results on real data sets show that the proposed method can achieve better performance than those state-of-the-art discrete sequence clustering algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523002507",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Random forest"
    ],
    "authors": [
      {
        "surname": "Jiang",
        "given_name": "Mudi"
      },
      {
        "surname": "Wang",
        "given_name": "Jiaqi"
      },
      {
        "surname": "Hu",
        "given_name": "Lianyu"
      },
      {
        "surname": "He",
        "given_name": "Zengyou"
      }
    ]
  },
  {
    "title": "A Lie group kernel learning method for medical image classification",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109735",
    "abstract": "Medical image classification is a basic step in medical image analysis and has been an essential task in computer-aided diagnosis. Existing classification methods are proved to be effective in conventional image classification tasks, but they often achieve a suboptimal performance when applied to medical images characterizing by complex nonlinear variation. Aiming at this challenge, this paper proposes a Lie group kernel learning method for medical image classification by combining Lie group theory, kernel functions, SVM and KNN classifiers. The method represents each image with a Lie group feature descriptor constructed from low-level features and builds a SVM classifier from the training images. Geodesic distances between categorical pivots and each testing image are calculated with Lie group kernel functions to select either the SVM or a KNN classifier to do the classification. The proposed method is applied to three medical image datasets and the results demonstrate the efficacy of the method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323004338",
    "keywords": [
      "Artificial intelligence",
      "Categorical variable",
      "Classifier (UML)",
      "Combinatorics",
      "Computer science",
      "Contextual image classification",
      "Geodesic",
      "Image (mathematics)",
      "Kernel (algebra)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Li"
      },
      {
        "surname": "Sun",
        "given_name": "Haocheng"
      },
      {
        "surname": "Li",
        "given_name": "Fanzhang"
      }
    ]
  },
  {
    "title": "Handwriting word spotting in the space of difference between representations using vision transformers",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.08.008",
    "abstract": "Word spotting in handwritten documents is challenging due to the high intra-class and inter-class variability of handwritten forms. This paper addresses the word spotting problem in the segmentation and the training scenarios. Overall, this paper makes the following three contributions: (1) a new word text representation, called the Pyramid of Bidirectional Character Sequences (PBCS), which can solve both the word spotting problem and the word recognition problem. The use of the PBCS representation allows trained models to identify the character subsequences shared by words. Thus, words that are not seen during training can be represented and spotted. In addition, the PBCS representation encodes word texts redundantly, allowing for word discrimination. (2) A binary classification modeling of the word spotting problem in the difference space between representations, where spotting of non-vocabulary words is more efficient. Finally, (3) a new deep neural network architecture that combines the strengths of convolutional layers and transformers. We evaluated our solution on IAM and RIEMS datasets and showed that it outperforms recent state-of-the-art methods in the query-by-example scenario.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523002295",
    "keywords": [
      "Arithmetic",
      "Artificial intelligence",
      "Binary number",
      "Computer science",
      "Geometry",
      "Handwriting",
      "Keyword spotting",
      "Law",
      "Linguistics",
      "Mathematics",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Political science",
      "Politics",
      "Quantum mechanics",
      "Representation (politics)",
      "Speech recognition",
      "Spotting",
      "Transformer",
      "Vocabulary",
      "Voltage",
      "Word (group theory)"
    ],
    "authors": [
      {
        "surname": "Mhiri",
        "given_name": "Mohamed"
      },
      {
        "surname": "Hamdan",
        "given_name": "Mohammed"
      },
      {
        "surname": "Cheriet",
        "given_name": "Mohamed"
      }
    ]
  },
  {
    "title": "TAT: Targeted backdoor attacks against visual object tracking",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109629",
    "abstract": "Visual object tracking (VOT) is a fundamental computer vision task that aims to track a target in a sequence of video frames. It has been broadly adopted in safety- and security-critical applications, such as self-driving systems and traffic control systems. However, the VOT models (i.e., the trackers) that rely on third-party training resources face a severe threat of backdoor attacks, which refer to the type of the attacks that poison a portion of training data and mislead the tracker to track a wrong target. A surge of research interest has arisen in backdoor attacks in the domain of image classification, as a measure to expose the potential security risks of the classifiers and inspire new defense techniques. Despite the prosperity of the research in backdoor attacks in image classification, there still lacks investigation in backdoor attacks against VOT, due to their unique challenges: first, the architecture of a VOT model is much more complicated than that of an image classifier; second, VOT targets a sequence of video frames rather than individual images. To bridge the gap, we propose a novel and effective targeted backdoor attack approach TAT specifically against VOT tasks. In particular, TAT includes a basic version TAT-BA that can achieve effective and stealthy backdoor attacks against VOT trackers, and an advanced version TAT-DA that can evade two representative defense techniques. Our large-scale experimental evaluation demonstrates the effectiveness and the stealthiness of TAT. Moreover, we also demonstrate the performances of TAT-BA under real-world settings and the abilities of TAT-DA to counter defense techniques. The code will be available at https://github.com/MisakaZipi/TAT.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323003308",
    "keywords": [
      "Artificial intelligence",
      "Backdoor",
      "BitTorrent tracker",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Eye tracking"
    ],
    "authors": [
      {
        "surname": "Cheng",
        "given_name": "Ziyi"
      },
      {
        "surname": "Wu",
        "given_name": "Baoyuan"
      },
      {
        "surname": "Zhang",
        "given_name": "Zhenya"
      },
      {
        "surname": "Zhao",
        "given_name": "Jianjun"
      }
    ]
  },
  {
    "title": "Self-paced principal component analysis",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109692",
    "abstract": "Principal Component Analysis (PCA) has been widely used for dimensionality reduction and feature extraction. Robust PCA (RPCA), under different robust distance metrics, such as ℓ 1 -norm and ℓ 2 , p -norm, can deal with noise or outliers to some extent. However, real-world data may display structures that can not be fully captured by these simple functions. In addition, existing methods treat complex and simple samples equally. By contrast, a learning pattern typically adopted by human beings is to learn from simple to complex and less to more. Based on this principle, we propose a novel method called Self-paced PCA (SPCA) to further reduce the effect of noise and outliers. Notably, the complexity of each sample is calculated at the beginning of each iteration in order to integrate samples from simple to more complex into training. Based on an alternating optimization, SPCA finds an optimal projection matrix and filters out outliers iteratively. Theoretical analysis is presented to show the rationality of SPCA. Extensive experiments on popular data sets demonstrate that the proposed method can improve the state-of-the-art results considerably.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323003904",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Curse of dimensionality",
      "Dimensionality reduction",
      "Image (mathematics)",
      "Law",
      "Noise (video)",
      "Noise reduction",
      "Norm (philosophy)",
      "Outlier",
      "Pattern recognition (psychology)",
      "Political science",
      "Principal component analysis",
      "Projection (relational algebra)",
      "Robust principal component analysis",
      "Sparse PCA"
    ],
    "authors": [
      {
        "surname": "Kang",
        "given_name": "Zhao"
      },
      {
        "surname": "Liu",
        "given_name": "Hongfei"
      },
      {
        "surname": "Li",
        "given_name": "Jiangxin"
      },
      {
        "surname": "Zhu",
        "given_name": "Xiaofeng"
      },
      {
        "surname": "Tian",
        "given_name": "Ling"
      }
    ]
  },
  {
    "title": "Dual-branch spatio-temporal graph neural networks for pedestrian trajectory prediction",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109633",
    "abstract": "Pedestrian trajectory prediction is an important area in computer vision, with wide applications in autonomous driving, robot path planning, and surveillance systems. The core underlying technique of these applications is pattern recognition. A key challenge in this area is modeling social interactions between pedestrians, such as pedestrian view area and group behaviors. However, although many methods have been proposed to model social interactions, pedestrian view area and group behaviors have not been explored together to account for complex situations. Additionally, most existing studies require additional detectors and manual annotations to handle view area and group interactions, respectively. In this paper, we propose a dual-branch spatio-temporal graph neural network to automatically model view area and grouping together. Specifically, a spatio-temporal graph attention network (STGAT) branch is designed to handle pedestrian view area, and a spatio-temporal graph convolutional network (STGCN) branch is designed to model group interactions. The features of these branches are then fused to provide better feature representations, on which a temporal convolution operation (TCN) is performed for trajectory prediction. Experiments on public standard datasets demonstrate that the proposed method achieves very competitive performance and predicts socially acceptable trajectories in different challenging scenarios.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323003345",
    "keywords": [
      "Archaeology",
      "Art",
      "Artificial intelligence",
      "Astronomy",
      "Computer science",
      "Computer security",
      "Convolutional neural network",
      "Dual (grammatical number)",
      "Feature (linguistics)",
      "Geography",
      "Graph",
      "Key (lock)",
      "Linguistics",
      "Literature",
      "Pattern recognition (psychology)",
      "Pedestrian",
      "Pedestrian detection",
      "Philosophy",
      "Physics",
      "Theoretical computer science",
      "Trajectory"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Xingchen"
      },
      {
        "surname": "Angeloudis",
        "given_name": "Panagiotis"
      },
      {
        "surname": "Demiris",
        "given_name": "Yiannis"
      }
    ]
  },
  {
    "title": "Simultaneous local clustering and unsupervised feature selection via strong space constraint",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109718",
    "abstract": "Clustering is a fashion method applied in machine learning tasks. However, high dimensional data brings many obstacles for clustering approaches. To address such a problem, the unsupervised feature selection (UFS) method can be incorporated into clustering to reduce dimensionality. In general, most of the UFS methods adopt ℓ 2 , 1 -norm for subspace sparsity learning. However, its sparsity highly relies on the setting of trade-off parameter, which may lead to instability of ranking results and the difficulty in obtaining the optimal solution of projection matrix. In this paper, we propose to directly learn an absolutely row-sparsity subspace via the ℓ 2 , 0 -norm constraint, called Sparse constraint and Local learning for Unsupervised Feature Selection (SLUFS). It is an ideal sparse subspace constraint which can overcome the drawbacks of the ℓ 2 , 1 -norm. However, optimizing the ℓ 2 , 0 -norm constraint is an NP-hard problem, and at present, only some approximate solutions can be given, but the convergence can not be guaranteed. To tackle this challenge, we design a novel alternative iterative algorithm to directly optimize the ℓ 2 , 0 -norm based model. Most importantly, our strategy can obtain a closed-form solution with strict convergence guarantee. Comprehensive experiments are conducted on several real-world datasets to evaluate the performance of SLUFS with comparison to several related state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323004168",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Canopy clustering algorithm",
      "Cluster analysis",
      "Computer science",
      "Constrained clustering",
      "Constraint (computer-aided design)",
      "Correlation clustering",
      "Curse of dimensionality",
      "Feature selection",
      "Geometry",
      "Law",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Norm (philosophy)",
      "Political science",
      "Subspace topology"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Zheng"
      },
      {
        "surname": "Li",
        "given_name": "Qi"
      },
      {
        "surname": "Zhao",
        "given_name": "Haifeng"
      },
      {
        "surname": "Nie",
        "given_name": "Feiping"
      }
    ]
  },
  {
    "title": "Rethinking the unpretentious U-net for medical ultrasound image segmentation",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109728",
    "abstract": "Breast tumor segmentation from ultrasound images is one of the key steps that help us characterize and localize tumor regions. However, variable tumor morphology, blurred boundaries, and similar intensity distributions bring challenges for radiologists to segment breast tumors manually. During clinical diagnosis, there are higher demands on the segmentation accuracy and efficiency of breast ultrasound images, so there is an urgent need for an automated method to improve the segmentation accuracy as a technical tool to assist diagnosis. Inspired by the U-net and its many variations, this paper proposed an unpretentious nested U-net (NU-net) for accurate and efficient breast tumor segmentation. The key idea is to utilize U-nets with different depths and shared weights to achieve robust characterization of breast tumors. Specifically, we first utilize the deeper U-net (fifteen layers) as the backbone network to extract more sufficient breast tumor features. Then, we developed a multi-output U-net to be taken as the bond between the encoder and the decoder to enhance the network adaptability for breast tumors with different scales. Finally, the short-connection based on multi-step down-sampling is used to enhance the correlation of long-range information of encoded features. Extensive experimental results with fifteen state-of-the-art segmentation methods on three public breast ultrasound datasets demonstrate that our method has a more competitive segmentation performance on breast tumors. Furthermore, the robustness of our approach is further illustrated by the segmentation of renal ultrasound images. The source code is publicly available on https://github.com/CGPxy/NU-net.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323004260",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biochemistry",
      "Breast cancer",
      "Breast ultrasound",
      "Cancer",
      "Cartography",
      "Chemistry",
      "Computer science",
      "Gene",
      "Geography",
      "Image segmentation",
      "Internal medicine",
      "Mammography",
      "Medicine",
      "Pattern recognition (psychology)",
      "Robustness (evolution)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Gongping"
      },
      {
        "surname": "Li",
        "given_name": "Lei"
      },
      {
        "surname": "Zhang",
        "given_name": "Jianxun"
      },
      {
        "surname": "Dai",
        "given_name": "Yu"
      }
    ]
  },
  {
    "title": "Spatio-temporal modelling with multi-gradient features and elongated quinary pattern descriptor for dynamic facial expression recognition",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109647",
    "abstract": "We propose a new spatio-temporal modelling approach for Dynamic Facial Expression Recognition (DFER). We first convert the domain of the spatial images in the sequence to the gradient of magnitude and angle images at different orientations. Robust gradient components are developed to deal with difficult types of illuminations, such as darkness, by forming the eight edge responses of the Gaussian mask. To describe the dynamic Facial Expression (FE) changes we extend the Elongated Quinary Pattern (EQP) descriptor to encode separately the anisotropic structure of the uniform patterns from Three Orthogonal Planes (TOP) of each gradient sequence. Then each encoded sequence is divided into a stack of block volumes in the XY, XT and YT planes. For each plane, the co-occurrence of histogram features are calculated from each block volume and concatenated together. Simple three-dimensional histogram features are generated by concatenating the histogram features of all planes. A Multi Classifier System (MCS) based on a multi-class Support Vector Machine (SVM) is adopted to combine all scores for the encoded sequences. The proposed approach is evaluated with the challenging MMI and Oulu-CASIA databases with different set-ups and advantage has been shown in terms of generalisation to different databases, together with robustness against difficult pose variations and illumination changes. In terms of Recognition Accuracy (RA), a comparison is established with DFER methods in the literature. A high recognition rate of 79.23% is attained in the case of six classes when applied to the MMI database which surpasses all the state-of-the-art results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323003485",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Facial recognition system",
      "Gene",
      "Histogram",
      "Histogram of oriented gradients",
      "Image (mathematics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Robustness (evolution)",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Al-Sumaidaee",
        "given_name": "S.A.M."
      },
      {
        "surname": "Abdullah",
        "given_name": "M.A.M."
      },
      {
        "surname": "Al-Nima",
        "given_name": "R.R.O."
      },
      {
        "surname": "Dlay",
        "given_name": "S.S."
      },
      {
        "surname": "Chambers",
        "given_name": "J.A."
      }
    ]
  },
  {
    "title": "Global- and local-aware feature augmentation with semantic orthogonality for few-shot image classification",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109702",
    "abstract": "As for few-shot image classification, recently, some works revisit the standard transfer learning paradigm, i.e., pre-training and fine-tuning, and have achieved some success. However, we find that this kind of methods heavily relies on a naive image-level data augmentation (e.g., cropping and flipping) at the fine-tuning stage, which will easily suffer from the overfitting problem because of the limited-data regime. To tackle this issue, in this paper, we attempt to perform a novel feature-level semantic augmentation at the fine-tuning stage and propose a Global- and Local-aware Feature Augmentation method (GLFA) from both the channel- and spatial-wise perspectives. In addition, at the pre-training stage, we further propose a Semantic Orthogonal Learning Framework (SOLF) to make the learned feature channels more independently, orthogonal and diverse. Extensive experiments demonstrate that the proposed method can obtain significant performance improvements over the state of the arts. Code is available at https://github.com/onlyyao/GLFA-SOLF.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323004004",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Feature (linguistics)",
      "Feature learning",
      "Geometry",
      "Image (mathematics)",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Orthogonality",
      "Overfitting",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Semantic feature"
    ],
    "authors": [
      {
        "surname": "Shi",
        "given_name": "Boyao"
      },
      {
        "surname": "Li",
        "given_name": "Wenbin"
      },
      {
        "surname": "Huo",
        "given_name": "Jing"
      },
      {
        "surname": "Zhu",
        "given_name": "Pengfei"
      },
      {
        "surname": "Wang",
        "given_name": "Lei"
      },
      {
        "surname": "Gao",
        "given_name": "Yang"
      }
    ]
  },
  {
    "title": "A novel classification method combining phase-field and DNN",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109723",
    "abstract": "This paper proposes a novel classification method. Firstly, we use the deep neural network (DNN) to classify the training set. After several iterations, we obtain the output vector Y . The component of the largest value in vector Y is represented as the label being classified, which we take as the output value. Because we chose the sigmoid function as our activation function, the output value is between 0 and 1. Therefore, the output value can represents the probability of the classified label by the DNN. Depending on the distribution of output values, we set tolerance values ( T o l ) that categorize similar output values as the same label in the DNN. If the output value is lower than T o l , we consider it categorically anomalous. Subsequently, we use the Phase-Field model to classify these anomalies and obtain better classification results. As this classification method combines Phase-Field model and DNN, we named it Phase-Field-DNN. In the numerical experiment using MNIST handwritten digit data set as experimental data, the classification accuracy of Phase-Field-DNN model is higher than that of Phase-Field model and DNN model through the analysis of the classification results of binary classification and multi-classification problems with this data. In addition, the model we proposed is used to classify the normal and abnormal brain MRIs, and the classification results are compared with those of others. After comparison, we find that our proposed model achieve the best classification results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323004211",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Binary classification",
      "Biology",
      "Computer science",
      "Evolutionary biology",
      "Field (mathematics)",
      "Function (biology)",
      "MNIST database",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Programming language",
      "Pure mathematics",
      "Set (abstract data type)",
      "Sigmoid function",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Jian"
      },
      {
        "surname": "Han",
        "given_name": "Ziwei"
      },
      {
        "surname": "Jiang",
        "given_name": "Wenjing"
      },
      {
        "surname": "Kim",
        "given_name": "Junseok"
      }
    ]
  },
  {
    "title": "Learning Customised Decision Trees for Domain-knowledge Constraints",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109610",
    "abstract": "When applied to critical domains, machine learning models usually need to comply with prior knowledge and domain-specific requirements. For example, one may require that a learned decision tree model should be of limited size and fair, so as to be easily interpretable, trusted, and adopted. However, most state-of-the-art models, even on decision trees, only aim to maximising expected accuracy. In this paper, we propose a framework in which a diverse family of prior and domain knowledge can be formalised and imposed as constraints on decision trees. This framework is built upon a newly introduced tree representation that leads to two generic linear programming formulations of the optimal decision tree problem. The first one targets binary features, while the second one handles continuous features without the need for discretisation. We theoretically show how a diverse family of constraints can be formalised in our framework. We validate the framework with constraints on several applications and perform extensive experiments, demonstrating empirical evidence of comparable performance w.r.t. state-of-the-art tree learners.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323003114",
    "keywords": [
      "Artificial intelligence",
      "Binary decision diagram",
      "Computer science",
      "Decision tree",
      "Decision tree learning",
      "Discretization",
      "Domain (mathematical analysis)",
      "Domain knowledge",
      "Incremental decision tree",
      "Law",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Theoretical computer science",
      "Tree (set theory)"
    ],
    "authors": [
      {
        "surname": "Nanfack",
        "given_name": "Géraldin"
      },
      {
        "surname": "Temple",
        "given_name": "Paul"
      },
      {
        "surname": "Frénay",
        "given_name": "Benoît"
      }
    ]
  },
  {
    "title": "β -Random Walk: Collaborative sampling and weighting mechanisms based on a single parameter for node embeddings",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109730",
    "abstract": "Graph embedding transforms a graph into vector representations to facilitate subsequent graph-analytic tasks. Existing graph embedding methods ignore efficient node sampling and intelligent node weighting, leading to a weak node representation. This paper introduces the β -random walk model with two main contributions. Firstly, the traditional random walk sampling reveals instability. Thus, we associate a parameter β with each node to balance and stabilize the sampling process, producing high-efficient trajectories. Secondly, we design a weighting mechanism that incorporates these trajectories to generate accurate representations. The designed mechanism models the behavior of each node contextually at each episode, considering the current state and the previous weights to produce the next episode’s weights. The parameter β optimizes the node weights by simulating multiple high-order proximity walks from each node. This approach provides summarized insights about each node’s behavior and its neighbors’ context, which enables a consistent discovery of prominent paths variation in the graph. Experimental results demonstrate that the β -random walk outperforms the state-of-the-art baselines in handling small and large graphs.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323004284",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Embedding",
      "Engineering",
      "Filter (signal processing)",
      "Graph",
      "Mathematics",
      "Medicine",
      "Node (physics)",
      "Paleontology",
      "Radiology",
      "Random walk",
      "Sampling (signal processing)",
      "Statistics",
      "Structural engineering",
      "Theoretical computer science",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Hirchoua",
        "given_name": "Badr"
      },
      {
        "surname": "El Motaki",
        "given_name": "Saloua"
      }
    ]
  },
  {
    "title": "GriT-DBSCAN: A spatial clustering algorithm for very large databases",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109658",
    "abstract": "DBSCAN is a fundamental spatial clustering algorithm with numerous practical applications. However, a bottleneck of DBSCAN is its O ( n 2 ) worst-case time complexity. To address this limitation, we propose a new grid-based algorithm for exact DBSCAN in Euclidean space called GriT-DBSCAN, which is based on the following two techniques. First, we introduce grid tree to organize the non-empty grids for the purpose of efficient non-empty neighboring grids queries. Second, by utilizing the spatial relationships among points, we propose a technique that iteratively prunes unnecessary distance calculations when determining whether the minimum distance between two sets is less than or equal to a certain threshold. We theoretically demonstrate that GriT-DBSCAN has excellent reliability in terms of time complexity. In addition, we obtain two variants of GriT-DBSCAN by incorporating heuristics, or by combining the second technique with an existing algorithm. Experiments are conducted on both synthetic and real-world data sets to evaluate the efficiency of GriT-DBSCAN and its variants. The results show that our algorithms outperform existing algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032300359X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Bottleneck",
      "CURE data clustering algorithm",
      "Cluster analysis",
      "Computer science",
      "Correlation clustering",
      "DBSCAN",
      "Data mining",
      "Embedded system",
      "Euclidean distance",
      "Geometry",
      "Grid",
      "Heuristics",
      "Mathematics",
      "Medoid",
      "Operating system"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Xiaogang"
      },
      {
        "surname": "Ma",
        "given_name": "Tiefeng"
      },
      {
        "surname": "Liu",
        "given_name": "Conan"
      },
      {
        "surname": "Liu",
        "given_name": "Shuangzhe"
      }
    ]
  },
  {
    "title": "Polarization-based Camouflaged Object Detection",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.09.007",
    "abstract": "Camouflaged Object Detection (COD) is a challenging task aiming to locate the “indistinguishable” object with high similarity to the background. Existing COD methods are highly dependent on the differences in extrinsic appearances while neglecting inherent physical characteristics. In this letter, we introduce polarization information to enlarge the discrepancies between objects and their surroundings for the COD task. We construct a polarization-based dataset PCOD that comprises 639 challenging real-world scenes, along with a CNN-based network PolarNet tailored to process polarization images. Our method achieves state-of-the-art performance on PCOD and other COD datasets, and shows enhanced qualitative and quantitative effectiveness compared with existing COD methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523002532",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Object detection",
      "Pattern recognition (psychology)",
      "Physical chemistry",
      "Polarization (electrochemistry)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Xin"
      },
      {
        "surname": "Zhang",
        "given_name": "Zhao"
      },
      {
        "surname": "Gao",
        "given_name": "Jun"
      }
    ]
  },
  {
    "title": "Deep image compression using scene text quality assessment",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109696",
    "abstract": "Image compression is a fundamental technology for Internet communication engineering. However, a high compression rate with general methods may degrade images, resulting in unreadable texts. In this paper, we propose an image compression method for maintaining text quality. We developed a scene text image quality assessment model to assess text quality in compressed images. The assessment model iteratively searches for the best-compressed image holding high-quality text. Objective and subjective results showed that the proposed method was superior to existing methods. Furthermore, the proposed assessment model outperformed other deep-learning regression models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323003941",
    "keywords": [
      "Artificial intelligence",
      "Composite material",
      "Compression (physics)",
      "Computer science",
      "Computer vision",
      "Data compression",
      "Deep learning",
      "Engineering",
      "Epistemology",
      "Evaluation methods",
      "Image (mathematics)",
      "Image compression",
      "Image processing",
      "Image quality",
      "Materials science",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Quality (philosophy)",
      "Quality assessment",
      "Reliability engineering"
    ],
    "authors": [
      {
        "surname": "Uchigasaki",
        "given_name": "Shohei"
      },
      {
        "surname": "Miyazaki",
        "given_name": "Tomo"
      },
      {
        "surname": "Omachi",
        "given_name": "Shinichiro"
      }
    ]
  },
  {
    "title": "Knowledge driven weights estimation for large-scale few-shot image recognition",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109668",
    "abstract": "We study the topic of large-scale few-shot image recognition with semantic-visual relational knowledge-based transfer learning. Compared with classical few-shot learning, which is defined as a k -way ( k denotes the number of categories, usually 5/10-way) classification problem, large-scale few-shot recognition contains more categories (100-way +) with few samples per category and is easier to overfit. A promising direction of large-scale few-shot learning is transferring prior relevant semantic/visual knowledge from outside data to accelerate the convergence on limited positives. Inspired by this, we propose a novel Knowledge Driven Weights Estimation framework. Specifically, the framework leverages semantic and visual relations between new few-shot and existed many-shot categories to transfer knowledge trained on many-shot datasets (e.g., ImageNet-1000). We show that the transferred knowledge provides a good initialization for novel few-shot categories leading to faster convergence speed and higher performance than random/imprinting initialization. Experimental results on additional un-seen ImageNet categories (other than the 1000 categories) with few positives show that our method is effective on large-scale few-shot recognition.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323003692",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Chemistry",
      "Computer science",
      "Engineering",
      "False positive paradox",
      "Image (mathematics)",
      "Initialization",
      "Machine learning",
      "Mechanical engineering",
      "One shot",
      "Organic chemistry",
      "Overfitting",
      "Pattern recognition (psychology)",
      "Physics",
      "Programming language",
      "Quantum mechanics",
      "Scale (ratio)",
      "Shot (pellet)",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Jingjing"
      },
      {
        "surname": "Zhuo",
        "given_name": "Linhai"
      },
      {
        "surname": "Wei",
        "given_name": "Zhipeng"
      },
      {
        "surname": "Zhang",
        "given_name": "Hao"
      },
      {
        "surname": "Fu",
        "given_name": "Huazhu"
      },
      {
        "surname": "Jiang",
        "given_name": "Yu-Gang"
      }
    ]
  },
  {
    "title": "Multi-Scale correlation module for video-based facial expression recognition in the wild",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109691",
    "abstract": "The detection of facial muscle movements (e.g., mouth opening) is crucial for facial expression recognition (FER). However, extracting these facial motion features is challenging for a deep-learning recognition system for the following reasons: (1) without explicit labels of motion for training, there is no guarantee that convolutional neural networks (CNNs) can extract motions effectively; (2) compared to human action recognition (e.g., the object moving from left to right), some facial motions (e.g., raising eyebrows) are more subtle and thus harder to extract; and (3) the use of optical flow to extract motion features is time-consuming when using a commonly-used camera. In this work, we propose a Multi-Scale Correlation Module (MSCM) together with an adaptive fusion. Firstly, large as well as small facial motions are extracted by MSCM and encoded by CNNs. Then, an adaptive fusion module is used to aggregate motion features. With these modules, our recognition network is able to model both subtle and large motion features for video-based FER with only the RGB image frames as input. Experiments on two datasets, AFEW and DFEW, show that the network achieves state-of-art performances on the benchmarks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323003898",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Facial expression",
      "Facial expression recognition",
      "Facial recognition system",
      "Image (mathematics)",
      "Motion (physics)",
      "Optical flow",
      "Pattern recognition (psychology)",
      "RGB color model"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Tankun"
      },
      {
        "surname": "Chan",
        "given_name": "Kwok-Leung"
      },
      {
        "surname": "Tjahjadi",
        "given_name": "Tardi"
      }
    ]
  },
  {
    "title": "Learning an artificial neural network to discover bit-quad-based formulas to compute basic object properties",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109685",
    "abstract": "Shape analysis requires estimating object properties in many applications, including optical character recognition, tumor classification, skin cancer recognition, leaf and plant identification, and cell analysis. In particular, bit-quad-based linear expressions proposed by Gray and Duda for calculating the area and perimeter of binary images are widely used in the literature. Nevertheless, these formulas require computing 14 or 15 bit-quad patterns out of 16 possible, becoming critical in applications with limited computing resources. Hence, this paper introduces a method based on a single-layer artificial neural network (ANN) to discover new expressions to calculate the area and perimeter with fewer bit-quads than the original formulas without losing measuring accuracy. Besides, an iterative elimination process removes irrelevant bit-quads whose corresponding weights approach zero. After that, an inductive analysis from observing the learned weights provides interpretable formulas for the area and perimeter. Furthermore, the proposed approach is also applied to find bit-quad-based formulas for directly computing the contact perimeter property, whose original formula requires precomputing Gray’s area and perimeter of the object. In addition, aiming to show our method’s versatility in other applications, we address a real-world problem to discover a bit-quad-based formula to distinguish between two classes of loss of bone density caused by hyperthyroidism and aging in rats. The experimental results show that the proposed approach reduces by approximately half the bit-quads needed to calculate the area and perimeter of Gray and Duda. Likewise, the number of bit-quads to compute the contact perimeter is reduced from nine to six. Besides, the estimated value on test sets by all the found formulas is the same as their original counterparts. On the other hand, the classification of loss of bone density type using the found bit-quad-based formula reaches an accuracy of 100% in the test set. Therefore, the proposed method is an alternative to finding linear expressions with few bit-quads to measure basic object properties.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323003837",
    "keywords": [
      "Algorithm",
      "Arithmetic",
      "Artificial intelligence",
      "Artificial neural network",
      "Binary number",
      "Computer science",
      "Epistemology",
      "Geometry",
      "Mathematics",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Perimeter",
      "Philosophy",
      "Property (philosophy)"
    ],
    "authors": [
      {
        "surname": "Arce",
        "given_name": "Fernando"
      },
      {
        "surname": "Gómez-Flores",
        "given_name": "Wilfrido"
      },
      {
        "surname": "Escalona",
        "given_name": "Uriel"
      },
      {
        "surname": "Sossa",
        "given_name": "Humberto"
      }
    ]
  },
  {
    "title": "A novel cancelable finger vein templates based on LDM and RetinexGan",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109643",
    "abstract": "In this paper, we propose a new biometric template protection scheme, which can deal with the finger vein biometric security threats, through using the LDM and RetinexGAN model. The RetinexGAN model is mainly used to handle the illumination and low contrast problems effectively, while efficiently extracting discriminative features from the finger vein images. The projection of extracted features into dissimilarity space is done using Local Dissimilarity Map (LDM). LDM is an efficient way for finger vein features representation, which investigates the relationships and correlation inter and intra classes, while effectively coming up with the accidental shifts/rotations caused by the arbitrary position of the finger during image acquisition. The proposed approach is successfully evaluated in terms of non-invertibility, non-linkability, revocability and performances. Experimental results and comparison analysis with the state of arts methods confirm that the proposed framework can achieve promising results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323003448",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biometrics",
      "Computer science",
      "Computer vision",
      "Discriminative model",
      "Economics",
      "Finance",
      "Law",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Position (finance)",
      "Projection (relational algebra)",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Aherrahrou",
        "given_name": "N."
      },
      {
        "surname": "Tairi",
        "given_name": "H."
      }
    ]
  },
  {
    "title": "EasyFuse: Easy-to-learn visible and infrared image fusion framework based on unpaired set",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.09.002",
    "abstract": "The major hurdle in building a visible and infrared fusion model is the necessity of large-scale pixel-aligned and time-synchronized image pairs. In this paper, we propose an easy-to-learn visible and infrared image fusion framework that does not require pairs for training. In addition to easy training using unpaired sets, our framework provides fusion images with more textures and meaningful scene information compared to previous works. In particular, to mix features from each spectrum, we newly present a feature line-up module to identify important information in each source. Additionally, in order to provide a new option for benchmark evaluation, we construct a new sequence-based visible and infrared paired dataset that is aligned and synchronized. Finally, we perform extensive experiments to verify the performance of the proposed method by both public and proposed datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523002490",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Computer vision",
      "Construct (python library)",
      "Feature (linguistics)",
      "Fusion",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Image fusion",
      "Infrared",
      "Linguistics",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Pixel",
      "Programming language",
      "Quantum mechanics",
      "Scale (ratio)",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Ahn",
        "given_name": "Seongyong"
      },
      {
        "surname": "Shim",
        "given_name": "Inwook"
      },
      {
        "surname": "Min",
        "given_name": "Jihong"
      },
      {
        "surname": "Yoon",
        "given_name": "Kuk-Jin"
      }
    ]
  },
  {
    "title": "Hand-object information embedded dexterous grasping generation",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.09.006",
    "abstract": "Given the visual data of an object, directly predicting the high-degree-of-freedom grasping pose of a dexterous hand is a challenging task. In this paper, we propose a new grasp synthesis framework based on hand-object interaction for representing the grasping pose of a high degree of freedom hand. First, the attention operation embedded in the finger encoder guides the contact of each finger with the object to obtain the initial grasp pose; then, based on the hand-object interaction constraints, the pose is further optimized in the finger refinement module. The experimental results show that our model enables the robot to grasp objects stably.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523002544",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Encoder",
      "Engineering",
      "GRASP",
      "Object (grammar)",
      "Operating system",
      "Pose",
      "Programming language",
      "Robot",
      "Robot hand",
      "Robotic hand",
      "Systems engineering",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Ren",
        "given_name": "Shuyang"
      },
      {
        "surname": "Zhang",
        "given_name": "Yibiao"
      },
      {
        "surname": "Hang",
        "given_name": "Jinglue"
      },
      {
        "surname": "Lin",
        "given_name": "Xiangbo"
      }
    ]
  },
  {
    "title": "A Multi-Modal Transformer network for action detection",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109713",
    "abstract": "This paper proposes a novel multi-modal transformer network for detecting actions in untrimmed videos. To enrich the action features, our transformer network utilizes a new multi-modal attention mechanism that computes the correlations between different spatial and motion modalities combinations. Exploring such correlations for actions has not been attempted previously. To use the motion and spatial modality more effectively, we suggest an algorithm that corrects the motion distortion caused by camera movement. Such motion distortion, common in untrimmed videos, severely reduces the expressive power of motion features such as optical flow fields. Our proposed algorithm outperforms the state-of-the-art methods on two public benchmarks, THUMOS14 and ActivityNet. We also conducted comparative experiments on our new instructional activity dataset, including a large set of challenging classroom videos captured from elementary schools.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323004119",
    "keywords": [
      "Amplifier",
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Distortion (music)",
      "Electric power system",
      "Electrical engineering",
      "Engineering",
      "Image (mathematics)",
      "Modal",
      "Modalities",
      "Optical flow",
      "Pattern recognition (psychology)",
      "Physics",
      "Polymer chemistry",
      "Power (physics)",
      "Power flow",
      "Quantum mechanics",
      "Social science",
      "Sociology",
      "Telecommunications",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Korban",
        "given_name": "Matthew"
      },
      {
        "surname": "Youngs",
        "given_name": "Peter"
      },
      {
        "surname": "Acton",
        "given_name": "Scott T."
      }
    ]
  },
  {
    "title": "Multi-directional broad learning system for the unsupervised stereo matching method",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109648",
    "abstract": "The supervised stereo matching methods usually rely on ground truth disparity maps as the training labels, this limits its practical application in many situations. In this study, we propose a novel unsupervised stereo matching method based on a multi-directional broad learning system. A multi-directional broad learning system was constructed to generate multiple candidate disparity maps. During the generation of each candidate disparity map, an update criterion is proposed for the disparity value based on the maximum similarity of the inverse mapping region to remove the abnormal disparity values of the training samples. Subsequently, multi-direction consistency verification is performed to further eliminate abnormal disparity values, which are based on the uniqueness principle of disparity truth values at the same location. Finally, an invalid depth redefinition based on a local gravity weight method is introduced to select the appropriate disparity value to fill the invalid pixel positions from their neighborhood, which is calculated based on the local region of the color, matching cost, and geometric spaces in the stereo images. We provide the results of experiments on both indoor and outdoor scenarios to demonstrate the effectiveness and flexibility of our approach, including comparisons with state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323003497",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Consistency (knowledge bases)",
      "Flexibility (engineering)",
      "Ground truth",
      "Image (mathematics)",
      "Matching (statistics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pixel",
      "Similarity (geometry)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "zihao",
        "given_name": "Zhang"
      },
      {
        "surname": "Ying",
        "given_name": "Niu"
      },
      {
        "surname": "Fanman",
        "given_name": "Meng"
      },
      {
        "surname": "Tiejun",
        "given_name": "Yang"
      },
      {
        "surname": "Chao",
        "given_name": "Fan"
      },
      {
        "surname": "Xiaozhen",
        "given_name": "Ren"
      },
      {
        "surname": "Ruiqi",
        "given_name": "Wu"
      },
      {
        "surname": "Kun",
        "given_name": "Cao"
      },
      {
        "surname": "Haocheng",
        "given_name": "Wang"
      }
    ]
  },
  {
    "title": "NRPose: Towards noise resistance for multi-person pose estimation",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109680",
    "abstract": "The high signal-to-noise ratio is one of the main challenges of multi-person pose estimation (MPE) and receives little attention. In this work, we find that MPE suffers from two types of noise: aleatoric noise and epistemic noise. The former represents the noise inherent in the observations, such as the background. The latter indicates the noise brought by the priori hypotheses, such as the inappropriate keypoint relations. Both of them reduce the saliency of information available for keypoint localization. We propose the noise-resistance pose estimation (NRPose) that integrates keypoint-oriented region proposal module (KRPM) and pose-aware sparse relation module (PSRM). To mitigate aleatoric noise, KRPM generates keypoint-level RoIs by circumscribing semantically significant regions. To reduce epistemic noise, PSRM filters out the noisy relations dynamically by modeling the noise propagation and keypoint interaction. NRPose outperforms the state-of-the-art methods by at least 1.0 AP on COCO and OCHuman dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323003801",
    "keywords": [
      "A priori and a posteriori",
      "Artificial intelligence",
      "Computer science",
      "Epistemology",
      "Image (mathematics)",
      "Mathematics",
      "Noise (video)",
      "Noise measurement",
      "Noise reduction",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "He",
        "given_name": "Jianhang"
      },
      {
        "surname": "Sun",
        "given_name": "Junyao"
      },
      {
        "surname": "Liu",
        "given_name": "Qiong"
      },
      {
        "surname": "Peng",
        "given_name": "Shaowu"
      }
    ]
  },
  {
    "title": "An efficient EM algorithm for two-layer mixture model of gaussian process functional regressions",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109783",
    "abstract": "The mixture of Gaussian processes is effective for regression, but it cannot handle the non-stationary curve clustering problem well. The two-layer mixture of Gaussian process functional regressions (TMGPFR) model was established to deal with this problem. In this paper, we first propose the classification EM (CEM) algorithm to solve that the optimization algorithm is inefficient for TMGPFRs, and then propose the deterministic annealing CEM algorithm for TMGPFRs to overcome the local maximum problem of the CEM algorithm. Lastly, experiments are conducted on synthetic and real-world data sets, and the results show that our proposed algorithms are more effective than the compared algorithms on curve clustering and regression.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323004818",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Gaussian",
      "Gaussian process",
      "Mathematical optimization",
      "Mathematics",
      "Mixture model",
      "Operating system",
      "Physics",
      "Process (computing)",
      "Quantum mechanics",
      "Simulated annealing"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Di"
      },
      {
        "surname": "Xie",
        "given_name": "Yurong"
      },
      {
        "surname": "Qiang",
        "given_name": "Zhe"
      }
    ]
  },
  {
    "title": "VQAPT: A New visual question answering model for personality traits in social media images",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.10.016",
    "abstract": "Visual Question Answering (VQA) for personality trait images on social media is challenging because of multiple emotions and actions with complex backgrounds in social media images. This work aims at developing a new VQA model for different personality traits (VQAPT) identification in a single image. This work considers the Big Five Factors (BFF) for personality traits namely, Openness, Conscientiousness, Extraversion, Agreeableness and Neuroticism. VQA is proposed based on the observation that multiple personality traits can be seen in a single image. We propose a model integrating text recognition and person/face recognition to derive the unique relationship between the text and the person's action in the image. Furthermore, a dynamic text-object graph for personality traits identification is constructed according to the query. For understanding a query, we explore the Contrastive Language-Image Pre-trained (CLIP) transformer encoder in this work. Since it is the first work of its kind, we have created a new dataset under this work for evaluation and the dataset is available publicly as mentioned in Section 4. The effectiveness of the proposed method is also evaluated on two benchmark datasets, namely TextVQA for VQA and PTI for personality traits identification.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552300288X",
    "keywords": [
      "Agreeableness",
      "Artificial intelligence",
      "Big Five personality traits",
      "Biology",
      "Botany",
      "Cognitive psychology",
      "Computer science",
      "Conscientiousness",
      "Extraversion and introversion",
      "Facet (psychology)",
      "Identification (biology)",
      "Machine learning",
      "Natural language processing",
      "Openness to experience",
      "Personality",
      "Psychology",
      "Social media",
      "Social psychology",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Biswas",
        "given_name": "Kunal"
      },
      {
        "surname": "Shivakumara",
        "given_name": "Palaiahnakote"
      },
      {
        "surname": "Pal",
        "given_name": "Umapada"
      },
      {
        "surname": "Liu",
        "given_name": "Cheng-Lin"
      },
      {
        "surname": "Lu",
        "given_name": "Yue"
      }
    ]
  },
  {
    "title": "Gaussian kernel fuzzy c-means with width parameter computation and regularization",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109749",
    "abstract": "The conventional Gaussian kernel fuzzy c-means clustering algorithms require selecting the width hyper-parameter, which is data-dependent and fixed for the entire execution. Not only that, but these parameters are the same for every dataset variable. Therefore, the variables have the same importance in the clustering task, including irrelevant variables. This paper proposes a Gaussian kernel fuzzy c-means with kernelization of the metric and automated computation of width parameters. These width parameters change at each iteration of the algorithm and vary from each variable and from each cluster. Thus, this algorithm can re-scale the variables differently, thus highlighting those that are relevant to the clustering task. Fuzzy clustering algorithms with regularization have become popular due to their high performance in large-scale data clustering, robustness for initialization, and low computational complexity. Because the width parameters of the variables can also be controlled by entropy, this paper also proposes Gaussian kernel fuzzy c-means algorithms with kernelization of the metric and automated computation of width parameters through entropy regularization. To demonstrate their usefulness, the proposed algorithms are compared with the conventional KFCM-K algorithm and previous algorithms that automatically compute the width parameter of the Gaussian kernel.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323004478",
    "keywords": [
      "Algorithm",
      "Applied mathematics",
      "Artificial intelligence",
      "Combinatorics",
      "Computation",
      "Computer science",
      "Fuzzy logic",
      "Gaussian",
      "Gaussian function",
      "Kernel (algebra)",
      "Mathematical optimization",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Regularization (linguistics)"
    ],
    "authors": [
      {
        "surname": "C． Simões",
        "given_name": "Eduardo"
      },
      {
        "surname": "T． de Carvalho",
        "given_name": "Francisco de A."
      }
    ]
  },
  {
    "title": "KD-Former: Kinematic and dynamic coupled transformer network for 3D human motion prediction",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109806",
    "abstract": "Recent studies have made remarkable progress on 3D human motion prediction by describing motion with kinematic knowledge. However, kinematics only considers the 3D positions or rotations of human skeletons, failing to reveal the physical characteristics of human motion. Motion dynamics reflects the forces between joints, explicitly encoding the skeleton topology, whereas rarely exploited in motion prediction. In this paper, we propose the Kinematic and Dynamic coupled transFormer (KD-Former), which incorporates dynamics with kinematics, to learn powerful features for high-fidelity motion prediction. Specifically, We first formulate a reduced-order dynamic model of human body to calculate the forces of all joints. Then we construct a non-autoregressive encoder-decoder framework based on the transformer structure. The encoder involves a kinematic encoder and a dynamic encoder, which are respectively responsible for extracting the kinematic and dynamic features for given history sequences via a spatial transformer and a temporal transformer. Future query sequences are decoded in parallel in the decoder by leveraging the encoded kinematic and dynamic information of history sequences. Experiments on Human3.6M and CMU MoCap benchmarks verify the effectiveness and superiority of our method. Code will be available at: https://github.com/wslh852/KD-Former.git.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005046",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Autoregressive model",
      "Classical mechanics",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Econometrics",
      "Electrical engineering",
      "Encoder",
      "Engineering",
      "Kinematics",
      "Mathematics",
      "Motion (physics)",
      "Motion capture",
      "Operating system",
      "Physics",
      "Topology (electrical circuits)",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Dai",
        "given_name": "Ju"
      },
      {
        "surname": "Li",
        "given_name": "Hao"
      },
      {
        "surname": "Zeng",
        "given_name": "Rui"
      },
      {
        "surname": "Bai",
        "given_name": "Junxuan"
      },
      {
        "surname": "Zhou",
        "given_name": "Feng"
      },
      {
        "surname": "Pan",
        "given_name": "Junjun"
      }
    ]
  },
  {
    "title": "EarlGAN: An enhanced actor–critic reinforcement learning agent-driven GAN for de novo drug design",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.10.001",
    "abstract": "Deep generative models, such as Generative Adversarial Networks (GANs), have attracted the attention of researchers in the d e n o v o drug design field. However, traditional GANs are typically used for image processing. Therefore, they are unsuitable for Simplified Molecular-Input Line-Entry System (SMILES) strings due to their discrete nature. Previous studies addressed this problem by combining Reinforcement Learning (RL) approaches with Monte Carlo tree search. However, for large chemical datasets, the molecule generation process is time-consuming due to the lengthy atom-by-atom sampling process with cumulative reward, an essence of the Monte Carlo tree search-based RL approaches. To address this problem, we propose an e nhanced a ctor–critic RL agent-driven GAN , called EarlGAN, for d e n o v o drug design. Specifically, EarlGAN’s generator acts as an actor to generate SMILES strings, and the discriminator acts as a critic to perform discrimination. EarlGAN makes autoregressive predictions at the atomic level. While the generator is based on previously generated atoms, the discriminator discriminates using a bidirectional pass over the atoms, including the current atom that is being predicted. We integrate moment, global-level discrimination rewards, and information entropy maximization. The moment rewards reduce the computation time, and the global-level rewards ensure the consistency of the molecule, whereas the information entropy maximization leads to a more diverse sample generation. Experiments and ablation studies verify the effectiveness of EarlGAN for d e n o v o drug design on the QM9 and ZINC datasets. In addition, the visualization analysis provides insight into EarlGAN and supports our conclusion.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523002726",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Detector",
      "Discriminator",
      "Generator (circuit theory)",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Maximization",
      "Monte Carlo method",
      "Monte Carlo tree search",
      "Physics",
      "Power (physics)",
      "Quantum mechanics",
      "Reinforcement learning",
      "Statistics",
      "Telecommunications",
      "Tree (set theory)"
    ],
    "authors": [
      {
        "surname": "Tang",
        "given_name": "Huidong"
      },
      {
        "surname": "Li",
        "given_name": "Chen"
      },
      {
        "surname": "Jiang",
        "given_name": "Shuai"
      },
      {
        "surname": "Yu",
        "given_name": "Huachong"
      },
      {
        "surname": "Kamei",
        "given_name": "Sayaka"
      },
      {
        "surname": "Yamanishi",
        "given_name": "Yoshihiro"
      },
      {
        "surname": "Morimoto",
        "given_name": "Yasuhiko"
      }
    ]
  },
  {
    "title": "Efficient probability intervals for classification using inductive venn predictors",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109734",
    "abstract": "Learning enabled components are frequently used by autonomous systems and it is common for deep neural networks to be integrated in such systems for their ability to learn complex, non-linear data patterns and make accurate predictions in dynamic environments. However, their large number of parameters and their use as black boxes introduce risks as the confidence in each prediction is unknown and output values like softmax scores are not usually well-calibrated. Different frameworks have been proposed to compute accurate confidence measures along with the predictions but at the same time introduce a number of limitations like execution time overhead or inability to be used with high-dimensional data. In this paper, we use the Inductive Venn Predictors framework for computing probability intervals regarding the correctness of each prediction in real-time. We propose taxonomies based on distance metric learning to compute informative probability intervals in applications involving high-dimensional inputs. By assigning pseudo-labels to unlabeled input data during system deployment we further improve the efficiency of the computed probability intervals. Empirical evaluation on image classification and botnet attacks detection in Internet-of-Things (IoT) applications demonstrates improved accuracy and calibration. The proposed method is computationally efficient, and therefore, can be used in real-time. The code is available at https://github.com/dboursinos/Efficient-Probability-Intervals-Classification-Inductive-Venn-Predictors.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323004326",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Correctness",
      "Data mining",
      "Economics",
      "Machine learning",
      "Mathematics",
      "Mathematics education",
      "Metric (unit)",
      "Operating system",
      "Operations management",
      "Overhead (engineering)",
      "Softmax function",
      "Venn diagram"
    ],
    "authors": [
      {
        "surname": "Boursinos",
        "given_name": "Dimitrios"
      },
      {
        "surname": "Koutsoukos",
        "given_name": "Xenofon"
      }
    ]
  },
  {
    "title": "Uncovering Hidden Vulnerabilities in Convolutional Neural Networks through Graph-based Adversarial Robustness Evaluation",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109745",
    "abstract": "Convolutional neural networks (CNNs) are widely used for image classification, but their vulnerability to adversarial attacks poses challenges to their reliability and security. However, current adversarial robustness (AR) measures lack a theoretical foundation, limiting the insight into the decision process. To address this issue, we propose a new AR evaluation framework based on Graph of Patterns (GoPs) models and graph distance algorithms. Our approach provides a fine-grained analysis of AR from three perspectives, providing targeted insight into the vulnerability of CNNs. Compared to current standards, our approach is theoretically grounded and allows fine-tuning of model components without repeated attempts and validation. Our experimental results demonstrate its effectiveness in uncovering hidden vulnerabilities in CNNs and providing actionable approaches to improve their AR. Our GoPs modeling approach and graph distance algorithms can be extended to apply to other graph machine learning tasks such as Metric Learning on multi-relational graphs. Overall, our framework represents significant progress in AR evaluation, providing a more interpretable, targeted, and efficient approach to assess CNN robustness in complex graph-based systems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323004430",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Convolutional neural network",
      "Data mining",
      "Gene",
      "Graph",
      "Machine learning",
      "Robustness (evolution)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Ke"
      },
      {
        "surname": "Chen",
        "given_name": "Zicong"
      },
      {
        "surname": "Dang",
        "given_name": "Xilin"
      },
      {
        "surname": "Fan",
        "given_name": "Xuan"
      },
      {
        "surname": "Han",
        "given_name": "Xuming"
      },
      {
        "surname": "Chen",
        "given_name": "Chien-Ming"
      },
      {
        "surname": "Ding",
        "given_name": "Weiping"
      },
      {
        "surname": "Yiu",
        "given_name": "Siu-Ming"
      },
      {
        "surname": "Weng",
        "given_name": "Jian"
      }
    ]
  },
  {
    "title": "Data-free quantization via mixed-precision compensation without fine-tuning",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109780",
    "abstract": "Neural network quantization is a very promising solution in the field of model compression, but its resulting accuracy highly depends on a training/fine-tuning process and requires the original data. This not only brings heavy computation and time costs but also is not conducive to privacy and sensitive information protection. Therefore, a few recent works are starting to focus on data-free quantization. However, data-free quantization does not perform well while dealing with ultra-low precision quantization. Although researchers utilize generative methods of synthetic data to address this problem partially, data synthesis needs to take a lot of computation and time. In this paper, we propose a data-free mixed-precision compensation (DF-MPC) method to recover the performance of an ultra-low precision quantized model without any data and fine-tuning process. By assuming the quantized error caused by a low-precision quantized layer can be restored via the reconstruction of a high-precision quantized layer, we mathematically formulate the reconstruction loss between the pre-trained full-precision model and its layer-wise mixed-precision quantized model. Based on our formulation, we theoretically deduce the closed-form solution by minimizing the reconstruction loss of the feature maps. Since DF-MPC does not require any original/synthetic data, it is a more efficient method to approximate the full-precision model. Experimentally, our DF-MPC is able to achieve higher accuracy for an ultra-low precision quantized model compared to the recent methods without any data and fine-tuning process.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323004788",
    "keywords": [
      "Algorithm",
      "Computation",
      "Computer science",
      "Fine-tuning",
      "Physics",
      "Quantization (signal processing)",
      "Quantum mechanics"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Jun"
      },
      {
        "surname": "Bai",
        "given_name": "Shipeng"
      },
      {
        "surname": "Huang",
        "given_name": "Tianxin"
      },
      {
        "surname": "Wang",
        "given_name": "Mengmeng"
      },
      {
        "surname": "Tian",
        "given_name": "Guanzhong"
      },
      {
        "surname": "Liu",
        "given_name": "Yong"
      }
    ]
  },
  {
    "title": "Source-free and black-box domain adaptation via distributionally adversarial training",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109750",
    "abstract": "Source-free unsupervised domain adaptation is one class of practical deep learning methods which generalize in the target domain without transferring data from source domain. However, existing source-free domain adaptation methods rely on source model transferring. In many data-critical scenarios, the transferred source models may suffer from membership inference attacks and expose private data. In this paper, we aim to overcome a more practical and challenging setting where the source models cannot be transferred to the target domain. The source models are considered as queryable black-box models which only output hard labels. We use public third-party data to probe the source model and obtain supervision information, dispensing with transferring source model. To fill the gap between third-party data and target data, we further propose Distributionally Adversarial Training (DAT) to align the distribution of third-party data with target data, gain more informative query results and improve the data efficiency. We call this new framework Black-box Probe Domain Adaptation (BPDA) which adopts query mechanism and DAT to probe and refine supervision information. Experimental results on several domain adaptation datasets demonstrate the practicability and data efficiency of BPDA in query-only and source-free unsupervised domain adaptation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032300448X",
    "keywords": [
      "Adaptation (eye)",
      "Adversarial system",
      "Artificial intelligence",
      "Black box",
      "Computer science",
      "Data mining",
      "Data modeling",
      "Database",
      "Domain (mathematical analysis)",
      "Inference",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Optics",
      "Physics"
    ],
    "authors": [
      {
        "surname": "Shi",
        "given_name": "Yucheng"
      },
      {
        "surname": "Wu",
        "given_name": "Kunhong"
      },
      {
        "surname": "Han",
        "given_name": "Yahong"
      },
      {
        "surname": "Shao",
        "given_name": "Yunfeng"
      },
      {
        "surname": "Li",
        "given_name": "Bingshuai"
      },
      {
        "surname": "Wu",
        "given_name": "Fei"
      }
    ]
  },
  {
    "title": "Addressing the class-imbalance and class-overlap problems by a metaheuristic-based under-sampling approach",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109721",
    "abstract": "The problem of imbalanced class distribution in real-world datasets severely impairs the performance of classification algorithms. The learning task becomes more complicated and challenging when there is also the class-overlap problem in imbalanced data. This research tackles these problems by presenting an under-sampling approach based on a metaheuristic method in which the under-sampling problem is mapped into an optimization problem. The proposed approach aims to select an optimal subset of the majority samples to handle the imbalanced and the class-overlap problems simultaneously while avoiding the excessive elimination of majority samples, especially in overlapped regions. The quality of the generated solutions is evaluated by a classifier and optimized in an evolutionary process. Unlike most existing under-sampling methods, the majority samples are not removed only from the overlapped regions; the classifier performance determines the desired regions for eliminating the majority samples. Extensive experiments conducted on 66 synthetic and 24 real-world datasets with different imbalance ratios and overlapping degrees and two large high-dimensional datasets show a significant performance improvement from the proposed method compared to the competitors.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323004193",
    "keywords": [
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Class (philosophy)",
      "Classifier (UML)",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Economics",
      "Filter (signal processing)",
      "Machine learning",
      "Management",
      "Metaheuristic",
      "Oversampling",
      "Pattern recognition (psychology)",
      "Sampling (signal processing)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Soltanzadeh",
        "given_name": "Paria"
      },
      {
        "surname": "Feizi-Derakhshi",
        "given_name": "M. Reza"
      },
      {
        "surname": "Hashemzadeh",
        "given_name": "Mahdi"
      }
    ]
  },
  {
    "title": "Variational Bayesian deep network for blind Poisson denoising",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109810",
    "abstract": "Deep learning-based approaches have recently achieved considerable results in Poisson denoising under low-light conditions. However, most existing methods mainly focus on the network architecture design, which lacks physical interpretability and thus unsuitable for blind denoising in real environments with unknown levels of noises. To address this issue, we propose a variational Bayesian deep network for blind Poisson denoising (VBDNet). We mainly consider an approximate posterior form for the noise variance in a variational Bayesian framework and utilize a neural network to parameterize the variance of Poisson noise. For network design, VBDNet is divided into two sub-networks. The noise estimation sub-network is responsible for the Bayesian inference. This network improves the blind denoising ability of the subsequent denoising sub-network by learning Poisson noise characteristics under different noise levels in the training process. A network of U-Net structures implements the denoising sub-network for noise removal. By combining the advantage of Bayesian inference (noise estimation sub-network) and deep learning (denoising sub-network), VBDNet outperforms other state-of-the-art methods on both synthetic and natural data. The code and details are available at https://github.com/HLImg/VBDNet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005083",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer security",
      "Deep learning",
      "Image (mathematics)",
      "Inference",
      "Interpretability",
      "Machine learning",
      "Network architecture",
      "Noise (video)",
      "Noise reduction",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Liang",
        "given_name": "Hao"
      },
      {
        "surname": "Liu",
        "given_name": "Rui"
      },
      {
        "surname": "Wang",
        "given_name": "Zhongyuan"
      },
      {
        "surname": "Ma",
        "given_name": "Jiayi"
      },
      {
        "surname": "Tian",
        "given_name": "Xin"
      }
    ]
  },
  {
    "title": "Foreword to Special Section on SIBGRAPI 2022",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.09.011",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523002593",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Engineering",
      "Engineering physics",
      "Operating system",
      "Section (typography)",
      "Special section"
    ],
    "authors": [
      {
        "surname": "dos Santos",
        "given_name": "Jefersson A."
      },
      {
        "surname": "Apolinário",
        "given_name": "Antonio L."
      },
      {
        "surname": "Miranda",
        "given_name": "Fabio"
      },
      {
        "surname": "Distante",
        "given_name": "Cosimo"
      }
    ]
  },
  {
    "title": "One Shot Learning with class partitioning and cross validation voting (CP-CVV)",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109797",
    "abstract": "One Shot Learning includes all those techniques that make it possible to classify images using a single image per category. One of its possible applications is the identification of food products. For a grocery store, it is interesting to record a single image of each product and be able to recognise it again from other images, such as photos taken by customers. Within deep learning, Siamese neural networks are able to verify whether two images belong to the same category or not. In this paper, a new Siamese network training technique, called CP-CVV, is presented. It uses the combination of different models trained with different classes. The separation of validation classes has been done in such a way that each of the combined models is different in order to avoid overfitting with respect to the validation. Unlike normal training, the test images belong to classes that have not previously been used in training, allowing the model to work on new categories, of which only one image exists. Different backbones have been evaluated in the Siamese composition, but also the integration of multiple models with different backbones. The results show that the model improves on previous works and allows the classification problem to be solved, an additional step towards the use of Siamese networks. To the best of our knowledge, there is no existing work that has proposed integrating Siamese neural networks using a class-based validation set separation technique so as to be better at generalising for unknown classes. Additionally, we have applied Cross-Validation-Voting with ConvNeXt to improve the existing classification results of a well-known Grocery Store Dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323004958",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Botany",
      "Class (philosophy)",
      "Computer science",
      "Cross-validation",
      "Generalization",
      "Geometry",
      "Identification (biology)",
      "Image (mathematics)",
      "Law",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Overfitting",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Product (mathematics)",
      "Programming language",
      "Set (abstract data type)",
      "Test set",
      "Voting"
    ],
    "authors": [
      {
        "surname": "Duque-Domingo",
        "given_name": "Jaime"
      },
      {
        "surname": "Aparicio",
        "given_name": "Roberto Medina"
      },
      {
        "surname": "Rodrigo",
        "given_name": "Luis Miguel González"
      }
    ]
  },
  {
    "title": "A self-adaptive soft-recoding strategy for performance improvement of error-correcting output codes",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109813",
    "abstract": "The technique of error-correcting output codes (ECOC) has been proven to be of high discriminative ability in many classification applications. However, most algorithms on the ECOC were designed based on the binary or ternary codes (referred to as the hard codes), which might fail to precisely correct errors in dealing with tough tasks. In this study, a Soft-Recoding strategy based on a self-adaptive algorithm is proposed, which replaces the traditional hard codes with the real-value elements to better fit the output distribution of the base learners. This is achieved by minimizing the ratio of two distances: the distance of the output vector to the ground-truth class, and the average distance of the output vector to the remaining classes. Extensive experiments using five different hard ECOC algorithms and the corresponding softened versions on twenty datasets with diversified numbers of features and classes confirm the effectiveness of our Soft-Recoding strategy in promoting the performance of the original ECOC algorithms. Our source code and additional results are available at: github.com/MLDMXM2017/SA-soft-recoding.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005113",
    "keywords": [
      "Algorithm",
      "Arithmetic",
      "Artificial intelligence",
      "Binary number",
      "Class (philosophy)",
      "Code (set theory)",
      "Computer science",
      "Discriminative model",
      "Error detection and correction",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Programming language",
      "Set (abstract data type)",
      "Value (mathematics)"
    ],
    "authors": [
      {
        "surname": "Lin",
        "given_name": "Guangyi"
      },
      {
        "surname": "Gao",
        "given_name": "Jie"
      },
      {
        "surname": "Zeng",
        "given_name": "Nan"
      },
      {
        "surname": "Xu",
        "given_name": "Yong"
      },
      {
        "surname": "Liu",
        "given_name": "Kunhong"
      },
      {
        "surname": "Wang",
        "given_name": "Beizhan"
      },
      {
        "surname": "Yao",
        "given_name": "Junfeng"
      },
      {
        "surname": "Wu",
        "given_name": "Qingqiang"
      }
    ]
  },
  {
    "title": "Auto-attention mechanism for multi-view deep embedding clustering",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109764",
    "abstract": "In several fields, deep learning has achieved tremendous success. Multi-view learning is a workable method for handling data from several sources. For clustering multi-view data, deep learning and multi-view learning are excellent options. However, a persistent challenge is a need for the current deep learning approach to independently drive divergent neural networks for different perspectives while working with multi-view data. The current methods use the number of viewpoints to calculate neural network statistics. Consequently, as the number of views rises, it results in a considerable calculation. Furthermore, they vainly try to unite various viewpoints at the training. Incorporating a triple fusion technique, this research suggests an innovative multi-view deep embedding clustering (MDEC) model. The suggested model can jointly acquire the specific knowledge in each view as well as the information fragment of the collective views. The main goal of the MDEC is to lower the errors made when learning the features of each view and correlating data from many views. To address the optimization problem, the MDEC model advises a suitable iterative updating approach. In testing modern deep learning and non-deep learning algorithms, the experimental study on small and large-scale multi-view data shows encouraging results for the MDEC model. In multi-view clustering, this work demonstrates the benefit of the deep learning-based approach over the non-ones. However, future work will address a variety of issues related to MDEC including the speed.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323004624",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Artificial neural network",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Deep learning",
      "Embedding",
      "Machine learning",
      "Variety (cybernetics)",
      "Viewpoints",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Diallo",
        "given_name": "Bassoma"
      },
      {
        "surname": "Hu",
        "given_name": "Jie"
      },
      {
        "surname": "Li",
        "given_name": "Tianrui"
      },
      {
        "surname": "Khan",
        "given_name": "Ghufran Ahmad"
      },
      {
        "surname": "Liang",
        "given_name": "Xinyan"
      },
      {
        "surname": "Wang",
        "given_name": "Hongjun"
      }
    ]
  },
  {
    "title": "Trigonometric projection statistics histograms for 3D local feature representation and shape description",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109727",
    "abstract": "Feature representation as a significant approach to three-dimensional (3D) shape description has been widely employed in computer vision. However, most existing methods are suffering from the emerging challenges for descriptiveness, robustness and efficiency. This paper presents a novel feature descriptor named trigonometric projection statistics histograms (TPSH). By constructing the repeatable local reference frame based on a multi-attribute weighting strategy, TPSH can address many prevailing nuisances such as noise, occlusion and varying resolution. The trigonometric projection mechanism is originally proposed for TPSH generation, which combines two perspective views to encode both spatial distribution and geometrical measurements from local shape into statistics histograms. The experimental evaluation on public datasets proves that TPSH outperforms state-of-the-art methods in descriptiveness and robustness while maintaining storage compactness and computational efficiency. It is demonstrated that TPSH can not only be suited for 3D object recognition and shape registration, but also generalized across various acquisition devices, data modalities and application scenarios.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323004259",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Classifier (UML)",
      "Computer science",
      "Computer vision",
      "Gene",
      "Histogram",
      "Image (mathematics)",
      "Mathematics",
      "Medicine",
      "Pattern recognition (psychology)",
      "Radiology",
      "Robustness (evolution)",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Xingsheng"
      },
      {
        "surname": "Li",
        "given_name": "Anhu"
      },
      {
        "surname": "Sun",
        "given_name": "Jianfeng"
      },
      {
        "surname": "Lu",
        "given_name": "Zhiyong"
      }
    ]
  },
  {
    "title": "Local nonlinear dimensionality reduction via preserving the geometric structure of data",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109663",
    "abstract": "Dimensionality reduction has many applications in data visualization and machine learning. Existing methods can be classified into global ones and local ones. The global methods usually learn the linear relationship in data, while the local ones learn the manifold intrinsic geometry structure, which has a significant impact on pattern recognition. However, most of existing local methods obtain an embedding with eigenvalue or singular value decomposition, where the computational complexities are very high in a large amount of high-dimensional data. In this paper, we propose a local nonlinear dimensionality reduction method named Vec2vec, which employs a neural network with only one hidden layer to reduce the computational complexity. We first build a neighborhood similarity graph from the input matrix, and then define the context of data points with the random walk properties in the graph. Finally, we train the neural network with the context of data points to learn the embedding of the matrix. We conduct extensive experiments of data classification and clustering on nine image and text datasets to evaluate the performance of our method. Experimental results show that Vec2vec is better than several state-of-the-art dimensionality reduction methods, except that it is equivalent to UMAP on data clustering tasks in the statistical hypothesis tests, but Vec2vec needs less computational time than UMAP in high-dimensional data. Furthermore, we propose a more lightweight method named Approximate Vec2vec (AVec2vec) with little performance degradation, which employs an approximate method to build the neighborhood similarity graph. AVec2vec is still better than some state-of-the-art local dimensionality reduction methods and competitive with UMAP on data classification and clustering tasks in the statistical hypothesis tests.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323003643",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Cluster analysis",
      "Computer science",
      "Context (archaeology)",
      "Curse of dimensionality",
      "Data point",
      "Dimensionality reduction",
      "Embedding",
      "Graph",
      "Nonlinear dimensionality reduction",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Singular value decomposition",
      "Spectral clustering",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Xiang"
      },
      {
        "surname": "Zhu",
        "given_name": "Junxing"
      },
      {
        "surname": "Xu",
        "given_name": "Zichen"
      },
      {
        "surname": "Ren",
        "given_name": "Kaijun"
      },
      {
        "surname": "Liu",
        "given_name": "Xinwang"
      },
      {
        "surname": "Wang",
        "given_name": "Fengyun"
      }
    ]
  },
  {
    "title": "Tri-objective optimization-based cascade ensemble pruning for deep forest",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109744",
    "abstract": "Deep forest is a new multi-layer ensemble model, where the high time costs and storage requirements inhibit its large-scale application. However, current deep forest pruning methods used to alleviate these drawbacks do not consider its cascade coupling characteristics. Therefore, we propose a tri-objective optimization-based cascade ensemble pruning (TOOCEP) algorithm for it. Concretely, we first present a tri-objective optimization-based single-layer pruning (TOOSLP) method to prune its single-layer by simultaneously optimizing three objectives, namely accuracy, independent diversity, and coupled diversity. Particularly, the coupled diversity is designed for deep forest to deal with the coupling relationships between its adjacent layers. Then, we perform TOOSLP in a cascade framework to prune the deep forest layer-by-layer. Experimental results on 15 UCI datasets show that TOOCEP outperforms several state-of-the-art methods in accuracy and pruned rate, which significantly reduces the storage space and accelerate the prediction speed of deep forest.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323004429",
    "keywords": [
      "Agronomy",
      "Artificial intelligence",
      "Biology",
      "Cascade",
      "Chemical engineering",
      "Chemistry",
      "Computer science",
      "Data mining",
      "Engineering",
      "Ensemble forecasting",
      "Layer (electronics)",
      "Machine learning",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Physics",
      "Pruning",
      "Quantum mechanics",
      "Scale (ratio)"
    ],
    "authors": [
      {
        "surname": "Ji",
        "given_name": "Junzhong"
      },
      {
        "surname": "Li",
        "given_name": "Junwei"
      }
    ]
  },
  {
    "title": "SMPR: Single-stage multi-person pose regression",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109743",
    "abstract": "Existing multi-person pose estimators can be roughly divided into two-stage approaches (top-down and bottom-up approaches) and one-stage approaches. The two-stage methods either suffer high computational redundancy for additional person detectors or group keypoints heuristically after predicting all the instance-free keypoints. The recently proposed single-stage methods do not rely on the above two extra stages but have lower performance than the latest bottom-up approaches. In this work, a novel single-stage multi-person pose regression, termed SMPR, is presented. It follows the paradigm of dense prediction and predicts instance-aware keypoints from every location. Besides feature aggregation, we propose better strategies to define positive pose hypotheses for training which all play an important role in dense pose estimation. The network also learns the scores of estimated poses. The pose scoring strategy further improves the pose estimation performance by prioritizing superior poses during non-maximum suppression (NMS). We show that our method not only outperforms existing single-stage methods but also be competitive with the latest bottom-up methods, with 70.2 AP and 77.5 AP75 on the COCO test-dev pose benchmark. The code is available at https://github.com/cmdi-dlut/SMPR .",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323004417",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Code (set theory)",
      "Computer science",
      "Estimator",
      "Feature (linguistics)",
      "Geodesy",
      "Geography",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pose",
      "Programming language",
      "Redundancy (engineering)",
      "Regression",
      "Set (abstract data type)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Miao",
        "given_name": "Huixin"
      },
      {
        "surname": "Lin",
        "given_name": "Junqi"
      },
      {
        "surname": "Cao",
        "given_name": "Junjie"
      },
      {
        "surname": "He",
        "given_name": "Xiaoguang"
      },
      {
        "surname": "Su",
        "given_name": "Zhixun"
      },
      {
        "surname": "Liu",
        "given_name": "Risheng"
      }
    ]
  },
  {
    "title": "MultiCardioNet: Interoperability between ECG and PPG biometrics",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.09.009",
    "abstract": "Compared to other well-known biometric technologies based on physiological traits (e.g., fingerprint, iris, and face), heart biometrics are more robust to presentation attacks and are particularly suitable for continuous/periodic recognition. Most studies on heart biometrics concern electrocardiogram (ECG) and photoplethysmogram (PPG). While the reported results are encouraging, to the best of our knowledge, no studies have been conducted on the interoperability between ECG and PPG biometrics. We present a novel method that is capable of performing single-domain and multiple-domain identity verifications for ECG and PPG signals, providing interoperability between the heterogeneous cardiac signals. Our method does not require the computation of any reference/fiducial point and uses a compact representation of the given signals. We propose MultiCardioNet, a novel Siamese neural network trained by using an ad hoc learning algorithm. MultiCardioNet computes a similarity score between two spectrogram-based representations of cardiac signals. Our learning algorithm iteratively computes a balanced subset of genuine and impostor pairs during the training epochs. We performed experiments on a dataset containing 1,008 pairs of ECG and PPG samples, obtaining accuracy comparable to that of the state-of-the-art methods for single-domain scenarios and demonstrating only a relatively small performance decrease in the multiple-domain scenario.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552300257X",
    "keywords": [
      "Artificial intelligence",
      "Biometrics",
      "Computer science",
      "Computer vision",
      "Domain (mathematical analysis)",
      "Filter (signal processing)",
      "Frequency domain",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Photoplethysmogram",
      "Spectrogram"
    ],
    "authors": [
      {
        "surname": "Donida Labati",
        "given_name": "Ruggero"
      },
      {
        "surname": "Piuri",
        "given_name": "Vincenzo"
      },
      {
        "surname": "Rundo",
        "given_name": "Francesco"
      },
      {
        "surname": "Scotti",
        "given_name": "Fabio"
      }
    ]
  },
  {
    "title": "An encoded histogram of ridge bifurcations and contours for fingerprint presentation attack detection",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109782",
    "abstract": "In recent years, the exponential growth of internet technologies has made personal authentication an integral part of security applications. The fingerprint-based biometric systems are essentially used to safeguard the users' privacy and confidentiality. However, such systems are prone to spoof attacks by artificial replicas of the fingerprints. This paper presents an improved feature extractor called BiRi-PAD (Encoded Histogram of Ridge Bifurcations and Contours for fingerprint Presentation Attack Detection) that aims to enhance the accuracy of live fingerprint detection. The feature extraction process of the proposed BiRi-PAD consists of four steps. First, the fingerprint image undergoes a process of extracting 2-channel ridge contour maps (2-RC maps). The first channel of 2-RC maps consists of ridge contours extracted by a set of derivative filters in the spatial domain whereas the second channel of 2-RC maps consists of the ridge contours extracted by the maximum moments based on phase congruency in the frequency domain. Further, minutiae-based feature information i.e., ridge bifurcations are extracted by the minimum moments based on phase congruency covariance. Moreover, a fusion equation is proposed to integrate 2-RC maps and bifurcations into a single feature map. Second, an improved Comprehensive Local Phase Quantization (CLPQ) based on the well-known feature descriptor Rotation Invariant Local Phase Quantization ( L P Q r i ) is proposed to extract the phase information of ridges. CLPQ extracts the orientation of the ridges by using the complex parts of the significant frequency components of L P Q r i and monogenic filters. Third, the proposed BiRi-PAD quantizes the 2-RC maps into pre-determined intervals. Finally, both 2-RC maps and CLPQ features are integrated to generate a feature vector of a single fingerprint image. Performance evaluations of BiRi-PAD are conducted on three publicly available benchmarks from the LivDet competition, namely LivDet 2013, 2011, and 2015. Experimental evaluations demonstrate that the proposed BiRi-PAD achieves significant reductions in average rates compared to state-of-the-art techniques of fingerprint liveness detection. Specifically, on LivDet 2013, LivDet 2011, and LivDet 2015, the average rates are reduced to 1.92%, 4.39%, and 4.55%, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323004806",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Fingerprint (computing)",
      "Fingerprint recognition",
      "Geology",
      "Histogram",
      "Image (mathematics)",
      "Linguistics",
      "Minutiae",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Quantization (signal processing)",
      "Ridge"
    ],
    "authors": [
      {
        "surname": "Mehboob",
        "given_name": "Rubab"
      },
      {
        "surname": "Dawood",
        "given_name": "Hassan"
      },
      {
        "surname": "Dawood",
        "given_name": "Hussain"
      }
    ]
  },
  {
    "title": "MKConv: Multidimensional feature representation for point cloud analysis",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109800",
    "abstract": "Despite the remarkable success of deep learning, an optimal convolution operation on point clouds remains elusive owing to their irregular data structure. Existing methods mainly focus on designing an effective continuous kernel function that can handle an arbitrary point in continuous space. Various approaches exhibiting high performance have been proposed, but we observe that the standard pointwise feature is represented by 1D channels and can become more informative when its representation involves additional spatial feature dimensions. In this paper, we present Multidimensional Kernel Convolution (MKConv), a novel convolution operator that learns to transform the point feature representation from a vector to a multidimensional matrix. Unlike standard point convolution, MKConv proceeds via two steps. (i) It first activates the spatial dimensions of local feature representation by exploiting multidimensional kernel weights. These spatially expanded features can represent their embedded information through spatial correlation as well as channel correlation in feature space, carrying more detailed local structure information. (ii) Then, discrete convolutions are applied to the multidimensional features which can be regarded as a grid-structured matrix. In this way, we can utilize the discrete convolutions for point cloud data without voxelization that suffers from information loss. Furthermore, we propose a spatial attention module, Multidimensional Local Attention (MLA), to provide comprehensive structure awareness within the local point set by reweighting the spatial feature dimensions. We demonstrate that MKConv has excellent applicability to point cloud processing tasks including object classification, object part segmentation, and scene semantic segmentation with superior results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323004983",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Combinatorics",
      "Computer science",
      "Convolution (computer science)",
      "Feature (linguistics)",
      "Feature learning",
      "Feature vector",
      "Focus (optics)",
      "Kernel (algebra)",
      "Law",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Point cloud",
      "Pointwise",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Woo",
        "given_name": "Sungmin"
      },
      {
        "surname": "Lee",
        "given_name": "Dogyoon"
      },
      {
        "surname": "Hwang",
        "given_name": "Sangwon"
      },
      {
        "surname": "Kim",
        "given_name": "Woo Jin"
      },
      {
        "surname": "Lee",
        "given_name": "Sangyoun"
      }
    ]
  },
  {
    "title": "Improving the matching of deformable objects by learning to detect keypoints",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.08.012",
    "abstract": "We propose a novel learned keypoint detection method to increase the number of correct matches for the task of non-rigid image correspondence. By leveraging true correspondences acquired by matching annotated image pairs with a specified descriptor extractor, we train an end-to-end convolutional neural network (CNN) to find keypoint locations that are more appropriate to the considered descriptor. Experiments demonstrate that our method enhances the Mean Matching Accuracy of numerous descriptors when used in conjunction with our detection method, while outperforming the state-of-the-art keypoint detectors on real images of non-rigid objects by 20 p.p. We also apply our method on the complex real-world task of object retrieval where our detector performs on par with the finest keypoint detectors currently available for this task. The source code and trained models are publicly available at https://github.com/verlab/LearningToDetect_PRL_2023.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523002325",
    "keywords": [
      "Artificial intelligence",
      "Code (set theory)",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Detector",
      "Economics",
      "Engineering",
      "Extractor",
      "Image (mathematics)",
      "Management",
      "Matching (statistics)",
      "Mathematics",
      "Object (grammar)",
      "Object detection",
      "Operating system",
      "Pattern recognition (psychology)",
      "Process engineering",
      "Programming language",
      "Set (abstract data type)",
      "Source code",
      "Statistics",
      "Task (project management)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Cadar",
        "given_name": "Felipe"
      },
      {
        "surname": "Melo",
        "given_name": "Welerson"
      },
      {
        "surname": "Kanagasabapathi",
        "given_name": "Vaishnavi"
      },
      {
        "surname": "Potje",
        "given_name": "Guilherme"
      },
      {
        "surname": "Martins",
        "given_name": "Renato"
      },
      {
        "surname": "Nascimento",
        "given_name": "Erickson R."
      }
    ]
  },
  {
    "title": "Matching based on variance minimization of component distances using edges of free-form surfaces",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109729",
    "abstract": "The basis for guidance in the field of automated robot processing is modeling by visual scanning. Matching algorithms are the real link that can be made between the ideal model and the subsequent robot processing. The matching algorithm that meets the machining requirements plays a pivotal role in the entire process, which provides the exact location of design models and measurement data. In order to meet the requirement of making the machining allowance uniform, a fine registration method which considers the variance minimization of the normal and tangential distances between the edge neighbors of the scattered point cloud is proposed. The edge neighbors are achieved by local growing of edge seeds based on the minimization of energy of supervoxel, which can represent the model more accurate than edge points extracted directly. The edge neighbor points are used to participate in the calculation, and the objective function of minimizing the variance of the two-way distance with the introduction of weight coefficients is proposed to constrain the iterative process. The effect of the distance in two directions on the result is analyzed, to determine the appropriate weight coefficients so that the matching calculation converges quickly and accurately. In comparison with other classical and state-of-the-art matching methods, the method in this paper performs well in terms of solution efficiency and accuracy of results. Moreover, the ability of this paper’s method to resist Gaussian noise is investigated, and it is found that this paper’s method has good robustness when σ is less than 1 for Gaussian noise. Ultimately, a uniformly distributed residual model is obtained to provide a visually guided basis for subsequent machining.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323004272",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Basis (linear algebra)",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Enhanced Data Rates for GSM Evolution",
      "Gaussian",
      "Gene",
      "Geometry",
      "Matching (statistics)",
      "Mathematical optimization",
      "Mathematics",
      "Minification",
      "Physics",
      "Point cloud",
      "Quantum mechanics",
      "Robot",
      "Robustness (evolution)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Sun",
        "given_name": "Jingyu"
      },
      {
        "surname": "Gong",
        "given_name": "Yadong"
      },
      {
        "surname": "Zhao",
        "given_name": "Jibin"
      },
      {
        "surname": "Zhang",
        "given_name": "Huan"
      },
      {
        "surname": "Jin",
        "given_name": "Liya"
      }
    ]
  },
  {
    "title": "AGMN: Association graph-based graph matching network for coronary artery semantic labeling on invasive coronary angiograms",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109789",
    "abstract": "Semantic labeling of coronary arterial segments in invasive coronary angiography (ICA) is important for automated assessment and report generation of coronary artery stenosis in computer-aided coronary artery disease (CAD) diagnosis. However, separating and identifying individual coronary arterial segments is challenging because morphological similarities of different branches on the coronary arterial tree and human-to-human variabilities exist. Inspired by the training procedure of interventional cardiologists for interpreting the structure of coronary arteries, we propose an association graph-based graph matching network (AGMN) for coronary arterial semantic labeling. We first extract the vascular tree from invasive coronary angiography (ICA) and convert it into multiple individual graphs. Then, an association graph is constructed from two individual graphs where each vertex represents the relationship between two arterial segments. Thus, we convert the arterial segment labeling task into a vertex classification task; ultimately, the semantic artery labeling becomes equivalent to identifying the artery-to-artery correspondence on graphs. More specifically, the AGMN extracts the vertex features by the embedding module using the association graph, aggregates the features from adjacent vertices and edges by graph convolution network, and decodes the features to generate the semantic mappings between arteries. By learning the mapping of arterial branches between two individual graphs, the unlabeled arterial segments are classified by the labeled segments to achieve semantic labeling. A dataset containing 263 ICAs was employed to train and validate the proposed model, and a five-fold cross-validation scheme was performed. Our AGMN model achieved an average accuracy of 0.8264, an average precision of 0.8276, an average recall of 0.8264, and an average F1-score of 0.8262, which significantly outperformed existing coronary artery semantic labeling methods. In conclusion, we have developed and validated a new algorithm with high accuracy, interpretability, and robustness for coronary artery semantic labeling on ICAs.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323004879",
    "keywords": [
      "Artery",
      "Artificial intelligence",
      "Cardiology",
      "Computer science",
      "Coronary artery disease",
      "Graph",
      "Internal medicine",
      "Medicine",
      "Pattern recognition (psychology)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Chen"
      },
      {
        "surname": "Xu",
        "given_name": "Zhihui"
      },
      {
        "surname": "Jiang",
        "given_name": "Jingfeng"
      },
      {
        "surname": "Esposito",
        "given_name": "Michele"
      },
      {
        "surname": "Pienta",
        "given_name": "Drew"
      },
      {
        "surname": "Hung",
        "given_name": "Guang-Uei"
      },
      {
        "surname": "Zhou",
        "given_name": "Weihua"
      }
    ]
  },
  {
    "title": "Integrating topology beyond descriptions for zero-shot learning",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109738",
    "abstract": "Zero-shot learning (ZSL) aims to discriminate object categories through the identification of their attributes and has received much attention for its capability to predict unseen categories without collecting training data. Recently, excellent works have been devoted to optimizing the model inference by mining the topology among categories/attributes, which proves that the topology learning is beneficial and important for ZSL. However, existing works focus almost exclusively on the construction of semantic topological knowledge with textual descriptions, which, though effective, still suffer from two deficiencies: first, the semantic gap between modalities makes it difficult for the category attributes to accurately describe the corresponding visual characters, resulting in the topology constructed in the semantic modality being distorted in the visual modality; second, it is difficult for one to enumerate all the attributes hidden in images, resulting in an incomplete topology mined only from the defined attributes. Therefore, we propose a Cross-Modality Topology Propagation Matcher (CTPM) to construct a more complete topology system by collaborative mining of topological knowledge in both the visual and semantic modalities. We stand at the dataset level to construct sample-based visual topological knowledge based on the global image features to preserve the integrity of visual information. Meanwhile, we exploit the matching relationship between visual and semantic modalities to make topological knowledge propagate effectively across modalities, and fully enjoy the benefits of multi-modality topological knowledge in category/attribute reasoning. We validate the effectiveness of our CTPM through extensive experiments and achieve state-of-the-art performance on four ZSL datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323004363",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Combinatorics",
      "Computer science",
      "Construct (python library)",
      "Identification (biology)",
      "Inference",
      "Machine learning",
      "Matching (statistics)",
      "Mathematics",
      "Modalities",
      "Modality (human–computer interaction)",
      "Object (grammar)",
      "Programming language",
      "Social science",
      "Sociology",
      "Statistics",
      "Topology (electrical circuits)"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Ziyi"
      },
      {
        "surname": "Gao",
        "given_name": "Yutong"
      },
      {
        "surname": "Lang",
        "given_name": "Congyan"
      },
      {
        "surname": "Wei",
        "given_name": "Lili"
      },
      {
        "surname": "Li",
        "given_name": "Yidong"
      },
      {
        "surname": "Liu",
        "given_name": "Hongzhe"
      },
      {
        "surname": "Liu",
        "given_name": "Fayao"
      }
    ]
  },
  {
    "title": "Resampling approach for one-Class classification",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109731",
    "abstract": "The performance of a classification model depends significantly on the degree to which the support of each data class overlaps. Successfully distinguishing between classes is difficult if the support is similar. In the one-class classification (OCC) problem, wherein the data comprise only a single class, the classifier performance is significantly degraded if the population support of each class is similar. In this study, we propose a resampling algorithm that enhances classifier performance by utilizing the macro information that is most easily obtainable in these two problem situations. The algorithm aims to improve classifier performance by reprocessing the given data into data with mitigated class imbalance through raking and sampling techniques. This performance improvement is demonstrated by comparing representative classifiers used in the existing OCC problem with traditional binary classifier models, which are unavailable on a single-class dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323004296",
    "keywords": [
      "Artificial intelligence",
      "Binary classification",
      "Class (philosophy)",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Machine learning",
      "Macro",
      "Pattern recognition (psychology)",
      "Programming language",
      "Resampling",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Lee",
        "given_name": "Hae-Hwan"
      },
      {
        "surname": "Park",
        "given_name": "Seunghwan"
      },
      {
        "surname": "Im",
        "given_name": "Jongho"
      }
    ]
  },
  {
    "title": "Domain embedding transfer for unequal RGB-D image recognition",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109771",
    "abstract": "Most recent unsupervised domain adaptation (UDA) approaches concentrate on single RGB source to single RGB target task. They have to face the real-world scenario, where the source domain can be collected from multiple modalities, e.g., RGB data and depth data. Our work focuses on a more practical and challenging scenario which recognizes RGB images by learning from RGB-D data under the label inequality scenario. We are confronted with three challenges: multiple modalities in the source domain, domain shifting problem and unequal label numbers. To address the aforementioned settings, a novel method, referred to as Domain depth Embedding Transfer (DdET) is proposed, which takes advantage of the depth data in the source domain and handles the domain distribution mismatch under label inequality scenario simultaneously. We conduct comprehensive experiments on five cross domain image classification tasks and observe that DdET can perform favorably against state-of-the-art methods, especially under label inequality scenario.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323004697",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Computer vision",
      "Domain (mathematical analysis)",
      "Domain adaptation",
      "Embedding",
      "Image (mathematics)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "RGB color model",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Cai",
        "given_name": "Ziyun"
      },
      {
        "surname": "Jing",
        "given_name": "Xiao-Yuan"
      },
      {
        "surname": "Shao",
        "given_name": "Ling"
      }
    ]
  },
  {
    "title": "Mixed data clustering based on a number of similar features",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109815",
    "abstract": "Finding the degree of similarity measurement is one of the challenges of mixed data clustering. In this article, it has been tried to design a more efficient method by innovating in three important parts of clustering. In the part of the general method, for assigning data objects to the cluster, in addition to the distance, attention is paid to the “number of similar features”. Compared to assigning each object to a cluster, in cases where the distances are equal or close, the cluster center with the highest number of features similar to the given objects will be appropriate. This method is more accurate than the Hamming distance. To determine the cluster centers, instead of random selection, a more suitable object is identified with a distance-based method. In accuracy in three datasets, the proposed algorithm has performed at least two percent better than the other algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005137",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "CURE data clustering algorithm",
      "Cluster (spacecraft)",
      "Cluster analysis",
      "Complete-linkage clustering",
      "Computer science",
      "Data mining",
      "Fuzzy clustering",
      "Hamming distance",
      "Image (mathematics)",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Programming language",
      "Selection (genetic algorithm)",
      "Similarity (geometry)",
      "Single-linkage clustering",
      "k-medians clustering"
    ],
    "authors": [
      {
        "surname": "Rezaei",
        "given_name": "Hamid"
      },
      {
        "surname": "Daneshpour",
        "given_name": "Negin"
      }
    ]
  },
  {
    "title": "Contrastive deep support vector data description",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109820",
    "abstract": "In comparison with support vector data description (SVDD), deep SVDD (DSVDD) is more suitable for dealing with large-scale data sets. DSVDD uses mapping network to replace the role of kernel mapping in SVDD. Moreover, the objective of DSVDD is to simultaneously learn the optimal connection weights of mapping network and the minimum volume of hypersphere. To further improve the performance of DSVDD for tackling large-scale data sets and obtain the discriminative features of the given samples in a self-supervised learning manner, contrastive DSVDD (CDSVDD) is proposed in this study. In the pre-training phase of CDSVDD, the contrastive loss and the rotation prediction loss are jointly minimized to achieve the optimal feature representations. Furthermore, the learned feature representations are utilized to determine the hypersphere center. In the training phase of CDSVDD, the distances between the obtained feature representations and the hypersphere center together with the contrastive loss are simultaneously minimized to derive the optimal network connection weights, the minimum volume of hypersphere and the optimal feature representations. In addition, CDSVDD can efficiently solve the hypersphere collapse problem of DSVDD. The ablation study on CDSVDD verifies that compared with the case of determining the hypersphere center by the feature representations of the original samples, the hypersphere center determined by the feature representations of the augmented samples makes CDSVDD achieve better hypersphere boundary and more compact feature representations. Experimental results on the four benchmark data sets demonstrate that the proposed CDSVDD acquires better detection performance in comparison with its six pertinent methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005186",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Combinatorics",
      "Computer science",
      "Discriminative model",
      "Feature (linguistics)",
      "Feature vector",
      "Geodesy",
      "Geography",
      "Hypersphere",
      "Kernel (algebra)",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Xing",
        "given_name": "Hong-Jie"
      },
      {
        "surname": "Zhang",
        "given_name": "Ping-Ping"
      }
    ]
  },
  {
    "title": "DITAN: A deep-learning domain agnostic framework for detection and interpretation of temporally-based multivariate ANomalies",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109814",
    "abstract": "We present DITAN, a novel unsupervised domain-agnostic framework for detecting and interpreting temporal-based anomalies. It is based on an encoder-decoder architecture with both implicit/explicit attention and adjustable layers/units for predicting normality as regular patterns in sequential data. A two-stage thresholding methodology with built-in pruning is used to detect anomalies, while root cause and similarities are interpreted in data and units space. Our approach is designed to intersect the 9 fundamental characteristics extracted from the union of related works. We demonstrate the DITAN modules on real-world datasets of 6 multivariate time series contaminated by point and contextual temporal-based anomalies at a varying duration. Experiments show a dominant predictability power of DITAN against the originally proposed models. DITAN is able to determine critical regions and thus identify anomalous events similarly well. Informative similarities between anomalous records are interpreted, since almost all similarities in units space are also verified in data space.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005125",
    "keywords": [
      "Agronomy",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Data mining",
      "Geometry",
      "Machine learning",
      "Mathematics",
      "Multivariate statistics",
      "Pattern recognition (psychology)",
      "Point (geometry)",
      "Pruning"
    ],
    "authors": [
      {
        "surname": "Giannoulis",
        "given_name": "Michail"
      },
      {
        "surname": "Harris",
        "given_name": "Andrew"
      },
      {
        "surname": "Barra",
        "given_name": "Vincent"
      }
    ]
  },
  {
    "title": "Exploring multivariate generalized gamma manifold for color texture retrieval",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109748",
    "abstract": "This work proposes a novel method for color-textured image retrieval on a Multivariate Generalized Gamma Distribution manifold (MG Γ D). Thanks to the Gaussian copula theory, we define the expression of MG Γ D, which efficiently models the statistical dependence structure between dual-tree complex wavelet transform (DTCWT) of the color components. The major contribution of this paper is to provide a geometric perspective to the MG Γ D by treating it as a Riemannian manifold while proposing the geodesic distance (GD) as a measure of Riemannian similarity on it. Based on information geometry tools, we conduct a geometrical study of the MG Γ D manifold, allowing us to derive two suitable approximations of the GD. The experiments are performed on five well-known color texture databases, considering the content-based image retrieval (CBIR) framework and using the RGB color space. The obtained results demonstrate the efficiency of the geometric interpretation through the proposed GD as a natural and intuitive similarity measure on the studied statistical manifold.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323004466",
    "keywords": [
      "Artificial intelligence",
      "Complex wavelet transform",
      "Computer science",
      "Computer vision",
      "Curvature",
      "Discrete wavelet transform",
      "Geodesic",
      "Geometry",
      "Image (mathematics)",
      "Image retrieval",
      "Information geometry",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pure mathematics",
      "RGB color model",
      "Riemannian manifold",
      "Scalar curvature",
      "Similarity measure",
      "Statistical manifold",
      "Wavelet",
      "Wavelet transform"
    ],
    "authors": [
      {
        "surname": "Abbad",
        "given_name": "Zakariae"
      },
      {
        "surname": "El Maliani",
        "given_name": "Ahmed Drissi"
      },
      {
        "surname": "El Alaoui",
        "given_name": "Said Ouatik"
      },
      {
        "surname": "El Hassouni",
        "given_name": "Mohammed"
      },
      {
        "surname": "Abbassi",
        "given_name": "Mohamed Tahar Kadaoui"
      }
    ]
  },
  {
    "title": "MSINet: Mining scale information from digital surface models for semantic segmentation of aerial images",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109785",
    "abstract": "Compared with other kinds of images, aerial images have more obvious object scale distinction and larger resolution, which results in that the whole scale information of aerial images can hardly be explored. To address this difficulty, we develop a novel network based on the digital surface models (DSMs) of aerial images in this paper. The proposed network termed MSINet can efficiently mine scale information through the DSMs from two aspects. Firstly, we propose an interpolation pyramid algorithm to encode the scale information from the DSMs and hence provide a scale prior information to the normal segmentation network. The interpolation pyramid algorithm implements interpolation operations with different scales on the DSMs and detects the pixel value change after the interpolation operations. Objects with different scales will express diverse changes, which provides useful information to encode their scale information. Secondly, aiming to address the problem that the DSMs contain a large amount of noise in the boundary part, a spatial information enhancement module and a mutual-guidance module are developed in this paper. These two modules can fix the misleading guidance information caused by the noise in the boundary part of the DSMs and hence achieve more accurate scale information inserting. The extensive experimental results prove that our methods can outperform other competitors in terms of qualitative and quantitative performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323004831",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Data mining",
      "ENCODE",
      "Gene",
      "Geometry",
      "Image (mathematics)",
      "Interpolation (computer graphics)",
      "Mathematics",
      "Noise (video)",
      "Pattern recognition (psychology)",
      "Physics",
      "Pyramid (geometry)",
      "Quantum mechanics",
      "Scale (ratio)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Peng",
        "given_name": "Chengli"
      },
      {
        "surname": "Li",
        "given_name": "Haifeng"
      },
      {
        "surname": "Tao",
        "given_name": "Chao"
      },
      {
        "surname": "Li",
        "given_name": "Yansheng"
      },
      {
        "surname": "Ma",
        "given_name": "Jiayi"
      }
    ]
  },
  {
    "title": "FedCL: Federated contrastive learning for multi-center medical image classification",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109739",
    "abstract": "Federated learning, which allows distributed medical institutions to train a shared deep learning model with privacy protection, has become increasingly popular recently. However, in practical application, due to data heterogeneity between different hospitals, the performance of the model will be degraded in the training process. In this paper, we propose a federated contrastive learning (FedCL) approach. FedCL integrates the idea of contrastive learning into the federated learning framework. Specifically, it combines the local model and the global model for contrastive learning, so that the local model gradually approaches the global model with the increase of communication rounds, which improves the generalization ability of the model. We validate our method on two public datasets. Extensive experiments show that our method is superior to other federated learning algorithms in medical image classification.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323004375",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Deep learning",
      "Federated learning",
      "Generalization",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Operating system",
      "Process (computing)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Zhenbing"
      },
      {
        "surname": "Wu",
        "given_name": "Fengfeng"
      },
      {
        "surname": "Wang",
        "given_name": "Yumeng"
      },
      {
        "surname": "Yang",
        "given_name": "Mengyu"
      },
      {
        "surname": "Pan",
        "given_name": "Xipeng"
      }
    ]
  },
  {
    "title": "Modeling global distribution for federated learning with label distribution skew",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109724",
    "abstract": "Federated learning achieves joint training of deep models by connecting decentralized datasources, which can significantly mitigate the risk of privacy leakage. However, in a more general case, the distributions of labels among clients are different, called “label distribution skew”. Directly applying conventional federated learning without consideration of label distribution skew issue significantly hurts the performance of the global model. To this end, we propose a novel federated learning method, named FedMGD, to alleviate the performance degradation caused by the label distribution skew issue. It introduces a global Generative Adversarial Network to model the global data distribution without access to local datasets, so the global model can be trained using the global information of data distribution without privacy leakage. The experimental results demonstrate that our proposed method significantly outperforms the state-of-the-art on several public benchmarks. Code is available at https://www.github.com/Sheng-T/FedMGD.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323004223",
    "keywords": [
      "Artificial intelligence",
      "Code (set theory)",
      "Computer science",
      "Data mining",
      "Deep learning",
      "Distribution (mathematics)",
      "Federated learning",
      "Generative adversarial network",
      "Joint probability distribution",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Programming language",
      "Set (abstract data type)",
      "Skew",
      "Statistics",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Sheng",
        "given_name": "Tao"
      },
      {
        "surname": "Shen",
        "given_name": "Chengchao"
      },
      {
        "surname": "Liu",
        "given_name": "Yuan"
      },
      {
        "surname": "Ou",
        "given_name": "Yeyu"
      },
      {
        "surname": "Qu",
        "given_name": "Zhe"
      },
      {
        "surname": "Liang",
        "given_name": "Yixiong"
      },
      {
        "surname": "Wang",
        "given_name": "Jianxin"
      }
    ]
  },
  {
    "title": "Momentum contrast transformer for COVID-19 diagnosis with knowledge distillation",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109732",
    "abstract": "Intelligent diagnosis has been widely studied in diagnosing novel corona virus disease (COVID-19). Existing deep models typically do not make full use of the global features such as large areas of ground glass opacities, and the local features such as local bronchiolectasis from the COVID-19 chest CT images, leading to unsatisfying recognition accuracy. To address this challenge, this paper proposes a novel method to diagnose COVID-19 using momentum contrast and knowledge distillation, termed MCT-KD. Our method takes advantage of Vision Transformer to design a momentum contrastive learning task to effectively extract global features from COVID-19 chest CT images. Moreover, in transfer and fine-tuning process, we integrate the locality of convolution into Vision Transformer via special knowledge distillation. These strategies enable the final Vision Transformer simultaneously focuses on global and local features from COVID-19 chest CT images. In addition, momentum contrastive learning is self-supervised learning, solving the problem that Vision Transformer is challenging to train on small datasets. Extensive experiments confirm the effectiveness of the proposed MCT-KD. In particular, our MCT-KD is able to achieve 87.43% and 96.94% accuracy on two publicly available datasets, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323004302",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Contrast (vision)",
      "Coronavirus disease 2019 (COVID-19)",
      "Disease",
      "Electrical engineering",
      "Engineering",
      "Infectious disease (medical specialty)",
      "Linguistics",
      "Locality",
      "Machine learning",
      "Medicine",
      "Pathology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Dong",
        "given_name": "Aimei"
      },
      {
        "surname": "Liu",
        "given_name": "Jian"
      },
      {
        "surname": "Zhang",
        "given_name": "Guodong"
      },
      {
        "surname": "Wei",
        "given_name": "Zhonghe"
      },
      {
        "surname": "Zhai",
        "given_name": "Yi"
      },
      {
        "surname": "Lv",
        "given_name": "Guohua"
      }
    ]
  },
  {
    "title": "Longitudinal prediction of postnatal brain magnetic resonance images via a metamorphic generative adversarial network",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109715",
    "abstract": "Missing scans are inevitable in longitudinal studies due to either subject dropouts or failed scans. In this paper, we propose a deep learning framework to predict missing scans from acquired scans, catering to longitudinal infant studies. Prediction of infant brain MRI is challenging owing to the rapid contrast and structural changes particularly during the first year of life. We introduce a trustworthy metamorphic generative adversarial network (MGAN) for translating infant brain MRI from one time point to another. MGAN has three key features: (i) Image translation leveraging spatial and frequency information for detail-preserving mapping; (ii) Quality-guided learning strategy that focuses attention on challenging regions. (iii) Multi-scale hybrid loss function that improves translation of image contents. Experimental results indicate that MGAN outperforms existing GANs by accurately predicting both tissue contrasts and anatomical details.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323004132",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Biochemistry",
      "Biology",
      "Chemistry",
      "Computer science",
      "Deep learning",
      "Evolutionary biology",
      "Function (biology)",
      "Gene",
      "Generative grammar",
      "Generative model",
      "Image (mathematics)",
      "Machine learning",
      "Magnetic resonance imaging",
      "Medicine",
      "Messenger RNA",
      "Neuroimaging",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Psychology",
      "Radiology",
      "Translation (biology)"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Yunzhi"
      },
      {
        "surname": "Ahmad",
        "given_name": "Sahar"
      },
      {
        "surname": "Han",
        "given_name": "Luyi"
      },
      {
        "surname": "Wang",
        "given_name": "Shuai"
      },
      {
        "surname": "Wu",
        "given_name": "Zhengwang"
      },
      {
        "surname": "Lin",
        "given_name": "Weili"
      },
      {
        "surname": "Li",
        "given_name": "Gang"
      },
      {
        "surname": "Wang",
        "given_name": "Li"
      },
      {
        "surname": "Yap",
        "given_name": "Pew-Thian"
      }
    ]
  },
  {
    "title": "TCX: Texture and channel swappings for domain generalization",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.10.006",
    "abstract": "Neural networks suffer performance degradation when the source and target data lie in a different distribution hampering direct deployment of the model to diverse target domains. To this end, domain generalization (DG) aims to generalize the model well to an unknown target domain by utilizing multiple source domains. This paper proposes two simple swapping mechanisms, texture and channel swapping (TCX), for DG. Texture swapping augments the source dataset by replacing textures of an image with other textures in source dataset to alleviate the texture bias problem in convolution neural networks (CNNs). Furthermore, channel swapping switches channels of input feature vectors of the classifier along with its labels to encourage the model to utilize more channels when classifying an image. Together, we expect the model to learn less domain-specific features but more generalized class-specific features resulting in better domain generalization performance. We demonstrate the effectiveness of our approach with state-of-the-art results on three domain generalization benchmarks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523002787",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Channel (broadcasting)",
      "Classifier (UML)",
      "Computer network",
      "Computer science",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Domain (mathematical analysis)",
      "Feature (linguistics)",
      "Generalization",
      "Image (mathematics)",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Choi",
        "given_name": "Jaehyun"
      },
      {
        "surname": "Seong",
        "given_name": "Hyun Seok"
      },
      {
        "surname": "Park",
        "given_name": "Sanguk"
      },
      {
        "surname": "Heo",
        "given_name": "Jae-Pil"
      }
    ]
  },
  {
    "title": "Guided deep embedded clustering regularization for multifeature medical signal classification",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109812",
    "abstract": "Medical signal classification often focuses on one representation (raw signal or time frequency). In that context, recent works have shown the value of exploiting different representations simultaneously. We propose a regularized end-to-end trained model for classification in a medical context exploiting both the raw signal and a time-frequency representation (TFR). First, a 2D convolutional neural network (CNN) encoder and a 1D CNN-transformer encoder start by extracting embedded representations from the TFR and the raw signal, respectively. Then, the obtained embeddings are fused to form a common latent space that is used for classification. We propose to guide the training of each encoder by applying two iterated losses. Moreover, we propose to regularize the fused common latent space using deep embedded clustering. Extensive experiments on three medical datasets and ablation studies show the adaptability and good performance of our method for medical signal classification. Our method makes it possible to improve the classification performance from 4 % to 12 % MCC on a transcranial Doppler dataset, when compared with single-feature counterparts, while giving more stable models. The code is available at: https://github.com/gdec-submission/gdec/.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005101",
    "keywords": [
      "Artificial intelligence",
      "Autoencoder",
      "Cluster analysis",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Encoder",
      "Feature learning",
      "Operating system",
      "Pattern recognition (psychology)",
      "Regularization (linguistics)"
    ],
    "authors": [
      {
        "surname": "Vindas",
        "given_name": "Yamil"
      },
      {
        "surname": "Roux",
        "given_name": "Emmanuel"
      },
      {
        "surname": "Guépié",
        "given_name": "Blaise Kévin"
      },
      {
        "surname": "Almar",
        "given_name": "Marilys"
      },
      {
        "surname": "Delachartre",
        "given_name": "Philippe"
      }
    ]
  },
  {
    "title": "Attention‐guided evolutionary attack with elastic‐net regularization on face recognition",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109760",
    "abstract": "In recent years, face recognition has achieved promising results along with the development of advanced Deep Neural Networks (DNNs). The existing face recognition systems are vulnerable to adversarial examples, which brings potential security risks. Evolutionary Attack (EA) has been successfully used to fool face recognition by inducing a minimum perturbation to a face image with few queries. However, EA employs the global information of face images but ignores their local characteristics. In addition, restricting the ℓ 2 -norm of adversarial perturbations hinders the diversity of adversarial perturbations. To solve the above problems, we propose Attention-guided Evolutionary Attack with Elastic-Net Regularization (ERAEA) for attacking face recognition. ERAEA extracts local facial characteristics by attention mechanism, effectively improving the attack effect and image perception quality. In particular, ERAEA adopts an attention mechanism to guide evolutionary direction, which operates on the covariance matrix as it contains crucial information about the evolutionary path. Furthermore, we design an adaptive elastic-net regularization to diversify the adversarial perturbation, accelerating the optimization performance. Extensive experiments obtained on three benchmarks demonstrate that our proposed method achieves better perturbation norm than the state-of-the-art methods with limited queries on face recognition and generates adversarial face images with higher perceptual quality. Besides, ERAEA requires fewer queries to achieve a fixed adversarial perturbation norm.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323004582",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Biometrics",
      "Computer science",
      "Elastic net regularization",
      "Facial recognition system",
      "Feature selection",
      "Law",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Norm (philosophy)",
      "Pattern recognition (psychology)",
      "Perturbation (astronomy)",
      "Physics",
      "Political science",
      "Quantum mechanics",
      "Regularization (linguistics)"
    ],
    "authors": [
      {
        "surname": "Hu",
        "given_name": "Cong"
      },
      {
        "surname": "Li",
        "given_name": "Yuanbo"
      },
      {
        "surname": "Feng",
        "given_name": "Zhenhua"
      },
      {
        "surname": "Wu",
        "given_name": "Xiaojun"
      }
    ]
  },
  {
    "title": "Conditional pseudo-supervised contrast for data-Free knowledge distillation",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109781",
    "abstract": "Data-free knowledge distillation (DFKD) is an effective manner to solve model compression and transmission restrictions while retaining privacy protection, which has attracted extensive attention in recent years. Currently, the majority of existing methods utilize a generator to synthesize images to support the distillation. Although the current methods have achieved great success, there are still many issues to be explored. Firstly, the outstanding performance of supervised learning in deep learning drives us to explore a pseudo-supervised paradigm on DFKD. Secondly, current synthesized methods cannot distinguish the distributions of different categories of samples, thus producing ambiguous samples that may lead to an incorrect evaluation by the teacher. Besides, current methods cannot optimize the category-wise diversity samples, which will hinder the student model learning from diverse samples and further achieving better performance. In this paper, to address the above limitations, we propose a novel learning paradigm, i.e., conditional pseudo-supervised contrast for data-free knowledge distillation (CPSC-DFKD). The primary innovations of CPSC-DFKD are: (1) introducing a conditional generative adversarial network to synthesize category-specific diverse images for pseudo-supervised learning, (2) improving the modules of the generator to distinguish the distributions of different categories, and (3) proposing pseudo-supervised contrastive learning based on teacher and student views to enhance diversity. Comprehensive experiments on three commonly-used datasets validate the performance lift of both the student and generator brought by CPSC-DFKD. The code is available at https://github.com/RoryShao/CPSC-DFKD.git",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032300479X",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Chemistry",
      "Code (set theory)",
      "Computer science",
      "Contrast (vision)",
      "Distillation",
      "Generator (circuit theory)",
      "Lift (data mining)",
      "Machine learning",
      "Organic chemistry",
      "Physics",
      "Power (physics)",
      "Programming language",
      "Quantum mechanics",
      "Set (abstract data type)",
      "Supervised learning"
    ],
    "authors": [
      {
        "surname": "Shao",
        "given_name": "Renrong"
      },
      {
        "surname": "Zhang",
        "given_name": "Wei"
      },
      {
        "surname": "Wang",
        "given_name": "Jun"
      }
    ]
  },
  {
    "title": "An active foveated gaze prediction algorithm based on a Bayesian ideal observer",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109694",
    "abstract": "Predicting human eye movements is a crucial task for understanding human behavior and has numerous applications in machine vision. Most current models for predicting eye movements are data-driven and require large datasets of recorded eye movements, which can be expensive and time-consuming to collect. In this paper, we present a novel theory-based model for predicting eye movements in a foveated visual system that maximizes information gain at each fixation. Our model uses a region-proposal network and eccentricity-based max pooling to account for the loss of detail in peripheral vision. We apply our model to predict human fixations in a visual search task for objects in real-world scenes. Unlike data-driven models, our model does not require training on large eye movement datasets and can generalize to any set of natural images and targets. We evaluate the generalization capability of our model by demonstrating its results on two publicly available visual search datasets, Ehinger and COCO-search18, without any further training on those datasets. Our model outperforms or performs comparably to data-driven models that are directly trained on human eye movement datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323003928",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Demography",
      "Eye movement",
      "Eye tracking",
      "Fixation (population genetics)",
      "Gaze",
      "Generalization",
      "Human eye",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pooling",
      "Population",
      "Sociology",
      "Visual search"
    ],
    "authors": [
      {
        "surname": "Rashidi",
        "given_name": "Shima"
      },
      {
        "surname": "Xu",
        "given_name": "Weilun"
      },
      {
        "surname": "Lin",
        "given_name": "Dian"
      },
      {
        "surname": "Turpin",
        "given_name": "Andrew"
      },
      {
        "surname": "Kulik",
        "given_name": "Lars"
      },
      {
        "surname": "Ehinger",
        "given_name": "Krista"
      }
    ]
  },
  {
    "title": "Higher-order memory guided temporal random walk for dynamic heterogeneous network embedding",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109766",
    "abstract": "Network embedding (NE) aims at learning node embeddings via structure-based sampling. However, there are complex patterns in network structure (heterogeneity, higher-order dependence, dynamics) in the real world. The existing methods suffer from high dependence and constraints on manually designed higher-order structures and loss of fine-grained temporal information. To solve the above challenges, we propose a novel higher-order memory guided temporal random walk for dynamic heterogeneous network embedding (HoMo-DyHNE). The proposed model is a two-stage architecture consisting of a meta-structure-independent random walk algorithm namely HoMo-TRW with transition vectors and higher-order memory, and a Hawkes-based featured Skip-gram (HFSG) incorporating a multivariate Hawkes point process to measure the history-current association intensity. Extensive experiments demonstrate the superior effectiveness of our proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323004648",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Economics",
      "Embedding",
      "Engineering",
      "Finance",
      "Mathematics",
      "Measure (data warehouse)",
      "Node (physics)",
      "Order (exchange)",
      "Point process",
      "Random walk",
      "Statistics",
      "Structural engineering",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Ji",
        "given_name": "Cheng"
      },
      {
        "surname": "Zhao",
        "given_name": "Tao"
      },
      {
        "surname": "Sun",
        "given_name": "Qingyun"
      },
      {
        "surname": "Fu",
        "given_name": "Xingcheng"
      },
      {
        "surname": "Li",
        "given_name": "Jianxin"
      }
    ]
  },
  {
    "title": "Learning pixel-adaptive weights for portrait photo retouching",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109775",
    "abstract": "The lookup table-based methods achieve promising retouching performance by learning image-adaptive weights to combine 3-dimensional lookup tables (3D LUTs) and conducting pixel-to-pixel color transformation. However, this paradigm ignores the local context cues and applies the same transformation to portrait pixels and background pixels that exhibit the same raw RGB values. In contrast, an expert usually conducts different operations to adjust the color temperatures, tones of portrait regions, and background regions. This inspires us to model local context cues to improve the retouching quality explicitly.Thus, the center pixel of an image patch is first retouched by predicting pixel-adaptive lookup table weights. To modulate the influence of neighboring pixels, as neighboring pixels exhibit different affinities to the center pixel, a local attention mask is estimated. Then, the quality of the local attention mask is further improved by applying supervision, which is based on the affinity map calculated by the ground-truth portrait mask. For group-level consistency, we propose to directly constrain the variance of mean color components in the Lab space. Extensive experiments on the PPR10K dataset demonstrate the effectiveness of the proposed method, the retouching performance on high-resolution photos is improved by over 0.5dB in terms of PSNR, and the group-level inconsistency is reduced by 2.1.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323004739",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Consistency (knowledge bases)",
      "Context (archaeology)",
      "Economics",
      "Geography",
      "Lookup table",
      "Metric (unit)",
      "Operations management",
      "Pixel",
      "Portrait",
      "Programming language",
      "RGB color model"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Binglu"
      },
      {
        "surname": "Lu",
        "given_name": "Chengzhe"
      },
      {
        "surname": "Yan",
        "given_name": "Dawei"
      },
      {
        "surname": "Zhao",
        "given_name": "Yongqiang"
      },
      {
        "surname": "Li",
        "given_name": "Ning"
      },
      {
        "surname": "Li",
        "given_name": "Xuelong"
      }
    ]
  },
  {
    "title": "WSDS-GAN: A weak-strong dual supervised learning method for underwater image enhancement",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109774",
    "abstract": "Underwater Image Enhancement (UIE) is a crucial preprocessing step for underwater vision tasks. Addressing the challenge of training supervised deep learning models on large, diverse datasets while learning the intrinsic degradation factors of underwater images is essential for improving model generalization performance. In this paper, we propose a Weak-Strong Dual Supervised Generative Adversarial Network (WSDS-GAN) for UIE. During the first weakly supervised learning phase, unpaired images, consisting of degraded underwater images and clear in-air images, are used to train the model with the goal of recovering color, brightness, and content. In the second strongly supervised learning phase, a limited number of paired images are fed into the model to further train the image detail recovery generator. Comprehensive experiments on public datasets and self-photographed images demonstrate the effectiveness of our proposed method over existing state-of-the-art methods, both qualitatively and quantitatively. Additionally, we show that our method significantly enhances image details to support subsequent underwater vision tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323004727",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Brightness",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Generalization",
      "Generator (circuit theory)",
      "Geology",
      "Image (mathematics)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Oceanography",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Power (physics)",
      "Preprocessor",
      "Quantum mechanics",
      "Supervised learning",
      "Underwater"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Qiong"
      },
      {
        "surname": "Zhang",
        "given_name": "Qi"
      },
      {
        "surname": "Liu",
        "given_name": "Wei"
      },
      {
        "surname": "Chen",
        "given_name": "Wenbai"
      },
      {
        "surname": "Liu",
        "given_name": "Xinwang"
      },
      {
        "surname": "Wang",
        "given_name": "Xiangke"
      }
    ]
  },
  {
    "title": "FGBC: Flexible graph-based balanced classifier for class-imbalanced semi-supervised learning",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109793",
    "abstract": "Semi-supervised learning (SSL) has witnessed resounding success in many standard class-balanced benchmark datasets. However, real-world data often exhibit class-imbalanced distributions, which poses significant challenges for existing SSL algorithms. In general, fully supervised models trained on a class-imbalanced dataset are biased toward the majority classes, and this issue becomes more severe for class-imbalanced semi-supervised learning (CISSL) conditions. To address this issue, we put forward a novel CISSL framework dubbed FGBC by introducing a flexible graph-based balanced classifier with three innovations. Specifically, because the propagation of label information becomes difficult for tail classes, we propose a graph-based classifier head attached to the representation layer of the existing SSL framework for efficient pseudo-label propagation. Then, by considering that the learning status of different classes in CISSL may vary, we introduce a flexible threshold adjustment in pseudo-labeling to further select balanced samples to participate in training. Furthermore, to alleviate the risk of overfitting tail classes, we devised a class-aware feature MixUp (CFM) augmentation algorithm, which can further enhance the features of each class by considering their class sizes. Experimental results demonstrate that FGBC achieves state-of-the-art performance on datasets from CIFAR-10/100, SVHN and Small ImageNet-127 under various levels of CISSL conditions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323004910",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Class (philosophy)",
      "Classifier (UML)",
      "Computer science",
      "Graph",
      "Machine learning",
      "Overfitting",
      "Pattern recognition (psychology)",
      "Semi-supervised learning",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Kong",
        "given_name": "Xiangyuan"
      },
      {
        "surname": "Wei",
        "given_name": "Xiang"
      },
      {
        "surname": "Liu",
        "given_name": "Xiaoyu"
      },
      {
        "surname": "Wang",
        "given_name": "Jingjie"
      },
      {
        "surname": "Xing",
        "given_name": "Weiwei"
      },
      {
        "surname": "Lu",
        "given_name": "Wei"
      }
    ]
  },
  {
    "title": "Corrigendum to “Combining Minkowski and Chebyshev: New distance proposal and survey of distance metrics using k-nearest neighbours classifier” [Pattern Recognition Letters 110 (2018) 66–71]",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.10.005",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523002775",
    "keywords": [
      "Artificial intelligence",
      "Chebyshev filter",
      "Classifier (UML)",
      "Computer science",
      "Computer vision",
      "Euclidean distance",
      "Geometry",
      "Mathematics",
      "Minkowski distance",
      "Minkowski space",
      "Pattern recognition (psychology)",
      "k-nearest neighbors algorithm"
    ],
    "authors": [
      {
        "surname": "Rodrigues",
        "given_name": "É.O."
      }
    ]
  },
  {
    "title": "Preferred vector machine for forest fire detection",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109722",
    "abstract": "Machine learning-based fire detection/recognition is very popular in forest-monitoring systems. However, without considering the prior knowledge, e.g., equal attention on both classes of the fire and non-fire samples, fire miss-detected phenomena frequently appeared in the current methods. In this work, considering model’s interpretability and the limited data for model-training, we propose a novel pixel-precision method, termed as PreVM (Preferred Vector Machine). To guarantee high fire detection rate under precise control, a new L 0 norm constraint is introduced to the fire class. Computationally, instead of the traditional L 1 re-weighted techniques in L 0 norm approximation, this L 0 constraint can be converted into linear inequality and incorporated into the process of parameter selection. To further speed up model-training and reduce error warning rate, we also present a kernel-based L 1 norm PreVM ( L 1 -PreVM). Theoretically, we firstly prove the existence of dual representation for the general L p ( p ≥ 1) norm regularization problems in RKHS (Reproducing Kernel Hilbert Space). Then, we provide a mathematical evidence for L 1 norm kernelization to conquer the case when feature samples do not appear in pairs. The work also includes an extensive experimentation on the real forest fire images and videos. Compared with the-state-of-art methods, the results show that our PreVM is capable of simultaneously achieving higher fire detection rates and lower error warning rates, and L 1 -PreVM is also superior in real-time detection.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032300420X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Hilbert space",
      "Interpretability",
      "Kernelization",
      "Law",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Norm (philosophy)",
      "Parameterized complexity",
      "Pattern recognition (psychology)",
      "Political science",
      "Reproducing kernel Hilbert space",
      "Support vector machine",
      "Word error rate"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Xubing"
      },
      {
        "surname": "Hua",
        "given_name": "Zhichun"
      },
      {
        "surname": "Zhang",
        "given_name": "Li"
      },
      {
        "surname": "Fan",
        "given_name": "Xijian"
      },
      {
        "surname": "Zhang",
        "given_name": "Fuquan"
      },
      {
        "surname": "Ye",
        "given_name": "Qiaolin"
      },
      {
        "surname": "Fu",
        "given_name": "Liyong"
      }
    ]
  },
  {
    "title": "Switching clusters’ synchronization for discrete space-time complex dynamical networks via boundary feedback controls",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109763",
    "abstract": "Unlike the existing literatures that consider only discrete-time networks, this paper explores the double effects of both discrete time and discrete spatial diffusions in a switching complex dynamical networks. By means of the knowledge of clusters controls, a clusters synchronous frame of space-time discrete switching complex networks with boundary feedback controller is newly proposed and established. With the helps of some indispensable vector-valued sequence inequalities and Lyapunov function with switching signals and clusters’ information, the boundary feedback controllers are designed to synchronize space-time discrete switching complex networks coupled with nodes’ states or spatial diffusions in the form of clusters. Additionally, a realizable computer algorithm is given to make the derived results of this paper easier to enforce. The current work is pioneering in consideration of discrete spatial diffusions and provides a theoretical and practical basis for future research in this regard.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323004612",
    "keywords": [
      "Agronomy",
      "Artificial intelligence",
      "Biology",
      "Boundary (topology)",
      "Combinatorics",
      "Computer science",
      "Control (management)",
      "Control theory (sociology)",
      "Controller (irrigation)",
      "Discrete space",
      "Discrete time and continuous time",
      "Frame (networking)",
      "Genetics",
      "Lyapunov function",
      "Mathematical analysis",
      "Mathematics",
      "Nonlinear system",
      "Physics",
      "Quantum mechanics",
      "Sequence (biology)",
      "Statistics",
      "Synchronization (alternating current)",
      "Synchronization networks",
      "Telecommunications",
      "Topology (electrical circuits)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Tianwei"
      },
      {
        "surname": "Li",
        "given_name": "Zhouhong"
      }
    ]
  },
  {
    "title": "Conditional Independence Induced Unsupervised Domain Adaptation",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109787",
    "abstract": "Learning domain-adaptive features is important to tackle the dataset bias problem, where data distributions in the labeled source domain and the unlabeled target domain can be different. The critical issue is to identify and then reduce the redundant information including class-irrelevant and domain-specific features. In this paper, a conditional independence induced unsupervised domain adaptation (CIDA) method is proposed to tackle the challenges. It aims to find the low-dimensional and transferable feature representation of each observation, namely the latent variable in the domain-adaptive subspace. Technically, two mutual information terms are optimized at the same time. One is the mutual information between the latent variable and the class label, and the other is the mutual information between the latent variable and the domain label. Note that the key module can be approximately reformulated as a conditional independence/dependence based optimization problem, and thus, it has a probabilistic interpretation with the Gaussian process. Temporary labels of the target samples and the model parameters are alternatively optimized. The objective function can be incorporated with deep network architectures, and the algorithm is implemented iteratively in an end-to-end manner. Extensive experiments are conducted on several benchmark datasets, and the results show effectiveness of CIDA.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323004855",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Conditional independence",
      "Conditional probability distribution",
      "Data mining",
      "Domain (mathematical analysis)",
      "Feature (linguistics)",
      "Geodesy",
      "Geography",
      "Independence (probability theory)",
      "Latent variable",
      "Law",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Mutual information",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Probabilistic logic",
      "Representation (politics)",
      "Statistics",
      "Subspace topology",
      "Variable (mathematics)"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Xiao-Lin"
      },
      {
        "surname": "Xu",
        "given_name": "Geng-Xin"
      },
      {
        "surname": "Ren",
        "given_name": "Chuan-Xian"
      },
      {
        "surname": "Dai",
        "given_name": "Dao-Qing"
      },
      {
        "surname": "Yan",
        "given_name": "Hong"
      }
    ]
  },
  {
    "title": "Autoencoders for a manifold learning problem with a jacobian rank constraint",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109777",
    "abstract": "We formulate the manifold learning problem as the problem of finding an operator that maps any point to a close neighbor that lies on a “hidden” k -dimensional manifold. We call this operator the correcting function. Under this formulation, autoencoders can be viewed as a tool to approximate the correcting function. Given an autoencoder whose Jacobian has rank k , we deduce from the classical Constant Rank Theorem that its range has a structure of a k -dimensional manifold. A k -dimensionality of the range can be forced by the architecture of an autoencoder (by fixing the dimension of the code space), or alternatively, by an additional constraint that the rank of the autoencoder mapping is not greater than k . This constraint is included in the objective function as a new term, namely a squared Ky-Fan k -antinorm of the Jacobian function. We claim that this constraint is a factor that effectively reduces the dimension of the range of an autoencoder, additionally to the reduction defined by the architecture. We also add a new curvature term into the objective. To conclude, we experimentally compare our approach with the CAE+H method on synthetic and real-world datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323004752",
    "keywords": [
      "Algorithm",
      "Applied mathematics",
      "Artificial intelligence",
      "Autoencoder",
      "Biochemistry",
      "Chemistry",
      "Combinatorics",
      "Computer science",
      "Constraint (computer-aided design)",
      "Curse of dimensionality",
      "Deep learning",
      "Dimension (graph theory)",
      "Dimensionality reduction",
      "Engineering",
      "Gene",
      "Geometry",
      "Intrinsic dimension",
      "Jacobian matrix and determinant",
      "Manifold (fluid mechanics)",
      "Mathematics",
      "Mechanical engineering",
      "Nonlinear dimensionality reduction",
      "Operator (biology)",
      "Pattern recognition (psychology)",
      "Rank (graph theory)",
      "Repressor",
      "Transcription factor"
    ],
    "authors": [
      {
        "surname": "Takhanov",
        "given_name": "Rustem"
      },
      {
        "surname": "Abylkairov",
        "given_name": "Y. Sultan"
      },
      {
        "surname": "Tezekbayev",
        "given_name": "Maxat"
      }
    ]
  },
  {
    "title": "Improving human–object interaction with auxiliary semantic information and enhanced instance representation",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.09.013",
    "abstract": "Human–Object Interaction (HOI) detection has garnered considerable attention among computer vision researchers as it involves identifying and describing actions between humans and objects. Numerous approaches, such as sequential and end-to-end methods, have been proposed to tackle this problem, with a recent focus on exploring end-to-end systems. This study presents an enhanced end-to-end transformer-based human–object detector based on HOTR, which introduces three improvements. The proposed model improves instance representation through a simple yet effective mechanism, utilizes semantic information to provide contextual understanding and additional knowledge, and incorporates a cross-attention mechanism for fusing multi-level high-level feature maps within the Transformer architecture. Experimental results demonstrate significant performance gains over the baseline HOTR model, making it competitive with other state-of-the-art models on two widely-used datasets: V-COCO and HICO-DET.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523002611",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Focus (optics)",
      "Law",
      "Machine learning",
      "Object (grammar)",
      "Object detection",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Political science",
      "Politics",
      "Quantum mechanics",
      "Representation (politics)",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Nguyen",
        "given_name": "Khang"
      },
      {
        "surname": "Le",
        "given_name": "Thinh V."
      },
      {
        "surname": "Van",
        "given_name": "Huyen Ngoc N."
      },
      {
        "surname": "Bui",
        "given_name": "Doanh C."
      }
    ]
  },
  {
    "title": "Deep forest auto-Encoder for resource-Centric attributes graph embedding",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109747",
    "abstract": "Graph embedding is an important technique used for representing graph structure data that preserves intrinsic features in a low-dimensional space suitable for graph-based applications. Graphs containing node attributes and weighted links are commonly employed to model various real-world problems and issues in computer science. In recent years, a hot research topic has been the exploitation of diverse information, including node attributes and topological semantic information, in graph embedding. However, due to limitations in deep learning based on neural networks, such information has not been fully utilized nor adequately integrated in existing models, leaving graph embedding unsatisfactory, especially for large resource graphs (e.g., knowledge graphs and task interaction graphs). In this study, we introduce a resource-centric graph embedding approach based on deep random forests learning, which reconstructs graphs using a deep autoencoder to achieve high effectiveness. To accomplish this, our approach employs three key components. The first component is a preprocessor driven by graph similarity, alongside modularity and self-attention modules, to comprehensively integrate graph representation. The second component utilizes local graph information structures to enhance the raw graph. Finally, we integrate diverse information using multi-grained scanning and dual-level cascade forests in the deep learning extractor and generator, ultimately producing the final graph embedding. Experimental results on seven real-world scenarios show that our approach outperforms state-of-the-art embedding methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323004454",
    "keywords": [
      "Artificial intelligence",
      "Autoencoder",
      "Combinatorics",
      "Computer science",
      "Data mining",
      "Deep learning",
      "Embedding",
      "Graph",
      "Graph embedding",
      "Machine learning",
      "Mathematics",
      "Null model",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Ding",
        "given_name": "Yan"
      },
      {
        "surname": "Zhai",
        "given_name": "Yujuan"
      },
      {
        "surname": "Hu",
        "given_name": "Ming"
      },
      {
        "surname": "Zhao",
        "given_name": "Jia"
      }
    ]
  },
  {
    "title": "Infrared small target segmentation networks: A survey",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109788",
    "abstract": "Fast and robust small target detection is one of the key technologies in the infrared (IR) search and tracking systems. With the development of deep learning, there are many data-driven IR small target segmentation algorithms, but they have not been extensively surveyed; we believe our proposed survey is the first to systematically survey them. Focusing on IR small target segmentation tasks, we summarized 7 characteristics of IR small targets, 3 feature extraction methods, 8 design strategies, 30 segmentation networks, 8 loss functions, and 13 evaluation indexes. Then, the accuracy, robustness, and computational complexities of 18 segmentation networks on 5 public datasets were compared and analyzed. Finally, we have discussed the existing problems and future trends in the field of IR small target detection. The proposed survey is a valuable reference for both beginners adapting to current trends in IR small target detection and researchers already experienced in this field.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323004867",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Feature extraction",
      "Field (mathematics)",
      "Gene",
      "Image segmentation",
      "Key (lock)",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pure mathematics",
      "Robustness (evolution)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Kou",
        "given_name": "Renke"
      },
      {
        "surname": "Wang",
        "given_name": "Chunping"
      },
      {
        "surname": "Peng",
        "given_name": "Zhenming"
      },
      {
        "surname": "Zhao",
        "given_name": "Zhihe"
      },
      {
        "surname": "Chen",
        "given_name": "Yaohong"
      },
      {
        "surname": "Han",
        "given_name": "Jinhui"
      },
      {
        "surname": "Huang",
        "given_name": "Fuyu"
      },
      {
        "surname": "Yu",
        "given_name": "Ying"
      },
      {
        "surname": "Fu",
        "given_name": "Qiang"
      }
    ]
  },
  {
    "title": "Exploring transformers for behavioural biometrics: A case study in gait recognition",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109798",
    "abstract": "Biometrics on mobile devices has attracted a lot of attention in recent years as it is considered a user-friendly authentication method. This interest has also been motivated by the success of Deep Learning (DL). Architectures based on Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) have established convenience for the task, improving the performance and robustness in comparison to traditional machine learning techniques. However, some aspects must still be revisited and improved. To the best of our knowledge, this is the first article that explores and proposes a novel gait biometric recognition systems based on Transformers, which currently obtain state-of-the-art performance in many applications. Several state-of-the-art architectures (Vanilla, Informer, Autoformer, Block-Recurrent Transformer, and THAT) are considered in the experimental framework. In addition, new Transformer configurations are proposed to further increase the performance. Experiments are carried out using the two popular public databases: whuGAIT and OU-ISIR. The results achieved prove the high ability of the proposed Transformer, outperforming state-of-the-art CNN and RNN architectures.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032300496X",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Biometrics",
      "Chemistry",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Electrical engineering",
      "Engineering",
      "Gene",
      "Machine learning",
      "Recurrent neural network",
      "Robustness (evolution)",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Delgado-Santos",
        "given_name": "Paula"
      },
      {
        "surname": "Tolosana",
        "given_name": "Ruben"
      },
      {
        "surname": "Guest",
        "given_name": "Richard"
      },
      {
        "surname": "Deravi",
        "given_name": "Farzin"
      },
      {
        "surname": "Vera-Rodriguez",
        "given_name": "Ruben"
      }
    ]
  },
  {
    "title": "Kinship verification using multi-level dictionary pair learning for multiple resolution images",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109742",
    "abstract": "Kinship verification using facial images is gaining substantial attention by computer vision researchers. The real challenge in kinship verification is to effectively represent the discriminative features to ease the differences between kinship image pairs. Further, existing kinship methods only focus on a single resolution, and ignore the variability of resolutions in practical scenarios. To address these issues, we propose a multi-level dictionary pair learning (MLDPL) method to learn dictionary pairs by incorporating multiple resolution images for kinship verification. We learn dictionary pairs jointly by transforming discriminative features of image pairs into different coding coefficients in the same space, thereby reducing the differences between them. Further, multiple resolution images are incorporated into dictionary pair learning to effectively deal with resolution variations in kinship verification. Extensive experiments are performed on different kinship datasets to validate the efficacy of proposed MLDPL method. Experimental results show that MLDPL achieves competitive performance on all kinship datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323004405",
    "keywords": [
      "Artificial intelligence",
      "Coding (social sciences)",
      "Computer science",
      "Dictionary learning",
      "Discriminative model",
      "Image (mathematics)",
      "Kinship",
      "Law",
      "Machine learning",
      "Mathematics",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Political science",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Goyal",
        "given_name": "Aarti"
      },
      {
        "surname": "Meenpal",
        "given_name": "Toshanlal"
      }
    ]
  },
  {
    "title": "Learning discriminative feature representation with pixel-level supervision for forest smoke recognition",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109761",
    "abstract": "Existing vision-based smoke recognition methods still face the issues of low detection rates and high false alarm rates in complex scenes. One reason is that they label light smoke and heavy smoke as the same value, which ignores the differences in multiple attribute information involved in the smoke imaging process. To solve this issue, this paper presents a pixel-level supervision neural network (PSNet) to learn discriminative feature representations for forest smoke recognition. First, the pixel-level supervision information, including the background component, smoke component, fusion ratio, and class information, is cooperatively considered to effectively guide the model training process. To avoid negative transfer caused by the asynchronous optimization of shared layer parameters and achieve synchronous minimization of each loss term, a regularization term based on the smoke imaging principle and a weight dynamic updating method are proposed to balance the weight coefficients of different loss terms. Second, a detail-difference-aware module (DDAM) based on a detail-difference-aware block (DDAB) and a spatial attention block (SAB) is proposed to distinguish smoke and smoke-like targets by fusing xy-shared convolution and z-shared convolution, which adaptively allocates the weights over different positions to prioritize the most informative visual elements in the spatial domain. Third, an attention-based feature separation module (AFSM) is proposed to relieve mutual interference in extracting background features and smoke features by designing component interaction attention (CIA), background component attention (BCA), smoke component attention (SCA), and enhanced residual blocks (ERBs), which can guide the interaction and separation process of background information and smoke information to enhance the discriminative spatial features and suppress interference features. ERB effectively eliminates noise and enhances smoke edge information based on median filters. Finally, to further enhance the feature representation capability, a multiconnection aggregation method (MCAM) is proposed by fully aggregating local and global features simultaneously. Extensive experiments show that our method achieves better performance than existing smoke recognition methods. Extensive experiments show that our PSNet achieves better performance than existing smoke recognition methods. For smoke recognition, our PSNet achieves a 96.95% detection rate, 3.02% false alarm rate, and 0.9694 F1-score. The average calculation time for each image is only 0.0195. For smoke component separation, our PSNet also achieves 0.0014 on evaluation criteria mean square error between predicted smoke component images and labelled smoke component images. These key experimental results are better than those of previous methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323004594",
    "keywords": [
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Computer science",
      "Discriminative model",
      "Engineering",
      "Feature (linguistics)",
      "Geometry",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pixel",
      "Smoke",
      "Waste management"
    ],
    "authors": [
      {
        "surname": "Tao",
        "given_name": "Huanjie"
      },
      {
        "surname": "Duan",
        "given_name": "Qianyue"
      },
      {
        "surname": "Lu",
        "given_name": "Minghao"
      },
      {
        "surname": "Hu",
        "given_name": "Zhenwu"
      }
    ]
  },
  {
    "title": "Construction of a feature enhancement network for small object detection",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109801",
    "abstract": "Limited by the size, location, number of samples and other factors of the small object itself, the small object is usually insufficient, which degrades the performance of the small object detection algorithms. To address this issue, we construct a novel Feature Enhancement Network (FENet) to improve the performance of small object detection. Firstly, an improved data augmentation method based on collision detection and spatial context extension (CDCI) is proposed to effectively expand the possibility of small object detection. Then, based on the idea of Granular Computing, a multi-granular deformable convolution network is constructed to acquire the offset feature representation at the different granularity levels. Finally, we design a high-resolution block (HR block) and build High-Resolution Block-based Feature Pyramid by parallel embedding HR block in FPN (HR-FPN) to make full use different granularity and resolution features. By above strategies, FENet can acquire sufficient feature information of small objects. In this paper, we firstly applied the multi-granularity deformable convolution to feature extraction of small objects. Meanwhile, a new feature fusion module is constructed by optimizing feature pyramid to maintain the detailed features and enrich the semantic information of small objects. Experiments show that FENet achieves excellent performance compared with performance of other methods when applied to the publicly available COCO dataset, VisDrone dataset and TinyPerson dataset. The code is available at https://github.com/cowarder/FENet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323004995",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Block (permutation group theory)",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Convolution (computer science)",
      "Data mining",
      "Embedding",
      "Feature (linguistics)",
      "Feature extraction",
      "Geometry",
      "Granularity",
      "Linguistics",
      "Mathematics",
      "Object (grammar)",
      "Object detection",
      "Operating system",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pyramid (geometry)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Hongyun"
      },
      {
        "surname": "Li",
        "given_name": "Miao"
      },
      {
        "surname": "Miao",
        "given_name": "Duoqian"
      },
      {
        "surname": "Pedrycz",
        "given_name": "Witold"
      },
      {
        "surname": "Wang",
        "given_name": "Zhaoguo"
      },
      {
        "surname": "Jiang",
        "given_name": "Minghui"
      }
    ]
  },
  {
    "title": "Assisting Multimodal Named Entity Recognition by cross-modal auxiliary tasks",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.10.004",
    "abstract": "Although the existing Multimodal Named Entity Recognition (MNER) methods have achieved promising performance, they suffer from the following drawbacks in social media scenarios. Firstly, most existing methods are based on a strong assumption that the textual content and the associated images are matched, which is not always valid in real scenarios; Secondly, current methods fail to filter out modality-specific random noise, which impedes models from exploiting modality-shared features. In this paper, a novel multi-task multimodal learning architecture is put forward, which aims to improve Multimodal Named Entity Recognition (MNER) performance by cross-modal auxiliary tasks (CMAT). Specifically, we first separate the shared and task-specific features for the main task and auxiliary tasks respectively, which is accomplished by cross-modal gate-control mechanism. Subsequently, without extra pre-processing or annotations, we utilize the cross-modal matching to address the issue of mismatched image-text pairs, and the cross-modal mutual information maximization to optimize the most relevant cross-modal features. Moreover, experimental results on the two widely used datasets confirm the superiority of our proposed approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523002751",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Economics",
      "Image (mathematics)",
      "Machine learning",
      "Management",
      "Matching (statistics)",
      "Mathematics",
      "Maximization",
      "Microeconomics",
      "Modal",
      "Modality (human–computer interaction)",
      "Natural language processing",
      "Noise (video)",
      "Pattern recognition (psychology)",
      "Polymer chemistry",
      "Speech recognition",
      "Statistics",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Zhengjie"
      },
      {
        "surname": "Zhang",
        "given_name": "Yu"
      },
      {
        "surname": "Mi",
        "given_name": "Siya"
      }
    ]
  },
  {
    "title": "Improved polar complex exponential transform for robust local image description",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109786",
    "abstract": "Image description via robust local descriptors plays a vital role in a large number of image representation and matching applications. In this paper, we propose a novel distinctive local image descriptor that is based on the phase and amplitude information of Polar Complex Exponential Transform (PCET). The proposed descriptor, called IPCET (Improved PCET), is robust to the common photometric transformations (e.g., illumination, noise, JPEG compression, and blur) and geometric transformations (e.g., scaling, rotation, translation, and significant affine distortion). We perform extensive experiments to compare our IPCET descriptor with six most cutting-edge region descriptors (i.e., SIFT, Zernike Moment, GLOH, PCA-SIFT, SURF, and ORB). Experimental results demonstrate that our IPCET descriptor outperforms cutting-edge moment-based descriptors.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323004843",
    "keywords": [
      "Affine transformation",
      "Amplifier",
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Distortion (music)",
      "Geometry",
      "Image (mathematics)",
      "Mathematics",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Rotation (mathematics)",
      "Scale-invariant feature transform",
      "Wavefront",
      "Zernike polynomials"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Zhanlong"
      },
      {
        "surname": "Yang",
        "given_name": "Linzhi"
      },
      {
        "surname": "Chen",
        "given_name": "Geng"
      },
      {
        "surname": "Yap",
        "given_name": "Pew-Thian"
      }
    ]
  },
  {
    "title": "Face age synthesis: A review on datasets, methods, and open research areas",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109791",
    "abstract": "Face age synthesis is the determination of how a person looks in the future or the past by reconstructing their facial image. Determining the change in the human face over the years is a critical process for cross-age face recognition systems in forensic issues such as finding missing people and fugitive criminals. Therefore, it is a subject that has attracted attention in recent years. With the implementation of deep learning methods, better quality and photo-realistic images began to be produced. However, researchers continue to improve both aging accuracy and identity preservation requirements. We group the studies in the literature under two categories: classical methods and deep learning methods. We review both categories in the methods used, evaluation methods, and databases.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323004892",
    "keywords": [
      "Aesthetics",
      "Artificial intelligence",
      "Computer science",
      "Deep learning",
      "Epistemology",
      "Face (sociological concept)",
      "Facial recognition system",
      "Identity (music)",
      "Machine learning",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Process (computing)",
      "Quality (philosophy)",
      "Social science",
      "Sociology",
      "Subject (documents)",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Kale",
        "given_name": "Ayşe"
      },
      {
        "surname": "Altun",
        "given_name": "Oğuz"
      }
    ]
  },
  {
    "title": "Graph-based learning of nonlinear physiological interactions for classification of emotions",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109794",
    "abstract": "Emotion recognition has been drawing the attention of researchers and practitioners in recent years. While various research studies show successful applications to recognize and distinguish emotions based on physiological responses using machine learning techniques, less research to date is focused on the network properties of physiological interactions under different emotional states. To this end, we propose a multi-modal graph learning framework to quantify the interactions among physiological systems and present representative networks associated with emotional states. More specifically, we introduce a novel information-theoretic-based time delay stability to quantify complex interactions between physiological modalities. We test our quantification approach on three publicly available benchmark databases for emotion recognition and demonstrate the comparative performances of measuring the interactions of physiological systems in response to emotional states. Finally, we present the visualization of multi-modal physiological network topology, which may be useful for emotional interpretations in practice.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323004922",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Chemistry",
      "Cognitive psychology",
      "Computer science",
      "Geodesy",
      "Geography",
      "Graph",
      "Machine learning",
      "Modal",
      "Modalities",
      "Polymer chemistry",
      "Psychology",
      "Social science",
      "Sociology",
      "Stability (learning theory)",
      "Theoretical computer science",
      "Visualization"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Huiyu"
      },
      {
        "surname": "Fan",
        "given_name": "Miaolin"
      },
      {
        "surname": "Chou",
        "given_name": "Chun-An"
      }
    ]
  },
  {
    "title": "Data-Driven single image deraining: A Comprehensive review and new perspectives",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109740",
    "abstract": "Single Image Deraining (SID) aims at recovering the rain-free background from an image degraded by rain streaks. For the powerful fitting ability of deep neural networks and massive training data, data-driven deep SID methods have obtained significant improvement over traditional model/prior-based ones. Current studies usually focus on improving the deraining performance by proposing different categories of deraining networks, while neglecting the interpretation of the solving process. As a result, the generalization ability may still be limited in real-world scenarios, and the deraining results also cannot effectively improve the performance of subsequent high-level tasks (e.g., object detection). To explore these issues, we in this paper re-examine the three important factors (i.e., data, rain model and network architecture) for the SID problem, and specifically analyze them by proposing new and more reasonable criteria (i.e., general vs. specific, synthetical vs. mathematical, black-box vs. white-box). We also study the relationship of the three factors from a new perspective of data, and reveal two different solving paradigms (explicit vs. implicit) for the SID task. We further discuss the current mainstream data-driven SID methods from five aspects, i.e., training strategy, network pipeline, domain knowledge, data preprocessing, and objective function, and some useful conclusions are summarized by statistics. Besides, we profoundly studied one of the three factors, i.e., data, and measured the performance of current methods on different datasets through extensive experiments to reveal the effectiveness of SID data. Finally, with the comprehensive review and in-depth analysis, we draw some valuable conclusions and suggestions for future research.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323004387",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Data mining",
      "Economics",
      "Generalization",
      "Image (mathematics)",
      "Machine learning",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Perspective (graphical)",
      "Pipeline (software)",
      "Preprocessor",
      "Programming language",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Zhao"
      },
      {
        "surname": "Wei",
        "given_name": "Yanyan"
      },
      {
        "surname": "Zhang",
        "given_name": "Haijun"
      },
      {
        "surname": "Yang",
        "given_name": "Yi"
      },
      {
        "surname": "Yan",
        "given_name": "Shuicheng"
      },
      {
        "surname": "Wang",
        "given_name": "Meng"
      }
    ]
  },
  {
    "title": "The Florence multi-resolution 3D facial expression dataset",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.09.015",
    "abstract": "In the literature, several 3D face datasets have been collected, aiming at advancing the field of 3D face analysis from different perspectives. Data collection generally follows specific research needs, and the existing 3D face datasets all have different characteristics that are tailored for investigating different tasks, encompassing face recognition, facial expressions and emotions analysis, 3D face reconstruction. However, the majority of these datasets are either collected with high-resolution scanners, or consumer level devices, such as the Kinect, the latter being motivated by the burdensome and costly process of collecting high-quality scans. Differently from 2D imagery, the difference in resolution in 3D data represents a non negligible problem that is under-investigated, and still prevents the successful development of methods that can work in real scenarios. In this paper, we propose a new 3D face dataset, named “Florence Multi-Resolution 3D Facial Expression” (Florence 3DMRE), which aims at bridging the gap between high- and low-resolution 3D face datasets. Its peculiarity consists in (1) including high-resolution (HR) models obtained with a HR scanner, and paired samples collected with a Kinect sensor, (2) LR and HR scans are synchronized and capture extreme and asymmetric facial deformations as used in facial rehabilitation exercises. In total, our dataset consists of 14 subjects, each performing 19 complex and asymmetric expressions. For each of them, we collected a high-resolution scan, and an RGB-D sequence. Finally, to highlight the value of the dataset and the challenges it introduces, we use the collected data to perform baseline experiments for cross-resolution 3D face recognition and reconstruction. The dataset is released for research purposes only, and complies to GDPR for data treatment. The dataset can be found at this link.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523002714",
    "keywords": [
      "Artificial intelligence",
      "Bridging (networking)",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Face (sociological concept)",
      "Facial expression",
      "Facial recognition system",
      "Geology",
      "High resolution",
      "Pattern recognition (psychology)",
      "Remote sensing",
      "Resolution (logic)",
      "Scanner",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Ferrari",
        "given_name": "Claudio"
      },
      {
        "surname": "Berretti",
        "given_name": "Stefano"
      },
      {
        "surname": "Pala",
        "given_name": "Pietro"
      },
      {
        "surname": "Del Bimbo",
        "given_name": "Alberto"
      }
    ]
  },
  {
    "title": "Optimal transport based pyramid graph kernel for autism spectrum disorder diagnosis",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109716",
    "abstract": "Brain network, which characterizes the functional and structural interactions of brain regions with graph theory, has been widely utilized to diagnose brain diseases, such as autism spectrum disorder (ASD). It is a challenge to measure the network (or graph) similarity in brain network analysis. Graph kernel (i.e., kernel defined on graphs) offers an efficient tool for measuring the similarity of paired brain networks and yields the excellent classification performance in brain disease diagnosis. However, most of the existing graph kernels neglected the hierarchical architecture information of brain networks. To address this problem, in this paper, we propose an optimal transport based pyramid graph kernel for measuring brain network similarity and then apply it to brain disease classification. The main idea is to transform brain networks into pyramid structures, which reflect the hierarchical architecture information of the brain network with multi-resolution histograms. The optimal transport distance in pyramid structures is calculated for measuring transport costs between paired brain networks. Finally, the optimal transport based pyramid graph kernel is computed based on this optimal transport distance. To evaluate the effectiveness of the proposed optimal transport based pyramid graph kernel, the extensive experiments are performed in functional magnetic resonance imaging data of brain disease from the Autism Brain Imaging Data Exchange database. The experimental results show that our proposed optimal transport based pyramid graph kernel outperforms the state-of-the-art methods in ASD classification tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323004144",
    "keywords": [
      "Artificial intelligence",
      "Autism",
      "Autism spectrum disorder",
      "Combinatorics",
      "Computer science",
      "Geometry",
      "Graph",
      "Graph kernel",
      "Graph theory",
      "Image (mathematics)",
      "Kernel (algebra)",
      "Kernel method",
      "Mathematics",
      "Medicine",
      "Pattern recognition (psychology)",
      "Polynomial kernel",
      "Power graph analysis",
      "Psychiatry",
      "Pyramid (geometry)",
      "Similarity (geometry)",
      "Support vector machine",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Ma",
        "given_name": "Kai"
      },
      {
        "surname": "Huang",
        "given_name": "Shuo"
      },
      {
        "surname": "Wan",
        "given_name": "Peng"
      },
      {
        "surname": "Zhang",
        "given_name": "Daoqiang"
      }
    ]
  },
  {
    "title": "Knowledge transfer evolutionary search for lightweight neural architecture with dynamic inference",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109790",
    "abstract": "Relying on the availability of massive labeled samples, most neural architecture search (NAS) methods focus on searching large and complex models; and adopt fixed structures and parameters at the inference stage. Few approaches automatically design lightweight networks for label-limited tasks and further consider the inference differences between inputs. To address these issues, we introduce evolutionary computation (EC) and attention mechanism and propose a knowledge transfer evolutionary search for lightweight neural architecture with dynamic inference, then verify it using synthetic aperture radar (SAR) images. SAR image classification is a typical label-limited task due to the inherent imaging mechanism of SAR. We design the EC-based architecture search and attention-based dynamic inference for SAR image scene classification. Specifically, we build a SAR-tailored search space, explore topology pruning-based mutation operators to search lightweight architectures, and further design a dynamic Ridgelet convolution capable of adaptive reasoning to enhance the representation ability of searched lightweight networks. Moreover, we propose a knowledge transfer training strategy and hybrid evaluation criteria to ensure searching quickly and robustly. Experimental results show that the proposed method can search for superior neural architectures, thus improving the classification performance of SAR images.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323004880",
    "keywords": [
      "Agronomy",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer science",
      "Contextual image classification",
      "Image (mathematics)",
      "Inference",
      "Law",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Pruning",
      "Representation (politics)",
      "Synthetic aperture radar"
    ],
    "authors": [
      {
        "surname": "Qian",
        "given_name": "Xiaoxue"
      },
      {
        "surname": "Liu",
        "given_name": "Fang"
      },
      {
        "surname": "Jiao",
        "given_name": "Licheng"
      },
      {
        "surname": "Zhang",
        "given_name": "Xiangrong"
      },
      {
        "surname": "Huang",
        "given_name": "Xinyan"
      },
      {
        "surname": "Li",
        "given_name": "Shuo"
      },
      {
        "surname": "Chen",
        "given_name": "Puhua"
      },
      {
        "surname": "Liu",
        "given_name": "Xu"
      }
    ]
  },
  {
    "title": "High-order knowledge-based Discriminant features for kinship verification",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.09.008",
    "abstract": "This research work aims to propose an effective and robust face kinship verification system by leveraging several axes, including advanced learning techniques, deep learning, and CNN networks. The contributions of this work include the use of a preprocessing method known as Multiscale Retinex with Chromaticity Preservation (MSRCP) and the Gradientfaces technique (GRF) to improve image quality and contrast enhancement. Additionally, a novel discriminative handcrafted descriptor called Histograms of dual-tree complex wavelet transform (Hist-DTCWT) is proposed. Moreover, a logistic regression for score-level fusion is used to enhance the matching process. Furthermore, a high-order knowledge-based feature using a tensor subspace model was proposed to combine multiple discriminative features. Tests were conducted out using three datasets, where the proposed method has outperformed the state the art. Typically, verification accuracies of 96.62%, 95.54% and 92.94% have been reached under Cornell KinFace, UB KinFace and TS KinFace datasets, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523002568",
    "keywords": [
      "Artificial intelligence",
      "Complex wavelet transform",
      "Computer science",
      "Convolutional neural network",
      "Discrete wavelet transform",
      "Discriminative model",
      "Feature extraction",
      "Histogram",
      "Image (mathematics)",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Preprocessor",
      "Subspace topology",
      "Wavelet",
      "Wavelet transform"
    ],
    "authors": [
      {
        "surname": "Belabbaci",
        "given_name": "El Ouanas"
      },
      {
        "surname": "Khammari",
        "given_name": "Mohammed"
      },
      {
        "surname": "Chouchane",
        "given_name": "Ammar"
      },
      {
        "surname": "Ouamane",
        "given_name": "Abdelmalik"
      },
      {
        "surname": "Bessaoudi",
        "given_name": "Mohcene"
      },
      {
        "surname": "Himeur",
        "given_name": "Yassine"
      },
      {
        "surname": "Hassaballah",
        "given_name": "Mahmoud"
      }
    ]
  },
  {
    "title": "Segmenting white matter hyperintensities in brain magnetic resonance images using convolution neural networks",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.07.014",
    "abstract": "White matter hyperintensities (WMHs) are found on magnetic resonance (MR) images of older individuals and are associated with many neurodegenerative disorders, although the exactrole of WMHs in Alzheimer’s disease and other dementias remains an open area of research. Fluid-attenuated inversion recovery (FLAIR) MR imaging sequences show WMHs with good image contrast. Manual segmentation of WMHs on FLAIR images is the widely accepted “gold standard”, however, this step is often time-consuming and has a high inter-rater variability. The absence of an automated, robust and accurate approach to segment WMHs remains a processing bottleneck. We explored convolutional neural networks (CNNs) for performing semantic segmentation of WMHs in FLAIR images. Two sets of experiments were conducted: (1) Variations of U-shaped CNNs (U-Nets) were evaluated in 186 individuals, specifically, four architectures (VGG16, VGG19, ResNet152 and EfficientNetB0) having three dimensionalities (2D, 2.5D and 3D). (2) New data from 60 individuals were added to test the generalizability of U-Net, LinkNet and Feature-Pyramid Network (FPN) variants. The first experiment showed that the 2.5D implementation with VGG16 or VGG19 was the most suitable configuration when segmenting WMH (F-measure > 95% and intersection-over-union > 90%). The second experiment confirmed generalizability of these variants when using unprocessed FLAIR images.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523002179",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Developmental psychology",
      "Fluid-attenuated inversion recovery",
      "Generalizability theory",
      "Hyperintensity",
      "Magnetic resonance imaging",
      "Medicine",
      "Pattern recognition (psychology)",
      "Psychology",
      "Radiology",
      "Segmentation",
      "White matter"
    ],
    "authors": [
      {
        "surname": "Duarte",
        "given_name": "Kauê T.N."
      },
      {
        "surname": "Gobbi",
        "given_name": "David G."
      },
      {
        "surname": "Sidhu",
        "given_name": "Abhijot S."
      },
      {
        "surname": "McCreary",
        "given_name": "Cheryl R."
      },
      {
        "surname": "Saad",
        "given_name": "Feryal"
      },
      {
        "surname": "Camicioli",
        "given_name": "Richard"
      },
      {
        "surname": "Smith",
        "given_name": "Eric E."
      },
      {
        "surname": "Frayne",
        "given_name": "Richard"
      }
    ]
  },
  {
    "title": "Multi-proxy feature learning for robust fine-grained visual recognition",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109779",
    "abstract": "Visual representation for fine-grained visual recognition can be learned by mandatorily enforcing all samples of the same category into a uniform representation. This strict training objective performs well under closed-set setting but is not applicable to data in the wild containing noisy annotations and long-tailed distributions, e.g., it may lead to a feature space biased to head categories. This paper tackles this challenge by pursuing a more balanced and discriminative feature space by first retaining intra-class variances to isolate noises, then eliminating intra-class variances to improve the visual recognition performance. We propose the Compact Memory Updater to maintain a memory bank, which memorizes proxy features to represent multiple typical appearances of each category in the training set. The Proxy-based Feature Enhancement hence leverages proxy features to ensure samples of the same category have similar features. Iteratively running those two modules boosts the robustness and discriminative power of the learnt representation, hence facilitates various fine-grained visual recognition tasks including person re-identification (re-id), image classification and retrieval. Extensive experiments on noisy and long-tailed training sets show this Multi-Proxy Feature Learning (MPFL) framework achieves promising performance. For instance on a training set with 90% one-shot categories, MPFL outperforms the recent long-tailed person re-id method LEAP-AF by 16.9% in rank-1 accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323004776",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Discriminative model",
      "Feature (linguistics)",
      "Feature learning",
      "Feature vector",
      "Gene",
      "Law",
      "Linguistics",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Programming language",
      "Proxy (statistics)",
      "Representation (politics)",
      "Robustness (evolution)",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Mao",
        "given_name": "Shunan"
      },
      {
        "surname": "Wang",
        "given_name": "Yaowei"
      },
      {
        "surname": "Wang",
        "given_name": "Xiaoyu"
      },
      {
        "surname": "Zhang",
        "given_name": "Shiliang"
      }
    ]
  },
  {
    "title": "Tri-HGNN: Learning triple policies fused hierarchical graph neural networks for pedestrian trajectory prediction",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109772",
    "abstract": "In complex and dynamic urban traffic scenarios, the accurate trajectory prediction of surrounding pedestrians with interactive behaviors plays a vital role in the self-driving system. Intrinsic factors and extrinsic factors will inevitably influence the pedestrians trajectory. Intrinsic factors such as pedestrians diversified intentions bring rich and diverse multi-modal future possibilities. Besides, extrinsic factors affecting the future trajectory are accompanied by context semantics such as interactions among pedestrians. However, most of the existing methods discuss two problems (interaction and intention) separately. Considering both two factors impact the trajectory of pedestrians, a Triple Policies Fused Hierarchical Graph Neural Networks (Tri-HGNN) is proposed to model spatial and temporal interactions and intentions among the whole scene of pedestrians at each time step and predict the multiple future trajectories. Tri-HGNN contains three different policies: (i) Extrinsic-level policy is used to extract spatial nodes embedding from the interaction graph of pedestrian trajectories by using the Graph Attention Network. (ii) Intrinsic-level policy adopts the Graph Convolutional Network to infer the human intention for more accurate prediction. Moreover, human intention is influenced by the intrinsic interaction generated among pedestrians, so we fuse the interaction features to grasp the influence of the extrinsic interaction. (iii) Basic-level policy then integrates the heuristic information obtained from other two policies and concatenates it with historical trajectories to make multiple predictions through Temporal Convolution Network. Experimental results show that our model improves performance compared with state-of-the-art methods on the ETH/UCY and SDD benchmarks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323004703",
    "keywords": [
      "Artificial intelligence",
      "Astronomy",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Convolutional neural network",
      "Engineering",
      "GRASP",
      "Graph",
      "Heuristic",
      "Human–computer interaction",
      "Interaction information",
      "Machine learning",
      "Mathematics",
      "Paleontology",
      "Pedestrian",
      "Physics",
      "Programming language",
      "Statistics",
      "Theoretical computer science",
      "Trajectory",
      "Transport engineering"
    ],
    "authors": [
      {
        "surname": "Zhu",
        "given_name": "Wenjun"
      },
      {
        "surname": "Liu",
        "given_name": "Yanghong"
      },
      {
        "surname": "Wang",
        "given_name": "Peng"
      },
      {
        "surname": "Zhang",
        "given_name": "Mengyi"
      },
      {
        "surname": "Wang",
        "given_name": "Tian"
      },
      {
        "surname": "Yi",
        "given_name": "Yang"
      }
    ]
  },
  {
    "title": "Fast deep autoencoder for federated learning",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109805",
    "abstract": "This paper presents a novel, fast and privacy preserving implementation of deep autoencoders. DAEF (Deep AutoEncoder for Federated learning), unlike traditional neural networks, trains a deep autoencoder network in a non-iterative way, which drastically reduces training time. Training can be performed incrementally, in parallel and distributed and, thanks to its mathematical formulation, the information to be exchanged does not endanger the privacy of the training data. The method has been evaluated and compared with other state-of-the-art autoencoders, showing interesting results in terms of accuracy, speed and use of available resources. This makes DAEF a valid method for edge computing and federated learning, in addition to other classic machine learning scenarios.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005034",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Autoencoder",
      "Cartography",
      "Cloud computing",
      "Computer science",
      "Deep belief network",
      "Deep learning",
      "Deep neural networks",
      "Edge device",
      "Enhanced Data Rates for GSM Evolution",
      "Geography",
      "Machine learning",
      "Meteorology",
      "Operating system",
      "Physics",
      "Train",
      "Training (meteorology)"
    ],
    "authors": [
      {
        "surname": "Novoa-Paradela",
        "given_name": "David"
      },
      {
        "surname": "Fontenla-Romero",
        "given_name": "Oscar"
      },
      {
        "surname": "Guijarro-Berdiñas",
        "given_name": "Bertha"
      }
    ]
  },
  {
    "title": "Video anomaly detection with NTCN-ML: A novel TCN for multi-instance learning",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109765",
    "abstract": "A key challenge in video anomaly detection is the identification of rare abnormal patterns in the positive instances as they exhibit only a small variation compared to normal patterns, and they are largely biased by the dominant negative instances. To address this issue, we propose a weakly supervised video anomaly detection model called NTCN-ML - Novel Temporal Convolutional Network Multi-Instance Learning Model. The NTCN-ML model extracts temporal representations of video data to construct a time-series pattern to optimize the multi-instance learning process. The model examines the correlation between positive and negative samples in the multi-instance learning process to balance the feature association between rare positive and negative instances. The video anomaly detection with the NTCN-ML model achieved 95.3% and 85.1% accuracy for UCF-Crime and ShanghaiTech datasets, respectively, and outperformed the baseline models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323004636",
    "keywords": [
      "Anomaly (physics)",
      "Anomaly detection",
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Condensed matter physics",
      "Construct (python library)",
      "Feature (linguistics)",
      "Key (lock)",
      "Linguistics",
      "Machine learning",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Process (computing)",
      "Programming language"
    ],
    "authors": [
      {
        "surname": "Shao",
        "given_name": "Wenhao"
      },
      {
        "surname": "Xiao",
        "given_name": "Ruliang"
      },
      {
        "surname": "Rajapaksha",
        "given_name": "Praboda"
      },
      {
        "surname": "Wang",
        "given_name": "Mengzhu"
      },
      {
        "surname": "Crespi",
        "given_name": "Noel"
      },
      {
        "surname": "Luo",
        "given_name": "Zhigang"
      },
      {
        "surname": "Minerva",
        "given_name": "Roberto"
      }
    ]
  },
  {
    "title": "Haar wavelet downsampling: A simple but effective downsampling module for semantic segmentation",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109819",
    "abstract": "Downsampling operations such as max pooling or strided convolution are ubiquitously utilized in Convolutional Neural Networks (CNNs) to aggregate local features, enlarge receptive field, and minimize computational overhead. However, for a semantic segmentation task, pooling features over the local neighbourhood may result in the loss of important spatial information, which is conducive for pixel-wise predictions. To address this issue, we introduce a simple yet effective pooling operation called the Haar Wavelet-based Downsampling (HWD) module. This module can be easily integrated into CNNs to enhance the performance of semantic segmentation models. The core idea of HWD is to apply Haar wavelet transform for reducing the spatial resolution of feature maps while preserving as much information as possible. Furthermore, to investigate the benefits of HWD, we propose a novel metric, named as feature entropy index (FEI), which measures the degree of information uncertainty after downsampling in CNNs. Specifically, the FEI can be used to indicate the ability of downsampling methods to preserve essential information in semantic segmentation. Our comprehensive experiments demonstrate that the proposed HWD module could (1) effectively improve the segmentation performance across different modality image datasets with various CNN architectures, and (2) efficiently reduce information uncertainty compared to the conventional downsampling methods. Our implementation are available at https://github.com/apple1986/HWD.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005174",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Discrete wavelet transform",
      "Haar wavelet",
      "Image (mathematics)",
      "Pattern recognition (psychology)",
      "Pooling",
      "Segmentation",
      "Upsampling",
      "Wavelet",
      "Wavelet transform"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Guoping"
      },
      {
        "surname": "Liao",
        "given_name": "Wentao"
      },
      {
        "surname": "Zhang",
        "given_name": "Xuan"
      },
      {
        "surname": "Li",
        "given_name": "Chang"
      },
      {
        "surname": "He",
        "given_name": "Xinwei"
      },
      {
        "surname": "Wu",
        "given_name": "Xinglong"
      }
    ]
  },
  {
    "title": "Graph classification via discriminative edge feature learning",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109799",
    "abstract": "Spectral graph convolutional neural networks (GCNNs) have been producing encouraging results in graph classification tasks. However, most spectral GCNNs utilize fixed graphs when aggregating node features while omitting edge feature learning and failing to get an optimal graph structure. Moreover, many existing graph datasets do not provide initialized edge features, further restraining the ability of learning edge features via spectral GCNNs. In this paper, we try to address this issue by designing an edge feature scheme and an add-on layer between every two stacked graph convolution layers in spectral GCNN. Both are lightweight while effective in filling the gap between edge feature learning and performance enhancement of graph classification. The edge feature scheme makes edge features adapt to node representations at different spectral graph convolution layers. The add-on layer helps adjust the edge features to an optimal graph structure. To test the effectiveness of our method, we take Euclidean positions as initial node features and extract graphs with semantic information from point cloud objects. The node features of our extracted graphs are more scalable for edge feature learning than most existing graph datasets (in one-hot encoded label format). Three new graph datasets are constructed based on ModelNet40, ModelNet10 and ShapeNet Part datasets. Experimental results show that our method outperforms state-of-the-art graph classification methods on the new datasets. Our code and the constructed graph datasets will be released to the community.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323004971",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Discriminative model",
      "Feature (linguistics)",
      "Feature learning",
      "Graph",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Yi",
        "given_name": "Yang"
      },
      {
        "surname": "Lu",
        "given_name": "Xuequan"
      },
      {
        "surname": "Gao",
        "given_name": "Shang"
      },
      {
        "surname": "Robles-Kelly",
        "given_name": "Antonio"
      },
      {
        "surname": "Zhang",
        "given_name": "Yuejie"
      }
    ]
  },
  {
    "title": "Information-diffused graph tracking with linear complexity",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109809",
    "abstract": "Mainstream tracking approaches have achieved remarkable performance by adopting transformer structures. However, transformer structures’ inherent design of dot-product with softmax normalization incurs quadratic computation complexity regarding sequence length. This issue is further complicated when vision tasks employ softmax attention, as sequence length scales with the square of images’ sizes. Even though sparse attention and low-rank decomposition can alleviate over-inflated computation, it is still laborious to balance trackers’ accuracy, computation cost, and inference speed. To tackle the above problems, we propose an Information-Diffused Graph tracking pipeline with linear complexity (IDGtrack). As the feature constraint relationship in the physical world is an important cue for vision tasks, graph modules are constructed with information-diffused adjacency matrices to substitute softmax attention, which is not only efficient for linear computations but also maintains the non-negativity and global distribution of the attention matrix. Distinct from traditional linear attention methods exclusive to self-attention, a self-integrated and cross-context graph module with linear complexity is explored where a complete bipartite graph is established between the target and search region, facilitating a comprehensive perception of local and background information. Extensive experiments are conducted on public tracking benchmarks, demonstrating that our method achieves state-of-the-art (SOTA) performance with 111 FPS on GPU RTX3090.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005071",
    "keywords": [
      "Adjacency list",
      "Adjacency matrix",
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Bipartite graph",
      "Computation",
      "Computer science",
      "Encoder",
      "Graph",
      "Inference",
      "Operating system",
      "Pattern recognition (psychology)",
      "Softmax function",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Zhixing"
      },
      {
        "surname": "Yao",
        "given_name": "Jinzhen"
      },
      {
        "surname": "Tang",
        "given_name": "Chuanming"
      },
      {
        "surname": "Zhang",
        "given_name": "Jianlin"
      },
      {
        "surname": "Bao",
        "given_name": "Qiliang"
      },
      {
        "surname": "Peng",
        "given_name": "Zhenming"
      }
    ]
  },
  {
    "title": "Single-particle reconstruction in cryo-EM based on three-dimensional weighted nuclear norm minimization",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109736",
    "abstract": "Single-particle reconstruction (SPR) in cryogenic electron microscopy (cryo-EM) aims at aligning and averaging two-dimensional micrographs to reconstruct a three-dimensional particle. How to reconstruct micrographs from heavy noise is a crucial point for achieving better micrograph quality, and thus many methods focus on noise removal. However, new problems such as over-smoothing often occur in their results due to failure in handling heavy noise well. This paper proposes a three-dimensional weighted nuclear norm minimization (3DWNNM) model for SPR in the cryo-EM task to address these issues. Specifically, we design a minimization solver based on the forward-backward splitting algorithm to tackle our model efficiently. Under certain conditions, this solution has an energy-decaying feature and performs exceptionally well in reconstruction. Numerical experiments fully demonstrate the effectiveness and the robustness of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032300434X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Focus (optics)",
      "Gene",
      "Law",
      "Mathematical optimization",
      "Mathematics",
      "Minification",
      "Noise reduction",
      "Norm (philosophy)",
      "Optics",
      "Physics",
      "Political science",
      "Robustness (evolution)",
      "Smoothing",
      "Solver"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Chaoyan"
      },
      {
        "surname": "Wu",
        "given_name": "Tingting"
      },
      {
        "surname": "Li",
        "given_name": "Juncheng"
      },
      {
        "surname": "Dong",
        "given_name": "Bin"
      },
      {
        "surname": "Zeng",
        "given_name": "Tieyong"
      }
    ]
  },
  {
    "title": "A discriminative SPD feature learning approach on Riemannian manifolds for EEG classification",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109751",
    "abstract": "Covariance matrix learning methods have become popular for many classification tasks owing to their ability to capture interesting structures in non-linear data while respecting the Riemannian geometry of the underlying symmetric positive definite (SPD) manifolds. Several deep learning architectures applied to these matrix learning methods have recently been proposed in classification tasks by learning discriminative Euclidean-based embeddings. In this paper, we propose a new Riemannian-based deep learning network to generate more discriminative features for electroencephalogram (EEG) classification. Our key innovation lies in learning the Riemannian barycenter for each class within a Riemannian geometric space. The proposed model normalizes the distribution of SPD matrices and learns the center of each class to penalize the distances between the matrix and the corresponding class centers. As a result, our framework can further simultaneously reduce the intra-class distances, enlarge the inter-class distances for the learned features, and consistently outperform other state-of-the-art methods on three widely used EEG datasets and the data from our stress-induced experiment in virtual reality. Experimental results demonstrate the superiority of the proposed framework for learning the non-stationary nature of EEG signals due to the robustness of the covariance descriptor and the benefits of considering the barycenters on the Riemannian geometry.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323004491",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Composite material",
      "Computer science",
      "Covariance",
      "Covariance matrix",
      "Deep learning",
      "Discriminative model",
      "Euclidean space",
      "Feature (linguistics)",
      "Feature vector",
      "Gene",
      "Linguistics",
      "Machine learning",
      "Materials science",
      "Mathematical analysis",
      "Mathematics",
      "Matrix (chemical analysis)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Riemannian geometry",
      "Robustness (evolution)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Kim",
        "given_name": "Byung Hyung"
      },
      {
        "surname": "Choi",
        "given_name": "Jin Woo"
      },
      {
        "surname": "Lee",
        "given_name": "Honggu"
      },
      {
        "surname": "Jo",
        "given_name": "Sungho"
      }
    ]
  },
  {
    "title": "Constrained DTW preserving shapelets for explainable time-series clustering",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109804",
    "abstract": "The analysis of time series is becoming ever more popular due to the proliferation of sensors. A well-known similarity measure for time-series is Dynamic Time Warping (DTW), which does not respect the axioms of a metric. These, however, can be reintroduced through Learning DTW-Preserving Shapelets (LDPS). This article extends LDPS and presents constrained DTW-preserving shapelets (CDPS). CDPS directs the time-series representation to captures the user’s interpretation of the data by considering a limited amount of user knowledge in the from of must-link- cannot link constraints. Subsequently, unconstrained algorithms can be used to generate a clustering that respects the constraints without explicit knowledge of them. Out-of-sample data can be transformed into this space, overcoming the limitations of traditional transductive constrained-clustering algorithms. Furthermore, several Shapelet Cluster Explanation (SCE) approaches are proposed that explain the clustering and can simplify the representation while preserving clustering performance. State-of-the-art performance is demonstrated on multiple time-series datasets and an open-source implementation will be made publicly available upon acceptance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005022",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Dynamic time warping",
      "Economics",
      "Image (mathematics)",
      "Law",
      "Machine learning",
      "Measure (data warehouse)",
      "Metric (unit)",
      "Operations management",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Series (stratigraphy)",
      "Similarity (geometry)",
      "Similarity measure"
    ],
    "authors": [
      {
        "surname": "El Amouri",
        "given_name": "Hussein"
      },
      {
        "surname": "Lampert",
        "given_name": "Thomas"
      },
      {
        "surname": "Gançarski",
        "given_name": "Pierre"
      },
      {
        "surname": "Mallet",
        "given_name": "Clément"
      }
    ]
  },
  {
    "title": "Efficient 2D Tikhonov smoothness regularization with recursive filtering",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.07.001",
    "abstract": "In this work, we describe an implementation of the 2D Tikhonov regularization filter which scales linearly with the input signal’s size. In the homogeneous case, we propose a novel algorithm to decompose the filter’s 2D kernel as a sum of axis-aligned Gaussians. Our algorithm uses symmetries of the kernel to provide a fast computation of the Gaussian decomposition in the frequency domain, where the 2D Tikhonov kernel has a closed-form expression. The convolution with each Gaussian is then computed using linear-time separable recursive filtering. In the non-homogeneous case, we also decompose the 2D problem as a series of iterated linear-time separable recursive filters, which can be combined with the Biconjugate Gradient Stabilized method for fast convergence. In this way, a fast solution to the 2D Tikhonov regularization problem is obtained.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523001988",
    "keywords": [
      "Algorithm",
      "Applied mathematics",
      "Artificial intelligence",
      "Computer science",
      "Discrete mathematics",
      "Inverse problem",
      "Kernel (algebra)",
      "Mathematical analysis",
      "Mathematics",
      "Regularization (linguistics)",
      "Separable space",
      "Tikhonov regularization"
    ],
    "authors": [
      {
        "surname": "Ferreira",
        "given_name": "Hermes H."
      },
      {
        "surname": "Gastal",
        "given_name": "Eduardo S.L."
      }
    ]
  },
  {
    "title": "High-order interaction feature selection for classification learning: A robust knowledge metric perspective",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109733",
    "abstract": "Feature selection is an important learning task in data mining and knowledge discovery. Nevertheless, the fuzziness, uncertainty, and noise presented by the data greatly complicate the construction of learning models. Moreover, most works focus on exploring low-order correlations between variables using low-dimensional mutual information, without paying attention to high-order interaction for multiple variables, resulting in the loss of some potentially important dependency information. Driven by these two issues, a robust knowledge metric approach is invented to perceive and excavate the latent information hidden in interaction. In this study, firstly, a robust fuzzy granularity space is constructed from different granular structures induced by different features, and the robust fuzzy uncertainty measures (RFUMs) are successively devised. Then, RFUMs are used to measure pair-wise, three-order, and even higher-order interaction dependencies among features. Further, a constrained high-order interaction evaluation function inspired by the N-gram language model is formulated, and a corresponding high-order interaction feature selection algorithm with RFUMs (HIFS-RFUMs) is designed. Next, comparative experiments with seven representative algorithms on twenty datasets illustrate its effectiveness. In addition, ablation experiments are conducted on the high-order interaction feature selection algorithm with fuzzy uncertainty measures (HIFS-FUMs) and the relative reduction algorithm with RFUMs (R2-RFUMs), which demonstrate the robustness of the metric and the effectiveness for mining high-order interactive features, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323004314",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Data mining",
      "Engineering",
      "Feature (linguistics)",
      "Feature selection",
      "Fuzzy logic",
      "Gene",
      "Granularity",
      "Interaction information",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Metric (unit)",
      "Mutual information",
      "Operating system",
      "Operations management",
      "Philosophy",
      "Robustness (evolution)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Wan",
        "given_name": "Jihong"
      },
      {
        "surname": "Chen",
        "given_name": "Hongmei"
      },
      {
        "surname": "Li",
        "given_name": "Tianrui"
      },
      {
        "surname": "Li",
        "given_name": "Min"
      },
      {
        "surname": "Yang",
        "given_name": "Xiaoling"
      }
    ]
  },
  {
    "title": "Music Mobility Patterns: How Songs Propagate Around The World Through Spotify",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109807",
    "abstract": "Nowadays, music streaming services allow users to get instant access to an unprecedented amount of music of any type. This entails that songs, including potential new hits, can be discovered by listeners in any part of the globe and their propagation can be tracked through these streaming services. In this context, the present work focuses on recognizing the mobility patterns that songs follow in such a propagation among different countries of the world. To this end, this work defines a novel mechanism to uncover such mobility patterns of music from a directed-graph structure where nodes are countries and each edge reflects a frequent propagation of songs between pairs of countries. The resulting patterns reflect strong correlations with the migratory flows and the cultural and social similarities among regions. For instance, a propagation pattern was observed among North European countries with a good command of English. From such patterns, potential predictors for anticipating the mobility of a song are discussed. The results of this work can be beneficial for record companies and artists in their marketing campaigns for album releases and the organization of concerts and festivals.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005058",
    "keywords": [
      "Advertising",
      "Archaeology",
      "Business",
      "Computer science",
      "Context (archaeology)",
      "Engineering",
      "Geography",
      "Globe",
      "Mechanical engineering",
      "Neuroscience",
      "Psychology",
      "Telecommunications",
      "Work (physics)",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Terroso-Saenz",
        "given_name": "Fernando"
      },
      {
        "surname": "Soto",
        "given_name": "Jesús"
      },
      {
        "surname": "Muñoz",
        "given_name": "Andres"
      }
    ]
  },
  {
    "title": "An enhanced noise-tolerant hashing for drone object detection",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109762",
    "abstract": "Drone, a.k.a. Unmanned aerial vehicle (UAV), has been pervasively applied in geological hazard monitoring, smart agriculture, and urban planning in the past decade. In this work, we fuse multiple attributes into a noise-tolerant hashing framework that can detect objects from drone pictures extremely fast. Our method can intrinsically and flexibly encode various topological structures from each target object, based on which multi-scale objects can be discovered in a view- and altitude-invariant way. Moreover, by leveraging l F and l 1 norms collaboratively, the calculated hash codes are robust to low quality drone pictures and noisy semantic labels. More specifically, for each drone-borne picture, we extract visually/semantically salient object parts inside it. To characterize their topological structure, we construct a graphlet by linking the spatially adjacent object patches into a small graph. Subsequently, a binary matrix factorization (MF) is designed to hierarchically exploit the semantics of these graphlets, wherein three attributes: i) deep binary hash codes learning, ii) contaminated pictures/labels denoising, and iii) adaptive data graph updating are seamlessly incorporated. Such multi-attribute binary MF can be solved iteratively, and in turn each graphlet is transformed into the binary hash codes. Finally, the hash codes corresponding to graphlets within each drone photo are utilized for ranking-based object discovery. Comprehensive experiments on the DAC-SDC, MOHR, and our self-compiled data set have demonstrated the competitively speed and accuracy of our method. As a byproduct, we employ an elaborately-designed FPGA architecture to calculate our hash codes. On average, a 57 frames per second (fps) object detection speed is achieved on 4K drone videos (without temporal modeling).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323004600",
    "keywords": [
      "Arithmetic",
      "Artificial intelligence",
      "Binary code",
      "Binary number",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Data mining",
      "Data structure",
      "Graph",
      "Hash function",
      "Hash table",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Programming language",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Luming"
      },
      {
        "surname": "Wang",
        "given_name": "Guifeng"
      },
      {
        "surname": "Chen",
        "given_name": "Ming"
      },
      {
        "surname": "Ren",
        "given_name": "Fuji"
      },
      {
        "surname": "Shao",
        "given_name": "Ling"
      }
    ]
  },
  {
    "title": "APUNet: Attention-guided upsampling network for sparse and non-uniform point cloud",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109796",
    "abstract": "Point cloud upsampling is a basic low-level task, that is important for improving the quality of a point cloud. However, existing point cloud upsampling methods perform poorly on sparse and non-uniform point clouds, due to that they fail to fully model the relationship between points. To address this issue, in this paper, we propose an attention-guided network called APUNet to exploit the correlation between points, which can perform unsampling for sparse and non-uniform point cloud. In particular, we first propose a feature extraction unit, DisTransformer, which can effectively model the relationship between points by introducing a distance prior to the attention mechanism. We also design a point cloud feature extraction network based on DisTransformer. By computing the correlation between patches and the correlation between points, we fuse the global and local features to better model the correlation of the whole object. Furthermore, we propose a feature prediction module based on attention mechanisms that avoids generating clustered points by transforming the point cloud expansion task into a point cloud prediction task. Qualitative and quantitative experiments reveal the superiority of our method compared to the current state-of-the-art methods. Compared with other point cloud upsampling methods, APUNet can much better upsample non-uniform and extremely sparse point clouds.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323004946",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Cloud computing",
      "Computer science",
      "Data mining",
      "Feature (linguistics)",
      "Feature extraction",
      "Geometry",
      "Image (mathematics)",
      "Linguistics",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Point (geometry)",
      "Point cloud",
      "Upsampling"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Tianming"
      },
      {
        "surname": "Li",
        "given_name": "Linfeng"
      },
      {
        "surname": "Tian",
        "given_name": "Tian"
      },
      {
        "surname": "Ma",
        "given_name": "Jiayi"
      },
      {
        "surname": "Tian",
        "given_name": "Jinwen"
      }
    ]
  },
  {
    "title": "Dynamic scene deblurring with continuous cross-layer attention transmission",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109719",
    "abstract": "The deep convolutional neural networks (CNNs) using attention mechanism have achieved great success for dynamic scene deblurring. In most of these networks, only the features refined by the attention maps can be passed to the next layer and the attention maps of different layers are separated from each other, which does not make full use of the attention information from different layers in the CNN. To address this problem, we introduce a new continuous cross-layer attention transmission (CCLAT) mechanism that can exploit hierarchical attention information from all the convolutional layers. Based on the CCLAT mechanism, we use a very simple attention module to construct a novel residual dense attention fusion block (RDAFB). In RDAFB, the attention maps inferred from the outputs of the preceding RDAFB and each layer are directly connected to the subsequent ones, leading to a CCLAT mechanism. Taking RDAFB as the building block, we design an effective architecture for dynamic scene deblurring named RDAFNet. The experiments on benchmark datasets show that the proposed model outperforms the state-of-the-art deblurring approaches, and demonstrate the effectiveness of CCLAT mechanism. The source code is available on: https://github.com/xjmz6/RDAFNet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032300417X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Block (permutation group theory)",
      "Chemistry",
      "Computer science",
      "Construct (python library)",
      "Convolutional neural network",
      "Deblurring",
      "Epistemology",
      "Geodesy",
      "Geography",
      "Geometry",
      "Image (mathematics)",
      "Image processing",
      "Image restoration",
      "Layer (electronics)",
      "Mathematics",
      "Mechanism (biology)",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Residual",
      "Telecommunications",
      "Transmission (telecommunications)"
    ],
    "authors": [
      {
        "surname": "Hua",
        "given_name": "Xia"
      },
      {
        "surname": "Li",
        "given_name": "Mingxin"
      },
      {
        "surname": "Fei",
        "given_name": "Junxiong"
      },
      {
        "surname": "Liu",
        "given_name": "Jianguo"
      },
      {
        "surname": "Shi",
        "given_name": "Yu"
      },
      {
        "surname": "Hong",
        "given_name": "Hanyu"
      }
    ]
  },
  {
    "title": "Exploiting spatial relationships for visual tracking",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.09.010",
    "abstract": "Transformer has been widely applied to visual tracking tasks, and the performance of object tracking keeps getting better since the attention mechanism excels in capturing long-range dependencies. However, conventional attention mechanisms can only capture feature dependencies from a single dimension, limiting their ability to fully leverage the spatial position information that is critical for accurate target localization. In this work, we propose a novel axial attention mechanism that is specifically designed for object tracking tasks that require precise target positioning. The axial attention mechanism utilizes both row attention and column attention, enabling it to capture global feature dependencies from both dimensions. Additionally, we propose a low-complexity yet highly effective feature fusion network that employs a customized dual attention mixing strategy. Our CRTrack outperforms all state-of-the-art trackers on GOT-10k, LaSOT, TrackingNet, and UAV123, demonstrating the superiority of the framework. Notably, the MACs and Params of our proposed feature fusion network are only 40% and 24% of those of the Transformer-based feature fusion network, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523002581",
    "keywords": [
      "Artificial intelligence",
      "BitTorrent tracker",
      "Computer science",
      "Computer vision",
      "Eye tracking",
      "Feature (linguistics)",
      "Fusion",
      "Fusion mechanism",
      "Leverage (statistics)",
      "Linguistics",
      "Lipid bilayer fusion",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Yao"
      },
      {
        "surname": "Li",
        "given_name": "Lunbo"
      },
      {
        "surname": "Guo",
        "given_name": "Jianhui"
      },
      {
        "surname": "Yang",
        "given_name": "Chen"
      },
      {
        "surname": "Zhang",
        "given_name": "Haofeng"
      }
    ]
  },
  {
    "title": "RADAM: Texture recognition through randomized aggregated encoding of deep activation maps",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109802",
    "abstract": "Texture analysis is a classical yet challenging task in computer vision for which deep neural networks are actively being applied. Most approaches are based on building feature aggregation modules around a pre-trained backbone and then fine-tuning the new architecture on specific texture recognition tasks. Here we propose a new method named Random encoding of Aggregated Deep Activation Maps (RADAM) which extracts rich texture representations without ever changing the backbone. The technique consists of encoding the output at different depths of a pre-trained deep convolutional network using a Randomized Autoencoder (RAE). The RAE is trained locally to each image using a closed-form solution, and its decoder weights are used to compose a 1-dimensional texture representation that is fed into a linear SVM. This means that no fine-tuning or backpropagation is needed for the backbone. We explore RADAM on several texture benchmarks and achieve state-of-the-art results with different computational budgets. Our results suggest that pre-trained backbones may not require additional fine-tuning for texture recognition if their learned representations are better encoded.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005009",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Autoencoder",
      "Backpropagation",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Encoding (memory)",
      "Feature (linguistics)",
      "Feature extraction",
      "Image (mathematics)",
      "Image processing",
      "Image texture",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Texture (cosmology)",
      "Texture compression"
    ],
    "authors": [
      {
        "surname": "Scabini",
        "given_name": "Leonardo"
      },
      {
        "surname": "Zielinski",
        "given_name": "Kallil M."
      },
      {
        "surname": "Ribas",
        "given_name": "Lucas C."
      },
      {
        "surname": "Gonçalves",
        "given_name": "Wesley N."
      },
      {
        "surname": "De Baets",
        "given_name": "Bernard"
      },
      {
        "surname": "Bruno",
        "given_name": "Odemir M."
      }
    ]
  },
  {
    "title": "Fully context-aware image inpainting with a learned semantic pyramid",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109741",
    "abstract": "Restoring reasonable and realistic content for arbitrary missing regions in images is an important yet challenging task. Although recent image inpainting models have made significant progress in generating vivid visual details, they can still lead to texture blurring or structural distortions due to contextual ambiguity when dealing with more complex scenes. To address this issue, we propose the Semantic Pyramid Network (SPN) motivated by the idea that learning multi-scale semantic priors from specific pretext tasks can greatly benefit the recovery of locally missing content in images. SPN consists of two components. First, it distills semantic priors from a pretext model into a multi-scale feature pyramid, achieving a consistent understanding of the global context and local structures. Within the prior learner, we present an optional module for variational inference to realize probabilistic image inpainting driven by various learned priors. The second component of SPN is a fully context-aware image generator, which adaptively and progressively refines low-level visual representations at multiple scales with the (stochastic) prior pyramid. We train the prior learner and the image generator as a unified model without any post-processing. Our approach achieves the state of the art on multiple datasets, including Places2, Paris StreetView, CelebA, and CelebA-HQ, under both deterministic and probabilistic inpainting setups.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323004399",
    "keywords": [
      "Artificial intelligence",
      "Bayesian probability",
      "Biology",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Generator (circuit theory)",
      "Geometry",
      "Image (mathematics)",
      "Inference",
      "Inpainting",
      "Machine learning",
      "Mathematics",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Physics",
      "Power (physics)",
      "Prior probability",
      "Probabilistic logic",
      "Pyramid (geometry)",
      "Quantum mechanics"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Wendong"
      },
      {
        "surname": "Wang",
        "given_name": "Yunbo"
      },
      {
        "surname": "Ni",
        "given_name": "Bingbing"
      },
      {
        "surname": "Yang",
        "given_name": "Xiaokang"
      }
    ]
  },
  {
    "title": "Towards reliable multi-person pose estimation using Conditional Random Fields",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.10.003",
    "abstract": "Multi-person pose estimation is the task of estimating the coordinates of body joints and predicting the body poses for multiple people in an images. This problem has made breakthroughs in recent years, but the solutions still suffer from some shortcomings. A serious weakness of the state-of-the-art models is the number of poses detected by these models which is generally much larger than the actual number of human instances in the input image. This makes the existing models unreliable and thus unusable in real-world tasks. In this paper, we propose a more reliable multi-person pose estimation method consisting of three main blocks: a top-down multi-person pose estimation, a human detection, and a pose selection block. The proposed method incorporates the bounding box of the segmented objects to select the best subset of the initial pose set. We formulate the pose selection problem using Conditional Random Fields. First, we introduce a set of potential functions to form a general probability model. Then an inference algorithm is proposed to select the best poses which maximize the probability function. Finally, the proposed solution is implemented by a neural network. The proposed pose selection model is a model-agnostic method that can be easily used in conjunction with other pose estimation and object detection models. Experiments demonstrate that the reliability and precision of the proposed model are higher than those of the state-of-the-art models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552300274X",
    "keywords": [
      "3D pose estimation",
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Bounding overwatch",
      "Computer science",
      "Conditional random field",
      "Economics",
      "Geometry",
      "Image (mathematics)",
      "Inference",
      "Machine learning",
      "Management",
      "Mathematics",
      "Minimum bounding box",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Pose",
      "Programming language",
      "Selection (genetic algorithm)",
      "Set (abstract data type)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Ghasemi-Naraghi",
        "given_name": "Zeinab"
      },
      {
        "surname": "Nickabadi",
        "given_name": "Ahmad"
      },
      {
        "surname": "Safabakhsh",
        "given_name": "Reza"
      }
    ]
  },
  {
    "title": "Temporal-Relational hypergraph tri-Attention networks for stock trend prediction",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109759",
    "abstract": "Predicting the future price trends of stocks is a challenging yet intriguing problem given its critical role to help investors make profitable decisions. In this paper, we present a collaborative temporal-relational modeling framework for end-to-end stock trend prediction. Different from existing studies relying on the pairwise correlations between stocks, we argue that stocks are naturally connected as a collective group, and introduce two heterogeneous hypergraphs to separately characterize the stock group-wise relationships of industry-belonging and fund-holding. A novel hypergraph tri-attention network (HGTAN) is proposed to augment the hypergraph convolutional networks with a hierarchical organization of intra-hyperedge, inter-hyperedge, and inter-hypergraph attention modules. In this manner, HGTAN adaptively determines the importance of nodes, hyperedges, and hypergraphs during the information propagation among stocks, so that the potential synergies between stock movements can be fully exploited. Experimental evaluation and investment simulation on real-world stock data demonstrate the effectiveness of our approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323004570",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Discrete mathematics",
      "Econometrics",
      "Economics",
      "Geography",
      "Hypergraph",
      "Mathematics",
      "Pairwise comparison",
      "Relational database",
      "Stock (firearms)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Cui",
        "given_name": "Chaoran"
      },
      {
        "surname": "Li",
        "given_name": "Xiaojie"
      },
      {
        "surname": "Zhang",
        "given_name": "Chunyun"
      },
      {
        "surname": "Guan",
        "given_name": "Weili"
      },
      {
        "surname": "Wang",
        "given_name": "Meng"
      }
    ]
  },
  {
    "title": "Feature perturbation augmentation for reliable evaluation of importance estimators in neural networks",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.10.012",
    "abstract": "Post-hoc explanation methods (such as importance estimators and saliency maps) attempt to make the inner workings of deep neural networks (DNNs) more comprehensible and trustworthy, which otherwise act as black box models. However, since a ground truth is in general lacking, local post-hoc explanation methods, which assign importance scores to input features, are challenging to evaluate. One of the most popular evaluation frameworks is to perturb features deemed important by an explanation and to measure the change in prediction accuracy. Intuitively, a large decrease in prediction accuracy would indicate that the explanation has correctly quantified the importance of features with respect to the prediction outcome (e.g., logits). However, the change in the prediction outcome may stem from perturbation artifacts, since perturbed samples in the test dataset are out of distribution (OOD) compared to the training dataset and can therefore potentially disturb the model in an unexpected manner. To overcome this challenge, we propose feature perturbation augmentation (FPA) which creates and adds perturbed images during the model training. Using three different datasets and several importance estimators, our computational experiments demonstrate that FPA makes DNNs more robust against perturbations. During evaluation, we considered model accuracy curves obtained from perturbing input features according to most important first (MIF) and least important first (LIF) orders, which are quantitatively summarized as fidelity metrics. Additionally, our results suggest that frequently observed fluctuations in the sign of importance scores describe the model characteristics rather accurately if perturbation artifacts are suppressed by FPA. Overall, FPA is an intuitive and straightforward data augmentation technique that renders the evaluation of post-hoc explanations more trustworthy. Reproducible codes and pre-trained models with FPA are available on Github: https://github.com/lenbrocki/Feature-Perturbation-Augmentation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523002842",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer security",
      "Data mining",
      "Deep neural networks",
      "Dentistry",
      "Estimator",
      "Feature (linguistics)",
      "Ground truth",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Medicine",
      "Perturbation (astronomy)",
      "Philosophy",
      "Physics",
      "Post hoc",
      "Quantum mechanics",
      "Statistics",
      "Trustworthiness"
    ],
    "authors": [
      {
        "surname": "Brocki",
        "given_name": "Lennart"
      },
      {
        "surname": "Chung",
        "given_name": "Neo Christopher"
      }
    ]
  },
  {
    "title": "Contrastive Generative Network with Recursive-Loop for 3D point cloud generalized zero-shot classification",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109843",
    "abstract": "Generalized Zero-Shot Learning (GZSL) aims to recognize objects from both seen and unseen categories by transferring semantic knowledge and merely utilizing seen class data for training. Recent feature generation methods in the 2D image domain have made great progress. However, very little is known about its usefulness in 3D point cloud zero-shot learning. This work aims to facilitate research on 3D point cloud generalized zero-shot learning. Different from previous works, we focus on synthesizing the more high-level discriminative point cloud features. To this end, we design a representation enhancement strategy to generate the features. Specifically, we propose a Contrastive Generative Network with Recursive-Loop, termed as CGRL, which can be leveraged to enlarge the inter-class distances and narrow the intra-class gaps. By applying the contrastive representations to the generative model in a recursive-loop form, it can provide the self-guidance for the generator recurrently, which can help yield more discriminative features and train a better classifier. To validate the effectiveness of the proposed method, extensive experiments are conducted on three benchmarks, including ModelNet40, McGill, and ScanObjectNN. Experimental evaluations demonstrate the superiority of our approach and it can outperform the state-of-the-arts by a large margin. Code is available at https://github.com/photon-git/CGRL",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005411",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Discriminative model",
      "Feature learning",
      "Generative grammar",
      "Generative model",
      "Generator (circuit theory)",
      "Law",
      "Machine learning",
      "Margin (machine learning)",
      "Pattern recognition (psychology)",
      "Physics",
      "Point cloud",
      "Political science",
      "Politics",
      "Power (physics)",
      "Quantum mechanics",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Hao",
        "given_name": "Yun"
      },
      {
        "surname": "Su",
        "given_name": "Yukun"
      },
      {
        "surname": "Lin",
        "given_name": "Guosheng"
      },
      {
        "surname": "Su",
        "given_name": "Hanjing"
      },
      {
        "surname": "Wu",
        "given_name": "Qingyao"
      }
    ]
  },
  {
    "title": "Cross-Domain Few-Shot classification via class-shared and class-specific dictionaries",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109811",
    "abstract": "In Cross-Domain Few-Shot Classification, researchers mainly utilize models which trained with source domain tasks to adapt to the target domain with very few samples, thus causing serious class-difference-caused domain differences. Although researchers have proposed methods to minimize the domain differences, the existing methods have the following drawbacks: 1) most models do not utilize the common knowledge between the source and target domains, and 2) require additional labeled samples from the target domain for finetuning or domain alignment, which is hard to obtain in reality. To address the problem mentioned above, we propose a class-shared and class-specific dictionaries (CSCSD) learning method. To make better utilization of the common knowledge, we apply a class-shared dictionary which is learned to represent the generality of source and target domain. Moreover, class-specific dictionaries are applied to represent the class-specific knowledge that can’t be represented in the class-shared dictionary. Furthermore, unlike most other models, our CSCSD does not require additional target domain samples to meta-train or finetune. With the dictionaries, CSCSD can obtain more distinguishable collaborative representations of samples with the origin representations extracted with the model. To evaluate the effectiveness of CSCSD, we utilize larger datasets, e.g., MiniImageNet and TieredImageNet as source domains and fine-grained datasets, e.g., CUB, Cars, Places, and Plantae as target domains. With our CSCSD, the Cross-Domain Few-Shot accuracy exceeds most domain adaptive Few-Shot which utilizes additional training set in target domains.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005095",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Domain (mathematical analysis)",
      "Generality",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Programming language",
      "Psychology",
      "Psychotherapist",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Renjie"
      },
      {
        "surname": "Xing",
        "given_name": "Lei"
      },
      {
        "surname": "Liu",
        "given_name": "Baodi"
      },
      {
        "surname": "Tao",
        "given_name": "Dapeng"
      },
      {
        "surname": "Cao",
        "given_name": "Weijia"
      },
      {
        "surname": "Liu",
        "given_name": "Weifeng"
      }
    ]
  },
  {
    "title": "CDS-Net: Cooperative dual-stream network for image manipulation detection",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.11.005",
    "abstract": "To accurately locate manipulated regions, many existing approaches employ a dual-stream framework to extract a wide range of manipulation clues, including local noise, edge artifacts, and global inconsistency. However, these approaches treat each stream in isolation and fail to consider the complementary and mutual guidance ability between the streams. Moreover, we notice the use of vanilla vision transformers in previous approaches can result in disruptions of object semantics, causing incomplete predictions. To address these challenges, we introduce the cooperative dual-stream network (CDS-Net) comprising an RGB Stream and a Noise Stream. In the Noise Stream, we propose a K-means Transformer (KT) that encourages both inter-patch and intra-patch information transmission to mitigate the semantic fragmentation phenomenon caused by patch partitioning. Additionally, we introduce a novel Feature Interaction Block (FIB) that explicitly encourages cross-stream collaboration at each encoding stage. Comprehensive experiments on publicly available datasets demonstrate the effectiveness and robustness of CDS-Net.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523003148",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Bitstream",
      "Chemistry",
      "Computer science",
      "Data mining",
      "Decoding methods",
      "Distributed computing",
      "Gene",
      "Pattern recognition (psychology)",
      "Robustness (evolution)",
      "Stream processing"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Haoran"
      },
      {
        "surname": "Deng",
        "given_name": "Jiahao"
      },
      {
        "surname": "Lin",
        "given_name": "Xun"
      },
      {
        "surname": "Tang",
        "given_name": "Wenzhong"
      },
      {
        "surname": "Wang",
        "given_name": "Shuai"
      }
    ]
  },
  {
    "title": "Dynamics-aware loss for learning with label noise",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109835",
    "abstract": "Label noise poses a serious threat to deep neural networks (DNNs). Employing robust loss functions which reconcile fitting ability with robustness is a simple but effective strategy to handle this problem. However, the widely-used static trade-off between these two factors contradicts the dynamics of DNNs learning with label noise, leading to inferior performance. Therefore, we propose a dynamics-aware loss (DAL) to solve this problem. Considering that DNNs tend to first learn beneficial patterns, then gradually overfit harmful label noise, DAL strengthens the fitting ability initially, then gradually improves robustness. Moreover, at the later stage, to further reduce the negative impact of label noise and combat underfitting simultaneously, we let DNNs put more emphasis on easy examples than hard ones and introduce a bootstrapping term. Both the detailed theoretical analyses and extensive experimental results demonstrate the superiority of our method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005332",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Bootstrapping (finance)",
      "Chemistry",
      "Computer science",
      "Deep neural networks",
      "Econometrics",
      "Gene",
      "Image (mathematics)",
      "Machine learning",
      "Mathematics",
      "Noise (video)",
      "Overfitting",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Xiu-Chuan"
      },
      {
        "surname": "Xia",
        "given_name": "Xiaobo"
      },
      {
        "surname": "Zhu",
        "given_name": "Fei"
      },
      {
        "surname": "Liu",
        "given_name": "Tongliang"
      },
      {
        "surname": "Zhang",
        "given_name": "Xu-Yao"
      },
      {
        "surname": "Liu",
        "given_name": "Cheng-Lin"
      }
    ]
  },
  {
    "title": "Adaptive graph fusion learning for multi-view spectral clustering",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.10.027",
    "abstract": "Multi-view data suffer from issues related to low quality and heterogeneity, which leads to instability issues in existing learning models for clustering. To overcome the limitations of traditional clustering methods, we propose a novel multi-view spectral clustering based on a contrastive feedback strategy. The proposed method constructs kernelized graphs to obtain higher-order feature representations, mapping primary sample data from different views into an isomorphic feature space and masking differences in type and structure between multiple views. In the meantime, the proposed method leverages a contrastive feedback optimization strategy based on mutual information to iteratively optimize the kernelized graphs of all views and their dynamically fused graph. This method effectively exploits the consistency and complementary information between multi-view, further improving the clustering accuracy. Experiments on multiple datasets demonstrate that the proposed method achieves satisfactory results in terms of clustering, validating the feasibility and effectiveness of the contrastive feedback strategy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523002982",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Fusion",
      "Graph",
      "Linguistics",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Spectral clustering",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Bo"
      },
      {
        "surname": "Liu",
        "given_name": "Wenliang"
      },
      {
        "surname": "Shen",
        "given_name": "Meizhou"
      },
      {
        "surname": "Lu",
        "given_name": "Zhengyu"
      },
      {
        "surname": "Zhang",
        "given_name": "Wenzhen"
      },
      {
        "surname": "Zhang",
        "given_name": "Luyun"
      }
    ]
  },
  {
    "title": "ARAI-MVSNet: A multi-view stereo depth estimation network with adaptive depth range and depth interval",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109885",
    "abstract": "Multi-View Stereo (MVS) is a fundamental problem in geometric computer vision which aims to reconstruct a scene using multi-view images with known camera parameters. However, the mainstream approaches represent the scene with a fixed all-pixel depth range and equal depth interval partition, which will result in inadequate utilization of depth planes and imprecise depth estimation. In this paper, we present a novel multi-stage coarse-to-fine framework to achieve adaptive all-pixel depth range and depth interval. We predict a coarse depth map in the first stage, then an Adaptive Depth Range Prediction module is proposed in the second stage to zoom in the scene by leveraging the reference image and the obtained depth map in the first stage and predict a more accurate all-pixel depth range for the following stages. In the third and fourth stages, we propose an Adaptive Depth Interval Adjustment module to achieve adaptive variable interval partition for pixel-wise depth range. The depth interval distribution in this module is normalized by Z-score, which can allocate dense depth hypothesis planes around the potential ground truth depth value and vice versa to achieve more accurate depth estimation. Extensive experiments on four widely used benchmark datasets (DTU, TnT, BlendedMVS, ETH 3D) demonstrate that our model achieves state-of-the-art performance and yields competitive generalization ability. Particularly, our method achieves the highest Acc and Overall on the DTU dataset, while attaining the highest Recall and F 1 -score on the Tanks and Temples intermediate and advanced dataset. Moreover, our method also achieves the lowest e 1 and e 3 on the BlendedMVS dataset and the highest Acc and F 1 -score on the ETH 3D dataset, surpassing all listed methods. Project website: https://github.com/zs670980918/ARAI-MVSNet",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005836",
    "keywords": [
      "Aerospace engineering",
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Depth map",
      "Engineering",
      "Geology",
      "Geophysics",
      "Image (mathematics)",
      "Interval (graph theory)",
      "Mathematics",
      "Measured depth",
      "Range (aeronautics)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Song"
      },
      {
        "surname": "Xu",
        "given_name": "Wenjia"
      },
      {
        "surname": "Wei",
        "given_name": "Zhiwei"
      },
      {
        "surname": "Zhang",
        "given_name": "Lili"
      },
      {
        "surname": "Wang",
        "given_name": "Yang"
      },
      {
        "surname": "Liu",
        "given_name": "Junyi"
      }
    ]
  },
  {
    "title": "FusionCalib: Automatic extrinsic parameters calibration based on road plane reconstruction for roadside integrated radar camera fusion sensors",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.10.015",
    "abstract": "Calibrating the relative positional posture of the roadside sensor with respect to the road is crucial for target tracking and sensor fusion. However, most previous studies rely on prior information or require additional manual measurements, which increases calibration error and labor costs. This paper proposes an automatic extrinsic parameters calibration method for roadside integrated radar camera fusion sensors for the first time. In particular, we leverage the radar camera fusion framework to automatically reconstruct the road plane. Specifically, to integrate sensing abilities, a stable bidirectional selection association method is adopted to match radar and camera trajectories. Based on the association results, the Extended Kalman Filter (EKF) is adopted to fuse pixel positions and three-dimensional (3D) velocities of vehicles, and then a series of 3D positions of vehicle bottoms are obtained to reconstruct the road plane. Experimental results show that our proposed method achieves automatic and accurate calibration of extrinsic parameters.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523002878",
    "keywords": [
      "Artificial intelligence",
      "Calibration",
      "Computer science",
      "Computer vision",
      "Fusion",
      "Geology",
      "Geometry",
      "Linguistics",
      "Mathematics",
      "Philosophy",
      "Plane (geometry)",
      "Radar",
      "Remote sensing",
      "Sensor fusion",
      "Statistics",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Deng",
        "given_name": "Jiayin"
      },
      {
        "surname": "Hu",
        "given_name": "Zhiqun"
      },
      {
        "surname": "Lu",
        "given_name": "Zhaoming"
      },
      {
        "surname": "Wen",
        "given_name": "Xiangming"
      }
    ]
  },
  {
    "title": "A novel method of human identification based on dental impression image",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109864",
    "abstract": "In large-scale natural disasters and special criminal cases, surface features of bodies, such as faces and fingerprints, are easily destroyed. Teeth possess strong high-temperature resistance, corrosion resistance, and high hardness, which can compensate for the shortcomings of the aforementioned situations. This paper proposes an identification method based on the aggregated features of multi-scale dental impression images. Firstly, a method exploiting the adaptive object detection method based on YOLOv8 is proposed to segment toothprints. Next, a novel geometric feature named calibrated offset distance is extracted, combined with the SIFT feature method, to extract multi-scale and multi-dimensional features from the global toothprint, local toothprints, and single-tooth prints. Finally, all features are aggregated to enhance the descriptive ability and robustness. Experimental results indicate that the method proposed in this paper demonstrates good identification performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005629",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Biology",
      "Botany",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Feature matching",
      "Gene",
      "Identification (biology)",
      "Image (mathematics)",
      "Impression",
      "Offset (computer science)",
      "Pattern recognition (psychology)",
      "Programming language",
      "Robustness (evolution)",
      "Scale-invariant feature transform",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Mao",
        "given_name": "Jiafa"
      },
      {
        "surname": "Wang",
        "given_name": "Lixin"
      },
      {
        "surname": "Wang",
        "given_name": "Ning"
      },
      {
        "surname": "Hu",
        "given_name": "Yahong"
      },
      {
        "surname": "Sheng",
        "given_name": "Weigou"
      }
    ]
  },
  {
    "title": "A survey on deep learning-based image forgery detection",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109778",
    "abstract": "Image is known as one of the communication tools between humans. With the development and availability of digital devices such as cameras and cell phones, taking images has become easy anywhere. Images are used in many medical, forensic medicine, and judiciary applications. Sometimes images are used as evidence, so the authenticity and reliability of digital images are increasingly important. Some people manipulate images by adding or deleting parts of an image, which makes the image invalid. Therefore, image forgery detection and localization are important. The development of image editing tools has made this issue an important problem in the field of computer vision. In recent years, many different algorithms have been proposed to detect forgery in the image and pixel levels. All these algorithms are categorized into two main methods: traditional and deep-learning methods. The deep learning method is one of the important branches of artificial intelligence science. This method has become one of the most popular methods in most computer vision problems due to the automatic identification and prediction process and robustness against geometric transformations and post-processing operations. In this study, a comprehensive review of image forgery types, benchmark datasets, evaluation metrics in forgery detection, traditional forgery detection methods, discovering the weaknesses and limitations of traditional methods, forgery detection with deep learning methods, and the performance of this method is presented. According to the expansion of deep-learning methods and their successful performance in most computer vision problems, our main focus in this study is forgery detection based on deep-learning methods. This survey can be helpful for a researcher to obtain a deep background in the forgery detection field.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323004764",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Digital image",
      "Field (mathematics)",
      "Gene",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Image editing",
      "Image processing",
      "Machine learning",
      "Mathematics",
      "Pure mathematics",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Mehrjardi",
        "given_name": "Fatemeh Zare"
      },
      {
        "surname": "Latif",
        "given_name": "Ali Mohammad"
      },
      {
        "surname": "Zarchi",
        "given_name": "Mohsen Sardari"
      },
      {
        "surname": "Sheikhpour",
        "given_name": "Razieh"
      }
    ]
  },
  {
    "title": "Online portfolio selection with predictive instantaneous risk assessment",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109872",
    "abstract": "Online portfolio selection (OPS) has received increasing attention from machine learning and quantitative finance communities. Despite their effectiveness, the pioneering OPS methods have several key limitations. First, price predictions are usually based on predetermined trends, which is inadequate for a fast-changing market patterns. Second, each asset is treated individually, ignoring the pervading relevance among the assets. Third, the risk terms are usually missing or inappropriate in optimizations. This paper proposes a novel OPS method, namely, the online low-dimension ensemble method, to overcome the limitations. Motivated by the stylized facts for the co-movements of assets, the financial market is regarded as a high-dimensional dynamical system (HDS), and a large number of low-dimensional subsystems (LDSs) are randomly generated from the HDS to extract the correlation information among the assets. The assets’ price predictions are first made using these LDSs and then aggregated to formulate the final prediction using ensemble learning techniques. Thanks to the particular merits brought by our predicting scheme, we also develop a novel high-dimensional covariance matrix estimation/prediction method for short-term data, efficiently assessing the instantaneous risk of the projected portfolios. Compared with state-of-the-art methods, our approach obtains more accurate predictions as the correlation information is fully exploited. With the predictive instantaneous risk assessment, a more appropriate optimization problem is proposed, substantially improving the OPS setting and leading to significantly better investment performance. Therefore, this study develops a flexible and promising approach to learning fast-changing market patterns and demonstrates that the high-dimensional feature of the market is a crucial information source for financial modeling with short-term data rather than a barrier in the conventional sense. Extensive experiments on real-world datasets are conducted to illustrate our method further.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005708",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Asset (computer security)",
      "Computer science",
      "Computer security",
      "Covariance matrix",
      "Data mining",
      "Econometrics",
      "Economics",
      "Finance",
      "Investment strategy",
      "Law",
      "Machine learning",
      "Macroeconomics",
      "Market liquidity",
      "Political science",
      "Portfolio",
      "Portfolio optimization",
      "Relevance (law)",
      "Selection (genetic algorithm)",
      "Stylized fact"
    ],
    "authors": [
      {
        "surname": "Xi",
        "given_name": "Wenzhi"
      },
      {
        "surname": "Li",
        "given_name": "Zhanfeng"
      },
      {
        "surname": "Song",
        "given_name": "Xinyuan"
      },
      {
        "surname": "Ning",
        "given_name": "Hanwen"
      }
    ]
  },
  {
    "title": "Sparse kernel k-means for high-dimensional data",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109873",
    "abstract": "The kernel k -means method usually loses its power when clustering high-dimensional data, due to a large number of irrelevant features. We propose a novel sparse kernel k -means clustering (SKKM) to extend the advantages of kernel k -means to the high-dimensional cases. We assign each feature a 0 - 1 indicator and optimize an equivalent kernel k -means loss function while penalizing the sum of the indicators. An alternating minimization algorithm is proposed to estimate both the class labels and the feature indicators. We prove the consistency of both clustering and feature selection of the proposed method. In addition, we apply the proposed framework to the normalized cut. In the numerical experiments, we demonstrate that the proposed method provides better/comparable performance compared to the existing high-dimensional clustering methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032300571X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Kernel (algebra)",
      "Kernel method",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Guan",
        "given_name": "Xin"
      },
      {
        "surname": "Terada",
        "given_name": "Yoshikazu"
      }
    ]
  },
  {
    "title": "Audio-driven talking face generation with diverse yet realistic facial animations",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109865",
    "abstract": "Audio-driven talking face generation, which aims to synthesize talking faces with realistic facial animations (including accurate lip movements, vivid facial expression details and natural head poses) corresponding to the audio, has achieved rapid progress in recent years. However, most existing work focuses on generating lip movements only without handling the closely correlated facial expressions, which degrades the realism of the generated faces greatly. This paper presents DIRFA, a novel method that can generate talking faces with diverse yet realistic facial animations from the same driving audio. To accommodate fair variation of plausible facial animations for the same audio, we design a transformer-based probabilistic mapping network that can model the variational facial animation distribution conditioned upon the input audio and autoregressively convert the audio signals into a facial animation sequence. In addition, we introduce a temporally-biased mask into the mapping network, which allows to model the temporal dependency of facial animations and produce temporally smooth facial animation sequence. With the generated facial animation sequence and a source image, photo-realistic talking faces can be synthesized with a generic generation network. Extensive experiments show that DIRFA can generate talking faces with realistic facial animations effectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005630",
    "keywords": [
      "Animation",
      "Artificial intelligence",
      "Computer animation",
      "Computer facial animation",
      "Computer graphics (images)",
      "Computer science",
      "Computer vision",
      "Face (sociological concept)",
      "Face detection",
      "Facial expression",
      "Facial motion capture",
      "Facial recognition system",
      "Pattern recognition (psychology)",
      "Social science",
      "Sociology",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Rongliang"
      },
      {
        "surname": "Yu",
        "given_name": "Yingchen"
      },
      {
        "surname": "Zhan",
        "given_name": "Fangneng"
      },
      {
        "surname": "Zhang",
        "given_name": "Jiahui"
      },
      {
        "surname": "Zhang",
        "given_name": "Xiaoqin"
      },
      {
        "surname": "Lu",
        "given_name": "Shijian"
      }
    ]
  },
  {
    "title": "Patch-guided point matching for point cloud registration with low overlap",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109876",
    "abstract": "Point cloud registration is a classic and fundamental problem. Existing point cloud registration methods obtain correspondence point pairs by calculating the correlation between point features. However, the instability of point features makes the outlier rate of corresponding point pairs high, resulting in poor matching results, especially when facing low overlap point clouds. An obvious fact is that patch matching has higher reliability than point matching. To this end, we propose a patch-guided point cloud registration network. Specifically, we perform fusion on patches and points at both the feature and result levels to achieve the guidance of patch to point matching and improve the accuracy of predicted point pairs. At the feature level, we propose a Matching Pyramid Network (MPN) for multi-level patch/point matching. The core of the MPN is an attention-based cross-layer context aggregation (CCA) module, which is used for the fusion of matching features between upper and lower layers. At the result level, we design a matching consistency judgment module to ensure that the point pairs are consistent in the matching of each layer, which greatly reduces the outlier ratio. Based on the above design, the corresponding point pairs predicted by our network have a high inlier ratio, which makes our method perform well in the face of low overlapping point clouds. Extensive experimental results show that our method outperforms other existing methods for indoor and outdoor datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005745",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Computer science",
      "Consistency (knowledge bases)",
      "Context (archaeology)",
      "Feature (linguistics)",
      "Geography",
      "Geometry",
      "Image (mathematics)",
      "Linguistics",
      "Matching (statistics)",
      "Mathematics",
      "Outlier",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Point (geometry)",
      "Point cloud",
      "Point set registration",
      "Shape context",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Tianming"
      },
      {
        "surname": "Li",
        "given_name": "Linfeng"
      },
      {
        "surname": "Tian",
        "given_name": "Tian"
      },
      {
        "surname": "Ma",
        "given_name": "Jiayi"
      },
      {
        "surname": "Tian",
        "given_name": "Jinwen"
      }
    ]
  },
  {
    "title": "A reflectance re-weighted Retinex model for non-uniform and low-light image enhancement",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109823",
    "abstract": "Image enhancement is a fundamental low-level task of significant importance that can directly affect high-level image processing tasks. Although various methods have been proposed to enhance images, the effectiveness of current methods deteriorates significantly under non-uniform lighting. Since the brightness may vary dramatically in different regions of real-world photos, current methods hardly achieve a good balance between enhancing low-light regions and retaining normal-light regions in the same image. Consequently, either the low-light regions are under-enhanced or the normal-light regions are over-enhanced, while at the same time, color distortion and artifacts are frequently found. To overcome this shortcoming, we propose a robust Retinex-based model with reflectance map re-weighting that can improve the brightness level of the low-light image and re-balance the brightness concurrently. We introduce an alternating scheme to solve our proposed model, in which the illumination map, reflectance map, and weighting map are updated iteratively. By utilizing the regularization terms, the noise is well-suppressed during the process. An initialization scheme for the weighting map is also proposed to make our model adaptable to a wide range of light conditions. To the best of our knowledge, we are the first to propose a variational model with an explicitly constructed re-weighting prior and the associated weighing map concept for the reflectance map. It can estimate the reflectance map, suppress noise, and re-balance the brightness simultaneously. A series of experimental results on a variety of popular datasets demonstrate the efficacy of our method and its superiority in enhancing real low-light images when compared to other state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005216",
    "keywords": [
      "Acoustics",
      "Amplifier",
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Brightness",
      "Color constancy",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Distortion (music)",
      "Fidelity",
      "Image (mathematics)",
      "Initialization",
      "Optics",
      "Physics",
      "Programming language",
      "Telecommunications",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Jia",
        "given_name": "Fan"
      },
      {
        "surname": "Wong",
        "given_name": "Hok Shing"
      },
      {
        "surname": "Wang",
        "given_name": "Tiange"
      },
      {
        "surname": "Zeng",
        "given_name": "Tieyong"
      }
    ]
  },
  {
    "title": "Revisiting the transferability of adversarial examples via source-agnostic adversarial feature inducing method",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109828",
    "abstract": "Though deep neural networks (DNNs) have revealed their extraordinary performance in the fields of computer vision, it is evident that the vulnerability of DNNs to adversarial attacks with crafted human-imperceptible perturbations. Most existing adversarial attacks draw their attention to invading target deep task models by enhancing input-diagnostic features via image rotation, warp, or transformation to improve adversarial transferability. Such manners pay close concentration to operation on original inputs regardless of the properties from different source information. Research has inspired us to consider utilizing source-agnostic information and integrating generated features with raw inputs to enrich adversarial properties. For such needs, we propose a simple and flexible adversarial attack method with source-agnostic Feature Inducing Method (FIM) for improving the transferability of adversarial examples (AEs). FIM first focuses on generating perturbed features by imitating diverse patterns from multi-domain sources. Instead of exploiting the original inputs’ diversity, such proposed work gains the various properties by random feature imitation referring to different source distributions. By optimizing the generated features with norm bounds, FIM then integrates original inputs with imitative features. Such manner can diverse row positive class-general features, which reduce the capability of class-specific patterns on cross-model transferability. Based on the crafted property, FIM employs the adaptive gradient-based strategy on such information to generate perturbations, which helps to decrease probability dropping into local optimal when searching for the decision boundary of source and target models. We conduct detailed experiments to evaluate the performance of our proposed approach with existing baselines on three public datasets. The experimental results reveal the better performance of the proposed works on fooling source and target task models leading to a considerable margin in most adversarial scenarios. We further investigate adversarial attacks on adversarial defense models (with adversarial training and trades). Such a proposed attack strategy achieves better attack quality by a margin over 3.00% on CIFAR10 and reduces the robust accuracy of adversarially trained models by a large margin near 9.00% on MNIST. Furthermore, we exploit the performance of the proposed attack strategy applied to feature-level adversarial domains and conduct evaluations to demonstrate its adversarial feasibility in integrating with various attack mechanisms, which gains better adversarial effectiveness over 20.00% than the base attacks on studied deep task models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005265",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Computer science",
      "Feature (linguistics)",
      "Linguistics",
      "Logit",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Transferability"
    ],
    "authors": [
      {
        "surname": "Xiao",
        "given_name": "Yatie"
      },
      {
        "surname": "Zhou",
        "given_name": "Jizhe"
      },
      {
        "surname": "Chen",
        "given_name": "Kongyang"
      },
      {
        "surname": "Liu",
        "given_name": "Zhenbang"
      }
    ]
  },
  {
    "title": "Two-step multi-view data classification based on dynamic Graph-ELM",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.11.015",
    "abstract": "This paper focuses on the classification problem of multi-view data, aiming to improve the classification accuracy of current algorithms on multi-view data. Previous multi-view classification algorithms are usually based on exploiting the complementarity of different views and fusing features from different views. A representative category is the graph-based method, which builds a graph matrix for each view, and then fuses the graph matrices of different views to obtain a unified graph. These methods have the following problems: firstly, the graph matrix is simply based on sample similarity usually; secondly, the learned graph matrix does not change dynamically; thirdly, the weight of the graph representation matrix for a single view cannot be learned in the unified graph matrix. Therefore, this paper designs a Two-step classification algorithm based on Dynamic Graph-ELM, called TSDGELM. In the TSDGELM, the dynamic Graph-ELM is used to obtain the graph representation matrix of each view to save the local neighbor information of the data, and then a joint graph learning algorithm is designed based on the GBS (Graph-Based System) mechanism to fuse the graph matrix of the single-view, and finally the united graph is input into the classifier. To evaluate the effectiveness of the proposed method in this work, we conduct a series of experiments on eight datasets, and the results demonstrate the superiority of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523003252",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Graph",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Li"
      },
      {
        "surname": "Han",
        "given_name": "Qihong"
      },
      {
        "surname": "Li",
        "given_name": "Jiayao"
      },
      {
        "surname": "Cui",
        "given_name": "Zhanqi"
      }
    ]
  },
  {
    "title": "Compositional clustering: Applications to multi-label object recognition and speaker identification",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109829",
    "abstract": "We consider a novel clustering task in which clusters can have compositional relationships, e.g., one cluster contains images of rectangles, one contains images of circles, and a third (compositional) cluster contains images with both objects. In contrast to hierarchical clustering in which a parent cluster represents the intersection of properties of the child clusters, our problem is about finding compositional clusters that represent the union of the properties of the constituent clusters. This task is motivated by recently developed few-shot learning and embedding models (Alfassy et al., 2019; Li et al., 2021) that can distinguish the label sets, not just the individual labels, assigned to the examples. We propose three new algorithms – Compositional Affinity Propagation (CAP), Compositional k -means (CKM), and Greedy Compositional Reassignment (GCR) – that can partition examples into coherent groups and infer the compositional structure among them. We show promising results, compared to popular algorithms such as Gaussian mixtures, Fuzzy c -means, and Agglomerative Clustering, on the OmniGlot and LibriSpeech datasets. Our work has applications to open-world multi-label object recognition and speaker identification & diarization with simultaneous speech from multiple speakers.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005277",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Cluster analysis",
      "Cognitive neuroscience of visual object recognition",
      "Computer science",
      "Identification (biology)",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Speaker identification",
      "Speaker recognition",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Zeqian"
      },
      {
        "surname": "He",
        "given_name": "Xinlu"
      },
      {
        "surname": "Whitehill",
        "given_name": "Jacob"
      }
    ]
  },
  {
    "title": "CrowdMLP: Weakly-supervised crowd counting via multi-granularity MLP",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109830",
    "abstract": "Currently, state-of-the-art crowd counting algorithms rely excessively on location-level annotations, which are burdensome to acquire. When only weak supervisory signals at the count level are available, it is arduous and error-prone to regress total counts due to the lack of explicit spatial constraints. To address this issue, we propose a novel and efficient counter, CrowdMLP, which explores the modelling of global dependencies of embeddings and regresses total counts by designing a multi-granularity MLP regressor. Specifically, a locally-focused pre-trained frontend is used to extract crude feature maps with intrinsic spatial cues, preventing the model from collapsing into trivial outcomes. The crude embeddings, along with the raw crowd scenes, are tokenized at different granularity levels. Next, the multi-granularity MLP mixes tokens at the dimensions of cardinality, channel, and spatial for mining global information. We also propose an effective proxy task called Split-Counting to overcome the limited samples and the lack of spatial hints in a self-supervised manner. Extensive experiments demonstrate that CrowdMLP significantly outperforms existing weakly-supervised counting algorithms and performs better than state-of-the-art location-level supervised approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005289",
    "keywords": [
      "Artificial intelligence",
      "Cardinality (data modeling)",
      "Computer science",
      "Data mining",
      "Feature (linguistics)",
      "Granularity",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Spatial analysis",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Mingjie"
      },
      {
        "surname": "Zhou",
        "given_name": "Jun"
      },
      {
        "surname": "Cai",
        "given_name": "Hao"
      },
      {
        "surname": "Gong",
        "given_name": "Minglun"
      }
    ]
  },
  {
    "title": "Discriminative projection fuzzy K-Means with adaptive neighbors",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.10.008",
    "abstract": "Fuzzy K-Means (FKM) based on fuzzy theory is a classic method to effectively handle overlapping regions between clusters. However, redundant features and noises brought by increasing data dimensions affect the effectiveness of FKM. To cope with this issue, we propose a Discriminative Projection Fuzzy K-Means with adaptive neighbors (DPFKM) model, which embeds a discriminative subspace into FKM to facilitate learning of global structure and the most discriminative information. Firstly, a novel projection space with uncorrelated constraints are adopted to promote statistical independence among the data in the subspace as well as to enhance the ability of FKM to discern and utilize discriminative information. Secondly, the Frobenius norm is introduced as the regularization term to eliminate discrete solutions, while preserving the fuzziness and enhancing the sparsity of FKM. Finally, we propose a novel optimization method to finetune the model, with a particular focus on adaptive adjustment of the regularization parameter based on the proximity relationship between the samples and clusters. Comprehensive experiments are conducted on multiple data sets, and the results can prove the superiority of the proposed model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523002805",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Discriminative model",
      "Fuzzy logic",
      "Law",
      "Mathematics",
      "Norm (philosophy)",
      "Pattern recognition (psychology)",
      "Political science",
      "Projection (relational algebra)",
      "Regularization (linguistics)",
      "Subspace topology"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Jingyu"
      },
      {
        "surname": "Wang",
        "given_name": "Yidi"
      },
      {
        "surname": "Nie",
        "given_name": "Feiping"
      },
      {
        "surname": "Li",
        "given_name": "Xuelong"
      }
    ]
  },
  {
    "title": "Federated adaptive reweighting for medical image classification",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109880",
    "abstract": "Medical data sharing across institutes is crucial to large-scale multi-center studies and the development of real-world AI applications but suffers from serious privacy issues. A promising solution to address this challenge is federated learning, which typically aggregates a global model from heterogeneous data spread across numerous clients without exchanging data. However, the traditional federated learning algorithm (i.e., FedAvg) merely aggregates the locally distributed models according to the amount of data on each client and lacks the consideration of data heterogeneity. In this paper, we propose a novel Federated Adaptive Reweighting (FedAR) algorithm for medical image classification. FedAR employs a flexible re-weighting scheme that can balance adaptively the contributions of the amount of data and the performance of the local model on each client to the weight of that client. Specifically, we allow the amount of local data to contribute more to the weight of each client in the early training stage and let the performance of the local model play a more important role in the late stage. We have evaluated the proposed FedAR algorithm against the locally trained model, globally trained baseline, and two existing federated learning algorithms on the ISIC2018 dataset and Chest X-ray14 dataset under the settings with a variable number of clients. Our results suggest that FedAR is an effective federated learning algorithm that substantially outperforms existing federated learning approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005782",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Federated learning",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Medicine",
      "Radiology",
      "Scheme (mathematics)",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Ma",
        "given_name": "Benteng"
      },
      {
        "surname": "Feng",
        "given_name": "Yu"
      },
      {
        "surname": "Chen",
        "given_name": "Geng"
      },
      {
        "surname": "Li",
        "given_name": "Changyang"
      },
      {
        "surname": "Xia",
        "given_name": "Yong"
      }
    ]
  },
  {
    "title": "Self-supervised latent feature learning for partial point clouds recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.10.009",
    "abstract": "3D vision perception, especially point clouds classification is fundamental and popular in safety-critical systems such as autonomous driving and robotics automation control. However, their robustness against incomplete partial point clouds in practical scenes is less studied, limiting real-world deployment. In this paper, we propose to improve the robustness and generalization of 3D models on partial point clouds by self-supervised latent feature learning. Unlike those data augmentation methods that generate partial point clouds by geometric transforms in coordinate space (e.g drop local structure or remove global points), we regard the partial data as a transformation in latent feature space. We explicitly learn the perspective transformation of partial point clouds and implicitly learn the occlusion transformation in the latent feature space by self-supervised learning. Different from previous methods that are validated on generated data, we test our method on point clouds completion datasets (e.g. PCN and MVP) which contain both complete and partial point clouds and have been widely used. Extensive experiments show that our proposed method consistently improves the robustness of state-of-the-art methods on the partial point clouds dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523002817",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Gene",
      "Pattern recognition (psychology)",
      "Point cloud",
      "Robustness (evolution)",
      "Transformation (genetics)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Ziyu"
      },
      {
        "surname": "Da",
        "given_name": "Feipeng"
      }
    ]
  },
  {
    "title": "Semantic Similarity Distance: Towards better text-image consistency metric in text-to-image generation",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109883",
    "abstract": "Generating high-quality images from text remains a challenge in visual-language understanding, with text-image consistency being a major concern. Particularly, the most popular metric R -precision may not accurately reflect the text-image consistency, leading to misleading semantics in generated images. Albeit its significance, designing a better text-image consistency metric surprisingly remains under-explored in the community. In this paper, we make a further step forward to develop a novel CLIP-based metric, Semantic Similarity Distance ( S S D ), which is both theoretically founded from a distributional viewpoint and empirically verified on benchmark datasets. We also introduce Parallel Deep Fusion Generative Adversarial Networks (PDF-GAN), which use two novel components to mitigate inconsistent semantics and bridge the text-image semantic gap. A series of experiments indicate that, under the guidance of S S D , our developed PDF-GAN can induce remarkable enhancements in the consistency between texts and images while preserving acceptable image quality over the CUB and COCO datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005812",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Consistency (knowledge bases)",
      "Economics",
      "Generative grammar",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Image quality",
      "Information retrieval",
      "Metric (unit)",
      "Natural language processing",
      "Operations management",
      "Pattern recognition (psychology)",
      "Programming language",
      "Semantic similarity",
      "Semantics (computer science)",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Tan",
        "given_name": "Zhaorui"
      },
      {
        "surname": "Yang",
        "given_name": "Xi"
      },
      {
        "surname": "Ye",
        "given_name": "Zihan"
      },
      {
        "surname": "Wang",
        "given_name": "Qiufeng"
      },
      {
        "surname": "Yan",
        "given_name": "Yuyao"
      },
      {
        "surname": "Nguyen",
        "given_name": "Anh"
      },
      {
        "surname": "Huang",
        "given_name": "Kaizhu"
      }
    ]
  },
  {
    "title": "EEG-based classification combining Bayesian convolutional neural networks with recurrence plot for motor movement/imagery",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109838",
    "abstract": "Electroencephalogram (EEG)-based Motor imagery (MI) is a key topic in the brain-computer interface (BCI). The EEG-based real execution and motor imagery multi-class classification tasks are also crucial, but only a few kinds of literature research it. In addition, classification accuracy still has room for improvement, and the inter-individual variability problems in BCI applications need to be solved. To address these issues, we developed a novel model (RP-BCNNs) that combines the recurrence plot (RP) and Bayesian Convolutional Neural Networks (BCNNs). First, we employ an RP computation for preprocessed EEG signals of each channel and merge all RPs of all channels into one based on the weighted average method. Then, we feed the RP features into BCNNs to classify 2-class, 3-class, 4-class, and 5-class on real/imaginary movements classification tasks. The results show that the RP-BCNNs model outperforms the state-of-the-art methods, achieving average accuracies of 92.86%, 94.12%, 91.37%, 92.61% for real movements and 94.07%, 93.77%, 90.54%, 91.85% for imaginary movements. Our findings suggest that combining complex network methods with deep learning can improve the classification performance of EEG-based BCI systems (e.g., motor imagery, emotion recognition, and epileptic seizure classification).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005368",
    "keywords": [
      "Artificial intelligence",
      "Brain–computer interface",
      "Computer science",
      "Convolutional neural network",
      "Electroencephalography",
      "Motor imagery",
      "Pattern recognition (psychology)",
      "Psychiatry",
      "Psychology"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Wenqie"
      },
      {
        "surname": "Yan",
        "given_name": "Guanghui"
      },
      {
        "surname": "Chang",
        "given_name": "Wenwen"
      },
      {
        "surname": "Zhang",
        "given_name": "Yuchan"
      },
      {
        "surname": "Yuan",
        "given_name": "Yueting"
      }
    ]
  },
  {
    "title": "Graph clustering network with structure embedding enhanced",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109833",
    "abstract": "Recently, deep clustering utilizing Graph Neural Networks has shown good performance in the graph clustering. However, the structure information of graph was underused in existing deep clustering methods. Particularly, the lack of concern on mining different types structure information simultaneously. To tackle with the problem, this paper proposes a Graph Clustering Network with Structure Embedding Enhanced (GC-SEE) which extracts nodes importance-based and attributes importance-based structure information via a feature attention fusion graph convolution module and a graph attention encoder module respectively. Additionally, it captures different orders-based structure information through multi-scale feature fusion. Finally, a self-supervised learning module has been designed to integrate different types structure information and guide the updates of the GC-SEE. The comprehensive experiments on benchmark datasets commonly used demonstrate the superiority of the GC-SEE. The results showcase the effectiveness of the GC-SEE in exploiting multiple types of structure for deep clustering.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005319",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Clustering coefficient",
      "Computer science",
      "Data mining",
      "Embedding",
      "Graph",
      "Graph embedding",
      "Pattern recognition (psychology)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Ding",
        "given_name": "Shifei"
      },
      {
        "surname": "Wu",
        "given_name": "Benyu"
      },
      {
        "surname": "Xu",
        "given_name": "Xiao"
      },
      {
        "surname": "Guo",
        "given_name": "Lili"
      },
      {
        "surname": "Ding",
        "given_name": "Ling"
      }
    ]
  },
  {
    "title": "Semi-supervised transfer learning with hierarchical self-regularization",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109831",
    "abstract": "Both semi-supervised learning and transfer learning aim at lowering the annotation burden for training models. However, such two tasks are usually studied separately, i.e. most semi-supervised learning algorithms train models from scratch while transfer learning assumes pre-trained models as the initialization. In this work, we focus on a previously-less-concerned setting that further reduces the annotation efforts through incorporating both semi-supervised and transfer learning, where specifically a pre-trained source model is used as the initialization of semi-supervised learning. As those powerful pre-trained models are ubiquitously available nowadays and can considerably benefit various down-streaming tasks, such a setting is relevant to real-world applications yet challenging to design effective algorithms. Aiming at enabling transfer learning under semi-supervised settings, we propose a hierarchical self-regularization mechanism to exploit unlabeled samples more effectively, where a novel self-regularizer has been introduced to incorporate both individual-level and population-level regularization terms. The former term employs self-distillation to regularize learned deep features for each individual sample, and the latter one enforces self-consistency on feature distributions between labeled and unlabeled samples. Samples involved in both regularizers are weighted by an adaptive strategy, where self-regularization effects of both terms are adaptively controlled by the confidence of every sample. To validate our algorithm, exhaustive experiments have been conducted on diverse datasets such as CIFAR-10 for general object recognition, CUB-200-2011/MIT-indoor-67 for fine-grained classification and MURA for medical image classification. Compared with state-of-the-art semi-supervised learning methods including Pseudo Label, Mean Teacher, MixMatch and FixMatch, our algorithm demonstrates two advantages: first of all, the proposed approach adopts a new point of view to tackle problems caused by inadequate supervision and achieves very competitive results; then, it is complementary to these state-of-the-art methods and thus can be combined with them to get additional improvements. Furthermore, our method can also be applied to fully supervised transfer learning and self-supervised learning. We have published our code at https://github.com/SHI-Labs/Semi-Supervised-Transfer-Learning.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005290",
    "keywords": [
      "Annotation",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer security",
      "Exploit",
      "Initialization",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Programming language",
      "Regularization (linguistics)",
      "Semi-supervised learning",
      "Supervised learning",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Xingjian"
      },
      {
        "surname": "Abuduweili",
        "given_name": "Abulikemu"
      },
      {
        "surname": "Shi",
        "given_name": "Humphrey"
      },
      {
        "surname": "Yang",
        "given_name": "Pengkun"
      },
      {
        "surname": "Dou",
        "given_name": "Dejing"
      },
      {
        "surname": "Xiong",
        "given_name": "Haoyi"
      },
      {
        "surname": "Xu",
        "given_name": "Chengzhong"
      }
    ]
  },
  {
    "title": "Hyperspectral image destriping and denoising from a task decomposition view",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109832",
    "abstract": "The generalized mathematical model for HSI denoising or destriping lacks stability and uniqueness properties, failing to accurately portray the distribution and effects of stripes. Solutions following such a model would inevitably result in excessive destriping of strip-free areas, leading to the loss of texture detail. To remedy the above deficiencies, we reformulate the destriping task and introduce a novel solution from the task decomposition view. It is broken down into auxiliary sub-tasks involving stripe mask detection, stripe intensity estimation, and HSI restoration, which greatly reduces the difficulty of solving such an ill-posed problem. Based on this, we adopt a sequential multi-task learning framework and propose a stripes location-dependent restoration network, termed SLDR, which integrates the distribution and intensity features of stripes to achieve accurate destriping and high-fidelity restoration. Furthermore, we design a stripe attribute-aware estimator and a weighted total variation loss function to capture the unique properties of stripes and adaptively adjust the restoration weights of striped and non-striped regions. Extensive evaluation and comprehensive ablation studies on synthetic and practical scenes show the effectiveness and superiority of our model and architecture.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005307",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer vision",
      "Decomposition",
      "Ecology",
      "Economics",
      "Estimator",
      "Evolutionary biology",
      "Fidelity",
      "Function (biology)",
      "Image (mathematics)",
      "Image processing",
      "Image restoration",
      "Machine learning",
      "Management",
      "Mathematics",
      "Noise reduction",
      "Pattern recognition (psychology)",
      "Stability (learning theory)",
      "Statistics",
      "Task (project management)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Pan",
        "given_name": "Erting"
      },
      {
        "surname": "Ma",
        "given_name": "Yong"
      },
      {
        "surname": "Mei",
        "given_name": "Xiaoguang"
      },
      {
        "surname": "Huang",
        "given_name": "Jun"
      },
      {
        "surname": "Chen",
        "given_name": "Qihai"
      },
      {
        "surname": "Ma",
        "given_name": "Jiayi"
      }
    ]
  },
  {
    "title": "DTEC: Decision tree-based evidential clustering for interpretable partition of uncertain data",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109846",
    "abstract": "Recently, the evidential clustering has been developed as a promising clustering framework for uncertain data, which generalizes those hard, fuzzy, possibilistic and rough clustering. However, the resulting cluster assignments are less interpretable in terms of human cognition, which limits its applications in those security, privacy or ethic related fields. In this study, the unsupervised decision tree model is introduced into the evidential clustering framework to improve the interpretability of the evidential partition. A Decision Tree-based Evidential Clustering (DTEC) algorithm is developed to build an unsupervised evidential decision tree, which uses the paths from the root node to leaf nodes to achieve the interpretability of each cluster. The proposed algorithm is composed of three procedures, i.e., cutting-point selection, node evidential splitting, and cluster adjustment, in which the first two procedures are carried out iteratively to build a preliminary unsupervised decision tree and the last procedure is designed to adjust the preliminary decision tree if the number of clusters is available. Both synthetic and real datasets are used to evaluate the performance of the proposed algorithm, and the experimental results demonstrate the good performance of the proposal compared with some representative fuzzy, evidential or decision tree-based clustering algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005447",
    "keywords": [
      "Artificial intelligence",
      "Business decision mapping",
      "CURE data clustering algorithm",
      "Cluster analysis",
      "Computer science",
      "Conceptual clustering",
      "Data mining",
      "Decision support system",
      "Decision tree",
      "Decision tree learning",
      "Evidential reasoning approach",
      "Fuzzy clustering",
      "ID3 algorithm",
      "Incremental decision tree",
      "Interpretability",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Jiao",
        "given_name": "Lianmeng"
      },
      {
        "surname": "Yang",
        "given_name": "Haoyu"
      },
      {
        "surname": "Wang",
        "given_name": "Feng"
      },
      {
        "surname": "Liu",
        "given_name": "Zhun-ga"
      },
      {
        "surname": "Pan",
        "given_name": "Quan"
      }
    ]
  },
  {
    "title": "Counting-based visual question answering with serial cascaded attention deep learning",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109850",
    "abstract": "The counting-based questions play a major part in Visual Question Answering (VQA), the most challenging factor is counting the different objects present in the images. Recently more attention is paid to design a model of count-aided VQA. Based on the questions, the VQA system responds with appropriate answers. Yet, the complex questions are necessitating in the system with answers. The earlier models are still facing the challenging problems of counting the various objects within the images as the models become futile to select the features and lack fine-grained representation. In order to sustain the image representation, this paper proposes a new model for VQA using the heuristic approach of serial cascaded deep learning methods. Initially, the standard data regarding images and text data are gathered and fed to the pre-processing process. Consequently, the feature extraction is done on both the image and the text data. Here, the deep features from images are taken using Visual Geometry Group 16 (VGG16) and the text features are extracted using Text Convolutional Neural Network (TCNN). Then, the optimal weighted fused features are obtained, where the weights used for getting the necessary features are tuned via the Improved Tuna Swarm Optimization (ITSO) algorithm. Finally, the counting answers are retrieved based on the given queries, which is carried out via Serial Cascaded Recurrent Neural Network with Attention Mechanism-based Long Short-Term Memory (SCRAM-LSTM). The performance is examined with divergent metrics compared with conventional models. Hence, the findings reveal that it offers superior performance in estimating the appropriate answers. Therefore, the proposed work is widely used for such potential applications as helping blind or visually impaired people to get information, integrating with image retrieval systems, and also for search engines. Especially, it is utilized for the vision and language systems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005484",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Feature (linguistics)",
      "Heuristic",
      "Image (mathematics)",
      "Law",
      "Linguistics",
      "Machine learning",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Process (computing)",
      "Question answering",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "MeshuWelde",
        "given_name": "Tesfayee"
      },
      {
        "surname": "Liao",
        "given_name": "Lejian"
      }
    ]
  },
  {
    "title": "Few pixels attacks with generative model",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109849",
    "abstract": "Adversarial attacks have attracted much attention in recent years, and a number of works have demonstrated the effectiveness of attacks on the entire image at perturbation generation. However, in practice, specially designed perturbation of the entire image is impractical. Some work has crafted adversarial samples with a few scrambled pixels by advanced search, e.g., SparseFool, OnePixel, etc., but they take more time to find such pixels that can be perturbed. Therefore, to construct the adversarial samples with few pixels perturbed in an end-to-end way, we propose a new framework, in which a dual-decoder VAE for perturbations finding is designed. To make adversarial learning more effective, we proposed a new version of the adversarial loss by considering the generalization. To evaluate the sophistication of the proposed framework, we compared it with more than a dozen existing related attack methods. The effectiveness and efficiency of the proposed framework are verified from the extensive experimental results. The validity of the model structure is also validated by the ablation study.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005472",
    "keywords": [
      "Adversarial system",
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Generalization",
      "Generative grammar",
      "Image (mathematics)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pixel",
      "Social science",
      "Sociology",
      "Sophistication"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Yang"
      },
      {
        "surname": "Pan",
        "given_name": "Quan"
      },
      {
        "surname": "Feng",
        "given_name": "Zhaowen"
      },
      {
        "surname": "Cambria",
        "given_name": "Erik"
      }
    ]
  },
  {
    "title": "AdvMask: A sparse adversarial attack-based data augmentation method for image classification",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109847",
    "abstract": "Data augmentation has been an essential technique for improving the generalization ability of deep neural networks in image classification tasks. However, intensive changes in appearance and different degrees of occlusion in images are the key factors that severely affect the generalization ability of image classification models. Therefore, in order to enhance the generalization performance and robustness of deep models, data augmentation approaches by providing models with more diverse training data in various scenarios are widely applied. Although many existing data augmentation methods simulate occlusion in the augmented images to enhance the generalization of models, these methods randomly delete some areas in images without considering the semantic information of images. In this work, we propose a novel data augmentation method named AdvMask for image classification based on sparse adversarial attack techniques. AdvMask first identifies the key points that have the greatest influence on the classification results via a proposed end-to-end sparse adversarial attack module. During the data augmentation process, AdvMask efficiently generates diverse augmented data with structured occlusions based on the key points. By doing so, AdvMask can force deep models to seek other relevant content while the most discriminative content is hidden. Extensive experimental results on various benchmark datasets and deep models demonstrate that our proposed method can effectively improve the generalization performance of deep models and significantly outperforms previous data augmentation methods. Code for reproducing our results is available at https://github.com/Jackbrocp/AdvMask.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005459",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer security",
      "Contextual image classification",
      "Data mining",
      "Deep learning",
      "Discriminative model",
      "Gene",
      "Generalization",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Key (lock)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Suorong"
      },
      {
        "surname": "Li",
        "given_name": "Jinqiao"
      },
      {
        "surname": "Zhang",
        "given_name": "Tianyue"
      },
      {
        "surname": "Zhao",
        "given_name": "Jian"
      },
      {
        "surname": "Shen",
        "given_name": "Furao"
      }
    ]
  },
  {
    "title": "Few-shot forgery detection via Guided Adversarial Interpolation",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109863",
    "abstract": "The increase in face manipulation models has led to a critical issue in society—the synthesis of realistic visual media. With the emergence of new forgery approaches at an unprecedented rate, existing forgery detection methods suffer from significant performance drops when applied to unseen novel forgery approaches. In this work, we address the few-shot forgery detection problem by (1) designing a comprehensive benchmark based on coverage analysis among various forgery approaches, and (2) proposing Guided Adversarial Interpolation (GAI). Our key insight is that there exist transferable distribution characteristics between majority and minority forgery classes. 1 1 Majority class: class with abundant samples; minority class: class with scarce samples. Specifically, we enhance the discriminative ability against novel forgery approaches via adversarially interpolating the forgery artifacts of the minority samples to the majority samples under the guidance of a teacher network. Unlike the standard re-balancing method which usually results in over-fitting to minority classes, our method simultaneously takes account of the diversity of majority information as well as the significance of minority information. Extensive experiments demonstrate that our GAI achieves state-of-the-art performances on the established few-shot forgery detection benchmark. Notably, our method is also validated to be robust to choices of majority and minority forgery approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005617",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Chemistry",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Discriminative model",
      "Face (sociological concept)",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Interpolation (computer graphics)",
      "Key (lock)",
      "Machine learning",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Shot (pellet)",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Qiu",
        "given_name": "Haonan"
      },
      {
        "surname": "Chen",
        "given_name": "Siyu"
      },
      {
        "surname": "Gan",
        "given_name": "Bei"
      },
      {
        "surname": "Wang",
        "given_name": "Kun"
      },
      {
        "surname": "Shi",
        "given_name": "Huafeng"
      },
      {
        "surname": "Shao",
        "given_name": "Jing"
      },
      {
        "surname": "Liu",
        "given_name": "Ziwei"
      }
    ]
  },
  {
    "title": "Multi-label learning based on instance correlation and feature redundancy",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.10.029",
    "abstract": "Multi-label classification is a machine-learning task that simultaneously processes instances associated with multiple labels. Label-specific feature learning selects each label's most discriminative feature subset, effectively reducing the feature dimension and improving the classification performance. However, most methods only consider label correlation, ignoring the correlation between instances and feature redundancy. To solve this problem, a multi-label classification method based on instance correlation and feature redundancy is proposed. The proposed method merges instance correlation by updating the data set and removes redundant features by calculating mutual information. By jointly considering label correlation, instance correlation, and feature redundancy, our method promotes effective multi-label feature selection. The experimental results on ten data sets demonstrate the effectiveness of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523003100",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Correlation",
      "Discriminative model",
      "Feature (linguistics)",
      "Feature selection",
      "Geometry",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Minimum redundancy feature selection",
      "Multi-label classification",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Redundancy (engineering)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Yong"
      },
      {
        "surname": "Jiang",
        "given_name": "Yuqing"
      },
      {
        "surname": "Zhang",
        "given_name": "Qi"
      },
      {
        "surname": "Liu",
        "given_name": "Da"
      }
    ]
  },
  {
    "title": "Improving the adversarial robustness of quantized neural networks via exploiting the feature diversity",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.10.024",
    "abstract": "Quantized neural networks (QNNs) have become one of the most prevalent approaches in deep learning model compression due to their computational and storage efficiency. However, there is a lack of research specialized in the adversarial robustness of QNNs, which is important for applications in security-critical domains. Existing defenses focus on conventional full-precision networks, which can result in behavioral disparities and degrade the expected performance when directly transferred to QNNs. A novel defensive strategy promotes feature diversity through an orthogonal constraint, which can synergize well with quantization. Inspired by this intuition, we propose an orthogonal regularization with quantization to improve the adversarial robustness of QNNs in this paper. Moreover, we observe that quantization serves as an implicit regularization and is able to alleviate orthogonal degeneration. The proposed orthogonal regularization with quantization is validated on several typical network architectures and benchmark datasets. The results demonstrate that the proposed method can notably enhance adversarial robustness against both white-box and black-box attacks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523002969",
    "keywords": [
      "Adversarial system",
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Deep learning",
      "Deep neural networks",
      "Gene",
      "Machine learning",
      "Quantization (signal processing)",
      "Regularization (linguistics)",
      "Robustness (evolution)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Chu",
        "given_name": "Tianshu"
      },
      {
        "surname": "Fang",
        "given_name": "Kun"
      },
      {
        "surname": "Yang",
        "given_name": "Jie"
      },
      {
        "surname": "Huang",
        "given_name": "Xiaolin"
      }
    ]
  },
  {
    "title": "A dependency-based hybrid deep learning framework for target-dependent sentiment classification",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.10.026",
    "abstract": "One of the main challenges in target-dependent sentiment classification (TDSC) is dealing with sentences that contain multiple targets with varying polarities. Traditional sentiment analysis has shown the effectiveness of language characteristics. Therefore, we propose a method to extract target semantic-related tokens from sentences in order to simplify the sentiment classification task. To achieve this, we establish six grammatical principles that utilize grammatical knowledge to filter the relevant descriptions of targets. Since a target is typically a noun and acts as a subject, we summarize the six rules to extract the contexts contained in the objects and subordinate clauses. We use dependency parsing to analyze the grammatical relations between the target and its context. We design a data pre-processing method called Text Filtering (TF) to automate this procedure. After executing the TF algorithm, we pass the target-related words to a simple classifier to predict their sentiment polarities. Rather than feeding these features directly to a network and letting it learn features on its own, our approach employs dependency relations to extract context linked to the target. This provides the network with meaningful and representative features, resulting in superior results. We conduct ablation studies to investigate the effectiveness of the proposed TF algorithm. In the restaurant hard dataset, our approach improves accuracy by 13.76% and macro-F1 by 14.65% compared to a CNN-based method where TF is not implemented.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523002994",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Classifier (UML)",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Dependency (UML)",
      "Dependency grammar",
      "Filter (signal processing)",
      "Machine learning",
      "Natural language processing",
      "Paleontology",
      "Parsing",
      "Sentiment analysis"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Jingyi"
      },
      {
        "surname": "Li",
        "given_name": "Sheng"
      }
    ]
  },
  {
    "title": "A weakly supervised inpainting-based learning method for lung CT image segmentation",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109861",
    "abstract": "Recently, various fully supervised learning methods are successfully applied for lung CT image segmentation. However, pixel-wise annotations are extremely expert-demanding and labor-intensive, but the performance of unsupervised learning methods are failed to meet the demands of practical applications. To achieve a reasonable trade-off between the performance and label dependency, a novel weakly supervised inpainting-based learning method is introduced, in which only bounding box labels are required for accurate segmentation. Specifically, lesion regions are first detected by an object detection network, then we crop them out of the input image and recover the missing holes to normal regions using a progressive CT inpainting network (PCIN). Finally, a post-processing method is designed to get the accurate segmentation mask from the difference image of input and recovered images. In addition, real information (i.e., number, location and size) of the bounding boxes of lesions from the dataset guides us to make the training dataset for PCIN. We apply a multi-scale supervised strategy to train PCIN for a progressive and stable inpainting. Moreover, to remove the visual artifacts resulted from the invalid features of missing holes, an initial patch generation network (IPGN) is proposed for holes initialization with generated pseudo healthy image patches. Experiments on the public COVID-19 dataset demonstrate that PCIN is outstanding in lung CT images inpainting, and the performance of our proposed weakly supervised method is comparable to fully supervised methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005599",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Bounding overwatch",
      "Computer science",
      "Computer vision",
      "Image (mathematics)",
      "Image segmentation",
      "Initialization",
      "Inpainting",
      "Minimum bounding box",
      "Pattern recognition (psychology)",
      "Programming language",
      "Segmentation",
      "Supervised learning"
    ],
    "authors": [
      {
        "surname": "Lu",
        "given_name": "Fangfang"
      },
      {
        "surname": "Zhang",
        "given_name": "Zhihao"
      },
      {
        "surname": "Liu",
        "given_name": "Tianxiang"
      },
      {
        "surname": "Tang",
        "given_name": "Chi"
      },
      {
        "surname": "Bai",
        "given_name": "Hualin"
      },
      {
        "surname": "Zhai",
        "given_name": "Guangtao"
      },
      {
        "surname": "Chen",
        "given_name": "Jingjing"
      },
      {
        "surname": "Wu",
        "given_name": "Xiaoxin"
      }
    ]
  },
  {
    "title": "Self-imitation guided goal-conditioned reinforcement learning",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109845",
    "abstract": "Goal-conditioned reinforcement learning (GCRL) aims to control agents to reach desired goals, which poses a significant challenge due to task-specific variations in configurations. However, current GCRL methods suffer from limitations in sample efficiency and the need for substantial training data. While existing self-imitation-based GCRL approaches can improve sample efficiency, their scalability to large-scale tasks is limited. In this paper, we propose integrating self-imitation learning with goal-conditioned RL methods into a compatible and reasonable framework. Specifically, we introduce a novel target action value function to aggregate self-imitation learning and goal-conditioned reinforcement learning. The designed target value effectively combines these two policy training mechanisms to accomplish specific tasks. Moreover, we theoretically demonstrate that our approach can learn a superior policy compared to both self-imitation learning and goal-conditioned reinforcement learning. Additionally, experimental results showcase the stability and effectiveness of our method compared to existing approaches in various challenging robotic control tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005435",
    "keywords": [
      "Action (physics)",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Control (management)",
      "Engineering",
      "Evolutionary biology",
      "Function (biology)",
      "Generalization",
      "Imitation",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Physics",
      "Psychology",
      "Quantum mechanics",
      "Reinforcement",
      "Reinforcement learning",
      "Social psychology",
      "Stability (learning theory)",
      "Systems engineering",
      "Task (project management)",
      "Temporal difference learning"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Yao"
      },
      {
        "surname": "Wang",
        "given_name": "YuHui"
      },
      {
        "surname": "Tan",
        "given_name": "XiaoYang"
      }
    ]
  },
  {
    "title": "On complementing unsupervised learning with uncertainty quantification",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.10.023",
    "abstract": "Pseudo-label-based semi-supervised learning (SSL) recently has achieved significant success in unlabeled data utilization. Recent success on pseudo-label-based SSL methods crucially hinges on thresholded pseudo-labeling and consistency regularization for the unlabeled data. However, most of the existing methods do not measure and incorporate the uncertainties due to the noisy pseudo-labels or out-of-distribution unlabeled samples. Therefore, the model’s discernment becomes noisier in real-life applications that involve a substantial amount of out-of-distribution unannotated data. This leads to slow convergence in the training process and poor generalization performance. Inspired by the recent developments in SSL, our goal in this paper is to propose a novel unsupervised uncertainty-aware objective and threshold-mediated pseudo-labeling scheme that rely on uncertainty quantification from aleatoric and epistemic sources. By incorporating recent techniques in SSL, our proposed uncertainty-aware framework can mitigate the issue of confirmation bias and the impact of noisy pseudo-labels, resulting in improved training efficiency and enhanced generalization performance within the SSL domain. Despite its simplicity and computational efficiency, our approach demonstrates improved performance compared to state-of-the-art SSL methods on challenging datasets, such as CIFAR-100 and the real-world dataset Semi-iNat.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523002957",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Consistency (knowledge bases)",
      "Data mining",
      "Generalization",
      "Labeled data",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Regularization (linguistics)",
      "Unsupervised learning"
    ],
    "authors": [
      {
        "surname": "Kazemi",
        "given_name": "Ehsan"
      },
      {
        "surname": "Taherkhani",
        "given_name": "Fariborz"
      },
      {
        "surname": "Wang",
        "given_name": "Liqiang"
      }
    ]
  },
  {
    "title": "ANAS: Asymptotic NAS for large-scale proxyless search and multi-task transfer learning",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109821",
    "abstract": "Neural Architecture Search (NAS) is an emerging solution to design a lightweight network for researchers to obtain a trade-off between accuracy and speed, releasing researchers from tedious repeated trials. However, the main shortcoming of NAS is its high and unstable memory consumption of the search work, especially for large-scale tasks. In this study, the proposed Asymptotic Neural Architecture Search network (ANAS) achieved a proxyless search for large-scale tasks with economic and stable memory consumption. Instead of proxy search like other NAS algorithms, ANAS achieved the large-scale proxyless search that directly learns deep neural network architecture for target task. ANAS reduced the peak value of memory consumption by an asymptotic method, and kept the memory consumption stable by the linkage change of a series of key indexes. A pruning operation and efficient candidate operations decreased the total memory consumption. Finally, ANAS achieved a good trade-off between accuracy and speed for classification tasks on CIFAR-10, CIFAR-100, and ImageNet datasets. Besides, except for the classification task, it achieved excellent multi-task transfer learning ability for implementing the segmentation task on CamVid and Cityscapes. ANAS reached 22.8% test errs with 5 M parameter on ImageNet, and 72.9 mIoU (mean Intersection over Union) with 119.9 FPS (Frames Per Second) on Cityscapes dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005198",
    "keywords": [
      "Aerospace engineering",
      "Agronomy",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer science",
      "Deep learning",
      "Engineering",
      "Intersection (aeronautics)",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Pruning",
      "Segmentation",
      "Systems engineering",
      "Task (project management)",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Xie",
        "given_name": "Bangquan"
      },
      {
        "surname": "Yang",
        "given_name": "Zongming"
      },
      {
        "surname": "Yang",
        "given_name": "Liang"
      },
      {
        "surname": "Luo",
        "given_name": "Ruifa"
      },
      {
        "surname": "Lu",
        "given_name": "Jun"
      },
      {
        "surname": "Wei",
        "given_name": "Ailin"
      },
      {
        "surname": "Weng",
        "given_name": "Xiaoxiong"
      },
      {
        "surname": "Li",
        "given_name": "Bing"
      }
    ]
  },
  {
    "title": "Robust table structure recognition with dynamic queries enhanced detection transformer",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109817",
    "abstract": "We present a new table structure recognition (TSR) approach, called TSRFormer, to robustly recognize the structures of complex tables with geometrical distortions from various table images. Unlike previous methods, we formulate table separation line prediction as a line regression problem instead of an image segmentation problem and propose a new two-stage dynamic queries enhanced DETR based separation line regression approach, named DQ-DETR, to predict separation lines from table images directly. Compared to Vallina DETR, we propose three improvements in DQ-DETR to make the two-stage DETR framework work efficiently and effectively for the separation line prediction task: 1) A new query design, named Dynamic Query, to decouple single line query into separable point queries which could intuitively improve the localization accuracy for regression tasks; 2) A dynamic queries based progressive line regression approach to progressively regressing points on the line which further enhances localization accuracy for distorted tables; 3) A prior-enhanced matching strategy to solve the slow convergence issue of DETR. After separation line prediction, a simple relation network based cell merging module is used to recover spanning cells. With these new techniques, our TSRFormer achieves state-of-the-art performance on several benchmark datasets, including SciTSR, PubTabNet, WTW, FinTabNet, and cTDaR TrackB2-Modern. Furthermore, we have validated the robustness and high localization accuracy of our approach to tables with complex structures, borderless cells, large blank spaces, empty or spanning cells as well as distorted or even curved shapes on a more challenging real-world in-house dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005150",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Data mining",
      "Gene",
      "Geodesy",
      "Geography",
      "Pattern recognition (psychology)",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Jiawei"
      },
      {
        "surname": "Lin",
        "given_name": "Weihong"
      },
      {
        "surname": "Ma",
        "given_name": "Chixiang"
      },
      {
        "surname": "Li",
        "given_name": "Mingze"
      },
      {
        "surname": "Sun",
        "given_name": "Zheng"
      },
      {
        "surname": "Sun",
        "given_name": "Lei"
      },
      {
        "surname": "Huo",
        "given_name": "Qiang"
      }
    ]
  },
  {
    "title": "Implicit space pose consistent transfer network for deep face verification",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.10.017",
    "abstract": "Unconstrained face recognition has achieved state-of-the-art performance with the help of the powerful representation capabilities of deep convolutional neural networks. However, face recognition always suffers from other intra-class imbalances due to the influence of pose variations present in the real world, which significantly degrades the performance, especially for tilted face recognition under surveillance videos, which is rarely studied. Tilted face recognition is more challenging than frontal and profile face recognition due to the effect of self-obscuration. To this end, we consider promoting the frontalization of tilted faces in the implicit space, while preserving identity information. In this paper, we propose an implicit spatial pose consistent transfer network (PCTN) that exploits the prior guidance of pose information to enhance the model’s ability to perceive pose. Specifically, we first develop the multi-path attention block (MPAB), which is used as the main block for feature embedding in deep networks. Such a design is employed to enhance the relevance of multi-layer convolutional features and provide a powerful foundation for the deep asymptotic transfer of feature frontalization. Then, the pose consistent transfer module (PCTM) combines pose labels and deep embedding features to learn robust discriminative information. PCTM consists of two parts: the pose guidance block (PGB) explores the implicit nonlinear feature mapping from pose faces to frontal faces in feature space, and the attention compensation module (ACM) aims to exploit the complementarity of dense and sparse features, enhancing pose-related features and suppressing pose-unrelated features in terms of both spatial-wise and channel-wise. Furthermore, we investigate the performance comparison on datasets with multiple pose variations without performance degradation. Extensive experimental results on multiple benchmark datasets demonstrate the effectiveness of our proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523002891",
    "keywords": [
      "3D pose estimation",
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Convolutional neural network",
      "Deep learning",
      "Discriminative model",
      "Embedding",
      "Exploit",
      "Face (sociological concept)",
      "Facial recognition system",
      "Feature (linguistics)",
      "Feature learning",
      "Feature vector",
      "Geometry",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pose",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Zeng",
        "given_name": "Kangli"
      },
      {
        "surname": "Wang",
        "given_name": "Zhongyuan"
      },
      {
        "surname": "Lu",
        "given_name": "Tao"
      },
      {
        "surname": "Chen",
        "given_name": "Jianyu"
      },
      {
        "surname": "Han",
        "given_name": "Zhen"
      }
    ]
  },
  {
    "title": "PEAR: Positional-encoded Asynchronous Autoregression for satellite anomaly detection",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.10.007",
    "abstract": "This paper proposes a Positional-Encoded Asynchronous AutoRegression (PEAR) method for satellite anomaly detection. We empirically observe that a single classification model can hardly detect unknown anomalous situations and neglect the Markov nature of temporal satellite data. To address this, we adopt an autoregressive model to deal with the prediction of unknown anomaly for satellite data. We further propose a non-uniform temporal encoding method for asynchronous data and a median filtering method for more accurate detection. To reduce the effect of outliers, we employ an adaptive threshold selection method to achieve a more robust classification boundary. We test the proposed method on the time series prediction models including LSTM and transformers and we further employ the positional encoding strategy to improve the modeling capabilities of the Transformer model on high-frequency information. Experiments on real satellite data demonstrate that the proposed PEAR method outperforms the baseline method by 55.79%.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523002799",
    "keywords": [
      "Anomaly detection",
      "Artificial intelligence",
      "Asynchronous communication",
      "Autoregressive model",
      "Computer network",
      "Computer science",
      "Data mining",
      "Machine learning",
      "Mathematics",
      "Outlier",
      "Pattern recognition (psychology)",
      "Statistics",
      "Time series"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Peng"
      },
      {
        "surname": "Chen",
        "given_name": "Lei"
      },
      {
        "surname": "Zhang",
        "given_name": "Haopeng"
      },
      {
        "surname": "Zhang",
        "given_name": "Yunxiang"
      },
      {
        "surname": "Liu",
        "given_name": "Chao"
      },
      {
        "surname": "Li",
        "given_name": "Cheng"
      },
      {
        "surname": "Wang",
        "given_name": "Zhihui"
      }
    ]
  },
  {
    "title": "Improving point cloud classification and segmentation via parametric veronese mapping",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109784",
    "abstract": "Deep learning based 3D point cloud classification and segmentation has achieved remarkable success. Existing methods are usually implemented in the original space with 3D coordinates as inputs. However, we find that point networks taking only information of first-order coordinates hardly learn geometric features of higher order, such as point cloud normals or poses. In this study, we propose to map the input point clouds into a non-linear space to facilitate networks learning and leveraging high-order features. Firstly, we design the Parametric Veronese Mapping (PVM) function which automatically learns to map point clouds into a non-linear space. As a result, the mapped point clouds are enriched with high-order elements and maintain the basic point set properties as in the original 3D space. We can then exploit existing networks to learn high-order features from mapped point clouds. Secondly, we contribute a two-stage transformation learning module that modifies the previous one-stage module to better leverage high-order features for aligning point clouds in the projective space. Finally, an interaction module is designed to learn more discriminative features by aggregating information from both the original and projective space. Extensive experiments demonstrate that our method successfully improves the ability of most existing networks to learn high-order features and thus contributing to more accurate classification and segmentation. Moreover, the resulting models show stronger robustness to affine transformations and real-world perturbations.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032300482X",
    "keywords": [
      "Affine transformation",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Discriminative model",
      "Gene",
      "Geometry",
      "Leverage (statistics)",
      "Mathematics",
      "Parametric statistics",
      "Pattern recognition (psychology)",
      "Point cloud",
      "Robustness (evolution)",
      "Segmentation",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Ruibin"
      },
      {
        "surname": "Ying",
        "given_name": "Xianghua"
      },
      {
        "surname": "Xing",
        "given_name": "Bowei"
      },
      {
        "surname": "Tong",
        "given_name": "Xin"
      },
      {
        "surname": "Chen",
        "given_name": "Taiyan"
      },
      {
        "surname": "Yang",
        "given_name": "Jinfa"
      },
      {
        "surname": "Shi",
        "given_name": "Yongjie"
      }
    ]
  },
  {
    "title": "Memory efficient data-free distillation for continual learning",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109875",
    "abstract": "Deep neural networks suffer from the catastrophic forgetting phenomenon when trained on sequential tasks in continual learning, especially when data from previous tasks are unavailable. To mitigate catastrophic forgetting, various methods either store data from previous tasks, which may raise privacy concerns, or require large memory storage. Particularly, the distillation-based methods mitigate catastrophic forgetting by using proxy datasets. However, proxy datasets may not match the distributions of the original datasets of previous tasks. To address these problems in a setting where the full training data of previous tasks are unavailable and memory resources are limited, we propose a novel data-free distillation method. Our method encodes knowledge of previous tasks into network parameter gradients by Taylor expansion, deducing a regularizer relying on gradients in network training loss. To improve memory efficiency, we design an approach to compressing the gradients in the regularizer. Moreover, we theoretically analyze the approximation error of our method. Experimental results on multiple datasets demonstrate that our proposed method outperforms the existing approaches in continual learning.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005733",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Chemistry",
      "Computer science",
      "Data mining",
      "Distillation",
      "Forgetting",
      "Linguistics",
      "Machine learning",
      "Organic chemistry",
      "Philosophy",
      "Proxy (statistics)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Xiaorong"
      },
      {
        "surname": "Wang",
        "given_name": "Shipeng"
      },
      {
        "surname": "Sun",
        "given_name": "Jian"
      },
      {
        "surname": "Xu",
        "given_name": "Zongben"
      }
    ]
  },
  {
    "title": "Dense-and-Similar Object detection in aerial images",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.10.028",
    "abstract": "The general object detection performance has been improving significantly due to the prosperity of deep learning. When applied to aerial images, these algorithms perform poorly. There are, as we summarized, two practical reasons: (1) photographed from far above, objects are commonly small in size, dense in clusters, but sparsely scattered in the whole image; (2) some classes are naturally similar in appearance, not to mention the insufficiency of distinguishable visual features captured by cameras due to factors such as bad weather, various angles of view and long distances, etc. To address the two issues, we propose a dense-and-similar object detector with four key components: a coarse detector, an adaptive clustering procedure, a similar-class classifier, and a fine detector. At first, we input the original image into both the coarse and fine detectors. Then we cluster and patch the coarsely detected results adaptively to form a foreground region image. We feed the patched image into the coarse detector again and use the similar-class classifier to re-identify the labels of detected bounding boxes. At last, we put re-identified detections and outputs of the fine detector together, and obtain the final detections after non-maximal suppression. The approach proposed in this study achieved excellent results on both the VisDrone 2019 and DIOR datasets through experimental validation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523003008",
    "keywords": [
      "Artificial intelligence",
      "Bounding overwatch",
      "Classifier (UML)",
      "Cluster analysis",
      "Computer science",
      "Computer vision",
      "Detector",
      "Image (mathematics)",
      "Minimum bounding box",
      "Object detection",
      "Pattern recognition (psychology)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Xiaobin"
      },
      {
        "surname": "Yan",
        "given_name": "Ye"
      },
      {
        "surname": "Sun",
        "given_name": "Haohui"
      },
      {
        "surname": "Zhu",
        "given_name": "Dekang"
      }
    ]
  },
  {
    "title": "Heterogeneous network representation learning based on role feature extraction",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109870",
    "abstract": "Since most of the real-world networks are heterogeneous, existing methods cannot characterize the roles of nodes in heterogeneous networks. The neighborhood structure of nodes in heterogeneous networks largely determines the node roles, and the basic statistical features of nodes describe the topology of nodes to some extent, so extracting structural features from the adjacency matrix of networks is crucial for role-oriented network representation learning(structural equivalence). Therefore, in this paper, we propose a heterogeneous network representation learning model based on role feature extraction, called HRFE(Heterogeneous Network Representation Learning for Role Feature Extraction). Firstly, we perform feature extraction for each node in the heterogeneous network to obtain a high-dimensional feature matrix, then perform role discovery using non-negative matrix decomposition techniques to obtain a role-based node representation, and finally verify the effectiveness of the model HRFE through experiments on a large number of real datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032300568X",
    "keywords": [
      "Adjacency matrix",
      "Artificial intelligence",
      "Chemistry",
      "Complex network",
      "Computer science",
      "Data mining",
      "Engineering",
      "Feature (linguistics)",
      "Feature extraction",
      "Feature learning",
      "Graph",
      "Group (periodic table)",
      "Heterogeneous network",
      "Law",
      "Linguistics",
      "Machine learning",
      "Matrix representation",
      "Node (physics)",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Structural engineering",
      "Telecommunications",
      "Theoretical computer science",
      "Wireless",
      "Wireless network",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Sun",
        "given_name": "Yueheng"
      },
      {
        "surname": "Jia",
        "given_name": "Mengyu"
      },
      {
        "surname": "Liu",
        "given_name": "Chang"
      },
      {
        "surname": "Shao",
        "given_name": "Minglai"
      }
    ]
  },
  {
    "title": "BLoG: Bootstrapped graph representation learning with local and global regularization for recommendation",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109874",
    "abstract": "With the explosive growth of online information, the significant application value of recommender systems has received considerable attention. Since user–item interactions can naturally fit into graph structure data, graph neural networks (GNNs), by virtue of their strong ability in graph representation learning, have become the new state-of-the-art approach to recommender systems. Recently, GNN-based contrastive self-supervised learning (SSL) methods have received careful attention due to their superiority over graph-based recommendation under the typical supervised learning paradigm. However, to achieve state-of-the-art performance, GNN-based recommendation with SSL often needs a huge amount of negative examples and the model’s performance is heavily dependent on complex data augmentations. Also, the information interaction among various augmented views is often performed under a single perspective (e.g., structure/feature space or node/graph level). In this paper, we propose a novel bootstrapped graph representation learning with local and global regularization for recommendation, i.e., BLoG, which constructs positive/negative pairs based on the aggregated node features by referring to two alternate views of the original user–item graph structure. In particular, BLoG learns user–item representations by encoding two augmented versions of a user–item bipartite graph using two separate encoders: an online encoder and a target encoder. To facilitate the information interaction between these two distinct graph encoders, we introduce local and global regularization for recommendation, where a graph structural contrastive loss and a node-level semantic loss are defined for local regularization while a graph-level contrastive loss is used for global regularization. An alternative optimization approach is used to train the online encoder and the target encoder. Experimental studies on three benchmark datasets demonstrate that BLoG achieves better recommendation accuracy than several existing baselines.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005721",
    "keywords": [
      "Artificial intelligence",
      "Bipartite graph",
      "Computer science",
      "Feature learning",
      "Graph",
      "Machine learning",
      "Recommender system",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Ming"
      },
      {
        "surname": "Zhang",
        "given_name": "Lin"
      },
      {
        "surname": "Cui",
        "given_name": "Lixin"
      },
      {
        "surname": "Bai",
        "given_name": "Lu"
      },
      {
        "surname": "Li",
        "given_name": "Zhao"
      },
      {
        "surname": "Wu",
        "given_name": "Xindong"
      }
    ]
  },
  {
    "title": "Learning cross-task relations for panoptic driving perception",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.10.025",
    "abstract": "Accurately understanding traffic surroundings is crucial for various autonomous and assisted driving scenarios. The visual perception system must capture the entire scene, including vehicle positions, road conditions, and lane configurations. While existing methods co-train models for these tasks simultaneously, they overlook the topological relationships among roads, lanes, and traffic objects in images. In this paper, we propose leveraging inherent structural relations among these tasks to enhance precise panoptic driving perception. We introduce a cross-task relation mining (CRM) method to achieve this goal. Self-attention mechanisms are used to blend key spatial features within each task, and cross-attention facilitates essential information exchange between tasks, resulting in a more comprehensive scene interpretation. Extensive experiments demonstrate the effectiveness of our approach in complex traffic scenarios.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523002970",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Data mining",
      "Engineering",
      "Human–computer interaction",
      "Key (lock)",
      "Neuroscience",
      "Perception",
      "Relation (database)",
      "Spatial relation",
      "Systems engineering",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Song",
        "given_name": "Zhanjie"
      },
      {
        "surname": "Zhao",
        "given_name": "Linqing"
      }
    ]
  },
  {
    "title": "A fast stereo matching network based on temporal attention and 2D convolution",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109808",
    "abstract": "We propose a fast stereo matching network based on temporal attention and 2D convolution (TANet). Due to the high similarity of the disparity between consecutive frames in an image sequence, we propose a temporal attention (TA) module that uses the disparity map of the previous frame to guide the disparity search range in the current frame, thus significantly improving the efficiency of disparity calculation in the cost volume module. Additionally, we propose a hierarchical cost construction and 2D convolution aggregation module that constructs a pyramid cost volume by fusing edge cues to establish detail constraints. This overcomes the problem of difficult convergence caused by information loss when replacing 3D convolution with 2D convolution. Experimental results show that the TA module effectively optimizes the cost volume and, together with 2D convolution, improves the computational speed. Compared with state-of-the-art algorithms, TANet achieves a speedup of nearly 4x, with a running time of 0.061s, and reduces the parameter count by nearly half while decreasing accuracy by 1.1%. Code is available at https://github.com/Y0uchenZ/TANet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032300506X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Composite material",
      "Computer science",
      "Computer vision",
      "Convolution (computer science)",
      "Enhanced Data Rates for GSM Evolution",
      "Frame (networking)",
      "Frame rate",
      "Geometry",
      "Image (mathematics)",
      "Matching (statistics)",
      "Materials science",
      "Mathematics",
      "Parallel computing",
      "Physics",
      "Pyramid (geometry)",
      "Quantum mechanics",
      "Range (aeronautics)",
      "Similarity (geometry)",
      "Speedup",
      "Statistics",
      "Telecommunications",
      "Volume (thermodynamics)"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Youchen"
      },
      {
        "surname": "Zhong",
        "given_name": "Hua"
      },
      {
        "surname": "Jia",
        "given_name": "Boyuan"
      },
      {
        "surname": "Li",
        "given_name": "Haixiong"
      }
    ]
  },
  {
    "title": "Graph matching for knowledge graph alignment using edge-coloring propagation",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109851",
    "abstract": "Knowledge graph (KG) is a kind of structured human knowledge of modeling the relationships between real-world entities. High quality KG is of crucial importance for many knowledge-based applications, e.g., question answering, recommender systems, etc. This paper studies the problem of entity alignment in KGs to promote knowledge fusion. Existing methods model the semantic representation of entities by using graph structural information or attribute information of the KG and then align the entities across different domains by calculating the distances between entities’ embeddings. However, these methods only consider the node-to-node similarity in the alignment procedure while the edge-to-edge similarity is ignored. Our research hypothesis is that the graph edge alignment information is critical in entity alignment. We reformulate the knowledge entity alignment as a quadratic assignment problem (QAP) by adding relation alignment under the one-to-one mapping constraint. To solve the notorious QAP in a large-scale heterogeneous graph like KG, we propose a model, dual neighborhood consensus network (DNCN), which approximately decomposes the QAP into two small-scale linear assignment problems, i.e., entity alignment and relation alignment. After that, an edge-coloring propagation method is proposed to refine the coarse entity alignment result using the relation correspondence. Theoretical proof shows that this method can guarantee the isomorphism between local sub-graphs. The performance of DNCN is evaluated using the DBP15K and DWY100K benchmarks. Experimental results show that DNCN achieves the best performance on the DBP15K benchmark, and is computationally efficient. Ablation studies verify the importance of graph edge alignment information.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005496",
    "keywords": [
      "Algorithm",
      "Computer science",
      "Graph",
      "Graph factorization",
      "Line graph",
      "Matching (statistics)",
      "Mathematics",
      "Statistics",
      "Theoretical computer science",
      "Voltage graph"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Yuxuan"
      },
      {
        "surname": "Li",
        "given_name": "Yuanxiang"
      },
      {
        "surname": "Wei",
        "given_name": "Xian"
      },
      {
        "surname": "Yang",
        "given_name": "Yongsheng"
      },
      {
        "surname": "Liu",
        "given_name": "Lei"
      },
      {
        "surname": "Murphey",
        "given_name": "Yi Lu"
      }
    ]
  },
  {
    "title": "D-DARTS: Distributed Differentiable Architecture Search",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.10.019",
    "abstract": "Differentiable ARchiTecture Search (DARTS) is one of the most trending Neural Architecture Search (NAS) methods. It drastically reduces search cost by resorting to weight-sharing. However, this approach also dramatically reduces the search space, thus excluding potential promising architectures. In this article, we propose D-DARTS, a solution that addresses this problem by nesting neural networks at the cell level instead of using weight-sharing to produce more diversified and specialized architectures. Moreover, we introduce a novel algorithm that can derive deeper architectures from a few trained cells, increasing performance and saving computation time. In addition, we introduce DARTOpti, an alternative search space in which we optimize existing handcrafted architectures instead of searching from scratch. Our solution reaches competitive performance on image classification tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523002921",
    "keywords": [
      "Algorithm",
      "Architecture",
      "Art",
      "Artificial intelligence",
      "Artificial neural network",
      "Computation",
      "Computer science",
      "Differentiable function",
      "Distributed computing",
      "Economics",
      "Mathematical analysis",
      "Mathematics",
      "Microeconomics",
      "Operating system",
      "Search algorithm",
      "Search cost",
      "Space (punctuation)",
      "Theoretical computer science",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Heuillet",
        "given_name": "Alexandre"
      },
      {
        "surname": "Tabia",
        "given_name": "Hedi"
      },
      {
        "surname": "Arioui",
        "given_name": "Hichem"
      },
      {
        "surname": "Youcef-Toumi",
        "given_name": "Kamal"
      }
    ]
  },
  {
    "title": "Two phase cooperative learning for supervised dimensionality reduction",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109871",
    "abstract": "The simultaneous minimization of the reconstruction and classification error is a hard non convex problem, especially when a non-linear mapping is utilized. To overcome this obstacle, motivated by the widespread success of Cooperative Neural Networks, an innovative supervised dimensionality reduction framework is proposed, based on a cooperative two phase optimization strategy. Specifically, the proposed framework that requires minimal parameter adjustment consists of an autoencoder for dimensionality reduction and a separator network for separability assessment of the embedding. This scheme results in meaningful and discriminable codes, which are optimized for the classification task and are exploitable by any trainable classifier. The experimental results showed that the proposed methodology achieved competitive results against the state-of-the-art competing methods, while being much more efficient in terms of parameter count. Finally, it was empirically justified that the proposed methodology introduces advanced behavioural explainability, while enabling applicability for image generation tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005691",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Autoencoder",
      "Classifier (UML)",
      "Computer science",
      "Curse of dimensionality",
      "Dimensionality reduction",
      "Embedding",
      "Geometry",
      "Machine learning",
      "Mathematics",
      "Minification",
      "Pattern recognition (psychology)",
      "Programming language",
      "Reduction (mathematics)"
    ],
    "authors": [
      {
        "surname": "Nellas",
        "given_name": "Ioannis A."
      },
      {
        "surname": "Tasoulis",
        "given_name": "Sotiris K."
      },
      {
        "surname": "Georgakopoulos",
        "given_name": "Spiros V."
      },
      {
        "surname": "Plagianakos",
        "given_name": "Vassilis P."
      }
    ]
  },
  {
    "title": "Underwater object classification combining SAS and transferred optical-to-SAS Imagery",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109868",
    "abstract": "Combining synthetic aperture sonar (SAS) imagery with optical images for underwater object classification has the potential to overcome challenges such as water clarity, the stability of the optical image analysis platform, and strong reflections from the seabed for sonar-based classification. In this work, we propose this type of multi-modal combination to discriminate between man-made targets and objects such as rocks or litter. We offer a novel classification algorithm that overcomes the problem of intensity and object formation differences between the two modalities. To this end, we develop a novel set of geometrical shape descriptors that takes into account the geometrical relation between the object’s shadow and highlight. Results from 7,052 pairs of SAS and optical images collected during several sea experiments show improved classification performance compared to the state-of-the-art for better discrimination between different types of underwater objects. For reproducability, we share our database.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005666",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Contextual image classification",
      "Data mining",
      "Geology",
      "Image (mathematics)",
      "Object (grammar)",
      "Oceanography",
      "Pattern recognition (psychology)",
      "Programming language",
      "Psychology",
      "Psychotherapist",
      "Relation (database)",
      "Remote sensing",
      "Set (abstract data type)",
      "Shadow (psychology)",
      "Sonar",
      "Synthetic aperture sonar",
      "Underwater"
    ],
    "authors": [
      {
        "surname": "Abu",
        "given_name": "Avi"
      },
      {
        "surname": "Diamant",
        "given_name": "Roee"
      }
    ]
  },
  {
    "title": "A Gaussian kernel for Kendall’s space of m -D shapes",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109887",
    "abstract": "In this paper, we develop an approach to exploit kernel methods with data lying on the m -D Kendall shape space. When data arise in a finite-dimensional curved Riemannian manifold, as in this case, the usual Euclidean computer vision and machine learning algorithms must be treated carefully. A good approach is to use positive definite kernels on manifolds to embed the manifold with its corresponding metric in a high-dimensional reproducing kernel Hilbert space, where it is possible to utilize algorithms developed for linear spaces. Different Gaussian kernels can be found in the literature on the 2-D Kendall shape space to perform this embedding. The main novelty of this work is to provide a Gaussian kernel for the m -D Kendall shape space. This new Kernel coincides in the case m = 2 with the Gaussian kernels most widely used in the Kendall planar shape space and allows to define an embedding of the m -D Kendall shape space into a reproducible kernel Hilbert space. As far as we know, the complexity of the m -D Kendall shape space has meant that this embedding has not been addressed in the literature until now. This methodology will be tested on a machine learning problem with a simulated and a real data set.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032300585X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Economics",
      "Embedding",
      "Engineering",
      "Euclidean space",
      "Gaussian",
      "Gaussian function",
      "Hilbert space",
      "Kernel (algebra)",
      "Kernel embedding of distributions",
      "Kernel method",
      "Manifold (fluid mechanics)",
      "Mathematical analysis",
      "Mathematics",
      "Mechanical engineering",
      "Metric (unit)",
      "Operating system",
      "Operations management",
      "Physics",
      "Pure mathematics",
      "Quantum mechanics",
      "Reproducing kernel Hilbert space",
      "Space (punctuation)",
      "Stiefel manifold",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "i Garcia",
        "given_name": "Vicent Gimeno"
      },
      {
        "surname": "Gual-Arnau",
        "given_name": "Ximo"
      },
      {
        "surname": "Ibáñez",
        "given_name": "M. Victoria"
      },
      {
        "surname": "Simó",
        "given_name": "Amelia"
      }
    ]
  },
  {
    "title": "mr2vec: Multiple role-based social network embedding",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.11.002",
    "abstract": "Learning representation of networks is increasingly important in social network analysis. Network embedding techniques have a wide range of applications in various downstream tasks, such as node classification and clustering. However, many existing approaches focus on representing each node in a network as a single vector, which is often not practical. In this paper, we propose a multiple role-based social network embedding, called mr2vec, by identifying multiple roles of each node through overlapping clustering and encoding each of them to a vector. Our proposed algorithm transforms a given network into the persona graph using overlapping clustering. Each pair of nodes and roles is mapped to a persona node, enabling us to learn multi-role-based embedding using conventional network embedding techniques. Moreover, we develop a new loss function to have close vector representations when the similarity between the nodes in a persona graph is high based not only on the structural information of node, but also on the role information of nodes. We conducted link prediction experiments within the network to demonstrate the effectiveness of representation learning that takes into account the multiple role information discovered through overlapping clustering of each node. Through extensive experiments on social networks, we have shown that mr2vec achieves higher accuracy than the state-of-the-art algorithms for single- or multi-aspect network embedding.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523003124",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Clustering coefficient",
      "Computer science",
      "Embedding",
      "Engineering",
      "Graph",
      "Graph embedding",
      "Image (mathematics)",
      "Node (physics)",
      "Similarity (geometry)",
      "Social media",
      "Social network (sociolinguistics)",
      "Structural engineering",
      "Theoretical computer science",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Jeong",
        "given_name": "Soohwan"
      },
      {
        "surname": "Park",
        "given_name": "Jongmin"
      },
      {
        "surname": "Lim",
        "given_name": "Sungsu"
      }
    ]
  },
  {
    "title": "Crisis event summary generative model based on hierarchical multimodal fusion",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109890",
    "abstract": "How to quickly obtain information about crisis events on social media such as Twitter and Weibo is crucial for follow-up rescue work and the promotion of postdisaster reconstruction. Therefore, it is very important to obtain useful information through multimodal summary generation technology. The current technology for generating crisis event summaries is mainly affected by unimodal bias and disregards the diversity of information in text and images. To solve these problems, this paper proposes a hierarchical multimodal crisis event summary generation model based on the modal alignment premise and hierarchical thinking. First, the visual context vector and text context vector are obtained, and then the hierarchical multimodal pointer model is employed to generate the text summary. Thus, the modal deviation is solved. Second, to select high-quality images, this paper proposes a dynamic selection strategy, which to some extent considers the requirements of the high correlation between text and images and the diversity of crisis information. Last, the experimental results based on the crisis event data in the MSMO dataset show that the proposed model achieves good performance in the summary generation and image selection of crisis events.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005885",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Data mining",
      "Event (particle physics)",
      "Generative grammar",
      "Hierarchical database model",
      "Linguistics",
      "Machine learning",
      "Paleontology",
      "Philosophy",
      "Physics",
      "Premise",
      "Quantum mechanics",
      "Selection (genetic algorithm)",
      "Social media",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Jing"
      },
      {
        "surname": "Yang",
        "given_name": "Shuo"
      },
      {
        "surname": "Zhao",
        "given_name": "Hui"
      }
    ]
  },
  {
    "title": "Attribute network joint embedding based on global attention",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.11.012",
    "abstract": "The attribute network not only has a complex topology, but its nodes also contain rich attribute information. Attribute network embedding methods extract both network topology and node attribute information to learn low-dimensional embedding of large attribute networks, which are of great importance for network analysis. In this paper, we propose attribute network joint embedding based on global attention (GAJE). First, GAJE uses the structure information of the attribute network to obtain the structure embedding vectors of nodes. Second, we propose a global attention method to obtain the attribute embedding vectors of nodes. This method captures the relationships of different attributes within and between nodes through convolutional neural networks and attention mechanisms, respectively. Finally, GAJE concatenates the structure embedding vectors and the attribute embedding vectors to obtain the final joint embedding vectors which simultaneously reflects the network structure and attributes information. For link prediction and node classification, we compared GAJE with nine well-known network embedding methods in four real datasets. The experimental results show that the algorithm has good attribute network embedding effects. Our tensorflow implementation of the GAJE is available at https://github.com/Andrewsama/GAJE-master.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523003227",
    "keywords": [
      "Architectural engineering",
      "Artificial intelligence",
      "Combinatorics",
      "Computer network",
      "Computer science",
      "Convolutional neural network",
      "Data mining",
      "Embedding",
      "Engineering",
      "Joint (building)",
      "Mathematics",
      "Network topology",
      "Node (physics)",
      "Structural engineering",
      "Theoretical computer science",
      "Topology (electrical circuits)"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Xu-Hua"
      },
      {
        "surname": "Ma",
        "given_name": "Gang-Feng"
      },
      {
        "surname": "Ma",
        "given_name": "Fang-Nan"
      },
      {
        "surname": "Ye",
        "given_name": "Lei"
      },
      {
        "surname": "Zhang",
        "given_name": "Yu-Di"
      }
    ]
  },
  {
    "title": "Adaptive filters in Graph Convolutional Neural Networks",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109867",
    "abstract": "Over the last few years, the availability of an increasing data generated from non-Euclidean domains, which are usually represented as graphs with complex relationships, and Graph Neural Networks (GNN) have gained a high interest because of their potential in processing graph-structured data. In particular, there is a strong interest in performing convolution on graphs using an extension of the GNN architecture, generally referred to as Graph Convolutional Neural Networks (ConvGNN). Convolution on graphs has been achieved mainly in two forms: spectral and spatial convolutions. Due to the higher flexibility in exploring and exploiting the graph structure of data, there is recently an increasing interest in investigating the possibilities that the spatial approach can offer. The idea of finding a way to adapt the network behaviour to the inputs they process to maximize the total performances has aroused much interest in the neural networks literature over the years. This paper presents a novel method to adapt the behaviour of a ConvGNN to the input performing spatial convolution on graphs using input-specific filters, which are dynamically generated from nodes feature vectors. The experimental assessment confirms the capabilities of the proposed approach, achieving satisfying results using a low number of filters.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005654",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Graph",
      "Pattern recognition (psychology)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Apicella",
        "given_name": "Andrea"
      },
      {
        "surname": "Isgrò",
        "given_name": "Francesco"
      },
      {
        "surname": "Pollastro",
        "given_name": "Andrea"
      },
      {
        "surname": "Prevete",
        "given_name": "Roberto"
      }
    ]
  },
  {
    "title": "Uniform misclassification loss for unbiased model prediction",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109689",
    "abstract": "Deep learning algorithms have achieved tremendous success over the past few years. However, the biased behavior of deep models, where the models favor/disfavor certain demographic subgroups, is a major concern in the deep learning community. Several adverse consequences of biased predictions have been observed in the past. One solution to alleviate the problem is to train deep models for fair outcomes. Therefore, in this research, we propose a novel loss function, termed as Uniform Misclassification Loss (UML) to train deep models for unbiased outcomes. The proposed UML function penalizes the model for the worst-performing subgroup for mitigating bias and enhancing the overall model performance. The proposed loss function is also effective while training with imbalanced data as well. Further, a metric, Joint Performance Disparity Measure (JPD) is introduced to jointly measure the overall model performance and the bias in model prediction. Multiple experiments have been performed on four publicly available datasets for facial attribute prediction and comparisons are performed with existing bias mitigation algorithms. Experimental results are reported using performance and bias evaluation metrics. The proposed loss function outperforms existing bias mitigation algorithms that showcase its effectiveness in obtaining unbiased outcomes and improved performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323003874",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Data mining",
      "Deep learning",
      "Economics",
      "Evolutionary biology",
      "Function (biology)",
      "Machine learning",
      "Management",
      "Measure (data warehouse)",
      "Metric (unit)",
      "Operations management",
      "Performance metric"
    ],
    "authors": [
      {
        "surname": "Majumdar",
        "given_name": "Puspita"
      },
      {
        "surname": "Vatsa",
        "given_name": "Mayank"
      },
      {
        "surname": "Singh",
        "given_name": "Richa"
      }
    ]
  },
  {
    "title": "Multi-resolution distillation for self-supervised monocular depth estimation",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.11.001",
    "abstract": "Obtaining dense depth ground-truth is not trivial, which leads to the introduction of self-supervised monocular depth estimation. Most self-supervised methods utilize the photometric loss as the primary supervisory signal to optimize a depth network. However, such self-supervised training often falls into an undesirable local minimum due to the ambiguity of the photometric loss. In this paper, we propose a novel self-distillation training scheme that provides a new self-supervision signal, depth consistency among different input resolutions, to the depth network. We further introduce a gradient masking strategy that adjusts the self-supervision signal of the depth consistency during back-propagation to boost the effectiveness of our depth consistency. Experiments demonstrate that our method brings meaningful performance improvements when applied to various depth network architectures. Furthermore, our method outperforms the existing self-supervised methods on KITTI, Cityscapes, and DrivingStereo datasets by a noteworthy margin.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523003112",
    "keywords": [
      "Ambiguity",
      "Art",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer vision",
      "Consistency (knowledge bases)",
      "Ground truth",
      "Machine learning",
      "Margin (machine learning)",
      "Masking (illustration)",
      "Matching (statistics)",
      "Mathematical analysis",
      "Mathematics",
      "Monocular",
      "Pattern recognition (psychology)",
      "Programming language",
      "SIGNAL (programming language)",
      "Scheme (mathematics)",
      "Statistics",
      "Supervised learning",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Lee",
        "given_name": "Sebin"
      },
      {
        "surname": "Im",
        "given_name": "Woobin"
      },
      {
        "surname": "Yoon",
        "given_name": "Sung-Eui"
      }
    ]
  },
  {
    "title": "AC2AS: Activation Consistency Coupled ANN-SNN framework for fast and memory-efficient SNN training",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109826",
    "abstract": "Spiking neural networks are efficient computation models for low-power environments. Spike-based BP algorithms and ANN-to-SNN (ANN2SNN) conversions are successful techniques for SNN training. Nevertheless, the spike-base BP training is slow and requires large memory costs, while ANN2SNN needs many inference steps to obtain good performance. In this paper, we propose an Activation Consistency Coupled ANN-SNN (A C 2 AS) framework to train the SNN in a fast and memory-efficient way. The A C 2 AS consists of two components: (a) a weight-shared architecture between ANN and SNN and (b) spiking mapping units. Firstly, the architecture trains the weight-shared parameters on the ANN branch, resulting in fast training and low memory costs for SNN. Secondly, the spiking mapping units are designed to ensure that the activation values of the ANN are the spiking features. As a result, the activation consistency is guaranteed, and the classification error of the SNN can be optimized by training the ANN branch. Besides, we design an adaptive threshold adjustment (ATA) algorithm to decrease the firing of noisy spikes. Experiment results show that our A C 2 AS-based models perform well on the benchmark datasets (CIFAR10, CIFAR100, and Tiny-ImageNet). Moreover, the A C 2 AS achieves comparable accuracy under 0 . 625 × time steps, 0 . 377 × training time, 0 . 27 × GPU memory costs, and 0 . 33 × spike activities of the Spike-based BP model. The code is available at https://github.com/TJXTT/AC2ASNN.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005241",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Computation",
      "Computer science",
      "Consistency (knowledge bases)",
      "Geodesy",
      "Geography",
      "Software engineering",
      "Spike (software development)",
      "Spiking neural network"
    ],
    "authors": [
      {
        "surname": "Tang",
        "given_name": "Jianxiong"
      },
      {
        "surname": "Lai",
        "given_name": "Jian-Huang"
      },
      {
        "surname": "Xie",
        "given_name": "Xiaohua"
      },
      {
        "surname": "Yang",
        "given_name": "Lingxiao"
      },
      {
        "surname": "Zheng",
        "given_name": "Wei-Shi"
      }
    ]
  },
  {
    "title": "Generalized black hole clustering algorithm",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.11.006",
    "abstract": "The Black Hole Clustering (BHC) algorithm is a density-based partitional clustering method inspired by the Density-based Spatial Clustering of Applications with Noise (DBSCAN). It does not require the number of clusters nor the computation of the pair-wise distance matrix between the data points, making it faster than DBSCAN. Also, it only needs one parameter that is intuitively easier to set than the epsilon parameter of DBSCAN. However, BHC needs the allocation of the so-called black holes that have to be linearly independent, making the algorithm in its current version suitable only for two or three-dimensional data sets. In this paper, we propose a generalized version of the black hole clustering algorithm (GBHC) by introducing a novel black hole allocation procedure for higher-dimensional data spaces. Furthermore, the proposed method is data-independent, so we have to run it once to obtain the black hole positions for all finite-dimensional metric spaces. We performed extensive computational experiments to compare GBHC with DBSCAN. The results show that both algorithms obtain comparable clustering solutions. GBHC, however, outperforms DBSCAN in computational complexity and explainability.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523003161",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "CURE data clustering algorithm",
      "Canopy clustering algorithm",
      "Cluster analysis",
      "Computation",
      "Computer science",
      "Correlation clustering",
      "DBSCAN",
      "Distance matrix",
      "Economics",
      "Image (mathematics)",
      "Mathematics",
      "Metric (unit)",
      "Noise (video)",
      "Operations management",
      "Pattern recognition (psychology)",
      "Programming language",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Saltos",
        "given_name": "Ramiro"
      },
      {
        "surname": "Weber",
        "given_name": "Richard"
      }
    ]
  },
  {
    "title": "Progressive dense feature fusion network for single image deraining",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.11.003",
    "abstract": "Deep Learning (DL) has achieved significant progress in single image deraining methods. Most of current DL methods, however, are still weak in image detail recovery and feature inherent correlation learning. In this work, we explore the detail recovery mechanisms in both network architecture and loss function for image deraining. A U-net like architecture named progressive dense feature fusion network (PDFFN) is proposed to encode rain images and decode them as clean ones. Specifically, a residual dense connection unit (ReDCU) is designed to handle rain streaks of various blurring degrees and resolutions by enriching features. Moreover, a progressive feature fusion module (PFFM), which fuses the features in different stages of U-net progressively, is devised to not only capture the inherent correlations of features across encoder and decoder but also intensify the fine-gained details. To better evaluate the perceptual similarity between the ground-truth and derained image, we propose detail perceptual loss by focusing on low-level unactivated features. Apart from the global rain removal strategy, this paper applies contextual loss on target regions to conduct joint deraining and detection. Comprehensive experiments substantiate the superiority of the proposed method, especially its detail recovery capability, as compared with state-of-the-art methods both qualitatively and quantitatively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523003136",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "ENCODE",
      "Encoder",
      "Feature (linguistics)",
      "Gene",
      "Image (mathematics)",
      "Linguistics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Feng",
        "given_name": "Fuxiang"
      },
      {
        "surname": "Zhang",
        "given_name": "Youmei"
      },
      {
        "surname": "Zhang",
        "given_name": "Weidong"
      },
      {
        "surname": "Li",
        "given_name": "Bin"
      }
    ]
  },
  {
    "title": "A multi-grained unsupervised domain adaptation approach for semantic segmentation",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109841",
    "abstract": "When transferring knowledge between different datasets, domain mismatch greatly hinders model’s performance. So domain adaption has been brought up to tackle the problem. Traditional methods focusing either on global or local alignment play a limited role in improving model’s performance. In this paper, we propose a multi-grained unsupervised domain adaptation approach (Muda) for semantic segmentation. Muda aims to enforce multi-grained semantic consistency between domains by aligning domains at both global and category level. Specifically, coarse-grained adaptation uses global adversarial learning on an image translation model and a main segmentation model, which respectively attempts to eliminate appearance differences and to get similar segmentation maps from two domains. While fine-grained adaptation employs an auxiliary model to adapt category information to refine pseudo labels of target data. Experiments and ablation studies are conducted on two synthetic-to-real benchmarks: GTA5 → Cityscapes and SYNTHIA → Cityscapes, which show that our model outperforms the state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005393",
    "keywords": [
      "Adaptation (eye)",
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Domain (mathematical analysis)",
      "Domain adaptation",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Natural language processing",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Psychology",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Luyang"
      },
      {
        "surname": "Ma",
        "given_name": "Tai"
      },
      {
        "surname": "Lu",
        "given_name": "Yue"
      },
      {
        "surname": "Li",
        "given_name": "Qingli"
      },
      {
        "surname": "He",
        "given_name": "Lianghua"
      },
      {
        "surname": "Wen",
        "given_name": "Ying"
      }
    ]
  },
  {
    "title": "Multi-Dimension Compression of Feed-Forward Network in Vision Transformers",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.10.014",
    "abstract": "Vision Transformers (ViTs) have recently made a splash in computer vision domain and achieved state-of-the-art in many vision tasks. Nevertheless, due to their vast model size and high computational costs, rare transformer-based models are adopted in real-world applications. Since the computational costs of attention operation is the square of the input size, some compression methods for the Multi-Head Self-Attention (MHSA) module have been proposed, reducing its FLOPs successfully but almost without parameters reduction. Meanwhile, the number of parameters and computational costs in the Feed-Forward Network (FFN) module exceeds the MHSA larger, while its compression technologies have not been delved deeper. Consequently, we focus our insight on the compression of FFN layer and present a pruning method named Multi-Dimension Compression of Feed-Forward Network in Vision Transformers(MCF), which greatly reduces the model’s parameters and computational costs. Firstly, we identify the critical elements in the output of the FFN module and then employ them to guide the irregular sparsity of this layer, recognizing insignificant elements of FFN layer that have less impact on the output. Successively, to discard the insignificant elements, we transform the irregular sparsity into regular sparsity and prune them, thus reducing the models’ parameters and getting a substantial speed-up during inference. Extensive results on ImageNet-1K validate the effectiveness of our proposed method, which obtains significant parameters and computational costs reduction with almost unimpaired generalization. For example, we compress DeiT-Tiny with 42% reduction in FLOPs and 33% reduction in parameters, almost without losing accuracy on the ImageNet dataset. Further, we verify the effectiveness of our method in the downstream task, using the pruned DeiT-Small as the backbone for the object detection task on the COCO dataset, gaining revenue without compromising its performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523002866",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computational complexity theory",
      "Computer engineering",
      "Computer science",
      "Electrical engineering",
      "Engineering",
      "FLOPS",
      "Geometry",
      "Mathematics",
      "Parallel computing",
      "Reduction (mathematics)",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Bai",
        "given_name": "Shipeng"
      },
      {
        "surname": "Chen",
        "given_name": "Jun"
      },
      {
        "surname": "Yang",
        "given_name": "Yu"
      },
      {
        "surname": "Liu",
        "given_name": "Yong"
      }
    ]
  },
  {
    "title": "Deep fidelity in DNN watermarking: A study of backdoor watermarking for classification models",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109844",
    "abstract": "Backdoor watermarking is a promising paradigm to protect the copyright of deep neural network (DNN) models. In the existing works on this subject, researchers have intensively focused on watermarking robustness, while the concept of fidelity, which is concerned with the preservation of the model’s original functionality, has received less attention. In this paper, focusing on deep image classification models, we show that the existing shared notion of the sole measurement of learning accuracy is inadequate to characterize backdoor fidelity. Meanwhile, we show that the analogous concept of embedding distortion in multimedia watermarking, interpreted as the total weight loss (TWL) in DNN backdoor watermarking, is also problematic for fidelity measurement. To address this challenge, we propose the concept of deep fidelity, which states that the backdoor watermarked DNN model should preserve both the feature representation and decision boundary of the unwatermarked host model. To achieve deep fidelity, we propose two loss functions termed penultimate feature loss (PFL) and softmax probability-distribution loss (SPL) to preserve feature representation, while the decision boundary is preserved by the proposed fix last layer (FixLL) treatment, inspired by the recent discovery that deep learning with a fixed classifier causes no loss of learning accuracy. With the above designs, both embedding from scratch and fine-tuning strategies are implemented to evaluate the deep fidelity of backdoor embedding, whose advantages over the existing methods are verified via experiments using ResNet18 for MNIST and CIFAR-10 classifications, and wide residual network (i.e., WRN28_10) for CIFAR-100 task. PyTorch codes are available at https://github.com/ghua-ac/dnn_watermark.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005423",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Backdoor",
      "Computer science",
      "Computer security",
      "Contextual image classification",
      "Deep learning",
      "Digital watermarking",
      "Electrical engineering",
      "Embedding",
      "Engineering",
      "Feature (linguistics)",
      "Feature learning",
      "Fidelity",
      "High fidelity",
      "Image (mathematics)",
      "Linguistics",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Softmax function",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Hua",
        "given_name": "Guang"
      },
      {
        "surname": "Teoh",
        "given_name": "Andrew Beng Jin"
      }
    ]
  },
  {
    "title": "Adaptive local adversarial attacks on 3D point clouds",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109825",
    "abstract": "Modern artificial intelligence systems rely heavily on deep learning techniques. However, deep neural networks are easily disturbed by adversarial objects. Adversarial examples are beneficial to improve the robustness of the 3D neural network model and enhance the stability of the artificial intelligence system. At present, most 3D adversarial attack methods perturb the entire point cloud to generate adversarial examples, which results in high perturbation costs and low operability in the physical world. In this paper, we propose an adaptive local adversarial attack method (AL-Adv) on 3D point clouds to generate adversarial point clouds. First, we analyze the vulnerability of the 3D network model and extract the salient regions of the input point cloud, namely the vulnerable regions. Second, we propose an adaptive gradient attack algorithm that targets salient regions. The proposed attack algorithm adaptively assigns different disturbances in different directions of the three-dimensional coordinates of the point cloud. Experimental results show that our proposed AL-Adv method achieves a higher attack success rate than the global attack method. Specifically, the adversarial examples generated by AL-Adv demonstrate good imperceptibility and small generation costs.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032300523X",
    "keywords": [
      "Adversarial system",
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer security",
      "Deep learning",
      "Gene",
      "Point cloud",
      "Robustness (evolution)",
      "Salient",
      "Vulnerability (computing)"
    ],
    "authors": [
      {
        "surname": "Zheng",
        "given_name": "Shijun"
      },
      {
        "surname": "Liu",
        "given_name": "Weiquan"
      },
      {
        "surname": "Shen",
        "given_name": "Siqi"
      },
      {
        "surname": "Zang",
        "given_name": "Yu"
      },
      {
        "surname": "Wen",
        "given_name": "Chenglu"
      },
      {
        "surname": "Cheng",
        "given_name": "Ming"
      },
      {
        "surname": "Wang",
        "given_name": "Cheng"
      }
    ]
  },
  {
    "title": "Attribute subspaces for zero-shot learning",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109869",
    "abstract": "Zero-shot learning (ZSL) aims to recognize unseen categories without corresponding training samples, which is a practical yet challenging task in computer vision and pattern recognition community. Current state-of-the-art locality-based ZSL methods aim to learn the explicit locality of discriminative attributes, which may suffer from insufficient class-level attribute supervision. In this paper, we introduce an Attribute Subspace learning method for ZSL (AS-ZSL) to learn implicit attribute composition, which is more general than attribute localization with only class-level attribute supervision. AS-ZSL exploits subspace representations that can effectively capture the intrinsic composition of high-dimensional image features and the diversity within attribute appearance. Furthermore, we develop a subspace distance based triplet loss to improve the distinguishability of the attribute subspace representation. Attribute subspace learning module is only needed for the training phase to jointly learn discriminative global features. This leads to a compact inference phase. Furthermore, the proposed AS-ZSL can be naturally extended to adapt to the transductive ZSL setting using a novel self-supervised training strategy. Extensive experimental results on several widely used ZSL datasets, i.e., CUB, AwA2, and SUN, demonstrate the advantage of AS-ZSL compared with the state-of-the-art under different ZSL settings.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005678",
    "keywords": [
      "Artificial intelligence",
      "Association (psychology)",
      "Class (philosophy)",
      "Computer science",
      "Discriminative model",
      "Economics",
      "Epistemology",
      "Geometry",
      "Inference",
      "Law",
      "Linear subspace",
      "Linguistics",
      "Locality",
      "Machine learning",
      "Management",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Subspace topology",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Lei"
      },
      {
        "surname": "Liu",
        "given_name": "Yang"
      },
      {
        "surname": "Bai",
        "given_name": "Xiao"
      },
      {
        "surname": "Li",
        "given_name": "Na"
      },
      {
        "surname": "Yu",
        "given_name": "Xiaohan"
      },
      {
        "surname": "Zhou",
        "given_name": "Jun"
      },
      {
        "surname": "Hancock",
        "given_name": "Edwin R."
      }
    ]
  },
  {
    "title": "Multiple spheres detection problem—Center based clustering approach",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.10.018",
    "abstract": "In this paper, we propose an adaptation of the well-known k -means algorithm for solving the multiple spheres detection problem when data points are homogeneously scattered around several spheres. We call this adaptation the k -closest spheres algorithm. In order to choose good initial spheres, we use a few iterations of the global optimizing algorithm DIRECT , resulting in the high efficiency of the proposed k -closest spheres algorithm. We present illustrative examples for the case of non-intersecting and for the case of intersecting spheres. We also show a real-world application in analyzing earthquake depths.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523002908",
    "keywords": [
      "Artificial intelligence",
      "Astronomy",
      "Center (category theory)",
      "Chemistry",
      "Cluster analysis",
      "Computer science",
      "Crystallography",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "SPHERES"
    ],
    "authors": [
      {
        "surname": "Sabo",
        "given_name": "Kristian"
      },
      {
        "surname": "Scitovski",
        "given_name": "Rudolf"
      },
      {
        "surname": "Ungar",
        "given_name": "Šime"
      }
    ]
  },
  {
    "title": "Cross-Modal Transformer for RGB-D semantic segmentation of production workshop objects",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109862",
    "abstract": "Scene understanding in a production workshop is an important technology to improve its intelligence level, semantic segmentation of production workshop objects is an effective method for realizing scene understanding. Since the varieties of information of production workshop, making full use of the complementary information of RGB image and depth image can effectively improve the semantic segmentation accuracy of production workshop objects. Aiming at solving the multi-scale and real-time problems of segmenting the production workshop objects, this paper proposes Cross-Modal Transformer (CMFormer), a Transformer-based cross-modal semantic segmentation model. Its key feature correction and feature fusion parts are composed of the Multi-Scale Channel Attention Correction(MS-CAC) module and the Global Feature Aggregation(GFA) module. By improving Multi-Head Self-Attention(MHSA) in Transformer, we design Cross-Modal Multi-Head Self-Attention(CM-MHSA) to build long-range interaction between RGB image and depth image, and further design the MS-CAC module and the GFA module on the basis of the CM-MHSA module, to achieve cross-modal information interaction in the channel and spatial dimensions. Among them, the MS-CAC module enriches the multi-scale features of each channel and achieve more accurate channel attention correction between the two modals; the GFA module interacts with RGB feature and depth feature in the spatial dimension and fuses global and local features at the same time. In the experiments on the NYU Depth v2 dataset, the CMFormer reached 68.00% MPA(Mean Pixel Accuracy) and 55.75% mIoU(Mean Intersection over Union), achieves the state-of-the-art results. While in the experiments on the Scene Objects for Production workshop dataset(SOP), the CMFormer achieves 96.74% MPA, 92.98% mIoU and 43 FPS(Frames Per Second), which has high precision and good real-time performance. Code is available at: https://github.com/FutureIAI/CMFormer",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005605",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Electrical engineering",
      "Engineering",
      "Feature (linguistics)",
      "Linguistics",
      "Modal",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Polymer chemistry",
      "RGB color model",
      "Segmentation",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Ru",
        "given_name": "Qingjun"
      },
      {
        "surname": "Chen",
        "given_name": "Guangzhu"
      },
      {
        "surname": "Zuo",
        "given_name": "Tingyu"
      },
      {
        "surname": "Liao",
        "given_name": "Xiaojuan"
      }
    ]
  },
  {
    "title": "Expansion window local alignment weighted network for fine-grained sketch-based image retrieval",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109892",
    "abstract": "Fine-Grained Sketch-Based Image Retrieval (FG-SBIR) is a worthwhile task, which can be useful in many scenarios like recommendation systems, receiving a great deal of attention. In this study, we analyze challenges faced in FG-SBIR and propose a novel Expansion Window Local Alignment Weighted Network (EWLAW-Net). Specifically, it contains two main components: the Expansion Window Local Alignment module (EWLA) and the Local Weighted Fusion module (LWF). The EWLA module adopts an expansion window mechanism to align local features extracted from the backbone with the same semantic meaning between photos and sketches. The LWF module assigns weights to each local feature of the sketch after evaluating their importance and fuses them to calculate the similarity between the sketch and photos for retrieval. Experiments are conducted on five datasets and the results demonstrate the effectiveness of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005903",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Economics",
      "Feature (linguistics)",
      "Image (mathematics)",
      "Image retrieval",
      "Information retrieval",
      "Linguistics",
      "Management",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Similarity (geometry)",
      "Sketch",
      "Task (project management)",
      "Window (computing)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Zi-Chao"
      },
      {
        "surname": "Xie",
        "given_name": "Zhen-Yu"
      },
      {
        "surname": "Chen",
        "given_name": "Zhen-Duo"
      },
      {
        "surname": "Zhan",
        "given_name": "Yu-Wei"
      },
      {
        "surname": "Luo",
        "given_name": "Xin"
      },
      {
        "surname": "Xu",
        "given_name": "Xin-Shun"
      }
    ]
  },
  {
    "title": "Unpaired image super-resolution using a lightweight invertible neural network",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109822",
    "abstract": "Unpaired image super-resolution (SR) has recently attracted considerable attention in the unsupervised SR community. In contrast to supervised SR, existing unpaired SR methods inevitably resort to the generative adversarial network (GAN) to explore data distribution on the given HR and unpaired LR dataset. Nevertheless, predominant strategies often strive for sophisticated network structures or training pipelines, making them intractable to apply in real-world scenarios. In this work, a lightweight invertible neural network (INN) is proposed for unpaired SR to alleviate this limitation. Specifically, we regard image degradation and SR as a pair of mutually-inverse tasks and replace the two generators in one-stage GAN with INN. Due to the information lossless nature of INN, it is impossible to generate noise in vain during image degradation. We thus design a simple noise injection network to induce realistic noise, thereby simulating real LR images. To further maintain the stability and realism of the noise, we propose to extract the noise prior from the real-world LR image. With extracted noise prior as input, our noise injection network can narrow the gap between the generated noise and the real one, thereby encouraging the degraded images to match the real-world LR domain. Extensive experiments demonstrate that our method achieves comparable performance with other SOTA methods in quantitative and qualitative evaluations while enjoying faster speed and much smaller parameters.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005204",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer vision",
      "Image (mathematics)",
      "Invertible matrix",
      "Mathematics",
      "Noise (video)",
      "Pattern recognition (psychology)",
      "Pure mathematics"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Huan"
      },
      {
        "surname": "Shao",
        "given_name": "Mingwen"
      },
      {
        "surname": "Qiao",
        "given_name": "Yuanjian"
      },
      {
        "surname": "Wan",
        "given_name": "Yecong"
      },
      {
        "surname": "Meng",
        "given_name": "Deyu"
      }
    ]
  },
  {
    "title": "Few-shot learning via weighted prototypes from graph structure",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.11.017",
    "abstract": "Few-shot learning is attracting extensive research because of its ability to classify only a few co-trainable samples. Current few-shot learning approaches focus on learning class prototypes representation to solve problems by a simple averaging approach, but this approach ignores intra-class differences. In this paper, we propose a new weighted prototype network for few-shot learning. Our model consists of two modules, feature extraction and prototype modification. We first construct graphs from the embeddings obtained from the feature extraction module. Then we fed these graphs into graph neural networks in order to explore the contribution of each sample to its class prototype from the graph structure. The experimental results on three benchmark datasets show that our proposed model is comparable to the state-of-the-art few-shot learning approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523003276",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Chemistry",
      "Class (philosophy)",
      "Computer science",
      "Construct (python library)",
      "Engineering",
      "Feature (linguistics)",
      "Feature extraction",
      "Feature learning",
      "Focus (optics)",
      "Geodesy",
      "Geography",
      "Graph",
      "Law",
      "Linguistics",
      "Machine learning",
      "Mechanical engineering",
      "One shot",
      "Optics",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Political science",
      "Politics",
      "Programming language",
      "Representation (politics)",
      "Shot (pellet)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Yifan"
      },
      {
        "surname": "Yu",
        "given_name": "Lei"
      }
    ]
  },
  {
    "title": "Paired contrastive feature for highly reliable offline signature verification",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109816",
    "abstract": "Signature verification requires high reliability. Especially in the writer-independent scenario with the skilled-forgery-only condition, achieving high reliability is challenging but very important. In this paper, we propose to apply two machine learning frameworks, learning with rejection and top-rank learning, to this task because they can suppress ambiguous results and thus give only reliable verification results. Since those frameworks accept a single input, we transform a pair of genuine and query signatures into a single feature vector, called Paired Contrastive Feature (PCF). PCF internally represents similarity (or discrepancy) between the two paired signatures; thus, reliable machine learning frameworks can make reliable decisions using PCF. Through experiments on three public signature datasets in the offline skilled-forgery-only writer-independent scenario, we evaluate and validate the effectiveness and reliability of the proposed models by comparing their performance with a state-of-the-art model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005149",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Data mining",
      "Economics",
      "Feature (linguistics)",
      "Feature vector",
      "Geometry",
      "Image (mathematics)",
      "Linguistics",
      "Machine learning",
      "Management",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Power (physics)",
      "Quantum mechanics",
      "Rank (graph theory)",
      "Reliability (semiconductor)",
      "Signature (topology)",
      "Similarity (geometry)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "ji",
        "given_name": "Xiaotong"
      },
      {
        "surname": "Suehiro",
        "given_name": "Daiki"
      },
      {
        "surname": "Uchida",
        "given_name": "Seiichi"
      }
    ]
  },
  {
    "title": "Learning scalable dynamic filter in convolutional networks",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.10.020",
    "abstract": "Most existing neural networks use static filters with fixed model capacity to process all samples. To cover the large variation among different samples, these networks contain considerable redundant computational budgets, thereby limiting their inference efficiency. To reduce these redundant computation for higher inference speed, in this paper, we propose a scalable dynamic filter that can customize its parameters and shapes conditioned on the input sample. Specifically, our scalable dynamic filter uses a kernel prediction head to predict dynamic kernels and employs a channel mask head to adaptively prune redundant channels. As a result, a kernel with dynamic parameters and an adaptive shape is produced for efficient inference. Extensive experiments on both image classification and object detection tasks demonstrate that our method achieves superior performance than previous methods with low computational complexity and high inference efficiency.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552300291X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Combinatorics",
      "Computation",
      "Computational complexity theory",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Database",
      "Filter (signal processing)",
      "Filter design",
      "Inference",
      "Kernel (algebra)",
      "Kernel adaptive filter",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Scalability"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Shuanglin"
      },
      {
        "surname": "Xiao",
        "given_name": "Chao"
      },
      {
        "surname": "Ying",
        "given_name": "Xinyi"
      },
      {
        "surname": "Wang",
        "given_name": "Longguang"
      },
      {
        "surname": "Yang",
        "given_name": "Jungang"
      },
      {
        "surname": "An",
        "given_name": "Wei"
      }
    ]
  },
  {
    "title": "Motional foreground attention-based video crowd counting",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109891",
    "abstract": "In this paper, we tackle the problem of video crowd counting. Compared with single image crowd counting, video provides gradual spatial and temporal variation information that would help to strengthen the robustness of crowd counting. Therefore, it is critical to make full use of neighboring frames both in feature extraction and final prediction for current frame’s estimation. Based on the above observations, we propose a motional foreground attention-based video crowd counting method. Specifically, we first leverage an foreground estimation module based on ConvNeXt to extract the motional features from bidirectional frame differences and output a foreground estimation map. Then the motional features combined with the static features of current frame are sent into feature fusion network, where foreground estimation map is transformed as attention weights for crowd number estimation. Three new indoor video datasets are manually annotated. The proposed method achieves state-of-the-art performance on all indoor and outdoor video datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005897",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Feature extraction",
      "Foreground detection",
      "Frame (networking)",
      "Gene",
      "Leverage (statistics)",
      "Linguistics",
      "Object detection",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Robustness (evolution)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Ling",
        "given_name": "Miaogen"
      },
      {
        "surname": "Pan",
        "given_name": "Tianhang"
      },
      {
        "surname": "Ren",
        "given_name": "Yi"
      },
      {
        "surname": "Wang",
        "given_name": "Ke"
      },
      {
        "surname": "Geng",
        "given_name": "Xin"
      }
    ]
  },
  {
    "title": "Encoder–decoder cycle for visual question answering based on perception-action cycle",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109848",
    "abstract": "In this study, we propose a novel encoder–decoder cycle (EDC) framework inspired by the human learning process called the perception-action cycle to tackle challenging problems such as visual question answering (VQA) and visual relationship detection (VRD). EDC considers the understanding of the visual features of an image as perception and the act of answering the question regarding that image as an action. In the perception-action cycle, information is primarily collected from the environment and then passed to sensory structures in the brain to form an understanding of the environment. Acquired knowledge is then passed to motor structures to perform an action on the environment. Next, sensory structures perceive the altered environment and improve their understanding of the surrounding world. This process of understanding the environment, performing an action correspondingly, and then re-evaluating the initial understanding occurs cyclically in human life. EDC initially mimics this mechanism of introspection by comprehending and refining visual features to acquire the proper knowledge for answering the question. Subsequently, it decodes visual and language features into answer features, feeding them back cyclically to the encoder. In the VRD task, EDC decodes visual features to generate predicate features. We evaluate the proposed framework on the TDIUC, VQA 2.0, and VRD datasets, which outperforms the state-of-the-art models on the TDIUC and VRD datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005460",
    "keywords": [
      "Action (physics)",
      "Artificial intelligence",
      "Computer science",
      "Decodes",
      "Decoding methods",
      "Encoder",
      "Engineering",
      "Natural language processing",
      "Neuroscience",
      "Operating system",
      "Perception",
      "Physics",
      "Predicate (mathematical logic)",
      "Process (computing)",
      "Programming language",
      "Psychology",
      "Quantum mechanics",
      "Question answering",
      "Systems engineering",
      "Task (project management)",
      "Telecommunications",
      "Visual perception"
    ],
    "authors": [
      {
        "surname": "Mohamud",
        "given_name": "Safaa Abdullahi Moallim"
      },
      {
        "surname": "Jalali",
        "given_name": "Amin"
      },
      {
        "surname": "Lee",
        "given_name": "Minho"
      }
    ]
  },
  {
    "title": "A balanced random learning strategy for CNN based Landsat image segmentation under imbalanced and noisy labels",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109824",
    "abstract": "Landsat image segmentation is important for obtaining large-scale land cover maps. The accuracy of CNN-based Landsat image segmentation highly depends on the quantity and quality of the training samples. However, enough accurate labels for Landsat images are difficult to access. Fortunately, traditional classifier induced segmentation results can be considered as an alternative, although they are noisy and unbalanced to a certain extent. To resist noisy labels and alleviate the impact of imbalanced samples, this paper proposes a confidence interval based balanced random learning strategy. Firstly, a confidence interval-based mask is employed to control the random learning rate of the network from the entire noisy training set. Then, the multi-layer feature maps of CNN are fully utilized to compensate for the information loss in random learning, in which down-sampled labels are used to decrease the uncertainty brought by up-sampling CNN feature maps. In addition, considering the corruption of noisy labels on different classes, a balanced random learning with different confidence levels is performed on each class to further improve the learning ability of CNN. Experimental results on two widely used backbones, namely VGGNet and ResNet, demonstrate that the proposed balanced random learning strategy can effectively improve the performance of CNN under imbalanced and noisy labels, which can be improved by 3.41%.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005228",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Feature (linguistics)",
      "Image segmentation",
      "Linguistics",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Random forest",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Xuemei"
      },
      {
        "surname": "Cheng",
        "given_name": "Yong"
      },
      {
        "surname": "Liang",
        "given_name": "Luo"
      },
      {
        "surname": "Wang",
        "given_name": "Haijian"
      },
      {
        "surname": "Gao",
        "given_name": "Xingyu"
      },
      {
        "surname": "Wu",
        "given_name": "Jun"
      }
    ]
  },
  {
    "title": "E3ID: An efficient end to end person search model",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.10.030",
    "abstract": "Person Search has recently emerged as a challenging task that aims to jointly solve Pedestrian Detection and Person Re-identification (Re-ID). However, the existing approaches still stay at the image processing stage. In this paper, we proposed E3ID, an efficient end-to-end person search model for video, to better solve the person search problem in real densely populated areas. To speed up the model, a low-quality image filter is proposed to adaptively adjust the sampling frequency of Yolov5 according to the moving speed of pedestrians in the video with minimal computational effort. And the cropped pedestrian images are stored in the gallery library for the Re-ID step. Then, we exploit color feature enhancer to mine color features in specific regions that contribute significantly to the Re-ID process. Compared to other state-of-the-art methods, our proposed E3ID can reach 77.94 % on the evaluation index rank 1, which creates a feasibility of using this model in other complex real-world scenarios.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523003094",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Data mining",
      "Economics",
      "Engineering",
      "Exploit",
      "Feature (linguistics)",
      "Filter (signal processing)",
      "Linguistics",
      "Management",
      "Operating system",
      "Pedestrian",
      "Pedestrian detection",
      "Philosophy",
      "Process (computing)",
      "Speedup",
      "Task (project management)",
      "Transport engineering"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Siyang"
      },
      {
        "surname": "Liang",
        "given_name": "Yanchun"
      },
      {
        "surname": "Li",
        "given_name": "Ao"
      },
      {
        "surname": "Wang",
        "given_name": "Zeqing"
      },
      {
        "surname": "Han",
        "given_name": "Xiaosong"
      }
    ]
  },
  {
    "title": "Neighborhood overlap-aware heterogeneous hypergraph neural network for link prediction",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109818",
    "abstract": "In real world, a large number of networks are heterogeneous, containing different types of semantics and connections. Existing studies typically only consider lower-order pairwise relations rather than higher-order group interactions. Furthermore, they tend to focus more on node attributes rather than graph structural information. This results models failing to maintain graph topology effectively, which reduces the effectiveness on link prediction. To address these limitations, we propose N eighborhood O verlap-aware H eterogeneous hypergraph neural network (NOH) that learns useful structural information from the heterogeneous graph and estimates overlapped neighborhood for link prediction. Our model fuses the heterogeneity of graphs with structural information so that the model maintains both lower-order pairwise relations and higher-order complex semantics. Our extensive experiments on four real-world datasets show that NOH consistently achieves state-of-the-art performance on link prediction.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005162",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer network",
      "Computer science",
      "Data mining",
      "Discrete mathematics",
      "Engineering",
      "Graph",
      "Hypergraph",
      "Link (geometry)",
      "Mathematics",
      "Node (physics)",
      "Pairwise comparison",
      "Programming language",
      "Semantics (computer science)",
      "Structural engineering",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Lu",
        "given_name": "Yifan"
      },
      {
        "surname": "Gao",
        "given_name": "Mengzhou"
      },
      {
        "surname": "Liu",
        "given_name": "Huan"
      },
      {
        "surname": "Liu",
        "given_name": "Zehao"
      },
      {
        "surname": "Yu",
        "given_name": "Wei"
      },
      {
        "surname": "Li",
        "given_name": "Xiaoming"
      },
      {
        "surname": "Jiao",
        "given_name": "Pengfei"
      }
    ]
  },
  {
    "title": "A multi-task network for speaker and command recognition in industrial environments",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.10.022",
    "abstract": "In industrial environments, it is crucial to establish a strong collaboration between humans and robots to enhance productivity. However, the nature of the work demands that workers have the authority to provide specific instructions to the robots. The scientific community has extensively investigated these dual requirements, aiming to develop advanced systems capable of recognizing voice commands and implementing speaker authentication. Nevertheless, in the industrial context, these tasks should be executed simultaneously on low-cost and low-power embedded devices that can be mounted on board the robotic platform. To overcome this challenge, we propose a multi-task network for Speech-Command Recognition and Speaker Identification. Additionally, we employ the GradNorm adaptive algorithm to address the issue of task imbalance. To evaluate the proposed system, we introduce a new dataset, MIVIA-ISC, consisting of 20,857 samples uttered by 562 speakers for 31 distinct commands. Our approach significantly reduces the network size by 47% and its execution time by 48% compared to the commonly used methodology, which employs one network for each task. Furthermore, our approach demonstrates a significant improvement in the accuracy of the Speaker Identification task, achieving an 11% increase compared to the corresponding single-task network. Importantly, this enhancement is achieved without compromising the accuracy of the Speech-Command Recognition task, which experiences only a minimal 3% decrease in performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523002945",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Computer science",
      "Context (archaeology)",
      "Economics",
      "Identification (biology)",
      "Management",
      "Paleontology",
      "Robot",
      "Speaker recognition",
      "Speech recognition",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Bini",
        "given_name": "Stefano"
      },
      {
        "surname": "Percannella",
        "given_name": "Gennaro"
      },
      {
        "surname": "Saggese",
        "given_name": "Alessia"
      },
      {
        "surname": "Vento",
        "given_name": "Mario"
      }
    ]
  },
  {
    "title": "Estimation of interlayer textural relationships to discriminate the benignancy/malignancy of brain tumors",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109879",
    "abstract": "A computer-aided diagnosis system is a popular tool to predict the risk factors of brain tumors. However, the existing techniques are unable to provide high detection accuracy due to the inability of capturing the hidden features of brain tumors. In this view, the present work decomposes the MR modality images into different layers using non-sub-sampled shearlet transformation (NSST). Following this, the proposed detection pipeline employs adaptive pulse-coupled neural network (A-PCNN) module and a fuzzy c-means (FCM) clustering algorithm for enhancement and segmentation of suspicious regions respectively. Interlayer textural relationships of tumors are estimated in terms of the layer-wise difference of the gray-level cooccurrence matrix (GLCM), similarity indices, and entropy distributions. Thenceforth, those feature coefficients are coded with binary sequences to express the textural homogeneity/randomness of tumors. Finally, these handcrafted multi-scale feature quantifiers are fed to some standard classifiers such as the k-nearest neighbor (kNN) technique, the linear square support vector machine (LS-SVM), and the decision tree (DT) for discriminating the state of benignancy/malignancy of tumors. Experimental results ensure that the proposed detection model shows superior performance compared to existing methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005770",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Entropy (arrow of time)",
      "Entropy estimation",
      "Estimator",
      "Feature vector",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Segmentation",
      "Statistics",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Das",
        "given_name": "Poulomi"
      },
      {
        "surname": "Das",
        "given_name": "Arpita"
      }
    ]
  },
  {
    "title": "Hierarchical multimodal transformers for Multipage DocVQA",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109834",
    "abstract": "Existing work on DocVQA only considers single-page documents. However, in real applications documents are mostly composed of multiple pages that should be processed altogether. In this work, we propose a new multimodal hierarchical method Hi-VT5, that overcomes the limitations of current methods to process long multipage documents. In contrast to previous hierarchical methods that focus on different semantic granularity (He et al., 2021) or different subtasks (Zhou et al., 2022) used in image classification. Our method is a hierarchical transformer architecture where the encoder learns to summarize the most relevant information of every page and then, the decoder uses this summarized representation to generate the final answer, following a bottom-up approach. Moreover, due to the lack of multipage DocVQA datasets, we also introduce MP-DocVQA, an extension of SP-DocVQA where questions are posed over multipage documents instead of single pages. Through extensive experimentation, we demonstrate that Hi-VT5 is able, in a single stage, to answer the questions and provide the page that contains the answer, which can be used as a kind of explainability measure.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005320",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Encoder",
      "Focus (optics)",
      "Granularity",
      "Information retrieval",
      "Law",
      "Operating system",
      "Optics",
      "Physics",
      "Political science",
      "Politics",
      "Programming language",
      "Quantum mechanics",
      "Representation (politics)",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Tito",
        "given_name": "Rubèn"
      },
      {
        "surname": "Karatzas",
        "given_name": "Dimosthenis"
      },
      {
        "surname": "Valveny",
        "given_name": "Ernest"
      }
    ]
  },
  {
    "title": "Deep multi-view spectral clustering via ensemble",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109836",
    "abstract": "Graph-based methods have achieved great success in multi-view clustering. However, existing graph-based models generally utilize shallow and linear embedding functions to obtain the common spectral embedding for clustering assignments. In addition, the fusion similarity graphs from multiple views are generally obtained by a simple weighted-sum rule. To this end, we propose a novel deep multi-view spectral clustering via ensemble model (DMCE), which applies ensemble clustering to fuse the similarity graphs from different views. On this basis, we employ the graph auto-encoder to learn the common spectral embedding, which can be regarded as the indicator matrix directly. Moreover, a unified optimization framework is designed to update the variables in the proposed DMCE, which consists of graph reconstruction loss, orthogonal loss, and graph contrastive learning loss. Extensive experiments on six real-world benchmark datasets have demonstrated the effectiveness of our model compared with the state-of-the-art multi-view clustering methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005344",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Autoencoder",
      "Cluster analysis",
      "Computer science",
      "Correlation clustering",
      "Data mining",
      "Deep learning",
      "Embedding",
      "Graph",
      "Graph partition",
      "Pattern recognition (psychology)",
      "Spectral clustering",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Mingyu"
      },
      {
        "surname": "Yang",
        "given_name": "Weidong"
      },
      {
        "surname": "Nie",
        "given_name": "Feiping"
      }
    ]
  },
  {
    "title": "Participants-based Synchronous Optimization Network for skeleton-based action recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.11.010",
    "abstract": "Nowadays, graph convolutional networks are widely used in skeleton-based action recognition. However, these methods ignore the difference between main participant and subordinate participant, as well as the consistency and causality reasoning in human–human interactive actions. In this paper, we construct a novel Participants-based Synchronous Optimization Network (PSONet). Firstly, we construct main participant branch, subordination participant branch and relative movements branch for the individual and interactive information of participants. Secondly, in the training process, Participants-based Synchronous Response (PSR) loss is constructed to optimize our network. Online mutual response mechanism in PSR regulates the consistency and captures the causality between the main participant action and the overall interactive action. Joint cross-entropy loss in PSR is used to constrain the action instances with individual and interactive action information. Finally, Representative Temporal Enhanced (RTE) block is proposed to complement representative temporal aggregation features and enhance the spatial modeling of representative temporal frames. Experiments have been conducted on the NTU RGB+D 60 dataset and the NTU RGB+D 120 dataset, which have verified that PSONet outperforms state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523003203",
    "keywords": [
      "Action (physics)",
      "Artificial intelligence",
      "Causality (physics)",
      "Computer science",
      "Consistency (knowledge bases)",
      "Construct (python library)",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Physics",
      "Programming language",
      "Quantum mechanics",
      "RGB color model"
    ],
    "authors": [
      {
        "surname": "Zhuang",
        "given_name": "Danfeng"
      },
      {
        "surname": "Jiang",
        "given_name": "Min"
      },
      {
        "surname": "Kong",
        "given_name": "Jun"
      }
    ]
  },
  {
    "title": "PRIME: Posterior Reconstruction of the Input for Model Explanations",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.11.009",
    "abstract": "Artificial intelligence (AI) has developed rapidly in the past few decades, but the corresponding explaining techniques do not keep up with the pace of development. This state of low interpretability leads developers to improve model performance by continuously increasing the use of features when training machine learning models, but it is difficult for them to judge the contribution of features to the prediction. Therefore, it is particularly important to evaluate features. Most of the existing research on model explaining focuses on characterizing the output space and ignored the research on the input space. In this paper, we propose a model-agnostic method to evaluate the feature importance by analyzing the change in the input space. The motivation is to come up with a flexible, general, and computationally efficient method of model explaining. The experiments on the synthetic datasets, standard datasets, and industrial dataset verify the effectiveness of our algorithm. At the same time, we compare our methods to the classic and late model explaining methods, such as SHAP, LIME, and Marginal Contribution Feature Importance (MCI) to further demonstrate the feasibility of our method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523003197",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Data mining",
      "Feature (linguistics)",
      "Feature vector",
      "Geodesy",
      "Geography",
      "Interpretability",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Operating system",
      "Pace",
      "Philosophy",
      "Prime (order theory)",
      "Space (punctuation)"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Yaqi"
      },
      {
        "surname": "Zhao",
        "given_name": "Yang"
      },
      {
        "surname": "Cheng",
        "given_name": "Yuan"
      }
    ]
  },
  {
    "title": "Feature fusion network for long-tailed visual recognition",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109827",
    "abstract": "Deep learning has achieved remarkable success in recent years; however, deep learning methods face significant challenges on long-tailed datasets, which are prevalent in real-world scenarios. In a long-tailed dataset, there are many more samples in the head classes than in the tail classes, and this class imbalance makes it difficult to learn a good feature representation for both head and tail classes simultaneously, particularly when using a single-stage method. Although the existing two-stage methods can alleviate the problem of single-stage methods not performing well on the tail classes by classifier retraining in the second stage, this does not resolve the problem of insufficient learning of head and tail features. Thus, in this paper, we propose a two-stage feature fusion network (FFN). The proposed FFN addresses this issue using one network for the head classes and another network for the tail classes, each of which is trained with a different loss function. This allows the feature learning module to effectively distinguish between the head and tail classes in the embedding space. The classifier learning module fuses the features obtained from the feature learning module, and the classifier is fine-tuned to classify the input images. Different from traditional two-stage methods, the proposed utilizes different loss functions for the head and tail classes; thus, the classifier can achieve balanced results between the head and tail classes. We conduct extensive experiments on three benchmark datasets comparing the proposed FFN with six state-of-the-art methods including two baseline methods, the experimental results demonstrate that the FFN achieves significant improvement on all three benchmark datasets. The code is publicly available at https://github.com/zxsong999/Feature-Fusion-Network.pytorch.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005253",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Embedding",
      "Feature (linguistics)",
      "Feature learning",
      "Feature vector",
      "Linguistics",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Xuesong"
      },
      {
        "surname": "Zhai",
        "given_name": "Junhai"
      },
      {
        "surname": "Cao",
        "given_name": "Yang"
      }
    ]
  },
  {
    "title": "Structure-preserving image translation for multi-source medical image domain adaptation",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109840",
    "abstract": "Domain adaptation is an important task for medical image analysis to improve generalization on datasets collected from diverse institutes using different scanners and protocols. For images with visible domain shift, using image translation models is an intuitive and effective way to perform domain adaptation, but the structure of the generated image may often be distorted when large content discrepancies between domains exist; resulting in poor downstream task performance. To address this, we propose a novel image translation model that disentangles structure and texture to only transfer the latter by using mutual information and texture co-occurrence losses. We translate source domain images to the target domain and employ the generated results as augmented samples for domain adaptation segmentation training. We evaluate our method on three public segmentation datasets: MMWHS, Fundus, and Prostate datasets acquired from diverse institutes. Experimental results show that a segmentation model trained using the augmented images from our approach outperforms state-of-the-art domain adaptation, image translation, and domain generalization methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005381",
    "keywords": [
      "Adaptation (eye)",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Domain (mathematical analysis)",
      "Economics",
      "Gene",
      "Generalization",
      "Image (mathematics)",
      "Image segmentation",
      "Image translation",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Messenger RNA",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Segmentation",
      "Similarity (geometry)",
      "Task (project management)",
      "Transfer of learning",
      "Translation (biology)"
    ],
    "authors": [
      {
        "surname": "Kang",
        "given_name": "Myeongkyun"
      },
      {
        "surname": "Chikontwe",
        "given_name": "Philip"
      },
      {
        "surname": "Won",
        "given_name": "Dongkyu"
      },
      {
        "surname": "Luna",
        "given_name": "Miguel"
      },
      {
        "surname": "Park",
        "given_name": "Sang Hyun"
      }
    ]
  },
  {
    "title": "Graph-based pattern recognition on spectral reduced graphs",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109859",
    "abstract": "Graph-based pattern recognition – in particular in conjunction with large graphs – is often computationally expensive. This hampers, or makes it at least challenging, to employ graph-based representations for real-world data. To address this issue, we propose a method for reducing the size of the underlying graphs to their most important substructures using spectral graph clustering. The proposed method partitions the nodes of the graphs into clusters and then merges each cluster into supernodes. The motivation of this procedure is to reduce the computational cost of any graph comparison algorithm while maintaining the accuracy of the final classification. To assess the benefits and limitations of our method, we conduct thorough experiments on nine real-world datasets with different levels of graph reductions. The classification is obtained by four different graph classifiers (viz. a KNN based on graph edit distance, two SVMs based on a shortest path graph and a Weisfeiler–Lehman graph kernel, as well as a graph neural network). The results indicate that we can reduce computation time by up to two orders of magnitude without substantially degrading the classification accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005575",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computation",
      "Computer science",
      "Graph",
      "Graph kernel",
      "Kernel method",
      "Pattern recognition (psychology)",
      "Radial basis function kernel",
      "Support vector machine",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Gillioz",
        "given_name": "Anthony"
      },
      {
        "surname": "Riesen",
        "given_name": "Kaspar"
      }
    ]
  },
  {
    "title": "Multi-view clustering via efficient representation learning with anchors",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2023.109860",
    "abstract": "Multi-view spectral clustering has gained considerable attention due to its potential to enhance clustering performance. Although many methods have shown promising results, they often suffer from high time complexity and are not suitable for large-scale datasets. On the other hand, anchor-based methods are well-known for their efficiency. These methods typically learn the similarity relationship between instances and anchors and then convert it into the similarity relationship between instances, involving a considerable number of calculations. To address this issue, we propose a novel method called Multi-view clustering via Efficient Representation LearnIng with aNchors (MERLIN) in this paper. Instead of learning the instance–instance relationship, MERLIN approaches the clustering problem from the perspective of representation learning. Specifically, MERLIN selects the same anchors for different views and utilizes these anchors to learn a consensus representation that integrates information from all views. Additionally, MERLIN adaptively learns weights for different views to fully exploit the complementary information among multiple views. In comparison with seven state-of-the-art baseline methods across five datasets, MERLIN demonstrates both efficiency and effectiveness in handling multi-view datasets and is suitable for handling large-scale datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005587",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Computer security",
      "Data mining",
      "Exploit",
      "Feature (linguistics)",
      "Feature learning",
      "Image (mathematics)",
      "Law",
      "Linguistics",
      "Machine learning",
      "Perspective (graphical)",
      "Philosophy",
      "Physics",
      "Political science",
      "Politics",
      "Quantum mechanics",
      "Representation (politics)",
      "Scale (ratio)",
      "Similarity (geometry)",
      "Spectral clustering"
    ],
    "authors": [
      {
        "surname": "Yu",
        "given_name": "Xiao"
      },
      {
        "surname": "Liu",
        "given_name": "Hui"
      },
      {
        "surname": "Zhang",
        "given_name": "Yan"
      },
      {
        "surname": "Sun",
        "given_name": "Shanbao"
      },
      {
        "surname": "Zhang",
        "given_name": "Caiming"
      }
    ]
  },
  {
    "title": "Exposing fake images generated by text-to-image diffusion models",
    "journal": "Pattern Recognition Letters",
    "year": "2023",
    "doi": "10.1016/j.patrec.2023.10.021",
    "abstract": "Text-to-image diffusion models (DM) have posed unprecedented challenges to the authenticity and integrity of digital images, which makes the detection of computer-generated images one of the most important image forensics techniques. However, the detection of images generated by text-to-image diffusion models is rarely reported in the literature. To tackle this issue, we first analyze the acquisition process of DM images. Then, we construct a hybrid neural network based on attention-guided feature extraction (AGFE) and vision transformers (ViTs)-based feature extraction (ViTFE) modules. An attention mechanism is adopted in the AGFE module to capture long-range feature interactions and boost the representation capability. ViTFE module containing sequential MobileNetv2 block (MNV2) and MobileViT blocks are designed to learn global representations. By conducting extensive experiments on different types of generated images, the results demonstrate the effectiveness and robustness of our method in exposing fake images generated by text-to-image diffusion models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523002933",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Block (permutation group theory)",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Digital image",
      "Feature (linguistics)",
      "Feature extraction",
      "Gene",
      "Geometry",
      "Image (mathematics)",
      "Image processing",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Qiang"
      },
      {
        "surname": "Wang",
        "given_name": "Hao"
      },
      {
        "surname": "Meng",
        "given_name": "Laijin"
      },
      {
        "surname": "Mi",
        "given_name": "Zhongjie"
      },
      {
        "surname": "Yuan",
        "given_name": "Jianye"
      },
      {
        "surname": "Yan",
        "given_name": "Hong"
      }
    ]
  }
]