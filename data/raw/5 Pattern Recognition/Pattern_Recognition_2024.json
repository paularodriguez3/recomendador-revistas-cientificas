[
  {
    "title": "Riemannian SPD learning to represent and characterize fixational oculomotor Parkinsonian abnormalities",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2023.09.012",
    "abstract": "Parkinson’s disease (PD) is the second most common neurodegenerative disorder, mainly characterized by motor alterations. Despite multiple efforts, there is no definitive biomarker to diagnose, quantify, and characterize the disease early. Recently, abnormal fixational oculomotor patterns have emerged as a promising disease biomarker with high sensitivity, even at early stages. Nonetheless, the complex patterns and potential correlations with the disease remain largely unexplored, among others, because of the limitations of standard setups that only analyze coarse measures and poorly exploit the associated PD alterations. This work introduces a new strategy to represent, analyze and characterize fixational patterns from non-invasive video analysis, adjusting a geometric learning strategy. A deep Riemannian framework is proposed to discover potential oculomotor patterns aimed at withstanding data scarcity and geometrically interpreting the latent space. A convolutional representation is first built, then aggregated onto a symmetric positive definite matrix (SPD). The latter encodes second-order statistics of deep convolutional features and feeds a non-linear hierarchical architecture that processes SPD data by maintaining them into their Riemannian manifold. The complete representation discriminates between Parkinson and Healthy (Control) fixational observations, even at PD stages 2.5 and 3. Besides, the proposed geometrical representation exhibit capabilities to statistically differentiate observations among Parkinson’s stages. The developed tool demonstrates coherent results from explainability maps back-propagated from output probabilities.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552300260X",
    "keywords": [
      "Artificial intelligence",
      "Cognitive psychology",
      "Computer science",
      "Convolutional neural network",
      "Disease",
      "Law",
      "Machine learning",
      "Medicine",
      "Neuroscience",
      "Parkinson's disease",
      "Pathology",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Psychology",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Olmos",
        "given_name": "Juan"
      },
      {
        "surname": "Manzanera",
        "given_name": "Antoine"
      },
      {
        "surname": "Martínez",
        "given_name": "Fabio"
      }
    ]
  },
  {
    "title": "Clustered FedStack: Intermediate Global Models with Bayesian Information Criterion",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2023.12.004",
    "abstract": "Federated Learning (FL) is currently one of the most popular technologies in the field of Artificial Intelligence (AI) due to its collaborative learning and ability to preserve client privacy. However, it faces challenges such as non-identically and non-independently distributed (non-IID) data with imbalanced labels among local clients. To address these limitations, the research community has explored various approaches such as using local model parameters, federated generative adversarial learning, and federated representation learning. In our study, we propose a novel Clustered FedStack framework based on the previously published Stacked Federated Learning (FedStack) framework. Here, the local clients send their model predictions and output layer weights to a server, which then builds a robust global model. This global model clusters the local clients based on their output layer weights using a clustering mechanism. We adopt three clustering mechanisms, namely K-Means, Agglomerative, and Gaussian Mixture Models, into the framework and evaluate their performance. Bayesian Information Criterion (BIC) is used with the maximum likelihood function to determine the number of clusters. Our results show that Clustered FedStack models outperform baseline models with clustering mechanisms. To estimate the convergence of our proposed framework, we use Cyclical learning rates.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523003513",
    "keywords": [
      "Artificial intelligence",
      "Bayesian probability",
      "Cluster analysis",
      "Computer science",
      "Convergence (economics)",
      "Data mining",
      "Economic growth",
      "Economics",
      "Field (mathematics)",
      "Independent and identically distributed random variables",
      "Law",
      "Machine learning",
      "Mathematics",
      "Mixture model",
      "Political science",
      "Politics",
      "Pure mathematics",
      "Random variable",
      "Representation (politics)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Shaik",
        "given_name": "Thanveer"
      },
      {
        "surname": "Tao",
        "given_name": "Xiaohui"
      },
      {
        "surname": "Li",
        "given_name": "Lin"
      },
      {
        "surname": "Higgins",
        "given_name": "Niall"
      },
      {
        "surname": "Gururajan",
        "given_name": "Raj"
      },
      {
        "surname": "Zhou",
        "given_name": "Xujuan"
      },
      {
        "surname": "Yong",
        "given_name": "Jianming"
      }
    ]
  },
  {
    "title": "Dynamic contrastive learning guided by class confidence and confusion degree for medical image segmentation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109881",
    "abstract": "This work proposes an intra-Class-confidence and inter-Class-confusion guided Dynamic Contrastive (CCDC) learning framework for medical image segmentation. A core contribution is to dynamically select the most expressive pixels to build positive and negative pairs for contrastive learning at different training phases. For the positive pairs, dynamically adaptive sampling strategies are introduced for sampling different sets of pixels based on their hardness (namely the easiest, easy, and hard pixels). For the negative pairs, to efficiently learn from the classes with high confusion degree w.r.t a query class (i.e., a class containing the query pixels), a new hard class mining strategy is presented. Furthermore, pixel-level representations are extended to the neighbourhood region to leverage the spatial consistency of adjacent pixels. Extensive experiments on the three public datasets demonstrate that the proposed method significantly surpasses the state-of-the-art.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005794",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Computer vision",
      "Confusion",
      "Consistency (knowledge bases)",
      "Image segmentation",
      "Leverage (statistics)",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Pixel",
      "Psychoanalysis",
      "Psychology",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Jingkun"
      },
      {
        "surname": "Chen",
        "given_name": "Changrui"
      },
      {
        "surname": "Huang",
        "given_name": "Wenjian"
      },
      {
        "surname": "Zhang",
        "given_name": "Jianguo"
      },
      {
        "surname": "Debattista",
        "given_name": "Kurt"
      },
      {
        "surname": "Han",
        "given_name": "Jungong"
      }
    ]
  },
  {
    "title": "Transfer easy to hard: Adversarial contrastive feature learning for unsupervised person re-identification",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109973",
    "abstract": "Unsupervised Person Re-Identification (Re-ID) is challenging due to the lack of ground-truth labels. Most existing methods address this problem by progressively mining high-confidence pseudo labels to guide the feature learning process. However, how to construct hard-enough samples while maintaining the fidelity of pseudo labels in these samples remains an open issue in the machine learning community. To tackle this challenge, we design a simple yet effective adversarial contrastive feature learning (ACFL) framework, which enhances the discriminative capability of features by introducing more transformed hard samples in the feature learning process. Specifically, it mainly consists of a discriminative feature learning module and a hard sample generation module. The discriminative feature learning module extracts recognizable features of unlabeled training samples to estimate the high-confidence relationship between samples. Then, the hard sample generation module utilizes these high-confidence relationships between samples to transfer all samples into the hard ones via an adversarial learning strategy. Finally, the generated hard samples are further fed into DFL to learn discriminative features for person Re-ID. Extensive experiments on Market-1501, DukeMTMC-reID, and MSMT17 datasets show that our method compares favorably with state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323006714",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Computer science",
      "Construct (python library)",
      "Discriminative model",
      "Feature (linguistics)",
      "Feature learning",
      "Identification (biology)",
      "Linguistics",
      "Machine learning",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Process (computing)",
      "Programming language",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Ji",
        "given_name": "Haoxuanye"
      },
      {
        "surname": "Wang",
        "given_name": "Le"
      },
      {
        "surname": "Zhou",
        "given_name": "Sanping"
      },
      {
        "surname": "Tang",
        "given_name": "Wei"
      },
      {
        "surname": "Zheng",
        "given_name": "Nanning"
      },
      {
        "surname": "Hua",
        "given_name": "Gang"
      }
    ]
  },
  {
    "title": "QBER: Quantum-based Entropic Representations for un-attributed graphs",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109877",
    "abstract": "In this paper, we propose a novel framework of computing the Quantum-based Entropic Representations (QBER) for un-attributed graphs, through the Continuous-time Quantum Walk (CTQW). To achieve this, we commence by transforming each original graph into a family of k -level neighborhood graphs, where each k -level neighborhood graph encapsulates the connected information between each vertex and its k -hop neighbor vertices, providing a fine representation to reflect the multi-level topological information for the original global graph structure. To further capture the complicated structural characteristics of the original graph through its neighborhood graphs, we propose to characterize the structure of each neighborhood graph with the Average Mixing Matrix (AMM) of the CTQW, that encapsulates the time-averaged behavior of the CTQW evolved on the neighborhood graph. More specifically, we show how the AMM matrix allows us to compute a Quantum Shannon Entropy for each vertex, and thus compute an entropic signature for each neighborhood graph by measuring the averaged value or the Jensen–Shannon Divergence between the entropies of its vertices. For each original graph, the resulting QBER is defined by gauging how the entropic signat ures vary on its k -level neighborhood graphs with increasing k , reflecting the multi-dimensional entropy-based structure information of the original graph. Experiments on standard graph datasets demonstrate the effectiveness of the proposed QBER approach in terms of the classification accuracies. The proposed approach can significantly outperform state-of-the-art entropic complexity measuring methods, graph kernel methods, as well as graph deep learning methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005757",
    "keywords": [
      "Combinatorics",
      "Computer science",
      "Discrete mathematics",
      "Graph",
      "Graph property",
      "Line graph",
      "Mathematics",
      "Theoretical computer science",
      "Vertex (graph theory)",
      "Voltage graph"
    ],
    "authors": [
      {
        "surname": "Cui",
        "given_name": "Lixin"
      },
      {
        "surname": "Li",
        "given_name": "Ming"
      },
      {
        "surname": "Bai",
        "given_name": "Lu"
      },
      {
        "surname": "Wang",
        "given_name": "Yue"
      },
      {
        "surname": "Li",
        "given_name": "Jing"
      },
      {
        "surname": "Wang",
        "given_name": "Yanchao"
      },
      {
        "surname": "Li",
        "given_name": "Zhao"
      },
      {
        "surname": "Chen",
        "given_name": "Yunwen"
      },
      {
        "surname": "Hancock",
        "given_name": "Edwin R."
      }
    ]
  },
  {
    "title": "Customization of latent space in semi-supervised Variational AutoEncoder",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2023.11.018",
    "abstract": "We propose a novel semi-supervised learning method of Variational AutoEncoder (VAE), which yields a customized latent space through our EXplainable encoder Network (EXoN). The customization involves a manual design of the interpolation and structural constraint, such as proximity, which enhances the interpretability of the latent space. To improve the classification performance, we introduce a new semi-supervised classification method called SCI (Soft-label Consistency Interpolation). Combining the classification loss and the Kullback–Leibler divergence is crucial in constructing an explainable latent space. Additionally, the variability of the generated samples is determined by an active latent subspace, which effectively captures distinctive characteristics. We conduct experiments using the MNIST, SVHN, and CIFAR-10 datasets, and the results demonstrate that our approach yields an explainable latent space while significantly reducing the effort required to analyze representation patterns within the latent space.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523003288",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Autoencoder",
      "Computer science",
      "Constraint (computer-aided design)",
      "Divergence (linguistics)",
      "Geometry",
      "Interpolation (computer graphics)",
      "Interpretability",
      "Latent variable",
      "Law",
      "Linguistics",
      "MNIST database",
      "Machine learning",
      "Mathematics",
      "Motion (physics)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Space (punctuation)",
      "Subspace topology"
    ],
    "authors": [
      {
        "surname": "An",
        "given_name": "Seunghwan"
      },
      {
        "surname": "Jeon",
        "given_name": "Jong-June"
      }
    ]
  },
  {
    "title": "Efficient Supervised Graph Embedding Hashing for large-scale cross-media retrieval",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109934",
    "abstract": "Recently, graph based hashing has gained much attention due to its effectiveness in multi-media retrieval. Although several graph embedding based works have been designed and achieved promising performance, there are still some issues that need to be further studied, including, (1) one significant drawback of graph embedding is its expensive memory and computation cost caused by the graph Laplacian matrix; (2) most pioneer works fail to fully explore the available class labels in training procedure, which generally makes them suffer from unsatisfactory retrieval performance. To overcome these drawbacks, we propose a simple yet effective supervised cross-media hashing scheme, termed Efficient Supervised Graph Embedding Hashing (ESGEH), which can simultaneously learn hash functions and discrete binary codes efficiently. Specifically, ESGEH leverages both class label based semantic embedding and graph embedding to generate a sharing semantic subspace, and class labels are also incorporated to minimize the quantization error for better approximating the generated binary codes. In order to reduce the computational sources, a well-designed intermediate terms decomposition is proposed to avoid explicitly computing the graph Laplacian matrix. Finally, an iterative discrete optimal algorithm is derived to solve above problem, and each subproblem can yield a closed-form solution. Extensive experimental results on four public datasets demonstrate the superiority of the proposed approach over several existing cross-media hashing methods in terms of both accuracy and efficiency.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323006325",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Embedding",
      "Graph",
      "Graph embedding",
      "Hash function",
      "Laplacian matrix",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Yao",
        "given_name": "Tao"
      },
      {
        "surname": "Wang",
        "given_name": "Ruxin"
      },
      {
        "surname": "Wang",
        "given_name": "Jintao"
      },
      {
        "surname": "Li",
        "given_name": "Ying"
      },
      {
        "surname": "Yue",
        "given_name": "Jun"
      },
      {
        "surname": "Yan",
        "given_name": "Lianshan"
      },
      {
        "surname": "Tian",
        "given_name": "Qi"
      }
    ]
  },
  {
    "title": "Self-supervised knowledge distillation in counterfactual learning for VQA",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2023.11.024",
    "abstract": "As a popular cross-modal reasoning task, Visual Question Answering (VQA) has achieved great progress in recent years. However, the issue of language bias has always affected the reliability of VQA models. To address this problem, counterfactual learning methods are proposed to learn more robust features to mitigate the bias problem. However, current counterfactual learning approaches mainly focus on generating synthesized samples and assigning answers to them, neglecting the relationship between factual and original data, which hinders robust feature learning for effective reasoning. To overcome this limitation, we propose a Self-supervised Knowledge Distillation approach in Counterfactual Learning for VQA, dubbed as VQA-SkdCL, which utilizes a self-supervised constraint to make good use of the hidden knowledge in the factual samples, enhancing the robustness of VQA models. We demonstrate the effectiveness of the proposed approach on VQA v2, VQA-CP v1, and VQA-CP v2 datasets and our approach achieves excellent performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523003331",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Counterfactual thinking",
      "Feature (linguistics)",
      "Gene",
      "Linguistics",
      "Machine learning",
      "Philosophy",
      "Psychology",
      "Question answering",
      "Robustness (evolution)",
      "Social psychology"
    ],
    "authors": [
      {
        "surname": "Bi",
        "given_name": "Yandong"
      },
      {
        "surname": "Jiang",
        "given_name": "Huajie"
      },
      {
        "surname": "Zhang",
        "given_name": "Hanfu"
      },
      {
        "surname": "Hu",
        "given_name": "Yongli"
      },
      {
        "surname": "Yin",
        "given_name": "Baocai"
      }
    ]
  },
  {
    "title": "Auto-adjustable hypergraph regularized non-negative matrix factorization for image clustering",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109963",
    "abstract": "Non-negative matrix factorization (NMF) is an effective method for image clustering. However, relatively fixed graph regularization terms and loss functions have been adopted by recently proposed variants of NMF, and their clustering performance can be improved by incorporating configurable parameters. In this paper, an auto-adjustable hypergraph regularized non-negative matrix factorization (AHRNMF) algorithm was proposed. In the AHRNMF framework, we proposed a piecewise loss function and an innovative auto-adjustable hypergraph. The loss function incorporates two adaptive parameters, harmonizing reconstruction error and anti-outlier efficacy. Hypergraph construction relies on the calculation of two k-nearest neighbors (KNN) with different scales. Furthermore, an KNN-based algorithm was developed to assist AHRNMF in achieving auto-adjustment, which can automatically detect outliers without determining the number of clusters in advance. It was demonstrated by extensive experiments that the proposed AHRNMF outperforms other state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323006611",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Discrete mathematics",
      "Eigenvalues and eigenvectors",
      "Hypergraph",
      "Mathematical analysis",
      "Mathematics",
      "Matrix decomposition",
      "Non-negative matrix factorization",
      "Outlier",
      "Pattern recognition (psychology)",
      "Physics",
      "Piecewise",
      "Quantum mechanics",
      "Regularization (linguistics)"
    ],
    "authors": [
      {
        "surname": "Zuo",
        "given_name": "Hongliang"
      },
      {
        "surname": "Li",
        "given_name": "Shuo"
      },
      {
        "surname": "Liang",
        "given_name": "Cong"
      },
      {
        "surname": "Li",
        "given_name": "Juntao"
      }
    ]
  },
  {
    "title": "Compositional Zero-Shot Learning using Multi-Branch Graph Convolution and Cross-layer Knowledge Sharing",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109916",
    "abstract": "The purpose of the Compositional Zero-Shot Learning (CZSL) is to recognize new state-object compositions of known objects and known states. For example, the CZSL model should recognize young cat when the model has seen images of a few state-object compositions like young tiger, old tiger and old cat. The visual features of a state may have significant variation across different compositions of the state with different objects. For example, in the compositions peeled apple and peeled orange, the state peeled has different visual features. This context dependency of state features is difficult to learn from the annotated images of different compositions. We propose a Graph Convolutional Network (GCN) with two distinct branches for object and state recognition. GCN utilizes its ability to aggregate features from the non-Euclidean neighbourhood. This aggregation ability of GCN can help our model to capture the intricate dependencies between visual features of state and object. We also propose a novel cross-layer knowledge sharing strategy for the purpose of reducing ambiguity in learning state features due to context dependency. The proposed cross-layer knowledge sharing helps in identifying a set of objects having feasible compositions with a particular state and thereby reducing the ambiguity in the state features. Finally, we propose a feasibility based penalization to better regularize the joint prediction from the two branches of the network. The proposed algorithm is evaluated on the challenging benchmarks and competitive results in comparison to state-of-the-art algorithms have been achieved.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323006143",
    "keywords": [
      "Ambiguity",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Graph",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Programming language",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Panda",
        "given_name": "Aditya"
      },
      {
        "surname": "Mukherjee",
        "given_name": "Dipti Prasad"
      }
    ]
  },
  {
    "title": "Abductive natural language inference by interactive model with structural loss",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2023.11.007",
    "abstract": "The abductive natural language inference task ( α NLI) is proposed to infer the most plausible explanation between the cause and the event. In the α NLI task, two observations are given, and the most plausible hypothesis is asked to pick out from the candidates. Existing methods model the relation between each candidate hypothesis separately and penalize the inference network uniformly. In this paper, we argue that it is unnecessary to distinguish the reasoning abilities among correct hypotheses; and similarly, all wrong hypotheses contribute the same when explaining the reasons of the observations. Therefore, we propose to group instead of ranking the hypotheses and design a structural loss called “joint softmax focal loss” in this paper. Based on the observation that the hypotheses are generally semantically related, we design a novel interactive language model aiming at exploiting the rich interaction among competing hypotheses. We name this new model for α NLI: Interactive Model with Structural Loss (IMSL). The experimental results show that our IMSL has achieved the highest performance on the RoBERTa-large pretrained model, with ACC and AUC results increased by about 1% and 5% respectively. We also compared the performance in terms of precision and sensitivity with publicly available code, demonstrating the efficiency and robustness of the proposed approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523003173",
    "keywords": [
      "Abductive reasoning",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Deep learning",
      "Economics",
      "Gene",
      "Inference",
      "Machine learning",
      "Management",
      "Natural language",
      "Natural language processing",
      "Ranking (information retrieval)",
      "Robustness (evolution)",
      "Softmax function",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Linhao"
      },
      {
        "surname": "Wang",
        "given_name": "Ao"
      },
      {
        "surname": "Xu",
        "given_name": "Ming"
      },
      {
        "surname": "Dong",
        "given_name": "Yongfeng"
      },
      {
        "surname": "Li",
        "given_name": "Xin"
      }
    ]
  },
  {
    "title": "SilentTrig: An imperceptible backdoor attack against speaker identification with hidden triggers",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2023.12.002",
    "abstract": "Speaker identification based on deep learning is known to be susceptible to backdoor attacks. However, the current research on audio backdoor attacks is limited, and these attacks often use obvious noises as triggers, which can raise suspicion among users. In this paper, we introduce SilentTrig, a novel and imperceptible backdoor attack method targeted at speaker identification. Our approach involves utilizing an optimized steganographic network to embed triggers into benign audio samples and implementing a two-stage adversarial optimization process. This ensures that the poisoned samples are acoustically indistinguishable from their benign counterparts, resulting in a substantially improved attack imperceptibility. We evaluate SilentTrig on two datasets and four state-of-the-art models. The results demonstrate a high Attack Success Rate (ASR) of up to 99.2%, a Just Noticeable Difference (JND) of only 0.3, and resistance to typical defense methods such as Neural Cleanse and Fine-Pruning.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523003495",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Backdoor",
      "Biology",
      "Botany",
      "Computer science",
      "Computer security",
      "Identification (biology)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Process (computing)",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Tang",
        "given_name": "Yu"
      },
      {
        "surname": "Sun",
        "given_name": "Lijuan"
      },
      {
        "surname": "Xu",
        "given_name": "Xiaolong"
      }
    ]
  },
  {
    "title": "Saliency-guided stairs detection on wearable RGB-D devices for visually impaired persons with Swin-Transformer",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2023.11.022",
    "abstract": "Accuracy stairs detection is crucial for people with visual impairment, as it can reduce the potential unforeseen risks of falling on stairs. Wearable RGB-D technology can assist blind and visually impaired individuals. However, existing stair detection algorithms on RGB-D images face difficulties in the stair material, texture, lighting, and direction. In this study, we proposed a saliency-guided stairs detection method based on Swin-Transformer to address the challenges mentioned above. First, saliency detection based on RGB-D images is used to learn spatial information for fast stair localization. Furthermore, we use the Swin-Transformer that incorporates key depth features of the stairs to solve orientation detection deficiencies. To evaluate the performance of our proposed method, we collected 3,290 RGB-D images, including the indoor and outdoor staircases. Experiments on our dataset show that our method can achieve high performance in terms of detection accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552300332X",
    "keywords": [
      "Artificial intelligence",
      "Civil engineering",
      "Computer science",
      "Computer vision",
      "Electrical engineering",
      "Embedded system",
      "Engineering",
      "RGB color model",
      "Stairs",
      "Transformer",
      "Voltage",
      "Wearable computer"
    ],
    "authors": [
      {
        "surname": "Zheng",
        "given_name": "Zhuowen"
      },
      {
        "surname": "He",
        "given_name": "Jiahui"
      },
      {
        "surname": "Gu",
        "given_name": "Jia"
      },
      {
        "surname": "Chen",
        "given_name": "Zhen"
      },
      {
        "surname": "Qin",
        "given_name": "Wenjian"
      }
    ]
  },
  {
    "title": "Gradient-based multi-label feature selection considering three-way variable interaction",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109900",
    "abstract": "Nowadays, Multi-Label Feature Selection (MLFS) attracts more and more attention to tackle the high-dimensional problem in multi-label data. A key characteristic of existing gradient-based MLFS methods is that they typically consider two-way variable correlations between features and labels, including feature-feature and label-label correlations. However, two-way correlations are not sufficient to steer feature selection since such correlations vary given different additional variables in practical scenarios, which leads to the selected features with relatively-poor classification performance. Motivated by this, we capture three-way variable interactions including feature-feature-label and feature-label-label interactions to further characterize the fluctuating correlations in the context of another variable, and propose a new gradient-based MLFS approach incorporating the above three-way variable interactions into a global optimization objective. Specifically, based on information theory, we develop second-order regularization penalty terms to regard three-way interactions while jointly combining with the main loss term in regard to feature relevance. Then the objective function can be efficiently optimized via a block-coordinate gradient descent schema. Meanwhile, we provide a theoretical analysis demonstrating the effectiveness of the regularization terms in exploiting three-way interaction. In addition, experiments conducted on a series of benchmark data sets also verify the validity of the proposed method on multiple evaluation metrics.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005988",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Data mining",
      "Feature (linguistics)",
      "Feature selection",
      "Gradient descent",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Regularization (linguistics)",
      "Variable (mathematics)"
    ],
    "authors": [
      {
        "surname": "Zou",
        "given_name": "Yizhang"
      },
      {
        "surname": "Hu",
        "given_name": "Xuegang"
      },
      {
        "surname": "Li",
        "given_name": "Peipei"
      }
    ]
  },
  {
    "title": "Depth Map Denoising Network and Lightweight Fusion Network for Enhanced 3D Face Recognition",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109936",
    "abstract": "With the increasing availability of consumer depth sensors, 3D face recognition (FR) has attracted more and more attention. However, the data acquired by these sensors are often coarse and noisy, making them impractical to use directly. In this paper, we introduce an innovative Depth map denoising network (DMDNet) based on the Denoising Implicit Image Function (DIIF) to reduce noise and enhance the quality of facial depth images for low-quality 3D FR. After generating clean depth faces using DMDNet, we further design a powerful recognition network called Lightweight Depth and Normal Fusion network (LDNFNet), which incorporates a multi-branch fusion block to learn unique and complementary features between different modalities such as depth and normal images. Comprehensive experiments conducted on four distinct low-quality databases demonstrate the effectiveness and robustness of our proposed methods. Furthermore, when combining DMDNet and LDNFNet, we achieve state-of-the-art results on the Lock3DFace database.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323006349",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Depth map",
      "Face (sociological concept)",
      "Facial recognition system",
      "Fusion",
      "Gene",
      "Image (mathematics)",
      "Linguistics",
      "Noise (video)",
      "Noise reduction",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Robustness (evolution)",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Ruizhuo"
      },
      {
        "surname": "Wang",
        "given_name": "Ke"
      },
      {
        "surname": "Deng",
        "given_name": "Chao"
      },
      {
        "surname": "Wang",
        "given_name": "Mei"
      },
      {
        "surname": "Chen",
        "given_name": "Xi"
      },
      {
        "surname": "Huang",
        "given_name": "Wenhui"
      },
      {
        "surname": "Feng",
        "given_name": "Junlan"
      },
      {
        "surname": "Deng",
        "given_name": "Weihong"
      }
    ]
  },
  {
    "title": "Efficient disentangled representation learning for multi-modal finger biometrics",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109944",
    "abstract": "Most multi-modal biometric systems use multiple devices to capture different traits and directly fuse multi-modal data while ignoring correlation information between modalities. In this paper, finger skin and finger vein images are acquired from the same region of the finger and therefore have a higher correlation. To represent data efficiently, we propose a novel Finger Disentangled Representation Learning Framework (FDRL-Net) that is based on a factorization concept, which disentangles each modality into shared and private features, thereby improving complementarity for better fusion and extracting modality-invariant features for heterogeneous recognition. Besides, to capture as much finger texture as possible, we utilize three-view finger images to reconstruct full-view multi-spectral finger traits, which increases the identity information and the robustness to finger posture variation. Finally, a Boat-Trackers-based multi-task distillation method is proposed to migrate the feature representation ability to a lightweight multi-task network. Extensive experiments on six single-view multi-spectral finger datasets and two full-view multi-spectral finger datasets demonstrate the effectiveness of our approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323006428",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Biometrics",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Feature learning",
      "Gene",
      "Invariant (physics)",
      "Mathematical physics",
      "Mathematics",
      "Modal",
      "Pattern recognition (psychology)",
      "Polymer chemistry",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Weili"
      },
      {
        "surname": "Huang",
        "given_name": "Junduan"
      },
      {
        "surname": "Luo",
        "given_name": "Dacan"
      },
      {
        "surname": "Kang",
        "given_name": "Wenxiong"
      }
    ]
  },
  {
    "title": "Self-supervised cross-modal visual retrieval from brain activities",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109915",
    "abstract": "We study the problem of restoring visual stimuli from visually-evoked electroencephalography (EEG) signals. Using a supervised classification-then-generation framework, the reconstruction-based approaches learn the mapping between distributions of two modalities but fail to reproduce the exact visual stimulus. Instead, we propose a self-supervised cross-modal retrieval paradigm that seeks instance-level alignment by maximizing the mutual information between the EEG encoding and associated visual stimulus. We demonstrate the threefold advantages of the self-supervised retrieval over supervised reconstruction on the largest visual-evoked EEG dataset with two evaluation protocols. First, it restores the exact visual stimulus without accessing the image class information, which was not possible with previous approaches. Second, it produces more recognizable results than generated ones and bypasses the challenge of training an image generator. Finally, it illustrates the benefits of self-supervision over supervised models in handling open-set data.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323006131",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Electroencephalography",
      "Machine learning",
      "Modal",
      "Modalities",
      "Mutual information",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Polymer chemistry",
      "Psychology",
      "Psychotherapist",
      "Social science",
      "Sociology",
      "Stimulus (psychology)",
      "Visualization"
    ],
    "authors": [
      {
        "surname": "Ye",
        "given_name": "Zesheng"
      },
      {
        "surname": "Yao",
        "given_name": "Lina"
      },
      {
        "surname": "Zhang",
        "given_name": "Yu"
      },
      {
        "surname": "Gustin",
        "given_name": "Sylvia"
      }
    ]
  },
  {
    "title": "Learning geometric consistency and discrepancy for category-level 6D object pose estimation from point clouds",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109896",
    "abstract": "Category-level 6D object pose estimation aims to predict the position and orientation of unseen object instances, which is a fundamental problem in robotic applications. Previous works mainly focused on exploiting visual cues from RGB images, while depth images received less attention. However, depth images contain rich geometric attributes about the object’s shape, which are crucial for inferring the object’s pose. This work achieves category-level 6D object pose estimation by performing sufficient geometric learning from depth images represented by point clouds. Specifically, we present a novel geometric consistency and geometric discrepancy learning framework called CD-Pose to resolve the intra-category variation, inter-category similarity, and objects with complex structures. Our network consists of a Pose-Consistent Module and a Pose-Discrepant Module. First, a simple MLP-based Pose-Consistent Module is utilized to extract geometrically consistent pose features of objects from the pre-computed object shape priors for each category. Then, the Pose-Discrepant Module, designed as a multi-scale region-guided transformer network, is dedicated to exploring each instance’s geometrically discrepant features. Next, the NOCS model of the object is reconstructed according to the integration of consistent and discrepant geometric representations. Finally, 6D object poses are obtained by solving the similarity transformation between the reconstruction and the observed point cloud. Experiments on the benchmark datasets show that our CD-Pose produces superior results to state-of-the-art competitors.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005940",
    "keywords": [
      "3D pose estimation",
      "Artificial intelligence",
      "Cognitive neuroscience of visual object recognition",
      "Computer science",
      "Computer vision",
      "Consistency (knowledge bases)",
      "Image (mathematics)",
      "Mathematics",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Point cloud",
      "Pose",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Zou",
        "given_name": "Lu"
      },
      {
        "surname": "Huang",
        "given_name": "Zhangjin"
      },
      {
        "surname": "Gu",
        "given_name": "Naijie"
      },
      {
        "surname": "Wang",
        "given_name": "Guoping"
      }
    ]
  },
  {
    "title": "Learning consensus-aware semantic knowledge for remote sensing image captioning",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109893",
    "abstract": "Tremendous progresses have been made in remote sensing image captioning (RSIC) task in recent years, yet there still some unresolved problems: (1) facing the gap between the visual features and semantic concepts, (2) reasoning the higher-level relationships between semantic concepts. In this work, we focus on injecting high-level visual-semantic interaction into RSIC model. Firstly, the semantic concept extractor (SCE), end-to-end trainable, precisely captures the semantic concepts contained in the RSIs. In particular, the visual-semantic co-attention (VSCA) is designed to grain coarse concept-related regions and region-related concepts for multi-modal interaction. Furthermore, we incorporate the two types of attentive vectors with semantic-level relational features into a consensus exploitation (CE) block for learning cross-modal consensus-aware knowledge. The experiments on three benchmark data sets show the superiority of our approach compared with the reference methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005915",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Chemistry",
      "Closed captioning",
      "Computer science",
      "Focus (optics)",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Information retrieval",
      "Modal",
      "Natural language processing",
      "Optics",
      "Physics",
      "Polymer chemistry",
      "Programming language",
      "Semantic Web",
      "Semantic computing",
      "Semantic similarity",
      "Semantics (computer science)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Yunpeng"
      },
      {
        "surname": "Zhang",
        "given_name": "Xiangrong"
      },
      {
        "surname": "Cheng",
        "given_name": "Xina"
      },
      {
        "surname": "Tang",
        "given_name": "Xu"
      },
      {
        "surname": "Jiao",
        "given_name": "Licheng"
      }
    ]
  },
  {
    "title": "Hidden classification layers: Enhancing linear separability between classes in neural networks layers",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2023.11.016",
    "abstract": "Many Deep Learning approaches are based on variations of standard multi-layer feed-forward neural networks. These are also referred to as deep networks. The basic idea is that each hidden neural layer accomplishes a data transformation which is expected to make the data representation “somewhat more linearly separable” than the previous one to obtain a final data representation which is as linearly separable as possible. However, determining the optimal network parameters for these transformations is a crucial challenge. In this study, we propose a Deep Neural Network architecture (Hidden Classification Layer, HCL) which induces an error function involving the output values of all the network layers. The proposed architecture leads toward solutions where the data representations in the hidden layers exhibit a higher degree of linear separability between classes compared to conventional methods. While similar approaches have been discussed in prior literature, this paper presents a new architecture with a novel error function and conducts an extensive experimental analysis. Furthermore, the architecture can be easily integrated into existing frameworks by simply adding densely connected layers and making a straightforward adjustment to the loss function to account for the output of the added layers. The experiments focus on image classification tasks using four well-established datasets, employing as baselines three widely recognized architectures in the literature. The findings reveal that the proposed approach consistently enhances accuracy on the test sets across all considered cases.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523003264",
    "keywords": [
      "Algorithm",
      "Architecture",
      "Art",
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Biology",
      "Chemistry",
      "Computer science",
      "Computer security",
      "Data mining",
      "Deep learning",
      "Evolutionary biology",
      "Focus (optics)",
      "Function (biology)",
      "Gene",
      "Law",
      "Layer (electronics)",
      "Linear map",
      "Mathematical analysis",
      "Mathematics",
      "Network architecture",
      "Optics",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Physics",
      "Political science",
      "Politics",
      "Pure mathematics",
      "Representation (politics)",
      "Separable space",
      "Transformation (genetics)",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Apicella",
        "given_name": "Andrea"
      },
      {
        "surname": "Isgrò",
        "given_name": "Francesco"
      },
      {
        "surname": "Prevete",
        "given_name": "Roberto"
      }
    ]
  },
  {
    "title": "MCNet: Magnitude consistency network for domain adaptive object detection under inclement environments",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109947",
    "abstract": "Deep learning-based object detection methods have achieved promising results in normal scenarios, while such methods often fail to locate objects from the disturbed images captured in inclement environments. Most existing methods utilized denoise modules to assist the detection network or exploit prior knowledge to reduce the environment interference remaining in the features, which ignores the essential role of the frequency element. From a novel perspective, we observe that the inclement environment alters the frequency content in the features, which in turn crushes the detection. To tickle this problem, we present the Magnitude Consistency Network (MCNet) to distill the irrelevant contents in the frequency domain. The MCNet is composed of the detection network and the magnitude corrector. The detection network is able to locate and classify objects. However, in inclement environments, the crucial information about the objects of interest is disrupted by the nuisance noise introduced from the inclement environments. The magnitude corrector can recover relevant information about the objects in the frequency domain by distilling the irrelevant factors and refining the affected features. Alternately optimizing the magnitude corrector and the detection network gradually makes the frequency content between the disturbed image and the clear image to be consistent. By distilling the irrelevant noise in the feature, the detection network can learn domain-invariant representations. Extensive experiments prove that the proposed method is effective and outperforms existing methods by a clear margin on four datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323006453",
    "keywords": [
      "Artificial intelligence",
      "Astronomy",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Consistency (knowledge bases)",
      "Exploit",
      "Feature (linguistics)",
      "Frequency domain",
      "Image (mathematics)",
      "Linguistics",
      "Machine learning",
      "Magnitude (astronomy)",
      "Margin (machine learning)",
      "Noise (video)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics"
    ],
    "authors": [
      {
        "surname": "Pang",
        "given_name": "Jian"
      },
      {
        "surname": "Liu",
        "given_name": "Weifeng"
      },
      {
        "surname": "Zhang",
        "given_name": "Bingfeng"
      },
      {
        "surname": "Yang",
        "given_name": "Xinghao"
      },
      {
        "surname": "Liu",
        "given_name": "Baodi"
      },
      {
        "surname": "Tao",
        "given_name": "Dapeng"
      }
    ]
  },
  {
    "title": "Multi-scale self-supervised representation learning with temporal alignment for multi-rate time series modeling",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109943",
    "abstract": "Deep sequential networks have shown great power in time series regression and classification. So far, most approaches naturally assume that the time sequential data are uniformly sampled. In practice, however, different variables usually have different sampling rates, thereby forming multi-rate time series (MR-TS). Particularly, the target variable (i.e., label) to be predicted usually has a lower sampling frequency due to the difficulty of manual annotations. The multi-rate problem poses two challenges. One is the diverse dynamics at different sampling rates, which is defined as multi-scale dynamics. The other is label scarcity. To tackle the above obstacles, this paper developed a Multi-scale Self-supervised Representation Learning technique with a Temporal Alignment mechanism (MSRL-TA) as a coherent framework. Concretely, a probabilistic masked autoencoding approach is pertinently developed, in which segment-wise masking schemes and rate-aware positional encodings are devised to enable the characterization of multi-scale temporal dynamics. In the course of pre-training, the encoder networks are able to generate rich and holistic representations of multi-rate data, thereby alleviating the label scarcity issue for supervised fine-tuning. Furthermore, a Temporal Alignment mechanism is devised to refine synthesized features for dynamic predictive modeling through feature block division and block-wise convolution. With empirical evaluation through extensive experiments, our proposed MSRL-TA achieved consistent state-of-the-art in both multi-rate time series regression and classification tasks on five real-world datasets, including air quality prediction, industrial soft sensing, human activity recognition, and speech voice classification.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323006416",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Block (permutation group theory)",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Feature (linguistics)",
      "Feature learning",
      "Filter (signal processing)",
      "Geometry",
      "Law",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Political science",
      "Politics",
      "Probabilistic logic",
      "Quantum mechanics",
      "Representation (politics)",
      "Sampling (signal processing)",
      "Scale (ratio)",
      "Supervised learning",
      "Time series"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Jiawei"
      },
      {
        "surname": "Song",
        "given_name": "Pengyu"
      },
      {
        "surname": "Zhao",
        "given_name": "Chunhui"
      }
    ]
  },
  {
    "title": "Cross-scale contrastive triplet networks for graph representation learning",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109907",
    "abstract": "Graph representation learning aims to learn low-dimensional representation for the graph, which has played a vital role in real-world applications. Without requiring additional labeled data, contrastive learning based graph representation learning (or graph contrastive learning) has attracted considerable attention. Recently, one of the most exciting advancement in graph contrastive learning is Deep Graph Infomax (DGI), which maximizes the Mutual Information (MI) between the node and graph representations. However, DGI only considers the contextual node information, ignoring the intrinsic node information (i.e., the similarity between node representations in different views). In this paper, we propose a novel Cross-scale Contrastive Triplet Networks (CCTN) framework, which captures both contextual and intrinsic node information for graph representation learning. Specifically, to obtain the contextual node information, we utilize an infomax contrastive network to maximize the MI between node and graph representations. For acquiring the intrinsic node information, we present a Siamese contrastive network by maximizing the similarity between node representations in different augmented views. Two contrastive networks learn together through a shared graph convolution network to form our cross-scale contrastive triplet networks. Finally, we evaluate CCTN on six real-world datasets. Extensive experimental results demonstrate that CCTN achieves state-of-the-art performance on node classification and clustering tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323006052",
    "keywords": [
      "Artificial intelligence",
      "Blind signal separation",
      "Channel (broadcasting)",
      "Computer network",
      "Computer science",
      "Engineering",
      "Feature learning",
      "Graph",
      "Infomax",
      "Natural language processing",
      "Node (physics)",
      "Structural engineering",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Yanbei"
      },
      {
        "surname": "Shan",
        "given_name": "Wanjin"
      },
      {
        "surname": "Wang",
        "given_name": "Xiao"
      },
      {
        "surname": "Xiao",
        "given_name": "Zhitao"
      },
      {
        "surname": "Geng",
        "given_name": "Lei"
      },
      {
        "surname": "Zhang",
        "given_name": "Fang"
      },
      {
        "surname": "Du",
        "given_name": "Dongdong"
      },
      {
        "surname": "Pang",
        "given_name": "Yanwei"
      }
    ]
  },
  {
    "title": "Source-free domain adaptation with Class Prototype Discovery",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109974",
    "abstract": "Source-free domain adaptation requires no access to the source domain training data during unsupervised domain adaption. This is critical for meeting particular data sharing, privacy, and license constraints, whilst raising novel algorithmic challenges. Existing source-free domain adaptation methods rely on either generating pseudo samples/prototypes of source or target domain style, or simply leveraging pseudo-labels (self-training). They suffer from low-quality generated samples/prototypes or noisy pseudo-label target samples. In this work, we address both limitations by introducing a novel Class Prototype Discovery (CPD) method. In contrast to all alternatives, our CPD is established on a set of semantic class prototypes each constructed for representing a specific class. By designing a classification score based prototype learning mechanism, we reformulate the source-free domain adaptation problem to class prototype optimization using all the target domain training data, and without the need for data generation. Then, class prototypes are used to cluster target features to assign them pseudo-labels, which highly complements the conventional self-training strategy. Besides, a prototype regularization is introduced for exploiting well-established distribution alignment based on pseudo-labeled target samples and class prototypes. Along with theoretical analysis, we conduct extensive experiments on three standard benchmarks to validate the performance advantages of our CPD over the state-of-the-art models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323006726",
    "keywords": [
      "Adaptation (eye)",
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Data mining",
      "Domain (mathematical analysis)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Optics",
      "Physics",
      "Regularization (linguistics)"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Lihua"
      },
      {
        "surname": "Li",
        "given_name": "Nianxin"
      },
      {
        "surname": "Ye",
        "given_name": "Mao"
      },
      {
        "surname": "Zhu",
        "given_name": "Xiatian"
      },
      {
        "surname": "Tang",
        "given_name": "Song"
      }
    ]
  },
  {
    "title": "Prior knowledge guided text to image generation",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2023.12.003",
    "abstract": "Generating a realistic and semantically consistent image from a given text is a challenging task. Due to the limited information of natural language, it is difficult to generate vivid images with fine details. To address this problem, we propose a Prior Knowledge Guided GAN for text to image generation. Specifically, the proposed method consists of several Knowledge Guided Up-Blocks. We decompose the image into a superposition of several visual regions, each of which requires corresponding prior knowledge to enrich its visual details. Correspondingly, we construct each Up-Block by incorporating relevant prior knowledge as input, aiming to enhance the quality of each visual region. Prior knowledge progressively provides more visual detail through affine transformations. Finally, high-quality images are synthesized by fusing all image regions. Experimental results on the CUB and COCO datasets demonstrate the superior performance of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523003501",
    "keywords": [
      "Affine transformation",
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Computer science",
      "Computer vision",
      "Construct (python library)",
      "Economics",
      "Geometry",
      "Image (mathematics)",
      "Management",
      "Mathematics",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Programming language",
      "Pure mathematics",
      "Question answering",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "An-An"
      },
      {
        "surname": "Sun",
        "given_name": "Zefang"
      },
      {
        "surname": "Xu",
        "given_name": "Ning"
      },
      {
        "surname": "Kang",
        "given_name": "Rongbao"
      },
      {
        "surname": "Cao",
        "given_name": "Jinbo"
      },
      {
        "surname": "Yang",
        "given_name": "Fan"
      },
      {
        "surname": "Qin",
        "given_name": "Weijun"
      },
      {
        "surname": "Zhang",
        "given_name": "Shenyuan"
      },
      {
        "surname": "Zhang",
        "given_name": "Jiaqi"
      },
      {
        "surname": "Li",
        "given_name": "Xuanya"
      }
    ]
  },
  {
    "title": "Restoring vision in hazy weather with hierarchical contrastive learning",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109956",
    "abstract": "Image restoration under hazy weather condition, which is called single image dehazing, has been of significant interest for various computer vision applications. In recent years, deep learning-based methods have achieved success. However, existing image dehazing methods typically neglect the hierarchy of features in the neural network and fail to exploit their relationships fully. To this end, we propose an effective image dehazing method named Hierarchical Contrastive Dehazing (HCD), which is based on feature fusion and contrastive learning strategies. HCD consists of a hierarchical dehazing network (HDN) and a novel hierarchical contrastive loss (HCL). Specifically, the core design in the HDN is a hierarchical interaction module, which utilizes multi-scale activation to revise the feature responses hierarchically. To cooperate with the training of HDN, we propose HCL which performs contrastive learning on hierarchically paired exemplars, facilitating haze removal. Extensive experiments on public datasets, RESIDE, HazeRD, and DENSE-HAZE, demonstrate that HCD quantitatively outperforms the state-of-the-art methods in terms of PSNR, SSIM and achieves better visual quality.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323006544",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer security",
      "Economics",
      "Exploit",
      "Feature (linguistics)",
      "Haze",
      "Hierarchy",
      "Image (mathematics)",
      "Linguistics",
      "Machine learning",
      "Market economy",
      "Meteorology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Tao"
      },
      {
        "surname": "Tao",
        "given_name": "Guangpin"
      },
      {
        "surname": "Lu",
        "given_name": "Wanglong"
      },
      {
        "surname": "Zhang",
        "given_name": "Kaihao"
      },
      {
        "surname": "Luo",
        "given_name": "Wenhan"
      },
      {
        "surname": "Zhang",
        "given_name": "Xiaoqin"
      },
      {
        "surname": "Lu",
        "given_name": "Tong"
      }
    ]
  },
  {
    "title": "Fast Multi-view Subspace Clustering with Balance Anchors Guidance",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109895",
    "abstract": "Multi-view subspace clustering (MVSC) has acquired satisfactory clustering performance since it effectively integrates the information from multiple views. However, existing MVSC methods often suffer from high time costs and are difficult to be used in real-life large-scale data. Anchor-based MVSC methods have been presented to select crucial landmarks to reduce time-consuming effectively. In addition, the processes of anchor selection of existing methods are performed in the raw space, in which the high-dimensional data often involve lots of noise information and outliers that inevitably lead to the degradation of clustering performance. Moreover, these methods also ignore the balance structure of data, such that the selected anchors cannot fully characterize the intrinsic structure information of the original data. To tackle the aforementioned issues, we present a novel MVSC method named Fast Multi-view Subspace Clustering with Balance Anchors Guidance (FMVSC-BAG). Specifically, FMVSC-BAG integrates the learning processes of anchors, anchor graphs, and labels into a united framework in embedding space seamlessly. This way, they can reinforce each other to improve final clustering performance while eliminating noise and outliers hidden in the original data. Furthermore, FMVSC-BAG constrains the learned labels to preserve the balance structure by a novel balance strategy to promote further that the intrinsic balance structure information of original data can be reserved in the learned anchors and anchor graph. Finally, extensive experiments on eight real-life large-scale datasets prove its efficiency and superiority compared to some advanced clustering methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005939",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Clustering high-dimensional data",
      "Computer science",
      "Data mining",
      "Embedding",
      "Image (mathematics)",
      "Machine learning",
      "Noise (video)",
      "Outlier",
      "Programming language",
      "Raw data",
      "Subspace topology"
    ],
    "authors": [
      {
        "surname": "Mi",
        "given_name": "Yong"
      },
      {
        "surname": "Chen",
        "given_name": "Hongmei"
      },
      {
        "surname": "Yuan",
        "given_name": "Zhong"
      },
      {
        "surname": "Luo",
        "given_name": "Chuan"
      },
      {
        "surname": "Horng",
        "given_name": "Shi-Jinn"
      },
      {
        "surname": "Li",
        "given_name": "Tianrui"
      }
    ]
  },
  {
    "title": "Hierarchical long-tailed classification based on multi-granularity knowledge transfer driven by multi-scale feature fusion",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109842",
    "abstract": "Long-tailed learning is attracting increasing attention due to the unbalanced distributions of real-world data. The aim is to train well-performing depth models. Traditional knowledge transfer methods for long-tailed learning are classified into feature-based horizontal knowledge transfer (HKT) and class-based vertical knowledge transfer (VKT). HKT transfers head-to-tail feature knowledge from different classes to improve classification performance when there are few tail classes. However, HKT easily leads to invalid transfer due to the deviation caused by the difference between the knowledge of head and tail classes. Fortunately, the class space has a multi-grained relationship and can form a multi-granularity knowledge graph (MGKG), which can be recast as coarse-grained and fine-grained losses to guide VKT. In this paper, we propose a hierarchical long-tailed classification method based on multi-granularity knowledge transfer (MGKT), which vertically transfers knowledge from coarse- to fine-grained classes. First, we exploit the semantic information of classes to construct an MGKG, which forms an affiliation of fine- and coarse-grained classes. Fine-grained knowledge can inherit coarse-grained knowledge to reduce transfer bias with the help of MGKG. We then propose a multi-scale feature fusion network, which aims to fully mine the rich information of the features to drive MGKT. Experiments show that the proposed model outperforms several state-of-the-art models in classifying long-tailed data. For example, our model performed 4.46% better than the next-best model on the SUN-LT dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032300540X",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Computer security",
      "Construct (python library)",
      "Data mining",
      "Exploit",
      "Feature (linguistics)",
      "Feature vector",
      "Granularity",
      "Knowledge graph",
      "Knowledge management",
      "Knowledge transfer",
      "Linguistics",
      "Machine learning",
      "Operating system",
      "Parallel computing",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Programming language",
      "Quantum mechanics",
      "Scale (ratio)",
      "Transfer (computing)",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Wei"
      },
      {
        "surname": "Zhao",
        "given_name": "Hong"
      }
    ]
  },
  {
    "title": "TransOSV: Offline Signature Verification with Transformers",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109882",
    "abstract": "Signature verification is a frequently-used forensics technology in numerous safety-critical situations. Although convolutional neural networks (CNNs) have made significant advancements in the field of signature verification, their reliance on local neighborhood operations poses limitations in capturing the global contextual relationships among signature strokes. To overcome this weakness, in this paper, we propose a novel holistic-part unified model named TransOSV based on the vision transformer framework to solve offline signature verification problem. The signature images are first encoded into patch sequences by the proposed transformer-based holistic encoder to learn the global signature representation. Second, considering the subtle local difference between the genuine signature and forged signature, we design a contrast based part decoder along with a sparsity loss, which are utilized to learn the discriminative part features. With the learned holistic features and part features, the proposed model is optimized by the contrast loss function. To reduce the influence of sample imbalance, we also formulate a new focal contrast loss function. Furthermore, we conduct the proposed model to learn signature representations for writer-dependent signature verification task. The experimental results demonstrate the potential of the proposed TransOSV model for both writer-independent and writer-dependent signature verification tasks, achieving remarkable performance improvements and competitive results on four widely-used offline signature datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005800",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Geometry",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Signature (topology)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Huan"
      },
      {
        "surname": "Wei",
        "given_name": "Ping"
      },
      {
        "surname": "Ma",
        "given_name": "Zeyu"
      },
      {
        "surname": "Li",
        "given_name": "Changkai"
      },
      {
        "surname": "Zheng",
        "given_name": "Nanning"
      }
    ]
  },
  {
    "title": "Global semantic enhancement network for video captioning",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109906",
    "abstract": "Video captioning aims to briefly describe the content of a video in accurate and fluent natural language, which is a hot research topic in multimedia processing. As a bridge between video and natural language, video captioning is a challenging task that requires a deep understanding of video content and effective utilization of diverse video multimodal information. Existing video captioning methods usually ignore the relative importance between different frames when aggregating frame-level video features and neglect the global semantic correlations between videos and texts in learning visual representations, resulting in the learned representations less effective. To address these problems, we propose a novel framework, namely Global Semantic Enhancement Network (GSEN) to generate high-quality captions for videos. Specifically, a feature aggregation module based on a lightweight attention mechanism is designed to aggregate frame-level video features, which highlights features of informative frames in video representations. In addition, a global semantic enhancement module is proposed to enhance semantic correlations for video and language representations in order to generate semantically more accurate captions. Extensive qualitative and quantitative experiments on two public benchmark datasets MSVD and MSR-VTT demonstrate that the proposed GSEN can achieve superior performance than state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323006040",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Closed captioning",
      "Computer science",
      "Economics",
      "Feature (linguistics)",
      "Frame (networking)",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Image retrieval",
      "Information retrieval",
      "Linguistics",
      "Management",
      "Metric (unit)",
      "Multimedia",
      "Natural language",
      "Natural language processing",
      "Operations management",
      "Philosophy",
      "Programming language",
      "Semantic gap",
      "Semantics (computer science)",
      "Task (project management)",
      "Telecommunications",
      "Video quality"
    ],
    "authors": [
      {
        "surname": "Luo",
        "given_name": "Xuemei"
      },
      {
        "surname": "Luo",
        "given_name": "Xiaotong"
      },
      {
        "surname": "Wang",
        "given_name": "Di"
      },
      {
        "surname": "Liu",
        "given_name": "Jinhui"
      },
      {
        "surname": "Wan",
        "given_name": "Bo"
      },
      {
        "surname": "Zhao",
        "given_name": "Lin"
      }
    ]
  },
  {
    "title": "Supervised penalty-based aggregation applied to motor-imagery based brain-computer-interface",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109924",
    "abstract": "In this paper we propose a new version of penalty-based aggregation functions, the Multi Cost Aggregation choosing functions (MCAs), in which the function to minimize is constructed using a convex combination of two relaxed versions of restricted equivalence and dissimilarity functions instead of a penalty function. We additionally suggest two different alternatives to train a MCA in a supervised classification task in order to adapt the aggregation to each vector of inputs. We apply the proposed MCA in a Motor Imagery-based Brain–Computer Interface (MI-BCI) system to improve its decision making phase. We also evaluate the classical aggregation with our new aggregation procedure in two publicly available datasets. We obtain an accuracy of 82.31% for a left vs. right hand in the Clinical BCI challenge (CBCIC) dataset, and a performance of 62.43% for the four-class case in the BCI Competition IV 2a dataset compared to a 82.15% and 60.56% using the arithmetic mean. Finally, we have also tested the goodness of our proposal against other MI-BCI systems, obtaining better results than those using other decision making schemes and Deep Learning on the same datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323006222",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Brain–computer interface",
      "Bubble",
      "Computer science",
      "Discrete mathematics",
      "Economics",
      "Electroencephalography",
      "Equivalence (formal languages)",
      "Evolutionary biology",
      "Function (biology)",
      "Interface (matter)",
      "Machine learning",
      "Management",
      "Mathematical optimization",
      "Mathematics",
      "Maximum bubble pressure method",
      "Motor imagery",
      "Parallel computing",
      "Pattern recognition (psychology)",
      "Penalty method",
      "Psychiatry",
      "Psychology",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Fumanal-Idocin",
        "given_name": "J."
      },
      {
        "surname": "Vidaurre",
        "given_name": "C."
      },
      {
        "surname": "Fernandez",
        "given_name": "J."
      },
      {
        "surname": "Gómez",
        "given_name": "M."
      },
      {
        "surname": "Andreu-Perez",
        "given_name": "J."
      },
      {
        "surname": "Prasad",
        "given_name": "M."
      },
      {
        "surname": "Bustince",
        "given_name": "H."
      }
    ]
  },
  {
    "title": "ENInst: Enhancing weakly-supervised low-shot instance segmentation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109888",
    "abstract": "We address a weakly-supervised low-shot instance segmentation, an annotation-efficient training method to deal with novel classes effectively. Since it is an under-explored problem, we first investigate the difficulty of the problem and identify the performance bottleneck by conducting systematic analyses of model components and individual sub-tasks with a simple baseline model. Based on the analyses, we propose ENInst with sub-task enhancement methods: instance-wise mask refinement for enhancing pixel localization quality and novel classifier composition for improving classification accuracy. Our proposed method lifts the overall performance by enhancing the performance of each sub-task. We demonstrate that our ENInst is 7.5 times more efficient in achieving comparable performance to the existing fully-supervised few-shot models and even outperforms them at times.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005861",
    "keywords": [
      "Annotation",
      "Artificial intelligence",
      "Artificial neural network",
      "Bottleneck",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Economics",
      "Embedded system",
      "Machine learning",
      "Management",
      "Pattern recognition (psychology)",
      "Segmentation",
      "Supervised learning",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Ye-Bin",
        "given_name": "Moon"
      },
      {
        "surname": "Choi",
        "given_name": "Dongmin"
      },
      {
        "surname": "Kwon",
        "given_name": "Yongjin"
      },
      {
        "surname": "Kim",
        "given_name": "Junsik"
      },
      {
        "surname": "Oh",
        "given_name": "Tae-Hyun"
      }
    ]
  },
  {
    "title": "Spatial transcriptomics analysis of gene expression prediction using exemplar guided graph neural network",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109966",
    "abstract": "Spatial transcriptomics (ST) is essential for understanding diseases and developing novel treatments. It measures the gene expression of each fine-grained area (i.e., different windows) in the tissue slide with low throughput. This paper proposes an exemplar guided graph network dubbed EGGN to accurately and efficiently predict gene expression from each window of a tissue slide image. We apply exemplar learning to dynamically boost gene expression prediction from nearest/similar exemplars of a given tissue slide image window. Our framework has three main components connected in a sequence: (i) an extractor to structure a feature space for exemplar retrievals; (ii) a graph construction strategy to connect windows and exemplars as a graph; (iii) a graph convolutional network backbone to process window and exemplar features, and a graph exemplar bridging block to adaptively revise the window features using its exemplars. Finally, we complete the gene expression prediction task with a simple attention-based prediction block. Experiments on standard benchmark datasets indicate the superiority of our approach when compared with past state-of-the-art methods. We release our code at https://github.com/Yan98/EGN.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323006647",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Graph",
      "Pattern recognition (psychology)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Yan"
      },
      {
        "surname": "Hossain",
        "given_name": "Md Zakir"
      },
      {
        "surname": "Stone",
        "given_name": "Eric"
      },
      {
        "surname": "Rahman",
        "given_name": "Shafin"
      }
    ]
  },
  {
    "title": "Learning correlation information for multi-label feature selection",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109899",
    "abstract": "In many real-world multi-label applications, the content of multi-label data is usually characterized by high dimensional features, which contains complex correlation information, i.e., label correlations and redundant features. To alleviate the problem, we present a novel scheme, called learning correlation information for multi-label feature selection (LCIFS) method, by jointly digging up label correlations and controlling feature redundancy. To be specific, the regression model via manifold framework is presented to fit the relationship between feature space and label distribution, during which adaptive spectral graph is leveraged to learn more precise structural correlations of labels simultaneously. Besides, we utilize the relevance of features to constrain the redundancy of the generated feature subset, and a general ℓ 2 , p -norm regularized model is employed to fulfill more robust feature selection. The proposed method is transformed into an explicit optimization function, which is conquered by an efficient iterative optimization algorithm. Finally, we conduct comprehensive experiments on twelve realistic multi-label datasets, including text domain, image domain, and audio domain. The statistic results demonstrate the effectiveness and superiority of the proposed method among nine competition methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005976",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Correlation",
      "Data mining",
      "Feature (linguistics)",
      "Feature selection",
      "Feature vector",
      "Geometry",
      "Graph",
      "Linguistics",
      "Mathematics",
      "Minimum redundancy feature selection",
      "Mutual information",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Redundancy (engineering)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Fan",
        "given_name": "Yuling"
      },
      {
        "surname": "Liu",
        "given_name": "Jinghua"
      },
      {
        "surname": "Tang",
        "given_name": "Jianeng"
      },
      {
        "surname": "Liu",
        "given_name": "Peizhong"
      },
      {
        "surname": "Lin",
        "given_name": "Yaojin"
      },
      {
        "surname": "Du",
        "given_name": "Yongzhao"
      }
    ]
  },
  {
    "title": "Exploiting temporal information to detect conversational groups in videos and predict the next speaker",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2023.10.002",
    "abstract": "Studies in human–human interaction have introduced the concept of F-formation to describe the spatial arrangement of participants during social interactions. This paper has two objectives. It aims at detecting F-formations in video sequences and at predicting the next speaker in a group conversation. The proposed approach exploits time information and multimodal signals of humans in video sequences. In particular, we rely on measuring the engagement level of people as a feature of group belonging. Our approach makes use of a recursive neural network, the Long Short Term Memory (LSTM), to predict who will take the speaker’s turn in a conversation group. Experiments on the MatchNMingle dataset led to 85% true positives in group detection and 98% accuracy in predicting the next speaker.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523002738",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Speaker diarisation",
      "Speaker recognition",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Tosato",
        "given_name": "Lucrezia"
      },
      {
        "surname": "Fortier",
        "given_name": "Victor"
      },
      {
        "surname": "Bloch",
        "given_name": "Isabelle"
      },
      {
        "surname": "Pelachaud",
        "given_name": "Catherine"
      }
    ]
  },
  {
    "title": "Automatic semantic modeling of structured data sources with cross-modal retrieval",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2023.11.014",
    "abstract": "Analyzing and modeling the implicit semantic relationships in data sources is the key to achieving integration and sharing of heterogeneous data information. However, manual modeling of data semantics is a laborious and error-prone task that demands significant human effort and expertise. The paper proposes a novel explainable representation learning-based method that adopts an attention-based table-graph cross-modal retrieval model as a rating function during incremental search for automatic semantic modeling. Our supervised model utilizes the graph representation learning technique to extract latent semantics from data and aims to retrieve the most reliable semantic model for structured data sources. Experimental results demonstrate the effectiveness and robustness of our method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523003240",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Data mining",
      "Data modeling",
      "Database",
      "Domain knowledge",
      "Gene",
      "Graph",
      "IDEF1X",
      "Information retrieval",
      "Machine learning",
      "Modal",
      "Natural language processing",
      "Ontology-based data integration",
      "Polymer chemistry",
      "Probabilistic latent semantic analysis",
      "Programming language",
      "Robustness (evolution)",
      "Semantic data model",
      "Semantics (computer science)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Ruiqing"
      },
      {
        "surname": "Mayer",
        "given_name": "Wolfgang"
      },
      {
        "surname": "Chu",
        "given_name": "Hailong"
      },
      {
        "surname": "Zhang",
        "given_name": "Yitao"
      },
      {
        "surname": "Zhang",
        "given_name": "Hong-Yu"
      },
      {
        "surname": "Wang",
        "given_name": "Yulong"
      },
      {
        "surname": "Liu",
        "given_name": "Youfa"
      },
      {
        "surname": "Feng",
        "given_name": "Zaiwen"
      }
    ]
  },
  {
    "title": "Evidential Pseudo-Label Ensemble for semi-supervised classification",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2023.11.027",
    "abstract": "Semi-supervised learning (SSL) focuses on effectively exploiting both a limited number of labeled data and relatively rich unlabeled data. Recent SSL researches have achieved great progress with pseudo-labeling. However, incorrect pseudo-label may cause severe error propagation, which has been widely recognized and termed as confirmation bias. Furthermore, vanilla ensemble strategies aiming at alleviating this issue will lead to serious conservative labeling due to the averaging property. Accordingly, they could not sufficiently explore the unlabeled data. In this paper, we proposed a novel SSL method termed Evidential Pseudo-Label Ensemble (EPLE) which aims to generate more accurate pseudo-labels with evidence support. Specifically, multiple networks with different augmented strategies are introduced to generate complementary evidence for unlabeled samples, and induce confident predictions with Dempster–Shafer theory (DST), which effectively addresses error propagation and confirmation bias. To enhance the consistency of predictions for the same samples with different noises, we employ multiple augmentation strategies for both weak and strong augmentations. We conduct experiments on standard semi-supervised learning datasets, including CIFAR-10/100, SVHN and STL-10. Extensive experiments show the superior performance of our proposed EPLE compared to state-of-the-art SSL models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523003379",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Consistency (knowledge bases)",
      "Ensemble learning",
      "Epistemology",
      "Labeled data",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Property (philosophy)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Kai"
      },
      {
        "surname": "Zhang",
        "given_name": "Changqing"
      },
      {
        "surname": "Geng",
        "given_name": "Yu"
      },
      {
        "surname": "Ma",
        "given_name": "Huan"
      }
    ]
  },
  {
    "title": "A vision transformer for fine-grained classification by reducing noise and enhancing discriminative information",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109979",
    "abstract": "Recently, several Vision Transformer (ViT) based methods have been proposed for Fine-Grained Visual Classification (FGVC). These methods significantly surpass existing CNN-based ones, demonstrating the effectiveness of ViT in FGVC tasks. However, there are some limitations when applying ViT directly to FGVC. First, ViT needs to split images into patches and calculate the attention of every pair, which may result in heavy noise calculation during the training phase and unsatisfying performance when handling fine-grained images with complex backgrounds and small objects. Second, complementary information is important for FGVC, but a standard ViT works by using the class token in the final layer for classification which is not enough to extract comprehensive fine-grained information at different levels. Third, the class token fuses the information of all patches in the same manner, in other words, the class token treats each patch equally. However, the discriminative parts should be more critical. To address these issues, we propose ACC-ViT including three novel components, i.e., Attention Patch Combination (APC), Critical Regions Filter (CRF), and Complementary Tokens Integration (CTI). Thereinto, APC pieces informative patches from two images to generate a new image to mitigate the noisy calculation and reinforce the differences between images. CRF emphasizes tokens corresponding to discriminative regions to generate a new class token for subtle feature learning. To extract comprehensive information, CTI integrates complementary information captured by class tokens in different ViT layers. We conduct comprehensive experiments on four widely used datasets and the results demonstrate that ACC-ViT can achieve competitive performance. The source code is available at https://github.com/Hector0426/fine-grained-image-classification-with-vit.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323006775",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Discriminative model",
      "Electrical engineering",
      "Engineering",
      "Feature (linguistics)",
      "Filter (signal processing)",
      "Image (mathematics)",
      "Linguistics",
      "Machine learning",
      "Noise (video)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Security token",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Zi-Chao"
      },
      {
        "surname": "Chen",
        "given_name": "Zhen-Duo"
      },
      {
        "surname": "Wang",
        "given_name": "Yongxin"
      },
      {
        "surname": "Luo",
        "given_name": "Xin"
      },
      {
        "surname": "Xu",
        "given_name": "Xin-Shun"
      }
    ]
  },
  {
    "title": "LCReg: Long-tailed image classification with Latent Categories based Recognition",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109971",
    "abstract": "In this work, we tackle the challenging problem of long-tailed image recognition. Previous long-tailed recognition approaches mainly focus on data augmentation or re-balancing strategies for the tail classes to give them more attention during model training. However, these methods are limited by the small number of training images for the tail classes, which results in poor feature representations. To address this issue, we propose the Latent Categories based long-tail Recognition (LCReg) method. Our hypothesis is that common latent features shared by head and tail classes can be used to improve feature representation. Specifically, we learn a set of class-agnostic latent features shared by both head and tail classes, and then use semantic data augmentation on the latent features to implicitly increase the diversity of the training sample. We conduct extensive experiments on five long-tailed image recognition datasets, and the results show that our proposed method significantly improves the baselines.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323006696",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Feature (linguistics)",
      "Focus (optics)",
      "Image (mathematics)",
      "Latent class model",
      "Latent semantic analysis",
      "Law",
      "Linguistics",
      "Machine learning",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Political science",
      "Politics",
      "Probabilistic latent semantic analysis",
      "Programming language",
      "Representation (politics)",
      "Set (abstract data type)",
      "Training set"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Weide"
      },
      {
        "surname": "Wu",
        "given_name": "Zhonghua"
      },
      {
        "surname": "Wang",
        "given_name": "Yiming"
      },
      {
        "surname": "Ding",
        "given_name": "Henghui"
      },
      {
        "surname": "Liu",
        "given_name": "Fayao"
      },
      {
        "surname": "Lin",
        "given_name": "Jie"
      },
      {
        "surname": "Lin",
        "given_name": "Guosheng"
      }
    ]
  },
  {
    "title": "Robust multi-agent reinforcement learning via Bayesian distributional value estimation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109917",
    "abstract": "Reinforcement learning in multi-agent scenarios is essential for real-world applications as it can vividly depict agents’ collaborative and competitive behaviors from a perspective closer to reality. However, most existing studies suffer from poor robustness, preventing multi-agent reinforcement learning from practical applications where robustness is the core indicator of system security and stability. In view of this, we propose a novel Bayesian Multi-Agent Reinforcement Learning method, named BMARL, which leverages the distributional value function calculated by Bayesian inference to improve the robustness of the model. Specifically, Bayesian linear regression is adopted to estimate a posterior distribution concerning value function parameters, rather than approximating an expectation value for Q-value by point estimation. In this way, the value function is more generalized than previously obtained by point estimation, which is beneficial to the robustness of our model. Meanwhile, we utilize the Gaussian prior knowledge to integrate more prior knowledge while estimating the value function, which improves learning efficiency. Extensive experimental results on three benchmark multi-agent environments comparing with seven state-of-the-art methods demonstrate the superiority of BMARL in terms of both robustness and efficiency.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323006155",
    "keywords": [
      "Artificial intelligence",
      "Bayesian inference",
      "Bayesian probability",
      "Bellman equation",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Gene",
      "Inference",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Point estimation",
      "Prior probability",
      "Reinforcement learning",
      "Robustness (evolution)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Du",
        "given_name": "Xinqi"
      },
      {
        "surname": "Chen",
        "given_name": "Hechang"
      },
      {
        "surname": "Wang",
        "given_name": "Che"
      },
      {
        "surname": "Xing",
        "given_name": "Yongheng"
      },
      {
        "surname": "Yang",
        "given_name": "Jielong"
      },
      {
        "surname": "Yu",
        "given_name": "Philip S."
      },
      {
        "surname": "Chang",
        "given_name": "Yi"
      },
      {
        "surname": "He",
        "given_name": "Lifang"
      }
    ]
  },
  {
    "title": "Learning node representations against perturbations",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109976",
    "abstract": "Recent graph neural networks (GNN) has achieved remarkable performance in node representation learning. One key factor of GNN’s success is the smoothness property on node representations. Despite this, most GNN models are fragile to the perturbations on graph inputs and could learn unreliable node representations. In this paper, we study how to learn node representations against perturbations in GNN. Specifically, we consider that a node representation should remain stable under slight perturbations on the input, and node representations from different structures should be identifiable, which two are termed as the stability and identifiability on node representations, respectively. To this end, we propose a novel model called Stability-Identifiability GNN Against Perturbations (SIGNNAP) that learns reliable node representations in an unsupervised manner. SIGNNAP formalizes the stability and identifiability by a contrastive objective and preserves the smoothness with existing GNN backbones. The proposed method is a generic framework that can be equipped with many other backbone models (e.g. GCN, GraphSage and GAT). Extensive experiments on six benchmarks under both transductive and inductive learning setups of node classification demonstrate the effectiveness of our method. Codes and data are available online: https://github.com/xuChenSJTU/SIGNNAP-master-online",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032300674X",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Computer security",
      "Engineering",
      "Epistemology",
      "Feature learning",
      "Graph",
      "Identifiability",
      "Key (lock)",
      "Law",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Node (physics)",
      "Philosophy",
      "Political science",
      "Politics",
      "Property (philosophy)",
      "Representation (politics)",
      "Smoothness",
      "Stability (learning theory)",
      "Structural engineering",
      "Theoretical computer science",
      "Topology (electrical circuits)"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Xu"
      },
      {
        "surname": "Pan",
        "given_name": "Yuangang"
      },
      {
        "surname": "Tsang",
        "given_name": "Ivor"
      },
      {
        "surname": "Zhang",
        "given_name": "Ya"
      }
    ]
  },
  {
    "title": "A novel minutiae-oriented approach for partial fingerprint-based MasterPrint mitigation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109935",
    "abstract": "Partial fingerprint identification systems recognise an individual when the sensor size has a small form factor in accepting a full fingerprint. However, the distinctive features within a partial fingerprint are significantly less. Hence, the uniqueness of a partial fingerprint cannot be assured, leading to the possibility of identifying multiple users. A MasterPrint is a partial fingerprint identifying at least 4% distinct individuals in a partial fingerprint identification system. This work addresses the MasterPrint vulnerability by proposing a novel partial fingerprint identification scheme that extracts minutiae-oriented local features from binarized and thinned partial fingerprint images over eight axes emerging from a reference minutia. It also introduces a metric to compute the similarity score between two partial fingerprint templates. The results are compared with the baseline minutiae matching (BMM) method, a modified Speeded-Up Robust Features (SURF) based approach, VeriFinger 12.1 SDK, standard NIST NBIS, and Ridge Shape Feature (RSF) scheme. The experiments employing partial fingerprint datasets cropped from standard FVC2002 DB1_A, FVC2002 DB2_A, NIST Special Databases (sd302b and sd302d), and CrossMatch VeriFinger dataset have demonstrated that the proposed method generates the lowest MasterPrints with the highest identification accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323006337",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Biometrics",
      "Botany",
      "Computer science",
      "Data mining",
      "Engineering",
      "Fingerprint (computing)",
      "Fingerprint Verification Competition",
      "Fingerprint recognition",
      "Identification (biology)",
      "Image (mathematics)",
      "Matching (statistics)",
      "Mathematics",
      "Metric (unit)",
      "Minutiae",
      "NIST",
      "Operations management",
      "Pattern recognition (psychology)",
      "Similarity (geometry)",
      "Speech recognition",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Joshi",
        "given_name": "Mahesh"
      },
      {
        "surname": "Mazumdar",
        "given_name": "Bodhisatwa"
      },
      {
        "surname": "Dey",
        "given_name": "Somnath"
      }
    ]
  },
  {
    "title": "Transformer-based network with temporal depthwise convolutions for sEMG recognition",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109967",
    "abstract": "Considerable progress has been made in pattern recognition of surface electromyography (sEMG) with deep learning, bringing improvements to sEMG-based gesture classification. Current deep learning techniques are mainly based on convolutional neural networks (CNNs), recurrent neural networks (RNNs), and their hybrids. However, CNNs focus on spatial and local information, while RNNs are unparallelizable, and they suffer from gradient vanishing/exploding. Their hybrids often face problems of model complexity and high computational cost. Because sEMG signals have a sequential nature, motivated by the sequence modeling network Transformer and its self-attention mechanism, we propose a Transformer-based network, temporal depthwise convolutional Transformer (TDCT), for sparse sEMG recognition. With this network, higher recognition accuracy is achieved with fewer convolution parameters and a lower computational cost. Specifically, this network has parallel capability and can capture long-range features inside sEMG signals. We improve the locality and channel correlation capture of multi-head self-attention (MSA) for sEMG modeling by replacing the linear transformation with the proposed temporal depthwise convolution (TDC), which can reduce the convolution parameters and computations for feature learning performance. Four sEMG datasets, Ninapro DB1, DB2, DB5, and OYDB, are used for evaluations and comparisons. In the results, our model outperforms other methods, including Transformer-based networks, in most windows at recognizing the raw signals of sparse sEMG, thus achieving state-of-the-art classification accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323006659",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Deep learning",
      "Electrical engineering",
      "Engineering",
      "Facial recognition system",
      "Feature extraction",
      "Linguistics",
      "Locality",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Recurrent neural network",
      "Sliding window protocol",
      "Speech recognition",
      "Transformer",
      "Voltage",
      "Window (computing)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Zefeng"
      },
      {
        "surname": "Yao",
        "given_name": "Junfeng"
      },
      {
        "surname": "Xu",
        "given_name": "Meiyan"
      },
      {
        "surname": "Jiang",
        "given_name": "Min"
      },
      {
        "surname": "Su",
        "given_name": "Jinsong"
      }
    ]
  },
  {
    "title": "A zero-shot learning boosting framework via concept-constrained clustering",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109937",
    "abstract": "Zero-shot learning (ZSL) aims to recognize novel classes that have no labeled samples during the training phase, which leads to the domain shift problem. In reality, there exists a large number of compounded unlabeled samples. Therefore, it is crucial to accurately estimate the data distribution of these compounded unlabeled samples and improve the performance of ZSL. This paper proposes a zero-shot learning boosting framework. Specifically, ZSL is transformed into a co-training problem between the data distribution estimation of the unlabeled samples and ZSL. The data distribution estimation is modeled as concept-constrained clustering. Furthermore, we design an alternative optimization strategy to realize mutual guidance between the two processes. Finally, systematic experiments verify the effectiveness of the proposed concept-constrained clustering for alleviating the domain shift problem in ZSL and the universality of the proposed framework for boosting different base ZSL models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323006350",
    "keywords": [
      "Artificial intelligence",
      "Boosting (machine learning)",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Labeled data",
      "Machine learning",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Yue",
        "given_name": "Qin"
      },
      {
        "surname": "Cui",
        "given_name": "Junbiao"
      },
      {
        "surname": "Bai",
        "given_name": "Liang"
      },
      {
        "surname": "Liang",
        "given_name": "Jianqing"
      },
      {
        "surname": "Liang",
        "given_name": "Jiye"
      }
    ]
  },
  {
    "title": "Few-shot classification guided by generalization error bound",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109904",
    "abstract": "Recently, transfer learning has generated promising performance in few-shot classification by pre-training a backbone network on base classes and then applying it to novel classes. Nevertheless, there lacks a theoretical analysis on how to reduce the generalization error during the learning process. To fill this gap, we prove that the classification error bound on novel classes is mainly determined by the base-class generalization error, given the base-novel domain divergence and the novel-class generalization error produced by an incremental learner using novel samples. The novel-class generalization error is further decided by the base-class empirical error and the VC-dimension of the hypothesis space. Based on this theoretical analysis, we propose a Born-Again Networks under Self-supervised Label Augmentation (BANs-SLA) method to improve the generalization capability of classifiers. In this method, cross-entropy and supervised contrastive losses are simultaneously used to minimize the base-class empirical error in the expanded space with SLA. Afterward, BANs are adopted to transfer the knowledge sequentially across generations, which acts as an effective regularizer to trade-off the VC-dimension. Extensive experimental results have verified the effectiveness of our method, which establishes the new state-of-the-art performance on popular few-shot classification benchmark datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323006027",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Base (topology)",
      "Benchmark (surveying)",
      "Class (philosophy)",
      "Computer science",
      "Dimension (graph theory)",
      "Generalization",
      "Generalization error",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pure mathematics",
      "Transfer of learning",
      "VC dimension",
      "Word error rate"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Fan"
      },
      {
        "surname": "Yang",
        "given_name": "Sai"
      },
      {
        "surname": "Chen",
        "given_name": "Delong"
      },
      {
        "surname": "Huang",
        "given_name": "Huaxi"
      },
      {
        "surname": "Zhou",
        "given_name": "Jun"
      }
    ]
  },
  {
    "title": "MVSSC: Meta-reinforcement learning based visual indoor navigation using multi-view semantic spatial context",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2023.11.023",
    "abstract": "In Visual Indoor Navigation (VIN), Deep Reinforcement Learning (DRL) is commonly used by agents to achieve end-to-end mapping from vision to action when navigating toward a target based on observation. However, current DRL-based work suffers from two challenges: partial observability resulting from using solely a single first-person view and poor generalization in the case of unknown scenes and unknown objects. In light of these issues, this paper introduces the integration of multi-view as an expansion of observability and meta-learning as a primary generalization technique into the DRL framework and presents the meta-reinforcement learning method that leverages Multi-View Semantic Spatial Context (MVSSC). Specifically, aiming to explore the informative multi-view context for better efficiency in searching for and navigating to the target, we model the objects’ relationship from two aspects, multi-view semantic context (MVSEC) and multi-view spatial context (MVSPC). MVSEC enables agents to encode prior semantic relationships adaptively via a multi-view modulated graph. Meanwhile, MVSPC enhances the spatial representation of target-related objects’ correlation through similarity grids of multi-view. After adaptively fusing the multi-view and context information under the meta-reinforcement learning framework, our method can encourage efficient target search and robust navigation with stronger generalization performance to unknown scenes and unknown objects. Extensive experimental results on the AI2-THOR simulator demonstrate that our method outperforms the current state-of-the-art approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523003343",
    "keywords": [
      "Applied mathematics",
      "Artificial intelligence",
      "Biochemistry",
      "Biology",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "ENCODE",
      "Gene",
      "Generalization",
      "Law",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Observability",
      "Paleontology",
      "Political science",
      "Politics",
      "Reinforcement learning",
      "Representation (politics)",
      "Spatial contextual awareness"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Wanruo"
      },
      {
        "surname": "Liu",
        "given_name": "Hong"
      },
      {
        "surname": "Wu",
        "given_name": "Jianbing"
      },
      {
        "surname": "Li",
        "given_name": "Yidi"
      }
    ]
  },
  {
    "title": "Distilled non-semantic speech embeddings with binary neural networks for low-resource devices",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2023.11.028",
    "abstract": "This work introduces BRILLsson, a novel binary neural network-based representation learning model for a broad range of non-semantic speech tasks. We train the model with knowledge distillation from a large and real-valued TRILLsson model with only a fraction of the dataset used to train TRILLsson. The resulting BRILLsson models are only 2MB in size with a latency less than 8 ms , making them suitable for deployment in low-resource devices such as wearables. We evaluate BRILLsson on eight benchmark tasks (including but not limited to spoken language identification, emotion recognition, human vocal sounds, and keyword spotting), and demonstrate that our proposed ultra-light and low-latency models perform as well as large-scale models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523003380",
    "keywords": [
      "Arithmetic",
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Binary classification",
      "Binary number",
      "Computer science",
      "Connectionism",
      "Geodesy",
      "Geography",
      "Keyword spotting",
      "Latency (audio)",
      "Law",
      "Machine learning",
      "Mathematics",
      "Natural language processing",
      "Operating system",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Software deployment",
      "Speech recognition",
      "Support vector machine",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Lee",
        "given_name": "Harlin"
      },
      {
        "surname": "Saeed",
        "given_name": "Aaqib"
      }
    ]
  },
  {
    "title": "Understanding open-set recognition by Jacobian norm and inter-class separation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109942",
    "abstract": "The findings on open-set recognition (OSR) show that models trained on classification datasets are capable of detecting unknown classes not encountered during the training process. Specifically, after trainig, the learned representations of known classes dissociate from the representations of the unknown class, facilitating OSR. In this paper, we investigate this emergent phenomenon by examining the relationship between the Jacobian norm of representations and the inter/intra-class learning dynamics. We provide a theoretical analysis, demonstrating that intra-class learning reduces the Jacobian norm for known class samples, while inter-class learning increases the Jacobian norm for unknown samples, even in the absence of direct exposure to any unknown sample. Overall, the discrepancy in the Jacobian norm between the known and unknown classes enables OSR. Based on this insight, which highlights the pivotal role of inter-class learning, we devise a marginal one-vs-rest (m-OvR) loss function that promotes strong inter-class separation. To further improve OSR performance, we integrate the m-OvR loss with additional strategies that maximize the Jacobian norm disparity. We present comprehensive experimental results that support our theoretical observations and demonstrate the efficacy of our proposed OSR approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323006404",
    "keywords": [
      "Algorithm",
      "Applied mathematics",
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Jacobian matrix and determinant",
      "Law",
      "Machine learning",
      "Mathematics",
      "Norm (philosophy)",
      "Political science",
      "Sample complexity"
    ],
    "authors": [
      {
        "surname": "Park",
        "given_name": "Jaewoo"
      },
      {
        "surname": "Park",
        "given_name": "Hojin"
      },
      {
        "surname": "Jeong",
        "given_name": "Eunju"
      },
      {
        "surname": "Teoh",
        "given_name": "Andrew Beng Jin"
      }
    ]
  },
  {
    "title": "Special section: Best papers of the international conference on pattern recognition and artificial intelligence (ICPRAI) 2022",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2023.12.011",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523003586",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Engineering",
      "Engineering physics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Section (typography)",
      "Special section"
    ],
    "authors": [
      {
        "surname": "El-Yacoubi",
        "given_name": "Mounîm A."
      },
      {
        "surname": "Pal",
        "given_name": "Umapada"
      },
      {
        "surname": "Granger",
        "given_name": "Eric"
      },
      {
        "surname": "Yuen",
        "given_name": "Pong Chi"
      }
    ]
  },
  {
    "title": "Mutual Domain Adaptation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109919",
    "abstract": "To solve the label sparsity problem, domain adaptation has been well-established, suggesting various methods such as finding a common feature space of different domains using projection matrices or neural networks. Despite recent advances, domain adaptation is still limited and is not yet practical. The most pronouncing problem is that the existing approaches assume source-target relationship between domains, which implies one domain supplies label information to another domain. However, the amount of label is only marginal in real-world domains, so it is unrealistic to find source domains having sufficient labels. Motivated by this, we propose a method that allows domains to mutually share label information. The proposed method finds a projection matrix that matches the respective distributions of different domains, preserves their respective geometries, and aligns their respective class boundaries. The experiments on benchmark datasets show that the proposed method outperforms relevant baselines. In particular, the results on varying proportions of labels present that the fewer labels the better improvement.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323006179",
    "keywords": [
      "Adaptation (eye)",
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Class (philosophy)",
      "Classifier (UML)",
      "Computer science",
      "Domain (mathematical analysis)",
      "Domain adaptation",
      "Feature (linguistics)",
      "Geodesy",
      "Geography",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Projection (relational algebra)"
    ],
    "authors": [
      {
        "surname": "Park",
        "given_name": "Sunghong"
      },
      {
        "surname": "Kim",
        "given_name": "Myung Jun"
      },
      {
        "surname": "Park",
        "given_name": "Kanghee"
      },
      {
        "surname": "Shin",
        "given_name": "Hyunjung"
      }
    ]
  },
  {
    "title": "Multi-label feature selection by strongly relevant label gain and label mutual aid",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109945",
    "abstract": "Multi-label feature selection, which addresses the challenge of high dimensionality in multi-label learning, has wide applicability in pattern recognition, machine learning, and related domains. Most existing studies on multi-label feature selection assume that all labels have the same importance with respect to features, however, they overlook the differences between labels and candidate features relative to selected features and the internal influence of the label space. To address this issue, we propose a novel method for multi-label feature selection that accounts for both the strongly relevant label gain and the label mutual aid. Firstly, we advance two new potential relationships between labels and candidate features relative to selected features, and the label discriminant function is introduced. Secondly, the mutual aid information between labels is presented to describe the internal correlation of the label space. Thirdly, the concept of strongly relevant label gain is defined based on the label discriminant function, which allows better exploration of positive correlation between features. Finally, the experimental results on sixteen multi-label benchmark datasets indicate that the proposed method outperforms other compared representative multi-label feature selection methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032300643X",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Correlation",
      "Curse of dimensionality",
      "Data mining",
      "Dimensionality reduction",
      "Discriminant",
      "Feature (linguistics)",
      "Feature selection",
      "Geodesy",
      "Geography",
      "Geometry",
      "Information gain",
      "Linear discriminant analysis",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Multi-label classification",
      "Mutual information",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Selection (genetic algorithm)"
    ],
    "authors": [
      {
        "surname": "Dai",
        "given_name": "Jianhua"
      },
      {
        "surname": "Huang",
        "given_name": "Weiyi"
      },
      {
        "surname": "Zhang",
        "given_name": "Chucai"
      },
      {
        "surname": "Liu",
        "given_name": "Jie"
      }
    ]
  },
  {
    "title": "Two-stage feature distribution rectification for few-shot point cloud semantic segmentation",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2023.12.008",
    "abstract": "Few-shot point cloud semantic segmentation segments new classes given few labeled examples and has attracted much attention recently. However, due to the scarcity of labeled data, there are biases between the ideal and the actual feature distributions. Addressing the above issues, we propose a two-stage feature distribution rectification method (TFDR) to reduce these biases. We define the biases in two aspects: interclass and intraclass distribution biases. Interclass distribution bias refers to the distribution shifting introduced by the difference between support data and query data. To reduce this bias, we design a novel feature alignment module (FAM). Intraclass distribution bias is defined as the bias between the ideal and the actual feature distribution of a class, which is introduced by the difference in local structures such as the seats and the legs of chairs. To mitigate the effects of intraclass distribution, we propose a distribution canonicalization module (DCM) rectifying the feature distributions of query data. The experimental results show that the proposed method outperforms several state-of-the-art methods with great significance on the S3DIS and ScanNet datasets, thus demonstrating the effectiveness of our model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523003550",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Distribution (mathematics)",
      "Feature (linguistics)",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Point cloud",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Tichao"
      },
      {
        "surname": "Hao",
        "given_name": "Fusheng"
      },
      {
        "surname": "Cui",
        "given_name": "Guosheng"
      },
      {
        "surname": "Wu",
        "given_name": "Fuxiang"
      },
      {
        "surname": "Yang",
        "given_name": "Mengjie"
      },
      {
        "surname": "Zhang",
        "given_name": "Qieshi"
      },
      {
        "surname": "Cheng",
        "given_name": "Jun"
      }
    ]
  },
  {
    "title": "Multi-label borderline oversampling technique",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109953",
    "abstract": "Class imbalance problem commonly exists in multi-label classification (MLC) tasks. It has non-negligible impacts on the classifier performance and has drawn extensive attention in recent years. Borderline oversampling has been widely used in single-label learning as a competitive technique in dealing with class imbalance. Nevertheless, the borderline samples in multi-label data sets (MLDs) have not been studied. Hence, this paper deeply discussed the borderline samples in MLDs and found they have different neighboring relationships with class borders, which makes their roles different in the classifier training. For that, they are divided into two types named the self-borderline samples and the cross-borderline samples. Further, a novel MLDs resampling approach called Multi-Label Borderline Oversampling Technique (MLBOTE) is proposed for multi-label imbalanced learning. MLBOTE identifies three types of seed samples, including interior, self-borderline, and cross-borderline samples, and different oversampling mechanisms are designed for them, respectively. Meanwhile, it regards not only the minority classes but also the classes suffering from one-vs-rest imbalance as those in need of oversampling. Experiments on eight data sets with nine MLC algorithms and three base classifiers are carried out to compare MLBOTE with some state-of-art MLDs resampling techniques. The results show MLBOTE outperforms other methods in various scenarios.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323006519",
    "keywords": [
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Class (philosophy)",
      "Classifier (UML)",
      "Computer network",
      "Computer science",
      "Machine learning",
      "Oversampling",
      "Pattern recognition (psychology)",
      "Resampling"
    ],
    "authors": [
      {
        "surname": "Teng",
        "given_name": "Zeyu"
      },
      {
        "surname": "Cao",
        "given_name": "Peng"
      },
      {
        "surname": "Huang",
        "given_name": "Min"
      },
      {
        "surname": "Gao",
        "given_name": "Zheming"
      },
      {
        "surname": "Wang",
        "given_name": "Xingwei"
      }
    ]
  },
  {
    "title": "Network pruning via resource reallocation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109886",
    "abstract": "Channel pruning is broadly recognized as an effective approach to obtain a small compact model through eliminating unimportant channels from a large cumbersome network. Contemporary methods typically perform iterative pruning procedure from the original over-parameterized model, which is both tedious and expensive especially when the pruning is aggressive. In this paper, we propose a simple yet effective channel pruning technique, termed network Pruning via rEsource rEalLocation (PEEL), to quickly produce a desired slim model with negligible cost. Specifically, PEEL first constructs a predefined backbone and then conducts resource reallocation on it to shift parameters from less informative layers to more important layers in one round, thus amplifying the positive effect of these informative layers. To demonstrate the effectiveness of PEEL , we perform extensive experiments on ImageNet with ResNet-18, ResNet-50, MobileNetV2, MobileNetV3-small and EfficientNet-B0. Experimental results show that structures uncovered by PEEL exhibit competitive performance with state-of-the-art pruning algorithms under various pruning settings. Encouraging results are also observed when applying PEEL to compress the semantic segmentation model. Our code is available at https://github.com/cardwing/Codes-for-PEEL.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005848",
    "keywords": [
      "Agronomy",
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Channel (broadcasting)",
      "Code (set theory)",
      "Computer network",
      "Computer science",
      "Deep learning",
      "Epistemology",
      "Machine learning",
      "Parameterized complexity",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Pruning",
      "Residual neural network",
      "Resource (disambiguation)",
      "Segmentation",
      "Set (abstract data type)",
      "Simple (philosophy)"
    ],
    "authors": [
      {
        "surname": "Hou",
        "given_name": "Yuenan"
      },
      {
        "surname": "Ma",
        "given_name": "Zheng"
      },
      {
        "surname": "Liu",
        "given_name": "Chunxiao"
      },
      {
        "surname": "Wang",
        "given_name": "Zhe"
      },
      {
        "surname": "Loy",
        "given_name": "Chen Change"
      }
    ]
  },
  {
    "title": "Robust implementation of foreground extraction and vessel segmentation for X-ray coronary angiography image sequence",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109926",
    "abstract": "The extraction of contrast-filled vessels from X-ray coronary angiography (XCA) image sequence has important clinical significance for intuitively diagnosis and therapy. In this study, the XCA image sequence is regarded as a 3D tensor input, the vessel layer is regarded as a sparse tensor, and the background layer is regarded as a low-rank tensor. Using tensor nuclear norm (TNN) minimization, a novel method for vessel layer extraction based on tensor robust principal component analysis (TRPCA) is proposed. Furthermore, considering the irregular movement of vessels and the low-frequency dynamic disturbance of surrounding irrelevant tissues, the total variation (TV) regularized spatial–temporal constraint is introduced to smooth the foreground layer. Subsequently, for vessel layer images with uneven contrast distribution, a two-stage region growing (TSRG) method is utilized for vessel enhancement and segmentation. A global threshold method is used as the preprocessing to obtain main branches, and the Radon-Like features (RLF) filter is used to enhance and connect broken minor segments. The final binary vessel mask is constructed by combining the two intermediate results. The visibility of TV-TRPCA algorithm for foreground extraction is evaluated on clinical XCA image sequences and third-party dataset, which can effectively improve the performance of commonly used vessel segmentation algorithms. Based on TV-TRPCA, the accuracy of TSRG algorithm for vessel segmentation is further evaluated. Both qualitative and quantitative results validate the superiority of the proposed method over existing state-of-the-art approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323006246",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Pattern recognition (psychology)",
      "Preprocessor",
      "Principal component analysis",
      "Robust principal component analysis",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Fu",
        "given_name": "Zeyu"
      },
      {
        "surname": "Fu",
        "given_name": "Zhuang"
      },
      {
        "surname": "Lu",
        "given_name": "Chenzhuo"
      },
      {
        "surname": "Yan",
        "given_name": "Jun"
      },
      {
        "surname": "Fei",
        "given_name": "Jian"
      },
      {
        "surname": "Han",
        "given_name": "Hui"
      }
    ]
  },
  {
    "title": "Visual camera relocalization using both hand-crafted and learned features",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109914",
    "abstract": "The localization of the camera is essential in AR, MR, and robotics. Diverse pipelines employ a hand-crafted or learning based way to predict the camera pose as per the task. In the localization process, both weaknesses and strengths are maintained. However, few current frameworks consider these two features simultaneously. In this study, a novel relocalization pipeline for RGB or RGB-D input is proposed, including a coarse stage with learned features, further refinement with hand-crafted features, and a stable process to measure the confidence of both stages for improving localization robustness. Instead of directly regressing the camera pose, the coarse procedure uses registration to the known source and predicted weighted target point cloud to obtain the initial result. Therefore, we design a deep network called PGNet to construct the weighted target point cloud with the image and previous poses as inputs. Moreover, in consideration of dynamic surroundings, we add a segmentation branch distinguishing each point as either fixed or dynamic with the purpose of promoting dynamic perception. Correspondingly, the segmentation-extended Chamfer Distance is added to optimize PGNet. During the pose refinement, the feature space is established via hand-crafted feature extraction and matching on the training set. Based on the coarse pose, we obtain the accurate pose by applying Kabsch or Perspective-n-Point (PnP) algorithm to point-to-point correspondences built through searching the space and matching Oriented Fast and Rotated Brief (ORB) features. Furthermore, an additional process is presented by defining coarse and refinement metrics to gain a more stable performance. Finally, experiments on both static and dynamic scenes are conducted. On the one side, the results demonstrate the state-of-the-art performance over other existing methods on 7 Scenes, INDOOR-6, Cambridge Landmarks and TUM RGB-D. On the other side, the positive effects of the pose learning part, dynamic branch, confidence regression and hand-crafted feature based refinement are also provided.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032300612X",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Gene",
      "Matching (statistics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pipeline (software)",
      "Point cloud",
      "Pose",
      "Programming language",
      "RGB color model",
      "Robustness (evolution)",
      "Segmentation",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Junyi"
      },
      {
        "surname": "Qi",
        "given_name": "Yue"
      }
    ]
  },
  {
    "title": "Benchmarking deep models on salient object detection",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109951",
    "abstract": "The performance discrepancies caused by different implementation details may obscure the actual progress of the Salient Object Detection (SOD) task. In this paper, we construct a SALient Object Detection (SALOD) benchmark to ensure a fair and comprehensive evaluation by unifying the implementation details of SOD methods. By doing so, we can reveal the reasons behind recent progress by analyzing the impact of network structure and optimization strategy. Based on the experimental results, we first find that U-shaped networks, both older and more recent variants, achieve better performance than other structures. Second, optimization strategy, e.g., training strategy and loss function, significantly impacts SOD accuracy. Finally, we provide a new perspective to validate the generalizability of SOD methods on objectness shifting. Code is available at https://github.com/moothes/SALOD.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323006490",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Benchmarking",
      "Business",
      "Code (set theory)",
      "Computer science",
      "Construct (python library)",
      "Developmental psychology",
      "Generalizability theory",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Marketing",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Perspective (graphical)",
      "Programming language",
      "Psychology",
      "Salient",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Huajun"
      },
      {
        "surname": "Lin",
        "given_name": "Yang"
      },
      {
        "surname": "Yang",
        "given_name": "Lingxiao"
      },
      {
        "surname": "Lai",
        "given_name": "Jianhuang"
      },
      {
        "surname": "Xie",
        "given_name": "Xiaohua"
      }
    ]
  },
  {
    "title": "Spammer detection on short video applications",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2023.11.020",
    "abstract": "Users can interact with the advertisements and share their impressions through the review system on short video applications. However, spammers may post false or malicious comments to mislead normal users due to profit-driven reasons, damaging the community’s positive atmosphere. In this paper, we introduce a new challenge of spammer detection on short video applications, where the multi-modal information of videos and reviews plays a more critical role than the spam relation graph. Then we propose SPAM-3, a novel baseline to detect SPAM reviews with Multi-Modal representation using attentive heterogeneous graph convolution. Our approach balances multi-modal representation fusion and graph relation extraction, enabling fine-grained interaction and generating discriminative features for the spammer classification task. We describe the methodology of dataset construction in detail and reveal the statistical properties of the collected dataset. SPAM-3 outperforms the former baseline models on both private and public benchmarks. Furthermore, we conduct comprehensive ablations and analyses to demonstrate our method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523003306",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Data mining",
      "Discriminative model",
      "Graph",
      "Information retrieval",
      "Law",
      "Machine learning",
      "Modal",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Polymer chemistry",
      "Representation (politics)",
      "Spambot",
      "Spamming",
      "The Internet",
      "Theoretical computer science",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Yi",
        "given_name": "Muyang"
      },
      {
        "surname": "Liang",
        "given_name": "Dong"
      },
      {
        "surname": "Wang",
        "given_name": "Rui"
      },
      {
        "surname": "Ding",
        "given_name": "Yue"
      },
      {
        "surname": "Lu",
        "given_name": "Hongtao"
      }
    ]
  },
  {
    "title": "Basis scaling and double pruning for efficient inference in network-based transfer learning",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2023.11.026",
    "abstract": "Network-based transfer learning allows the reuse of deep learning features with limited data, but the resulting models can be unnecessarily large. Although network pruning can improve inference efficiency, existing algorithms usually require fine-tuning that may not be suitable for small datasets. In this paper, using the singular value decomposition, we decompose a convolutional layer into two layers: a convolutional layer with the orthonormal basis vectors as the filters, and a “BasisScalingConv” layer which is responsible for rescaling the features and transforming them back to the original space. As the filters in each decomposed layer are linearly independent, when using the proposed basis scaling factors with the Taylor approximation of importance, pruning can be more effective and fine-tuning individual weights is unnecessary. Furthermore, as the numbers of input and output channels of the original convolutional layer remain unchanged after basis pruning, it is applicable to virtually all architectures and can be combined with existing pruning algorithms for double pruning to further increase the pruning capability. When transferring knowledge from ImageNet pre-trained models to different target domains, with less than 1% reduction in classification accuracies, we can achieve pruning ratios up to 74.6% for CIFAR-10 and 98.9% for MNIST in model parameters.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523003367",
    "keywords": [
      "Agronomy",
      "Algorithm",
      "Artificial intelligence",
      "Basis (linear algebra)",
      "Biology",
      "Computer science",
      "Deep learning",
      "Geometry",
      "Inference",
      "MNIST database",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pruning",
      "Reduction (mathematics)",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Wong",
        "given_name": "Ken C.L."
      },
      {
        "surname": "Kashyap",
        "given_name": "Satyananda"
      },
      {
        "surname": "Moradi",
        "given_name": "Mehdi"
      }
    ]
  },
  {
    "title": "Sequence-level affective level estimation based on pyramidal facial expression features",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109958",
    "abstract": "People tend to focus on changes in a certain complex human affect in the majority of practical applications of affective computing. Facial expression classification models are unable to represent all human affects through a limited number of expression categories. In this backdrop, this paper studies the Sequence-level affective level estimation (S-ALE), which is more relevant to real scenarios and can depict individual affective level in continuous manner. A spatio-temporal framework applied to S-ALE is proposed, which consists of a Facial Expression Features Pyramid Network (FEFPN) and a Temporal Transformer Encoder (TTE). FEFPN is capable of extracting pyramidal facial expression features, while TTE can effectively capture coarse-grained and fine-grained temporal variations of facial sequences. The proposed model is evaluated on six public datasets across three typical S-ALE tasks (engagement prediction, fatigue detection, and pain assessment), and experimental results show that our method is comparable to or outperforms the state-of-the-art algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323006568",
    "keywords": [
      "Affective computing",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Encoder",
      "Expression (computer science)",
      "Facial expression",
      "Focus (optics)",
      "Genetics",
      "Geometry",
      "Mathematics",
      "Operating system",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Programming language",
      "Pyramid (geometry)",
      "Sequence (biology)",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Liao",
        "given_name": "Jiacheng"
      },
      {
        "surname": "Hao",
        "given_name": "Yan"
      },
      {
        "surname": "Zhou",
        "given_name": "Zhuoyi"
      },
      {
        "surname": "Pan",
        "given_name": "Jiahui"
      },
      {
        "surname": "Liang",
        "given_name": "Yan"
      }
    ]
  },
  {
    "title": "InvFlow: Involution and multi-scale interaction for unsupervised learning of optical flow",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109918",
    "abstract": "The convolution neural network is still the main tool for extracting the image features and the motion features for most of the optical flow models. The convolution neural networks cannot model the long-range dependencies, and more details are lost in deeper layers. All the deficiencies in the extracted features affect the estimated flow. Therefore, in this work, we concentrated on optimizing the convolution neural network in both the encoder and decoder parts to improve the image and motion features. To enhance the image features, we utilize the involution to provide rich features and model the long-range dependencies. In addition, we propose a Multi-Scale-Interaction module which utilizes the self-attention to make an interaction between the feature scales to avoid detail loss. Additionally, we propose a Motion-Features-Optimization block that utilizes the deformable convolution to enhance the motion features. Our model achieves the state-of-the-art performance on Sintel and KITTI 2015 benchmarks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323006167",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer vision",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Encoder",
      "Feature (linguistics)",
      "Image (mathematics)",
      "Linguistics",
      "Operating system",
      "Optical flow",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Xiang",
        "given_name": "Xuezhi"
      },
      {
        "surname": "Abdein",
        "given_name": "Rokia"
      },
      {
        "surname": "Lv",
        "given_name": "Ning"
      },
      {
        "surname": "Saddik",
        "given_name": "Abdulmotaleb El"
      }
    ]
  },
  {
    "title": "Hyperbolic prototypical network for few shot remote sensing scene classification",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2023.11.021",
    "abstract": "Recently, in the computer vision and machine learning (ML) communities, a growing interest has been directed to similarity measures operating in hyperbolic spaces due to the geometric properties of these spaces which make them very suitable for embedding data with an underlying hierarchy. These hyperbolic spaces, although increasingly adopted, have received limited attention in the remote sensing (RS) community despite the hierarchical nature of RS data. The objective of this study is therefore to examine the relevance of hyperbolic embeddings of RS data, in particular when addressing the few-shot remote sensing scene classification problem. We adopt hyperbolic prototypical networks as a meta-learning approach to embed scene images along with a feature clipping technique to ensure a more numerically steady model. We then examine whether hyperbolic embeddings provide a better representation than Euclidean representations and better reflect the underlying structure of scene classes. Experimental results on the NWPU-RESISC45 RS dataset demonstrate the superiority of hyperbolic embeddings over their Euclidean counterparts. Our study provides a new perspective by suggesting that operating in hyperbolic spaces is an interesting alternative for the RS community.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523003318",
    "keywords": [
      "Artificial intelligence",
      "Clipping (morphology)",
      "Computer science",
      "Economics",
      "Embedding",
      "Euclidean geometry",
      "Feature (linguistics)",
      "Geometry",
      "Hierarchy",
      "Hyperbolic function",
      "Hyperbolic manifold",
      "Hyperbolic space",
      "Image (mathematics)",
      "Law",
      "Linguistics",
      "Market economy",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Perspective (graphical)",
      "Philosophy",
      "Political science",
      "Politics",
      "Pure mathematics",
      "Relevance (law)",
      "Representation (politics)",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Hamzaoui",
        "given_name": "Manal"
      },
      {
        "surname": "Chapel",
        "given_name": "Laetitia"
      },
      {
        "surname": "Pham",
        "given_name": "Minh-Tan"
      },
      {
        "surname": "Lefèvre",
        "given_name": "Sébastien"
      }
    ]
  },
  {
    "title": "Minimum description length clustering to measure meaningful image complexity",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109889",
    "abstract": "We present a new image complexity metric. Existing complexity metrics cannot distinguish meaningful content from noise, and give a high score to white noise images, which contain no meaningful information. We use the minimum description length principle to determine the number of clusters and designate certain points as outliers and, hence, correctly assign white noise a low score. The presented method is a step towards humans’ ability to detect when data contain a meaningful pattern. It also has similarities to theoretical ideas for measuring meaningful complexity. We conduct experiments on seven different sets of images, which show that our method assigns the most accurate scores to all images considered. Additionally, comparing the different levels of the hierarchy of clusters can reveal how complexity manifests at different scales, from local detail to global structure. We then present ablation studies showing the contribution of the components of our method, and that it continues to assign reasonable scores when the inputs are modified in certain ways, including the addition of Gaussian noise and the lowering of the resolution. Code is available at https://github.com/Lou1sM/meaningful_image_complexity.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005873",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Cluster analysis",
      "Computational complexity theory",
      "Computer science",
      "Data mining",
      "Economics",
      "Hierarchy",
      "Image (mathematics)",
      "Market economy",
      "Mathematics",
      "Measure (data warehouse)",
      "Metric (unit)",
      "Minimum description length",
      "Noise (video)",
      "Operations management",
      "Outlier",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Mahon",
        "given_name": "Louis"
      },
      {
        "surname": "Lukasiewicz",
        "given_name": "Thomas"
      }
    ]
  },
  {
    "title": "Clustering performance analysis using a new correlation-based cluster validity index",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109910",
    "abstract": "There are various cluster validity indices used for evaluating clustering results. One of the main objectives of using these indices is to seek the optimal unknown number of clusters. Some indices work well for clusters with different densities, sizes, and shapes. Yet, one shared weakness of those validity indices is that they often provide only one optimal number of clusters. That number is unknown in real-world problems, and there might be more than one possible option. We develop a new cluster validity index based on a correlation between an actual distance between a pair of data points and a centroid distance of clusters that the two points occupy. Our proposed index constantly yields several local peaks and overcomes the previously stated weakness. Several experiments in different scenarios, including UCI real-world data sets, have been conducted to compare the proposed validity index with several well-known ones.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323006088",
    "keywords": [
      "Artificial intelligence",
      "Centroid",
      "Cluster (spacecraft)",
      "Cluster analysis",
      "Computer science",
      "Correlation",
      "Data mining",
      "Geometry",
      "Index (typography)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Programming language",
      "Statistics",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Wiroonsri",
        "given_name": "Nathakhun"
      }
    ]
  },
  {
    "title": "Personalized recommendation via inductive spatiotemporal graph neural network",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109884",
    "abstract": "Graph neural network-based collaborative filtering methods have achieved excellent performance in recommender systems. However, previous works have primarily focused on representation using the entire graph or a single sampled subgraph, which fails to address the issue of noise introduced by prolonged ineffective interactions. Moreover, these approaches lack the capacity to model temporal information in an inductive manner, thereby limiting their ability to fully capture the evolving dynamic interests of users over time in a real world scenario. To address the aforementioned challenges, we propose a novel personalized inductive spatiotemporal graph neural network-based framework PistGNN. PistGNN extracts multiple temporal-aware subgraphs from user–item pairs to avoid long-time gap meaningless interactions, which are then fed into a spatiotemporal graph neural network module. It is worth noting that PistGNN trains the model only by relying on subgraphs extracted from bipartite graphs, without depending on global information, and is therefore capable of predicting for new users or items. Finally, we propose an attention-based meta-learning method to personalize the aggregation of subgraphs’ embeddings. Extensive experiments conducted on four real-world datasets demonstrate the superiority of PistGNN over both inductive and transductive baseline methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005824",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Bipartite graph",
      "Collaborative filtering",
      "Computer science",
      "Engineering",
      "Graph",
      "Limiting",
      "Machine learning",
      "Mechanical engineering",
      "Recommender system",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Gong",
        "given_name": "Jibing"
      },
      {
        "surname": "Zhao",
        "given_name": "Yi"
      },
      {
        "surname": "Zhao",
        "given_name": "Jinye"
      },
      {
        "surname": "Zhang",
        "given_name": "Jin"
      },
      {
        "surname": "Ma",
        "given_name": "Guixiang"
      },
      {
        "surname": "Zheng",
        "given_name": "Shaojie"
      },
      {
        "surname": "Du",
        "given_name": "Shuying"
      },
      {
        "surname": "Tang",
        "given_name": "Jie"
      }
    ]
  },
  {
    "title": "A novel interval dual convolutional neural network method for interval-valued stock price prediction",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109920",
    "abstract": "Accurate interval-valued stock price prediction is challenging and of great interest to investors and for-profit organizations. In this study, by considering individual stock information and relevant stock information simultaneously, we propose a novel interval dual convolutional neural network (Dual-CNN I ) model based method to predict interval-valued stock prices. First, the individual and relevant stock information are collected and transformed into images. Then, the Dual-CNN I model is proposed to predict interval-valued stock prices. Specifically, two convolutional neural network (CNN) models with different structures are constructed to respectively extract individual stock features and relevant stock features, and then an interval multilayer perceptron (MLP I ) model is used for final interval-valued stock price prediction. Finally, extensive experiments are conducted based on six randomly selected stocks, with comparison to several popular machine learning model based methods and interval-valued time series (ITS) prediction methods. The experimental results indicate that the proposed Dual-CNN I based method has superior predictive ability.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323006180",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Combinatorics",
      "Computer science",
      "Convolutional neural network",
      "Econometrics",
      "Engineering",
      "Interval (graph theory)",
      "Machine learning",
      "Mathematics",
      "Mechanical engineering",
      "Multilayer perceptron",
      "Paleontology",
      "Series (stratigraphy)",
      "Stock (firearms)",
      "Stock price"
    ],
    "authors": [
      {
        "surname": "Jiang",
        "given_name": "Manrui"
      },
      {
        "surname": "Chen",
        "given_name": "Wei"
      },
      {
        "surname": "Xu",
        "given_name": "Huilin"
      },
      {
        "surname": "Liu",
        "given_name": "Yanxin"
      }
    ]
  },
  {
    "title": "Relative-position embedding based spatially and temporally decoupled Transformer for action recognition",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109905",
    "abstract": "Recognition of human actions is to classify actions in a video. Recently, Vision Transformer (ViT) has been applied to action recognition. However, the Vision Transformer is unsuitable for high-resolution input videos due to the constraint of computing power since ViT splits frames into fixed-size patches embedded (i.e., tokens) with absolute-position information and adopts a pure Transformer encoder to model the relationships among these tokens. To address this issue, we propose a relative-position embedding based spatially and temporally decoupled Transformer (RPE-STDT) for action recognition, which can capture spatial–temporal information by stacked self-attention layers. The proposed RPE-STDT model consists of two separate series of Transformer encoders. The first series of encoders is the spatial Transformer encoders, which model interactions between tokens extracted from the same temporal index. The second series of encoders is the temporal Transformer encoders, which model interactions across time dimensions with a subsampling strategy. Furthermore, we replace the absolute-position embeddings in the Vision Transformer encoders with the proposed relative-position embeddings to capture the order of the embedded tokens to reduce computational costs. Finally, we conduct thorough ablation studies. Our RPE-STDT achieves state-of-the-art results on multiple action recognition datasets, exceeding prior convolution and Transformer-based networks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323006039",
    "keywords": [
      "Action recognition",
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Computer vision",
      "Electrical engineering",
      "Embedding",
      "Encoder",
      "Engineering",
      "Operating system",
      "Pattern recognition (psychology)",
      "Speech recognition",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Ma",
        "given_name": "Yujun"
      },
      {
        "surname": "Wang",
        "given_name": "Ruili"
      }
    ]
  },
  {
    "title": "ICAFusion: Iterative cross-attention guided feature fusion for multispectral object detection",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109913",
    "abstract": "Effective feature fusion of multispectral images plays a crucial role in multispectral object detection. Previous studies have demonstrated the effectiveness of feature fusion using convolutional neural networks, but these methods are sensitive to image misalignment due to the inherent deficiency in local-range feature interaction resulting in the performance degradation. To address this issue, a novel feature fusion framework of dual cross-attention transformers is proposed to model global feature interaction and capture complementary information across modalities simultaneously. This framework enhances the discriminability of object features through the query-guided cross-attention mechanism, leading to improved performance. However, stacking multiple transformer blocks for feature enhancement incurs a large number of parameters and high spatial complexity. To handle this, inspired by the human process of reviewing knowledge, an iterative interaction mechanism is proposed to share parameters among block-wise multimodal transformers, reducing model complexity and computation cost. The proposed method is general and effective to be integrated into different detection frameworks and used with different backbones. Experimental results on KAIST, FLIR, and VEDAI datasets show that the proposed method achieves superior performance and faster inference, making it suitable for various practical scenarios. Code will be available at https://github.com/chanchanchan97/ICAFusion.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323006118",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Feature (linguistics)",
      "Feature extraction",
      "Iterative and incremental development",
      "Linguistics",
      "Multispectral image",
      "Object detection",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Software engineering"
    ],
    "authors": [
      {
        "surname": "Shen",
        "given_name": "Jifeng"
      },
      {
        "surname": "Chen",
        "given_name": "Yifei"
      },
      {
        "surname": "Liu",
        "given_name": "Yue"
      },
      {
        "surname": "Zuo",
        "given_name": "Xin"
      },
      {
        "surname": "Fan",
        "given_name": "Heng"
      },
      {
        "surname": "Yang",
        "given_name": "Wankou"
      }
    ]
  },
  {
    "title": "Robust online hashing with label semantic enhancement for cross-modal retrieval",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109972",
    "abstract": "Online hashing technology has attracted extensive attention owing to its effectiveness and efficiency in processing large-scale streaming data. However, there are still some limitations: (1) In practical applications, the observed labels of multimedia data are obtained through manual annotation, which may inevitably introduce some noises into labels. This may lead to retrieval performance degradation when the noisy labels are directly applied to retrieval tasks. (2) The potential semantic correlation of multi-labels cannot be fully explored. To overcome these limitations, in this paper, we propose robust online hashing with label semantic enhancement (ROHLSE). Specifically, ROHLSE seeks to recover the clean labels from the provided noisy labels by imposing low-rank and sparse constraints. Meanwhile, it employs the representation of samples in the feature space to predict the labels via the dependency between sample instances and labels. To efficiently handle streaming data, ROHLSE preserves the similarity between new data, and establishes the semantic relationships between new and old data through chunk similarity, simultaneously. Furthermore, ROHLSE can fully utilize the semantic correlations between multiple labels of each instance. Extensive experiments are conducted on three benchmark datasets to demonstrate the superiority of the proposed ROHLSE approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323006702",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Computer security",
      "Data mining",
      "Feature (linguistics)",
      "Geodesy",
      "Geography",
      "Hash function",
      "Image (mathematics)",
      "Information retrieval",
      "Linguistics",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Semantic similarity",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Li"
      },
      {
        "surname": "Shu",
        "given_name": "Zhenqiu"
      },
      {
        "surname": "Yu",
        "given_name": "Zhengtao"
      },
      {
        "surname": "Wu",
        "given_name": "Xiao-Jun"
      }
    ]
  },
  {
    "title": "Robust embedding regression for semi-supervised learning",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109894",
    "abstract": "To utilize both labeled data and unlabeled data in real-world applications, semi-supervised learning is widely used as an effective technique. However, most semi-supervised methods do not perform well when there are many noises and redundant information in the original data. To address these issues, in this paper, we proposed a novel approach called robust embedding regression (RER) for semi-supervised learning by inheriting the advantages of the existing semi-supervised learning, robust linear regression, and low-rank representation techniques. Specifically, RER constructs a more robust and accurate graph by adaptively arranging the weight coefficient for each data point. Furthermore, the low-rank representation is introduced to reduce the negative influence of the redundant features and noises residing in the original data while the graph construction. More importantly, the proper norms are imposed on both the reconstruction and regularization terms to further improve the robustness and earn feature/sample selection. We designed an effective iterative algorithm to optimize the problem of RER. Comprehensive experimental results conducted on both synthetic and real-world datasets indicate that RER is superior in classification and clustering performance and robust to different types of noise compared with the existing semi-supervised methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005927",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Embedding",
      "Feature selection",
      "Gene",
      "Graph",
      "Graph embedding",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Regression",
      "Regularization (linguistics)",
      "Robustness (evolution)",
      "Semi-supervised learning",
      "Statistics",
      "Supervised learning",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Bao",
        "given_name": "Jiaqi"
      },
      {
        "surname": "Kudo",
        "given_name": "Mineichi"
      },
      {
        "surname": "Kimura",
        "given_name": "Keigo"
      },
      {
        "surname": "Sun",
        "given_name": "Lu"
      }
    ]
  },
  {
    "title": "A quantitative method for the assessment of facial attractiveness based on transfer learning with fine-grained image classification",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109970",
    "abstract": "In this paper, we investigate a new approach based on a combination of three-dimensional (3D) facial images and deep transfer learning (TL) with fine-grained image classification (FGIC) for quantitative evaluation of facial attractiveness. The 3D facial surface images of patients with and without filtering and the publicly available SCUT-FBP5500 dataset was used for transfer training and model pre-training, respectively. Experimental results show that a bilinear CNN model with a Gaussian filter freezing 80 % of the weights exhibit the strongest performance and lowest average error as a deep learning prediction model; the model was subsequently adopted for automatic assessment of facial attractiveness in clinical application. This is the first TL model with FGIC using 3D facial images for automatic quantitative evaluation of facial attractiveness in patients undergoing Orthognathic surgery (OGS). The developed web browser–based user interface enables effective and rapid assessment, thus contributing to effective patient–clinician communication and decision-making.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323006684",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Pattern recognition (psychology)",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Lo",
        "given_name": "Lun-Jou"
      },
      {
        "surname": "Yang",
        "given_name": "Chao-Tung"
      },
      {
        "surname": "Chiang",
        "given_name": "Wen-Chung"
      },
      {
        "surname": "Lin",
        "given_name": "Hsiu-Hsia"
      }
    ]
  },
  {
    "title": "PSCFormer: A lightweight hybrid network for gas identification in electronic nose system",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109912",
    "abstract": "Based on their powerful feature extraction capability, a convolutional neural network (CNN) has been gradually applied to gas identification in the electronic nose (e-nose) system. The responses of different intensities in the e-nose system are significantly correlated, and CNN extracts the local gas features by convolution while ignoring their global correlation. Transformer combines different responses and obtains the correlation between global features by self-attention. This paper proposes a lightweight hybrid network called Peak Search-based Convolutional Transformers (PSCFormer). First, combining the data characteristics of gas information, the Local Peak Search and Feature Fusion (LPSF) module is proposed to focus on the key gas features. Second, Transformer Encoder (TE) is proposed to obtain the global correlation between global features, and the parallel Convolution Encoder (CE) is proposed to capture the local dependence. Finally, a reasonable feature complementation mechanism is presented, and the preference of TE is alleviated for the slow-down response while solving the receptive field limitation of CE. This paper has evaluated three different datasets to validate the effectiveness of PSCFormer, all of which show stable and excellent performance with a good tradeoff between efficiency and complexity. The results prove that PSCFormer is an efficient and lightweight gas identification network, which provides a method to promote the engineering application of the e-nose system.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323006106",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Correlation",
      "Electronic nose",
      "Encoder",
      "Feature (linguistics)",
      "Feature extraction",
      "Geometry",
      "Linguistics",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Ziyang"
      },
      {
        "surname": "Kang",
        "given_name": "Siyuan"
      },
      {
        "surname": "Feng",
        "given_name": "Ninghui"
      },
      {
        "surname": "Yin",
        "given_name": "Chongbo"
      },
      {
        "surname": "Shi",
        "given_name": "Yan"
      }
    ]
  },
  {
    "title": "Feature disentanglement in one-stage object detection",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109878",
    "abstract": "In this paper, an enhanced disentanglement module is proposed to address feature misalignment caused by inherently irreconcilable conflicts between classification and regression tasks in Convolutional Neural Network-based object detectors. The proposed method disentangles features in the feature pyramid network (FPN) at the neck of the architecture. In addition, a response alignment strategy is proposed to reduce inconsistent responses and suppress inferior predictions. Extensive experiments are performed on the MS COCO and PASCAL VOC datasets with different backbones, confirming that the proposed method improves performance significantly. The proposed method exhibits two main advantages over existing solutions—features are disentangled at the neck instead of at the head, enabling comprehensive resolution of feature misalignment, and independent outputs of the two tasks after feature disentanglement are avoided, thereby preventing response inconsistencies.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005769",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Detector",
      "Feature (linguistics)",
      "Feature extraction",
      "Geometry",
      "Linguistics",
      "Mathematics",
      "Object (grammar)",
      "Object detection",
      "Pascal (unit)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Pyramid (geometry)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Lin",
        "given_name": "Wenjie"
      },
      {
        "surname": "Chu",
        "given_name": "Jun"
      },
      {
        "surname": "Leng",
        "given_name": "Lu"
      },
      {
        "surname": "Miao",
        "given_name": "Jun"
      },
      {
        "surname": "Wang",
        "given_name": "Lingfeng"
      }
    ]
  },
  {
    "title": "MMAN-M2: Multiple multi-head attentions network based on encoder with missing modalities",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2023.11.029",
    "abstract": "Multi-modal fusion is a hot topic in field of multi-modal learning. Most of the previous multi-modal fusion tasks are based on the complete modality. Existing researches on missing multi-modal fusion fail to consider the random missing of modalities, thereby lacking robustness. And most of methods are based on the correlation between missing and non-missing modalities, ignoring missing modalities contextual information. Considering the above two issues, we designed a multiple multi-head attentions network based on encoder with missing modalities (MMAN-M2). Firstly, the multi-head attention network is used to represent the single modality by extracting potential features based on the entire sequence, and then they are fused; Then, the missing modality context features are extracted by optimizing the result of multi-modal fusion including missing and non-missing features data, and the missing modalities are encoded through the encoding module; Finally, the Transformer encoder-decoder module is used to train the network model by mapping obtaining global information to multiple spaces and integrating our uncertain multi-modal encoding, and it realizes the classification of multi-modal fusion for evaluating model performance. Extensive experiments on multi-modal public datasets show that the proposed method has the best effect and can effectively improve the classification performance of multi-modal fusion.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523003458",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Data mining",
      "Encoder",
      "Fusion",
      "Gene",
      "Linguistics",
      "Machine learning",
      "Missing data",
      "Modal",
      "Modalities",
      "Modality (human–computer interaction)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Polymer chemistry",
      "Robustness (evolution)",
      "Sensor fusion",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Jiayao"
      },
      {
        "surname": "Li",
        "given_name": "Li"
      },
      {
        "surname": "Sun",
        "given_name": "Ruizhi"
      },
      {
        "surname": "Yuan",
        "given_name": "Gang"
      },
      {
        "surname": "Wang",
        "given_name": "Shufan"
      },
      {
        "surname": "Sun",
        "given_name": "Shulin"
      }
    ]
  },
  {
    "title": "PAMI: Partition Input and Aggregate Outputs for Model Interpretation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109898",
    "abstract": "There is an increasing demand for interpretation of model predictions especially in high-risk applications. Various visualization approaches have been proposed to estimate the part of input which is relevant to a specific model prediction. However, most approaches require model structure and parameter details in order to obtain the visualization results, and in general much effort is required to adapt each approach to multiple types of tasks particularly when model backbone and input format change over tasks. In this study, a simple yet effective visualization framework called PAMI is proposed based on the observation that deep learning models often aggregate features from local regions for model predictions. The basic idea is to mask majority of the input and use the corresponding model output as the relative contribution of the preserved input part to the original model prediction. For each input, since only a set of model outputs are collected and aggregated, PAMI does not require any model detail and can be applied to various prediction tasks with different model backbones and input formats. Extensive experiments on multiple tasks confirm the proposed method performs better than existing visualization approaches in more precisely finding class-specific input regions, and when applied to different model backbones and input formats. The source code is available at https://github.com/fuermowei/PAMI.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005964",
    "keywords": [
      "Aggregate (composite)",
      "Artificial intelligence",
      "Class (philosophy)",
      "Code (set theory)",
      "Combinatorics",
      "Composite material",
      "Computer science",
      "Data mining",
      "Interpretation (philosophy)",
      "Machine learning",
      "Materials science",
      "Mathematics",
      "Partition (number theory)",
      "Programming language",
      "Set (abstract data type)",
      "Source code",
      "Visualization"
    ],
    "authors": [
      {
        "surname": "Shi",
        "given_name": "Wei"
      },
      {
        "surname": "Zhang",
        "given_name": "Wentao"
      },
      {
        "surname": "Zheng",
        "given_name": "Wei-shi"
      },
      {
        "surname": "Wang",
        "given_name": "Ruixuan"
      }
    ]
  },
  {
    "title": "Training feedforward neural nets in Hopfield-energy-based configuration: A two-step approach",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109954",
    "abstract": "We introduce Hopfield-Energy-Based Learning, a general learning framework that is inspired by energy-based models, to train feedforward neural nets. Our approach includes two training phases applied iteratively: first, the minimization of the internal energy, which captures dependencies between input samples and network parameters, is carried out in an unsupervised manner; second, the problem-dependent supervised external energy (e.g., cross-entropy loss) combined with partially reversed internal energy gradients are back-propagated in a standard manner. The intuition is that the first stage helps parameters to settle into the state that simply partitions data into clusters; while in the second stage, the network is allowed to deviate from that clustering a bit (hence gradient reversal) in order to converge to parameters that ultimately perform well on the task at hand. Notably, the data used for the two steps might not be one and the same (e.g., can come from different domains) and the approach naturally tailors itself to solve unsupervised domain adaptation problems without adopting any distribution alignment techniques. We also show that the proposed training strategy substantially improves the performance of several ConvNets on standard supervised classification tasks; showing improvements of at least 1.2% (2.64% on CIFAR-10, 4.5% on CIFAR-100, and 1.35% on ImageNet). Our formulation is general, performs well in practice, and holds promise for scenarios where labeled data is limited.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323006520",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Chemistry",
      "Cluster analysis",
      "Computational chemistry",
      "Computer science",
      "Control engineering",
      "Energy (signal processing)",
      "Energy minimization",
      "Engineering",
      "Entropy (arrow of time)",
      "Epistemology",
      "Feed forward",
      "Feedforward neural network",
      "Hopfield network",
      "Intuition",
      "Machine learning",
      "Mathematics",
      "Minification",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Programming language",
      "Quantum mechanics",
      "Statistics",
      "Unsupervised learning"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Jing"
      },
      {
        "surname": "Chen",
        "given_name": "Jiahong"
      },
      {
        "surname": "Zhang",
        "given_name": "Kuangen"
      },
      {
        "surname": "Sigal",
        "given_name": "Leonid"
      }
    ]
  },
  {
    "title": "MISL: Multi-grained image-text semantic learning for text-guided image inpainting",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109961",
    "abstract": "Text-guided image inpainting aims to generate corrupted image patches and obtain a plausible image based on textual descriptions, considering the relationship between textual and visual semantics. Existing works focus on predicting missing patches from the residual pixels of corrupted images, ignoring the visual semantics of the objects of interest in the images corresponding to the textual descriptions. In this paper, we propose a text-guided image inpainting method with multi-grained image-text semantic learning (MISL), consisting of global-local generators and discriminators. More specifically, we devise hierarchical learning (HL) with global-coarse-grained, object-fine-grained, and global-fine-grained learning stages in the global-local generators to refine the corrupted images from the global to local. In particular, the object-fine-grained learning stage focuses on the visual semantics of objects of interest in corrupted images by using an encoder-decoder network with self-attention blocks. Not only that, we design a mask reconstruction (MR) module to further act on the restoration of the objects of interest corresponding to the given textual descriptions. To inject the textual semantics into the global-local generators, we implement a multi-attention (MA) module that incorporates the word-level and sentence-level textual features to generate three different-grained images. For training, we exploit a global discriminator and a flexible discriminator to penalize the whole image and the corrupted region, respectively. Extensive experiments conducted on four datasets show the outperformance of the proposed MISL.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323006593",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Detector",
      "Discriminator",
      "Encoder",
      "Focus (optics)",
      "Geometry",
      "Image (mathematics)",
      "Inpainting",
      "Mathematics",
      "Natural language processing",
      "Object (grammar)",
      "Operating system",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Pixel",
      "Programming language",
      "Semantics (computer science)",
      "Sentence",
      "Telecommunications",
      "Word (group theory)"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Xingcai"
      },
      {
        "surname": "Zhao",
        "given_name": "Kejun"
      },
      {
        "surname": "Huang",
        "given_name": "Qianding"
      },
      {
        "surname": "Wang",
        "given_name": "Qi"
      },
      {
        "surname": "Yang",
        "given_name": "Zhenguo"
      },
      {
        "surname": "Hao",
        "given_name": "Gefei"
      }
    ]
  },
  {
    "title": "Representation transfer and data cleaning in multi-views for text simplification",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2023.11.011",
    "abstract": "Representation transfer is a widely used technique in natural language processing. We propose methods of cleaning the dominant dataset of text simplification (TS) WikiLarge in multi-views to remove errors that impact model training and fine-tuning. The results show that our method can effectively refine the dataset. We propose to take the pre-trained text representations from a similar task (e.g., text summarization) to text simplification to conduct a continue-fine-tuning strategy to improve the performance of pre-trained models on TS. This approach will speed up the training and make the model convergence easier. Besides, we also propose a new decoding strategy for simple text generation. It is able to generate simpler and more comprehensible text with controllable lexical simplicity. The experimental results show that our method can achieve good performance on many evaluation metrics.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523003215",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Automatic summarization",
      "Computer science",
      "Convergence (economics)",
      "Decoding methods",
      "Economic growth",
      "Economics",
      "Epistemology",
      "Law",
      "Machine learning",
      "Management",
      "Natural language processing",
      "Parallel computing",
      "Philosophy",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Sentence",
      "Simple (philosophy)",
      "Simplicity",
      "Task (project management)",
      "Text simplification",
      "Transfer (computing)"
    ],
    "authors": [
      {
        "surname": "He",
        "given_name": "Wei"
      },
      {
        "surname": "Farrahi",
        "given_name": "Katayoun"
      },
      {
        "surname": "Chen",
        "given_name": "Bin"
      },
      {
        "surname": "Peng",
        "given_name": "Bohua"
      },
      {
        "surname": "Villavicencio",
        "given_name": "Aline"
      }
    ]
  },
  {
    "title": "Exploring global information for session-based recommendation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109911",
    "abstract": "Session-based recommendation (SBR) aims to recommend items based on anonymous behavior sequences. However, most existing SBR approaches focus solely on the current session while neglecting the item-transition information from other sessions, which suffer from the inability of modeling the complicated item-transition. To address the limitations, we introduce global item-transition information to augment the modeling of item-transitions. Specifically, we first propose a basic GNN-based framework (BGNN), which solely uses session-level item-transition information. Based on BGNN, we propose a novel approach, called Session-based Recommendation with Global Information (SRGI), which infers the user preferences via fully exploring item-transitions over all sessions from two different perspectives: (i) Fusion-based Model (SRGI-FM), which recursively incorporates the neighbor embeddings of each node on global graph into the learning process of item representation; and (ii) Constrained-based Model (SRGI-CM), which treats the global-level information as a constraint to ensure the learned item embeddings are consistent with the global item-transition. Extensive experiments conducted on three popular benchmark datasets demonstrate that both SRGI-FM and SRGI-CM outperform the state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032300609X",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Constraint (computer-aided design)",
      "Focus (optics)",
      "Gene",
      "Geodesy",
      "Geography",
      "Geometry",
      "Graph",
      "Information retrieval",
      "Law",
      "Machine learning",
      "Mathematics",
      "Optics",
      "Physics",
      "Political science",
      "Politics",
      "Recommender system",
      "Representation (politics)",
      "Session (web analytics)",
      "Theoretical computer science",
      "Transition (genetics)",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Ziyang"
      },
      {
        "surname": "Wei",
        "given_name": "Wei"
      },
      {
        "surname": "Zou",
        "given_name": "Ding"
      },
      {
        "surname": "Liu",
        "given_name": "Yifan"
      },
      {
        "surname": "Li",
        "given_name": "Xiao-Li"
      },
      {
        "surname": "Mao",
        "given_name": "Xian-Ling"
      },
      {
        "surname": "Qiu",
        "given_name": "Minghui"
      }
    ]
  },
  {
    "title": "GA-GWNN: Generalized Adaptive Graph Wavelet Neural Network",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2023.11.025",
    "abstract": "Wavelet-based graph neural networks have received increasing attention in the node classification task. Existing graph wavelet-based approaches, however, are not applicable to arbitrary graphs as they use predefined wavelet filters with built-in homophilic assumptions and disregard heterophily. Recent studies attempted to address this issue through a wavelet lifting transform, which requires a bipartite graph, therefore altering the graph topology and leading to undesirable wavelet filters. This paper proposes a generalized adaptive graph wavelet network that preserves the graph topology through computational trees while implementing the lifting scheme on arbitrary graphs. Moreover, this locally defined lifting scheme integrates both high-pass and low-pass frequency components to further enhance feature representation. Finally, we benchmark our model using nine homophilic and heterophilic datasets, and the results demonstrate the effectiveness of our method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523003355",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Bipartite graph",
      "Combinatorics",
      "Computer science",
      "Graph",
      "Lifting scheme",
      "Line graph",
      "Mathematics",
      "Stationary wavelet transform",
      "Theoretical computer science",
      "Topological graph theory",
      "Topology (electrical circuits)",
      "Voltage graph",
      "Wavelet",
      "Wavelet packet decomposition",
      "Wavelet transform"
    ],
    "authors": [
      {
        "surname": "Deb",
        "given_name": "Swakshar"
      },
      {
        "surname": "Rahman",
        "given_name": "Shafin"
      },
      {
        "surname": "Rahman",
        "given_name": "Sejuti"
      }
    ]
  },
  {
    "title": "Attack-invariant attention feature for adversarial defense in hyperspectral image classification",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109955",
    "abstract": "Although deep neural networks (DNNs) have achieved excellent performance on hyperspectral image (HSI) classification tasks, their robustness is threatened by carefully created adversarial examples. Therefore, adversarial defense methods have provided an effective defense strategy to protect HSI classification networks. However, most defense models are highly dependent on known types of adversarial examples, which leads to poor generalization to defend against unknown attacks. In this study, we propose an attack-invariant attention feature-based defense (AIAF-Defense) model to improve the generalization ability of the defense model. Specifically, the AIAF-Defense model has an encoder–decoder structure to remove the perturbations from the HSI adversarial examples. We design a feature-disentanglement network as the encoder structure to decouple the attack-invariant spectral–spatial feature and attack-variant feature in the adversarial example and apply a decoder structure to reconstruct the legitimate HSI example. In addition, an attention-guided reconstruction loss is proposed to address the attention-shift problem caused by perturbation and provide an attention constraint for the extraction of attack-invariant attention features. Extensive experiments are conducted on three benchmark hyperspectral image datasets, the PaviaU, HoustonU 2018, and Salinas datasets, and the obtained results show that the proposed AIAF-Defense model improves the defense ability on both known and unknown adversarial attacks. The code is available at https://github.com/AAAA-CS/AIAF_HyperspectralAdversarialDefense.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323006532",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Deep neural networks",
      "Feature (linguistics)",
      "Feature extraction",
      "Gene",
      "Generalization",
      "Invariant (physics)",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematical physics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Shi",
        "given_name": "Cheng"
      },
      {
        "surname": "Liu",
        "given_name": "Ying"
      },
      {
        "surname": "Zhao",
        "given_name": "Minghua"
      },
      {
        "surname": "Pun",
        "given_name": "Chi-Man"
      },
      {
        "surname": "Miao",
        "given_name": "Qiguang"
      }
    ]
  },
  {
    "title": "A uniform representation model for OCT-based fingerprint presentation attack detection and reconstruction",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109981",
    "abstract": "In current Optical Coherence Tomography (OCT)-based fingerprint recognition systems, Presentation Attack Detection (PAD) and subsurface fingerprint reconstruction are treated as two independent branches, resulting in high computation and system building complexity. Therefore, this paper proposes a uniform representation model for simultaneous PAD and subsurface fingerprint reconstruction. A novel semantic segmentation network using attention mechanisms was designed to extract and segment multiple subsurface structures from real finger slices (i.e., B-scans). The latent codes derived from the network are directly used to effectively detect PA because they contain abundant subsurface biological information, which is independent of PA materials and has strong robustness for unknown PAs. Segmented subsurface structures were adopted to reconstruct multiple subsurface 2D fingerprints. Extensive experiments were carried out on an in-house database, which is the largest public OCT-based fingerprint database with 2449 volumes. PAD performance was evaluated by comparing the results of the existing methods with those of other segmentation networks. The proposed uniform representation model can obtain an accuracy (Acc) of 96.63%, which achieves a state-of-the-art performance. The effectiveness of subsurface reconstruction was evaluated from the segmentation and recognition results. In the segmentation experiments, the proposed method achieved the best results with an Intersection of Union (mIOU) of 0.834 and a Pixel Accuracy of 0.937. By comparing the recognition performance on surface 2D fingerprints (e.g., commercial and high-resolution), the lowest results with an Equal Error Rate (EER) of 2.25% by minutiae matching and an EER of 5.42% by pore matching are achieved, which indicates the excellent reconstruction capability of the proposed uniform representation model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323006799",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Fingerprint (computing)",
      "Fingerprint recognition",
      "Gene",
      "Minutiae",
      "Pattern recognition (psychology)",
      "Robustness (evolution)",
      "Segmentation",
      "Word error rate"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Wentian"
      },
      {
        "surname": "Liu",
        "given_name": "Haozhe"
      },
      {
        "surname": "Liu",
        "given_name": "Feng"
      },
      {
        "surname": "Ramachandra",
        "given_name": "Raghavendra"
      }
    ]
  },
  {
    "title": "Exploiting stance similarity and graph neural networks for fake news detection",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2023.11.019",
    "abstract": "The widespread dissemination of fake news on social media has substantial economic and social implications. Although traditional propagation-based methods employing graph neural networks show promise for fake news detection, they disregard the influence of confirmation bias in the spread of fake news between users with similar viewpoints. This paper presents a detection approach that accounts for opinion similarity between users by scrutinizing their stances toward news articles and users’ post interactions. Using a graph transformer network, our method simultaneously extracts global structural information and interactions of similar stances. Furthermore, it addresses the challenges of stance analysis targeting microblogs while minimizing the effect of poorly represented stance features. We evaluated our approach using custom-crawled Twitter data and the benchmark FibVID dataset. It demonstrated a marked improvement in detection performance compared with conventional methods, including state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552300329X",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Computer science",
      "Data mining",
      "Data science",
      "Fake news",
      "Geodesy",
      "Geography",
      "Graph",
      "Haystack",
      "Image (mathematics)",
      "Information retrieval",
      "Internet privacy",
      "Machine learning",
      "Microblogging",
      "Similarity (geometry)",
      "Social media",
      "Theoretical computer science",
      "Viewpoints",
      "Visual arts",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Soga",
        "given_name": "Kayato"
      },
      {
        "surname": "Yoshida",
        "given_name": "Soh"
      },
      {
        "surname": "Muneyasu",
        "given_name": "Mitsuji"
      }
    ]
  },
  {
    "title": "CATNet: Convolutional attention and transformer for monocular depth estimation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109982",
    "abstract": "Monocular depth estimation has received more and more attention due to its wide range of application scenarios. In this paper, we propose a novel simple framework, called CATNet, which treats monocular depth estimation as an ordinal regression problem. At present, in order to obtain higher performance, the research on monocular depth estimation is achieved by increasing the amount of calculation and parameters of the model. Based on this, we propose a novel simple encoder–decoder architecture that aims to reduce the SOTA model parameters and complexity while keeping the depth estimation accuracy as high as possible rather than aiming for extremely lightweight. Meanwhile, in order to further refine the multi-scale information extracted by the encoder, we propose a Multi-dimensional Convolutional Attention (MCA) module. To enhance the extraction of global information for accurate pixel classification, we propose a Dual Attention Transformer (DAT) module to extract global features of images. Furthermore, experimental results on the KITTI and NYU datasets demonstrate that the advantage of our proposed framework is that it achieves almost equivalent depth estimation performance to the current SOTA with fewer parameters and lower complexity. To the best of our knowledge, CATNet is the first work that achieves nearly the same depth estimation accuracy as Transformer-based large model encoders with so few parameters.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323006805",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Electrical engineering",
      "Encoder",
      "Engineering",
      "Inference",
      "Monocular",
      "Operating system",
      "Pattern recognition (psychology)",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Tang",
        "given_name": "Shuai"
      },
      {
        "surname": "Lu",
        "given_name": "Tongwei"
      },
      {
        "surname": "Liu",
        "given_name": "Xuanxuan"
      },
      {
        "surname": "Zhou",
        "given_name": "Huabing"
      },
      {
        "surname": "Zhang",
        "given_name": "Yanduo"
      }
    ]
  },
  {
    "title": "Sparse self-attention transformer for image inpainting",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109897",
    "abstract": "Learning-based image inpainting methods have made remarkable progress in recent years. Nevertheless, these methods still suffer from issues such as blurring, artifacts, and inconsistent contents. The use of vanilla convolution kernels, which have limited perceptual fields and spatially invariant kernel coefficients, is one of the main causes for these problems. In contrast, the multi-headed attention in the transformer can effectively model non-local relations among input features by generating adaptive attention scores. Therefore, this paper explores the feasibility of employing the transformer model for the image inpainting task. However, the multi-headed attention transformer blocks pose a significant challenge due to their overwhelming computational cost. To address this issue, we propose a novel U-Net style transformer-based network for the inpainting task, called the sparse self-attention transformer (Spa-former). The Spa-former retains the long-range modeling capacity of transformer blocks while reducing the computational burden. It incorporates a new channel attention approximation algorithm that reduces attention calculation to linear complexity. Additionally, it replaces the canonical softmax function with the ReLU function to generate a sparse attention map that effectively excludes irrelevant features. As a result, the Spa-former achieves effective long-range feature modeling with fewer parameters and lower computational resources. Our empirical results on challenging benchmarks demonstrate the superior performance of our proposed Spa-former over state-of-the-art approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323005952",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Electrical engineering",
      "Engineering",
      "Image (mathematics)",
      "Inpainting",
      "Pattern recognition (psychology)",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Wenli"
      },
      {
        "surname": "Deng",
        "given_name": "Ye"
      },
      {
        "surname": "Hui",
        "given_name": "Siqi"
      },
      {
        "surname": "Wu",
        "given_name": "Yang"
      },
      {
        "surname": "Zhou",
        "given_name": "Sanping"
      },
      {
        "surname": "Wang",
        "given_name": "Jinjun"
      }
    ]
  },
  {
    "title": "Structure-preserving feature alignment for old photo colorization",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109968",
    "abstract": "Deep learning techniques have made significant advancements in reference-based colorization by training on large-scale datasets. However, directly applying these methods to the task of colorizing old photos is challenging due to the lack of ground truth and the notorious domain gap between natural gray images and old photos. To address this issue, we propose a novel CNN-based algorithm called SFAC, i.e., Structure-preserving Feature Alignment Colorizer. SFAC is trained on only two images for old photo colorization, eliminating the reliance on big data and allowing direct processing of the old photo itself to overcome the domain gap problem. Our primary objective is to establish semantic correspondence between the two images, ensuring that semantically related objects have similar colors. We achieve this through a feature distribution alignment loss that remains robust to different metric choices. However, utilizing robust semantic correspondence to transfer color from the reference to the old photo can result in inevitable structure distortions. To mitigate this, we introduce a structure-preserving mechanism that incorporates a perceptual constraint at the feature level and a frozen-updated pyramid at the pixel level. Extensive experiments demonstrate the effectiveness of our method for old photo colorization, as confirmed by qualitative and quantitative metrics.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323006660",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Constraint (computer-aided design)",
      "Domain (mathematical analysis)",
      "Feature (linguistics)",
      "Geometry",
      "Image (mathematics)",
      "Image retrieval",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pyramid (geometry)",
      "Semantic gap"
    ],
    "authors": [
      {
        "surname": "Pang",
        "given_name": "Yingxue"
      },
      {
        "surname": "Jin",
        "given_name": "Xin"
      },
      {
        "surname": "Fu",
        "given_name": "Jun"
      },
      {
        "surname": "Chen",
        "given_name": "Zhibo"
      }
    ]
  },
  {
    "title": "Jacobian norm with Selective Input Gradient Regularization for interpretable adversarial defense",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109902",
    "abstract": "Deep neural networks (DNNs) can be easily deceived by imperceptible alterations known as adversarial examples. These examples can lead to misclassification, posing a significant threat to the reliability of deep learning systems in real-world applications. Adversarial training (AT) is a popular technique used to enhance robustness by training models on a combination of corrupted and clean data. However, existing AT-based methods often struggle to handle transferred adversarial examples that can fool multiple defense models, thereby falling short of meeting the generalization requirements for real-world scenarios. Furthermore, AT typically fails to provide interpretable predictions, which are crucial for domain experts seeking to understand the behavior of DNNs. To overcome these challenges, we present a novel approach called Jacobian norm and Selective Input Gradient Regularization (J-SIGR). Our method leverages Jacobian normalization to improve robustness and introduces regularization of perturbation-based saliency maps, enabling interpretable predictions. By adopting J-SIGR, we achieve enhanced defense capabilities and promote high interpretability of DNNs. We evaluate the effectiveness of J-SIGR across various architectures by subjecting it to powerful adversarial attacks. Our experimental evaluations provide compelling evidence of the efficacy of J-SIGR against transferred adversarial attacks, while preserving interpretability. The project code can be found at https://github.com/Lywu-github/jJ-SIGR.git.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323006003",
    "keywords": [
      "Adversarial system",
      "Applied mathematics",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Deep learning",
      "Deep neural networks",
      "Gene",
      "Interpretability",
      "Jacobian matrix and determinant",
      "Law",
      "Machine learning",
      "Mathematics",
      "Norm (philosophy)",
      "Political science",
      "Regularization (linguistics)",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Deyin"
      },
      {
        "surname": "Wu",
        "given_name": "Lin Yuanbo"
      },
      {
        "surname": "Li",
        "given_name": "Bo"
      },
      {
        "surname": "Boussaid",
        "given_name": "Farid"
      },
      {
        "surname": "Bennamoun",
        "given_name": "Mohammed"
      },
      {
        "surname": "Xie",
        "given_name": "Xianghua"
      },
      {
        "surname": "Liang",
        "given_name": "Chengwu"
      }
    ]
  },
  {
    "title": "A coincidence detection perspective for the maximum mean discrepancy",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2023.11.013",
    "abstract": "An alternative perspective is proposed for the Maximum Mean Discrepancy (MMD), in which coincidence detectors replace Gaussian kernels. It is conjectured that coincidence-based statistics may be a relevant factor behind MMD, for it may explain why MMD works even for small high-dimensional sets of observations. It is further shown how this proposed perspective can be used to simplify interpretations in MMD-based tests, including a straightforward computation of thresholds for hypothesis tests, which is done through the Grassberger–Procaccia method, originally proposed for intrinsic dimensionality estimation. Experimental results corroborate the conjecture that an MMD based on coincidence detection would perform almost equivalently to the MMD based on (frequently used) Gaussian kernels, with advantages in terms of interpretability and computational load.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523003239",
    "keywords": [
      "Algorithm",
      "Alternative medicine",
      "Applied mathematics",
      "Artificial intelligence",
      "Coincidence",
      "Combinatorics",
      "Computation",
      "Computer science",
      "Conjecture",
      "Curse of dimensionality",
      "Detector",
      "Gaussian",
      "Interpretability",
      "Mathematics",
      "Medicine",
      "Pathology",
      "Pattern recognition (psychology)",
      "Perspective (graphical)",
      "Physics",
      "Quantum mechanics",
      "Statistics",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Montalvão",
        "given_name": "Jugurta"
      },
      {
        "surname": "Duarte",
        "given_name": "Dami"
      },
      {
        "surname": "Boccato",
        "given_name": "Levy"
      }
    ]
  },
  {
    "title": "Maximum Gaussianality training for deep speaker vector normalization",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109977",
    "abstract": "Automatic Speaker Verification (ASV) is a critical task in pattern recognition and has been applied to various security-sensitive scenarios. The current state-of-the-art technique for ASV is based on deep embedding. However, a significant challenge with this approach is that the resulting deep speaker vectors tend to be irregularly distributed. To address this issue, this paper proposes a novel training method called Maximum Gaussianality (MG), which regulates the distribution of the speaker vectors. Compared to the conventional normalization approach based on maximum likelihood (ML), the new approach directly maximizes the Gaussianality of the latent codes, and therefore can both normalize the between-class and within-class distributions in a controlled and reliable way and eliminate the unbound likelihood problem associated with the conventional ML approach. Our experiments on several datasets demonstrate that our MG-based normalization can deliver much better performance than the baseline systems without normalization and outperform discriminative normalization flow (DNF), an ML-based normalization method, particularly when the training data is limited. In theory, the MG criterion can be applied to any task in any research domain where Gaussian distributions are needed, making the MG training a versatile tool.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323006751",
    "keywords": [
      "Anthropology",
      "Artificial intelligence",
      "Computer science",
      "Deep learning",
      "Deep neural networks",
      "Discriminative model",
      "Embedding",
      "Gaussian",
      "Machine learning",
      "Mixture model",
      "Normalization (sociology)",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Sociology",
      "Speaker recognition",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Cai",
        "given_name": "Yunqi"
      },
      {
        "surname": "Li",
        "given_name": "Lantian"
      },
      {
        "surname": "Abel",
        "given_name": "Andrew"
      },
      {
        "surname": "Zhu",
        "given_name": "Xiaoyan"
      },
      {
        "surname": "Wang",
        "given_name": "Dong"
      }
    ]
  },
  {
    "title": "An adaptive error-correcting output codes algorithm based on gene expression programming and similarity measurement matrix",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109957",
    "abstract": "The multi-class classification task is one of the most common tasks in machine learning. As a typical solution based on a partitioning strategy, Error-Correcting Output Codes (ECOC) can transform a multi-class classification problem into multiple binary classification problems. The key of ECOC is to construct an effective codematrix to represent a set of class decomposition schemes, which transforms a multiclass problem into a group of binary class problems. Consequently, the design of a fast and effective ECOC codematrix generation method is of great research significance and value for solving multi-class classification problems. In ECOC algorithms, the design of codematrix is treated as a combination problem between different code columns, in which the evolutionary algorithm shows a great advantage. Based on this consideration, the Gene Expression Programming (GEP) is applied to search for the codematrix with high performance because its expressive tree structure makes it well represent codematrcies for subsequent optimization operations. This paper proposes an adaptive ECOC algorithm based on Gene Expression Programming (GEP) and similarity measurement matrix, named GEP-ECOC. In our GEP, each individual represents a set of columns to form a random ECOC codematrix, which is optimized in the evolutionary process. Meanwhile, the crossover and mutation operations are modified to include a legality checking process to ensure that the generated codematrix satisfies the ECOC constraints. The GEP-based ECOC codematrix generation algorithm can quickly produce a codematrix with better performance, which ensures the efficiency of the algorithm to a certain extent. In addition, an adaptive algorithm based on a similarity measurement matrix is proposed to add new columns to the current codematrix, aiming to better handle hard classes. Our algorithm is compared with other algorithms on various data sets, and the experimental results confirm that our GEP-ECOC can balance the efficiency and performance of the algorithm and achieve higher performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323006556",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Crossover",
      "Gene expression programming",
      "Genetic programming",
      "Multiclass classification",
      "Programming language",
      "Set (abstract data type)",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Xie",
        "given_name": "Shutong"
      },
      {
        "surname": "He",
        "given_name": "Zongbao"
      },
      {
        "surname": "Pan",
        "given_name": "Lifang"
      },
      {
        "surname": "Liu",
        "given_name": "Kunhong"
      },
      {
        "surname": "Su",
        "given_name": "Shubin"
      }
    ]
  },
  {
    "title": "Graph fairing convolutional networks for anomaly detection",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109960",
    "abstract": "Graph convolution is a fundamental building block for many deep neural networks on graph-structured data. In this paper, we introduce a simple, yet very effective graph convolutional network with skip connections for semi-supervised anomaly detection. The proposed layerwise propagation rule of our model is theoretically motivated by the concept of implicit fairing in geometry processing, and comprises a graph convolution module for aggregating information from immediate node neighbors and a skip connection module for combining layer-wise neighborhood representations. This propagation rule is derived from the iterative solution of the implicit fairing equation via the Jacobi method. In addition to capturing information from distant graph nodes through skip connections between the network’s layers, our approach exploits both the graph structure and node features for learning discriminative node representations. These skip connections are integrated by design in our proposed network architecture. The effectiveness of our model is demonstrated through extensive experiments on five benchmark datasets, achieving better or comparable anomaly detection results against strong baseline methods. We also demonstrate through an ablation study that skip connection helps improve the model performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323006581",
    "keywords": [
      "Algorithm",
      "Anomaly detection",
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Computer science",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Discriminative model",
      "Engineering",
      "Geodesy",
      "Geography",
      "Graph",
      "Node (physics)",
      "Pattern recognition (psychology)",
      "Structural engineering",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Mesgaran",
        "given_name": "Mahsa"
      },
      {
        "surname": "Hamza",
        "given_name": "A. Ben"
      }
    ]
  },
  {
    "title": "Chapter 1 A deep insight into intelligent fractal-based image analysis with pattern recognition",
    "journal": "Intelligent Fractal-Based Image Analysis",
    "year": "2024",
    "doi": "10.1016/B978-0-44-318468-0.00007-6",
    "abstract": "Digital images play a vital role in the daily activities of human life as they resolve various real-life problems in real time. The advancement in digital image processing systems has resulted in the generation of a vast number of digital images in various application domains. The availability of a vast amount of high dimensional and unstructured data has led to the development of efficient pattern recognition systems for identifying patterns efficiently. In recent years, fractal-based pattern recognition has been gaining significant importance among researchers because of its capability to handle feature information in a complex environment. The efficiency of Fractal image Analysis is not confined to a particular area. As fractal analysis has shown its efficacy towards irregular objects, it has been widely utilized in various application domains, such as pattern recognition, image segmentation, texture analysis, medical signal analysis, and so on. In this study, a detailed review of intelligent fractal-based image analysis with pattern recognition is presented. Initially, the study focuses on different approaches used in the development of pattern recognition systems along with their applications. This study also focuses on different image processing approaches and fractal features used in the analysis of digital images. Further, this study provides a critical investigation of various pattern recognition and image processing approaches used in the fractal analysis of images that may assist researchers in developing more sophisticated systems in distinct application domains.",
    "link": "https://www.sciencedirect.com/science/article/pii/B9780443184680000076",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Fractal",
      "Fractal analysis",
      "Fractal dimension",
      "Image (mathematics)",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Swapnarekha",
        "given_name": "H."
      },
      {
        "surname": "Nayak",
        "given_name": "Janmenjoy"
      },
      {
        "surname": "Naik",
        "given_name": "Bighnaraj"
      },
      {
        "surname": "Pelusi",
        "given_name": "Danilo"
      }
    ]
  },
  {
    "title": "Chapter 6 Deep CNNS and fractal-based sequence learning for violence detection in surveillance videos",
    "journal": "Intelligent Fractal-Based Image Analysis",
    "year": "2024",
    "doi": "10.1016/B978-0-44-318468-0.00013-1",
    "abstract": "Deep learning has received a great deal of attention from researchers for its reasonable outcomes in different fields of computer vision due to their wide range of applications. The recognition of different activities in videos is important for violence detection (VD) to ensure safety and security. VD also practice the latest deep learning-based algorithms. This chapter provides a detailed overview of CNN-based state-of-the-art VD methods and visual representation of their working strategy. Next, we also discuss the concept of sequence learning and its different variants for violent scene detection by investigating their internal mechanism. Similarly, we provide comprehensive detail of comparing the VD methods considering the accuracy obtained on different challenging datasets. Furthermore, the concrete details pertaining to the performance of the methods using standard metrics is provided. Finally, we discuss the main necessities of VD methods and future research directions.",
    "link": "https://www.sciencedirect.com/science/article/pii/B9780443184680000131",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Deep learning",
      "Fractal",
      "Genetics",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Sequence (biology)"
    ],
    "authors": [
      {
        "surname": "Ullah",
        "given_name": "Fath U Min"
      },
      {
        "surname": "Muhammad",
        "given_name": "Khan"
      },
      {
        "surname": "Baik",
        "given_name": "Sung Wook"
      }
    ]
  },
  {
    "title": "Chapter 12 Preliminary analysis and survey of retinal disease diagnosis through identification and segmentation of bright and dark lesions",
    "journal": "Intelligent Fractal-Based Image Analysis",
    "year": "2024",
    "doi": "10.1016/B978-0-44-318468-0.00020-9",
    "abstract": "Retinal diseases are the principal causes of blindness and vision loss worldwide. The initial detection can diminish the incidence of total vision impairment and blindness due to retinal diseases. Thus the present study was conducted to study retinal lesion classification on retinal fundus images for the initial detection of retinal diseases. The indicative confirmation of various retinal diseases relies on the recognition and segmentation of dark and bright retinal lesions structures. Bright lesions are mainly categorized into exudates and cotton wool spots and dark lesions into micro-aneurysms and hemorrhages. Nevertheless, variations present in retinal fundus images make it challenging to distinguish varying types of lesions in the incidence of landmark structures (healthy structures) such as retinal blood vasculature and the optic disk. Therefore it is vital to eliminate false responses due to healthy structures before the segmentation of dark and bright retinal lesions. Additionally, in order to design a robust automated retinal disease diagnostic method a retinal image dataset comprising of fundus images having varying attributes is essential. Putting these facts into consideration, the detailed preliminary study of various retinal lesions and their classification is carried out in this chapter in order to design an effective computer-aided diagnostic method for the detection of various retinal diseases.",
    "link": "https://www.sciencedirect.com/science/article/pii/B9780443184680000209",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Computer science",
      "Identification (biology)",
      "Medicine",
      "Ophthalmology",
      "Optometry",
      "Retinal",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Kaur",
        "given_name": "Jaskirat"
      },
      {
        "surname": "Mittal",
        "given_name": "Deepti"
      },
      {
        "surname": "Kaur",
        "given_name": "Ramanpreet"
      },
      {
        "surname": "Gagandeep",
        "given_name": ""
      }
    ]
  },
  {
    "title": "Foreword",
    "journal": "Intelligent Fractal-Based Image Analysis",
    "year": "2024",
    "doi": "10.1016/B978-0-44-318468-0.00022-2",
    "abstract": "Unknown",
    "link": "https://www.sciencedirect.com/science/article/pii/B9780443184680000222",
    "keywords": [
      "Psychology"
    ],
    "authors": [
      {
        "surname": "Del Ser",
        "given_name": "Javier (Javi)"
      }
    ]
  },
  {
    "title": "Chapter 8 Comparative analysis of approaches to optimize fractal image compression",
    "journal": "Intelligent Fractal-Based Image Analysis",
    "year": "2024",
    "doi": "10.1016/B978-0-44-318468-0.00015-5",
    "abstract": "In this chapter, a comparative study of strategies for faster fractal image compression (FIC) is presented. Several fields have widely accepted FIC as a favorable compression technique. The fractal compressor has an asymmetric encoder and decoder concerning the time spent in each. The encoding process includes searching for a large image pool. The quantity of time elapsed in searching is a matter of significant concern. Further, enormous mean square error (MSE) computation leads to delay on the encoder. Over the years, many methodologies have been suggested to improve the performance of the FIC algorithm. This chapter presents a comprehensive investigation of 14 broadly coordinated approaches with their strengths and weaknesses. Every approach was deliberated chronologically for a better understanding of the development of FIC. The frequency domains have been reported and used in combination with FIC to improve the performance of the algorithm. It is observed that the high value of threshold results in poor quality of the image, while a low threshold leads to long encoding times in searchless FIC. A brief discussion of the theoretical background of FIC is encompassed in the chapter. The literature is summarized and explored for a quick review. The underlying objective of this chapter is to recognize the methods that allow the expansion of new strategies to speed up the encoding procedure.",
    "link": "https://www.sciencedirect.com/science/article/pii/B9780443184680000155",
    "keywords": [
      "Artificial intelligence",
      "Composite material",
      "Compression (physics)",
      "Computer science",
      "Computer vision",
      "Fractal",
      "Fractal analysis",
      "Fractal compression",
      "Fractal dimension",
      "Fractal transform",
      "Image (mathematics)",
      "Image compression",
      "Image processing",
      "Materials science",
      "Mathematical analysis",
      "Mathematics"
    ],
    "authors": [
      {
        "surname": "Garg",
        "given_name": "Rakesh"
      },
      {
        "surname": "Gupta",
        "given_name": "Richa"
      }
    ]
  },
  {
    "title": "Chapter 4 Fractal feature based image classification",
    "journal": "Intelligent Fractal-Based Image Analysis",
    "year": "2024",
    "doi": "10.1016/B978-0-44-318468-0.00010-6",
    "abstract": "The Fractal Dimension (FD) of an image is a real number with self-similarity property that represents the roughness of an image and has been used widely in digital images as a feature extractor for Classification, Segmentation, etc., by using different fractal texture based techniques. Nonetheless, different techniques have presented inconsistent results that makes it important to analyze the effects of the FD used in digital imaging domain. This chapter aims to give a comprehensive account of the feasibility of FD as a feature extractor for Brain MRI image classification by considering various features such as Smoothness, Hausdorff Dimension, GLCM features like Contrast, Correlation, Homogeneity, Energy, and Entropy. The calculated feature vectors have been aggregated and various exploratory data analysis techniques have been presented to investigate the effects of the FD on the accuracy of classification. Data preprocessing techniques such as offline data augmentation and data normalization have been performed to achieve better results. Finally, the classification has been achieved by considering popular machine learning (ML) based classifiers such as SVM, KNN, Random Forest, and Artificial Neural Network on the binary dataset Normal and Tumor. The experimental study shows that the KNN achieved the highest accuracy with 94.67%, followed by ANN 92%, Random Forest 91.67%, and SVM with 87.67%. The results have been validated by using various evaluation metrics and the overall analysis reflects on the importance of Fractal Dimension based feature extraction of images.",
    "link": "https://www.sciencedirect.com/science/article/pii/B9780443184680000106",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Contextual image classification",
      "Feature (linguistics)",
      "Fractal",
      "Image (mathematics)",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Nayak",
        "given_name": "Soumya Ranjan"
      },
      {
        "surname": "Sinha",
        "given_name": "Utkarsh"
      }
    ]
  },
  {
    "title": "Chapter 5 The study of source image and its futuristic quantum applications: an insight from fractal analysis",
    "journal": "Intelligent Fractal-Based Image Analysis",
    "year": "2024",
    "doi": "10.1016/B978-0-44-318468-0.00012-X",
    "abstract": "Fractal analysis is an important tool that is used to decipher, analyze, and produce numerous image processes. It is also applied for quantum interference that best describes the prescription of fractal femtoscopy. A universal technique is conferred to examine the consequences of the image analysis on the signal exuded sources such as image patterns originated during the image-processing technique. This work also highlights the main features of image processing and the obtained results are highly smooth and qualitative. Fractal analysis gives a comprehensive keenness, as well as the normalized image parameter, which is evaluated to ascertain the analytical strength of quantum interferences. Specifically, we evaluate the produced image characteristics by the normalized fractal-based technique to deploy in scientific studies with its application. We also calculate and explore the performance of the hybrid system by differential equations to analyze the peculiarities of the image excreted sources.",
    "link": "https://www.sciencedirect.com/science/article/pii/B978044318468000012X",
    "keywords": [
      "Computer science",
      "Computer vision",
      "Fractal",
      "Fractal analysis",
      "Fractal dimension",
      "Image (mathematics)",
      "Mathematical analysis",
      "Mathematics",
      "Physics",
      "Quantum",
      "Quantum mechanics",
      "Statistical physics"
    ],
    "authors": [
      {
        "surname": "Bary",
        "given_name": "Ghulam"
      },
      {
        "surname": "Ahmad",
        "given_name": "Riaz"
      }
    ]
  },
  {
    "title": "Chapter 9 Alzheimer disease (AD) medical image analysis with convolutional neural networks AD-CNN",
    "journal": "Intelligent Fractal-Based Image Analysis",
    "year": "2024",
    "doi": "10.1016/B978-0-44-318468-0.00017-9",
    "abstract": "Amyloid is a protein resulting from a protein misfolding process that occurs parallel to physiological folding and generates toxic and insoluble proteins that are deposited in the tissues; in the case of Alzheimer's disease, amyloid forms tangles of fibrillar proteins that derive from the aggregation of a peptide of about 39 to 42 amino acids called “Aβ”. White matter, gray matter, and cerebral spinal fluid are three main elements that are investigated in the case of Alzheimer's disease and depending on these materials, doctors and radiologists will evaluate the brain abnormality and diagnose the pathological condition to provide the best treatment. Optimal processing of diagnostic investigations is essential to identify the pathological condition and establish the most appropriate clinic for the patient. This chapter analyzes the computational application of Convolutional Neural Networks (CNNs) in relation to the most commonly used diagnostic imaging and examines the applications and developments associated with the convolutional architecture that makes the system more plastic and extensible, since it can be applied to any type of 3D image collection.",
    "link": "https://www.sciencedirect.com/science/article/pii/B9780443184680000179",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Disease",
      "Internal medicine",
      "Medicine",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Sohail",
        "given_name": "Ayesha"
      },
      {
        "surname": "Fiaz",
        "given_name": "Muddassar"
      },
      {
        "surname": "Nutini",
        "given_name": "Alessandro"
      },
      {
        "surname": "Iqbal",
        "given_name": "M. Sohail"
      }
    ]
  },
  {
    "title": "Chapter 3 Chaos-based image encryption",
    "journal": "Intelligent Fractal-Based Image Analysis",
    "year": "2024",
    "doi": "10.1016/B978-0-44-318468-0.00009-X",
    "abstract": "Chaos-based encryption applications are at the forefront of engineering applications based on chaotic systems. When the studies in the literature are examined, it is seen that chaos-based image encryption applications are widely used. The randomness and unpredictable dynamic behavior of chaotic systems make chaos-based encryption applications very useful. For this reason, chaos-based image encryption is handled in a separate chapter here. In this chapter, first, continuous and discrete-time chaotic systems are mentioned and the chaos-based image encryption studies carried out in recent years are examined. The reason for this is to reflect the main effects of continuous-time or discrete-time systems on chaos-based image encryption. In addition, integer-order, fractional-order, and variable-order systems are also discussed and their use in image encryption applications is seen. Then, the methods used in encryption applications are introduced and the analysis methods used in chaos-based image encryption applications are explained. In the explanations about chaos-based image encryption applications, the commonly used steps for these applications are discussed. As these steps, numerical solutions of chaotic systems, random number generation operations, randomness tests, and encryption operations using the xor operation are explained. Security analysis, which is another process that needs to be performed regarding chaos-based image encryption, is also included in this chapter. In the literature, entropy, correlation, differential attack, histogram, keyspace analysis, which are the most widely used analysis methods for chaos-based image encryption, are explained. Although widely used in the literature, chaos-based encryption methods also have advantages and disadvantages. Therefore in this chapter, the security evaluation of chaos-based encryption algorithms is also presented. In particular, the convenience and effectiveness of these methods in practice are stated. On the other hand, the aspects that are not accepted according to the classical cryptology approach are also emphasized. Finally, a case study on medical images is handled with a chaos-based image encryption algorithm and the results of the case study are presented. In this application, numerical solutions and random numbers are obtained by using a classic 4D chaotic system. Then, the encryption process is carried out by xoring with the random numbers. The resulting encrypted image is analyzed and the results are presented.",
    "link": "https://www.sciencedirect.com/science/article/pii/B978044318468000009X",
    "keywords": [
      "CHAOS (operating system)",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Encryption",
      "Image (mathematics)"
    ],
    "authors": [
      {
        "surname": "Kaçar",
        "given_name": "Sezgin"
      },
      {
        "surname": "Çavuşoğlu",
        "given_name": "Ünal"
      },
      {
        "surname": "Jahanshahi",
        "given_name": "Hadi"
      }
    ]
  },
  {
    "title": "Chapter 11 Fractal dimension analysis using hybrid RDBC and IDBC for gray scale images",
    "journal": "Intelligent Fractal-Based Image Analysis",
    "year": "2024",
    "doi": "10.1016/B978-0-44-318468-0.00019-2",
    "abstract": "Fractal dimension (FD) plays a significant role in determining surface quality in digital images, considering essential aspects of fractal geometry for pattern recognition. Among the numerous techniques, the differential box-counting (DBC) strategy is generally used to compute the spatial pattern of digital images. In this chapter, a modified DBC (RIDBC) is proposed to overcome three issues in existing DBC, i.e., computational error, least friction surface, and homogeneous fractal dimension, determined either by increasing or decreasing the constant value at each intensity point. The modified DBC is composed of hybrid RDBC and IDBC. Therefore an improved method is proposed to solve the issues. The modified DBC is compared with other existing approaches, such as DBC, RDBC (relative), and IDBC (improved). For accurate computation, the adjustment coefficient factor is formulated for fractal dimension analysis. The proposed method is experimental on different varieties of images such as Brodatz images, smooth texture using a linear transformation, smooth synthetic textured images, and rough synthetic textured images. The results show that the proposed method provides more promising outcomes by covering a wider range of fractal dimensions.",
    "link": "https://www.sciencedirect.com/science/article/pii/B9780443184680000192",
    "keywords": [
      "Artificial intelligence",
      "Cartography",
      "Computer science",
      "Dimension (graph theory)",
      "Fractal",
      "Fractal analysis",
      "Fractal dimension",
      "Fractal dimension on networks",
      "Geography",
      "Gray (unit)",
      "Grayscale",
      "Image (mathematics)",
      "Mathematical analysis",
      "Mathematics",
      "Medicine",
      "Physics",
      "Pure mathematics",
      "Radiology",
      "Scale (ratio)",
      "Statistical physics"
    ],
    "authors": [
      {
        "surname": "Vijh",
        "given_name": "Surbhi"
      },
      {
        "surname": "Kumar",
        "given_name": "Sumit"
      },
      {
        "surname": "Saraswat",
        "given_name": "Mukesh"
      }
    ]
  },
  {
    "title": "Chapter 2 Analysis of Mandelbrot set fractal images using a machine learning based approach",
    "journal": "Intelligent Fractal-Based Image Analysis",
    "year": "2024",
    "doi": "10.1016/B978-0-44-318468-0.00008-8",
    "abstract": "Fractals is an important research domain in the field of image processing. Fractal image processing plays a crucial role in the current scenario. Machine learning (ML) based approaches can be used for the processing and analysis of several fractal images. In this chapter, an ML based approach is used to analyze several Mandelbrot set fractal images (MSFIs). The machine learning based approach mainly focuses on the clustering techniques such as hierarchical clustering, distance map, multidimensional scaling (MDS), and distance matrix for the processing of MSFIs. These clustering techniques are used to generate the MSFIs with several clusters. The performance of each of these clustering techniques is analyzed after processing several MSFIs. The simulation of this work is carried out using Orange3-3.24.1.",
    "link": "https://www.sciencedirect.com/science/article/pii/B9780443184680000088",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Fractal",
      "Fractal analysis",
      "Fractal dimension",
      "Mandelbrot set",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Programming language",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Jena",
        "given_name": "Kalyan Kumar"
      },
      {
        "surname": "Bhoi",
        "given_name": "Sourav Kumar"
      },
      {
        "surname": "Nayak",
        "given_name": "Soumya Ranjan"
      }
    ]
  },
  {
    "title": "Preface",
    "journal": "Intelligent Fractal-Based Image Analysis",
    "year": "2024",
    "doi": "10.1016/B978-0-44-318468-0.00023-4",
    "abstract": "Unknown",
    "link": "https://www.sciencedirect.com/science/article/pii/B9780443184680000234",
    "keywords": [
      "Sociology"
    ],
    "authors": []
  },
  {
    "title": "Erratum to ‘L1-norm discriminant analysis via Bhattacharyya error bounds under Laplace distributions’ [Pattern Recognition 141 (2023) 109609]",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110070",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323007677",
    "keywords": [
      "Artificial intelligence",
      "Bhattacharyya distance",
      "Computer science",
      "Discriminant",
      "Epistemology",
      "Laplace transform",
      "Linear discriminant analysis",
      "Mathematical analysis",
      "Mathematics",
      "Norm (philosophy)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Liang",
        "given_name": "Zhizheng"
      },
      {
        "surname": "Zhang",
        "given_name": "Lei"
      }
    ]
  },
  {
    "title": "A critical review of multi-output support vector regression",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2023.12.007",
    "abstract": "Single-output regression is a widely used statistical modeling method to predict an output based on one or more features of a datapoint. If a dataset has multiple outputs, they can be predicted independently from each other although this disregards potential correlations and thus may negatively affect the predictive performance. Therefore, multi-output regression methods predict multiple outputs simultaneously. One way to approach single-output regression is by using methods based on support vectors such as support vector regression (SVR) or least-squares SVR (LS-SVR). Based on these two, previous works have devised multi-output support vector regression methods. In this review, we introduce a unified notation to summarize the single-output support vector regression methods SVR and LS-SVR as well as state-of-the-art multi-output support vector regression methods. Furthermore, we implemented a workflow for subject- and record-wise bootstrapping and nested cross-validation experiments, which we used for an exhaustive evaluation of all single- and multi-output support vector regression methods on synthetic and non-synthetic datasets. Although the reviewed papers claim that their multi-output methods improve regression performance, we find that none of them outperform both single-output methods SVR and LS-SVR for various reasons. Due to these results, we reflected about the general concept of support vector regression and then concluded that support vector regression methods do not appear to be suitable for the task of multi-output regression.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523003549",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Least squares support vector machine",
      "Machine learning",
      "Mathematics",
      "Regression",
      "Regression analysis",
      "Statistics",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Tran",
        "given_name": "Nguyen Khoa"
      },
      {
        "surname": "Kühle",
        "given_name": "Laura C."
      },
      {
        "surname": "Klau",
        "given_name": "Gunnar W."
      }
    ]
  },
  {
    "title": "GLOCAL: A self-supervised learning framework for global and local motion estimation",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2023.12.024",
    "abstract": "Motions in videos are typically a mixture of local dynamic object motions and global camera motion, which are inconsistent in some cases, and even interfere with each other, causing difficulties in various downstream applications, such as video stabilization that requires the global motion, and action recognition that consumes local motions. Therefore, it is crucial to estimate them separately. Existing methods separate two motions from the mixed motion fields, such as optical flow. However, the quality of mixed motion determines the higher bounds of the performance. In this work, we propose a framework, GLOCAL, to directly estimate global and local motions simultaneously from adjacent frames in a self-supervised manner. Our GLOCAL consists of a Global Motion Estimation (GME) module and a Local Motion Estimation (LME) module. The GME module involves a mixed motion estimation backbone, an implicit bottleneck structure for feature dimension reduction, and an explicit bottleneck for global motion recovery based on the global motion bases with foreground mask under the training guidance of proposed global reconstruction loss. An attention U-Net is adopted for LME which produces local motions while excluding motion of irrelevant regions under the guidance of proposed local reconstruction loss. Our method can achieve better performance than the existing methods on the homography estimation dataset DHE and the action recognition dataset NCAA and UCF-101.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552300377X",
    "keywords": [
      "Artificial intelligence",
      "Bottleneck",
      "Computer science",
      "Computer vision",
      "Economics",
      "Embedded system",
      "Feature (linguistics)",
      "Globalization",
      "Glocalization",
      "Image (mathematics)",
      "Linguistics",
      "Market economy",
      "Motion (physics)",
      "Motion estimation",
      "Motion field",
      "Optical flow",
      "Philosophy",
      "Quarter-pixel motion",
      "Structure from motion"
    ],
    "authors": [
      {
        "surname": "Zheng",
        "given_name": "Yihao"
      },
      {
        "surname": "Luo",
        "given_name": "Kunming"
      },
      {
        "surname": "Liu",
        "given_name": "Shuaicheng"
      },
      {
        "surname": "Li",
        "given_name": "Zun"
      },
      {
        "surname": "Xiang",
        "given_name": "Ye"
      },
      {
        "surname": "Wu",
        "given_name": "Lifang"
      },
      {
        "surname": "Zeng",
        "given_name": "Bing"
      },
      {
        "surname": "Chen",
        "given_name": "Chang Wen"
      }
    ]
  },
  {
    "title": "SSL-Net: Sparse semantic learning for identifying reliable correspondences",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110039",
    "abstract": "Feature matching aims to identify reliable correspondences between two sets of given initial feature points, which is of considerable importance to photogrammetry and computer vision. In this study, we propose an innovative sparse semantic learning-based network, named SSL-Net, for feature matching. Specifically, SSL-Net includes a novel sparsity constraint (SC) block, which builds a sparse graph for sparse semantic learning. The SC block adopts a region-to-whole learning strategy to measure the confidence of nodes in the sparse graph. It helps the sparse graph preserve the semantic information of positive influence while rejecting unnecessary ones, thereby suppressing the negative influence of incorrect correspondences. In addition, SSL-Net also includes a channel-spatial attention feature gathering block, which gathers features along the spatial direction and channel dimension of correspondences. To mitigate the existence of label ambiguity, we incorporate the accommodation factor into the loss function of SSL-Net for feature matching. As a result, our network outperforms the state-of-the-art method by a considerable margin. Notably, SSL-Net achieves a 9.05% improvement under an error threshold of 5 ° over the state-of-the-art method for the relative pose estimation task on the YFCC100M dataset. Our code will be available at https://github.com/guobaoxiao/SSL-Net.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323007367",
    "keywords": [
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Computer science",
      "Feature (linguistics)",
      "Geometry",
      "Graph",
      "Linguistics",
      "Machine learning",
      "Margin (machine learning)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Shunxing"
      },
      {
        "surname": "Xiao",
        "given_name": "Guobao"
      },
      {
        "surname": "Shi",
        "given_name": "Ziwei"
      },
      {
        "surname": "Guo",
        "given_name": "Junwen"
      },
      {
        "surname": "Ma",
        "given_name": "Jiayi"
      }
    ]
  },
  {
    "title": "Inter-domain mixup for semi-supervised domain adaptation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110023",
    "abstract": "Semi-supervised domain adaptation (SSDA) aims to bridge source and target domain distributions, with a small number of target labels available, achieving better classification performance than unsupervised domain adaptation (UDA). However, existing SSDA work fails to make full use of label information from both source and target domains for feature alignment across domains, resulting in label mismatch in the label space during model testing. This paper presents a novel SSDA approach, Inter-domain Mixup with Neighborhood Expansion (IDMNE), to tackle this issue. Firstly, we introduce a cross-domain feature alignment strategy, Inter-domain Mixup, that incorporates label information into model adaptation. Specifically, we employ sample-level and manifold-level data mixing to generate compatible training samples. These newly established samples, combined with reliable and actual label information, display diversity and compatibility across domains, while such extra supervision thus facilitates cross-domain feature alignment and mitigates label mismatch. Additionally, we utilize Neighborhood Expansion to leverage high-confidence pseudo-labeled samples in the target domain, diversifying the label information of the target domain and thereby further increasing the performance of the adaptation model. Accordingly, the proposed approach outperforms existing state-of-the-art methods, achieving significant accuracy improvements on popular SSDA benchmarks, including DomainNet, Office-Home, and Office-31.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323007203",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Domain (mathematical analysis)",
      "Domain adaptation",
      "Feature (linguistics)",
      "Leverage (statistics)",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Jichang"
      },
      {
        "surname": "Li",
        "given_name": "Guanbin"
      },
      {
        "surname": "Yu",
        "given_name": "Yizhou"
      }
    ]
  },
  {
    "title": "Hierarchical image-to-image translation with nested distributions modeling",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110058",
    "abstract": "Unpaired image-to-image translation among category domains has achieved remarkable success in past decades. Recent studies mainly focus on two challenges. For one thing, such translation is inherently multi-modal (i.e. many-to-many mapping) due to variations of domain-specific information (e.g., the domain of house cat contains multiple sub-modes), which is usually addressed by predefined distribution sampling. For another, most existing multi-modal approaches have limits in handling more than two domains with one model, i.e. they have to independently build two distributions to capture variations for every pair of domains. To address these problems, we propose a Hierarchical Image-to-image Translation (HIT) method which jointly formulates the multi-domain and multi-modal problem in a semantic hierarchy structure by modeling a common and nested distribution space. Specifically, domains have inclusion relationships under a particular hierarchy structure. With the assumption of Gaussian prior for domains, distributions of domains at lower levels capture the local variations of their ancestors at higher levels, leading to the so-called nested distributions. To this end, we propose a nested distribution loss in light of the distribution divergence measurement and information entropy theory to characterize the aforementioned inclusion relations among domain distributions. Experiments on ImageNet, ShapeNet, and CelebA datasets validate the promising results of our HIT against state-of-the-arts, and as additional benefits of nested modeling, one can even control the uncertainty of multi-modal translations at different hierarchy levels.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323007550",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Data mining",
      "Divergence (linguistics)",
      "Domain (mathematical analysis)",
      "Economics",
      "Gene",
      "Hierarchy",
      "Image (mathematics)",
      "Linguistics",
      "Market economy",
      "Mathematical analysis",
      "Mathematics",
      "Messenger RNA",
      "Modal",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Polymer chemistry",
      "Theoretical computer science",
      "Translation (biology)"
    ],
    "authors": [
      {
        "surname": "Qiao",
        "given_name": "Shishi"
      },
      {
        "surname": "Wang",
        "given_name": "Ruiping"
      },
      {
        "surname": "Shan",
        "given_name": "Shiguang"
      },
      {
        "surname": "Chen",
        "given_name": "Xilin"
      }
    ]
  },
  {
    "title": "Efficient Long-Short Temporal Attention network for unsupervised Video Object Segmentation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110078",
    "abstract": "Unsupervised Video Object Segmentation (VOS) aims at identifying the contours of primary foreground objects in videos without any prior knowledge. However, previous methods do not fully use spatial–temporal context and fail to tackle this challenging task in real-time. This motivates us to develop an efficient Long-Short Temporal Attention network (termed LSTA) for unsupervised VOS task from a holistic view. Specifically, LSTA consists of two dominant modules, i.e., Long Temporal Memory and Short Temporal Attention. The former captures the long-term global pixel relations of the past frames and the current frame, which models constantly present objects by encoding appearance pattern. Meanwhile, the latter reveals the short-term local pixel relations of one nearby frame and the current frame, which models moving objects by encoding motion pattern. To speedup the inference, the efficient projection and the locality-based sliding window are adopted to achieve nearly linear time complexity for the two light modules, respectively. Extensive empirical studies on several benchmarks have demonstrated promising performances of the proposed method with high efficiency.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323007756",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Economics",
      "Encoding (memory)",
      "Frame (networking)",
      "Inference",
      "Linguistics",
      "Locality",
      "Management",
      "Object (grammar)",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pixel",
      "Segmentation",
      "Task (project management)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Ping"
      },
      {
        "surname": "Zhang",
        "given_name": "Yu"
      },
      {
        "surname": "Yuan",
        "given_name": "Li"
      },
      {
        "surname": "Xiao",
        "given_name": "Huaxin"
      },
      {
        "surname": "Lin",
        "given_name": "Binbin"
      },
      {
        "surname": "Xu",
        "given_name": "Xianghua"
      }
    ]
  },
  {
    "title": "Time-aware and task-transferable adversarial attack for perception of autonomous vehicles",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.01.010",
    "abstract": "With rapid development of self-driving vehicles, recent work in adversarial machine learning started to study adversarial examples (AEs) for perception of autonomous driving (AD). However, generating practical AEs for the perception module remains a significant challenge. Traditional adversarial attacks tend to focus on a single computer vision task, making it difficult to compromise multiple perception tasks such as object detection and segmentation simultaneously. Additionally, the limited computational resources available on-board and the necessity for online operation pose further obstacles to deploying adversarial attacks on real autonomous driving platforms. To address the aforementioned issues, we propose the Time-aware Perception Attack (TPA), which is a real-time cross-task adversarial attack for perception of autonomous driving. In particular, we propose a novel backbone based adversarial attack method to modify input images to approach Lipschitz Constant Point (LCP), which results in erroneous inferences for all the sub-models in perception module. The novel part of this work is proposing an efficient yet effective LCP approaching algorithm. Comparing to conventional LCP based attacks, which consume significant amount of computation resources and can be only applied on small DNNs, TPA generates AEs on an intermediate layer of surrogate backbone, significantly enhancing the cross-task transferability and accelerates the attack process. Evaluation results on Berkeley Driving Dataset 100k (BDD100k) show that, comparing to the state-of-the-art baselines, the proposed TPA achieves higher attack effectiveness and faster processing speed and outperforms the baselines by a large margin.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524000102",
    "keywords": [
      "Adversarial system",
      "Adversary",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer security",
      "Economics",
      "Machine learning",
      "Management",
      "Margin (machine learning)",
      "Neuroscience",
      "Perception",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Lu",
        "given_name": "Yantao"
      },
      {
        "surname": "Ren",
        "given_name": "Haining"
      },
      {
        "surname": "Chai",
        "given_name": "Weiheng"
      },
      {
        "surname": "Velipasalar",
        "given_name": "Senem"
      },
      {
        "surname": "Li",
        "given_name": "Yilan"
      }
    ]
  },
  {
    "title": "Two-stage fine-grained image classification model based on multi-granularity feature fusion",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110042",
    "abstract": "Fine-grained visual classification (FGVC) is a difficult task due to the challenges of discriminative feature learning. Most existing methods directly use the final output of the network which always contains the global feature with high-level semantic information. However, the differences between fine-grained images are reflected in subtle local regions which often appear in the front of the network. When the texture of the background and object are similar or the proportion of the background is too large, the prediction will be greatly affected. In order to solve the above problems, this paper proposes multi-granularity feature fusion module (MGFF) and two-stage classification based on Vision-Transformer (ViT). The former comprehensively represents images by fusing features of different granularities, thus avoiding the limitations of single-scale features. The latter leverages the ViT model to separate the object from the background at a very small cost, thereby improving the accuracy of the prediction. We conduct comprehensive experiments and achieves the best performance in two fine-grained tasks on CUB-200-2011 and NA-Birds.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323007392",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Discriminative model",
      "Feature (linguistics)",
      "Fusion",
      "Granularity",
      "Linguistics",
      "Machine learning",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Yang"
      },
      {
        "surname": "Wu",
        "given_name": "Shanshan"
      },
      {
        "surname": "Wang",
        "given_name": "Biqi"
      },
      {
        "surname": "Yang",
        "given_name": "Ming"
      },
      {
        "surname": "Wu",
        "given_name": "Zebin"
      },
      {
        "surname": "Yao",
        "given_name": "Yazhou"
      },
      {
        "surname": "Wei",
        "given_name": "Zhihui"
      }
    ]
  },
  {
    "title": "Gaze estimation with semi-supervised eye landmark detection as an auxiliary task",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109980",
    "abstract": "The changes in gaze are often reflected in the movements of eye landmarks, highlighting the relevance of eye landmark learning for accurate gaze estimation. To leverage eye landmarks, we propose a gaze estimation framework that incorporates eye landmark detection as an auxiliary task. However, obtaining eye landmark annotations for real-world gaze datasets is challenging. To address this, we exploit synthetic data, which provides precise eye landmark labels, by jointly training an eye landmark detector using labeled synthetic data and unlabeled real-world data in a semi-supervised manner. To reduce the influence of discrepancy between synthetic and real-world data, we improve the generalization ability of the landmark detector by performing a self-supervised learning strategy on a large scale of unlabeled real-world images. The proposed method outperforms other state-of-the-art gaze estimation methods on three gaze datasets, indicating the effectiveness of leveraging eye landmark detection as an auxiliary task to enhance gaze estimation performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323006787",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Engineering",
      "Estimation",
      "Eye tracking",
      "Gaze",
      "Landmark",
      "Pattern recognition (psychology)",
      "Systems engineering",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Sun",
        "given_name": "Yunjia"
      },
      {
        "surname": "Zeng",
        "given_name": "Jiabei"
      },
      {
        "surname": "Shan",
        "given_name": "Shiguang"
      }
    ]
  },
  {
    "title": "Generative adversarial networks via a composite annealing of noise and diffusion",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110034",
    "abstract": "Generative adversarial network (GAN) is a framework for generating fake data using a set of real examples. However, GAN is unstable in the training stage. In order to stabilize GANs, the noise injection has been used to enlarge the overlap of the real and fake distributions at the cost of increasing variance. The diffusion process (or data smoothing in its spatial domain) removes fine details in order to capture the structure and important patterns in data but it suppresses the capability of GANs to learn high-frequency information in the training procedure. Based on these observations, we propose a data representation for the GAN training, called noisy scale-space (NSS), that recursively applies the smoothing with a balanced noise to data in order to replace the high-frequency information by random data, leading to a coarse-to-fine training of GANs. We experiment with NSS using DCGAN and StyleGAN2 based on benchmark datasets in which the NSS-based GANs outperforms the state-of-the-arts in most cases.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323007318",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Generative grammar",
      "Image (mathematics)",
      "Law",
      "Machine learning",
      "Noise (video)",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Smoothing"
    ],
    "authors": [
      {
        "surname": "Nakamura",
        "given_name": "Kensuke"
      },
      {
        "surname": "Korman",
        "given_name": "Simon"
      },
      {
        "surname": "Hong",
        "given_name": "Byung-Woo"
      }
    ]
  },
  {
    "title": "Parallel disentangling network for human–object interaction detection",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110021",
    "abstract": "Human–object interaction (HOI) detection aims to localize and classify triplets of human, object and interaction from a given image. Earlier two-stage methods suffer both from mutually independent training processes and the interference of redundant negative human–object pairs. Prevailing one-stage transformer-based methods are free from the above problems by tackling HOI in an end-to-end manner. However, one-stage transformer-based methods carry the unnecessary entanglements of the query for different tasks, i.e., human–object detection and interaction classification, and thus bring in poor performance. In this paper, we propose a new transformer-based approach that parallelly disentangles human–object detection and interaction classification in a triplet-wise manner. To make each query focus on one specific task clearly, we exhaustively disentangle HOI by parallelly expanding the naive query in vanilla transformer as triple explicit queries. Then, we introduce a semantic communication layer to preserve the consistent semantic association of each HOI through mixing the feature representations of each query triplet of the correspondence constraint. Extensive experiments demonstrate that our proposed framework outperforms the existing methods and achieves the state-of-the-art performance, with significant reduction in parameters and FLOPs.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323007185",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Machine learning",
      "Object detection",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Cheng",
        "given_name": "Yamin"
      },
      {
        "surname": "Duan",
        "given_name": "Hancong"
      },
      {
        "surname": "Wang",
        "given_name": "Chen"
      },
      {
        "surname": "Chen",
        "given_name": "Zhijun"
      }
    ]
  },
  {
    "title": "A Sparse Local Binary Pattern extraction algorithm applied to event sensor data for object classification",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110004",
    "abstract": "Recently, new sensors with active pixels were brought to market. These sensors export local variations of light intensity in the form of asynchronous events with low latency. Since the data output format is a stream of addressable events and not a complete image of light intensities, new algorithms are required for known problems in the field of Computer Vision, such as segmentation, VO, SLAM, object, and scene recognition. There are some proposed methodologies for object recognition using conventional methods, convolutional neural networks, and third-generation neural networks based on spikes. However, convolutional neural networks and spike neural networks require specific hardware for processing, hard to miniaturize. Also, several traditional Computer Vision operators and feature descriptors have been neglected in the context of event sensors and could contribute to lighter methodologies in object recognition. This paper proposes an algorithm for local binary pattern extraction in sparse structures, typically found in this context. This paper also proposes two methodologies using local binary patterns to captures with event-based sensors for object recognition. The first methodology exploits the known motion performed by the sensor, while the second is motion agnostic. It is demonstrated experimentally that the LBP operator is a fast and light alternative that enables variable reduction using PCA in some cases. The experiments also show that it is possible to reduce the final feature vector for classification by up to 99 , 73 % when compared to conventional methods considered state-of-the-art while maintaining comparable accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323007021",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Cognitive neuroscience of visual object recognition",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Convolutional neural network",
      "Event (particle physics)",
      "Feature extraction",
      "Histogram",
      "Image (mathematics)",
      "Local binary patterns",
      "Object detection",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Physics",
      "Pixel",
      "Quantum mechanics",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Fardo",
        "given_name": "Fernando Azevedo"
      },
      {
        "surname": "Rodrigues",
        "given_name": "Paulo Sérgio Silva"
      }
    ]
  },
  {
    "title": "Beyond k -Means++: Towards better cluster exploration with geometrical information",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110036",
    "abstract": "Although k -means and its variants are known for their remarkable efficiency, they suffer from a strong dependence on the prior knowledge of K and the assumption of a circle-like pattern, which can result in the algorithms dividing the input space instead of discovering non-predetermined data patterns. Thus, we propose beyond k -means++ that infers and utilizes explicit clusters by emphasizing local geometrical information for better cluster exploration. To avoid the K dependence, a novel framework of iterative division and aggregation (IDA) over k -means++ is presented. It begins with any K ≥ 1 , then increases and reduces K along with the procedure of clusters’ division and aggregation, respectively. To break through the circle-like pattern limitation, we introduce a reasonability checking strategy (RCS) for cluster division. Given local geometrical information, RCS achieves arbitrary cluster shape support by rejecting edge patterns with distinguished convergence direction and merging adjacent clusters with pseudo-edge patterns. Furthermore, we design an edge shrinkage strategy (ESS). Taking edge patterns as the cluster prototype, it benefits accuracy by effectively avoiding representability reduction due to irregular distribution. To compensate for the loss of efficiency, a near maximin and random sampling algorithm is suggested for large-scale data with high dimensionality. Experimental results confirm that beyond k -means++ is featured by handling arbitrary cluster shapes with remarkable accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323007331",
    "keywords": [
      "Algorithm",
      "Arithmetic",
      "Artificial intelligence",
      "Cluster (spacecraft)",
      "Cluster analysis",
      "Combinatorics",
      "Computer science",
      "Convergence (economics)",
      "Curse of dimensionality",
      "Data mining",
      "Division (mathematics)",
      "Economic growth",
      "Economics",
      "Enhanced Data Rates for GSM Evolution",
      "Mathematics",
      "Programming language",
      "Topology (electrical circuits)"
    ],
    "authors": [
      {
        "surname": "Ping",
        "given_name": "Yuan"
      },
      {
        "surname": "Li",
        "given_name": "Huina"
      },
      {
        "surname": "Hao",
        "given_name": "Bin"
      },
      {
        "surname": "Guo",
        "given_name": "Chun"
      },
      {
        "surname": "Wang",
        "given_name": "Baocang"
      }
    ]
  },
  {
    "title": "TransBoNet: Learning camera localization with Transformer Bottleneck and Attention",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109975",
    "abstract": "6DoF camera localization is an important component of autonomous driving and navigation. Deep learning has achieved impressive results in localization, but its robustness in dynamic environments has not been adequately addressed. In this paper, we propose a framework based on hybrid attention mechanism which can be generally applied to existing CNN-based pose regressors to improve their robustness in dynamic environments. Specifically, we propose a novel Transformer Bottleneck (TBo) block including convolution, channel attention, and a position-aware self-attention mechanism, which extracts more geometrically robust features by capturing the corresponding long-term dependencies between pixels. Furthermore, we introduce shuffle attention (SA) before the pose regressor, which integrates feature information in both spatial and channel dimensions, forcing the network to learn geometrically robust features, reducing the effects of dynamic objects and illumination conditions to improve camera localization accuracy. We evaluate our method on commonly benchmarked indoor and outdoor datasets and the experimental results show that our proposed method can significantly improve localization performance compared compare favorably to contemporary pose regressors schemes. In addition, extensive ablation evaluations are conducted to prove the effectiveness of our proposed hybrid attention bottleneck block for pose regression networks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323006738",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Block (permutation group theory)",
      "Bottleneck",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Deep learning",
      "Embedded system",
      "Gene",
      "Geometry",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Song",
        "given_name": "Xiaogang"
      },
      {
        "surname": "Li",
        "given_name": "Hongjuan"
      },
      {
        "surname": "Liang",
        "given_name": "Li"
      },
      {
        "surname": "Shi",
        "given_name": "Weiwei"
      },
      {
        "surname": "Xie",
        "given_name": "Guo"
      },
      {
        "surname": "Lu",
        "given_name": "Xiaofeng"
      },
      {
        "surname": "Hei",
        "given_name": "Xinhong"
      }
    ]
  },
  {
    "title": "Multi-perspective thought navigation for source-free entity linking",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2023.12.020",
    "abstract": "Neural entity-linking models excel at bridging the lexical gap of multiple facets of facts, such as entity-related claims or evidence documents. Despite advancements in self-supervised learning and pretrained language models, challenges persist in entity linking, particularly in interpretability and transferability. Moreover, these models need many aligned documents to adapt to emerging entities, which may not be available due to data scarcity. In this work, we propose a novel Demonstrative Self-TrAining fRamework (D-STAR) that leverages multi-perspective thought navigation. D-STAR iteratively optimizes a question generator and an entity retriever by navigating thoughts on a dynamic graph reasoning across multiple perspectives for question generation. The generated question–answer pairs, along with hard negatives shared in the graph, enable adaptation with minimal computational overhead. Additionally, we introduce a new task, source-free entity linking, focusing on unsupervised transfer learning without direct access to original domain data. To demonstrate the feasibility of this task, we provide a generated question–answering dataset, FandomWiki, for novel entities. Our experiments show that D-STAR significantly improves baselines on SciFact, Zeshel, and FandomWiki.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523003677",
    "keywords": [
      "Artificial intelligence",
      "Bridging (networking)",
      "Computer network",
      "Computer science",
      "Dependency grammar",
      "Information retrieval",
      "Interpretability",
      "Machine learning",
      "Natural language processing",
      "Parsing",
      "Perspective (graphical)",
      "Question answering",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Peng",
        "given_name": "Bohua"
      },
      {
        "surname": "He",
        "given_name": "Wei"
      },
      {
        "surname": "Chen",
        "given_name": "Bin"
      },
      {
        "surname": "Villavicencio",
        "given_name": "Aline"
      },
      {
        "surname": "Wu",
        "given_name": "Chengfu"
      }
    ]
  },
  {
    "title": "Customized meta-dataset for automatic classifier accuracy evaluation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110026",
    "abstract": "Automatic classifier accuracy evaluation (ACAEval) on unlabeled test sets is critical for unseen real-world environments. The use of dataset-level regression on synthesized meta-datasets (comprised of many sample sets) has shown promising results for ACAEval. However, the existing meta-dataset for ACAEval is created using simple image transformations such as rotation and background substitution, which can make it difficult to ensure a reasonable distribution shift between the sample set and the test set. When the distribution shift is large, it becomes challenging to estimate the classifier accuracy on the test set using those sample sets. To ensure more robust ACAEval, this paper attempts to customize a meta-dataset in which each sample set has a reasonable distribution shift to the test set. An intra-class cycle-consistent adversarial learning (ICAL) method is introduced to transfer the style of a labeled training set to the style of the test set, by jointly considering the domain shift issue, the label flipping issue (the semantic information may be changed after style transformation), and the diversity of multiple sample sets in the meta-dataset. Experiments validate that under the same experimental setup, our method outperforms the existing ACAEval methods by a good margin, and achieves state-of-the-art performance on several standard benchmark datasets, including digit classification and natural image classification.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323007239",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Test set"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Yan"
      },
      {
        "surname": "Zhang",
        "given_name": "Zhang"
      },
      {
        "surname": "Huang",
        "given_name": "Yan"
      },
      {
        "surname": "Wu",
        "given_name": "Qiang"
      },
      {
        "surname": "Huang",
        "given_name": "Han"
      },
      {
        "surname": "Zhong",
        "given_name": "Yi"
      },
      {
        "surname": "Wang",
        "given_name": "Liang"
      }
    ]
  },
  {
    "title": "Semi-supervised class-conditional image synthesis with Semantics-guided Adaptive Feature Transforms",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110022",
    "abstract": "Generative Adversarial Networks (GANs) have become the mainstream models for class-conditional synthesis of high-fidelity images. To reduce the demand for labeled data, we propose a class-conditional GAN with Semantic-guided Adaptive Feature Transforms, which is referred to as SAFT-GAN for semi-supervised image synthesis. Instead of simply incorporating a classifier to infer the class labels of unlabeled data, the key idea behind SAFT-GAN is to incorporate class-semantic guidance in real-fake discrimination. More specifically, we adopt a two-head architecture for a discriminator: A label-embedded head identifies real and fake instances, conditioned on class label. To focus more on class-related regions, we exploit class-aware attention information to regularize this head via regional feature transforms. On the other hand, to make better use of unlabeled data, we design a label-free head, on which channel-adaptive feature transforms are imposed to fuse the discriminator and classifier features, such that the class semantics of synthesized images can be improved. Extensive experiments are performed to demonstrate how class-conditional image synthesis can benefit from the proposed feature transforms, and also demonstrate the superiority of SAFT-GAN.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323007197",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Classifier (UML)",
      "Computer science",
      "Detector",
      "Discriminator",
      "Feature (linguistics)",
      "Linguistics",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Huo",
        "given_name": "Xiaoyang"
      },
      {
        "surname": "Zhang",
        "given_name": "Yunfei"
      },
      {
        "surname": "Wu",
        "given_name": "Si"
      }
    ]
  },
  {
    "title": "Learning Foreground Information Bottleneck for few-shot semantic segmentation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109993",
    "abstract": "Few-shot semantic segmentation aims to segment unseen classes with only a few annotated samples, which has great values for the real-world application in the wild. However, since the target class is treated as the background in the training, the network tends to extract much irrelevant nuisance factors, which results in the feature undermining problem for the target class. Consequently, it is difficult to produce an accurate segmentation map. To address this problem, in this paper, we apply the information bottleneck theory to few-shot semantic segmentation and propose the Foreground Information Bottleneck (FIB) module. Based on the support information, FIB module filters out the irrelevant information and promotes the foreground-related feature paradigms. Meanwhile, to solve the intractable mutual information and enable the end-to-end optimization of FIB module, we derive the Foreground Information Bottleneck Loss (FIBLoss) according to the inherent attribute of few-shot segmentation. Moreover, since there exists severe noise interference in the wild, we design a Target Information Refinement (TIR) block to further exploit discriminative cues of foreground. TIR block calculates the pairwise interaction and exploits the detailed information of the foreground object, which is beneficial to the feature refinement. Extensive experiments on two challenging datasets reflect the proposed FIB module significantly improves the performance of few-shot segmentation and delivers the state-of-the-art results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032300691X",
    "keywords": [
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Bottleneck",
      "Class (philosophy)",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Discriminative model",
      "Embedded system",
      "Exploit",
      "Feature (linguistics)",
      "Geometry",
      "Information bottleneck method",
      "Linguistics",
      "Mathematics",
      "Mutual information",
      "Pairwise comparison",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Hu",
        "given_name": "Yutao"
      },
      {
        "surname": "Huang",
        "given_name": "Xin"
      },
      {
        "surname": "Luo",
        "given_name": "Xiaoyan"
      },
      {
        "surname": "Han",
        "given_name": "Jungong"
      },
      {
        "surname": "Cao",
        "given_name": "Xianbin"
      },
      {
        "surname": "Zhang",
        "given_name": "Jun"
      }
    ]
  },
  {
    "title": "MFAN: Mixing Feature Attention Network for trajectory prediction",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109997",
    "abstract": "Accurate trajectory prediction of surrounding agents is essential for autonomous vehicles, where the key challenge is to understand the complex interactions among agents. Previous works treat all interacted features between agents equally in modeling interaction, while neglecting their different importance to the interaction, thus inevitably limiting the interaction modeling ability. Besides, existing methods suffer from significant performance degradation when domain shifts, resulting in severely deviant prediction from reality. To address these issues, we propose a novel prediction framework, dubbed Mixing Feature Attention Network (MFAN). Specifically, the proposed mixing feature attention is a parallel design to adaptively determine the importance of different interacted features and simultaneously capture the global interaction feature to improve interaction modeling. Meanwhile, the spatial global interaction is modeled from a spatial edge-featured graph input to capture the enhanced spatial interaction. The temporal motion pattern is modeled from a temporal edge-featured graph input to enhance the domain adaption. Finally, we estimate the parameters of bivariant Gaussian distribution for trajectory prediction. Experimental results show that our method achieves superior performance in trajectory prediction while maintaining low computational complexity and performs accurate prediction even when domain shifts.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323006957",
    "keywords": [
      "Artificial intelligence",
      "Astronomy",
      "Computer science",
      "Domain (mathematical analysis)",
      "Enhanced Data Rates for GSM Evolution",
      "Feature (linguistics)",
      "Gaussian",
      "Graph",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Mixing (physics)",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Theoretical computer science",
      "Trajectory"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Jingzhong"
      },
      {
        "surname": "Yang",
        "given_name": "Lin"
      },
      {
        "surname": "Chen",
        "given_name": "Yuxuan"
      },
      {
        "surname": "Jin",
        "given_name": "Yue"
      }
    ]
  },
  {
    "title": "Towards self-explainable graph convolutional neural network with frequency adaptive inception",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109991",
    "abstract": "Graph convolutional neural networks (GCNs) have demonstrated powerful representing ability of irregular data, e.g., skeletal data and graph-structured data, providing the effective mechanism to fuse the neighbor nodes. However, inheriting from the deep learning, GCN also lacks interpretability, which hinders its application to scenarios that have high demand for transparency. Although, there have been many efforts on the interpretability of deep learning, they mainly concentrate on i.i.d data that is hard to be deployed to GCNs, which involve not only the node feature, but also the graph structure. There are few works that attempt to explain it with post-hoc manner, which can be biased, resulting in mis-representation of the true explanation. Therefore, in this paper, we propose a framework, namely ExpFiGCN, that reveals explainability of the GCNs from the perspective of graph structure and mathematical analysis. Specifically, ExpFiGCN can find the most intrinsically relevant node to the central node and obtain the informative and discriminative signals while performing denoising. For the graph structure, we find K -nearest nodes; for the mathematical analysis, every channel of a node and its neighborhoods contribute dynamically to the final channel signal, which can capture the inherent difference of different channels and neighbor nodes. Meanwhile, it can enhance the representation ability of nodes and ameliorate the over-smoothing problem. On the other hand, our model can dynamically adjust the importance of neighborhoods to the central vertex. We empirically validate the effectiveness of the proposed framework ExpFiGCN on various benchmark datasets. Experimental results show that our method achieves substantial improvements and outperforms the state-of-the-art performance strikingly.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323006891",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Data mining",
      "Deep learning",
      "Discriminative model",
      "Feature learning",
      "Graph",
      "Interpretability",
      "Machine learning",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Wei",
        "given_name": "Feifei"
      },
      {
        "surname": "Mei",
        "given_name": "Kuizhi"
      }
    ]
  },
  {
    "title": "Multiple instance learning from similarity-confidence bags",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109984",
    "abstract": "Multiple instance learning (MIL) is a classic weakly supervised learning approach, in which samples are grouped into bags that may contain varying numbers of instances. A bag is designated as positive if it contains at least one positive instance; otherwise, it is considered negative. Previous studies have consistently assumed that the bag labels are completely known. In fact, labeling every bag can be extremely challenging or even unfeasible due to the exorbitant expenses in terms of time and labor. Fortunately, it is much easier to obtain the similarity confidence, which represents the probability of two bags sharing the same label. How to employ it in MIL is worthy of study. Inspired by the above study, we present the first attempt to investigate MIL from similarity-confidence bags. Therefore, this paper proposes a new framework for training bag-level classifiers that adheres to the principle of empirical risk minimization. Moreover, we theoretically derive a generalization error bound to guarantee model convergence. Finally, we implement risk correction to mitigate potential over-fitting problem and provide theoretical consistency. Numerical experiments on eight datasets further validate the effectiveness of the proposed bag-level classifier.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323006829",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Consistency (knowledge bases)",
      "Data mining",
      "Generalization",
      "Image (mathematics)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Xuan"
      },
      {
        "surname": "Xu",
        "given_name": "Yitian"
      },
      {
        "surname": "Liu",
        "given_name": "Xuhua"
      }
    ]
  },
  {
    "title": "Introducing instance label correlation in multiple instance learning. Application to cancer detection on histopathological images",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110057",
    "abstract": "In the last years, the weakly supervised paradigm of multiple instance learning (MIL) has become very popular in many different areas. A paradigmatic example is computational pathology, where the lack of patch-level labels for whole-slide images prevents the application of supervised models. Probabilistic MIL methods based on Gaussian Processes (GPs) have obtained promising results due to their excellent uncertainty estimation capabilities. However, these are general-purpose MIL methods that do not take into account one important fact: in (histopathological) images, the labels of neighboring patches are expected to be correlated. In this work, we extend a state-of-the-art GP-based MIL method, which is called VGPMIL-PR, to exploit such correlation. To do so, we develop a novel coupling term inspired by the statistical physics Ising model. We use variational inference to estimate all the model parameters. Interestingly, the VGPMIL-PR formulation is recovered when the weight that regulates the strength of the Ising term vanishes. The performance of the proposed method is assessed in two real-world problems of prostate cancer detection. We show that our model achieves better results than other state-of-the-art probabilistic MIL methods. We also provide different visualizations and analysis to gain insights into the influence of the novel Ising term. These insights are expected to facilitate the application of the proposed model to other research areas.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323007549",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Correlation",
      "Gaussian",
      "Geometry",
      "Inference",
      "Ising model",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Probabilistic logic",
      "Quantum mechanics",
      "Statistical model",
      "Statistical physics"
    ],
    "authors": [
      {
        "surname": "Morales-Álvarez",
        "given_name": "Pablo"
      },
      {
        "surname": "Schmidt",
        "given_name": "Arne"
      },
      {
        "surname": "Hernández-Lobato",
        "given_name": "José Miguel"
      },
      {
        "surname": "Molina",
        "given_name": "Rafael"
      }
    ]
  },
  {
    "title": "Convolutional neural networks rarely learn shape for semantic segmentation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110018",
    "abstract": "Shape learning, or the ability to leverage shape information, could be a desirable property of convolutional neural networks (CNNs) when target objects have specific shapes. While some research on the topic is emerging, there is no systematic study to conclusively determine whether and under what circumstances CNNs learn shape. Here, we present such a study in the context of segmentation networks where shapes are particularly important. We define shape and propose a new behavioral metric to measure the extent to which a CNN utilizes shape information. We then execute a set of experiments with synthetic and real-world data to progressively uncover under which circumstances CNNs learn shape and what can be done to encourage such behavior. We conclude that (i) CNNs do not learn shape in typical settings but rather rely on other features available to identify the objects of interest, (ii) CNNs can learn shape, but only if the shape is the only feature available to identify the object, (iii) sufficiently large receptive field size relative to the size of target objects is necessary for shape learning; (iv) a limited set of augmentations can encourage shape learning; (v) learning shape is indeed useful in the presence of out-of-distribution data.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032300715X",
    "keywords": [
      "Active shape model",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Convolutional neural network",
      "Economics",
      "Heat kernel signature",
      "Leverage (statistics)",
      "Machine learning",
      "Metric (unit)",
      "Object (grammar)",
      "Operations management",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Programming language",
      "Segmentation",
      "Set (abstract data type)",
      "Shape analysis (program analysis)",
      "Static analysis"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Yixin"
      },
      {
        "surname": "Mazurowski",
        "given_name": "Maciej A."
      }
    ]
  },
  {
    "title": "A segmentation method based on the deep fuzzy segmentation model in combined with SCANDLE clustering",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110027",
    "abstract": "To enhance the low clustering accuracy of the fuzzy clustering segmentation algorithm for analyzing high spatial resolution remote sensing images (HSRRSIs), a deep fuzzy segmentation model (DFSM)combined with Spectral Clustering with Adaptive Neighbors for Deep Learning (SCANDLE) clustering is proposed. The DFSM is used to over-segment the image, and the automatic coding structure is used to adaptively fuse the image features, minimizing the internal compactness and maximizing the external separability of the clustering, yielding better results. Meanwhile, the SCANDLE clustering model is used to cluster the over-segmentation results, and the matrix construction algorithm for adaptive neighborhood allocation is used to map the frame of the connected layer and optimally combine the over-segmentation images to realize the final segmentation results. The new method can accurately segment HSRRSIs with good segmentation performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323007240",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Computer vision",
      "Fuzzy clustering",
      "Fuzzy logic",
      "Image segmentation",
      "Pattern recognition (psychology)",
      "Region growing",
      "Scale-space segmentation",
      "Segmentation",
      "Segmentation-based object categorization",
      "Spectral clustering"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Zenan"
      },
      {
        "surname": "Niu",
        "given_name": "Haipeng"
      },
      {
        "surname": "Wang",
        "given_name": "Xiaoxuan"
      },
      {
        "surname": "Fan",
        "given_name": "Liangxin"
      }
    ]
  },
  {
    "title": "Traffic sign attack via pinpoint region probability estimation network",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110035",
    "abstract": "Recent work show that Deep Neural Networks (DNNs) have created great performance in many tasks, but they are vulnerable to adversarial examples which trigger Artificial Intelligence (AI) security risks. Especially in the autonomous driving field, attacking a traffic sign classification network results in a serious consequence. Most existing researches prefer to digital level attacks focusing on smaller or more imperceptible adversarial noise. Given that attacks available to real-world implementation usually emerge in more security-critical scenarios, we propose an adaptively adversarial example generation algorithm for physical attacks in the real-world setting. Taking account of the traffic sign classification, our approach is divided into two steps. The first step is to generate a probability map which precisely predicts the probability of being attacked for each pixel in the input image through the proposed Pinpoint Region Probability Estimation Network (PRPEN) and meanwhile, try to reduce the size of the highlighted area in the map. It can also be regarded as a classification problem in which every pixel has two classes, suitable for attacking or not, including the restrictions on the number of items in the suitable sort. The second one is to remake a mask depending on the probability map and optimize adversarial patches only on what mask decides. Experimental results show that our method achieves almost 100% misclassification rate in several widely used networks with even smaller patches. We also find how to effectively disguise as a target class to mislead the DNN classifiers and improve AI security.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032300732X",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Artificial neural network",
      "Class (philosophy)",
      "Computer science",
      "Computer vision",
      "Field (mathematics)",
      "Image (mathematics)",
      "Information retrieval",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Noise (video)",
      "Pattern recognition (psychology)",
      "Pixel",
      "Pure mathematics",
      "Sign (mathematics)",
      "Traffic sign",
      "Traffic sign recognition",
      "sort"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Yue"
      },
      {
        "surname": "Liu",
        "given_name": "Minjie"
      },
      {
        "surname": "Ren",
        "given_name": "Yanli"
      },
      {
        "surname": "Zhang",
        "given_name": "Xinpeng"
      },
      {
        "surname": "Feng",
        "given_name": "Guorui"
      }
    ]
  },
  {
    "title": "Capturing natural position relationships: A neural differential equation approach",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2023.12.006",
    "abstract": "The Transformer has emerged as the predominant model in Natural Language Processing due to its exceptional performance in various sequence modeling tasks, particularly in handling long-term dependencies. However, the traditional absolute and relative position encoding methods, which do not learn from data, tend to ignore the inherent structure of natural language sequences due to the position embedding layer at the input end of the Transformer model. This paper introduces a novel learnable neural Ordinary Differential Equation Position Encoding (ODEPE) method that can implicitly capture the natural position relationships within a sequence without requiring additional position embeddings. ODEPE can model continuous sequences and leverage differential equations to simulate the evolution of position information along the sequence, enabling position information to flow seamlessly between sequences. Additionally, a highly effective recurrent attention framework is proposed, which hybridizes attention with the ODEPE method to improve model performance. Compared to the Transformer-based sequence modeling network, our framework demonstrates a performance improvement of 24.0 points on the WikiText-103 dataset, while also achieving a performance improvement of 1.06 points on the Enwik8 dataset. This corresponds to an improvement of 4.9% and 0.17%, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523003537",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer science",
      "Differential equation",
      "Economics",
      "Electrical engineering",
      "Embedding",
      "Engineering",
      "Finance",
      "Genetics",
      "Leverage (statistics)",
      "Mathematical analysis",
      "Mathematics",
      "Ordinary differential equation",
      "Position (finance)",
      "Sequence (biology)",
      "Theoretical computer science",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Ji",
        "given_name": "Changqing"
      },
      {
        "surname": "Wang",
        "given_name": "Liyong"
      },
      {
        "surname": "Qin",
        "given_name": "Jing"
      },
      {
        "surname": "Kang",
        "given_name": "Xin"
      },
      {
        "surname": "Wang",
        "given_name": "Zumin"
      }
    ]
  },
  {
    "title": "A network classification method based on density time evolution patterns extracted from network automata",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109946",
    "abstract": "Network modeling has proven to be an efficient tool for many interdisciplinary areas, including social, biological, transportation, and various other complex real-world systems. In addition, cellular automata (CA) are a formalism that has received significant attention in recent decades as a model for investigating patterns in the dynamic spatio-temporal behavior of these systems, based on local rules. Some studies investigate the use of cellular automata to analyze the dynamic behavior of networks and refer to them as network automata (NA). Recently, it has been demonstrated that NA is effective for network classification, as it employs a Time-Evolution Pattern (TEP) for feature extraction. However, the TEPs investigated in previous studies consist of binary values (states) that do not capture the intrinsic details of the analyzed network. Therefore, in this work, we propose alternative sources of information that can be used as descriptors for the classification task, which we refer as Density Time-Evolution Pattern (D-TEP) and State Density Time-Evolution Pattern (SD-TEP). We examine the density of alive neighbors of each node, which is a continuous value, and compute feature vectors based on histograms of TEPs. Our results demonstrate significant improvement over previous studies on five synthetic network datasets, as well as seven real datasets. Our proposed method is not only a promising approach for pattern recognition in networks, but also shows considerable potential for other types of data that can be transformed into network.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323006441",
    "keywords": [
      "Arithmetic",
      "Art",
      "Artificial intelligence",
      "Automaton",
      "Binary number",
      "Biological network",
      "Cellular automaton",
      "Combinatorics",
      "Computer science",
      "Data mining",
      "Formalism (music)",
      "Histogram",
      "Image (mathematics)",
      "Mathematics",
      "Musical",
      "Pattern recognition (psychology)",
      "Theoretical computer science",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Zielinski",
        "given_name": "Kallil M.C."
      },
      {
        "surname": "Ribas",
        "given_name": "Lucas C."
      },
      {
        "surname": "Machicao",
        "given_name": "Jeaneth"
      },
      {
        "surname": "Bruno",
        "given_name": "Odemir M."
      }
    ]
  },
  {
    "title": "Sequential visual and semantic consistency for semi-supervised text recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.01.008",
    "abstract": "Scene text recognition (STR) is a challenging task that requires large-scale annotated data for training. However, collecting and labeling real text images is expensive and time-consuming, which limits the availability of real data. Therefore, most existing STR methods resort to synthetic data, which may introduce domain discrepancy and degrade the performance of STR models. To alleviate this problem, recent semi-supervised STR methods exploit unlabeled real data by enforcing character-level consistency regularization between weakly and strongly augmented views of the same image. However, these methods neglect word-level consistency, which is crucial for sequence recognition tasks. This paper proposes a novel semi-supervised learning method for STR that incorporates word-level consistency regularization from both visual and semantic aspects. Specifically, we devise a shortest path alignment module to align the sequential visual features of different views and minimize their distance. Moreover, we adopt a reinforcement learning framework to optimize the semantic similarity of the predicted strings in the embedding space. We conduct extensive experiments on several standard and challenging STR benchmarks and demonstrate the superiority of our proposed method over existing semi-supervised STR methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524000084",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Consistency (knowledge bases)",
      "Embedding",
      "Exploit",
      "Image (mathematics)",
      "Machine learning",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Regularization (linguistics)",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Mingkun"
      },
      {
        "surname": "Yang",
        "given_name": "Biao"
      },
      {
        "surname": "Liao",
        "given_name": "Minghui"
      },
      {
        "surname": "Zhu",
        "given_name": "Yingying"
      },
      {
        "surname": "Bai",
        "given_name": "Xiang"
      }
    ]
  },
  {
    "title": "Domain generalization via Inter-domain Alignment and Intra-domain Expansion",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110029",
    "abstract": "The performance of traditional deep learning models tends to drop dramatically during being deployed in real-world scenarios when the distribution shift between the seen training and unseen test data occurs. Domain Generalization methods are designed to achieve generalizability to deal with the above issue. Since the features extracted by softmax cross-entropy loss are not adequately domain-invariant, previous works in Domain Generalization have attempted to overcome this problem by employing contrastive-based losses which pull positive pairs (i.e., samples with the same class label) from different domains closer. Unfortunately, these approaches tend to produce an extremely small feature space, which is not robust facing unseen domain and easily overfits to source domains. To address the aforementioned issue, we propose a novel loss named IAIE Loss to simultaneously perform Inter-domain Alignment and Intra-domain Expansion for positive pairs, which facilitates the model to extract domain-invariant features and mitigates overfitting. Specifically, we design two sets of positive samples named “easy positive samples” and “hard positive samples”. IAIE Loss pulls the hard positive pairs closer (alignment) while pushing the easy positive pairs apart (expansion). The state-of-the-art results on multiple DG benchmark datasets verify the effectiveness of our method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323007264",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Computer science",
      "Deep learning",
      "Domain (mathematical analysis)",
      "Feature (linguistics)",
      "Generalizability theory",
      "Generalization",
      "Geodesy",
      "Geography",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Overfitting",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Softmax function",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Hu",
        "given_name": "Jiajun"
      },
      {
        "surname": "Qi",
        "given_name": "Lei"
      },
      {
        "surname": "Zhang",
        "given_name": "Jian"
      },
      {
        "surname": "Shi",
        "given_name": "Yinghuan"
      }
    ]
  },
  {
    "title": "Coordinate Descent Optimized Trace Difference Model for Joint Clustering and Feature Extraction",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110062",
    "abstract": "Joint clustering and dimensionality reduction methods are a promising solution to clustering due to its scalability to high-dimensional data. Some methods leverage trace ratio criterion and attain clusters by borrowing the K-means algorithm. However, trace ratio criterion has no close-formed solution for the discriminative projection matrix and the K-means algorithm has a limited capacity to handle the many-cluster problem. In this paper, Coordinate Descent Optimized Trace Difference model (CDOTD) is proposed for joint clustering and feature extraction. Formulating the objective function as a direct trace difference criterion containing a balance parameter, CDOTD harmonizes between-cluster scatter maximization and within-cluster scatter minimization by the balance parameter. Using the direct trace difference criterion, CDOTD can straightforward solve for the discriminative projection matrix and avoid obtaining a poor discriminative projection matrix in the iterative manner when a bad cluster start is given. CDOTD uses the coordinate descent method for clustering optimization, improving the ability to address the many-cluster problem. Extensive experiments show that CDOTD has achieved significant performance improvements compared to previous trace ratio criterion related joint clustering and feature extraction methods, and also outperformed other clustering methods in most cases.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323007598",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Canopy clustering algorithm",
      "Cluster analysis",
      "Computer science",
      "Coordinate descent",
      "Correlation clustering",
      "Dimensionality reduction",
      "Discriminative model",
      "Gradient descent",
      "Linguistics",
      "Mathematical optimization",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "TRACE (psycholinguistics)",
      "k-medians clustering"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Quan"
      },
      {
        "surname": "Wang",
        "given_name": "Fei"
      },
      {
        "surname": "Li",
        "given_name": "Zhongheng"
      },
      {
        "surname": "Wang",
        "given_name": "Zheng"
      },
      {
        "surname": "Nie",
        "given_name": "Feiping"
      }
    ]
  },
  {
    "title": "Adaptive local Principal Component Analysis improves the clustering of high-dimensional data",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110030",
    "abstract": "In local Principal Component Analysis (PCA), a distribution is approximated by multiple units, each representing a local region by a hyper-ellipsoid obtained through PCA. We present an extension for local PCA which adaptively adjusts both the learning rate of each unit and the potential function which guides the competition between the local units. Our local PCA method is an online neural network method where unit centers and shapes are modified after the presentation of each data point. For several benchmark distributions, we demonstrate that our method improves the overall quality of clustering, especially for high-dimensional distributions where many conventional methods do not perform satisfactorily. Our online method is also well suited for the processing of streaming data: The two adaptive mechanisms lead to a quick reorganization of the clustering when the underlying distribution changes.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323007276",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Astronomy",
      "Benchmark (surveying)",
      "Cluster analysis",
      "Clustering high-dimensional data",
      "Computer science",
      "Data mining",
      "Dimensionality reduction",
      "Ellipsoid",
      "Geodesy",
      "Geography",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Principal component analysis"
    ],
    "authors": [
      {
        "surname": "Migenda",
        "given_name": "Nico"
      },
      {
        "surname": "Möller",
        "given_name": "Ralf"
      },
      {
        "surname": "Schenck",
        "given_name": "Wolfram"
      }
    ]
  },
  {
    "title": "Fast generalized ramp loss support vector machine for pattern classification",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109987",
    "abstract": "Support vector machine (SVM) is widely recognized as an effective classification tool and has demonstrated superior performance in diverse applications. However, for large-scale pattern classification problems, it may require much memory and incur prohibitive computational costs. Motivated by this, we propose a new SVM model with novel generalized ramp loss ( L R -SVM). The first-order optimality conditions for the non-convex and non-smooth L R -SVM are developed by the newly developed P-stationary point, based on which, the L R support vectors and working set of L R -SVM are defined, interestingly, which shows that all of the L R support vectors are on the two support hyperplanes under mild conditions. A fast proximal alternating direction method of multipliers with working set ( L R -ADMM) is developed to handle L R -SVM and L R -ADMM has been demonstrated to achieve global convergence while maintaining a significantly low computational complexity. Numerical comparisons with nine leading solvers show that L R -ADMM demonstrates outstanding performance, particularly when applied to large-scale pattern classification problems with fewer support vectors, higher prediction accuracy and shorter computational time.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323006854",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computational complexity theory",
      "Computer science",
      "Convergence (economics)",
      "Economic growth",
      "Economics",
      "Geometry",
      "Hyperplane",
      "Mathematical optimization",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Programming language",
      "Quantum mechanics",
      "Regular polygon",
      "Scale (ratio)",
      "Set (abstract data type)",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Huajun"
      },
      {
        "surname": "Shao",
        "given_name": "Yuanhai"
      }
    ]
  },
  {
    "title": "LiDAR video object segmentation with dynamic kernel refinement",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2023.12.013",
    "abstract": "In this paper, we formalize memory- and tracking-based methods to perform the LiDAR-based Video Object Segmentation (VOS) task, which segments points of the specific 3D target (given in the first frame) in a LiDAR sequence. LiDAR-based VOS can directly provide target-aware geometric information for practical application scenarios like behavior analysis and anticipating danger. We first construct a LiDAR-based VOS dataset named KITTI-VOS based on SemanticKITTI, which acts as a testbed and facilitates comprehensive evaluations of algorithm performance. Next, we provide two types of baselines, i.e., memory-based and tracking-based baselines, to explore this task. Specifically, the first memory-based pipeline is built on a space–time memory network equipped with the non-local spatiotemporal attention-based memory bank. We further design a more potent variant to introduce the locality into the spatiotemporal attention module by local self-attention and cross-attention modules. For the second tracking-based baseline, we modify two representative 3D object tracking methods to adapt to LiDAR-based VOS tasks. Finally, we propose a refine module that takes mask priors and generates object-aware kernels, which could boost all the baselines’ performance. We evaluate the proposed methods on the dataset and demonstrate their effectiveness.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523003604",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Economics",
      "Frame (networking)",
      "Geology",
      "Kernel (algebra)",
      "Lidar",
      "Linguistics",
      "Locality",
      "Management",
      "Mathematics",
      "Object (grammar)",
      "Object detection",
      "Pedagogy",
      "Philosophy",
      "Pipeline (software)",
      "Programming language",
      "Psychology",
      "Remote sensing",
      "Segmentation",
      "Task (project management)",
      "Telecommunications",
      "Tracking (education)",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Mei",
        "given_name": "Jianbiao"
      },
      {
        "surname": "Yang",
        "given_name": "Yu"
      },
      {
        "surname": "Wang",
        "given_name": "Mengmeng"
      },
      {
        "surname": "Li",
        "given_name": "Zizhang"
      },
      {
        "surname": "Ra",
        "given_name": "Jongwon"
      },
      {
        "surname": "Liu",
        "given_name": "Yong"
      }
    ]
  },
  {
    "title": "Deep orientated distance-transform network for geometric-aware centerline detection",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110028",
    "abstract": "The detection of structure centerlines from imaging data plays a crucial role in the understanding, application and further analysis of many diverse problems, such as road mapping, crack detection, medical imaging and biometric identification. In each of these cases, pixel-wise segmentation is not sufficient to understand and quantify overall graph structure and connectivity without further processing that can lead to compound error. We thus require a method for automatic extraction of graph representations of patterning. In this paper, we propose a novel Deep Orientated Distance-transform Network (DODN), which predicts the centerline map and an orientated distance map, comprising orientation and distance in relation to the centerline and allowing exploitation of its geometric properties. This is refined by jointly modeling the relationship between neighboring pixels and connectivity to further enhance the estimated centerline and produce a graph of the structure. The proposed approach is evaluated on a diverse range of problems, including crack detection, road mapping and superficial vein centerline detection from infrared/ color images, improving over the state-of-the-art by 2.1%, 10.9% and 17.3%/ 4.6% respectively in terms of quality, demonstrating its generalizability and performance in a wide range of mapping problems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323007252",
    "keywords": [
      "Artificial intelligence",
      "Biometrics",
      "Computer science",
      "Computer vision",
      "Distance transform",
      "Generalizability theory",
      "Graph",
      "Image (mathematics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pixel",
      "Segmentation",
      "Statistics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Jiang",
        "given_name": "Zheheng"
      },
      {
        "surname": "Rahmani",
        "given_name": "Hossein"
      },
      {
        "surname": "Angelov",
        "given_name": "Plamen"
      },
      {
        "surname": "Vyas",
        "given_name": "Ritesh"
      },
      {
        "surname": "Zhou",
        "given_name": "Huiyu"
      },
      {
        "surname": "Black",
        "given_name": "Sue"
      },
      {
        "surname": "Williams",
        "given_name": "Bryan"
      }
    ]
  },
  {
    "title": "Reparameterizing and dynamically quantizing image features for image generation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109962",
    "abstract": "For autoregressive image generation, vector-quantized VAEs (VQ-VAEs) quantize image features with discrete codebook entries and reconstruct images from quantized features. However, they treat each codebook entry separately, which causes losses of image details. In this paper, we propose to reparameterize image features with weight vectors to treat all codebook entries as an entity, and present a novel dynamically vector quantized VAE (DVQ-VAE) to quantize reparameterized image features. Specifically, each image feature corresponds to a weight vector and we sum weighted codebook entries to obtain values of image features. In this way, image features can incorporate information from different codebook entries. Additionally, a novel continuous weight regularization loss is proposed to improve the reconstruction of image details. Our method achieves competitive results with prior state-of-the-art works for image generation and extensive experiments are conducted to take a deep insight into our DVQ-VAE.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032300660X",
    "keywords": [
      "Artificial intelligence",
      "Autoregressive model",
      "Codebook",
      "Computer science",
      "Computer vision",
      "Econometrics",
      "Feature (linguistics)",
      "Feature vector",
      "Image (mathematics)",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Regularization (linguistics)",
      "Vector quantization"
    ],
    "authors": [
      {
        "surname": "Sun",
        "given_name": "Mingzhen"
      },
      {
        "surname": "Wang",
        "given_name": "Weining"
      },
      {
        "surname": "Zhu",
        "given_name": "Xinxin"
      },
      {
        "surname": "Liu",
        "given_name": "Jing"
      }
    ]
  },
  {
    "title": "Frequency-aware feature aggregation network with dual-task consistency for RGB-T salient object detection",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110043",
    "abstract": "RGB-Thermal salient object detection (SOD) aims to merge two spectral images to segment visually appealing objects. Current methods primarily extract salient object information in the pixel perspective. However, biological and psychological research indicates notable frequency sensitivity of the human visual system (HVS). The high-frequency (HF) and low-frequency (LF) information in images are processed by different neural channels, which has been overlooked in SOD. In this study, we argue that the objective of RGB-T SOD is not only to enhance feature representation in the pixel-aware but also to emulate human visual mechanisms. To our best knowledge, we explore RGB-T SOD from the frequency perspective for the first time. Specifically, we first present a frequency-aware multi-spectral feature aggregation module (FMFA) to exploit the separability and complementarity of frequency-aware features, generating and making full use of LF and HF cues. FMFA improves the feature representation of RGB-T from the frequency perspective and provides stronger frequency cues for boundary auxiliary tasks. Then, we develop an HF-guided signed distance map prediction module (HF-SDM) with dual-task consistency to effectively alleviate the coarse mask caused by blur boundary. HF-SDM employs the geometric relationship of objects which boosts the interaction between salient regions and boundaries. As a result, the model can gain sharper boundaries for salient objects. Finally, we propose a frequency-aware feature aggregation network (FFANet) incorporated with dual-task learning. Extensive experiments on RGB-T SOD datasets demonstrate that our proposed method outperforms other state-of-the-art methods. Ablation studies and visualizations further verify the effectiveness and interpretability of our method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323007409",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "RGB color model",
      "Salient"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Heng"
      },
      {
        "surname": "Tian",
        "given_name": "Chunna"
      },
      {
        "surname": "Zhang",
        "given_name": "Zhenxi"
      },
      {
        "surname": "Li",
        "given_name": "Chengyang"
      },
      {
        "surname": "Xie",
        "given_name": "Yongqiang"
      },
      {
        "surname": "Li",
        "given_name": "Zhongbo"
      }
    ]
  },
  {
    "title": "Weighted subspace anomaly detection in high-dimensional space",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110056",
    "abstract": "Anomaly detection aims at finding anomalies deviating from the normal data patterns. Virtually all anomaly detection methods create a model of the normal patterns before finding anomalies. In high-dimensional scenarios, due to the curse of dimensionality, it is difficult to construct the model of normal patterns in the full dimensional space. Subspace methods assuming that data can be characterized by low-dimensional manifolds have attracted a great deal of research. However, in unsupervised setting, unlabeled data is composed of both the normal and the abnormal data. The existence of anomalies might affect the establishment of the underlying normal subspaces. The undetermined number of the underlying subspaces also brings difficulties in subspace selection. To tackle the aforementioned problems, we come up with a weighted subspace anomaly detection (WSAD) method. We utilize correntropy to construct an objective function to mitigate the influence of the anomalies, which can be regarded as a weighting method for different data. Besides, we introduce an auxiliary variable with block sparsity regularization to achieve adaptive subspace selection, which can be regarded as a weighting method for different subspaces. After the normal underlying subspaces being established, we define the outlier scores by considering the deviation from the underlying subspaces, the local outlier score within subspaces, and the subspace scale. We use the half-quadratic theory to transform the optimization problem defined in WSAD, and apply alternating optimization to solve the transformed problem. Theoretically, we prove the convergence of the optimization algorithm. Experimentally, we demonstrate the effectiveness of the proposed method on both synthetic data and real datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323007537",
    "keywords": [
      "Algorithm",
      "Anomaly (physics)",
      "Anomaly detection",
      "Artificial intelligence",
      "Computer science",
      "Condensed matter physics",
      "Curse of dimensionality",
      "Geometry",
      "Linear subspace",
      "Mathematics",
      "Medicine",
      "Outlier",
      "Pattern recognition (psychology)",
      "Physics",
      "Radiology",
      "Regularization (linguistics)",
      "Subspace topology",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Tu",
        "given_name": "Jiankai"
      },
      {
        "surname": "Liu",
        "given_name": "Huan"
      },
      {
        "surname": "Li",
        "given_name": "Chunguang"
      }
    ]
  },
  {
    "title": "Phase Randomization: A data augmentation for domain adaptation in human action recognition",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110051",
    "abstract": "Human action recognition models often suffer from achieving both accurate recognition and subject independence when the amount of training data is limited. In this paper, we propose a data-efficient domain adaptation approach to learning a subject-agnostic action recognition classifier. The core component of our approach is a novel data augmentation called Phase Randomization. On the basis of the observation that individual body size is highly correlated with the amplitude component of the motion sequence, we disentangle the individuality and action features by using contrastive self-supervised learning with data augmentation that randomizes only the phase component of the motion sequence. This enables us to estimate the subject label of each motion sequence and to train a subject-agnostic action recognition classifier by performing adversarial learning with the estimated subject labels. We empirically demonstrate the superiority of our method on two different action recognition tasks (skeleton-based action recognition and sensor-based activity recognition).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323007483",
    "keywords": [
      "Action recognition",
      "Artificial intelligence",
      "Class (philosophy)",
      "Classifier (UML)",
      "Computer science",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Mitsuzumi",
        "given_name": "Yu"
      },
      {
        "surname": "Irie",
        "given_name": "Go"
      },
      {
        "surname": "Kimura",
        "given_name": "Akisato"
      },
      {
        "surname": "Nakazawa",
        "given_name": "Atsushi"
      }
    ]
  },
  {
    "title": "GNaN: A natural neighbor search algorithm based on universal gravitation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110063",
    "abstract": "The natural neighbor (NaN) method and its search algorithm (NaN-Searching) are widely used in many fields, including pattern recognition and image processing. NaN-Searching fundamentally overcomes the problem of the conventional nearest neighbor algorithm in selecting parameters for datasets with arbitrary shapes and achieves good results. However, this algorithm uses the conventional distance metric as the neighbor judgment criterion, which cannot accurately reflect the overall structure of the dataset in the process of neighbor search. Inspired by Newton’s law of universal gravitation, we propose a NaN search algorithm based on universal gravitation (GNaN-Searching). Our algorithm calculates gravitation using the structural features of data points in the dataset, it utilizes the gravitation between data as the neighbor judgment criterion, and inherits the no-parameter and dynamic neighborhood characteristics of the NaN search algorithm. Experimental results show that the natural neighborhood graph obtained by our method has a high performance in the representation of manifold data. We also applied the new method to clustering and outlier detection and achieved satisfactory results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323007604",
    "keywords": [
      "Algorithm",
      "Archaeology",
      "Artificial intelligence",
      "Classical mechanics",
      "Computer science",
      "Geography",
      "Gravitation",
      "Mathematics",
      "Natural (archaeology)",
      "Newton's law of universal gravitation",
      "Physics"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Juntao"
      },
      {
        "surname": "Yang",
        "given_name": "Lijun"
      },
      {
        "surname": "Zhang",
        "given_name": "Jinghui"
      },
      {
        "surname": "Liang",
        "given_name": "Qiwen"
      },
      {
        "surname": "Wang",
        "given_name": "Wentong"
      },
      {
        "surname": "Tang",
        "given_name": "Dongming"
      },
      {
        "surname": "Liu",
        "given_name": "Tao"
      }
    ]
  },
  {
    "title": "Self-distillation and self-supervision for partial label learning",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110016",
    "abstract": "As a main branch of weakly supervised learning paradigm, partial label learning (PLL) copes with the situation where each sample corresponds to ambiguous candidate labels containing the unknown true label. The primary difficulty of PLL lies in label ambiguities, most existing researches focus on individual instance knowledge while ignore the importance of cross-sample knowledge. To circumvent this difficulty, an innovative multi-task framework is proposed in this work to integrate self-supervision and self-distillation to tackle PLL problem. Specifically, in the self-distillation task, cross-sample knowledge in the same batch is utilized to refine ensembled soft targets to supervise the distillation operation without using multiple networks. The auxiliary self-supervised task of recognizing rotation transformations of images provides more supervisory signal for feature learning. Overall, training supervision is constructed not only from the input data itself but also from other instances within the same batch. Empirical results on benchmark datasets reveal that this method is effective in learning from partially labeled data.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323007136",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Data mining",
      "Distillation",
      "Engineering",
      "Feature (linguistics)",
      "Focus (optics)",
      "Geodesy",
      "Geography",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Optics",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Sample (material)",
      "Scheme (mathematics)",
      "Systems engineering",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Yu",
        "given_name": "Xiaotong"
      },
      {
        "surname": "Sun",
        "given_name": "Shiding"
      },
      {
        "surname": "Tian",
        "given_name": "Yingjie"
      }
    ]
  },
  {
    "title": "Fast anchor graph preserving projections",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109996",
    "abstract": "The existing graph-based dimensionality reduction algorithms need to learn an adjacency matrix or construct it in advance, therefore the time complexity of the graph-based dimensionality reduction algorithms is not less than O ( n 2 d ) , where n denotes the number of samples, d denotes the number of dimensions. Moreover, the existing dimensionality reduction algorithms do not consider the cluster information in the original space, resulting in the weakening or even loss of valuable information after dimensionality reduction. To address the above problems, we propose Fast Anchor Graph Preserving Projections (FAGPP), which learns the projection matrix, the anchors and the membership matrix at the same time. Especially, FAGPP has a built-in Principal Component Analysis (PCA) item, which makes our model not only deal with the cluster information of data, but also deal with the global information of data. The time complexity of FAGPP is O ( n m d ) , where m denotes the number of the anchors and m is much less than n . We propose a novel iterative algorithm to solve the proposed model and the convergence of the algorithm is proved theoretically. The experimental results on a large number of high-dimensional benchmark image data sets demonstrate the efficiency of FAGPP. The data sets and the source code are available from https://github.com/511lab/FAGPP.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323006945",
    "keywords": [
      "Adjacency list",
      "Adjacency matrix",
      "Algorithm",
      "Artificial intelligence",
      "Composite material",
      "Computational complexity theory",
      "Computer science",
      "Curse of dimensionality",
      "Dimensionality reduction",
      "Geometry",
      "Graph",
      "Materials science",
      "Mathematics",
      "Matrix (chemical analysis)",
      "Principal component analysis",
      "Projection (relational algebra)",
      "Reduction (mathematics)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Jikui"
      },
      {
        "surname": "Wu",
        "given_name": "Yiwen"
      },
      {
        "surname": "Li",
        "given_name": "Bing"
      },
      {
        "surname": "Yang",
        "given_name": "Zhenguo"
      },
      {
        "surname": "Nie",
        "given_name": "Feiping"
      }
    ]
  },
  {
    "title": "Open set transfer learning through distribution driven active learning",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110055",
    "abstract": "Domain adaptation enables effective transfer between source and target domains with different distributions. The latest research focuses on open set domain adaptation; that is, the target domain contains unknown categories that do not exist in the source domain. The existing open set domain adaptation cannot realize the fine-grained recognition of unknown categories. In this paper, we propose an uncertainty analysis evidence model and design a distribution driven active transfer learning (DATL) algorithm. DATL realizes fine-grained recognition of unknown categories with no requirements on the source domain to contain the unknown categories. To explore unknown distributions, the uncertainty analysis evidence model was adopted to divide the high uncertainty space. To select critical instances, a cluster-diversity query strategy was proposed to identify new categories. To enrich the label categories of the source domain, a global dynamic alignment strategy was designed to avoid negative transfers. Comparative experiments with state-of-the-art methods on the standard Office-31/Office-Home/Office-Caltech10 benchmarks showed that the DATL algorithm: (1) outperformed its competitors; (2) realized accurate identification of unknown subcategories from a fine-grained perspective; and (3) achieved outstanding performance even with a very high degree of openness.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323007525",
    "keywords": [
      "Adaptation (eye)",
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Competitor analysis",
      "Computer science",
      "Data mining",
      "Domain (mathematical analysis)",
      "Economics",
      "Identification (biology)",
      "Machine learning",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Optics",
      "Perspective (graphical)",
      "Physics",
      "Programming language",
      "Set (abstract data type)",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Min"
      },
      {
        "surname": "Wen",
        "given_name": "Ting"
      },
      {
        "surname": "Jiang",
        "given_name": "Xiao-Yu"
      },
      {
        "surname": "Zhang",
        "given_name": "An-An"
      }
    ]
  },
  {
    "title": "A non-regularization self-supervised Retinex approach to low-light image enhancement with parameterized illumination estimation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110025",
    "abstract": "In current Retinex-based low-light image enhancement (LLIE) methods, fine-tuning regularization parameters for Retinex decomposition and illumination estimation can be cumbersome. To address this, we present a novel non-regularization self-supervised Retinex approach for illumination estimation. Our contributions are twofold: First, we introduce a self-supervised method that incorporates edge-aware smoothness properties in bilateral learning, eliminating the need for regularization terms and simplifying parameter adjustments. Second, to enforce smoothness constraints on the estimated bilateral grid, we propose a bilateral grid parameterization network. This network employs a generative encoder to parameterize the bilateral grid of illumination and a trainable slicing layer guided by a map, reconstructing the grid into an illumination map. Despite the absence of regularization terms, our model excels in generating piece-wise smooth illumination, resulting in enhanced naturalness and improved contrast in images. Our model offers exceptional flexibility by eliminating the need for additional regularization terms and parameter fine-tuning. Moreover, it does not depend on external datasets for training, overcoming dataset collection challenges. Extensive experiments, comparing our model with eight state-of-the-art methods across five public available datasets, unequivocally demonstrate our model’s state-of-the-art performance based on key metrics such as NIQE, NIQMC, and CPCQI. These results reaffirm the effectiveness of our approach in low-light image enhancement. Code will be available at: https://github.com/zhaozunjin/NeurBR.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323007227",
    "keywords": [
      "Artificial intelligence",
      "Color constancy",
      "Computer science",
      "Computer vision",
      "Detector",
      "Discriminator",
      "Geometry",
      "Grid",
      "Image (mathematics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Regularization (linguistics)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Zunjin"
      },
      {
        "surname": "Lin",
        "given_name": "Hexiu"
      },
      {
        "surname": "Shi",
        "given_name": "Daming"
      },
      {
        "surname": "Zhou",
        "given_name": "Guoqing"
      }
    ]
  },
  {
    "title": "Efficient search of comprehensively robust neural architectures via multi-fidelity evaluation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110038",
    "abstract": "Neural architecture search (NAS) has emerged as one successful technique to find robust deep neural network (DNN) architectures. However, most existing robustness evaluations in NAS only consider l ∞ norm-based adversarial noises. In order to improve the robustness of DNN models against multiple types of noises, it is necessary to consider a comprehensive evaluation in NAS for robust architectures. But with the increasing number of types of robustness evaluations, it also becomes more time-consuming to find comprehensively robust architectures. To alleviate this problem, we propose a novel efficient search of comprehensively robust neural architectures via multi-fidelity evaluation (ES-CRNA-ME). Specifically, we first search for comprehensively robust architectures under multiple types of evaluations using the weight-sharing-based NAS method, including different l p norm attacks, semantic adversarial attacks, and composite adversarial attacks. In addition, we reduce the number of robustness evaluations by the correlation analysis, which can incorporate similar evaluations and decrease the evaluation cost. Finally, we propose a multi-fidelity online surrogate during optimization to further decrease the search cost. On the basis of the surrogate constructed by low-fidelity data, the online high-fidelity data is utilized to finetune the surrogate. Experiments on CIFAR10 and CIFAR100 datasets show the effectiveness of our proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323007355",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Fidelity",
      "Gene",
      "Machine learning",
      "Robustness (evolution)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Sun",
        "given_name": "Jialiang"
      },
      {
        "surname": "Yao",
        "given_name": "Wen"
      },
      {
        "surname": "Jiang",
        "given_name": "Tingsong"
      },
      {
        "surname": "Chen",
        "given_name": "Xiaoqian"
      }
    ]
  },
  {
    "title": "Contrastive clustering with a graph consistency constraint",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110032",
    "abstract": "Compared with classical contrastive learning methods, the performance of contrastive clustering is more easily affected by the quality of positive and negative samples, due to the fact that the clustering assumption requires neighbors of points as their positives. In order to reduce the effect of the uncertainty of positives and negatives on contrastive clustering, we propose a new contrastive clustering algorithm with graph consistency constraint. In this algorithm, a loss of graph consistency is proposed to reduce the impact of false negative samples by comparing the neighbor distribution of positive samples. Furthermore, an incremental training method is designed to control the quality of the selected positives. Extensive experiments show that our algorithm outperforms other deep clustering methods on wide-used benchmark data sets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032300729X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Consistency (knowledge bases)",
      "Constraint (computer-aided design)",
      "Constraint satisfaction problem",
      "Data mining",
      "False positive paradox",
      "Geometry",
      "Graph",
      "Local consistency",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Probabilistic logic",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Yunxiao"
      },
      {
        "surname": "Bai",
        "given_name": "Liang"
      }
    ]
  },
  {
    "title": "MCFP: A multi-target 3D perception method with weak dependence on 2D detectors",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.01.011",
    "abstract": "3D perception for multi-target in complex traffic environments plays an important role in autonomous driving. Nowadays, some mainstream models use 2D detectors to provide auxiliary information for 3D perception. But it is a challenge that the accuracy of 2D detectors has a sensitive impact on the final results. In this paper, we propose MCFP, a multi-target 3D perception model with weak dependence on 2D detectors, to improve the 3D perception performance. We use the point cloud expansion as the main idea to provide more information about instance objects for model learning. Firstly, the centroid awareness strategy is integrated into the front-end part for downsampling and feature extraction to reduce the loss of object-related information. Secondly, the ball query operation with adaptive radius is designed to obtain more complete object information. Experiments show that our model outperforms baseline models in normal environments, and achieves detection performances close to those of SOTA models in extreme environments, which implies that our model has a strong ability to obtain semantic information and hence enhancing the perception ability. The code and data will be made available at https://github.com/MrSakasky/MCFP.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524000114",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Code (set theory)",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Detector",
      "Image (mathematics)",
      "Marketing buzz",
      "Neuroscience",
      "Perception",
      "Point cloud",
      "Programming language",
      "Set (abstract data type)",
      "Telecommunications",
      "Upsampling",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Guo",
        "given_name": "Haoran"
      },
      {
        "surname": "He",
        "given_name": "Mingyun"
      },
      {
        "surname": "Li",
        "given_name": "Fan"
      },
      {
        "surname": "He",
        "given_name": "Kexin"
      },
      {
        "surname": "Chen",
        "given_name": "Lina"
      }
    ]
  },
  {
    "title": "A Cross-Lingual Summarization method based on cross-lingual Fact-relationship Graph Generation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109952",
    "abstract": "The aim of cross-lingual summarization (CLS) is to condense the content of a document in one language into a summary in another language. In essence, a CLS model requires both translation and summarization capabilities, which presents a unique challenge, as the model must effectively tackle the difficulties associated with both tasks simultaneously (e.g., semantic alignment, information compression and factual inconsistency). Graph-based semantic representation can model important text information in a structured manner, which may alleviate these challenges. Therefore, in this paper, we propose a Cross-Lingual Summarization method based on cross-lingual Fact-relationship Graph Generation (FGGCLS). Specifically, we first construct fact-relationship graphs for source language documents and target language summaries. Then, we introduce a cross-lingual fact-relationship graph generation method, which converts the CLS problem into a cross-lingual fact-relationship graph generation problem. This approach simplifies semantic alignment and information compression through the generation of graphs and leads to improved fact consistency. Finally, the generated fact-relationship graph of the target language summary serves as a draft for generating the summary, which enhances the quality of the generated summary. We conduct systematic experiments on the Zh2EnSum and En2ZhSum datasets, and the results demonstrate that our method can effectively improve the performance of CLS and alleviate factual inconsistency.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323006507",
    "keywords": [
      "Artificial intelligence",
      "Automatic summarization",
      "CLs upper limits",
      "Computer science",
      "Consistency (knowledge bases)",
      "Graph",
      "Image (mathematics)",
      "Information retrieval",
      "Language model",
      "Machine translation",
      "Medicine",
      "Natural language",
      "Natural language generation",
      "Natural language processing",
      "Optometry",
      "Semantic similarity",
      "Similarity (geometry)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Yongbing"
      },
      {
        "surname": "Gao",
        "given_name": "Shengxiang"
      },
      {
        "surname": "Huang",
        "given_name": "Yuxin"
      },
      {
        "surname": "Tan",
        "given_name": "Kaiwen"
      },
      {
        "surname": "Yu",
        "given_name": "Zhengtao"
      }
    ]
  },
  {
    "title": "Neural ordinary differential equation for irregular human motion prediction",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2023.12.016",
    "abstract": "Human motion prediction often assumes that the input sequence is of fixed frame rates. However, in real-world applications, the motion capture system may work unstably sometimes and miss some frames, which leads to inferior performance. To solve this problem, this paper leverages neural Ordinary Differential Equations and proposes a human Motion Prediction method named MP-ODE to handle irregular-time human pose series. First, a Difference Operator and a Positional Encoding are proposed to explicitly provide the kinematic and time information for the model. Second, we construct the encoder–decoder model with ODE-GRU unit, which enables us to learn continuous-time dynamics of human motion. Third, a Quaternion Loss transforms exponential maps to quaternion to train MP-ODE. The Quaternion Loss can avoid the discontinuities and singularities of exponential maps, boosting the convergence of the model. Comprehensive experiments on Human3.6 m and CMU-Mocap datasets demonstrate that the proposed MP-ODE achieves promising performance in both normal and irregular-time conditions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523003628",
    "keywords": [
      "Algorithm",
      "Applied mathematics",
      "Artificial intelligence",
      "Classical mechanics",
      "Computer science",
      "Differential equation",
      "Dual quaternion",
      "Geometry",
      "Kinematics",
      "Mathematical analysis",
      "Mathematics",
      "Motion (physics)",
      "Motion capture",
      "Ode",
      "Ordinary differential equation",
      "Physics",
      "Quaternion"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Yang"
      },
      {
        "surname": "Liu",
        "given_name": "Hong"
      },
      {
        "surname": "Song",
        "given_name": "Pinhao"
      },
      {
        "surname": "Li",
        "given_name": "Wenhao"
      }
    ]
  },
  {
    "title": "Pseudo-label estimation via unsupervised Identity Link Prediction for one-shot person Re-Identification",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110060",
    "abstract": "In this paper, we propose an unsupervised identity link prediction (ILP) method for label estimation in one-shot person Re-ID. ILP aims to relax the constraints of labeled samples and group a set of unlabeled pedestrians by their potential identities. The main idea is that the category relationships between pedestrians (nodes) can be inferred from their local context in the feature space. Specifically, an Identity Link Subgraph (ILS) describes the link relationship between nodes and their nearest neighbors, which is constructed by a two-step procedure. Meanwhile, a Dynamic Penalty Module (DPM) is introduced at each ILS construction step to infer which linkage between pairs in the ILS should be pruned to assign higher-quality classification labels. To fully use the accurate identity information in initial labeled samples, we jointly use identity pseudo-labels (which are estimated by adopting the Nearest Neighbors classifier) with classification pseudo-labels for model training. Moreover, we design a Dual-Branch Fusion network (DBF-Net) to optimize the CNN model simultaneously through all (pseudo-)labeled samples. Results on multiple datasets prove that DBF-Net outperforms traditional one-shot Re-ID methods by a large margin.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323007574",
    "keywords": [
      "Acoustics",
      "Art",
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Dual (grammatical number)",
      "Feature vector",
      "Identity (music)",
      "Literature",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Physics",
      "Programming language",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Yulin"
      },
      {
        "surname": "Ma",
        "given_name": "Bo"
      },
      {
        "surname": "Li",
        "given_name": "Meng"
      },
      {
        "surname": "Liu",
        "given_name": "Ying"
      },
      {
        "surname": "Chen",
        "given_name": "Feng"
      },
      {
        "surname": "Hou",
        "given_name": "Junyu"
      }
    ]
  },
  {
    "title": "A topic modeling and image classification framework: The Generalized Dirichlet variational autoencoder",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110037",
    "abstract": "Latent Dirichlet allocation model (LDA) has been widely used in topic modeling. Recent works have shown the effectiveness of integrating neural network mechanisms with this generative model for learning text representation. However, one of the significant setbacks of LDA is that it is based on a Dirichlet prior that has a restrictive covariance structure. All its variables are considered to be negatively correlated, which makes the model restrictive. In a practical sense, topics can be positively or negatively correlated. To address this problem, we proposed a generalized Dirichlet variational autoencoder (GD-VAE) for topic modeling. The Generalized Dirichlet (GD) distribution has a more general covariance structure than the Dirichlet distribution because it takes into account both positively and negatively correlated topics in the corpus. Our proposed model leverages rejection sampling variational inference using a reparameterization trick for effective training. GD-VAE compares favorably to recent works on topic models on several benchmark corpora. Experiments show that accounting for topics’ positive and negative correlations results in better performance. We further validate the superiority of our proposed framework on two image data sets. GD-VAE demonstrates its significance as an integral part of a classification architecture. For reproducibility and further research purposes, code for this work can be found at https://github.com/hormone03/GD-VAE.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323007343",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Autoencoder",
      "Benchmark (surveying)",
      "Boundary value problem",
      "Code (set theory)",
      "Computer science",
      "Covariance",
      "Dirichlet distribution",
      "Generative grammar",
      "Generative model",
      "Geodesy",
      "Geography",
      "Inference",
      "Latent Dirichlet allocation",
      "Latent variable",
      "Law",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Programming language",
      "Representation (politics)",
      "Set (abstract data type)",
      "Statistics",
      "Topic model"
    ],
    "authors": [
      {
        "surname": "Ojo",
        "given_name": "Akinlolu Oluwabusayo"
      },
      {
        "surname": "Bouguila",
        "given_name": "Nizar"
      }
    ]
  },
  {
    "title": "Recovering a clean background: A parallel deep network architecture for single-image deraining",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.01.006",
    "abstract": "Deep convolutional neural networks have been popularly applied in single-image deraining recently. Nevertheless, as the network becomes deeper, it is easy to cause training over-fitting and performance saturation, particularly in the case of insufficient training data. In this paper, we report the design of a new network, namely parallel deraining convolutional neural network (PARDNet), for single-image deraining. Specifically, PARDNet adopts two parallel residual sub-networks based on different receptive fields to extract more comprehensive characteristics of the rain streaks, as well as decrease the depth of the network. The hybrid dilated convolution is employed to enlarge the sub-network’s receptive field to capture more context information. The efficient channel attention module is integrated into the proposed PARDNet to capture rain streaks more effectively and preserve more background details. Furthermore, to facilitate the network training, the residual learning is also fused into PARDNet in a holistic way. Extensive experiments on synthetic and real-world rainy image datasets demonstrate the superiority of PARDNet for single-image deraining.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524000060",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Context (archaeology)",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Deep learning",
      "Field (mathematics)",
      "Geology",
      "Image (mathematics)",
      "Mathematics",
      "Network architecture",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Pure mathematics",
      "Residual"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Nanrun"
      },
      {
        "surname": "Deng",
        "given_name": "Jibin"
      },
      {
        "surname": "Pang",
        "given_name": "Meng"
      }
    ]
  },
  {
    "title": "Enhancing texture representation with deep tracing pattern encoding",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109959",
    "abstract": "Texture representation is a challenging problem due to the complex underlying physics of texture as well as the variations caused by changes in viewpoint. Recent progress in texture analysis has been made by the power of convolutional neural networks (CNNs) in feature learning. However, most current methods aggregate the features from the last convolutional layer of the CNN to obtain a global feature vector, which fails to leverage shallow low-level visual cues and cross-layer feature patterns, limiting their performance. In this paper, we propose to trace the features generated along the convolutional layers via a histogram of local 3D invariant binary patterns, called deep tracing patterns. This leads to a highly discriminative yet robust global feature representation module. Building such a module into a CNN backbone, we develop an effective approach for texture recognition. Extensive experiments on six benchmark datasets show that the proposed approach provides a discriminative and robust texture descriptor, with state-of-the-art performance achieved.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032300657X",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Discriminative model",
      "Feature (linguistics)",
      "Feature learning",
      "Geodesy",
      "Geography",
      "Histogram",
      "Image (mathematics)",
      "Leverage (statistics)",
      "Linguistics",
      "Local binary patterns",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Tracing"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Zhile"
      },
      {
        "surname": "Quan",
        "given_name": "Yuhui"
      },
      {
        "surname": "Xu",
        "given_name": "Ruotao"
      },
      {
        "surname": "Jin",
        "given_name": "Lianwen"
      },
      {
        "surname": "Xu",
        "given_name": "Yong"
      }
    ]
  },
  {
    "title": "Matrix randomized autoencoder",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109992",
    "abstract": "Randomized autoencoder (RAE) has attracted much attention due to its strong capability of representation with fast learning speed. However, the mainstream RAEs are still designed for scalar/vector data, which inevitably destroys the structure information of tensor data. To alleviate this deficiency, a novel convolutions based matrix randomized autoencoder (MRAE) is developed for two-dimensional (2D) data in this paper, including a one-side MRAE (OMRAE) exploiting the row or column information and a double-side MRAE (DMRAE) that simultaneously extracts the row and column information by 2 parallel OMRAEs. To reduce meaningless encoded features, the within-class scatter matrix (WSI) and within-class interaction distance (WID) constraints are added into OMRAE resulting WSI-OMRAE and WID-OMRAE, respectively. To demonstrate the superiority, stacked MRAEs are embedded into hierarchical regularized least squares for one-class classification and comparisons with several state-of-the-art methods are provided. The source code would be available at https://github.com/ML-HDU/MRAE.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323006908",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Autoencoder",
      "Class (philosophy)",
      "Code (set theory)",
      "Column (typography)",
      "Composite material",
      "Computer science",
      "Deep learning",
      "Frame (networking)",
      "Geometry",
      "Law",
      "Materials science",
      "Mathematics",
      "Matrix (chemical analysis)",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Programming language",
      "Pure mathematics",
      "Representation (politics)",
      "Scalar (mathematics)",
      "Set (abstract data type)",
      "Telecommunications",
      "Tensor (intrinsic definition)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Shichen"
      },
      {
        "surname": "Wang",
        "given_name": "Tianlei"
      },
      {
        "surname": "Cao",
        "given_name": "Jiuwen"
      },
      {
        "surname": "Zhang",
        "given_name": "Wandong"
      },
      {
        "surname": "Chen",
        "given_name": "Badong"
      }
    ]
  },
  {
    "title": "DataMap: Dataset transferability map for medical image classification",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110044",
    "abstract": "Deep learning (DL)-based models especially Convolutional Neural Network (CNN) models have recently achieved great success in medical image classifications. It is usually time-consuming and labor-intensive to train a practical classification model due to the requirement of large data volume. Since medical images are more difficult to acquire and label for model training, many scholars have applied transfer learning by pre-training a model on a larger dataset and then fine-tuning it on the target dataset to obtain better classification results. However, such approach, relying on individual expertise to select related datasets, is subjective and inconsistent in performance. In this paper, we propose a simple yet effective method for measuring the transferability between different datasets, and build a Dataset Map (DataMap) that can be used as a tool to find the most relevant datasets for transfer learning on target dataset. Recent studies show that the convolutional kernels in CNN models have different function roles. Therefore, we adopt the similarity between the convolution kernels to measure the transferability between datasets. Firstly, the gradient attribution is adopted to attribute the task related convolution kernels from last few convolution layers of the same pre-trained model architecture trained with different datasets. Then, the similarity between attributed convolutional kernels is calculated to denote the transferability between different datasets. Finally, we build a DataMap with 20 medical image datasets. Extensive experimental tests on 3 mainstream CNN architectures show that the proposed method can effectively measure the transferability between different datasets. With the guidance of the DataMap, the transfer learning can achieve the best performance on various training tasks, and the accuracy of the CNN classifier can be improved by 1% to 5% through pre-training.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323007410",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Contextual image classification",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Data mining",
      "Deep learning",
      "Image (mathematics)",
      "Logit",
      "Machine learning",
      "Measure (data warehouse)",
      "Pattern recognition (psychology)",
      "Similarity (geometry)",
      "Transfer of learning",
      "Transferability"
    ],
    "authors": [
      {
        "surname": "Du",
        "given_name": "Xiangtong"
      },
      {
        "surname": "Liu",
        "given_name": "Zhidong"
      },
      {
        "surname": "Feng",
        "given_name": "Zunlei"
      },
      {
        "surname": "Deng",
        "given_name": "Hai"
      }
    ]
  },
  {
    "title": "A multi-aspect framework for explainable sentiment analysis",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.01.001",
    "abstract": "The demand for explainable sentiment analysis has intensified, emphasizing the need for models that are both accurate and interpretable. This research introduces the Multi-Aspect Framework for Explainable Sentiment Analysis (MAFESA), a groundbreaking model that seamlessly integrates aspect extraction, sentiment prediction, and explainability. By harnessing the power of Latent Dirichlet Allocation (LDA) for aspect extraction and leveraging hierarchical neural networks for sentiment prediction, MAFESA achieves remarkable performance metrics. Notably, our framework outperforms state-of-the-art baseline models across benchmark datasets such as IMDB Movie Reviews, Amazon Product Reviews, and Twitter Sentiment Analysis. The inclusion of an explainability module, built around techniques like LIME, offers unparalleled insights into the model’s decision-making, ensuring predictions are transparent and trustworthy. Our performance evaluations, underscored by a thorough ablation study, cross-validation, and rigorous statistical tests, attest to MAFESA’s robustness, generalizability, and superiority. A detailed qualitative analysis further showcases the model’s adeptness at discerning aspect-level nuances and delivering clear explanations for sentiment predictions. This research not only sets a new benchmark in explainable sentiment analysis but also provides a holistic framework that balances prediction precision with interpretability.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524000011",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Data mining",
      "Data science",
      "Gene",
      "Generalizability theory",
      "Geodesy",
      "Geography",
      "Interpretability",
      "Latent Dirichlet allocation",
      "Machine learning",
      "Mathematics",
      "Robustness (evolution)",
      "Sentiment analysis",
      "Statistics",
      "Topic model"
    ],
    "authors": [
      {
        "surname": "V.",
        "given_name": "Jothi Prakash"
      },
      {
        "surname": "S.",
        "given_name": "Arul Antran Vijay"
      }
    ]
  },
  {
    "title": "Patterns of vehicle lights: Addressing complexities of camera-based vehicle light datasets and metrics",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.01.003",
    "abstract": "This paper explores the representation of vehicle lights in computer vision and its implications for various pattern recognition tasks in autonomous driving. Different representations for vehicle lights, including bounding boxes, center points, corner points, and segmentation masks, are discussed in terms of their strengths and weaknesses toward a variety of domain tasks, as well as associated data collection and annotation challenges. This leads to the introduction of the LISA Vehicle Lights Dataset, providing light annotations related to position, state, color, and signal, specifically designed for downstream applications in vehicle detection, intent and trajectory prediction, and safe path planning. A comparison of existing vehicle light datasets is provided, highlighting the unique features and limitations of each dataset. Because occlusions from vehicle pose and passing objects can limit camera observation, we introduce a group of Light Visibility neural networks, which take as input a detected vehicle image and return as output whether a corresponding vehicle light is present in the image. This is especially important for the evaluation of light localizations, states, and signals, as systems decisions should account for differences of lights being in unknown states due to occlusion versus model uncertainty. We show that our trained Light Visibility Models achieve over 90% accuracy on each of the four light classes. Our dataset and model are made available at https://cvrr.ucsd.edu/vehicle-lights-dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524000047",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Image (mathematics)",
      "Law",
      "Minimum bounding box",
      "Optics",
      "Physics",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Segmentation",
      "Visibility"
    ],
    "authors": [
      {
        "surname": "Greer",
        "given_name": "Ross"
      },
      {
        "surname": "Gopalkrishnan",
        "given_name": "Akshay"
      },
      {
        "surname": "Keskar",
        "given_name": "Maitrayee"
      },
      {
        "surname": "Trivedi",
        "given_name": "Mohan M."
      }
    ]
  },
  {
    "title": "L 0 regularized logistic regression for large-scale data",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110024",
    "abstract": "In this paper, we investigate L 0 -regularized logistic regression models, and design two fast and efficient algorithms for high-dimensional correlated data and massive data, respectively. Our first algorithm, the Variable Sorted Active Set (VSAS) algorithm, is based on the local quadratic approximation of the KKT conditions for L 0 -penalized maximum log-likelihood function in high-dimensional correlated data. We establish an L ∞ error upper bound for the estimator obtained by the VSAS algorithm and prove its optimal convergence rate. Moreover, when the target signal exceeds the detectable level, the estimator obtained by the VSAS algorithm can achieve the oracle estimator with high probability. Our second algorithm, Communication Effective Variable Sorted Active Set (CEVSAS), aims to solve high-dimensional and large-sample L 0 -regularized logistic regression models by reduce computational and communication costs, while maintaining estimation efficiency. Finally, simulations and real data demonstrate the effectiveness of our proposed VSAS and CEVSAS algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323007215",
    "keywords": [
      "Algorithm",
      "Computer science",
      "Estimator",
      "Geometry",
      "Karush–Kuhn–Tucker conditions",
      "Logistic regression",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Oracle",
      "Quadratic equation",
      "Software engineering",
      "Statistics",
      "Variable (mathematics)"
    ],
    "authors": [
      {
        "surname": "Ming",
        "given_name": "Hao"
      },
      {
        "surname": "Yang",
        "given_name": "Hu"
      }
    ]
  },
  {
    "title": "Dynamic gradient reactivation for backward compatible person re-identification",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110000",
    "abstract": "We study the backward compatible problem for person re-identification (Re-ID), which aims to constrain the features of an updated new model to be comparable with the existing features from the old model in galleries. Most of the existing works adopt distillation-based methods, which focus on pushing new features to imitate the distribution of the old ones. However, the distillation-based methods are intrinsically sub-optimal since it forces the new feature space to imitate the inferior old feature space. To address this issue, we propose the Ranking-based Backward Compatible Learning (RBCL), which directly optimizes the ranking metric between new features and old features. Different from previous methods, RBCL only pushes the new features to find best-ranking positions in the old feature space instead of strictly alignment, and is in line with the ultimate goal of backward retrieval. However, the sharp sigmoid function used to make the ranking metric differentiable also incurs the gradient vanish issue, therefore stems the ranking refinement during the later period of training. To address this issue, we propose the Dynamic Gradient Reactivation (DGR), which can reactivate the suppressed gradients by adding dynamically computed constants during the forward step. To further help target the best-ranking positions, we include the Neighbor Context Agents (NCAs) to approximate the entire old feature space during training. Unlike previous works that mainly test on the in-domain settings, we make the early attempt to introduce the cross-domain settings (including both supervised and unsupervised) for the backward compatible person Re-ID task, which are more challenging yet meaningful. The experimental results on all five settings show that the proposed RBCL outperforms previous state-of-the-art methods by large margins.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323006982",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Botany",
      "Computer science",
      "Context (archaeology)",
      "Differentiable function",
      "Domain (mathematical analysis)",
      "Economics",
      "Evolutionary biology",
      "Feature (linguistics)",
      "Feature vector",
      "Focus (optics)",
      "Function (biology)",
      "Identification (biology)",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Metric (unit)",
      "Operating system",
      "Operations management",
      "Optics",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Ranking (information retrieval)",
      "Sigmoid function",
      "Space (punctuation)"
    ],
    "authors": [
      {
        "surname": "Pan",
        "given_name": "Xiao"
      },
      {
        "surname": "Luo",
        "given_name": "Hao"
      },
      {
        "surname": "Chen",
        "given_name": "Weihua"
      },
      {
        "surname": "Wang",
        "given_name": "Fan"
      },
      {
        "surname": "Li",
        "given_name": "Hao"
      },
      {
        "surname": "Jiang",
        "given_name": "Wei"
      },
      {
        "surname": "Zhang",
        "given_name": "Jianming"
      },
      {
        "surname": "Gu",
        "given_name": "Jianyang"
      },
      {
        "surname": "Li",
        "given_name": "Peike"
      }
    ]
  },
  {
    "title": "Deep semi-supervised clustering based on pairwise constraints and sample similarity",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2023.12.010",
    "abstract": "Semi-supervised clustering methods enhance the performance of completely unsupervised clustering tasks by incorporating pairwise relationship information from a subset of samples. However, previous methods based on pairwise constraints have struggled to effectively leverage available prior knowledge and fully exploit the similarity relationships between samples. To address these issues, this paper proposes an advanced approach called Deep Semi-Supervised Clustering based on Pairwise Constraints and Sample Similarity (DSCPS). Specifically, DSCPS consists of two training stages: the coarse clustering stage and the fine clustering stage. In the coarse clustering stage, DSCPS assigns probabilities to each sample indicating its belongingness to each cluster by computing the distance to the cluster centers. In the fine clustering stage, DSCPS introduces a predictor network in the clustering space to predict the class labels of each sample. Simultaneously, the thresholds for similarity and dissimilarity between samples are determined based on pairwise constraints. Finally, the encoder and prediction network are further optimized using sample similarity relationships and pairwise constraints as loss conditions, resulting in the final clustering results. Extensive experiments demonstrate that DSCPS outperforms state-of-the-art methods, achieving the highest performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523003574",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Chromatography",
      "Cluster analysis",
      "Computer science",
      "Correlation clustering",
      "Data mining",
      "Image (mathematics)",
      "Leverage (statistics)",
      "Machine learning",
      "Pairwise comparison",
      "Pattern recognition (psychology)",
      "Sample (material)",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Qin",
        "given_name": "Xiao"
      },
      {
        "surname": "Yuan",
        "given_name": "Changan"
      },
      {
        "surname": "Jiang",
        "given_name": "Jianhui"
      },
      {
        "surname": "Chen",
        "given_name": "Long"
      }
    ]
  },
  {
    "title": "Finding hierarchy of clusters",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2023.12.009",
    "abstract": "In this work a novel hierarchical clustering technique is proposed which can be used to find a hierarchical structure of data items in a dataset. The emphasis is on maintaining consistency of the clusters at each level in the hierarchy as well as to have a natural number of clusters at each level. We propose a method by using Cluster Number Assisted k-Means (CNAK) to find the possible natural numbers of clusters for that dataset. Next, we find the association between clusters at different levels. We propose three criteria and use them to remove the insignificant levels to obtain a final hierarchical structure. This method can be used to find the number of clusters at each of the different levels, the clusters at those levels and their association with clusters at adjacent levels. Naïve Hierarchical k-means algorithm uses a pre-determined branching factor at all the levels. In most traditional agglomerative and divisive clustering, at each different level of the hierarchical structure, exactly one node is split into several nodes for the next lower level. Either this number is the same for each parent or decided independently. Clusters with multiple parents are not allowed in traditional hierarchical clustering. However, in reality a cluster may naturally have multiple parents. Experimentation have been carried out on multiple datasets to demonstrate the effectiveness of the proposed technique.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523003562",
    "keywords": [
      "Artificial intelligence",
      "CURE data clustering algorithm",
      "Cluster (spacecraft)",
      "Cluster analysis",
      "Complete-linkage clustering",
      "Computer science",
      "Consistency (knowledge bases)",
      "Control (management)",
      "Data mining",
      "Economics",
      "Fuzzy clustering",
      "Hierarchical clustering",
      "Hierarchical clustering of networks",
      "Hierarchical control system",
      "Hierarchical database model",
      "Hierarchy",
      "Market economy",
      "Programming language",
      "Single-linkage clustering"
    ],
    "authors": [
      {
        "surname": "Pal",
        "given_name": "Shankho Subhra"
      },
      {
        "surname": "Mukhopadhyay",
        "given_name": "Jayanta"
      },
      {
        "surname": "Sarkar",
        "given_name": "Sudeshna"
      }
    ]
  },
  {
    "title": "Deep image clustering with contrastive learning and multi-scale graph convolutional networks",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110065",
    "abstract": "Deep clustering has shown its promising capability in joint representation learning and clustering via deep neural networks. Despite the significant progress, the existing deep clustering works mostly utilize some distribution-based clustering loss, lacking the ability to unify representation learning and multi-scale structure learning. To address this, this paper presents a new deep clustering approach termed Image clustering with contrastive learning and multi-scale Graph Convolutional Networks (IcicleGCN), which bridges the gap between convolutional neural network (CNN) and graph convolutional network (GCN) as well as the gap between contrastive learning and multi-scale structure learning for the deep clustering task. Our framework consists of four main modules, namely, the CNN-based backbone, the Instance Similarity Module (ISM), the Joint Cluster Structure Learning and Instance reconstruction Module (JC-SLIM), and the Multi-scale GCN module (M-GCN). Specifically, the backbone network with two weight-sharing views is utilized to learn the representations for the two augmented samples (from each image). The learned representations are then fed to ISM and JC-SLIM for joint instance-level and cluster-level contrastive learning, respectively, during which an auto-encoder in JC-SLIM is also pretrained to serve as a bridge to the M-GCN module. Further, to enforce multi-scale neighborhood structure learning, two streams of GCNs and the auto-encoder are simultaneously trained via (i) the layer-wise interaction with representation fusion and (ii) the joint self-adaptive learning. Experiments on multiple image datasets demonstrate the superior clustering performance of IcicleGCN over the state-of-the-art. The code is available at https://github.com/xuyuankun631/IcicleGCN.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323007628",
    "keywords": [
      "Artificial intelligence",
      "Autoencoder",
      "Cluster analysis",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Encoder",
      "Feature learning",
      "Graph",
      "Machine learning",
      "Operating system",
      "Pattern recognition (psychology)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Yuankun"
      },
      {
        "surname": "Huang",
        "given_name": "Dong"
      },
      {
        "surname": "Wang",
        "given_name": "Chang-Dong"
      },
      {
        "surname": "Lai",
        "given_name": "Jian-Huang"
      }
    ]
  },
  {
    "title": "Coarse-to-fine online latent representations matching for one-stage domain adaptive semantic segmentation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110019",
    "abstract": "Domain adaptive semantic segmentation is meaningful since collecting numerous labeled samples in different domains is expensive and time-consuming. Recent domain adaptation methods yield not so efficient performance compared with supervised learning. With the hypothesis that semantic feature can be shared across domains, this paper proposes a coarse-to-fine online matching architecture (COM) for one-stage domain adaptation. We consider subsequent learning stages progressively refining the task in the latent feature space, i.e. the finer set at each component is hierarchically derived from the coarser set of the previous components, including cross-domain global prototypes, categories and instances matching and anchor-points contrastive learning, which further achieve self-supervised learning with region-level pseudo label generated only in a single training step. Beforehand, feature refinement are performed to realize edge perception and inter-feature augmentation. Then, coarse-to-fine network fuses global and local consistency matching via specific distribution alignment between the source and target domain. Finally, the adversarial structure controls the uncertainty of generator prediction through the maximization of classification results and minimization of two classifiers discrepancy. This proposed method is evaluated in two unsupervised domain adaptation tasks, i.e. GTA5 → Cityscapes and SYNTHIA → Cityscapes. Extensive experiments verify the effectiveness of our proposed COM model and demonstrate its superiority over several state-of-the-art approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323007161",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Domain (mathematical analysis)",
      "Feature (linguistics)",
      "Feature learning",
      "Generator (circuit theory)",
      "Linguistics",
      "Machine learning",
      "Margin (machine learning)",
      "Matching (statistics)",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Power (physics)",
      "Programming language",
      "Quantum mechanics",
      "Segmentation",
      "Set (abstract data type)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Dong",
        "given_name": "Zihao"
      },
      {
        "surname": "Niu",
        "given_name": "Sijie"
      },
      {
        "surname": "Gao",
        "given_name": "Xizhan"
      },
      {
        "surname": "Shao",
        "given_name": "Xiuli"
      }
    ]
  },
  {
    "title": "Multilevel depression status detection based on fine-grained prompt learning",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.01.005",
    "abstract": "As a common psychological disorder, depression is generally detected based on scales and interviews, which are often affected by subjective or environmental factors. In order to assist in the diagnosis, automatic depression detection (ADD) is developed to provide an objective, efficient, and convenient technique based on the analysis of different psychophysiological data. Among them, text is one of the most frequently used modalities in the research field of ADD. Despite its success, several key issues still need to be addressed. Specifically, binary detection performed in the classical ADD methods cannot satisfy the demand for early detection tasks. Therefore, a more fine-grained analysis of depression status is highly needed. Moreover, due to factors such as small dataset size and improper organization of interview texts by selecting response texts only or choosing topics subjectively, insufficient learnable representations for depression detection are inevitable. To tackle these issues, we propose a multilevel depression status detection approach based on fine-grained prompt learning, called MDSD-FGPL. In our method, we design multiple sets of prompts from coarse-grained to fine-grained and therefore train the language model with features extracted from different depressive levels. We also reorganize the interview text at the question-response level and utilize the attention mechanism to capture and fuse the key features. Specifically, two model branches are constructed based on pre-trained language models for extracting comprehensive semantic features and depression-specific semantic tendencies, respectively. Our method was evaluated on the DAIC-WOZ dataset, showing satisfying performance in fine-grained depression detection. Additionally, our method achieves a state-of-the-art F1-score of 0.8276 in the coarse-grained binary classification.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524000059",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Depression (economics)",
      "Economics",
      "Key (lock)",
      "Machine learning",
      "Macroeconomics",
      "Natural language processing"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Jun"
      },
      {
        "surname": "Guo",
        "given_name": "Yanrong"
      }
    ]
  },
  {
    "title": "Weighted side-window based gradient guided image filtering",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110006",
    "abstract": "Image filtering under guidance image, known as guided filtering (GF), has been successfully applied to a variety of applications. Existing GF methods utilize either conventional full window-based framework (FWF) or simple uniformly weighted aggregation strategy (UWA); thereby they suffer from edge-blurring. In this paper, based upon gradient guided filtering (GGF), a weighted side-window based gradient guided filtering (WSGGF) is proposed to address the aforementioned problem. First, both regression and adaptive regularization terms in GGF are improved upon eight side windows by introducing side window-based framework (SWF). L 1 norm is adopted to choose the results calculated in side windows. Second, UWA strategy in GGF is replaced by a refined variance-based weighted average (VWA) aggregation. In VWA, the value of each weight is chosen inversely proportional to the corresponding estimator. We show that with these improvements our method can well retain the edge sharpness and is robust to visual artifacts. To cut down the time consumption, a fast version of WSGGF (FWSGGF) is further proposed by incorporating a simple but effective down-sampling strategy, which is about four times faster while maintaining the superior performance. By comparing with the state-of-the-art (SOTA) methods on edge-aware smoothing, detail enhancement, high dynamic range image (HDR) compression, image luminance adjustment, depth map upsampling and single image haze removal, the effectiveness and flexibility of our proposed methods are verified. The source code is available at: https://github.com/weimin581/WSGGF",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323007033",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Dynamic range",
      "High dynamic range",
      "Image (mathematics)",
      "Smoothing",
      "Tone mapping",
      "Upsampling"
    ],
    "authors": [
      {
        "surname": "Yuan",
        "given_name": "Weimin"
      },
      {
        "surname": "Meng",
        "given_name": "Cai"
      },
      {
        "surname": "Bai",
        "given_name": "Xiangzhi"
      }
    ]
  },
  {
    "title": "Aggregated-attention deformable convolutional network for few-shot SAR jamming recognition",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109990",
    "abstract": "This work simultaneously addresses the challenges of unseen classes and low-data problems on synthetic aperture radar jamming recognition (SAR-JR). Currently, very few studies have tackled both challenges. Inspired by the success of few-shot learning, which learns a robust model from a few instances, we formulate SAR-JR as a few-shot task in a metric-learning framework to alleviate the above challenges. Against the jamming features with significant dispersion and complex geometric transformations, as well as feature obscuration in time–frequency images (TF), we propose an aggregated-attention deformable convolutional network (A 2 -DCNet) framework consisting of an aggregated-attention deformable convolutional module (A 2 -DC-Module) and a prototype classification module based on polynomial loss (PolyLoss-PC-Module). The former learns informative and refined embeddings from the TF images, while the latter performs the SAR-JR in an embedding space by calculating distances to prototypes of each class. Specifically, the modulated deformable convolution of the A 2 -DC-Module can capture long-range spatial contextual information from a global perspective, while the aggregated attention is designed to refine the representations of obscured features in the TF images. To further optimize the framework, we introduce a novel PolyLoss and customize the optimal form for our model to learn an embedding space with robust inter-class separability. Finally, to realize few-shot SAR-JR tasks, we simulate a novel dataset called JamSet . Extensive experimental results on our dataset have demonstrated substantial improvement of our proposed A 2 -DCNet method over the benchmarks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032300688X",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer vision",
      "Convolution (computer science)",
      "Economics",
      "Embedding",
      "Feature (linguistics)",
      "Linguistics",
      "Metric (unit)",
      "Operations management",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Synthetic aperture radar"
    ],
    "authors": [
      {
        "surname": "Du",
        "given_name": "Jinbiao"
      },
      {
        "surname": "Fan",
        "given_name": "Weiwei"
      },
      {
        "surname": "Gong",
        "given_name": "Chen"
      },
      {
        "surname": "Liu",
        "given_name": "Jun"
      },
      {
        "surname": "Zhou",
        "given_name": "Feng"
      }
    ]
  },
  {
    "title": "Deep feature extraction with tri-channel textual feature map for text classification",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2023.12.019",
    "abstract": "The complexity and diversity of texts make it difficult for shallow text classification models to capture deeper text features. Therefore, this paper takes advantage of the BiLSTM-CNN hybrid network based on the self-attention mechanism to extract text features, and constructs a new text feature representation similar to RGB-channel images, that is, tri-channel text feature maps. Drawing on the effectiveness of ResNet and SA-Net in deep networks, we designed a deep feature extraction network to capture deeper features in the text. We use the tri-channel text feature map as the input to the deep feature extraction network, and propose the Deeper Feature Text Classification (DFTC) end-to-end text classification model. Experiments prove that the DFTC model is competitive compared to the most advanced methods on five challenging text classification datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523003665",
    "keywords": [
      "Artificial intelligence",
      "Channel (broadcasting)",
      "Computer network",
      "Computer science",
      "Feature (linguistics)",
      "Feature extraction",
      "Law",
      "Linguistics",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "RGB color model",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Kunyan"
      },
      {
        "surname": "Kang",
        "given_name": "Chen"
      }
    ]
  },
  {
    "title": "CLUE: Contrastive language-guided learning for referring video object segmentation",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2023.12.017",
    "abstract": "Referring video object segmentation (R-VOS), the task of separating the object described by a natural language query from the video frames, has become increasingly critical with recent advances in multi-modal understanding. Existing approaches are mainly visual-dominant in both representation-learning and decision-making process, and are less sensitive to fine-grained clues in text description. To address this, we propose a language-guided contrastive learning and data augmentation framework to enhance the model sensitivity to the fine-grained textual clues (i.e., color, location, subject) in the text that relate heavily to the video information. By substituting key information of the original sentences and paraphrasing them with a text-based generation model, our approach conducts contrastive learning through automatically building diverse and fluent contrastive samples. We further enhance the multi-modal alignment with a sparse attention mechanism, which can find the most relevant video information by optimal transport. Experiments on a large-scale R-VOS benchmark show that our method significantly improves strong Transformer-based baselines, and further analysis demonstrates the better ability of our model in identifying textual semantics.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523003641",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Feature learning",
      "Geodesy",
      "Geography",
      "Natural language",
      "Natural language processing",
      "Object (grammar)",
      "Physics",
      "Programming language",
      "Quantum mechanics",
      "Segmentation",
      "Semantics (computer science)",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Gao",
        "given_name": "Qiqi"
      },
      {
        "surname": "Zhong",
        "given_name": "Wanjun"
      },
      {
        "surname": "Li",
        "given_name": "Jie"
      },
      {
        "surname": "Zhao",
        "given_name": "Tiejun"
      }
    ]
  },
  {
    "title": "Transformer-based visual object tracking via fine–coarse concatenated attention and cross concatenated MLP",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109964",
    "abstract": "Transformer-based trackers have demonstrated promising performance in visual object tracking tasks. Nevertheless, two drawbacks limited the potential performance improvement of transformer-based trackers. Firstly, the static receptive field of the tokens within one attention layer of the original self-attention learning neglects the multi-scale nature in the object tracking task. Secondly, the learning procedure of the multi-layer perception (MLP) in the feed forward network (FFN) is lack of local interaction information among samples. To address the above issues, a new self-attention learning method, fine–coarse concatenated attention (FCA), is proposed to learn self-attention with fine and coarse granularity information. Moreover, the cross-concatenation MLP (CC-MLP) is developed to capture local interaction information across samples. Based on the two proposed modules, a novel encoder and decoder are constructed, and augmented in an all-attention tracking algorithm, FCAT. Comprehensive experiments on popular tracking datasets, OTB2015, LaSOT, GOT-10K and TrackingNet, reveal the effectiveness of FCA and CC-MLP, and FCAT achieves the state-of-art on the datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323006623",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Concatenation (mathematics)",
      "Encoder",
      "Eye tracking",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Gao",
        "given_name": "Long"
      },
      {
        "surname": "Chen",
        "given_name": "Langkun"
      },
      {
        "surname": "Liu",
        "given_name": "Pan"
      },
      {
        "surname": "Jiang",
        "given_name": "Yan"
      },
      {
        "surname": "Li",
        "given_name": "Yunsong"
      },
      {
        "surname": "Ning",
        "given_name": "Jifeng"
      }
    ]
  },
  {
    "title": "Feature incremental learning with causality",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110033",
    "abstract": "With the emerging of new data collection ways, the features are incremental and accumulated gradually. Due to the expansion of feature spaces, it is more common that there are unknown biases between the distribution of training and testing datasets. It is known as the unknown data selection bias, which belongs to the learning scenario with non-i.i.d samples. The performance of traditional approaches, which need the i.i.d. assumption, will be aggravated seriously. How to design an algorithm to address the problem of data selection bias in this feature incremental scenario is crucial but rarely studied. In this paper, we propose a feature incremental classification algorithm with causality. Firstly, we embed the confounding variable balance algorithm in causal learning into the prediction modeling and utilize the logical regression algorithm with balancing regular terms as a baseline. Then, to satisfy the special requirement of feature increment, we design a new regularizer, which maintains the consistency of the regression coefficients between the data in the current and previous stages. It retains the correlation between the old features and labels. Finally, we propose the Multiple Balancing Logistic Regression model (MBRLR) to jointly optimize the balancing regularizer and weighted logistic regression model with multiple feature sets. We also present theoretical results to show that our proposed algorithm can make precise and stable predictions. Besides, the numerical results also demonstrate that our MBRLR algorithm is superior to other methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323007306",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Causality (physics)",
      "Computer science",
      "Consistency (knowledge bases)",
      "Data mining",
      "Feature (linguistics)",
      "Feature selection",
      "Linguistics",
      "Logistic regression",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Regression",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Ni",
        "given_name": "Haotian"
      },
      {
        "surname": "Gu",
        "given_name": "Shilin"
      },
      {
        "surname": "Fan",
        "given_name": "Ruidong"
      },
      {
        "surname": "Hou",
        "given_name": "Chenping"
      }
    ]
  },
  {
    "title": "High-order relational generative adversarial network for video super-resolution",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110059",
    "abstract": "Video super-resolution can reconstruct a sequence of high-resolution frames with temporally consistent contents from their corresponding low-resolution sequences. The key challenge for this task is how to effectively utilize both inter-frame temporal relations and intra-frame spatial relations. The existing methods for super-resolving the videos commonly estimate optical flows to align the features of multiple frames based on temporal correlations. However, motion estimation is often error-prone and hence largely hinders the recovery of plausible details. Moreover, high-order contextual dependencies in the feature space are rarely exploited for further enhancing the spatio-temporal information fusion. To this end, we propose a novel generative adversarial network to super-resolve low-resolution videos, which makes full use of patch embeddings and is effective in exploring high-order spatio-temporal relations of the feature patches. Specifically, a motion-aware relation module is designed to handle the alignment between neighboring frames and reference ones. Depending on a patch-matching strategy for adaptive selection of multiple most similar patches, the cross-scale graph is constructed to reliably aggregate these patches using a feature pyramid. Based on the structure of multi-scale graph, a context-aware relation module is developed to capture high-order dependencies among resulting warped patches for better leveraging long-range complementary contexts. To further enhance reconstruction ability, the temporal position information of video sequences is also encoded into this module. Dual discriminators with cycle consistent constraints are adopted to provide more informative feedback to the generator while maintaining the global coherence. Extensive experiments have demonstrated the effectiveness of the proposed method in terms of quantitative and qualitative evaluation metrics.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323007562",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Coherence (philosophical gambling strategy)",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Feature (linguistics)",
      "Geometry",
      "Graph",
      "Linguistics",
      "Mathematics",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pyramid (geometry)",
      "Statistics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Rui"
      },
      {
        "surname": "Mu",
        "given_name": "Yang"
      },
      {
        "surname": "Zhang",
        "given_name": "Yan"
      }
    ]
  },
  {
    "title": "Temporal segment dropout for human action video recognition",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109985",
    "abstract": "Temporal information is important for human action video recognition. With the widely used spatio-temporal neural networks, researchers have found that the learned high-level features preserve overfitted spatial information and limited temporal information, leading to inferior performance. This is because existing networks lack efficient regularization for the temporal structure. To learn more robust temporal features, we propose a temporal regularization method named Temporal Segment Dropout (TSD). TSD drops the most salient spatial features in order to enhance the temporal features in a clip of temporal segments. Without learning from complex examples, TSD can be easily deployed in existing networks. In the experiment, TSD is extensively evaluated on benchmark action recognition datasets, which brings consistent improvements over the baselines, especially for the action-centric classes.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323006830",
    "keywords": [
      "Action recognition",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Cartography",
      "Class (philosophy)",
      "Computer science",
      "Data mining",
      "Dropout (neural networks)",
      "Geography",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Regularization (linguistics)",
      "Salient",
      "Temporal database"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Yu"
      },
      {
        "surname": "Chen",
        "given_name": "Zhengjie"
      },
      {
        "surname": "Xu",
        "given_name": "Tianyu"
      },
      {
        "surname": "Zhao",
        "given_name": "Junjie"
      },
      {
        "surname": "Mi",
        "given_name": "Siya"
      },
      {
        "surname": "Geng",
        "given_name": "Xin"
      },
      {
        "surname": "Zhang",
        "given_name": "Min-Ling"
      }
    ]
  },
  {
    "title": "Monte Carlo DropBlock for modeling uncertainty in object detection",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110003",
    "abstract": "With the advancements made in deep learning, computer vision problems have seen a great improvement in performance. However, in many real-world applications such as autonomous driving vehicles, the risk associated with incorrect predictions of objects or segmentation of images is very high. Standard deep learning models for object detection and segmentation such as YOLO models are often overconfident in their predictions and do not take into account the uncertainty in predictions on out-of-distribution data. In this work, we propose an efficient and effective approach, Monte-Carlo DropBlock (MC-DropBlock), to model uncertainty in YOLO and convolutional vision Transformers for object detection. The proposed approach applies drop-block during training time and testing time on the convolutional layer of the deep learning models such as YOLO and convolutional transformer. We theoretically show that this leads to a Bayesian convolutional neural network capable of capturing the epistemic uncertainty in the model. Additionally, we capture the aleatoric uncertainty in the data using a Gaussian likelihood. We demonstrate the effectiveness of the proposed approach on modeling uncertainty in object detection and segmentation tasks using out-of-distribution experiments. Experimental results show that MC-DropBlock improves the generalization, calibration, and uncertainty modeling capabilities of YOLO models and convolutional Transformer models for object detection and segmentation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032300701X",
    "keywords": [
      "Artificial intelligence",
      "Bayesian inference",
      "Bayesian probability",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Electrical engineering",
      "Engineering",
      "Machine learning",
      "Mathematics",
      "Monte Carlo method",
      "Object detection",
      "Pattern recognition (psychology)",
      "Segmentation",
      "Statistics",
      "Transformer",
      "Uncertainty quantification",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Yelleni",
        "given_name": "Sai Harsha"
      },
      {
        "surname": "Kumari",
        "given_name": "Deepshikha"
      },
      {
        "surname": "P.K.",
        "given_name": "Srijith"
      },
      {
        "surname": "C.",
        "given_name": "Krishna Mohan"
      }
    ]
  },
  {
    "title": "Learning disentangled representations for controllable human motion prediction",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109998",
    "abstract": "Generative model-based motion prediction techniques have recently realized predicting controlled human motions, such as predicting multiple upper human body motions with similar lower-body motions. However, to achieve this, the state-of-the-art methods require either subsequently learning mapping functions to seek similar motions or training the model repetitively to enable control over the desired portion of body. In this paper, we propose a novel framework to learn disentangled representations for controllable human motion prediction. Our task is to predict multiple future human motions based on the past observed sequence, with the control of partial-body movements. Our network involves a conditional variational auto-encoder (CVAE) architecture to model full-body human motion, and an extra CVAE path to learn only the corresponding partial-body (e.g., lower-body) motion. Specifically, the inductive bias imposed by the extra CVAE path encourages two latent variables in two paths to respectively govern separate representations for each partial-body motion. With a single training, our model is able to provide two types of controls for the generated human motions: (i) strictly controlling one portion of human body and (ii) adaptively controlling the other portion, by sampling from a pair of latent spaces. Additionally, we extend and adapt a sampling strategy to our trained model to diversify the controllable predictions. Our framework also potentially allows new forms of control by flexibly customizing the input for the extra CVAE path. Extensive experimental results and ablation studies demonstrate that our approach is capable of predicting state-of-the-art controllable human motions both qualitatively and quantitatively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323006969",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer vision",
      "Filter (signal processing)",
      "Generative grammar",
      "Generative model",
      "Genetics",
      "Machine learning",
      "Motion (physics)",
      "Motion capture",
      "Path (computing)",
      "Programming language",
      "Sampling (signal processing)",
      "Sequence (biology)"
    ],
    "authors": [
      {
        "surname": "Gu",
        "given_name": "Chunzhi"
      },
      {
        "surname": "Yu",
        "given_name": "Jun"
      },
      {
        "surname": "Zhang",
        "given_name": "Chao"
      }
    ]
  },
  {
    "title": "Low-light image enhancement using gamma correction prior in mixed color spaces",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110001",
    "abstract": "In this paper, we propose an efficient and fast low-light image enhancement method using an atmospheric scattering model based on an inverted low-light image. The transmission map is derived as a function of two saturations of the original image in the two color spaces. Due to the difficulty in estimating the saturation of the original image, the transmission map is converted into a function of the average and maximum values of the original image. These two values are estimated from a given low-light image using the gamma correction prior. In addition, a pixel-adaptive gamma value determination algorithm is proposed to prevent under- or over-enhancement. The proposed algorithm is fast because it does not require the training or refinement process. The simulation results show that the proposed low-light enhancement scheme outperforms state-of-the-art approaches regarding both computational simplicity and enhancement efficiency. The code is available on https://github.com/TripleJ2543.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323006994",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Color image",
      "Computer science",
      "Computer vision",
      "Gamma correction",
      "Image (mathematics)",
      "Image enhancement",
      "Image processing",
      "Mathematics",
      "Operating system",
      "Physics",
      "Pixel",
      "Process (computing)",
      "Quantum mechanics",
      "Simplicity",
      "Telecommunications",
      "Transmission (telecommunications)"
    ],
    "authors": [
      {
        "surname": "Jeon",
        "given_name": "Jong Ju"
      },
      {
        "surname": "Park",
        "given_name": "Jun Young"
      },
      {
        "surname": "Eom",
        "given_name": "Il Kyu"
      }
    ]
  },
  {
    "title": "Reducing redundancy in the bottleneck representation of autoencoders",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.01.013",
    "abstract": "Autoencoders (AEs) are a type of unsupervised neural networks, which can be used to solve various tasks, e.g., dimensionality reduction, image compression, and image denoising. An AE has two goals: (i) compress the original input to a low-dimensional space at the bottleneck of the network topology using an encoder, (ii) reconstruct the input from the representation at the bottleneck using a decoder. Both encoder and decoder are optimized jointly by minimizing a distortion-based loss which implicitly forces the model to keep only the information in input data required to reconstruct them and to reduce redundancies. In this paper, we propose a scheme to explicitly penalize feature redundancies in the bottleneck representation. To this end, we propose an additional loss term, based on the pairwise covariances of the network units, which complements the data reconstruction loss forcing the encoder to learn a more diverse and richer representation of the input. We tested our approach across different tasks, namely dimensionality reduction, image compression, and image denoising. Experimental results show that the proposed loss leads consistently to superior performance compared to using the standard AE loss.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524000126",
    "keywords": [
      "Algorithm",
      "Amplifier",
      "Artificial intelligence",
      "Artificial neural network",
      "Bandwidth (computing)",
      "Bottleneck",
      "Cluster analysis",
      "Computer network",
      "Computer science",
      "Curse of dimensionality",
      "Data compression",
      "Dimensionality reduction",
      "Distortion (music)",
      "Embedded system",
      "Encoder",
      "Image (mathematics)",
      "Image compression",
      "Image processing",
      "Information bottleneck method",
      "Law",
      "Operating system",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Redundancy (engineering)",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Laakom",
        "given_name": "Firas"
      },
      {
        "surname": "Raitoharju",
        "given_name": "Jenni"
      },
      {
        "surname": "Iosifidis",
        "given_name": "Alexandros"
      },
      {
        "surname": "Gabbouj",
        "given_name": "Moncef"
      }
    ]
  },
  {
    "title": "Objformer: Boosting 3D object detection via instance-wise interaction",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110061",
    "abstract": "Deep learning on point clouds drives 3D object detection. Despite rapid progress, point-based methods still suffer from the problems such as incompletion and occlusion, which are caused by the material properties of objects and cluttered scenes. These difficult targets increase the difficulty of identification or even lead to misidentification, severely weakening the performance of point-based methods on 3D object detection. To alleviate the above problems, we propose the Objformer to boost point-based 3D object detection via instance-wise interaction. We design an instance feature encoder to encode clean instance features, which contain key geometric priors and holistic semantic information. Further, an instance interaction module is devised to aggregate the complementary features across instances with label-guided interaction, boosting the performance of the 3D object detection. Experiments show that Objformer outperforms previous point-based state-of-the-arts on two popular benchmarks, ScanNet V2 and SUN RGB-D. Especially, our single-modal Objformer even outperforms the competing advanced multi-modal fusion method on both SUN RGB-D and ScanNet V2.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323007586",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Boosting (machine learning)",
      "Chemistry",
      "Cognitive neuroscience of visual object recognition",
      "Computer science",
      "Computer vision",
      "ENCODE",
      "Gene",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Point cloud",
      "RGB color model"
    ],
    "authors": [
      {
        "surname": "Tao",
        "given_name": "Manli"
      },
      {
        "surname": "Zhao",
        "given_name": "Chaoyang"
      },
      {
        "surname": "Tang",
        "given_name": "Ming"
      },
      {
        "surname": "Wang",
        "given_name": "Jinqiao"
      }
    ]
  },
  {
    "title": "Updatable Siamese tracker with two-stage one-shot learning",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109965",
    "abstract": "Offline-trained Siamese networks have realized very promising tracking precision and efficiency. However, the performance is still limited by the drawbacks in online update. Traditional strategies cannot tackle the irregular variations of object and the sampling noise, so it is quite risky to adopt them to update Siamese trackers. In this paper, we present a two-stage one-shot learner by exploring the learning scheme of Siamese network, which reveals there are two key issues during online update, i.e., feature fusion and feature comparison. Based on this finding, we propose an updatable Siamese tracker by introducing two independent transformers (SiamTOL). Concretely, a Cross-aware transformer is designed to combine the features of the initial and the dynamic templates, while a Decoder-favored transformer is exploited to compare the fusing template and the search region. By combining these transformers, our tracker is able to adequately model the feature dependencies between multi-frame object samples. Extensive experimental results on several popular benchmarks well manifest that the proposed approach achieves the leading performance, and outperforms other state-of-the-art trackers.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323006635",
    "keywords": [
      "Artificial intelligence",
      "BitTorrent tracker",
      "Computer science",
      "Computer vision",
      "Electrical engineering",
      "Engineering",
      "Eye tracking",
      "Pattern recognition (psychology)",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Sun",
        "given_name": "Xinglong"
      },
      {
        "surname": "Sun",
        "given_name": "Haijiang"
      },
      {
        "surname": "Li",
        "given_name": "Jianan"
      }
    ]
  },
  {
    "title": "Enhancing deep feature representation in self-knowledge distillation via pyramid feature refinement",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2023.12.014",
    "abstract": "In recent years, various self-knowledge distillation approaches have been proposed to reduce the cost of training teacher networks. However, these methods often overlook the significance of deep features. To address this limitation and strengthen the capability of deep features while preserving the ability of shallow features, we propose performing Self-Knowledge Distillation via Pyramid Feature Refinement (PR-SKD). Inspired by the representation learning characteristics of deep neural networks, PR-SKD builds a cohort of sub-networks with a pyramid architecture to hierarchically transfer refined information to the target network. According to the different contributions and functions between deep and shallow feature maps, our PR-SKD fully utilizes feature information to improve deep feature representation ability without compromising the capability of shallow feature maps. Extensive experiments on various image classification datasets demonstrate the superiority of our proposed method over widely used state-of-the-art knowledge distillation methods. The code is available at: https://github.com/wo16pao/PR-SKD.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523003616",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Chemistry",
      "Computer science",
      "Data mining",
      "Deep learning",
      "Distillation",
      "Feature (linguistics)",
      "Feature engineering",
      "Feature extraction",
      "Feature learning",
      "Geometry",
      "Law",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Pyramid (geometry)",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Yu",
        "given_name": "Hao"
      },
      {
        "surname": "Feng",
        "given_name": "Xin"
      },
      {
        "surname": "Wang",
        "given_name": "Yunlong"
      }
    ]
  },
  {
    "title": "Learnable Chamfer Distance for point cloud reconstruction",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2023.12.015",
    "abstract": "As point clouds are 3D signals with permutation invariance, most existing works train their reconstruction networks by measuring shape differences with the average point-to-point distance between point clouds matched with predefined rules. However, the static matching rules may deviate from actual shape differences. Although some works propose dynamically-updated learnable structures to replace matching rules, they need more iterations to converge well. In this work, we propose a simple but effective reconstruction loss, named Learnable Chamfer Distance (LCD) by dynamically paying attention to matching distances with different weight distributions controlled with a group of learnable networks. By training with adversarial strategy, LCD learns to search defects in reconstructed results and overcomes the weaknesses of static matching rules, while the performances at low iterations can also be guaranteed by the basic matching algorithm. Experiments on multiple reconstruction networks confirm that LCD can help achieve better reconstruction performances and extract more representative representations with faster convergence and comparable training efficiency.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552300363X",
    "keywords": [
      "Acoustics",
      "Algorithm",
      "Artificial intelligence",
      "Chamfer (geometry)",
      "Computer science",
      "Computer vision",
      "Convergence (economics)",
      "Economic growth",
      "Economics",
      "Epistemology",
      "Geometry",
      "Matching (statistics)",
      "Mathematics",
      "Permutation (music)",
      "Philosophy",
      "Physics",
      "Point (geometry)",
      "Point cloud",
      "Simple (philosophy)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Tianxin"
      },
      {
        "surname": "Liu",
        "given_name": "Qingyao"
      },
      {
        "surname": "Zhao",
        "given_name": "Xiangrui"
      },
      {
        "surname": "Chen",
        "given_name": "Jun"
      },
      {
        "surname": "Liu",
        "given_name": "Yong"
      }
    ]
  },
  {
    "title": "Micro-expression spotting with a novel wavelet convolution magnification network in long videos",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.01.004",
    "abstract": "Facial Micro-Expressions (MEs) are transient and spontaneous, reflecting a person's authentic internal emotions and have more significant value in many fields. Due to the presence of many background disturbances, including irrelevant motion (such as blinks and head movements) and noise in long videos, it is challenging to spot subtle MEs from these disturbances. To spot subtle MEs, a novel Wavelet Convolution Magnification Network (WCMN) with optical flow feature enhancement for spotting facial micro-expressions in long videos is proposed, which has a U-Net-like architecture and consists of discrete wavelet transform networks and an attention magnification mechanism. It can effectively suppress background disturbances and magnify the optical flow features of MEs, making them easy to detect. Experiments are conducted on two long video datasets (CAS(ME)2 and SAMM Long Videos). The results show that our WCMN outperforms the state-of-the-art ME spotting methods, achieving overall F1-scores of 0.3791 on CAS(ME)2, and 0.3272 on SAMM Long Videos, respectively. We also provide visualization and analysis to explain what the network enhances, and to demonstrate the effectiveness of our WCMN through ablation studies.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524000035",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer vision",
      "Convolution (computer science)",
      "Feature (linguistics)",
      "Image (mathematics)",
      "Linguistics",
      "Magnification",
      "Noise (video)",
      "Optical flow",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Spotting",
      "Visualization",
      "Wavelet",
      "Wavelet transform"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Jianxiong"
      },
      {
        "surname": "Wu",
        "given_name": "Ying"
      }
    ]
  },
  {
    "title": "TU 2 Net-GAN: A temporal precipitation nowcasting model with multiple decoding modules",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2023.12.025",
    "abstract": "With the Earth’s temperature rising and abnormal weather events becoming frequent, the mechanisms of precipitation formation have become increasingly complex, leading to more significant spatiotemporal variability. This increased variability often results in severe flooding events. Despite extensive research on deep learning methods for rainfall prediction, challenges such as forecasting uncertainty and inaccurate predictions persist. Therefore, this study addresses these issues by proposing temporal U 2 Net (TU 2 Net) within the GAN framework. TU 2 Net comprises a nested UNet with two layers designed to handle time series data. The model also connects multiple Conv-GRU decoding modules to enhance spatiotemporal performance and improve prediction quality using an improved loss function. Experimental results demonstrate that TU 2 Net with the improved loss function outperforms DGMR and UNet in terms of prediction accuracy. Our project is open source and available on GitHub at https://github.com/clearlyzerolxd/TU2Net.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552300380X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Data mining",
      "Evolutionary biology",
      "Flooding (psychology)",
      "Function (biology)",
      "Machine learning",
      "Meteorology",
      "Physics",
      "Precipitation",
      "Psychology",
      "Psychotherapist",
      "Scalable Vector Graphics",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Ling",
        "given_name": "XuDong"
      },
      {
        "surname": "Li",
        "given_name": "ChaoRong"
      },
      {
        "surname": "Yang",
        "given_name": "Peng"
      },
      {
        "surname": "Huang",
        "given_name": "Yuanyuan"
      },
      {
        "surname": "Qin",
        "given_name": "Fengqing"
      }
    ]
  },
  {
    "title": "Conditional feature generation for transductive open-set recognition via dual-space consistent sampling",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110046",
    "abstract": "Open-set recognition (OSR) aims to simultaneously detect unknown-class samples and classify known-class samples. Most of the existing OSR methods are inductive methods, which generally suffer from the domain shift problem that the learned model from the known-class domain might be unsuitable for the unknown-class domain. Addressing this problem, inspired by the success of transductive learning for alleviating the domain shift problem in many other visual tasks, we propose an Iterative Transductive OSR framework, called IT-OSR, which implements three explored modules iteratively, including a reliability sampling module, a feature generation module, and a baseline update module. Specifically at the initialization stage, a baseline method, which could be an arbitrary inductive OSR method, is used for assigning pseudo labels to the test samples. At the iteration stage, based on the consistency of the assigned pseudo labels between the output/logit space and the latent feature space of the baseline method, a dual-space consistent sampling approach is presented in the reliability sampling module for sampling some reliable ones from the test samples. Then in the feature generation module, a conditional dual-adversarial generative network is designed to generate discriminative features of both known and unknown classes. This generative network employs two discriminators for implementing fake/real and known/unknown-class discriminations respectively. And it is trained by jointly utilizing the test samples with their pseudo labels selected in the reliability sampling module and the labeled training samples. Finally in the baseline update module, the above baseline method is updated/re-trained for sample re-prediction by jointly utilizing the generated features, the selected test samples with pseudo labels, and the training samples. Extensive experimental results on both the standard-dataset and the cross-dataset settings demonstrate that the derived transductive methods, by introducing two typical inductive OSR methods into the proposed IT-OSR framework, achieve better performances than 19 state-of-the-art methods in most cases.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323007434",
    "keywords": [
      "Artificial intelligence",
      "Baseline (sea)",
      "Class (philosophy)",
      "Computer science",
      "Computer vision",
      "Discriminative model",
      "Feature (linguistics)",
      "Feature vector",
      "Filter (signal processing)",
      "Geology",
      "Initialization",
      "Linguistics",
      "Machine learning",
      "Oceanography",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Sampling (signal processing)",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Sun",
        "given_name": "Jiayin"
      },
      {
        "surname": "Dong",
        "given_name": "Qiulei"
      }
    ]
  },
  {
    "title": "Stochastic first-order learning for large-scale flexibly tied Gaussian mixture models",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2023.12.021",
    "abstract": "Gaussian Mixture Models (GMMs) are one of the most potent parametric density models used extensively in many applications. Flexibly-tied factorization of the covariance matrices in GMMs is a powerful approach for coping with the challenges of common GMMs when faced with high-dimensional data and complex densities which often demand a large number of Gaussian components. However, the expectation–maximization algorithm for fitting flexibly-tied GMMs still encounters difficulties with streaming and very large dimensional data. To overcome these challenges, this paper suggests the use of first-order stochastic optimization algorithms. Specifically, we propose a new stochastic optimization algorithm on the manifold of orthogonal matrices. Through numerous empirical results on both synthetic and real datasets, we observe that stochastic optimization methods can outperform the expectation–maximization algorithm in terms of attaining better likelihood, needing fewer epochs for convergence, and consuming less time per each epoch.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523003744",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Convergence (economics)",
      "Covariance",
      "Economic growth",
      "Economics",
      "Eigenvalues and eigenvectors",
      "Expectation–maximization algorithm",
      "Gaussian",
      "Gaussian process",
      "Mathematical optimization",
      "Mathematics",
      "Matrix decomposition",
      "Maximization",
      "Maximum likelihood",
      "Mixture model",
      "Non-negative matrix factorization",
      "Physics",
      "Quantum mechanics",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Pasande",
        "given_name": "Mohammad"
      },
      {
        "surname": "Hosseini",
        "given_name": "Reshad"
      },
      {
        "surname": "Nadjar Araabi",
        "given_name": "Babak"
      }
    ]
  },
  {
    "title": "MF-Net: Multi-frequency intrusion detection network for Internet traffic data",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109999",
    "abstract": "The rapid growth of Internet technology renders intrusion detection an important research topic in the field of pattern recognition. Considering that traffic data relate to not only temporal information, but also attack frequency, this paper presents a novel deep learning framework termed the multi-frequency intrusion detection network (MF-Net). MF-Net regards the pattern of Internet traffic as a superposition of sequential data with various frequencies, and is able to recognize the multi-frequency nature of network traffic data. The core of MF-Net is the multi-frequency LSTM (MF-LSTM) and multi-frequency transformer (MF-Transformer) module, both of which consist of high-frequency and low-frequency layers. In comparison with other state-of-the-art approaches on 4 public datasets, namely UNSW-NB15, KDD Cup 99, NSL-KDD and CICIDS 2017, as well as an IPv6 traffic dataset we created, MF-Net has shown better result in both binary and multi-class classification, which demonstrates the superiority of MF-Net over other compared approaches on network traffic intrusion detection.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323006970",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Electrical engineering",
      "Engineering",
      "Frequency domain",
      "Geometry",
      "Internet traffic",
      "Intrusion detection system",
      "Machine learning",
      "Mathematics",
      "Net (polyhedron)",
      "Pattern recognition (psychology)",
      "The Internet",
      "Transformer",
      "Voltage",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Ding",
        "given_name": "Zhaoxu"
      },
      {
        "surname": "Zhong",
        "given_name": "Guoqiang"
      },
      {
        "surname": "Qin",
        "given_name": "Xianping"
      },
      {
        "surname": "Li",
        "given_name": "Qingyang"
      },
      {
        "surname": "Fan",
        "given_name": "Zhenlin"
      },
      {
        "surname": "Deng",
        "given_name": "Zhaoyang"
      },
      {
        "surname": "Ling",
        "given_name": "Xiao"
      },
      {
        "surname": "Xiang",
        "given_name": "Wei"
      }
    ]
  },
  {
    "title": "On the bias in the AUC variance estimate",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2023.12.012",
    "abstract": "The area under the Receiver Operating Characteristic (ROC) curve (AUC) is a standard metric for quantifying and comparing binary classifiers. A popular approach to estimating the AUCs and the associated variabilities – the variance of the AUC or the full covariance matrix of multiple correlated AUCs – is the one proposed by DeLong et al. (1988), which is based on the Mann Whitney two-sample U-statistics. The bias of a variance estimator is an important factor in applications such as hypothesis testing and construction of confidence intervals – a negatively biased variance estimator may lead to incorrect conclusions, and a positive bias is conservative hence preferable. In this work, we show that the (co-)variance estimate in DeLong’s approach is always positively biased. More specifically, the difference matrix between the expectation of the estimated covariance and the true covariance is a positive semi-definite matrix. This bias is non-negligible when the sample size is small, and quickly diminishes as the sample size increases. Our method relies on constructing, from the AUC kernel, a random variable whose (co-)variance matrix coincides with the bias, thereby establishing the claim. We also discuss alternative approaches to AUC variance estimation that may potentially reduce the bias.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523003598",
    "keywords": [
      "Accounting",
      "Bias of an estimator",
      "Business",
      "Combinatorics",
      "Confidence interval",
      "Covariance",
      "Covariance matrix",
      "Economics",
      "Estimator",
      "Kernel (algebra)",
      "Mathematics",
      "Metric (unit)",
      "Minimum-variance unbiased estimator",
      "Operations management",
      "Sample size determination",
      "Statistics",
      "Variance (accounting)"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Jingyan"
      }
    ]
  },
  {
    "title": "Token labeling-guided multi-scale medical image classification",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2023.12.018",
    "abstract": "Vision transformer has been widely used in medical image analysis. However, in most of current methods, only the class token is concerned during training, while the output patch tokens’ information is not well utilized. To track this problem, we propose a two-stage token labeling guided multi-scale model for medical image classification. In the first stage, we pre-train a classification model to extract critical areas as token labeling. In the second stage, we adopt coarse and fine branches to encode visual features, which adapts to the various lesions in medical images. Then, the class token output by each branch is fused for classification. The token labeling is used to supervise the representation learning of patch tokens, which can integrate the local information into the learning. The experimental results on Laryngoscope8, ISIC 2018, and REFUGE data sets show that after adding token labeling, this dual-branch classification model achieves significantly better performance than the model using only class token loss, which demonstrates the effectiveness of our method for medical image classification tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523003653",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Class (philosophy)",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Contextual image classification",
      "ENCODE",
      "Gene",
      "Image (mathematics)",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Security token"
    ],
    "authors": [
      {
        "surname": "Yan",
        "given_name": "Fangyuan"
      },
      {
        "surname": "Yan",
        "given_name": "Bin"
      },
      {
        "surname": "Liang",
        "given_name": "Wei"
      },
      {
        "surname": "Pei",
        "given_name": "Mingtao"
      }
    ]
  },
  {
    "title": "HIE-EDT: Hierarchical interval estimation-based evidential decision tree",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110040",
    "abstract": "Decision tree algorithm, because of its strong interpretability and high algorithm efficiency, is widely used in the field of pattern recognition and classification. When the number of data samples is small and there is uncertainty in the data, it is difficult for the traditional decision tree algorithm to fully mine the effective information in the data. In this paper, we use the Dempster–Shafer framework to model data uncertainty and propose a hierarchical interval estimation method to improve decision tree algorithms. The proposed method constructs intervals through two methods of attribute boundary and mean square error estimation, which not only utilizes the characteristics of intervals to model the inaccuracy of data, but also constrains intervals from two aspects, narrowing the representation range of available information. By comparing with the classic decision tree algorithm and the decision tree algorithm based on single interval estimation, the proposed method can perform classification tasks robustly and accurately in different types of data under seven data sets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323007379",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Combinatorics",
      "Composite material",
      "Computer science",
      "Data mining",
      "Decision tree",
      "Decision tree learning",
      "ID3 algorithm",
      "Incremental decision tree",
      "Interpretability",
      "Interval (graph theory)",
      "Interval tree",
      "Law",
      "Materials science",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Range (aeronautics)",
      "Representation (politics)",
      "Search algorithm",
      "Search tree",
      "Tree (set theory)"
    ],
    "authors": [
      {
        "surname": "Gao",
        "given_name": "Bingjie"
      },
      {
        "surname": "Zhou",
        "given_name": "Qianli"
      },
      {
        "surname": "Deng",
        "given_name": "Yong"
      }
    ]
  },
  {
    "title": "Information geometry based extreme low-bit neural network for point cloud",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109986",
    "abstract": "Deep learning has significantly advanced three-dimensional computer vision applied to point clouds. Nevertheless, the substantial consumption of time, storage, and energy substantially limits its deployment on edge devices with constrained resources. Extremely low bit quantization has received wide attention due to its extremely high compression ratio, but the problem of a significant drop in accuracy cannot be ignored. To alleviate the obvious accuracy degradation for extreme low-bit quantization, this paper proposes a novel compression framework for binary and ternary neural networks applied to point clouds, which introduces information geometry to model quantization to compensate the severe feature manifold distortion. It applies differential geometry on manifolds to study the implicit information of the point cloud feature data. Based on the theoretical analysis from the novel perspective of information geometry, two optimization modules are proposed to alleviate severe geometry distortions on differential manifolds. The first module, scaling recovery, provides layer-wise scaling parameters to reduce geometric distortion caused by quantization. The second module, Pooling Recovery, is specially designed to alleviate more severe pooling geometry distortions in point clouds. These two modules benefit both binary and ternary neural networks with ignored overheads. For ternary quantization, optimizations on convolution weights and gradients are additionally introduced. The proposed self-adaptive gradient estimation provides a more accurate approximation to the non-differential ternary staircase function. Convolution weight optimization is implemented on an information-geometry optimized model to achieve even higher accuracy and less memory consumption. Experimental results validate that the proposed models significantly outperform state-of-the-art methods and demonstrate better scalability. Overall, this compression framework has the potential to facilitate the deployment of deep learning models on edge devices with limited resources, opening up new opportunities for applications of three-dimensional computer vision.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323006842",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Geometry",
      "Geometry processing",
      "Mathematics",
      "Point cloud",
      "Polygon mesh",
      "Quantization (signal processing)",
      "Scaling"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Zhi"
      },
      {
        "surname": "Ma",
        "given_name": "Yanxin"
      },
      {
        "surname": "Xu",
        "given_name": "Ke"
      },
      {
        "surname": "Wan",
        "given_name": "Jianwei"
      }
    ]
  },
  {
    "title": "GITGAN: Generative inter-subject transfer for EEG motor imagery analysis",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110015",
    "abstract": "Domain adaptation (DA) plays a crucial role in achieving subject-independent performance in Brain-Computer Interface (BCI). However, previous studies have primarily focused on developing intricate network architecture designs, neglecting the impact of source data quality and the challenges posed by the out-of-distribution target data problem. To address these limitations, we argue that a target data-centered space, augmented by a carefully selected set of high-quality source data, can significantly enhance DA. In this study, we present an unsupervised end-to-end subject adaptation approach called GITGAN, a generative inter-subject transfer for electroencephalography motor imagery analysis. We also propose a practical and effective method for selecting source data, which further enhances performance. Our approach is non-intrusive, as it does not modify the target data distribution, thus preserving its integrity for further numerical and visual analysis. Our extensive experiments with two different datasets demonstrate not only the superiority of our approach compared to existing methods, but also its phenomenal potential for practical BCI applications. The findings of this study provide valuable insights into the potential of BCI and illustrate the importance of considering source data quality in DA. The implementation is available at https://github.com/Kang1121/GITGAN.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323007124",
    "keywords": [
      "Adaptation (eye)",
      "Artificial intelligence",
      "Brain–computer interface",
      "Bubble",
      "Computer science",
      "Domain (mathematical analysis)",
      "Electroencephalography",
      "Epistemology",
      "Generative grammar",
      "Interface (matter)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Maximum bubble pressure method",
      "Motor imagery",
      "Optics",
      "Parallel computing",
      "Philosophy",
      "Physics",
      "Programming language",
      "Psychiatry",
      "Psychology",
      "Quality (philosophy)",
      "Set (abstract data type)",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Yin",
        "given_name": "Kang"
      },
      {
        "surname": "Lim",
        "given_name": "Elissa Yanting"
      },
      {
        "surname": "Lee",
        "given_name": "Seong-Whan"
      }
    ]
  },
  {
    "title": "CS-GAC: Compressively sensed geodesic active contours",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110007",
    "abstract": "This paper proposes an edge based compressively sensed (CS) geodesic active contour (GAC) model, termed CS-GAC, to ensure faithful edge detection and accurate object segmentation. The motivation behind this paper is that edge information driving the contour evolution can be iteratively obtained by incomplete CS measurements. In each iteration, the CS-GAC is a three-step process including edge detection, active contouring and sparse reconstruction. Instead of working on the final reconstructed images themselves, the evolution of the CS-GAC is driven by a few CS measurements and guided by updatable edge information. The edge information is generated by a complex shearlet transform (CST) based edge map. In the framework, reconstruction and edge detection work alternately. The iterative update property that takes advantages of both edge sparsity and edge detection can largely improve the evolution precision. Numerical experiments show that the CS-GAC can obtain challenging segmentation results in comparisons with the state of the art methods, and has competitive prospects.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323007045",
    "keywords": [
      "Active contour model",
      "Artificial intelligence",
      "Computer graphics (images)",
      "Computer science",
      "Computer vision",
      "Contouring",
      "Edge detection",
      "Enhanced Data Rates for GSM Evolution",
      "Geodesic",
      "Geometry",
      "Image (mathematics)",
      "Image processing",
      "Image segmentation",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Process (computing)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Shan",
        "given_name": "Hao"
      }
    ]
  },
  {
    "title": "Coherent chord computation and cross ratio for accurate ellipse detection",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109983",
    "abstract": "This paper presents a new method for detecting ellipses in images, which has many applications in pattern recognition and robotic tasks. Previous approaches typically use sophisticated arc grouping strategies or calculate differential such as tangents, and thereby they are less efficient or more sensitive to noise. In this work, we present a novel ellipse detector, based on the simple yet effective chord computation, and on the projective invariant cross ratio, which achieves promising performance in both accuracy and efficiency. First, elliptical arcs are extracted by fast vector computations along with the removal of straight segments to speed up detection. Then, arcs from the same ellipse are grouped together according to the relative location and the intersecting chord constraints, both are on coherent chord computation without differential. Additionally, an efficient additive principle is applied to further accelerate the grouping process. Finally, a novel and robust verification by area-deduced cross ratio is introduced to pick out salient ellipses. Compared with predecessor methods, cross ratio is not only simple for computation, but also has invariant properties (used to discriminate ellipses). Extensive experiments on seven public datasets (including synthetic and real-world images) are implemented. The results highlight the salient advantages of the proposed method compared to state-of-the-art detectors: Easier to implementation, more robust against occlusion and noise, as well as attaining higher F-measure.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323006817",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Chord (peer-to-peer)",
      "Computation",
      "Computer science",
      "Computer vision",
      "Detector",
      "Distributed computing",
      "Ellipse",
      "Geometry",
      "Invariant (physics)",
      "Mathematical physics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Tangent",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Mingyang"
      },
      {
        "surname": "Jia",
        "given_name": "Xiaohong"
      },
      {
        "surname": "Ma",
        "given_name": "Lei"
      },
      {
        "surname": "Hu",
        "given_name": "Li-Ming"
      },
      {
        "surname": "Yan",
        "given_name": "Dong-Ming"
      }
    ]
  },
  {
    "title": "Controllable Style Transfer via Test-time Training of Implicit Neural Representation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109988",
    "abstract": "Existing CNN-based style transfer frameworks have suffered from inaccurate control of pixel-wise stylization. As the CNN operation is designed based on kernel-wise operation, such a design unavoidably makes pixels affect each other. To mitigate this problem, we propose a controllable style transfer framework that leverages Implicit Neural Representation to encode each pixel respectively and optimize each style and content pair via test-time training. Unlike previous CNN-based style transfer frameworks, this formulation naturally enables accurate pixel-wise stylization control. In addition, to give explicit controllability on the degree of stylization, we define two vectors that represent the content and style respectively, enabling control by interpolating these vectors. We further demonstrate that, after being test-time trained once, our framework can show a various range of applications by precisely controlling the stylized images pixel-wise and freely adjusting image resolution and degree of stylization without further optimization or training.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323006866",
    "keywords": [
      "Applied mathematics",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Controllability",
      "Economics",
      "Law",
      "Macroeconomics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pixel",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Stylized fact"
    ],
    "authors": [
      {
        "surname": "Kim",
        "given_name": "Sunwoo"
      },
      {
        "surname": "Min",
        "given_name": "Youngjo"
      },
      {
        "surname": "Jung",
        "given_name": "Younghun"
      },
      {
        "surname": "Kim",
        "given_name": "Seungryong"
      }
    ]
  },
  {
    "title": "EIGAN: An explicitly and implicitly feature-aligned GAN for degraded image classification",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.01.012",
    "abstract": "The implementation of classification networks encounters a substantial decline in performance when subjected to degraded images due to factors such as blur, noise, and low resolution. Existing methods focus on addressing a specific kind of degraded images and thus cannot simultaneously adapt to multiple degradation scenarios. Besides, insufficient attention has been given to the causes of the decline. In this paper, we discuss about the reasons for the decrease and propose an Explicitly and Implicitly feature-aligned Generative Adversarial Network to guide the model to learn features that are more consistent with high-quality(HQ) images, named EIGAN. Initially, we introduce a feature matching loss to enable the model to focus on target regions as it does on high-quality images. Subsequently, we propose an adversarial loss intended to steer the model toward aligning with the feature distribution observed in high-quality images. As a result, our method demonstrates an enhancement in the classification accuracy of degraded images without introducing additional parameters. Extensive experiments across four types of degraded datasets indicate that as degradation intensifies, the advantages of our proposed method compared to other methods become notably more pronounced.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524000138",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Computer science",
      "Degradation (telecommunications)",
      "Epistemology",
      "Feature (linguistics)",
      "Feature matching",
      "Focus (optics)",
      "Generative adversarial network",
      "Generative grammar",
      "Image (mathematics)",
      "Linguistics",
      "Matching (statistics)",
      "Mathematics",
      "Noise (video)",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quality (philosophy)",
      "Statistics",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Hu",
        "given_name": "Jing"
      },
      {
        "surname": "Zhong",
        "given_name": "Weiwei"
      },
      {
        "surname": "Zhang",
        "given_name": "Meiqi"
      },
      {
        "surname": "Kang",
        "given_name": "Susu"
      },
      {
        "surname": "Yan",
        "given_name": "Ouyang"
      }
    ]
  },
  {
    "title": "An effective deep learning adversarial defense method based on spatial structural constraints in embedding space",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.01.007",
    "abstract": "Deep neural networks are highly vulnerable to adversarial samples. Most existing adversarial defense methods do not consider the distribution of adversarial samples. We argue that very few adversarial samples in the natural sample set prevent the deep neural networks from learning a complete and effective representation of the adversarial samples. This causes the spatial structures between the natural and the adversarial samples to be vastly different from that of the input space, thus making the models vulnerable to adversarial attacks. Based on this viewpoint, this paper proposes an effective deep-learning adversarial defense method, which incorporates information about the spatial structures of the natural and the adversarial samples in the embedding space during the training process. This proposed approach improves the deep learning model’s generalization to new adversarial samples and achieves the purpose of defending against adversarial attacks. Four deep neural networks with different scales are used and experimentally verified on four typical publicly available image data. The experimental results show that our method effectively improves the defense ability of deep learning models against adversarial attacks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524000072",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Deep learning",
      "Deep neural networks",
      "Embedding",
      "Generalization",
      "Law",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Operating system",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Space (punctuation)"
    ],
    "authors": [
      {
        "surname": "Miao",
        "given_name": "Junzhong"
      },
      {
        "surname": "Yu",
        "given_name": "Xiangzhan"
      },
      {
        "surname": "Hu",
        "given_name": "Zhichao"
      },
      {
        "surname": "Song",
        "given_name": "Yanru"
      },
      {
        "surname": "Liu",
        "given_name": "Likun"
      },
      {
        "surname": "Zhou",
        "given_name": "Zhigang"
      }
    ]
  },
  {
    "title": "A Novel Attention-Driven Framework for Unsupervised Pedestrian Re-identification with Clustering Optimization",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110045",
    "abstract": "Unsupervised pedestrian re-identification (re-ID) is not merely a visual recognition task; it represents a significant sub-field within the domain of pattern recognition. Despite the remarkable success of Convolutional Neural Networks (CNN) in re-ID, they still face challenges in handling variations in pose, occlusion, and lighting conditions. To effectively tackle these challenges, it is imperative to prioritize implementing efficient sampling strategies. We propose a Novel Attention-Driven Framework for Unsupervised Pedestrian re-ID with Clustering Optimization (AFC) to address the above issues. First, we introduce a new attention mechanism that enhances multi-scale spatial attention and reduces the number of trainable parameters. Then, we employed a straightforward and effective method of group sampling. In addition, we apply a clustering consensus approach to estimate pseudo-label similarity in continuous training and use temporal propagation and ensembles to improve pseudo-labels. Extensive experiments on Market-1501, duketmc-reID and MSMT17 datasets show that our method achieves significant performance improvement in unsupervised pedestrian re-ID, which provides important theoretical and practical value for the research on deep fusion of pattern recognition field with pedestrian re-ID and promotes the further development of the related fields.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323007422",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Cluster analysis",
      "Computer science",
      "Convolutional neural network",
      "Economics",
      "Engineering",
      "Field (mathematics)",
      "Identification (biology)",
      "Machine learning",
      "Management",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pedestrian",
      "Pedestrian detection",
      "Pure mathematics",
      "Task (project management)",
      "Transport engineering",
      "Unsupervised learning"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Xuan"
      },
      {
        "surname": "Sun",
        "given_name": "Zhaojie"
      },
      {
        "surname": "Chehri",
        "given_name": "Abdellah"
      },
      {
        "surname": "Jeon",
        "given_name": "Gwanggil"
      },
      {
        "surname": "Song",
        "given_name": "Yongchao"
      }
    ]
  },
  {
    "title": "Sharing-Net: Lightweight feedforward network for skeleton-based action recognition based on information sharing mechanism",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110050",
    "abstract": "With the development of metaverse, augmented reality and human–robot teleoperation, action recognition plays an increasingly important role. In this work, we propose a Lightweight Feedforward Cross-channel Information Sharing Network (Sharing-Net) for action recognition. A multi-feature input module is constructed, which includes Cartesian Motion features, Global Joint Distances (GJD), and Global Joint Angles (GJA). The three types of features can tackle the problems of velocity differentiation, viewpoint diversification and object-distance variation, respectively. In order to take full use of the restricted parameters caused by the lightweight structure to enhance the accuracy under the premise of guaranteeing high speed, a multi-feature cross-channel information sharing mechanism is proposed. Dynamic nonlinear composite mapping between feature channels uses cross-channel residual blocks to share data information and establish coupling relationships. Extensive experiments on 3 public datasets and a self-built dataset verify the effectiveness of proposed methods. Compared with the state-of-the-art (SOAT) methods, Sharing-Net achieves the best accuracy with high speed on JHMDB and SHREC and performs superior balance of accuracy and computational cost on NTU RGB+D.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323007471",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Control engineering",
      "Engineering",
      "Feature (linguistics)",
      "Feed forward",
      "Information sharing",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "RGB color model",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Yinan"
      },
      {
        "surname": "Gao",
        "given_name": "Qing"
      },
      {
        "surname": "Ju",
        "given_name": "Zhaojie"
      },
      {
        "surname": "Zhou",
        "given_name": "Jian"
      },
      {
        "surname": "Guo",
        "given_name": "Yulan"
      }
    ]
  },
  {
    "title": "CATNet: Cross-modal fusion for audio–visual speech recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.01.002",
    "abstract": "Automatic speech recognition (ASR) is a typical pattern recognition technology that converts human speeches into texts. With the aid of advanced deep learning models, the performance of speech recognition is significantly improved. Especially, the emerging Audio–Visual Speech Recognition (AVSR) methods achieve satisfactory performance by combining audio-modal and visual-modal information. However, various complex environments, especially noises, limit the effectiveness of existing methods. In response to the noisy problem, in this paper, we propose a novel cross-modal audio–visual speech recognition model, named CATNet. First, we devise a cross-modal bidirectional fusion model to analyze the close relationship between audio and visual modalities. Second, we propose an audio–visual dual-modal network to preprocess audio and visual information, extract significant features and filter redundant noises. The experimental results demonstrate the effectiveness of CATNet, which achieves excellent WER, CER and converges speeds, outperforms other benchmark models and overcomes the challenge posed by noisy environments.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524000023",
    "keywords": [
      "Artificial intelligence",
      "Audio visual",
      "Chemistry",
      "Computer science",
      "Fusion",
      "Linguistics",
      "Modal",
      "Multimedia",
      "Natural language processing",
      "Philosophy",
      "Polymer chemistry",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Xingmei"
      },
      {
        "surname": "Mi",
        "given_name": "Jiachen"
      },
      {
        "surname": "Li",
        "given_name": "Boquan"
      },
      {
        "surname": "Zhao",
        "given_name": "Yixu"
      },
      {
        "surname": "Meng",
        "given_name": "Jiaxiang"
      }
    ]
  },
  {
    "title": "FoodMask: Real-time food instance counting, segmentation and recognition",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110017",
    "abstract": "Food computing has long been studied and deployed to several applications. Understanding a food image at the instance level, including recognition, counting and segmentation, is essential to quantifying nutrition and calorie consumption. Nevertheless, existing techniques are limited to either category-specific instance detection, which does not reflect precisely the instance size at the pixel level, or category-agnostic instance segmentation, which is insufficient for dish recognition. This paper presents a compact and fast multi-task network, namely FoodMask, for clustering-based food instance counting, segmentation and recognition. The network learns a semantic space simultaneously encoding food category distribution and instance height at pixel basis. While the former value addresses instance recognition, the latter value provides prior knowledge for instance extraction. Besides, we integrate into the semantic space a pathway for class-specific counting. With these three outputs, we propose a clustering algorithm to segment and recognize food instances at a real-time speed. Empirical studies are made on three large-scale food datasets, including Mixed Dishes, UECFoodPixComp and FoodSeg103, which cover Western, Chinese, Japanese and Indian cuisines. The proposed networks outperform benchmarks in both terms of instance map quality and speed efficiency.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323007148",
    "keywords": [
      "Artificial intelligence",
      "Cartography",
      "Class (philosophy)",
      "Cluster analysis",
      "Computer science",
      "Geography",
      "Pattern recognition (psychology)",
      "Pixel",
      "Scale (ratio)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Nguyen",
        "given_name": "Huu-Thanh"
      },
      {
        "surname": "Cao",
        "given_name": "Yu"
      },
      {
        "surname": "Ngo",
        "given_name": "Chong-Wah"
      },
      {
        "surname": "Chan",
        "given_name": "Wing-Kwong"
      }
    ]
  },
  {
    "title": "GDB: Gated Convolutions-based Document Binarization",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109989",
    "abstract": "Document binarization is a crucial pre-processing step for various document analysis tasks. However, existing methods fail to accurately capture stroke edges, primarily due to the inherent limitations of vanilla convolutions and the absence of adequate boundary-related supervision during stroke edge extraction. In this paper, we formulate text extraction as the learning of gating values and propose an end-to-end network architecture based on gated convolutions, named GDB, to address the problem of imprecise stroke edge extraction. The gated convolutions enable the selective extraction of stroke feature with different attention. Our proposed framework comprises two stages. Firstly, a coarse sub-network with an extra edge branch is trained to enhance the precision of feature maps by incorporating a priori mask and edge information. Secondly, a refinement sub-network is cascaded to enhance the output of the first stage using gated convolutions based on the sharp edges. To effectively incorporate global information, GDB also integrates a parallelized multi-scale operation that combines local and global features. We conduct comprehensive experiments on ten Document Image Binarization Contest (DIBCO) datasets from 2009 to 2019 and Document Deblurring Datasets. Experimental results show that our proposed methods outperform the state-of-the-art methods across all metrics on average. Extensive ablation studys demonstrate the efficacy of key components. Available codes: https://github.com/Royalvice/GDB.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323006878",
    "keywords": [
      "A priori and a posteriori",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Data mining",
      "Enhanced Data Rates for GSM Evolution",
      "Epistemology",
      "Feature (linguistics)",
      "Feature extraction",
      "Image (mathematics)",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Zongyuan"
      },
      {
        "surname": "Liu",
        "given_name": "Baolin"
      },
      {
        "surname": "Xiong",
        "given_name": "Yongping"
      },
      {
        "surname": "Wu",
        "given_name": "Guibin"
      }
    ]
  },
  {
    "title": "Debiased Visual Question Answering via the perspective of question types",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.01.009",
    "abstract": "Visual Question Answering (VQA) aims to answer questions according to the given image. However, current VQA models tend to rely solely on textual information from the questions and ignore the visual information in the images to get answers, which is caused by bias that is generated during the training phase. Previous studies have shown that bias in VQA is mainly caused by the text modality, and our analysis suggests that question type is a crucial factor in bias formation. To address this bias, we proposed a self-supervised method including the Against Biased Samples(ABS) module that performs targeted debiasing by selecting samples that are prone to bias, and the Shuffle Question types(SQT) module that constructs negative samples by randomly replacing the question types of the samples selected by the ABS, to interrupting the shortcuts from question type to answer. Our approach mitigates the question-to-answer bias without using external annotations, overcoming the prior language problem. Additionally, we designed a new objective function for negative samples. Experimental results indicate that our method outperforms both self-supervised-based and supervised-based state-of-the-art approaches, achieving 70.36% accuracy on the VQA-CP v2 dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524000096",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Debiasing",
      "Evolutionary biology",
      "Function (biology)",
      "Image (mathematics)",
      "Information retrieval",
      "Machine learning",
      "Modality (human–computer interaction)",
      "Natural language processing",
      "Perspective (graphical)",
      "Psychology",
      "Question answering",
      "Social psychology"
    ],
    "authors": [
      {
        "surname": "Huai",
        "given_name": "Tianyu"
      },
      {
        "surname": "Yang",
        "given_name": "Shuwen"
      },
      {
        "surname": "Zhang",
        "given_name": "Junhang"
      },
      {
        "surname": "Zhao",
        "given_name": "Jiabao"
      },
      {
        "surname": "He",
        "given_name": "Liang"
      }
    ]
  },
  {
    "title": "Collaborative contrastive learning for hypergraph node classification",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109995",
    "abstract": "Plenty of models have been presented to handle the hypergraph node classification. However, very few of these methods consider contrastive learning, which is popular due to its great power to represent instances. This paper makes an attempt to leverage contrastive learning to hypergraph representation learning. Specifically, we propose a novel method called Collaborative Contrastive Learning (CCL), which incorporates a generated standard graph with the hypergraph. The main technical contribution here is that we develop a collaborative contrastive schema, which performs contrast between the node views obtained from the standard graph and hypergraph in each network layer, thus making the contrast collaborative. To be precise, in the first layer, the view from the standard graph is used to augment that from the hypergraph. Then, in the next layer, the augmented features are adopted to train a new representation to augment the view from the standard graph conversely. With this setting, the learning procedure is alternated between the standard graph and hypergraph. As a result, the learning on the standard graph and hypergraph is collaborative and leads to the final informative node representation. Experimental results on several widely used datasets validate the effectiveness of the proposed model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323006933",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Discrete mathematics",
      "Graph",
      "Hypergraph",
      "Leverage (statistics)",
      "Machine learning",
      "Mathematics",
      "Schema (genetic algorithms)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Hanrui"
      },
      {
        "surname": "Li",
        "given_name": "Nuosi"
      },
      {
        "surname": "Zhang",
        "given_name": "Jia"
      },
      {
        "surname": "Chen",
        "given_name": "Sentao"
      },
      {
        "surname": "Ng",
        "given_name": "Michael K."
      },
      {
        "surname": "Long",
        "given_name": "Jinyi"
      }
    ]
  },
  {
    "title": "Dynamic receptive field adaptation for scene text recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2023.12.005",
    "abstract": "Scene text recognition methods of the Encoder–Decoder framework, generally assume that the proportion of characters in the same text instance are basically the same. However, this assumption does not always hold in the context of complex scene images. For adaptively revising the receptive field according to the different font in the scene text image, we propose a Dynamic Receptive Field Adaption Framework which consists of Memory Attention (MA) module and Dynamic Feature Adaptive (DFA) module. MA percepts the historical location information to adapt to the change of character position in the decoder. DFA selects the most distinguishing features from feature maps of different levels dynamically. Additionally, MA and DFA can be easily extended to the existing attention-based and transformer-based text recognition methods to improve their performance. With extension experiments on public benchmark datasets, including IIIT-5K, SVT, SVTP, CUTE80, RECTS, LSVT, and RCTW, our method has shown effectiveness and robustness.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523003525",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Encoder",
      "Feature (linguistics)",
      "Gene",
      "Linguistics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Receptive field",
      "Robustness (evolution)",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Tian",
        "given_name": "Shu"
      },
      {
        "surname": "Zhu",
        "given_name": "Kang-Xi"
      },
      {
        "surname": "Qin",
        "given_name": "Hai-Bo"
      },
      {
        "surname": "Yang",
        "given_name": "Chun"
      }
    ]
  },
  {
    "title": "Shadow-aware dynamic convolution for shadow removal",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109969",
    "abstract": "With a wide range of shadows in many collected images, shadow removal has aroused increasing attention since uncontaminated images are of vital importance for many computer vision tasks. Current methods consider the same convolution operations for both shadow and non-shadow regions while ignoring the large gap between the color mappings for the shadow region and the non-shadow region, leading to poor quality of reconstructed images and a heavy computation burden. To solve this problem, this paper introduces a novel plug-and-play Shadow-Aware Dynamic Convolution (SADC) module to decouple the interdependence between the shadow region and the non-shadow region. Inspired by the fact that the color mapping of the non-shadow region is easier to learn, our SADC processes the non-shadow region with a lightweight convolution module in a computationally cheap manner and recovers the shadow region with a more complicated convolution module to ensure the quality of image reconstruction. Given that the non-shadow region often contains more background color information, we further develop a novel intra-convolution distillation loss to strengthen the information flow from the non-shadow region to the shadow region. Extensive experiments on the ISTD and SRD datasets show our method achieves better performance in shadow removal over many state-of-the-art methods. Codes have been made available at https://github.com/xuyimin0926/SADC.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323006672",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computation",
      "Computer science",
      "Computer vision",
      "Convolution (computer science)",
      "Image (mathematics)",
      "Psychology",
      "Psychotherapist",
      "Shadow (psychology)",
      "Shadow mapping"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Yimin"
      },
      {
        "surname": "Lin",
        "given_name": "Mingbao"
      },
      {
        "surname": "Yang",
        "given_name": "Hong"
      },
      {
        "surname": "Chao",
        "given_name": "Fei"
      },
      {
        "surname": "Ji",
        "given_name": "Rongrong"
      }
    ]
  },
  {
    "title": "RIC-CNN: Rotation-Invariant Coordinate Convolutional Neural Network",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.109994",
    "abstract": "Due to the lack of rotation invariance in traditional convolution operations, even acting a slight rotation on the input can severely degrade the performance of Convolutional Neural Networks (CNNs). To address this, we propose a Rotation-Invariant Coordinate Convolution (RIC-C), which achieves natural invariance to arbitrary rotations around the input center without additional trainable parameters or data augmentation. We first evaluate the rotational invariance of RIC-C using the MNIST dataset and compare its performance with most previous rotation-invariant CNN models. RIC-C achieves state-of-the-art classification on the MNIST-rot test set without data augmentation and with lower computational costs. Then, the interchangeability of RIC-C with traditional convolution operations is demonstrated by seamlessly integrating it into common CNN models like VGG, ResNet, and DenseNet. We conduct remote sensing image classification on the NWPU VHR-10, MTARSI and AID datasets and patch matching experiments on the UBC benchmark dataset, showing that RIC-C significantly enhances the performance of CNN models across different applications, especially when training data is limited. Our codes can be downloaded from https://github.com/HanlinMo/Rotation-Invariant-Coordinate-Convolutional-Neural-Network.git.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323006921",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Computer science",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Coordinate system",
      "Deep learning",
      "Geodesy",
      "Geography",
      "Invariant (physics)",
      "MNIST database",
      "Mathematical physics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Rotation (mathematics)",
      "Rotational invariance"
    ],
    "authors": [
      {
        "surname": "Mo",
        "given_name": "Hanlin"
      },
      {
        "surname": "Zhao",
        "given_name": "Guoying"
      }
    ]
  },
  {
    "title": "Semi-supervised medical image segmentation via hard positives oriented contrastive learning",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110020",
    "abstract": "Semi-supervised learning (SSL) has been a popular technique to resolve the annotation scarcity problem in pattern recognition and medical image segmentation, which usually focuses on two critical issues: 1) learning a well-structured categorizable embedding space, and 2) establishing a robust mapping from the embedding space to the pixel space. In this paper, to resolve the first issue, we propose a h ard p ositives oriented c ontrastive (HPC) learning strategy to pre-train an encoder-decoder-based segmentation model. Different from vanilla contrastive learning tending to focus only on hard negatives, our HPC learning strategy additionally concentrates on hard positives (i.e., samples with the same category but dissimilar feature representations to the anchor), which are considered to play an even more crucial role in delivering discriminative knowledge for semi-supervised medical image segmentation. Specifically, the HPC is constructed from two levels, including an unsupervised image-level HPC (IHPC) and a supervised pixel-level HPC (PHPC), empowering the embedding space learned by the encoder with both local and global senses. Particularly, the PHPC learning strategy is implemented in a region-based manner, saving memory usage while delivering more multi-granularity information. In response to the second issue, we insert several feature swap (FS) modules into the pre-trained decoder. These FS modules aim to perturb the mapping from the intermediate embedding space towards the pixel space, trying to encourage more robust segmentation predictions. Experiments on two public clinical datasets demonstrate that our proposed framework surpasses the state-of-the-art methods by a large margin. Source codes are available at https://github.com/PerPerZXY/BHPC.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323007173",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Discriminative model",
      "Embedding",
      "Encoder",
      "Feature (linguistics)",
      "Feature learning",
      "Feature vector",
      "Image segmentation",
      "Linguistics",
      "Machine learning",
      "Margin (machine learning)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Tang",
        "given_name": "Cheng"
      },
      {
        "surname": "Zeng",
        "given_name": "Xinyi"
      },
      {
        "surname": "Zhou",
        "given_name": "Luping"
      },
      {
        "surname": "Zhou",
        "given_name": "Qizheng"
      },
      {
        "surname": "Wang",
        "given_name": "Peng"
      },
      {
        "surname": "Wu",
        "given_name": "Xi"
      },
      {
        "surname": "Ren",
        "given_name": "Hongping"
      },
      {
        "surname": "Zhou",
        "given_name": "Jiliu"
      },
      {
        "surname": "Wang",
        "given_name": "Yan"
      }
    ]
  },
  {
    "title": "Joint discriminative representation learning for end-to-end person search",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110053",
    "abstract": "Person search simultaneously detects and retrieves a query person from uncropped scene images. Existing methods are either two-step or end-to-end. The former employs two standalone models for the two sub-tasks, while the latter conducts person search with a unified model. Despite encouraging progress, most existing end-to-end methods focus on balancing the model between detection and retrieval sub-tasks, while ignoring to enhance the learned representation for retrieval, which leads to inferior accuracy to two-step approaches. To that end, we propose a novel hierarchical framework that jointly optimizes instance-aware and part-aware embedding to enable discriminative representation learning. Specifically, we develop a region-of-interest cosegment (ROICoseg) module that captures part-aware information without requiring extra annotations to enable fine-grained discriminative representation. On top of that, a Contextual Instance Batch Sampling (CIBS) method is introduced to effectively employ contextual information for constructing training batches, thus facilitating effective instance-aware representation learning. We further introduce the first cross-door person search dataset (CDPS) that retrieves a target person in outdoor cameras with an indoor captured image or vice versa. Extensive experiments show that our proposed model achieves competitive performance on CUHK-SYSU and outperforms state-of-the-art end-to-end methods on the more challenging PRW and CDPS. 1 1 Code and dataset are available at https://github.com/PatrickZad/CDPS.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323007501",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Discriminative model",
      "Embedding",
      "End-to-end principle",
      "Feature learning",
      "Focus (optics)",
      "Law",
      "Machine learning",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Political science",
      "Politics",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Pengcheng"
      },
      {
        "surname": "Yu",
        "given_name": "Xiaohan"
      },
      {
        "surname": "Bai",
        "given_name": "Xiao"
      },
      {
        "surname": "Wang",
        "given_name": "Chen"
      },
      {
        "surname": "Zheng",
        "given_name": "Jin"
      },
      {
        "surname": "Ning",
        "given_name": "Xin"
      }
    ]
  },
  {
    "title": "A lightweight unsupervised adversarial detector based on autoencoder and isolation forest",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110127",
    "abstract": "Although deep neural networks (DNNs) have performed well on many perceptual tasks, they are vulnerable to adversarial examples that are generated by adding slight but maliciously crafted perturbations to benign images. Adversarial detection is an important technique for identifying adversarial examples before they are entered into target DNNs. Previous studies that were performed to detect adversarial examples either targeted specific attacks or required expensive computation. Designing a lightweight unsupervised detector is still a challenging problem. In this paper, we propose an AutoEncoder-based Adversarial Examples (AEAE) detector that can guard DNN models by detecting adversarial examples with low computation in an unsupervised manner. The AEAE includes only a shallow autoencoder that performs two roles. First, a well-trained autoencoder has learned the manifold of benign examples. This autoencoder can produce a large reconstruction error for adversarial images with large perturbations, so we can detect significantly perturbed adversarial examples based on the reconstruction error. Second, the autoencoder can filter out small noises and change the DNN’s prediction on adversarial examples with small perturbations. It helps to detect slightly perturbed adversarial examples based on the prediction distance. To cover these two cases, we utilize the reconstruction error and prediction distance from benign images to construct a two-tuple feature set and train an adversarial detector using the isolation forest algorithm. We show empirically that AEAE is an unsupervised and inexpensive detector against most state-of-the-art attacks. Through the detection in these two cases, there is nowhere to hide adversarial examples.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323008245",
    "keywords": [
      "Adversarial machine learning",
      "Adversarial system",
      "Algorithm",
      "Artificial intelligence",
      "Autoencoder",
      "Computation",
      "Computer science",
      "Deep learning",
      "Detector",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Telecommunications",
      "Unsupervised learning"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Hui"
      },
      {
        "surname": "Zhao",
        "given_name": "Bo"
      },
      {
        "surname": "Guo",
        "given_name": "Jiabao"
      },
      {
        "surname": "Zhang",
        "given_name": "Kehuan"
      },
      {
        "surname": "Liu",
        "given_name": "Peng"
      }
    ]
  },
  {
    "title": "MSA-GCN: Multiscale Adaptive Graph Convolution Network for gait emotion recognition",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110117",
    "abstract": "Gait emotion recognition plays a crucial role in the intelligent system. Most existing approaches identify emotions by focusing on local actions over time. However, some valuable observational facts that the effective distances of different emotions in the time domain are different, and the local actions during walking are quite similar, are put aside in those methods. And this ignorance often ends up impairing performance of emotion recognition. To address the issues, a novel model, named MSA-GCN (MultiScale Adaptive Graph Convolution Network), is proposed to utilize the valuable observational knowledge for improving emotion recognition performance. In the proposed model, an adaptive spatio-temporal graph convolution is designed to dynamically select convolution kernels to learn the spatio-temporal features of different emotions. Moreover, a Cross-Scale Mapping Interaction mechanism (CSMI) is proposed to construct an adaptive adjacency matrix for high-quality aggregation of the multiscale information. Extensive experimental results on public datasets indicate that, compared with the state-of-the-art methods, the proposed approach achieves better performance in terms of emotion recognition accuracy, and shows the proposed approach is promising.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323008142",
    "keywords": [
      "Adjacency list",
      "Adjacency matrix",
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Convolution (computer science)",
      "Graph",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Yin",
        "given_name": "Yunfei"
      },
      {
        "surname": "Jing",
        "given_name": "Li"
      },
      {
        "surname": "Huang",
        "given_name": "Faliang"
      },
      {
        "surname": "Yang",
        "given_name": "Guangchao"
      },
      {
        "surname": "Wang",
        "given_name": "Zhuowei"
      }
    ]
  },
  {
    "title": "Vital information is only worth one thumbnail: Towards efficient human pose estimation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110111",
    "abstract": "In pursuit of impressive performance, existing DCNN-based approaches of human pose estimation usually use massive networks and large-size images to train a deep model. When applying these deep based methods in real-time systems, current works try to compress the deep network by reducing the number of layers and channels, but such approaches are complex and poorly generalized since they require elaborate design of small-scale network structures. Based on the fact that large-size images contain redundant information, in this paper, we explore the influence of image-size on system complexity and propose a novel framework called ThumbPose to accelerate and compress deep models by inferring on thumbnail representations in the task of human pose estimation. In our framework, we first propose a style supervised online downscaler to reduce an input image into a thumbnail image. Furthermore, a training strategy of dual-branch auto-encoding is designed to obtain effective and accurate thumbnail representation in a knowledge distillation manner, which is further used to maintain the performance of thumbnail images as the original-size input images. For heat-map based human pose estimation, ThumbPose is an orthogonal and implementation-friendly method, that can not only compress and accelerate the inference network but also obtain an image downscaler in a supervised manner that can be used in other high-level tasks (e.g. detection, segmentation, etc. in practical applications). Extensive experiments on MS COCO dataset demonstrate the effectiveness of our proposed method, and ThumbPose achieves superior performance (＋ 1.3% AP and ＋ 0.7% AR) with negligible additional cost ( < 0.2 GFLOPs) compared to previous state-of-the-art methods when using small-size images as inputs. Moreover, experiments on MPII show that our model achieves higher accuracy (＋ 0.2% Mean@0.5) with minimal computation (2.5 GFLOPs) compared to superior lightweight models obtained by the network compression methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323008087",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Economics",
      "Image (mathematics)",
      "Inference",
      "Machine learning",
      "Management",
      "Pattern recognition (psychology)",
      "Pose",
      "Segmentation",
      "Task (project management)",
      "Thumbnail"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Zian"
      },
      {
        "surname": "Zhang",
        "given_name": "Yongqiang"
      },
      {
        "surname": "Zhang",
        "given_name": "Yin"
      },
      {
        "surname": "Tian",
        "given_name": "Rui"
      },
      {
        "surname": "Ding",
        "given_name": "Mingli"
      }
    ]
  },
  {
    "title": "RoCNet++: Triangle-based descriptor for accurate and robust point cloud registration",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110108",
    "abstract": "This paper introduces RoCNet++, a point cloud registration method with two main contributions, one concerning the design of a robust descriptor and another concerning the estimation of the rigid transformation. First, to robustly capture the local geometric properties of the surface, i.e., each point is characterized by all the triangles formed by itself and its nearest neighbours in the 3D point cloud. The idea is to assist the learning of the descriptor by introducing a priori information about interesting geometric properties such as the invariance of triangle angles under rigid transformations. This local triangle-based descriptor is integrated into the recently developed RoCNet architecture for estimating the correspondences between source and target point clouds. We then introduce the Farthest Sampling-guided Registration (FSR), which relies on successive farthest point samplings to estimate the global rigid transformation between 3D point clouds. The new proposed architecture RoCNet++ has been evaluated in different configurations: clean, noisy and partial data on both synthetic and real databases such as ModelNet40, KITTI, and 3DMatch. RoCNet++ shows improved performances on these benchmark datasets in favourable and unfavourable conditions. Furthermore, both the local triangle-based descriptor and the Farthest Sampling-guided Registration (FSR) can be used in other registration algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323008051",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Geometry",
      "Image (mathematics)",
      "Image registration",
      "Mathematics",
      "Point (geometry)",
      "Point cloud"
    ],
    "authors": [
      {
        "surname": "Slimani",
        "given_name": "Karim"
      },
      {
        "surname": "Achard",
        "given_name": "Catherine"
      },
      {
        "surname": "Tamadazte",
        "given_name": "Brahim"
      }
    ]
  },
  {
    "title": "Learning conditional variational autoencoders with missing covariates",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110113",
    "abstract": "Conditional variational autoencoders (CVAEs) are versatile deep latent variable models that extend the standard VAE framework by conditioning the generative model with auxiliary covariates. The original CVAE model assumes that the data samples are independent, whereas more recent conditional VAE models, such as the Gaussian process (GP) prior VAEs, can account for complex correlation structures across all data samples. While several methods have been proposed to learn standard VAEs from partially observed datasets, these methods fall short for conditional VAEs. In this work, we propose a method to learn conditional VAEs from datasets in which auxiliary covariates can contain missing values as well. The proposed method augments the conditional VAEs with a prior distribution for the missing covariates and estimates their posterior using amortised variational inference. At training time, our method accounts for the uncertainty associated with the missing covariates while simultaneously maximising the evidence lower bound. We develop computationally efficient methods to learn CVAEs and GP prior VAEs that are compatible with mini-batching. Our experiments on simulated datasets as well as on real-world biomedical datasets show that the proposed method outperforms previous methods in learning conditional VAEs from non-temporal, temporal, and longitudinal datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323008105",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Conditional probability distribution",
      "Covariate",
      "Gaussian",
      "Generative grammar",
      "Generative model",
      "Inference",
      "Latent variable",
      "Machine learning",
      "Mathematics",
      "Missing data",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Ramchandran",
        "given_name": "Siddharth"
      },
      {
        "surname": "Tikhonov",
        "given_name": "Gleb"
      },
      {
        "surname": "Lönnroth",
        "given_name": "Otto"
      },
      {
        "surname": "Tiikkainen",
        "given_name": "Pekka"
      },
      {
        "surname": "Lähdesmäki",
        "given_name": "Harri"
      }
    ]
  },
  {
    "title": "M 3 TTS: Multi-modal text-to-speech of multi-scale style control for dubbing",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.02.005",
    "abstract": "Dubbing refers to the procedure of recording characters by professional voice actors in films and games. It is more expressive and immersive than conventional Text-to-Speech (TTS) technologies and requires synchronization and style consistency of audio and video. Previous dubbing methods use video to provide either a global style vector or a local prosody embedding, limiting the expressiveness of the predicted waveform. To generate more expressive audio with precise visual temporal alignment, we propose a multi-modal multi-scale expressive speech synthesis method, namely multi-modal multi-scale TTS (M 3 TTS), which introduces an auxiliary video input to provide style embeddings. Specifically, M 3 TTS adopts a memory network to bridge heterogeneous modalities and further solve the training-inference style mismatch in conventional multi-modal TTS. To enhance the expressiveness of synthesized audio, a multi-scale style modeling scheme is used for recovering style characteristics at different scales. In addition, M 3 TTS can convert the style of speech by choosing different reference videos. We conduct extensive experiments on the public GRID corpus, where our proposed M 3 TTS can generate high-quality video-aligned speech. It also shows superior performance over the other comparable methods, both subjectively and objectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524000382",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "History",
      "Modal",
      "Physics",
      "Polymer chemistry",
      "Quantum mechanics",
      "Scalable Vector Graphics",
      "Scale (ratio)",
      "Speech recognition",
      "Speech synthesis",
      "Style (visual arts)",
      "VRML",
      "Virtual reality",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Yan"
      },
      {
        "surname": "Wei",
        "given_name": "Li-Fang"
      },
      {
        "surname": "Qian",
        "given_name": "Xinyuan"
      },
      {
        "surname": "Zhang",
        "given_name": "Tian-Hao"
      },
      {
        "surname": "Chen",
        "given_name": "Song-Lu"
      },
      {
        "surname": "Yin",
        "given_name": "Xu-Cheng"
      }
    ]
  },
  {
    "title": "Neural architecture search: A contemporary literature review for computer vision applications",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110052",
    "abstract": "Deep Neural Networks have received considerable attention in recent years. As the complexity of network architecture increases in relation to the task complexity, it becomes harder to manually craft an optimal neural network architecture and train it to convergence. As such, Neural Architecture Search (NAS) is becoming far more prevalent within computer vision research, especially when the construction of efficient, smaller network architectures is becoming an increasingly important area of research, for which NAS is well suited. However, despite their promise, contemporary and end-to-end NAS pipeline require vast computational training resources. In this paper, we present a comprehensive overview of contemporary NAS approaches with respect to image classification, object detection, and image segmentation. We adopt consistent terminology to overcome contradictions common within existing NAS literature. Furthermore, we identify and compare current performance limitations in addition to highlighting directions for future NAS research.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323007495",
    "keywords": [
      "Architecture",
      "Art",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer security",
      "Data mining",
      "Data science",
      "Deep learning",
      "Engineering",
      "Linguistics",
      "Machine learning",
      "Network architecture",
      "Object detection",
      "Philosophy",
      "Pipeline (software)",
      "Programming language",
      "Relation (database)",
      "Segmentation",
      "Systems engineering",
      "Task (project management)",
      "Terminology",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Poyser",
        "given_name": "Matt"
      },
      {
        "surname": "Breckon",
        "given_name": "Toby P."
      }
    ]
  },
  {
    "title": "Feature fusion method based on spiking neural convolutional network for edge detection",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110112",
    "abstract": "NSNP-type neuron is a new type of neuron model inspired by nonlinear spiking mechanisms in nonlinear spiking neural P systems. In order to address the loss problem of edge detail information in edge detection methods based on deep learning, we propose a feature fusion method based on NSNP-type neurons. The architecture of this feature fusion method consists of two modules: feature extraction module and feature fusion module. In particular, the feature fusion module is composed of convolutional blocks constructed by NSNP-type neurons for multi-level feature fusions, and CoT blocks with Transformer style is introduced to extract rich contextual information from low-level features and high-level features. To fuse multi-level features and preserve contextual information, we design a new loss function that not only preserves feature prediction loss and fusion loss, but also considers contour-related and texture-related information. The proposed method is evaluated on BSDS500 and NYUDv2 data sets and compare it with 9 baseline methods and 12 CNN-based methods, and we achieve ODS of 0.808 and OIS of 0.827 on BSDS500. The comparison results demonstrate the advantages of the proposed method for edge detection. The source code is available at https://github.com/xhuph66/FF-CNSNP-master.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323008099",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Edge detection",
      "Electrical engineering",
      "Engineering",
      "Enhanced Data Rates for GSM Evolution",
      "Feature (linguistics)",
      "Feature extraction",
      "Fuse (electrical)",
      "Fusion",
      "Image (mathematics)",
      "Image processing",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Xian",
        "given_name": "Ronghao"
      },
      {
        "surname": "Xiong",
        "given_name": "Xin"
      },
      {
        "surname": "Peng",
        "given_name": "Hong"
      },
      {
        "surname": "Wang",
        "given_name": "Jun"
      },
      {
        "surname": "de Arellano Marrero",
        "given_name": "Antonio Ramírez"
      },
      {
        "surname": "Yang",
        "given_name": "Qian"
      }
    ]
  },
  {
    "title": "N-QGNv2: Predicting the optimum quadtree representation of a depth map from a monocular camera",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.01.027",
    "abstract": "Self-supervised monocular depth prediction is a widely researched field that aims to provide a better scene understanding. However, most existing methods prioritize prediction accuracy over computation cost, which can hinder the deployment of these methods in real-world applications. Our objective is to propose a solution that efficiently compresses the depth map while maintaining a high level of accuracy for navigation purpose. The proposed method is an expansion of the work presented in N-QGN, which utilizes a quadtree representation for compression. This approach has already shown promising results, but we aim to improve it further by making it more accurate, faster, and easier to train. Therefore, we introduce a new method that directly predicts the quadtree structure, resulting in a more consistent prediction, and we revise the network architecture to be lighter and produce state-of-the-art accuracy results, depending on the data compression rate. The new implementation is also faster, making it more suitable for real-time applications. Experiments have been conducted on various scene configuration highlighting the capability of the method to efficiently predicting a reliable quadtree depth representation of the scene at low computation cost and high accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524000321",
    "keywords": [
      "Artificial intelligence",
      "Computer graphics (images)",
      "Computer science",
      "Computer vision",
      "Law",
      "Monocular",
      "Political science",
      "Politics",
      "Quadtree",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Braun",
        "given_name": "Daniel"
      },
      {
        "surname": "Morel",
        "given_name": "Olivier"
      },
      {
        "surname": "Demonceaux",
        "given_name": "Cédric"
      },
      {
        "surname": "Vasseur",
        "given_name": "Pascal"
      }
    ]
  },
  {
    "title": "PWDformer: Deformable transformer for long-term series forecasting",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110118",
    "abstract": "Long-term forecasting is of paramount importance in numerous scenarios, including predicting future energy, water, and food consumption. For instance, extreme weather events and natural disasters can profoundly impact infrastructure operations and pose severe safety concerns. Traditional CNN-based models often struggle to capture long-distance dependencies effectively. In contrast, Transformers-based models have shown significant promise in long-term forecasting. This paper investigates the long-term forecasting problem and identifies a common limitation in existing Transformer-based models: they tend to reduce computational complexity at the expense of time information aggregation capability. Moreover, the order of time series plays a crucial role in accurate predictions, but current Transformer-based models lack sensitivity to time series order, rendering them unreasonable. To address these issues, we propose a novel Deformable-Local (DL) aggregation mechanism. This mechanism enhances the model’s ability to aggregate time information and allows the model to adaptively adjust the size of the time aggregation window. Consequently, the model can discern more complex time patterns, leading to more accurate predictions. Additionally, our model incorporates a Frequency Selection module to reinforce effective features and reduce noise. Furthermore, we introduce Position Weights to mitigate the order-insensitivity problem present in existing methods. In extensive evaluations of long-term forecasting tasks, we conducted benchmark tests on six datasets covering various practical applications, including energy, traffic, economics, weather, and disease. Our method achieved state-of-the-art (SOTA) results, demonstrating significant improvements. For instance, on the ETT dataset, our model achieved an average MSE improvement of approximately 19% and an average MAE improvement of around 27%. Remarkably, for predicted lengths of 96 and 192, we achieved outstanding MSE and MAE improvements of 32.1% and 30.9%, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323008154",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Data mining",
      "Electrical engineering",
      "Engineering",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Zheng"
      },
      {
        "surname": "Ran",
        "given_name": "Haowei"
      },
      {
        "surname": "Ren",
        "given_name": "Jinchang"
      },
      {
        "surname": "Sun",
        "given_name": "Meijun"
      }
    ]
  },
  {
    "title": "CustomDepth: Customizing point-wise depth categories for depth completion",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.02.006",
    "abstract": "Classification-based depth completion methods have achieved remarkable performance. However, the result is still coarse due to the limitation of using unified depth categories to represent depth distribution. In this work, we propose CustomDepth which can customize exclusive depth categories for each image point to boost performance. To this end, CustomDepth introduces a depth subdivision module that allocates adaptive depth categories for each point based on its properties, instead of refining a set of unified categories for all points. With these adaptive depth categories, CustomDepth utilizes a binary classifier to determine whether a point is located in front of or behind each depth category. The classification results are then accumulated using a rendering approach to calculate the final depth result. To reduce computational burden, CustomDepth also incorporates an image subdivision module that selectively processes a subset of error-prone points. Extensive experiments demonstrate that CustomDepth is a lightweight and flexible framework that achieves competitive performance compared to existing classification-based methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524000400",
    "keywords": [
      "Archaeology",
      "Arithmetic",
      "Artificial intelligence",
      "Binary number",
      "Classifier (UML)",
      "Computer science",
      "Computer vision",
      "Depth map",
      "Geometry",
      "History",
      "Image (mathematics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Point (geometry)",
      "Rendering (computer graphics)",
      "Subdivision"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Shenglun"
      },
      {
        "surname": "Ye",
        "given_name": "Xinchen"
      },
      {
        "surname": "Zhang",
        "given_name": "Hong"
      },
      {
        "surname": "Li",
        "given_name": "Haojie"
      },
      {
        "surname": "Wang",
        "given_name": "Zhihui"
      }
    ]
  },
  {
    "title": "Dynamic attention augmented graph network for video accident anticipation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110071",
    "abstract": "Accident anticipation (or the prediction of abnormal events in general) aims to forecast accidents before they occur by assessing risks based on the preceding frames in videos. The risk assessment heavily relies on understanding the semantics of the scene context and predicting the interactions among the involved subjects. Indeed, the comprehensive utilization of spatial relationships among the subjects of immediate interest in a single frame and temporal dependencies across consecutive frames is crucial for video accident anticipation. To address this challenge, we propose a novel approach called Dynamic Attention Augmented Graph Network (DAA-GNN), which leverages underlying spatial cues and models’ relationships among detected subjects of immediate interest. Specifically, our approach employs a graph neural network that is enhanced by global context clues, allowing effective message propagation and the discovery of interactions among the subjects of interest in the scene. The DAA-GNN includes a temporal attention module designed to identify long-term dependencies along the temporal axis, contributing to an end-to-end deep network solution for accurate accident anticipation. We extensively evaluate our method on the publicly available Dashcam Accident Dataset (DAD) and Epic Fail (EF) datasets, by conducting comprehensive experiments to assess its performance. The results unequivocally demonstrate that our method outperforms the state-of-the-art accident anticipation methods. Our source code and datasets are available at https://github.com/ZxyLinkstart/DAA-GNN.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323007689",
    "keywords": [
      "Anticipation (artificial intelligence)",
      "Artificial intelligence",
      "Biology",
      "Code (set theory)",
      "Computer science",
      "Context (archaeology)",
      "Graph",
      "Machine learning",
      "Paleontology",
      "Programming language",
      "Set (abstract data type)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Song",
        "given_name": "Wenfeng"
      },
      {
        "surname": "Li",
        "given_name": "Shuai"
      },
      {
        "surname": "Chang",
        "given_name": "Tao"
      },
      {
        "surname": "Xie",
        "given_name": "Ke"
      },
      {
        "surname": "Hao",
        "given_name": "Aimin"
      },
      {
        "surname": "Qin",
        "given_name": "Hong"
      }
    ]
  },
  {
    "title": "Conditional reiterative High-Fidelity GAN inversion for image editing",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110068",
    "abstract": "Our work introduces a conditional reiteration mechanism for High-Fidelity GAN (Generative Adversarial Networks) inversion (HFGI), preserving image-specific details (like background, appearance, etc.) for both normal and out-of-domain images (e.g. heavy makeup faces). The HFGI encoder’s single-stage conditional latent maps result in blurry regions in restored images and loss of detailed information during editing. To address this, we proposed a reiterative conditional latents method that restores image-specific details sharply. The process involves two stages of iterations, reconstructing the image in the first stage, and refining image-specific details using conditional latent codes in the second stage. Our model successfully inverts out-of-domain images while preserving all details and supports InterfaceGAN, GANspace, and StyleClip for editing. We compare our approach with state-of-the-art GAN inversion methods on FFHQ (Flickr-Faces-HQ) Dataset, demonstrating significant improvements in inversion and editing quality.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323007653",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer vision",
      "Electrical engineering",
      "Encoder",
      "Engineering",
      "Fidelity",
      "Generative grammar",
      "High fidelity",
      "Image (mathematics)",
      "Inversion (geology)",
      "Operating system",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Structural basin",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Dere",
        "given_name": "Vedant Vasant"
      },
      {
        "surname": "Shinde",
        "given_name": "Amita"
      },
      {
        "surname": "Vast",
        "given_name": "Prachi"
      }
    ]
  },
  {
    "title": "Enhanced blind face inpainting via structured mask prediction",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.02.004",
    "abstract": "Blind face inpainting is the task of automatically recovering an occluded face image without given masks indicating missing areas. Popular inpainting methods assume that the occlusion patterns are known with given occlusion masks. Previous blind inpainting methods, ignoring the structure in faces and occlusions, treat occlusion detection as an independent pixel prediction problem. To overcome the limitations, we propose an enhanced two-stage blind face inpainting framework which consists of a structured mask prediction module and an inpainting module. The structured mask prediction module is first trained with a consensus loss for non-sparse and structured prediction. Then the inpainting module reconstructs the predicted missing areas and generates a visually plausible face image with a context normalization that enhances the robustness against prediction errors. We conducted experimental evaluations on the FFHQ and LFW datasets. The results demonstrate that our method is effective in producing visually convincing results with more continuous occlusion mask predictions and outperforms state-of-the-art methods in synthesized occluded face inpainting. Additionally, the method can effectively remove certain natural occlusions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524000370",
    "keywords": [
      "Anthropology",
      "Artificial intelligence",
      "Biochemistry",
      "Biology",
      "Cardiology",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Face (sociological concept)",
      "Gene",
      "Image (mathematics)",
      "Inpainting",
      "Medicine",
      "Normalization (sociology)",
      "Occlusion",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Pixel",
      "Robustness (evolution)",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Honglei"
      },
      {
        "surname": "Zhang",
        "given_name": "Yifan"
      },
      {
        "surname": "Wang",
        "given_name": "Wenmin"
      }
    ]
  },
  {
    "title": "Adversarial and focused training of abnormal videos for weakly-supervised anomaly detection",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110119",
    "abstract": "Due to the sparsity and scarcity of abnormal events, intra-video and inter-video data imbalance problems are fundamental issues for the weakly supervised video anomaly detection (WS-VAD) task. Many previous works have made great progress in the intra-video data imbalance problem while lacking attention to the inter-video case. However, we find that when reducing the number of abnormal videos used for training, the performance of some existing state-of-the-art WS-VAD methods will be decreased. To alleviate this problem, we propose a novel solution by adversarial and focused training (AFT) of abnormal videos. Specifically, our solution consists of two modules. One is a data-based adversarial training (AT) module that performs data augmentation through latent space-based adversarial sample generation of abnormal videos, and the other is a model-based focused training (FT) module that focuses on the cost-sensitive loss of abnormal videos. Once the whole pipeline has been trained, a score-level late fusion strategy is employed to combine the abnormal scores of both adversarial training and focused training modules in the testing phase. The effectiveness of the proposed approach is demonstrated on UCF-Crime, ShanghaiTech, XD-Violence, and UCSD Peds datasets in both the inter-video data imbalanced experimental setting and the original experimental setting. The source code is available at: https://github.com/Destind/AFT_codes .",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323008166",
    "keywords": [
      "Adversarial system",
      "Anomaly detection",
      "Artificial intelligence",
      "Code (set theory)",
      "Computer science",
      "Economics",
      "Machine learning",
      "Management",
      "Meteorology",
      "Pattern recognition (psychology)",
      "Physics",
      "Pipeline (software)",
      "Programming language",
      "Set (abstract data type)",
      "Task (project management)",
      "Training (meteorology)"
    ],
    "authors": [
      {
        "surname": "He",
        "given_name": "Ping"
      },
      {
        "surname": "Zhang",
        "given_name": "Fan"
      },
      {
        "surname": "Li",
        "given_name": "Gang"
      },
      {
        "surname": "Li",
        "given_name": "Huibin"
      }
    ]
  },
  {
    "title": "TMNet: Triple-modal interaction encoder and multi-scale fusion decoder network for V-D-T salient object detection",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110074",
    "abstract": "Salient object detection methods based on two-modal images have achieved remarkable success with the aid of image acquisition equipment. However, environmental factors often interfere with the Depth and Thermal maps, rendering them ineffective in providing object information. To address this weakness, we utilize the VDT dataset, which includes Visible, Depth, and Thermal images, and propose a triple-modal interaction encoder and multi-scale fusion decoder network (TMNet) to highlight the salient regions. The triple-modal interaction encoder comprises the separation context-aware feature module, channel-wise fusion module, and triple-modal refinement and fusion module, enabling us to fully explore and utilize the complementarity between Visible, Depth, and Thermal information. The multi-scale fusion decoder involves the semantic-aware localizing module and contour-aware refinement module to extract and fuse the location and boundary information, yielding a high-quality saliency map. Extensive experiments on the public VDT-2048 dataset demonstrate that our TMNet outperforms existing state-of-the-art methods in terms of all evaluation metrics.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323007719",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Electrical engineering",
      "Encoder",
      "Engineering",
      "Fuse (electrical)",
      "Fusion",
      "Linguistics",
      "Modal",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Polymer chemistry",
      "Salient"
    ],
    "authors": [
      {
        "surname": "Wan",
        "given_name": "Bin"
      },
      {
        "surname": "lv",
        "given_name": "Chengtao"
      },
      {
        "surname": "Zhou",
        "given_name": "Xiaofei"
      },
      {
        "surname": "Sun",
        "given_name": "Yaoqi"
      },
      {
        "surname": "Zhu",
        "given_name": "Zunjie"
      },
      {
        "surname": "Wang",
        "given_name": "Hongkui"
      },
      {
        "surname": "Yan",
        "given_name": "Chenggang"
      }
    ]
  },
  {
    "title": "Time pattern reconstruction for classification of irregularly sampled time series",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110075",
    "abstract": "Irregularly Sampled Time Series (ISTS) include partially observed feature vectors caused by the lack of temporal alignment across dimensions and the presence of variable time intervals. Especially in medical applications, because patients’ examinations depend on their health status, observations in this event-based medical time series are nonuniformly distributed. When using deep learning models to classify ISTS, most work defines the problem that needs to be solved as alignment-caused data missing or nonuniformity-caused dependency change. However, they only modeled relationships between observed values, ignoring the fact that time is the independent variable for a time series. In this paper, we emphasize that irregularity is active, time-depended, and class-associated and is reflected in the Time Pattern (TP). To this end, this paper focused on the TP of ISTS for the first time, proposing a Time Pattern Reconstruction (TPR) method. It first encodes time information by the time encoding mechanism, then imputes values from time codes by the continuous-discrete Kalman network, selects key time points by the conditional masking mechanism, and finally classifies ISTS based on the reconstructed TP. Experiments on four real-world medical datasets and three other datasets show that TPR outperforms all baselines. We also show that TP can reveal biomarkers and key time points for diseases.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323007720",
    "keywords": [
      "Aesthetics",
      "Art",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer security",
      "Conditional probability",
      "Data mining",
      "Dependency (UML)",
      "Encoding (memory)",
      "Event (particle physics)",
      "Feature (linguistics)",
      "Kalman filter",
      "Key (lock)",
      "Linguistics",
      "Machine learning",
      "Masking (illustration)",
      "Mathematical analysis",
      "Mathematics",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Series (stratigraphy)",
      "Statistics",
      "Temporal database",
      "Time point",
      "Time series",
      "Variable (mathematics)",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Sun",
        "given_name": "Chenxi"
      },
      {
        "surname": "Li",
        "given_name": "Hongyan"
      },
      {
        "surname": "Song",
        "given_name": "Moxian"
      },
      {
        "surname": "Cai",
        "given_name": "Derun"
      },
      {
        "surname": "Zhang",
        "given_name": "Baofeng"
      },
      {
        "surname": "Hong",
        "given_name": "Shenda"
      }
    ]
  },
  {
    "title": "H-CapsNet: A capsule network for hierarchical image classification",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110135",
    "abstract": "In this paper, we present H-CapsNet, a capsule network for hierarchical image classification. Our network makes use of the natural capacity of CapsNets (capsule networks) to capture hierarchical relationships. Thus, our network is such that each multi-layer capsule network accounts for each of the class hierarchies using dedicated capsules. Further, we make use of a modified hinge loss that enforces consistency amongst the hierarchies involved. We also present a strategy to dynamically adjust the training parameters to achieve a better balance between the class hierarchies under consideration. We have performed experiments using several widely available datasets and compared them against several alternatives. In our experiments, H-CapsNet delivers a margin of improvement over competing hierarchical classification networks elsewhere in the literature.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323008324",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Capsule",
      "Class (philosophy)",
      "Computer science",
      "Consistency (knowledge bases)",
      "Image (mathematics)",
      "Machine learning",
      "Margin (machine learning)",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Noor",
        "given_name": "Khondaker Tasrif"
      },
      {
        "surname": "Robles-Kelly",
        "given_name": "Antonio"
      }
    ]
  },
  {
    "title": "Deep self-enhancement hashing for robust multi-label cross-modal retrieval",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110079",
    "abstract": "The goal of cross-modal hashing is to map data from several modalities into a compact Hamming space for efficient and accurate retrieval. Despite the satisfactory performance, existing approaches are reliant on the closed-world assumption. When confronted with real-world retrieval tasks involving out-of-distribution (OOD) semantic data, the similarity relationships of known data retained in hash codes tend to be disrupted by these unknown ones, resulting in retrieval performance degradation. To this end, we present a deep self-enhancing hashing (DSEH) method, simultaneously learning multi-level similarity-preserved hash codes of the known multi-label cross-modal data and robustness to OOD instances. Specifically, we propose to construct pseudo-OOD samples in the feature space using random linear combinations to explore OOD semantics, during the training process. Meanwhile, a prototype-based generative model is incorporated to aggregate batch data to enhance the data representation’s differences in known and unknown semantics. Furthermore, we describe a bounded cosine quadrupled loss with distance bound to preserve the multi-level similarity of multi-label data and control the maximum distance between known data and the minimum distance between known and pseudo-OOD data for learning OOD robustness. Extensive experiments show that the DSEH achieves state-of-the-art performance on closed-world tasks and good performance on simulated real-world tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323007768",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Block code",
      "Chemistry",
      "Computer science",
      "Computer security",
      "Data mining",
      "Decoding methods",
      "Double hashing",
      "Feature hashing",
      "Feature learning",
      "Feature vector",
      "Gene",
      "Hamming code",
      "Hamming space",
      "Hash function",
      "Hash table",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Robustness (evolution)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Song",
        "given_name": "Ge"
      },
      {
        "surname": "Su",
        "given_name": "Hanwen"
      },
      {
        "surname": "Huang",
        "given_name": "Kai"
      },
      {
        "surname": "Song",
        "given_name": "Fengyi"
      },
      {
        "surname": "Yang",
        "given_name": "Ming"
      }
    ]
  },
  {
    "title": "Scalable and accurate subsequence transform for time series classification",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110121",
    "abstract": "Time series classification using phase-independent subsequences called shapelets is one of the best approaches in the state of the art. This approach is especially characterized by its interpretable property and its fast prediction time. However, given a dataset of n time series of length at most m , learning shapelets requires a computation time of O ( n 2 m 4 ) which is too high for practical datasets. In this paper, we exploit the fact that shapelets are shared by the members of the same class to propose the SAST (Scalable and Accurate Subsequence Transform) algorithm which has a time complexity of O ( n m 3 ) . SAST is accurate, interpretable and does not learn redundant shapelets. The experiments we conducted on the UCR archive datasets showed that SAST is more accurate than the state of the art Shapelet Transform algorithm while being significantly more scalable.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032300818X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Bounded function",
      "Class (philosophy)",
      "Computation",
      "Computer science",
      "Database",
      "Mathematical analysis",
      "Mathematics",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Scalability",
      "Series (stratigraphy)",
      "Subsequence"
    ],
    "authors": [
      {
        "surname": "Mbouopda",
        "given_name": "Michael Franklin"
      },
      {
        "surname": "Mephu Nguifo",
        "given_name": "Engelbert"
      }
    ]
  },
  {
    "title": "Gauss-like Logarithmic Kernel Function to improve the performance of kernel machines on the small datasets",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.01.014",
    "abstract": "Support vector machine is one of the most used machine learning algorithms with a comprehensive mathematical infrastructure. The power behind the algorithm is the kernel trick that enables the model to overcome non-linear data distributions by using functions that satisfy the Mercer condition. Undoubtedly, the radial basis function (RBF) is among the most widely used of these functions. The RBF kernel, which has a Gaussian curve, performs local boundary surfaces and supports the generalization capability of the model. In this study, a novel kernel function named Logarithmic Kernel Function (LKF), which has a Gaussian-like curve is presented. The crucial contribution of the LKF is that it can model the dataset better than other similarly shaped kernels in the case of a few training samples (10% and 30%). In the study, 6 classifications and 5 regression sets are handled to compare kernels. Instead of giving the hyperparameters needed by the models manually, they are estimated through the Tree Parzen Estimator, which is based on Sequential Modeling Optimization. This estimation process is repeated 10 times and the average results are obtained. The proposed LKF surpasses the most competitive kernels in various classification and regression tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552400014X",
    "keywords": [
      "Algorithm",
      "Applied mathematics",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Discrete mathematics",
      "Evolutionary biology",
      "Function (biology)",
      "Gauss",
      "Kernel (algebra)",
      "Kernel embedding of distributions",
      "Kernel method",
      "Kernel smoother",
      "Logarithm",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Radial basis function kernel",
      "Support vector machine",
      "Variable kernel density estimation"
    ],
    "authors": [
      {
        "surname": "Hicdurmaz",
        "given_name": "Betul"
      },
      {
        "surname": "Calik",
        "given_name": "Nurullah"
      },
      {
        "surname": "Ustebay",
        "given_name": "Serpil"
      }
    ]
  },
  {
    "title": "OAMatcher: An overlapping areas-based network with label credibility for robust and accurate feature matching",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110094",
    "abstract": "Local feature matching involves establishing accurate pixel-wise correspondences between an image pair, which is a critical component in several visual applications (e.g., visual localization). Recently, detector-free techniques have realized excellent performance in this task. However, existing methods tend to focus on the entire image without prioritizing overlapping regions, resulting in undesirable interference from non-overlapping areas during the descriptors enhancement process. Moreover, these approaches neglect unreliable ground-truth matching labels triggered by measurement noise in datasets, leading to sub-optimal network optimization. In this study, we develop a novel overlapping areas-based network OAMatcher to resolve these issues. For the first issue, OAMatcher employs an overlapping regions perception block (ORPB) that captures the overlapping areas of image pairs to filter out plentiful mismatches and circumvent interference from non-overlapping regions during descriptors enhancement process. Specifically, the ORPB first enhances the descriptors of all keypoints to mimic the human behaviour of scrutinizing entire images back and forth at the start of feature matching. Subsequently, the ORPB introduces an overlapping regions extraction block (OREB) that captures the keypoints within overlapping zones to mimic the humans behaviour of shifting the focus from the whole images to co-visible areas. After OREB, ORPB performs descriptors enhancement exclusively among the keypoints within these co-visible regions, ensuring minimal disturbances from non-overlapping areas. In addition, the ORPB confines the predicted matches strictly to co-visible regions, thus efficiently filtering out a significant number of mismatches in non-overlapping zones. For the second issue, OAMatcher proposes a labels weighting algorithm (LWA) that predicts the label credibility for ground-truth matching labels. LWA assigns low credibility to unreliable labels and utilizes the credibility to weight loss, effectively diminishing the influence of unreliable labels. Extensive experiments show that OAMatcher delivers excellent results for homography estimation, pose estimation, and visual localization tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323007914",
    "keywords": [
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Filter (signal processing)",
      "Focus (optics)",
      "Geometry",
      "Linguistics",
      "Matching (statistics)",
      "Mathematics",
      "Medicine",
      "Operating system",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Pixel",
      "Process (computing)",
      "Radiology",
      "Statistics",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Dai",
        "given_name": "Kun"
      },
      {
        "surname": "Xie",
        "given_name": "Tao"
      },
      {
        "surname": "Wang",
        "given_name": "Ke"
      },
      {
        "surname": "Jiang",
        "given_name": "Zhiqiang"
      },
      {
        "surname": "Li",
        "given_name": "Ruifeng"
      },
      {
        "surname": "Zhao",
        "given_name": "Lijun"
      }
    ]
  },
  {
    "title": "Listwise learning to rank method combining approximate NDCG ranking indicator with Conditional Generative Adversarial Networks",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.01.015",
    "abstract": "Some previous empirical studies have shown that the performances of the listwise learning to rank approaches are in general better than the pointwise or pairwise learning to rank techniques. The listwise learning to rank methods which directly optimize information retrieval indicators are a type of essential and popular method of learning to rank. However, the existing learning to rank approaches based on Generative Adversarial Networks (GAN) do not utilize a loss function based on information retrieval indicators to optimize the generator and/or discriminator. Thus, an approach of learning to rank that combines approximate Normalized Discounted Cumulative Gain (NDCG) ranking indicators with Conditional Generative Adversarial Networks (CGAN) is proposed in this paper, named NCGAN-LTR. The NCGAN-LTR approach constructs loss functions of the generator and discriminator based on the Plackett-Luce model and an approximate version of the NDCG ranking indicator, which is utilized to train the network parameters of CGAN. The experimental results on four benchmark datasets of learning to rank, i.e., TREC TD2004, OHSUMED, MQ2008, and MSLR-WEB10K demonstrate that our proposed NCGAN-LTR approach has superior performance across almost various ranking indicators of information retrieval compared with the IRGAN-List approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524000151",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Deep learning",
      "Generative adversarial network",
      "Generative grammar",
      "Learning to rank",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Rank (graph theory)",
      "Ranking (information retrieval)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Jinzhong"
      },
      {
        "surname": "Zeng",
        "given_name": "Huan"
      },
      {
        "surname": "Xiao",
        "given_name": "Cunwei"
      },
      {
        "surname": "Ouyang",
        "given_name": "Chunjuan"
      },
      {
        "surname": "Liu",
        "given_name": "Hua"
      }
    ]
  },
  {
    "title": "Example forgetting and rehearsal in continual learning",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.01.021",
    "abstract": "A major challenge of training neural networks on different tasks in a sequential manner is catastrophic forgetting, where earlier experiences are forgotten while learning a new one. In recent years, rehearsal-based methods have become popular top-performing alleviation approaches. Rehearsal builds upon maintaining and repeatedly using for training a small buffer of data selected across encountered tasks. In this work, we examine in image classification whether all training examples are forgotten equally and which ones are worth keeping in the memory. Two different statistics of forgettableness are employed to rank examples based on them. We propose a simple strategy for example selection: keeping the least forgettable examples according to precomputed or continually updated forgetting statistics. Despite the simplicity of this method, it achieves better results compared to different memory-management strategies on standard benchmarks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524000217",
    "keywords": [
      "Artificial intelligence",
      "Cognitive psychology",
      "Combinatorics",
      "Computer science",
      "Epistemology",
      "Forgetting",
      "Machine learning",
      "Mathematics",
      "Philosophy",
      "Psychology",
      "Rank (graph theory)",
      "Selection (genetic algorithm)",
      "Simple (philosophy)",
      "Simplicity"
    ],
    "authors": [
      {
        "surname": "Benkő",
        "given_name": "Beatrix"
      }
    ]
  },
  {
    "title": "CS-net: Conv-simpleformer network for agricultural image segmentation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110140",
    "abstract": "Agricultural image segmentation needs to catch up to the development speed of deep learning, and the explosive computational overhead and limited high-quality labeled datasets are the main reasons preventing the application of Transformers to agricultural image segmentation. This study proposes a Simple-Attention Block (SIAB) using channel-by-channel and spatial convolutional computation, whose computational complexity is linearly correlated with the input image size. Then, we design a Simpleformer by cascading SIAB and FFN to reshape the Transformer architecture. Further, the fusion of CNN and Simpleformer constructs a dataset quality-independent agricultural image segmentation model (CS-Net). Finally, we evaluate CS-Net on four datasets, and compared with the state-of-the-art models, CS-Net has more advantageous inference speed and segmentation accuracy, which pushes the development of Transformers in the field of agricultural image processing. Additionally, we explore the reasons for the Transformers’ performance collapse for agricultural applications, providing research scholars with a theoretical foundation for related issues.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323008373",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computation",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Electrical engineering",
      "Engineering",
      "Image segmentation",
      "Inference",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Segmentation",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Lei"
      },
      {
        "surname": "Li",
        "given_name": "Guorun"
      },
      {
        "surname": "Du",
        "given_name": "Yuefeng"
      },
      {
        "surname": "Li",
        "given_name": "Xiaoyu"
      },
      {
        "surname": "Wu",
        "given_name": "Xiuheng"
      },
      {
        "surname": "Qiao",
        "given_name": "Zhi"
      },
      {
        "surname": "Wang",
        "given_name": "Tianyi"
      }
    ]
  },
  {
    "title": "A linear transportation L p distance for pattern recognition",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110080",
    "abstract": "The transportation L p distance, denoted TL p , has been proposed as a generalisation of Wasserstein W p distances motivated by the property that it can be applied directly to colour or multi-channelled images, as well as multivariate time-series without normalisation or mass constraints. Both TL p and W p assign a cost based on the transport distance (i.e. the “Lagrangian” model), the key difference between the distances is that TL p interprets the signal as a function whilst W p interprets the signal as a measure. Both distances are powerful tools in modelling data with spatial or temporal perturbations. However, their computational cost can make them infeasible to apply to even moderate pattern recognition tasks. The linear Wasserstein distance was proposed as a method for projecting signals into a Euclidean space where the Euclidean distance is approximately the Wasserstein distance (more formally, this is a projection on to the tangent manifold). We propose linear versions of the TL p distance ( L TL p ) and we show significant improvement over the linear W p distance on signal processing tasks, whilst being several orders of magnitude faster to compute than the TL p distance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032300777X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Combinatorics",
      "Computer science",
      "Computer security",
      "Data mining",
      "Euclidean distance",
      "Euclidean space",
      "Evolutionary biology",
      "Function (biology)",
      "Geometry",
      "Key (lock)",
      "Mathematics",
      "Measure (data warehouse)",
      "Programming language",
      "Projection (relational algebra)",
      "SIGNAL (programming language)",
      "Tangent"
    ],
    "authors": [
      {
        "surname": "Crook",
        "given_name": "Oliver M."
      },
      {
        "surname": "Cucuringu",
        "given_name": "Mihai"
      },
      {
        "surname": "Hurst",
        "given_name": "Tim"
      },
      {
        "surname": "Schönlieb",
        "given_name": "Carola-Bibiane"
      },
      {
        "surname": "Thorpe",
        "given_name": "Matthew"
      },
      {
        "surname": "Zygalakis",
        "given_name": "Konstantinos C."
      }
    ]
  },
  {
    "title": "Spatio-temporal human action localization in indoor surveillances",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110087",
    "abstract": "Spatio-temporal action localization is a crucial and challenging task in the field of video understanding. Existing benchmarks for spatio-temporal action detection are limited by factors such as incomplete annotations, high-level non-universal actions, and uncommon scenarios. To address these limitations and facilitate research in real-world security applications, we introduce a novel human-centric dataset for spatio-temporal localization of atomic actions in indoor surveillance settings, termed as HIA (Human-centric Indoor Actions). The HIA dataset is constructed by selecting 30 atomic action classes, compiling 100 surveillance videos, and annotating 219,225 frames with 370,937 bounding boxes. The primary characteristics of HIA include (1) accurate spatio-temporal annotations for atomic actions, (2) human-centric annotations at the frame level, (3) temporal linking of persons across discontinuous tracks, and (4) utilization of indoor surveillance videos. Our HIA, with its realistic settings in indoor surveillance scenes and comprehensive annotations, presents a valuable and novel challenge to the spatio-temporal action localization domain. To establish a benchmark, we evaluate various methods and provide an in-depth analysis of the HIA dataset. The HIA dataset will be made available soon, and we anticipate that it will serve as a standard and practical benchmark for the research community.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323007847",
    "keywords": [
      "Action (physics)",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Bounding overwatch",
      "Cartography",
      "Computer science",
      "Domain (mathematical analysis)",
      "Economics",
      "Field (mathematics)",
      "Frame (networking)",
      "Geography",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Physics",
      "Pure mathematics",
      "Quantum mechanics",
      "Task (project management)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Zihao"
      },
      {
        "surname": "Yan",
        "given_name": "Danfeng"
      },
      {
        "surname": "Cai",
        "given_name": "Yuanqiang"
      },
      {
        "surname": "Song",
        "given_name": "Yan"
      }
    ]
  },
  {
    "title": "A general elevating framework for label noise filters",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110072",
    "abstract": "In real applications, label noise has a great influence on data modeling. As one kind of label noise treatment method, noise filter has attracted extensive attention recently. The existing filters perform well in dealing with label noise completely at random (NCAR) but poorly in dealing with the label noise in the form of clusters (LLC). Besides, the existing filters may over remove the samples located at the classification boundaries, thereby affecting the generalization performance of classifiers. To fill these gaps, we propose a general elevating framework for label noise filters. The core idea of the framework is to improve the filtering performance of the existing filters by sample reduction. Specifically, since most of the existing filters are based on classifier prediction, and the complexity of samples at the boundary will affect the prediction performance of classifiers. Therefore, reducing the complexity of the boundary sample is very helpful to improve the performance of the filters. To this end, we propose a sample reduction method, which can not only reduce the complexity of the sample at the boundary but also convert LLC to NCAR, to get some representative samples. Next, the filter based on classifier prediction is employed to recognize the noisy representative samples. Finally, the noisy labeled samples in the given data set are found according to the identified noisy representatives. Furthermore, through empirical analysis, we found that compared with some classical metrics for evaluating the performance of noise filters, classification accuracy is more suitable to measure the performance of filters. Exhaustive experiments testify the validity of the framework, and the experimental results demonstrate that the performance of our framework is especially outstanding for LLC treatment.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323007690",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Image (mathematics)",
      "Noise (video)"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Qingqiang"
      },
      {
        "surname": "Jiang",
        "given_name": "Gaoxia"
      },
      {
        "surname": "Cao",
        "given_name": "Fuyuan"
      },
      {
        "surname": "Men",
        "given_name": "Changqian"
      },
      {
        "surname": "Wang",
        "given_name": "Wenjian"
      }
    ]
  },
  {
    "title": "A temporal densely connected recurrent network for event-based human pose estimation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110048",
    "abstract": "Event camera is an emerging bio-inspired vision sensors that report per-pixel brightness changes asynchronously. It holds noticeable advantage of high dynamic range, high speed response, and low power budget that enable it to best capture local motions in uncontrolled environments. This motivates us to unlock the potential of event cameras for human pose estimation, as the human pose estimation with event cameras is rarely explored. Due to the novel paradigm shift from conventional frame-based cameras, however, event signals in a time interval contain very limited information, as event cameras can only capture the moving body parts and ignores those static body parts, resulting in some parts to be incomplete or even disappeared in the time interval. This paper proposes a novel densely connected recurrent architecture to address the problem of incomplete information. By this recurrent architecture, we can explicitly model not only the sequential but also non-sequential geometric consistency across time steps to accumulate information from previous frames to recover the entire human bodies, achieving a stable and accurate human pose estimation from event data. Moreover, to better evaluate our model, we collect a large-scale multimodal event-based dataset that comes with human pose annotations, which is by far the most challenging one to the best of our knowledge. The experimental results on two public datasets and our own dataset demonstrate the effectiveness and strength of our approach. Code is available online for facilitating the future research.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323007458",
    "keywords": [
      "Artificial intelligence",
      "Code (set theory)",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Consistency (knowledge bases)",
      "Event (particle physics)",
      "Frame (networking)",
      "Interval (graph theory)",
      "Mathematics",
      "Physics",
      "Pose",
      "Programming language",
      "Quantum mechanics",
      "Set (abstract data type)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Shao",
        "given_name": "Zhanpeng"
      },
      {
        "surname": "Wang",
        "given_name": "Xueping"
      },
      {
        "surname": "Zhou",
        "given_name": "Wen"
      },
      {
        "surname": "Wang",
        "given_name": "Wuzhen"
      },
      {
        "surname": "Yang",
        "given_name": "Jianyu"
      },
      {
        "surname": "Li",
        "given_name": "Youfu"
      }
    ]
  },
  {
    "title": "DCapsNet: Deep capsule network for human activity and gait recognition with smartphone sensors",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110054",
    "abstract": "Recently, deep neural networks are used to recognize human activity/gait through mobile sensors which have attracted a great attention. Although the existing deep neural networks that perform automatic feature extraction have achieved desirable performance, their classification accuracy needs to be improved. In this paper, a deep neural network that combines a set of convolutional layers and capsule network is proposed. The proposed architecture named DCapsNet is suited to automatically extract the activity or gait features through built in sensors and classify them. The convolutional layers of the DCapsNet are more suitable for processing temporal sequences and provide scalar outputs but not the equivariance. The capsule network (CapsNet) is then trained by a dynamic routing algorithm to capture the equivariance having a magnitude and orientation, which increases the efficiency of the model classification. The performance of the proposed model is evaluated on four public datasets: two HAR datasets (UCI-HAR and WISDM) and two gait datasets (WhuGAIT). The recognition accuracy of the proposed model for the UCI-HAR and WISDM datasets are 97.92 % and 99.30 %, respectively, and for the WhuGAIT Dataset #1 and Dataset #2 are 94.75 % and 97.16 %, respectively. Experimental results show that the proposed model achieves the highest recognition accuracy over the reported results of the state-of-the-art models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323007513",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Deep learning",
      "Feature extraction",
      "Gait",
      "Geometry",
      "Mathematics",
      "Orientation (vector space)",
      "Pattern recognition (psychology)",
      "Physiology"
    ],
    "authors": [
      {
        "surname": "Sezavar",
        "given_name": "Ahmadreza"
      },
      {
        "surname": "Atta",
        "given_name": "Randa"
      },
      {
        "surname": "Ghanbari",
        "given_name": "Mohammed"
      }
    ]
  },
  {
    "title": "UniG-Encoder: A universal feature encoder for graph and hypergraph node classification",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110115",
    "abstract": "Despite the decent performance and fruitful applications of Graph Neural Networks (GNNs), Hypergraph Neural Networks (HGNNs), and their well-designed variants, on some commonly used benchmark graphs and hypergraphs, they are outperformed by even a simple Multi-Layer Perceptron. This observation motivates a reexamination of the design paradigm of the current GNNs and HGNNs and poses challenges of extracting graph features effectively. In this work, a universal feature encoder for both graph and hypergraph representation learning is designed, called UniG-Encoder. The architecture starts with a forward transformation of the topological relationships of connected nodes into edge or hyperedge features via a normalized projection matrix. The resulting edge/hyperedge features, together with the original node features, are fed into a neural network. The encoded node embeddings are then derived from the reversed transformation, described by the transpose of the projection matrix, of the network’s output, which can be further used for tasks such as node classification. The designed projection matrix, encoding the graph features, is intuitive and interpretable. The proposed architecture, in contrast to the traditional spectral-based and/or message passing approaches, simultaneously and comprehensively exploits the node features and graph/hypergraph topologies in an efficient and unified manner, covering both heterophilic and homophilic graphs. Furthermore, a variant version, UniG-Encoder II, is devised to leverage multi-hop node information. Extensive experiments are conducted and demonstrate the superior performance of the proposed framework on twelve representative hypergraph datasets and six real-world graph datasets, compared to the state-of-the-art methods. Our implementation is available online at https://github.com/MinhZou/UniG-Encoder.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323008129",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Autoencoder",
      "Computer science",
      "Discrete mathematics",
      "Encoder",
      "Graph",
      "Hypergraph",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Zou",
        "given_name": "Minhao"
      },
      {
        "surname": "Gan",
        "given_name": "Zhongxue"
      },
      {
        "surname": "Wang",
        "given_name": "Yutong"
      },
      {
        "surname": "Zhang",
        "given_name": "Junheng"
      },
      {
        "surname": "Sui",
        "given_name": "Dongyan"
      },
      {
        "surname": "Guan",
        "given_name": "Chun"
      },
      {
        "surname": "Leng",
        "given_name": "Siyang"
      }
    ]
  },
  {
    "title": "Data-efficient 3D instance segmentation by transferring knowledge from synthetic scans",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.02.001",
    "abstract": "The 3D comprehension ability of indoor environments is critical for robots. While deep learning-based methods have improved performance, they require significant amounts of annotated training data. Nevertheless, the cost of scanning and annotating point cloud data in real scenes is high, leading to data scarcity. Consequently, there is an urgent need to investigate data-efficient methods for point cloud instance segmentation. To tackle this issue, we propose to leverage the geometric and scene context knowledge inherent in synthetic data to reduce the need for annotation on real data. Specifically, we simulate the process of human scanning and collecting point cloud data in real-world scenes and construct three large-scale synthetic point cloud datasets using synthetic scenes. The scale of these three datasets is more than ten times that of currently available real-world data. Experimental results demonstrate that the incorporation of synthetic point cloud data can increase instance segmentation performance by over 18.8 percentage points. Further, to address the problem of domain shift between synthetic and real data, we propose a target-aware pre-training method. It integrates both real and synthetic data during the pre-training process, allowing the model to learn a feature representation that can effectively generalize to downstream real data. Experiments show that our method achieved stable improvements on all three synthetic datasets. The data and code will be publicly available in the future.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524000345",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Data mining",
      "Leverage (statistics)",
      "Machine learning",
      "Operating system",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Point cloud",
      "Process (computing)",
      "Segmentation",
      "Synthetic data"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Xiaodong"
      },
      {
        "surname": "Wang",
        "given_name": "Ruiping"
      },
      {
        "surname": "Chen",
        "given_name": "Xilin"
      }
    ]
  },
  {
    "title": "Distance-based Weighted Transformer Network for image completion",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110120",
    "abstract": "The challenge of image generation has been effectively modeled as a problem of structure priors or transformation. However, existing models have unsatisfactory performance in understanding the global input image structures because of particular inherent features (for example, local inductive prior). Recent studies have shown that self-attention is an efficient modeling technique for image completion problems. In this paper, we propose a new architecture that relies on Distance-based Weighted Transformer (DWT) to better understand the relationships between an image’s components. In our model, we leverage the strengths of both Convolutional Neural Networks (CNNs) and DWT blocks to enhance the image completion process. Specifically, CNNs are used to augment the local texture information of coarse priors and DWT blocks are used to recover certain coarse textures and coherent visual structures. Unlike current approaches that generally use CNNs to create feature maps, we use the DWT to encode global dependencies and compute distance-based weighted feature maps, which substantially minimizes the problem of visual ambiguities. Meanwhile, to better produce repeated textures, we introduce Residual Fast Fourier Convolution (Res-FFC) blocks to combine the encoder’s skip features with the coarse features provided by our generator. Furthermore, a simple yet effective technique is proposed to normalize the non-zero values of convolutions, and fine-tune the network layers for regularization of the gradient norms to provide an efficient training stabilizer. Extensive quantitative and qualitative experiments on three challenging datasets demonstrate the superiority of our proposed model compared to existing approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323008178",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Bayesian probability",
      "Computer science",
      "Convolutional neural network",
      "Encoder",
      "Leverage (statistics)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Prior probability",
      "Residual"
    ],
    "authors": [
      {
        "surname": "Shamsolmoali",
        "given_name": "Pourya"
      },
      {
        "surname": "Zareapoor",
        "given_name": "Masoumeh"
      },
      {
        "surname": "Zhou",
        "given_name": "Huiyu"
      },
      {
        "surname": "Li",
        "given_name": "Xuelong"
      },
      {
        "surname": "Lu",
        "given_name": "Yue"
      }
    ]
  },
  {
    "title": "Learning a target-dependent classifier for cross-domain semantic segmentation: Fine-tuning versus meta-learning",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110091",
    "abstract": "Recently proposed domain adaptation arts have dominated the field of cross-domain semantic segmentation by operating domain manifolds alignment and learning an optimal joint hypothesis (joint-domain classifier) for both source and target domains. However, a joint-domain classifier can still violate the cluster assumption in the target domain in case domain manifolds are not fully aligned after domain adaptation. In this work, we raise the intractability of perfect domain alignment and turn to exploit a novel hypothesis: a target-dependent classifier, to efficiently adapt to the target domain clusters even given a certain degree of domain misalignment. Specifically, we first propose an unsupervised fine-tuning strategy, which optimizes the joint hypothesis of vanilla domain adaptation into a target-dependent hypothesis to better fit with the target domain clusters. Second, we connect the “learning to learn” concept of meta-learning with pixel-wise domain adaptation, which serves as a reliable hypothesis initialization, providing an alternative solution to learning a more generalized target-dependent classifier. The proposed learning method is general to conventional domain adaptation models. In experiments, we recycle the pre-trained conventional DA models and learn target-dependent classifiers with the proposed method. Experimental results on synthetic-to-real adaptation and cross-city adaptation benchmarks demonstrate that the target-dependent classifier leads over state-of-the-art performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323007884",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Computer security",
      "Domain adaptation",
      "Exploit",
      "Initialization",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Programming language",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Tian",
        "given_name": "Haitao"
      },
      {
        "surname": "Qu",
        "given_name": "Shiru"
      },
      {
        "surname": "Payeur",
        "given_name": "Pierre"
      }
    ]
  },
  {
    "title": "Less is more: A minimalist approach to robust GAN-generated face detection",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.02.017",
    "abstract": "Hyper-realistic images that are not differentiable from authentic images to regular viewers have become extremely easy to generate and highly accessible. Furthermore, the increasing pervasiveness of social media networks in our daily lives has facilitated the easy dissemination of fake news accompanied by such synthetic images. Hyper-realistic artificial face images are often illicitly used as profile pictures on social media sites, further using such profiles to spread fabricated information, resulting in social perils. Most available synthetic image detectors are challenging to implement in practical scenarios due to their high complexity and performance degradation for images from Online Social Networks (OSNs). In this work, we develop a deep learning-based lightweight synthetic image detector called Relative Chrominance Distance Network (RCD-Net). In this paper, we introduce the RCD image feature set for the first time, which gives a pair-wise chrominance component-based distance measure. To show its effectiveness, we explore multiple luminance-chrominance spaces. Compared to the state-of-the-art (SOTA), our model hugely reduces the network parameter requirements, making it incredibly lightweight. We also study the robustness of the proposed solution against common post-processing operations in the context of online social media networks. Experimental results prove that the proposed solution achieves SOTA performance at a much lower complexity than available solutions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524000485",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Face (sociological concept)",
      "Face detection",
      "Facial recognition system",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Ghosh",
        "given_name": "Tanusree"
      },
      {
        "surname": "Naskar",
        "given_name": "Ruchira"
      }
    ]
  },
  {
    "title": "Adaptive proposal network based on generative adversarial learning for weakly supervised temporal sentence grounding",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.01.018",
    "abstract": "Temporal sentence grounding aims to locate the moment most related to the given natural language query. Noticing the time-consuming labeling process of the temporal bounding boxes, recent works started to focus on the weakly supervised temporal sentence grounding (WTSG) with only video-text pairwise annotations. Existing WTSG methods mainly adopted anchor-based structure to generate moment candidates and trained the network with triplet loss between the positive and negative samples, and thus the network performance was seriously affected by the preset anchors and the loss margin. In this paper, we propose a novel contrastive generative adversarial learning method with adaptive box generation for WTSG tasks. Specifically, the temporal proposals are adaptively generated by a transformer-based box generator in a complete anchor-free manner. And a novel contrastive generative adversarial learning process is proposed for the network optimization, which can effectively encourages the separation of the positive and negative samples without preset margin value. Extensive experiments indicate that our method achieves the state-of-the-art performance on both of the Charades-STA and the ActivityNet Captions datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524000175",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Computer science",
      "Electrical engineering",
      "Engineering",
      "Generative grammar",
      "Ground",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Sentence"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Weikang"
      },
      {
        "surname": "Su",
        "given_name": "Yuting"
      },
      {
        "surname": "Liu",
        "given_name": "Jing"
      },
      {
        "surname": "Jing",
        "given_name": "Peiguang"
      }
    ]
  },
  {
    "title": "Deep intra-image contrastive learning for weakly supervised one-step person search",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110047",
    "abstract": "Weakly supervised person search aims to perform joint pedestrian detection and re-identification (re-id) with only bounding-box annotations. Recently, the idea of contrastive learning is initially applied to weakly supervised person search, where two common contrast strategies are memory-based contrast and intra-image contrast. We argue that current intra-image contrast is shallow, which suffers from spatial-level and occlusion-level variance. In this paper, we present a novel deep intra-image contrastive learning using a Siamese network. Two key modules are spatial-invariant contrast (SIC) and occlusion-invariant contrast (OIC). SIC performs many-to-one contrasts between two branches of Siamese network and dense prediction contrasts in one branch of Siamese network. With these many-to-one and dense contrasts, SIC tends to learn discriminative scale-invariant and location-invariant features to solve spatial-level variance. OIC enhances feature consistency with the masking strategy to learn occlusion-invariant features. Extensive experiments are performed on two person search datasets. Our method achieves a state-of-the-art performance among weakly supervised one-step person search approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323007446",
    "keywords": [
      "Artificial intelligence",
      "Bounding overwatch",
      "Computer science",
      "Contrast (vision)",
      "Discriminative model",
      "Invariant (physics)",
      "Mathematical physics",
      "Mathematics",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Jiabei"
      },
      {
        "surname": "Pang",
        "given_name": "Yanwei"
      },
      {
        "surname": "Cao",
        "given_name": "Jiale"
      },
      {
        "surname": "Sun",
        "given_name": "Hanqing"
      },
      {
        "surname": "Shao",
        "given_name": "Zhuang"
      },
      {
        "surname": "Li",
        "given_name": "Xuelong"
      }
    ]
  },
  {
    "title": "JoyPose: Jointly learning evolutionary data augmentation and anatomy-aware global–local representation for 3D human pose estimation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110116",
    "abstract": "Video-based 3D human pose estimation is an important yet challenging task for many human-involved pattern recognition systems. Existing deep learning-based 3D human pose estimation methods are faced with the problems of lacking large-scale training data and lacking effective solutions to represent the complicated human body structure. To this end, this paper proposes a jointly learning framework entitled JoyPose that simultaneously leverages both human pose data augmentation and human pose estimation. In particular, JoyPose consists of an evolutionary data augmentation module and an anatomy-aware global–local pose feature representation module for 3D human pose estimation. The evolution for data augmentation is guided by a reinforcement learning strategy in a probabilistic way according to pose estimation loss. The anatomy-aware global–local pose feature representation module separately captures global features and local features according to anatomical and kinematic patterns observed from pose estimation errors across different human joints. The performance of the final human pose estimation is leveraged by both data augmentation and anatomy-aware global–local feature representation. Extensive experiments on three real-world datasets demonstrate the superiority and robustness against state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323008130",
    "keywords": [
      "3D pose estimation",
      "Articulated body pose estimation",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Feature (linguistics)",
      "Feature learning",
      "Gene",
      "Law",
      "Linguistics",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Pose",
      "Probabilistic logic",
      "Representation (politics)",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Du",
        "given_name": "Songlin"
      },
      {
        "surname": "Yuan",
        "given_name": "Zhiwei"
      },
      {
        "surname": "Lai",
        "given_name": "Peifu"
      },
      {
        "surname": "Ikenaga",
        "given_name": "Takeshi"
      }
    ]
  },
  {
    "title": "Text-guided Fourier Augmentation for long-tailed recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.01.017",
    "abstract": "Real-world data often exhibits a long-tailed distribution in practical scenarios. However, deep learning models usually face challenges when it comes to effectively identifying infrequent classes amidst the abundance of prevalent ones. The fundamental issue lies in the scarcity of available information for tail classes. A highly intuitive approach is to uncover a greater amount of valuable information specifically tailored to these tail classes. We find that textual information of class names and frequency domain information of images are ignored by previous works in long-tailed visual recognition. Therefore, we propose a Text-Guided Fourier Augmentation (TGFA) method with the aid of language models and the Fourier transform to excavate more useful information for tail classes. Extensive experiments demonstrate that our proposed method effectively enriches training data on-the-fly, allowing for an end-to-end one-stage supervised contrastive learning framework that surpasses other methods including two-stage or multi-experts methods, in terms of efficiency and performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524000199",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Fourier transform",
      "Mathematical analysis",
      "Mathematics",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Weiqiu"
      },
      {
        "surname": "Chen",
        "given_name": "Zining"
      },
      {
        "surname": "Su",
        "given_name": "Fei"
      },
      {
        "surname": "Zhao",
        "given_name": "Zhicheng"
      }
    ]
  },
  {
    "title": "On characterizing the evolution of embedding space of neural networks using algebraic topology",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.02.003",
    "abstract": "We study how the topology of feature embedding space changes as it passes through the layers of a well-trained deep neural network (DNN) through Betti numbers. Motivated by existing studies using simplicial complexes on shallow fully connected networks (FCN), we present an extended analysis using Cubical homology instead, with a variety of popular deep architectures and real image datasets. We demonstrate that as depth increases, a topologically complicated dataset is transformed into a simple one, resulting in Betti numbers attaining their lowest possible value. The rate of decay in topological complexity (as a metric) helps quantify the impact of architectural choices on the generalization ability. Interestingly from a representation learning perspective, we highlight several invariances such as topological invariance of (1) an architecture on similar datasets; (2) embedding space of a dataset for architectures of variable depth; (3) embedding space to input resolution/size, and (4) data sub-sampling. In order to further demonstrate the link between expressivity & the generalization capability of a network, we consider the task of ranking pre-trained models for downstream classification task (transfer learning). Compared to existing approaches, the proposed metric has a better correlation to the actually achievable accuracy via fine-tuning the pre-trained model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524000369",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Betti number",
      "Combinatorics",
      "Computer science",
      "Discrete mathematics",
      "Economics",
      "Embedding",
      "Generalization",
      "Mathematical analysis",
      "Mathematics",
      "Metric (unit)",
      "Operations management",
      "Pattern recognition (psychology)",
      "Persistent homology",
      "Theoretical computer science",
      "Topology (electrical circuits)"
    ],
    "authors": [
      {
        "surname": "Suresh",
        "given_name": "S."
      },
      {
        "surname": "Das",
        "given_name": "B."
      },
      {
        "surname": "Abrol",
        "given_name": "V."
      },
      {
        "surname": "Dutta Roy",
        "given_name": "S."
      }
    ]
  },
  {
    "title": "F-SCP: An automatic prompt generation method for specific classes based on visual language pre-training models",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110096",
    "abstract": "The zero-shot classification performance of large-scale vision-language pre-training models (e.g., CLIP, BLIP and ALIGN) can be enhanced by incorporating a prompt (e.g., “a photo of a [CLASS]”) before the class words. Modifying the prompt slightly can have significant effect on the classification outcomes of these models. Thus, it is crucial to include an appropriate prompt tailored to the classes. However, manual prompt design is labor-intensive and necessitates domain-specific expertise. The CoOp (Context Optimization) converts hand-crafted prompt templates into learnable word vectors to automatically generate prompts, resulting in substantial improvements for CLIP. However, CoOp exhibited significant variation in classification performance across different classes. Although CoOp-CSC (Class-Specific Context) has a separate prompt for each class, only shows some advantages on fine-grained datasets. In this paper, we propose a novel automatic prompt generation method called F-SCP (Filter-based Specific Class Prompt), which distinguishes itself from the CoOp-UC (Unified Context) model and the CoOp-CSC model. Our approach focuses on prompt generation for low-accuracy classes and similar classes. We add the Filter and SCP modules to the prompt generation architecture. The Filter module selects the poorly classified classes, and then reproduce the prompts through the SCP (Specific Class Prompt) module to replace the prompts of specific classes. Experimental results on six multi-domain datasets shows the superiority of our approach over the state-of-the-art methods. Particularly, the improvement in accuracy for the specific classes mentioned above is significant. For instance, compared with CoOp-UC on the OxfordPets dataset, the low-accuracy classes, such as, Class21 and Class26, are improved by 18% and 12%, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323007938",
    "keywords": [
      "Artificial intelligence",
      "Astrophysics",
      "Biology",
      "Class (philosophy)",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Domain (mathematical analysis)",
      "Filter (signal processing)",
      "Language model",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Natural language processing",
      "Paleontology",
      "Physics",
      "Variation (astronomy)"
    ],
    "authors": [
      {
        "surname": "Han",
        "given_name": "Baihong"
      },
      {
        "surname": "Jiang",
        "given_name": "Xiaoyan"
      },
      {
        "surname": "Fang",
        "given_name": "Zhijun"
      },
      {
        "surname": "Fujita",
        "given_name": "Hamido"
      },
      {
        "surname": "Gao",
        "given_name": "Yongbin"
      }
    ]
  },
  {
    "title": "A cascaded framework with cross-modality transfer learning for whole heart segmentation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110088",
    "abstract": "Automatic and accurate segmentation of the whole heart structure from 3D cardiac images plays an important role in helping physicians diagnose and treat cardiovascular disease. However, the time-consuming and laborious manual labeling of the heart images results in the inefficiency of utilizing the existing CT or MRI for training the deep learning network, which decrease the accuracy of whole heart segmentation. However, multi-modality data contains multi-level information of cardiac images due to different imaging mechanisms, which is beneficial to improve the segmentation accuracy. Therefore, this paper proposes a cascaded framework with cross-modality transfer learning for whole heart segmentation (CM-TranCaF), which consists of three key modules: modality transfer network (MTN), U-shaped multi-attention network (MAUNet) and spatial configuration network (SCN). In MTN, MRI images are transferred from MRI domain to CT domain, to increase the data volume by adopting the idea of adversarial training. The MAUNet is designed based on UNet, while the attention gates (AGs) are integrated into the skip connection to reduce the weight of background pixels. Moreover, to solve the problem of boundary blur, the position attention block (PAB) is also integrated into the bottom layer to aggregate similar features. Finally, the SCN is used to finetune the segmentation results by utilizing the anatomical information between different cardiac substructures. By evaluating the proposed method on the dataset of the MM-WHS challenge, CM-TranCaF achieves a Dice score of 91.1% on the testing dataset. The extensive experimental results prove the effectiveness of the proposed method compared to other state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323007859",
    "keywords": [
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Dice",
      "Geometry",
      "Mathematics",
      "Modality (human–computer interaction)",
      "Pattern recognition (psychology)",
      "Pixel",
      "Segmentation",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Ding",
        "given_name": "Yi"
      },
      {
        "surname": "Mu",
        "given_name": "Dan"
      },
      {
        "surname": "Zhang",
        "given_name": "Jiaqi"
      },
      {
        "surname": "Qin",
        "given_name": "Zhen"
      },
      {
        "surname": "You",
        "given_name": "Li"
      },
      {
        "surname": "Qin",
        "given_name": "Zhiguang"
      },
      {
        "surname": "Guo",
        "given_name": "Yingkun"
      }
    ]
  },
  {
    "title": "Disturbance rejection with compensation on features",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110129",
    "abstract": "In pattern recognition tasks, the information from system input is modeled through a series of nonlinear operations, which include but not limited to feature extraction, regression, and classification. Both theoretically and practically, these operations are inevitably subject to internal modeling error and external disturbance, resulting at a performance challenge. Those state-of-the-art methods, e.g. Convolutional Neural Network and Transformer, still display significant instabilities and failures under practical applications, so comes a lack of generalization. Consequently, the more robust pattern recognition methods and related theories still merit a further study. This paper firstly reviews those state-of-the-art technologies in the field. The bottleneck of performances in those latest researches is associated with a lack of disturbance estimation and corresponding compensation. Therefore, the implications of disturbance rejection in pattern recognition field are further discussed from a control point of view. Then, the open problems are summarized. Ultimately, a discussion of the potential solutions, which is related to the application of compensation on features, is given to highlight the future study. Through the systematic review in this paper, the disturbance rejection in pattern recognition is developed into a control problem. Hopefully, more effective control technologies for the compensation on features can be used to improve the robustness of pattern recognition theoretically and practically.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323008269",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Biology",
      "Bottleneck",
      "Chemistry",
      "Compensation (psychology)",
      "Computer science",
      "Control (management)",
      "Control engineering",
      "Control theory (sociology)",
      "Convolutional neural network",
      "Disturbance (geology)",
      "Embedded system",
      "Engineering",
      "Feed forward",
      "Field (mathematics)",
      "Gene",
      "Machine learning",
      "Mathematics",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Psychoanalysis",
      "Psychology",
      "Pure mathematics",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Hu",
        "given_name": "Xiaobo"
      },
      {
        "surname": "Su",
        "given_name": "Jianbo"
      },
      {
        "surname": "Zhang",
        "given_name": "Jun"
      }
    ]
  },
  {
    "title": "Channel-level Matching Knowledge Distillation for object detectors via MSE",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.01.016",
    "abstract": "Knowledge distillation (KD) has been widely used in different tasks as a practical model compression technique. Due to the poor performance of directly using Mean Square Error (MSE) between the intermediate features of the teacher and student, most feature-based detector distillation methods are primarily concerned with proposing diverse attention mechanisms and employing MSE to guide the student in learning critical information. However, the significance of MSE in detector distillation is often overlooked. To enhance the distillation performance of MSE and provide a novel perspective, we propose channel-level Matching Knowledge Distillation via MSE (MKD). The idea of MKD is simple but effective: based on the match or mismatch of student and teacher channel features in Feature Pyramid Networks (FPN), we divide channels at the same location in the FPN layers of the student and teacher into match channels and mismatch channels and then assign different weights to them to calculate the MSE-based distillation loss, which forces the student to distinguish the teacher’s channels and imitate the features of the corresponding channels to achieve better performance. We conduct extensive experiments on MS COCO and PASCAL VOC benchmarks to prove that MKD is superior to state-of-the-art distillation methods. For example, with a powerful RetinaNet-ResNeXt101 detector as the teacher, ResNet-50 based RetinaNet and RepPoints with our distillation method achieve 40.9% and 42.2% mAP on COCO2017, which are 3.5% and 3.6% higher than the baseline, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524000187",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Channel (broadcasting)",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Computer vision",
      "Detector",
      "Distillation",
      "Image (mathematics)",
      "Matching (statistics)",
      "Mathematics",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Statistics",
      "Telecommunications",
      "Template matching"
    ],
    "authors": [
      {
        "surname": "Jiang",
        "given_name": "Zetao"
      },
      {
        "surname": "Huang",
        "given_name": "Qinyang"
      },
      {
        "surname": "Zhang",
        "given_name": "Huijuan"
      }
    ]
  },
  {
    "title": "Discriminative features enhancement for low-altitude UAV object detection",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110041",
    "abstract": "Object detection is a pivotal task in low-altitude UAV application. Here the small scale objects are dominant due to shooting distance and angle and insufficient feature information due to the data from real world scenes. Although general detector has made great progress, it is not suitable for small scale object detection directly. Dense detector has potential because of the pixel-by-pixel detection but the resolving power of complex background and objects especially small scale objects is still insufficient. We propose a Feature Guided Enhancement module by designing two non-linear learning operators to guide more discriminative features when training. Further, a Scale-Aware Weighted loss function is proposed to dynamically weight the loss of various scale objects by statistical computing and highlight the contribution of small scale objects. Experimental results show that our method can effectively improve FCOS and ATSS, and our models obtain better performance by 1.5% and 0.6% AP respectively on VisDrone 2018 dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323007380",
    "keywords": [
      "Artificial intelligence",
      "Cartography",
      "Computer science",
      "Computer vision",
      "Detector",
      "Discriminative model",
      "Feature (linguistics)",
      "Geography",
      "Linguistics",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pixel",
      "Scale (ratio)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Shuqin"
      },
      {
        "surname": "Ren",
        "given_name": "Shasha"
      },
      {
        "surname": "Wu",
        "given_name": "Wei"
      },
      {
        "surname": "Liu",
        "given_name": "Qiong"
      }
    ]
  },
  {
    "title": "HyRSM++: Hybrid relation guided temporal set matching for few-shot action recognition",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110110",
    "abstract": "Few-shot action recognition is a challenging but practical problem aiming to learn a model that can be easily adapted to identify new action categories with only a few labeled samples. However, existing attempts still suffer from two drawbacks: (i) learning individual features without considering the entire task may result in limited representation capability, and (ii) existing alignment strategies are sensitive to noises and misaligned instances. To handle the two limitations, we propose a novel Hybrid Relation guided temporal Set Matching (HyRSM++) approach for few-shot action recognition. The core idea of HyRSM++ is to integrate all videos within the task to learn discriminative representations and involve a robust matching technique. To be specific, HyRSM++ consists of two key components, a hybrid relation module and a temporal set matching metric. Given the basic representations from the feature extractor, the hybrid relation module is introduced to fully exploit associated relations within and cross videos in an episodic task and thus can learn task-specific embeddings. Subsequently, in the temporal set matching metric, we carry out the distance measure between query and support videos from a set matching perspective and design a bidirectional Mean Hausdorff Metric to improve the resilience to misaligned instances. Furthermore, we extend the proposed HyRSM++ to deal with the more challenging semi-supervised few-shot action recognition and unsupervised few-shot action recognition tasks. Experimental results on multiple benchmarks demonstrate that our method consistently outperforms existing methods and achieves state-of-the-art performance under various few-shot settings. The source code is available at https://github.com/alibaba-mmai-research/HyRSMPlusPlus.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323008075",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Discriminative model",
      "Economics",
      "Feature (linguistics)",
      "Law",
      "Linguistics",
      "Machine learning",
      "Management",
      "Matching (statistics)",
      "Mathematics",
      "Metric (unit)",
      "Operations management",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Programming language",
      "Relation (database)",
      "Representation (politics)",
      "Set (abstract data type)",
      "Statistics",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Xiang"
      },
      {
        "surname": "Zhang",
        "given_name": "Shiwei"
      },
      {
        "surname": "Qing",
        "given_name": "Zhiwu"
      },
      {
        "surname": "Zuo",
        "given_name": "Zhengrong"
      },
      {
        "surname": "Gao",
        "given_name": "Changxin"
      },
      {
        "surname": "Jin",
        "given_name": "Rong"
      },
      {
        "surname": "Sang",
        "given_name": "Nong"
      }
    ]
  },
  {
    "title": "Deepfake detection via inter-frame inconsistency recomposition and enhancement",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110077",
    "abstract": "Due to the remarkable progress in face manipulation technology, malicious applications of these technologies may pose a great threat to the social stability. Therefore, it is essential to carry out the research of deepfake detection. In this paper, we assumed that the illumination on frames that skip a certain space is basically consistent in real videos, but tends to be inconsistent in fake videos. From this point, a network which contains a learnable Image Decomposition Module (IDM) and multi-level feature enhancement is proposed. IDM decomposes frames into illumination and reflection, and frame recomposition is followed to highlight the frame-level illumination inconsistency. Multi-level feature enhancement is proposed to enhance the illumination inconsistency at feature level. In addition, considering the computational complexity and human vision perception mechanism, we train the network in logarithm domain. Experimental results show that the proposed method is effective and superior compared with other state-of-the-art deepfake detection methods on mainstream deepfake datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323007744",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer vision",
      "Decomposition",
      "Domain (mathematical analysis)",
      "Ecology",
      "Feature (linguistics)",
      "Frame (networking)",
      "Geometry",
      "Image (mathematics)",
      "Linguistics",
      "Logarithm",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Point (geometry)",
      "Stability (learning theory)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Zhu",
        "given_name": "Chuntao"
      },
      {
        "surname": "Zhang",
        "given_name": "Bolin"
      },
      {
        "surname": "Yin",
        "given_name": "Qilin"
      },
      {
        "surname": "Yin",
        "given_name": "Chengxi"
      },
      {
        "surname": "Lu",
        "given_name": "Wei"
      }
    ]
  },
  {
    "title": "An analytic study on clustering driven self-supervised speaker verification",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.01.024",
    "abstract": "One of the most widely used self-supervised speaker verification system training methods is to optimize the speaker embedding network in a discriminative fashion using clustering algorithm-driven Pseudo-Labels. Although the pseudo-labels-based self-supervised training scheme showed impressive performance, recent studies have shown that label noise can significantly impact performance. In this paper, we have explored various pseudo-labels driven by different clustering algorithms and conducted a fine-grained analysis of the relationship between the quality of the pseudo-labels and the speaker verification performance. Experimentally, we shed light on several previously overlooked aspects of the pseudo-labels that can impact speaker verification performance. Moreover, we could observe that the self-supervised speaker verification performance is heavily dependent on multiple qualitative aspects of the clustering algorithms used to generate the pseudo-labels. Furthermore, we show that speaker verification performance can be severely degraded from overfitting the noisy pseudo-labels and that the mixup strategy can mitigate the memorization effects of label noise.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524000242",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Cluster analysis",
      "Computer science",
      "Discriminative model",
      "Embedding",
      "Image (mathematics)",
      "Machine learning",
      "Noise (video)",
      "Overfitting",
      "Pattern recognition (psychology)",
      "Speaker recognition",
      "Speaker verification",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Fathan",
        "given_name": "Abderrahim"
      },
      {
        "surname": "Alam",
        "given_name": "Jahangir"
      }
    ]
  },
  {
    "title": "Separated Fan-Beam Projection with Gaussian Convolution for Invariant and Robust Butterfly Image Retrieval.",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110083",
    "abstract": "Butterfly image retrieval is a challenging issue requiring the feature representation not only to be sensitive to the subtle inter-class difference but also to remain robust to large intra-class variations. Fan-beam projection is a mathematical tool originally applied to computed tomographic (CT) reconstruction of objects. In this paper, we introduce it for the first time into object recognition field. Separated fan-beam projection followed by Gaussian convolutions of different widths are designed to extract multiscale invariant features, patch-projection angles (PPA) and texture-projection angles (TPA), for separately depicting the patch patterns and texture properties of butterfly images. The PPA and TPA are then treated as heterogeneous co-occurrence patterns to be fused by a 2D histograms as final feature representation. We present a comprehensive experimental evaluation including image retrieval at species and subspecies levels, complementarity to deep-learning features, invariance and robustness. All the results consistently show the superior performance of the proposed method over the state-of-the arts.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032300780X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Gaussian",
      "Gene",
      "Invariant (physics)",
      "Mathematical physics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Projection (relational algebra)",
      "Quantum mechanics",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Xin"
      },
      {
        "surname": "Wang",
        "given_name": "Bin"
      },
      {
        "surname": "Gao",
        "given_name": "Yongsheng"
      }
    ]
  },
  {
    "title": "A deep learning-based global and segmentation-based semantic feature fusion approach for indoor scene classification",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.01.022",
    "abstract": "This work proposes a novel approach that uses a semantic segmentation mask to obtain a 2D spatial layout of the segmentation-categories across the scene, designated by segmentation-based semantic features (SSFs). These features represent, per segmentation-category, the pixel count, as well as the 2D average position and respective standard deviation values. Moreover, a two-branch network, GS 2 F 2 App, that exploits CNN-based global features extracted from RGB images and the segmentation-based features extracted from the proposed SSFs, is also proposed. GS 2 F 2 App was evaluated in two indoor scene benchmark datasets: the SUN RGB-D and the NYU Depth V2, achieving state-of-the-art results on both datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524000229",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Cartography",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Feature (linguistics)",
      "Geography",
      "Image segmentation",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pixel",
      "RGB color model",
      "Scale-space segmentation",
      "Segmentation",
      "Segmentation-based object categorization"
    ],
    "authors": [
      {
        "surname": "Pereira",
        "given_name": "Ricardo"
      },
      {
        "surname": "Barros",
        "given_name": "Tiago"
      },
      {
        "surname": "Garrote",
        "given_name": "Luís"
      },
      {
        "surname": "Lopes",
        "given_name": "Ana"
      },
      {
        "surname": "Nunes",
        "given_name": "Urbano J."
      }
    ]
  },
  {
    "title": "Adversarial mimicry attacks against image splicing forensics: An approach for jointly hiding manipulations and creating false detections",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.01.023",
    "abstract": "The term “mimicry attack” has been coined in computer security and used in adversarial machine learning: an attacker observes what a machine-learning system has learned and adjusts the malicious input so that it mimics a benign input. In this paper we extend this concept to image forensics, to allow an attacker modifying a manipulated image so that it appears pristine when analyzed by a target forensic detector. Recent work has shown that such attacks can be executed against detectors based on deep networks for hiding image tampering. We do more than that: our mimicry attack can force the target detector to identify arbitrary fictitious manipulations, while hiding the true ones. Accordingly, the user of the forensic detector is completely misled. From a methodological viewpoint, the proposed attack artificially alters the detector-specific intermediate representations according to the pixel distribution in the manipulated image, by applying a gradient-based optimization process. Experimental tests on different data sets and detectors demonstrate that our approach succeeds in jointly hiding manipulated areas and arbitrarily adding new ones, favorably comparing with the state-of-the-art in the first task.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524000230",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Biology",
      "Computational biology",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Ecology",
      "Image (mathematics)",
      "Mimicry",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Boato",
        "given_name": "Giulia"
      },
      {
        "surname": "De Natale",
        "given_name": "Francesco G.B."
      },
      {
        "surname": "De Stefano",
        "given_name": "Gianluca"
      },
      {
        "surname": "Pasquini",
        "given_name": "Cecilia"
      },
      {
        "surname": "Roli",
        "given_name": "Fabio"
      }
    ]
  },
  {
    "title": "PNSP: Overcoming catastrophic forgetting using Primary Null Space Projection in continual learning",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.02.009",
    "abstract": "Continual Learning (CL) plays a crucial role in enhancing learning performance for both new and previous tasks in continuous data streams, thus contributing to the advancement of cognitive computing. However, CL faces a fundamental challenge known as the stability-plasticity quandary. In this research, we present an innovative and effective CL algorithm called Primary Null Space Projection (PNSP) to strike a balance between network plasticity and stability. PNSP consists of three main components. Firstly, it leverages the NSP-LRA algorithm to project the gradient of network parameters from previous tasks into a meticulously designed null space. NSP-LRA harnesses high-dimensional geometric information extracted from the feature covariance matrix through low-rank approximation algorithm to obtain the basis of null space dynamically. This process constructs an innovation null space and ensures the continuous updating of orthonormal bases to accommodate changes in the input data. Secondly, we propose a Consistency-guided Task-specific Feature Learning (CTFL) mechanism to tackle the issue of catastrophic forgetting and facilitate continual learning. CTFL achieves this by aligning feature vectors and maintaining consistent feature learning directions, thereby preventing the loss of previously acquired knowledge. Lastly, we introduce Label Guided Self-Distillation (LGSD), a technique that utilizes true labels to guide the distillation process and incorporates a dynamic temperature mechanism to enhance performance. To evaluate the effectiveness of our proposed method, we conduct experiments on the CIFAR100 and TinyImageNet datasets. The results demonstrate significant improvements over state-of-the-art methods. We have made the implementation code of our approach available for reference.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524000424",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Feature (linguistics)",
      "Feature vector",
      "Forgetting",
      "Linguistics",
      "Machine learning",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Process (computing)",
      "Projection (relational algebra)",
      "Stability (learning theory)"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "DaiLiang"
      },
      {
        "surname": "Song",
        "given_name": "YongHong"
      }
    ]
  },
  {
    "title": "A lightness-aware loss for low-light image enhancement",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.02.011",
    "abstract": "Current low-light image enhancement methods have made great progress on improving the visibility of low-light images. Nevertheless, they pay less attention to preserving visual naturalness and therefore often introduce over-enhancement and local artifacts into their results. To address this issue, it is useful to introduce additional multi-view information of an image into enhancement models, such as illumination distribution. In this context, we propose a simple but effective loss term that expects the originally bright regions in input images and their corresponding enhanced images to be as similar as possible. Via fully exploring the illumination distribution of an image, the loss term makes enhancement models to know which regions should be preserved during training. Therefore, the unnatural effects in output images can be effectively relieved. In our experiments, we incorporate our loss term into several recently proposed low-light image enhancement models. The experimental results on multiple datasets show that over-enhancement and local artifacts can be effectively suppressed by using our loss term.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524000448",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Brightness",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Image (mathematics)",
      "Image enhancement",
      "Image processing",
      "Image restoration",
      "Information loss",
      "Lightness",
      "Naturalness",
      "Optics",
      "Paleontology",
      "Physics",
      "Quantum mechanics",
      "Term (time)",
      "Visibility"
    ],
    "authors": [
      {
        "surname": "Xie",
        "given_name": "Dian"
      },
      {
        "surname": "Xing",
        "given_name": "Huajun"
      },
      {
        "surname": "Chen",
        "given_name": "Liangyu"
      },
      {
        "surname": "Hao",
        "given_name": "Shijie"
      }
    ]
  },
  {
    "title": "SemanticFormer: Hyperspectral image classification via semantic transformer",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2023.12.023",
    "abstract": "Hyperspectral image (HSI) classification is an active research problem in computer vision and multimedia field. Contrary to traditional image data, HSIs contain rich spectral, spatial and semantic information. Thus, how to extract the discriminative features for HSIs by integrating spectral, spatial and semantic cues together is the core issue to address HSI classification task. Existing works mainly focus on exploring spectral and spatial information which usually fail to fully explore the rich semantic information in HSIs. To address this issue, in this paper, we first propose a novel semantic Transformer scheme, named SemanticFormer, which aims to learn discriminative visual representations for semantics by exploiting the interaction among different semantic tokens. Using the proposed SemanticFormer, we then propose a novel heterogeneous network that contains both spectral–spatial convolution network branch and SemanticFormer branch to extract spectral–spatial and semantic features simultaneously for HSIs. Experiments on two widely used datasets demonstrate the effectiveness of our SemanticFormer and HSI classification network method. Our codes will be available in https://github.com/SissiW/SemanticFormer.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865523003793",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Discriminative model",
      "Geology",
      "Hyperspectral imaging",
      "Pattern recognition (psychology)",
      "Programming language",
      "Remote sensing",
      "Semantics (computer science)",
      "Spatial analysis"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Yan"
      },
      {
        "surname": "Wang",
        "given_name": "Xixi"
      },
      {
        "surname": "Jiang",
        "given_name": "Bo"
      },
      {
        "surname": "Chen",
        "given_name": "Lan"
      },
      {
        "surname": "Luo",
        "given_name": "Bin"
      }
    ]
  },
  {
    "title": "Learning implicit labeling-importance and label correlation for multi-label feature selection with streaming labels",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110081",
    "abstract": "Multi-label feature selection plays an increasingly important role in alleviating the high dimensionality of multi-label learning tasks. Most extant methods posit that the learning task is performed in an environment where the label space is statically known. In reality, however, the environment is open and the labels may arrive dynamically, which is coined as streaming labels. Streaming labels-based multi-label feature selection suffers from many challenges derived from label space: (1) The label space expands dynamically; (2) Newly arrived labels exhibit complex relationships, often involving label correlation and labeling-importance. To cope with this challenge, in this paper, an intuitive yet effective algorithm named LLSL, i.e. learning implicit labeling-importance and label correlation for multi-label feature selection with streaming labels, is proposed. To be specific, the implicit labeling-importance with respect to streaming labels is firstly formalized by conducting the nearest neighbor reconstruction on feature space. Secondly, label correlation is seamlessly integrated into the objective function of feature relevance by designing the feature relevance influence factor. Based on the above, we build a feature conversion, which can realize the fusion of label-specific features for each streaming label. Finally, extensive experiments conducted on fifteen benchmark datasets provide clear evidence that LLSL has superior performance compared to three established streaming label-based MFS algorithms and seven static label space-based MFS algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323007781",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Correlation",
      "Curse of dimensionality",
      "Dimensionality reduction",
      "Feature (linguistics)",
      "Feature selection",
      "Feature vector",
      "Geodesy",
      "Geography",
      "Geometry",
      "Law",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Multi-label classification",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Relevance (law)",
      "Selection (genetic algorithm)",
      "Streaming algorithm",
      "Upper and lower bounds"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Jinghua"
      },
      {
        "surname": "Wei",
        "given_name": "Wei"
      },
      {
        "surname": "Lin",
        "given_name": "Yaojin"
      },
      {
        "surname": "Yang",
        "given_name": "Lijie"
      },
      {
        "surname": "Zhang",
        "given_name": "Hongbo"
      }
    ]
  },
  {
    "title": "RoMP-transformer: Rotational bounding box with multi-level feature pyramid transformer for object detection",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110067",
    "abstract": "This study proposes rotational bounding box with a multi-level feature pyramid transformer (RoMP-Transformer)—a fast and accurate one-stage deep neural network for object detection. The proposed RoMP-Transformer exhibits three characteristics. First, a rotational bounding box is utilized to minimize the effect of the background during the construction of feature maps, enhancing the robustness of the RoMP-Transformer. Second, the RoMP-Transformer employs a multi-level feature pyramid transformer by combining a multi-level feature pyramid network with a pyramid vision-transformer, effectively extracting high-quality features and achieving high accuracy. Third, the RoMP-Transformer executes bounding box optimization by minimizing the optimal intersection of union (IoU) loss by considering both the modified SKEW IoU and distance IoU. The modified SKEW IoU significantly accelerates the calculation, and the fused IoU calculation method improves prediction accuracy. Further, Bayesian optimization and weight lightening with half-tensor are performed to optimize the performance of the RoMP-Transformer for real-time applications. Experiments on three image sets—one on power transmission facilities, MSRA-TD500, and DOTA-v1.0—demonstrate that the proposed RoMP-Transformer outperforms other state-of-the-art neural networks in object detection in terms of accuracy, robustness, and calculation speed. Systematic analysis also reveals that the methods utilized by the RoMP-Transformer optimize object detection performance. The proposed architecture is expected to inspire further study of deep neural networks for object detection in real-world applications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323007641",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Composite material",
      "Computer science",
      "Computer vision",
      "Electrical engineering",
      "Engineering",
      "Materials science",
      "Metathesis",
      "Polymer",
      "Polymerization",
      "ROMP",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Moon",
        "given_name": "Joonhyeok"
      },
      {
        "surname": "Jeon",
        "given_name": "Munsu"
      },
      {
        "surname": "Jeong",
        "given_name": "Siheon"
      },
      {
        "surname": "Oh",
        "given_name": "Ki-Yong"
      }
    ]
  },
  {
    "title": "Fairness in face presentation attack detection",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110002",
    "abstract": "Face recognition (FR) algorithms have been proven to exhibit discriminatory behaviors against certain demographic and non-demographic groups, raising ethical and legal concerns regarding their deployment in real-world scenarios. Despite the growing number of fairness studies in FR, the fairness of face presentation attack detection (PAD) has been overlooked, mainly due to the lack of appropriately annotated data. To avoid and mitigate the potential negative impact of such behavior, it is essential to assess the fairness in face PAD and develop fair PAD models. To enable fairness analysis in face PAD, we present a Combined Attribute Annotated PAD Dataset (CAAD-PAD), offering seven human-annotated attribute labels. Then, we comprehensively analyze the fairness of PAD and its relation to the nature of the training data and the Operational Decision Threshold Assignment (ODTA) through a set of face PAD solutions. Additionally, we propose a novel metric, the Accuracy Balanced Fairness (ABF), that jointly represents both the PAD fairness and the absolute PAD performance. The experimental results pointed out that female and faces with occluding features (e.g. eyeglasses, beard, etc.) are relatively less protected than male and non-occlusion groups by all PAD solutions. To alleviate this observed unfairness, we propose a plug-and-play data augmentation method, FairSWAP, to disrupt the identity/semantic information and encourage models to mine the attack clues. The extensive experimental results indicate that FairSWAP leads to better-performing and fairer face PADs in 10 out of 12 investigated cases.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323007008",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Engineering",
      "Face (sociological concept)",
      "Image (mathematics)",
      "Medicine",
      "Metric (unit)",
      "Operating system",
      "Operations management",
      "Presentation (obstetrics)",
      "Programming language",
      "Radiology",
      "Set (abstract data type)",
      "Similarity (geometry)",
      "Social science",
      "Sociology",
      "Software deployment"
    ],
    "authors": [
      {
        "surname": "Fang",
        "given_name": "Meiling"
      },
      {
        "surname": "Yang",
        "given_name": "Wufei"
      },
      {
        "surname": "Kuijper",
        "given_name": "Arjan"
      },
      {
        "surname": "S̆truc",
        "given_name": "Vitomir"
      },
      {
        "surname": "Damer",
        "given_name": "Naser"
      }
    ]
  },
  {
    "title": "Topological safeguard for evasion attack interpreting the neural networks’ behavior",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110130",
    "abstract": "In the last years, Deep Learning technology has been proposed in different fields, bringing many advances in each of them, but raising new threats in these solutions regarding cybersecurity. Those implemented models have brought several vulnerabilities associated with Deep Learning technology. Moreover, those allow taking advantage of the implemented model, obtaining private information, and even modifying the model’s decision-making. Therefore, interest in studying those vulnerabilities/attacks and designing defenses to avoid or fight them is gaining prominence among researchers. In particular, the widely known evasion attack is being analyzed by researchers; thus, several defenses to avoid such a threat can be found in the literature. Since the presentation of the L-BFG algorithm, this threat concerns the research community. However, it continues developing new and ingenious countermeasures since there is no perfect defense for all the known evasion algorithms. In this work, a novel detector of evasion attacks is developed. It focuses on the information on the activations of the neurons given by the model when an input sample is injected. Moreover, it pays attention to the topology of the targeted deep learning model to analyze the activations according to which neurons are connecting. This approach is motivated from the observation that the literature shows that the targeted model’s topology contains essential information about if the evasion attack occurs. For this purpose, a huge data preprocessing is required to introduce all this information in the detector, which uses the Graph Convolutional Neural Network (GCN) technology. Thus, it understands the topology of the target model, obtaining promising results and improving the outcomes presented in the literature related to similar defenses.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323008270",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer science",
      "Computer security",
      "Convolutional neural network",
      "Deep learning",
      "Electrical engineering",
      "Engineering",
      "Evasion (ethics)",
      "Immune system",
      "Immunology",
      "Topology (electrical circuits)"
    ],
    "authors": [
      {
        "surname": "Echeberria-Barrio",
        "given_name": "Xabier"
      },
      {
        "surname": "Gil-Lerchundi",
        "given_name": "Amaia"
      },
      {
        "surname": "Mendialdua",
        "given_name": "Iñigo"
      },
      {
        "surname": "Orduna-Urrutia",
        "given_name": "Raul"
      }
    ]
  },
  {
    "title": "Feature enhancement and coarse-to-fine detection for RGB-D tracking",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.02.007",
    "abstract": "Existing RGB-D tracking algorithms advance the performance by constructing typical appearance models from the RGB-only tracking frameworks. There is no attempt to exploit any complementary visual information from the multi-modal input. This paper addresses this deficit and presents a novel algorithm to boost the performance of RGB-D tracking by taking advantage of collaborative clues. To guarantee input consistency, depth images are encoded into the three-channel HHA representation to create input of a similar structure to the RGB images, so that the deep CNN features can be extracted from both modalities. To highlight the discriminatory information in multi-modal features, a feature enhancement module using a cross-attention strategy is proposed. With the attention map produced by the proposed cross-attention method, the target area of the features can be enhanced and the negative influence of the background is suppressed. Besides, we address the potential tracking failure by introducing a long-term mechanism. The experimental results obtained on the well-known benchmarking datasets, including PTB, STC, and CTDB, demonstrate the superiority of the proposed RGB-D tracker. On PTB, the proposed method achieves the highest AUC scores against compared trackers across scenarios with five distinct challenging attributes. On STC and CDTB, our FECD obtains an overall AUC of 0.630 and an F-score of 0.630, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524000412",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Benchmarking",
      "BitTorrent tracker",
      "Business",
      "Computer science",
      "Computer vision",
      "Consistency (knowledge bases)",
      "Eye tracking",
      "Feature (linguistics)",
      "Geodesy",
      "Geography",
      "Linguistics",
      "Marketing",
      "Pattern recognition (psychology)",
      "Pedagogy",
      "Philosophy",
      "Psychology",
      "RGB color model",
      "Tracking (education)"
    ],
    "authors": [
      {
        "surname": "Zhu",
        "given_name": "Xue-Feng"
      },
      {
        "surname": "Xu",
        "given_name": "Tianyang"
      },
      {
        "surname": "Wu",
        "given_name": "Xiao-Jun"
      },
      {
        "surname": "Kittler",
        "given_name": "Josef"
      }
    ]
  },
  {
    "title": "Joint Feature Generation and Open-set Prototype Learning for generalized zero-shot open-set classification",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110133",
    "abstract": "In generalized zero-shot classification, test samples can belong to either seen or unseen classes. However, in real-world situations, there may be many open-set samples in the test set where neither visual nor semantic representations of the classes are provided. The new problem is defined as generalized zero-shot open-set classification (GZSOSC). The purpose is to tell whether an instance belongs to which seen or unseen classes, or to reject an instance if it belongs to the open-set classes. To address this problem, we propose a novel method called Joint Feature Generation and Open-Set Prototype Learning (JFGOPL) for GZSOSC tasks. JFGOPL is presented to combine GAN training with open-set prototype learning, where the former generates high-quality unseen and open-set samples and the latter learns some open-set prototypes. Specifically, a novel GAN training strategy is proposed, where an intra-class compactness loss and an inter-class dispersion loss are proposed to ensure the discrimination of the generated samples and to make the learned embedding network less susceptible to the domain shift problem. Furthermore, open-set prototypes are derived by projecting confident open-set samples into the semantic space using the updated embedding network. Experiments on widely used benchmarks demonstrate the superiority of JFGOPL over existing methods for tackling the challenging GZSOSC problem.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323008300",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Discrete mathematics",
      "Embedding",
      "Feature (linguistics)",
      "Feature vector",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Open set",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Set (abstract data type)",
      "Test set"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Xiao"
      },
      {
        "surname": "Fang",
        "given_name": "Min"
      },
      {
        "surname": "Zhai",
        "given_name": "Zhibo"
      }
    ]
  },
  {
    "title": "Coupled discriminative manifold alignment for low-resolution face recognition",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110049",
    "abstract": "In practical applications, due to a long distance between the monitored population and monitoring equipment, the face images or human pose captured by the cameras often incur low-resolution (LR), small size, and poor quality, which leads to extreme difficulty in directly matching an LR face with the high-resolution (HR) ones in the gallery. In this paper, we propose a novel coupled discriminative manifold alignment (CDMA) method for LR face recognition. Specifically, in the training stage the principal component analysis (PCA) is first used to reduce the dimensional gap between LR and HR facial features. Next the LR face images and the corresponding HR face images are converted into a common shared feature subspace by learning two linear mappings in a supervised manner, where the neighborhood samples within the same class and from different classes are jointly exploited to align the manifold structures of LR and HR faces. In the test stage, for a given LR face in the probe set, two learned coupled mappings (CMs) are applied to match the HR images in the gallery set through the correlative metric. Thorough experimental results on three representative face databases verify the effectiveness of the proposed method in comparing with other state-of-the-art competitors. In particular, the proposed method is capable of yielding more competitive recognition performance than other predecessors when lower dimensional feature subspaces are applied to match the expected HR faces.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032300746X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Discriminative model",
      "Economics",
      "Face (sociological concept)",
      "Facial recognition system",
      "Feature (linguistics)",
      "Geometry",
      "Linear discriminant analysis",
      "Linear subspace",
      "Linguistics",
      "Matching (statistics)",
      "Mathematics",
      "Metric (unit)",
      "Operations management",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Principal component analysis",
      "Programming language",
      "Set (abstract data type)",
      "Social science",
      "Sociology",
      "Statistics",
      "Subspace topology"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Kaibing"
      },
      {
        "surname": "Zheng",
        "given_name": "Dongdong"
      },
      {
        "surname": "Li",
        "given_name": "Jie"
      },
      {
        "surname": "Gao",
        "given_name": "Xinbo"
      },
      {
        "surname": "Lu",
        "given_name": "Jian"
      }
    ]
  },
  {
    "title": "Residual shape adaptive dense-nested Unet: Redesign the long lateral skip connections for metal surface tiny defect inspection",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110073",
    "abstract": "The state-of-the-art metal surface defect inspection methods have two problems: (1) they are sensitive to tiny defects because of their extreme small sizes, and (2) they cannot accurately locate the random appeared defects whose semantic relationship with the background context is weak. To solve these problems, Residual Shape Adaptive Dense-nested Unet, a pixel-based defect inspection method is proposed, to obtain the exact shape and location of the defect, by (1) assembling different depth Unet branches with dense skip connections as the feature extractor to combine multi-semantic level visual features; (2) adding Residual Shape Adaptive modules on the dense skip connections to help the model locate the defect regions; and (3) introducing the multi-branch training method which enables model pruning to reduce redundant parameters and accelerate the inspection speed. Experiments are conducted and demonstrated that the Residual Shape Adaptive Dense-nested Unet achieves the best performance among the state-of-the-art defect inspection methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323007707",
    "keywords": [
      "Agronomy",
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Feature (linguistics)",
      "Geology",
      "Geometry",
      "Linguistics",
      "Mathematics",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pruning",
      "Residual",
      "Surface (topology)",
      "Visual inspection"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Benyi"
      },
      {
        "surname": "Liu",
        "given_name": "Zhenyu"
      },
      {
        "surname": "Duan",
        "given_name": "Guifang"
      },
      {
        "surname": "Tan",
        "given_name": "Jianrong"
      }
    ]
  },
  {
    "title": "MPCCT: Multimodal vision-language learning paradigm with context-based compact Transformer",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110084",
    "abstract": "Transformer and its variants have become the preferred option for multimodal vision-language paradigms. However, they struggle with tasks that demand high-dependency modeling and reasoning, like visual question answering (VQA) and visual grounding (VG). For this, we propose a general scheme called MPCCT, which: (1) incorporates designed textual global-context information to facilitate precise computation of dependency relationships between language tokens in the language encoder; (2) dynamically modulates and filters image features using optimized textual global-context information, combined with designed spatial context information, to further enhance the dependency modeling of image tokens and the model’s reasoning ability; (3) reasonably align the language sequence containing textual global-context information with the image sequence information modulated by spatial position information. To validate MPCCT, we conducted extensive experiments on five benchmark datasets in VQA and VG, achieving new SOTA performance on multiple benchmarks, especially 73.71% on VQA-v2 and 99.15% on CLEVR. The code is available at https://github.com/RainyMoo/myvqa.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323007811",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Dependency (UML)",
      "Encoder",
      "Geodesy",
      "Geography",
      "Language model",
      "Natural language processing",
      "Operating system",
      "Paleontology",
      "Physics",
      "Quantum mechanics",
      "Question answering",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Chongqing"
      },
      {
        "surname": "Han",
        "given_name": "Dezhi"
      },
      {
        "surname": "Chang",
        "given_name": "Chin-Chen"
      }
    ]
  },
  {
    "title": "ConvGeN: A convex space learning approach for deep-generative oversampling and imbalanced classification of small tabular datasets",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110138",
    "abstract": "Oversampling is commonly used to improve classifier performance for small tabular imbalanced datasets. State-of-the-art linear interpolation approaches can be used to generate synthetic samples from the convex space of the minority class. Generative networks are common deep learning approaches for synthetic sample generation. However, their scope on synthetic tabular data generation in the context of imbalanced classification is not adequately explored. In this article, we show that existing deep generative models perform poorly compared to linear interpolation-based approaches for imbalanced classification problems on small tabular datasets. To overcome this, we propose a deep generative model, ConvGeN that combines the idea of convex space learning with deep generative models. ConvGeN learns coefficients for the convex combinations of the minority class samples, such that the synthetic data is distinct enough from the majority class. Our benchmarking experiments demonstrate that our proposed model ConvGeN improves imbalanced classification on such small datasets, as compared to existing deep generative models, while being on par with the existing linear interpolation approaches. Moreover, we discuss how our model can be used for synthetic tabular data generation in general, even outside the scope of data imbalance, and thus improves the overall applicability of convex space learning.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032300835X",
    "keywords": [
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Class (philosophy)",
      "Classifier (UML)",
      "Computer network",
      "Computer science",
      "Deep learning",
      "Generative grammar",
      "Image (mathematics)",
      "Interpolation (computer graphics)",
      "Linear classifier",
      "Machine learning",
      "Oversampling",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Schultz",
        "given_name": "Kristian"
      },
      {
        "surname": "Bej",
        "given_name": "Saptarshi"
      },
      {
        "surname": "Hahn",
        "given_name": "Waldemar"
      },
      {
        "surname": "Wolfien",
        "given_name": "Markus"
      },
      {
        "surname": "Srivastava",
        "given_name": "Prashant"
      },
      {
        "surname": "Wolkenhauer",
        "given_name": "Olaf"
      }
    ]
  },
  {
    "title": "A synthetic human-centric dataset generation pipeline for active robotic vision",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.01.019",
    "abstract": "Active vision aims to equip computer vision methods with the ability to dynamically adjust the capturing sensor’s viewpoint, position, or parameters in real time. This dynamic capability allows for improving the accuracy of the perception process. However, training and evaluating an active vision model often requires a large number of annotated images captured under different sensor and environmental settings, in order to emulate actions like moving around, approaching, or moving away from a person and thus effectively model the active perception dynamics. Obviously, collecting and annotating such datasets is a challenging and expensive task. To overcome these limitations, this paper introduces a synthetic image generation pipeline specifically designed to support active vision tasks. The pipeline is developed using a highly realistic simulation framework based on Unity and allows for the generation of images depicting humans, captured at varying view angles, distances, illumination conditions, and backgrounds, supporting a wide range of different tasks. Two annotated datasets, namely ActiveHuman and ActiveFace, are generated using the pipeline and the effectiveness of the proposed approach is demonstrated by a solid use case that involves training and evaluating an embedding-based active face recognizer. Furthermore, we demonstrate how the proposed generation approach enables expanding existing active face recognition methods by training models that control both the left/right movements, as well as the distance to a subject, leveraging the additional information provided by ActiveFace dataset. To facilitate replication and encourage the use of the generated datasets for training and evaluating other active vision approaches, the associated assets and the developed dataset generation pipeline is to become publicly available.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524000205",
    "keywords": [
      "Active vision",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Machine vision",
      "Pipeline (software)",
      "Programming language"
    ],
    "authors": [
      {
        "surname": "Georgiadis",
        "given_name": "Charalampos"
      },
      {
        "surname": "Passalis",
        "given_name": "Nikolaos"
      },
      {
        "surname": "Nikolaidis",
        "given_name": "Nikos"
      }
    ]
  },
  {
    "title": "Regional context-based recalibration network for cataract recognition in AS-OCT",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110069",
    "abstract": "Deep convolutional neural networks (CNNs) have been widely applied to cataract recognition tasks and have achieved promising results. However, most existing methods focused on designing data-driven CNN architectures, and failed to exploit asymmetric opacity distribution prior of cataract, which is significant for cataract diagnosis. To this end, this paper proposes a regional context-based recalibration (RCR) module, which fully leverages the clinical prior to recalibrate the feature maps with regional pooling, region-based context integration, and integrated context fusion. We stack these RCR modules to form an RCRNet based on anterior segment optical coherence tomography (AS-OCT) images for cataract recognition. Experiments on the AS-OCT-NC2 dataset and two publicly available medical datasets demonstrate that RCRNet achieves a better trade-off between performance and efficiency than state-of-the-art channel attention-based networks. We also explain the inherent behavior of RCRNet with the aid of the visual analysis. In addition, this paper is the first to study the effects of two performance evaluation methods on AS-OCT image-based cataract classification results: the single-image level and the single-eye level, suggesting that adopting the single-eye level to evaluate cataract classification performance according to clinical diagnosis requirement.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323007665",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Convolutional neural network",
      "Feature (linguistics)",
      "Linguistics",
      "Medicine",
      "Ophthalmology",
      "Optical coherence tomography",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pooling"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Xiaoqing"
      },
      {
        "surname": "Xiao",
        "given_name": "Zunjie"
      },
      {
        "surname": "Yang",
        "given_name": "Bing"
      },
      {
        "surname": "Wu",
        "given_name": "Xiao"
      },
      {
        "surname": "Higashita",
        "given_name": "Risa"
      },
      {
        "surname": "Liu",
        "given_name": "Jiang"
      }
    ]
  },
  {
    "title": "HairManip: High quality hair manipulation via hair element disentangling",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110132",
    "abstract": "Hair editing is challenging due to the complexity and variety of hair materials and shapes. Existing methods employ reference images or user-painted masks to edit hair and have achieved promising results. However, discrepancies in color and shape between the source and target hair can occasionally result in unrealistic results. Therefore, we propose a new hair editing method named HairManip, which decouples the hair information from the input source image into shape and color components. We then train hairstyle and hair color editing sub-networks to handle this complex information independently. To further enhance editing efficiency and accuracy, we introduce a latent code preprocessing module that effectively extracts meaningful features from hair regions, thereby improving the model’s editing capabilities. The experimental results demonstrate that our method achieves significant results in editing accuracy and authenticity, thanks to the carefully designed network structure and loss functions. Code can be found at https://github.com/Zlin0530/HairManip.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323008294",
    "keywords": [
      "Artificial intelligence",
      "Code (set theory)",
      "Computer graphics (images)",
      "Computer science",
      "Computer vision",
      "Image (mathematics)",
      "Image editing",
      "Preprocessor",
      "Programming language",
      "Set (abstract data type)",
      "Source code"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Huihuang"
      },
      {
        "surname": "Zhang",
        "given_name": "Lin"
      },
      {
        "surname": "Rosin",
        "given_name": "Paul L."
      },
      {
        "surname": "Lai",
        "given_name": "Yu-Kun"
      },
      {
        "surname": "Wang",
        "given_name": "Yaonan"
      }
    ]
  },
  {
    "title": "Feature specific progressive improvement for salient object detection",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110085",
    "abstract": "Benefiting from deep learning, Salient Object Detection (SOD) has made much progress. However, most existing methods adopt the same strategy to extract salient cues from different feature levels without fully considering their differences in the feature extraction stage and/or suffer from the accumulation of noise and dilution of spatial details in the feature fusion stage. These two problems hinder the further improvement in performance. In this paper, we propose an effective SOD model, PiNet, which can address the above problems via two novel mechanisms in the network: level-specific feature extraction and progressive refinement of saliency. We have designed the customized feature extraction components for each level of features—enabling us to extract better saliency cues from multi-level features. The saliency feature refinement in the branches follows a coarse-to-fine process, where the refined features progressively contain more location cues, internal and boundary details. Through short connections, the extracted saliency cues in different branches are selectively transmitted and integrated, which well mitigates the accumulation of noisy information and the dilution of detailed information. By using four different backbones, we verify our model has good adaptability and can make accurate saliency predictions under different pretrained models. Extensive experiments on five public datasets demonstrate that PiNet outperforms 19 state-of-the-art (SOTA) methods in SOD, with its small model size (56.1 MB) and fast inference speed (47 FPS).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323007823",
    "keywords": [
      "Adaptability",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Ecology",
      "Feature (linguistics)",
      "Feature extraction",
      "Image (mathematics)",
      "Inference",
      "Linguistics",
      "Noise (video)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Process (computing)",
      "Salient"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Xianheng"
      },
      {
        "surname": "Liu",
        "given_name": "Zhaobin"
      },
      {
        "surname": "Liesaputra",
        "given_name": "Veronica"
      },
      {
        "surname": "Huang",
        "given_name": "Zhiyi"
      }
    ]
  },
  {
    "title": "Even small correlation and diversity shifts pose dataset-bias issues",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.01.026",
    "abstract": "Distribution shifts hinder the deployment of deep learning in real-world problems. Distribution shifts appear when train and test data come from different sources, which commonly happens in practice. Despite shifts occurring concurrently in many forms (e.g., correlation and diversity shifts) and intensities, the literature focuses only on severe and isolated shifts. In this work, we propose a comprehensive examination of distribution shifts across different intensity levels, investigating the nuanced impacts of both mild and severe shifts on the learning process and assessing the interplay between correlation and diversity shifts. We train models in three different scenarios considering synthetic and real correlation and diversity shifts, spamming across eight different levels of correlation shift, and evaluate them in both in-distribution and diversity-shifted test sets. Our experiments reveal three major findings: (1) Even small correlation shifts pose dataset-bias issues, presenting a risk of accumulating and combining unaccountable weak biases; (2) Models learn robust features in high- and low-shift scenarios but prefer spurious ones during test regardless; (3) Diversity shift can attenuate the reliance on spurious correlations. Our work has implications for distribution shift research and practice, providing new insights into how models learn and rely on spurious correlations under different types and intensities of shifts.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524000333",
    "keywords": [
      "Anthropology",
      "Artificial intelligence",
      "Computer science",
      "Correlation",
      "Distribution (mathematics)",
      "Diversity (politics)",
      "Econometrics",
      "Geometry",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Sociology",
      "Spurious relationship",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Bissoto",
        "given_name": "Alceu"
      },
      {
        "surname": "Barata",
        "given_name": "Catarina"
      },
      {
        "surname": "Valle",
        "given_name": "Eduardo"
      },
      {
        "surname": "Avila",
        "given_name": "Sandra"
      }
    ]
  },
  {
    "title": "Gait feature learning via spatio-temporal two-branch networks",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110090",
    "abstract": "Gait recognition has become a mainstream technology for identification due to its ability to capture gait features over long distances without subject cooperation and resistance to camouflage. However, current gait recognition methods face challenges as they use a single network to extract both temporal and spatial features from gait sequences. This approach imposes a heavy burden on the network, resulting in reduced extraction efficiency. To solve this problem, we propose a two-branch network to extract the spatio-temporal features of gait sequences. One branch primarily focuses on spatial feature extraction, while the other concentrates on temporal feature extraction. This design can make one branch focus on a specific task, leading to significant performance improvements. For temporal feature extraction, we propose the Global Temporal Information Extraction Network (GTIEN). GTIEN extracts temporal features of gait sequences by sequentially exploring the relationship between adjacent gait silhouettes from pixel and block levels. For spatial feature extraction, we introduce the Selective Horizontal Pyramid Convolution Network (SHPCN). SHPCN explores the multi-granularity features of gait silhouettes from global and local perspectives and assigns them appropriate weights according to their importance. By reasonably combining the temporal features extracted from GTIEN and spatial features extracted from SHPCN, we can effectively learn the spatial–temporal information of the gait sequences. Extensive experiments on CASIA-B and OUMVLP demonstrate that our method has better performance than some state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323007872",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Biometrics",
      "Computer science",
      "Computer vision",
      "Convolution (computer science)",
      "Face (sociological concept)",
      "Feature (linguistics)",
      "Feature extraction",
      "Gait",
      "Granularity",
      "Linguistics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physiology",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Yifan"
      },
      {
        "surname": "Li",
        "given_name": "Xuelong"
      }
    ]
  },
  {
    "title": "View-coherent correlation consistency for semi-supervised semantic segmentation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110089",
    "abstract": "Semi-supervised semantic segmentation needs rich and robust supervision for unlabeled data. However, promoting or punishing feature similarities with vanilla contrastive learning can be unreliable for semi-supervised semantic segmentation: pixel pairs are assigned as either positive or negative based on noisy pseudo labels, and both reliable and wrongly-assigned pairs receive uniform penalties. To address this issue, we propose correlation consistency learning, which leverages rich pairwise relationships in self-correlation matrices and matches them to the similarities between soft pseudo labels to provide robust supervision. Unlike vanilla contrastive learning, our approach prioritizes pairs with highly confident pseudo labels and applies weaker penalties for pairs that are less confident. We also introduce a strong semi-supervised learning pipeline that applies data augmentation in a view-coherent manner: even under complex augmentation strategies, for each pixel, a match can be found in different augmentation views. The novelties of the proposed method are the correlation consistency loss and the view-coherent data augmentation, and their combination gives us the view-coherent correlation consistency (VC 3 ) system, which achieves state-of-the-art results in several semi-supervised settings on two datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323007860",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Consistency (knowledge bases)",
      "Correlation",
      "Feature (linguistics)",
      "Geometry",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Natural language processing",
      "Pairwise comparison",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pipeline (software)",
      "Pixel",
      "Programming language",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Hou",
        "given_name": "Yunzhong"
      },
      {
        "surname": "Gould",
        "given_name": "Stephen"
      },
      {
        "surname": "Zheng",
        "given_name": "Liang"
      }
    ]
  },
  {
    "title": "NCL++: Nested Collaborative Learning for long-tailed visual recognition",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110064",
    "abstract": "Long-tailed visual recognition has received increasing attention in recent years. Due to the extremely imbalanced data distribution in long-tailed learning, the learning process shows great uncertainties. For example, the predictions of different experts on the same image vary remarkably despite the same training settings. To alleviate the uncertainty, we propose a Nested Collaborative Learning (NCL++) which tackles the long-tailed learning problem by a collaborative learning. To be specific, the collaborative learning consists of two folds, namely inter-expert collaborative learning (InterCL) and intra-expert collaborative learning (IntraCL). InterCL learns multiple experts collaboratively and concurrently, aiming to transfer the knowledge among different experts. IntraCL is similar to InterCL, but it aims to conduct the collaborative learning on multiple augmented copies of the same image within the single expert. To achieve the collaborative learning in long-tailed learning, the balanced online distillation is proposed to force the consistent predictions among different experts and augmented copies, which reduces the learning uncertainties. Moreover, in order to improve the meticulous distinguishing ability on the confusing categories, we further propose a Hard Category Mining (HCM), which selects the negative categories with high predicted scores as the hard categories. Then, the collaborative learning is formulated in a nested way, in which the learning is conducted on not just all categories from a full perspective but some hard categories from a partial perspective. Extensive experiments manifest the superiority of our method with outperforming the state-of-the-art whether with using a single model or an ensemble. The code will be publicly released.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323007616",
    "keywords": [
      "Active learning (machine learning)",
      "Artificial intelligence",
      "Collaborative learning",
      "Computer science",
      "Ensemble learning",
      "Knowledge management",
      "Machine learning",
      "Operating system",
      "Perspective (graphical)",
      "Process (computing)",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Tan",
        "given_name": "Zichang"
      },
      {
        "surname": "Li",
        "given_name": "Jun"
      },
      {
        "surname": "Du",
        "given_name": "Jinhao"
      },
      {
        "surname": "Wan",
        "given_name": "Jun"
      },
      {
        "surname": "Lei",
        "given_name": "Zhen"
      },
      {
        "surname": "Guo",
        "given_name": "Guodong"
      }
    ]
  },
  {
    "title": "KGSR: A kernel guided network for real-world blind super-resolution",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110095",
    "abstract": "In recent years, deep learning-based methods have emerged as dominant players in the field of super-resolution (SR), owing to their exceptional reconstruction performance. The primary driver of their effectiveness lies in their utilization of extensive sets of paired low-resolution and high-resolution images for training deep learning models. This training enables the models to effectively replicate the intricate mapping relationship between low-resolution and high-resolution images. Nevertheless, at present, acquiring a sufficient quantity of such image pairs that satisfy the requirements remains a formidable obstacle. Therefore, in order to break the restriction of limited training sets, self-supervised learning has been introduced to train a model for each low-quality image, without requiring pairwise ground-truths. However, they generally presuppose the generation of low-resolution (LR) images from their high-resolution (HR) counterparts using a pre-defined kernel, such as Bicubic downscaling. Such an assumption is seldom valid for real-world LR images, where degradation processes in practical applications are diverse, intricate, and often undisclosed. Therefore, when the presumed downscaling kernel does not match the actual one, the outcomes of state-of-the-art approaches degrade substantially. In this paper, we introduce KGSR, a kernel-guided network for addressing real-world blind SR, effectively avoiding requiring large training image pairs and transforming the blind image super-resolution problem into a supervised learning and non-blind scenario. Specifically, KGSR trains two networks, namely Upscaling and Downscaling, utilizing only patches extracted from the input test image. On one hand, owing to the cross-scale recurrence property of the SR kernel within a single image, the Downscaling network acquires knowledge of the image-specific degradation process through a generative adversarial network. Consequently, the Downscaling network is capable of generating a downsampled version of the LR test image even when the acquisition process is unknown or less than ideal. Additionally, we employ a dedicated discriminator to compel the Downscaling network to prioritize the characterization of kernel orientations. Conversely, a precise blur kernel has the potential to yield superior performance. Guided by the accurate image-specific SR kernel acquired from the Downscaling network and the downsampled LR input, the Upscaling network is capable of producing a high-quality HR image from the LR input. Within the Upscaling network, we additionally introduce an effective module for harnessing the acquired image-specific SR kernel. KGSR operates as a fully unsupervised approach, yet it can concurrently produce both the image-specific SR kernel and high-quality HR images. Comprehensive experiments conducted on standard benchmarks validate the efficacy of the proposed approach compared to state-of-the-art methodologies. Moreover, the suggested method can deliver visually appealing SR outcomes while exhibiting shorter processing times when applied to real-world LR images.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323007926",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Downscaling",
      "Geography",
      "Image (mathematics)",
      "Kernel (algebra)",
      "Machine learning",
      "Mathematics",
      "Meteorology",
      "Pairwise comparison",
      "Pattern recognition (psychology)",
      "Precipitation"
    ],
    "authors": [
      {
        "surname": "Yan",
        "given_name": "Qingsen"
      },
      {
        "surname": "Niu",
        "given_name": "Axi"
      },
      {
        "surname": "Wang",
        "given_name": "Chaoqun"
      },
      {
        "surname": "Dong",
        "given_name": "Wei"
      },
      {
        "surname": "Woźniak",
        "given_name": "Marcin"
      },
      {
        "surname": "Zhang",
        "given_name": "Yanning"
      }
    ]
  },
  {
    "title": "Region-adaptive and context-complementary cross modulation for RGB-T semantic segmentation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110092",
    "abstract": "RGB-Thermal (RGB-T) semantic segmentation is an emerging task aiming to improve the robustness of segmentation methods under extreme imaging conditions with the aid of thermal infrared modality. Foreground–background distinguishment and complementary information mining are two key challenges of this task. Recent methods use naive channel attention and cross-attention to tackle these challenges, but they still struggle with a sub-optimal solution where salient foreground features and noisy background ones might be equally modulated without distinction. The quadratic computational overhead of cross-attention also blocks its application on high-resolution features. Moreover, lacking complementary information mining in the encoding phase hinders the comprehensive scene encoding as well. To alleviate these limitations, we propose a cross modulation process with two collaborative components. The first Region-Adaptive Channel Modulation (RACM) module conducts channel attention at a fine-grained region level where foreground and background regions can be modulated differently in each channel. The second Context-Complementary Spatial Modulation (CCSM) module mines and transfers complementary information between the two modalities early in the encoding phase. Experiments show that our method achieves state-of-the-art performances on current RGB-T segmentation benchmarks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323007896",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Biochemistry",
      "Channel (broadcasting)",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Encoding (memory)",
      "Gene",
      "Geography",
      "Pattern recognition (psychology)",
      "RGB color model",
      "Robustness (evolution)",
      "Segmentation",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Peng",
        "given_name": "Fengguang"
      },
      {
        "surname": "Ding",
        "given_name": "Zihan"
      },
      {
        "surname": "Chen",
        "given_name": "Ziming"
      },
      {
        "surname": "Wang",
        "given_name": "Gang"
      },
      {
        "surname": "Hui",
        "given_name": "Tianrui"
      },
      {
        "surname": "Liu",
        "given_name": "Si"
      },
      {
        "surname": "Shi",
        "given_name": "Hang"
      }
    ]
  },
  {
    "title": "Subdivided Mask Dispersion Framework for semi-supervised semantic segmentation",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.01.025",
    "abstract": "Learning the relationship between weak and strong perturbations has been considered a major part of semi-supervised semantic segmentation. We observed two problems with a publicly used perturbation method, which randomly generates a mask with a single large bounding box. The large single bounding box that entirely covers the important object components in an image, hindering the model from capturing partial object information. Furthermore, training the model with a single large bounding box as an image-level perturbation causes the model to be biased towards the shape of the large squared box, rather than the deformable object component shapes. In this paper, we propose Subdivided Mask Dispersion Framework (SMDF) to solve these problems. Our framework disperses the large squared box into small multi-scale boxes, capturing the crucial multi-scaled object components in the image. SMDF achieves state-of-the-art performance on five data partitions of PASCAL dataset and three partitions of Extended SBD dataset. Our extensive ablation studies show the effectiveness of dispersed small multi-scale bounding boxes in semi-supervised semantic segmentation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524000254",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Dispersion (optics)",
      "Natural language processing",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Yooseung"
      },
      {
        "surname": "Jang",
        "given_name": "Jaehyuk"
      },
      {
        "surname": "Kim",
        "given_name": "Changick"
      }
    ]
  },
  {
    "title": "ECLAD: Extracting Concepts with Local Aggregated Descriptors",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110146",
    "abstract": "Convolutional neural networks (CNNs) are increasingly being used in critical systems, where robustness and alignment are crucial. In this context, the field of explainable artificial intelligence has proposed the generation of high-level explanations of the prediction process of CNNs through concept extraction. While these methods can detect whether or not a concept is present in an image, they are unable to determine its location. What is more, a fair comparison of such approaches is difficult due to a lack of proper validation procedures. To address these issues, we propose a novel method for automatic concept extraction and localization based on representations obtained through pixel-wise aggregations of CNN activation maps. Further, we introduce a process for the quantitative comparison and validation of concept-extraction techniques based on synthetic datasets with pixel-wise annotations of their main components, mitigating possible confirmation biases induced by human visual inspection. Extensive experimentation on both synthetic and real-world datasets demonstrates that our method outperforms state-of-the-art alternatives.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323008439",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Biology",
      "Chemistry",
      "Computer science",
      "Context (archaeology)",
      "Convolutional neural network",
      "Data mining",
      "Field (mathematics)",
      "Gene",
      "Machine learning",
      "Mathematics",
      "Operating system",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Pixel",
      "Process (computing)",
      "Pure mathematics",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Posada-Moreno",
        "given_name": "Andrés Felipe"
      },
      {
        "surname": "Surya",
        "given_name": "Nikita"
      },
      {
        "surname": "Trimpe",
        "given_name": "Sebastian"
      }
    ]
  },
  {
    "title": "Self-Supervised Adversarial Variational Learning",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110156",
    "abstract": "A natural approach for representation learning is to combine the inference mechanisms of VAEs and the generative abilities of GANs, within a new model, namely VAEGAN. Most existing VAEGAN models would jointly train the generator and inference modules, which has limitations when learning representations generated by a pre-trained GAN model without data. In this paper, we develop a novel hybrid model, called the Self-Supervised Adversarial Variational Learning (SS-AVL) which introduces a two-step optimization procedure training separately the generator and the inference model. The primary advantage of SS-AVL over existing VAEGAN models is that SS-AVL optimizes the inference models in a self-supervised learning manner where the samples used for training the inference models are drawn from the generator distribution instead of using real samples. This can allow SS-AVL to learn representations from arbitrary GAN models without using real data. Additionally, we employ information maximization into the context of increasing the maximum likelihood, which encourages SS-AVL to learn meaningful latent representations. We perform extensive experiments to demonstrate the effectiveness of the proposed SS-AVL model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323008531",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Expectation–maximization algorithm",
      "Generator (circuit theory)",
      "Inference",
      "Law",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Maximization",
      "Maximum likelihood",
      "Paleontology",
      "Physics",
      "Political science",
      "Politics",
      "Power (physics)",
      "Quantum mechanics",
      "Representation (politics)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Ye",
        "given_name": "Fei"
      },
      {
        "surname": "Bors",
        "given_name": "Adrian. G."
      }
    ]
  },
  {
    "title": "Nonmonotone variable projection algorithms for matrix decomposition with missing data",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110150",
    "abstract": "This paper investigates algorithms for matrix factorization when some or many components are missing, a problem that arises frequently in computer vision and pattern recognition. We demonstrate that the Jacobian used in the damped Wiberg (DW) method is exactly the same as that of Kaufman’s simplified variable projection (VP) algorithm. Our analysis provides a novel perspective on the efficiency of VP algorithms by improving the strong convexity of the approximate function. To enhance numerical stability, we set a lower bound on the damping parameter instead of adding a null space like the DW algorithm. Another challenge of low-rank matrix decomposition with missing data is the existence of many sharp local minima, which are often distributed in narrow valleys of the landscape of objection functions. Falling into such minima tends to result in poor reconstruction results. To address this issue, we design a non-monotonic VP algorithm, which can facilitate the algorithm to escape from sharp minima and converge to flatter minima. Numerical experiments confirm the effectiveness and efficiency of the proposed nonmonotone VP algorithm.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323008476",
    "keywords": [
      "Algorithm",
      "Applied mathematics",
      "Biology",
      "Combinatorics",
      "Composite material",
      "Eigenvalues and eigenvectors",
      "Evolutionary biology",
      "Function (biology)",
      "Jacobian matrix and determinant",
      "Materials science",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Matrix (chemical analysis)",
      "Matrix decomposition",
      "Maxima and minima",
      "Missing data",
      "Monotonic function",
      "Physics",
      "Projection (relational algebra)",
      "Quantum mechanics",
      "Rank (graph theory)",
      "Statistics",
      "Variable (mathematics)"
    ],
    "authors": [
      {
        "surname": "Su",
        "given_name": "Xiang-xiang"
      },
      {
        "surname": "Gan",
        "given_name": "Min"
      },
      {
        "surname": "Chen",
        "given_name": "Guang-yong"
      },
      {
        "surname": "Yang",
        "given_name": "Lin"
      },
      {
        "surname": "Jin",
        "given_name": "Jun-wei"
      }
    ]
  },
  {
    "title": "IME: Efficient list-based method for incremental mining of maximal erasable patterns",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110166",
    "abstract": "Erasable pattern mining can help factories facing a financial crisis increase productivity by identifying and eliminating unprofitable products. The Flag-GenMax-EI algorithm extracts Maximal Erasable Itemsets (MEIs); however, it does not support dynamic data. In practice, many applications create databases incrementally. Using the Flag-GenMax-EI algorithm to mine maximal erasable patterns from incremental databases is clearly very costly because it must be run each time. In this paper, an efficient method called IME is proposed for incremental mining of maximal erasable patterns. IMEI-List and IMEP-List are two new data structures introduced by the proposed method. These lists allow the algorithm to update all tree nodes without rescanning the updated database (original database + new database) and recreating the nodes. This is the first study of incremental mining of maximal erasable patterns. Extensive experimental results on dense and sparse incremental data show that the proposed algorithm improves scalability. It extracts MEIs much faster than the Flag-GenMax-EI algorithm in different modes of database update.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323008634",
    "keywords": [
      "Algebra over a field",
      "Algorithm",
      "Association rule learning",
      "Computer science",
      "Data mining",
      "Database",
      "Flag (linear algebra)",
      "Mathematics",
      "Parallel computing",
      "Pure mathematics",
      "Scalability"
    ],
    "authors": [
      {
        "surname": "Davashi",
        "given_name": "Razieh"
      }
    ]
  },
  {
    "title": "Japanese historical character recognition by focusing on character parts",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110181",
    "abstract": "Japanese historical documents provide valuable information. Character recognition is a critical technology for the digitalization of historical documents. Sample imbalance is a significant obstacle in recognizing Japanese historical characters, kuzushiji. Thousands of kuzushiji only have less than a few samples. Thus, recognition performance deteriorates greatly in kuzushiji with a few samples. In this study, we propose a framework for transferring knowledge of character parts from font to kuzushiji. The pretraining learns character parts from synthesized font images. However, fine-tuning to kuzushiji is more complex. We propose calculating a mean squared error loss between feature vectors of kuzushiji and font images, resulting in consistent feature vectors in kuzushiji and font. Consequently, we can perform zero-shot recognition for kuzushiji using the font images of zero-sampled kuzushiji. The experimental results show that the proposed method recognized zero-sampled kuzushiji at approximately 48% accuracy. Consequently, we significantly expand the number of recognizable kuzushiji.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323008786",
    "keywords": [
      "Artificial intelligence",
      "Character (mathematics)",
      "Character recognition",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Feature vector",
      "Font",
      "Geometry",
      "Image (mathematics)",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Speech recognition",
      "Zero (linguistics)"
    ],
    "authors": [
      {
        "surname": "Ishikawa",
        "given_name": "Takuru"
      },
      {
        "surname": "Miyazaki",
        "given_name": "Tomo"
      },
      {
        "surname": "Omachi",
        "given_name": "Shinichiro"
      }
    ]
  },
  {
    "title": "MI 3 C: Mining intra- and inter-image context for person search",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110169",
    "abstract": "Person search aims to localize the queried person from a gallery of uncropped, realistic images. Unlike re-identification (Re-ID), person search deals with the entire scene image containing rich and diverse visual context information. However, existing works mainly focus on the person’s appearance while ignoring other essential intra- and inter-image context information. To comprehensively leverage the intra- and inter-image context, we propose a unified framework termed MI 3 C including the Intra-image Multi-View Context network (IMVC) and the Inter-image Group Context Ranking algorithm (IGCR). Concretely, the IMVC integrates the features from the scene, surrounding, instance, and part views collaboratively to generate the final ID feature for person search. Furthermore, the IGCR algorithm employs group matching results between query and gallery image pairs to measure the holistic image matching similarity, which is adopted as part of the sorting metric to yield a more robust ranking among the whole gallery. Extensive experiments on two popular person search benchmarks demonstrate that by mining intra- and inter-image context, our method outperforms previous state-of-the-art methods by conspicuous margins. Specifically, we achieve 96.7% mAP and 97.1% top-1 accuracy on the CUHK-SYSU dataset, 55.6% mAP, and 90.8% top-1 accuracy on the PRW dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032300866X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Economics",
      "Image (mathematics)",
      "Information retrieval",
      "Leverage (statistics)",
      "Matching (statistics)",
      "Mathematics",
      "Metric (unit)",
      "Operations management",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Ranking (information retrieval)",
      "Similarity (geometry)",
      "Sorting",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Tang",
        "given_name": "Zongheng"
      },
      {
        "surname": "Gao",
        "given_name": "Yulu"
      },
      {
        "surname": "Hui",
        "given_name": "Tianrui"
      },
      {
        "surname": "Peng",
        "given_name": "Fengguang"
      },
      {
        "surname": "Liu",
        "given_name": "Si"
      }
    ]
  },
  {
    "title": "DGD-cGAN: A dual generator for image dewatering and restoration",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110159",
    "abstract": "Underwater images are usually covered with a blue–greenish colour cast, making them distorted, blurry or low in contrast. This phenomenon occurs due to the light attenuation given by the scattering and absorption in the water column. In this paper we present an image dewatering approach motivated upon the observation that the image formation model can be used to drive the learning process by constraining the loss function and making used of paired data. To this end, we employ a conditional generative adversarial network (cGAN) with two generators. Our Dual Generator Dewatering cGAN (DGD-cGAN) removes the haze and colour cast induced by the water column and restores the true colours of underwater scenes whereby the effects of various attenuation and scattering phenomena that occur in underwater images are tackled by the two generators. The first generator takes as input the underwater image and predicts the dewatered scene, while the second generator learns the underwater image formation process by implementing a custom loss function based upon the transmission and the veiling light components of the image formation model. Extensive experiments show that DGD-cGAN consistently delivers a margin of improvement as compared with state-of-the-art methods on several widely available datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323008567",
    "keywords": [
      "Artificial intelligence",
      "Attenuation",
      "Computer science",
      "Dewatering",
      "Generator (circuit theory)",
      "Geology",
      "Geotechnical engineering",
      "Machine learning",
      "Margin (machine learning)",
      "Materials science",
      "Oceanography",
      "Optics",
      "Physics",
      "Power (physics)",
      "Quantum mechanics",
      "Scattering",
      "Underwater"
    ],
    "authors": [
      {
        "surname": "Gonzalez-Sabbagh",
        "given_name": "Salma"
      },
      {
        "surname": "Robles-Kelly",
        "given_name": "Antonio"
      },
      {
        "surname": "Gao",
        "given_name": "Shang"
      }
    ]
  },
  {
    "title": "Discriminative multi-label feature selection with adaptive graph diffusion",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110154",
    "abstract": "Feature selection can alleviate the problem of the curse of dimensionality by selecting more discriminative features, which plays an important role in multi-label learning. Recently, embedded feature selection methods have received increasing attentions. However, most existing methods learn the low-dimensional embeddings under the guidance of the local structure between the original instance pairs, thereby ignoring the high-order structure between instances and being sensitive to noise in the original features. To address these issues, we propose a feature selection method named discriminative multi-label feature selection with adaptive graph diffusion (MFS-AGD). Specifically, we first construct a graph embedding learning framework equipped with adaptive graph diffusion to uncover a latent subspace that preserves the higher-order structure information between four tuples. Then, the Hilbert–Schmidt independence criterion (HSIC) is incorporated into the embedding learning framework to ensure the maximum dependency between the latent representation and labels. Benefiting from the interactive optimization of the feature selection matrix, latent representation and similarity graph, the selected features can accurately explore the higher-order structural and supervised information of data. By further considering the correlation between labels, MFS-AG is extended to a more discriminative version,i.e., LMFS-AG. Extensive experimental results on various benchmark data sets validate the advantages of the proposed MFS-AGD and LMFS-AGD methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323008518",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Dimensionality reduction",
      "Discriminative model",
      "Embedding",
      "Feature (linguistics)",
      "Feature learning",
      "Feature selection",
      "Graph",
      "Linguistics",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Subspace topology",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Ma",
        "given_name": "Jiajun"
      },
      {
        "surname": "Xu",
        "given_name": "Fei"
      },
      {
        "surname": "Rong",
        "given_name": "Xiaofeng"
      }
    ]
  },
  {
    "title": "Hybrid ARMA-GARCH-Neural Networks for intraday strategy exploration in high-frequency trading",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110139",
    "abstract": "The frequency of armed conflicts increased during the last 20 years. The problems of the emergence of military disputes, not only concern social parameters, but also economic and financial dimensions. This study examines the potential impact of global geopolitical events on the stock market prices of the Dow Jones U.S. Aerospace & Defense Index and Foreign Exchange (FOREX) markets movements. We analyse whether defence stocks and exchange rate perform similarly during military incidents or geopolitical crises. We built an Autoregressive Moving Average Model with a Generalized Autoregressive Conditional Heteroskedasticity process (ARMA-GARCH) with the machine learning methods of Neural Networks, Deep Recurrent Convolutional Neural Networks, Deep Neural Decision Trees, Quantum Neural Networks, and Quantum Recurrent Neural Networks, aimed at detecting intraday patterns for forecasting defence stock market and FOREX markets disturbances in a market microstructure framework. The empirical results provide preliminary findings on the foreseeability of market disturbances and small differences are observed before and during geopolitical events. Additionally, we confirm the effectiveness of the hybrid model ARMA-GARCH with the machine learning approaches, being ARMA-GARCH-Quantum Recurrent Neural Network the technique that achieves the best accuracy results. Our work has a large potential impact on investment market agents and portfolio managers, as shocks from geopolitical events could provide a new methodology to support the decision-making process for trading in High-Frequency Trading.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323008361",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Autoregressive conditional heteroskedasticity",
      "Autoregressive model",
      "Autoregressive–moving-average model",
      "Biology",
      "Computer science",
      "Econometrics",
      "Economics",
      "Exchange rate",
      "Finance",
      "Financial economics",
      "Financial market",
      "Foreign exchange market",
      "Horse",
      "Paleontology",
      "Stock exchange",
      "Stock market",
      "Stock market index",
      "Volatility (finance)"
    ],
    "authors": [
      {
        "surname": "Alaminos",
        "given_name": "David"
      },
      {
        "surname": "Salas",
        "given_name": "M. Belén"
      },
      {
        "surname": "Partal-Ureña",
        "given_name": "Antonio"
      }
    ]
  },
  {
    "title": "SideNet: Learning representations from interactive side information for zero-shot Chinese character recognition",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110208",
    "abstract": "Existing methods for zero-shot Chinese character recognition usually exploit a single type of side information such as radicals, glyphs, or strokes to establish a mapping with the input characters for the recognition of unseen categories. However, these approaches have two limitations. Firstly, the mappings are inefficient owing to their complexity. Some existing methods design radical-level mappings using a non-differentiable dictionary-matching strategy, whereas others construct sophisticated embeddings to map seen and unseen characters into a unified latent space. Although the latter approach is straightforward, it lacks a learnable scheme for explicit structure construction. Secondly, the complementarity within multiple types of side information has not been effectively explored. For example, the radicals provide structural knowledge at an abstract level, whereas glyphs offer detailed information on their figurative counterparts. To this end, we propose a new method called SideNet that jointly learns character-level representations assisted by two types of interactive side information: radicals and glyphs. SideNet contains a structural conversion module that extracts radical knowledge via dimensional decomposition, and a spatial conversion module that encodes the radical counting map to produce an interactive outcome between radicals and glyph. Finally, we propose a new classifier that integrates the converted features by a similarity-guided fusion mechanism. To the best of our knowledge, this study represents the first attempt to integrate these two types of side information and explore a joint representation for zero-shot learning. Experiments show that SideNet consistently outperforms existing methods by a significant margin in diverse scenarios, including handwriting, printed art, natural scenes, and ancient Chinese characters, which demonstrates the potential of joint learning with multiple types of side information.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323009056",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Classifier (UML)",
      "Complementarity (molecular biology)",
      "Computer science",
      "Genetics",
      "Glyph (data visualization)",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Visualization"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Ziyan"
      },
      {
        "surname": "Huang",
        "given_name": "Yuhao"
      },
      {
        "surname": "Peng",
        "given_name": "Dezhi"
      },
      {
        "surname": "He",
        "given_name": "Mengchao"
      },
      {
        "surname": "Jin",
        "given_name": "Lianwen"
      }
    ]
  },
  {
    "title": "YOLOPX: Anchor-free multi-task learning network for panoptic driving perception",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110152",
    "abstract": "Panoptic driving perception encompasses traffic object detection, drivable area segmentation, and lane detection. Existing methods typically utilize anchor-based multi-task learning networks to complete this task. While these methods yield promising results, they suffer from the inherent limitations of anchor-based detectors. In this paper, we propose YOLOPX, a simple and efficient anchor-free multi-task learning network for panoptic driving perception. To the best of our knowledge, this is the first work to employ the anchor-free detection head in panoptic driving perception. This anchor-free manner simplifies training by avoiding anchor-related heuristic tuning, and enhances the adaptability and scalability of our multi-task learning network. In addition, YOLOPX incorporates a novel lane detection head that combines multi-scale high-resolution features and long-distance contextual dependencies to improve segmentation performance. Beyond structure optimization, we propose optimization improvements to enhance network training, enabling our multi-task learning network to achieve optimal performance through simple end-to-end training. Experimental results on the challenging BDD100K dataset demonstrate the state-of-the-art (SOTA) performance of YOLOPX: it achieves 93.7% recall and 83.3% mAP50 on traffic object detection, 93.2% mIoU on drivable area segmentation, and 88.6% accuracy and 27.2% IoU on lane detection. Moreover, YOLOPX has faster inference speed compared to the lightweight network YOLOP. Consequently, YOLOPX is a powerful solution for panoptic driving perception problems. The code is available at https://github.com/jiaoZ7688/YOLOPX.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032300849X",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer vision",
      "Database",
      "Engineering",
      "Human–computer interaction",
      "Inference",
      "Machine learning",
      "Neuroscience",
      "Object detection",
      "Perception",
      "Scalability",
      "Segmentation",
      "Systems engineering",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Zhan",
        "given_name": "Jiao"
      },
      {
        "surname": "Luo",
        "given_name": "Yarong"
      },
      {
        "surname": "Guo",
        "given_name": "Chi"
      },
      {
        "surname": "Wu",
        "given_name": "Yejun"
      },
      {
        "surname": "Meng",
        "given_name": "Jiawei"
      },
      {
        "surname": "Liu",
        "given_name": "Jingnan"
      }
    ]
  },
  {
    "title": "Prototype Guided Pseudo Labeling and Perturbation-based Active Learning for domain adaptive semantic segmentation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110203",
    "abstract": "This work aims at active domain adaptation to transfer knowledge from a fully-labeled source domain to an entirely unlabeled target domain. During the active learning period, some pixels in the target domain are selected and annotated as active labels through several selection rounds. Such active labels can improve the target domain model performance greatly. However, existing approaches solely rely on pseudo labels, highly-confident classifier predictions on target images, to train the initial target domain model, resulting in a sub-optimal solution for model training. This initial model will be used for active label selection. Meanwhile, previous methods use entropy-based measurement to select pixels for annotation, which fails to detect high-confidence errors in earlier selection rounds due to the absence of target information. To address these issues, we propose a prototype-guided pseudo-label generating approach that leverages the relationships between source prototypes and target features. It generates target pseudo labels based on diverse source prototypes, thereby alleviating the issue of classifier predictions. Furthermore, perturbation-based uncertainty measurement, calculating the discrepancy between the target image and the augmented one, is introduced to find the areas with unstable predictions. Extensive experiments demonstrate that our approach outperforms state-of-the-art active domain adaptation methods on two benchmarks, GTAV → Cityscapes, and SYNTHIA → Cityscapes. Comparable performance is also achieved when compared to fully-supervised methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323009007",
    "keywords": [
      "Annotation",
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Computer vision",
      "Domain adaptation",
      "Entropy (arrow of time)",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Physics",
      "Pixel",
      "Quantum mechanics",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Peng",
        "given_name": "Junkun"
      },
      {
        "surname": "Sun",
        "given_name": "Mingjie"
      },
      {
        "surname": "Lim",
        "given_name": "Eng Gee"
      },
      {
        "surname": "Wang",
        "given_name": "Qiufeng"
      },
      {
        "surname": "Xiao",
        "given_name": "Jimin"
      }
    ]
  },
  {
    "title": "Multi-grained clip focus for skeleton-based action recognition",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110188",
    "abstract": "Joint-level and part-level information are crucial for modeling actions with different granularity. In addition, the relevant information on different joints between consecutive frames is very useful for skeleton-based action recognition. To effectively capture the action information, a new multi-grained clip focus network (MGCF-Net) is proposed. Firstly, the skeleton sequence is divided into multiple clips, each containing several consecutive frames. According to the structure of the human body, each clip is divided into several tuples. Then an intra-clip attention module is proposed to capture intra-clip action information. Specifically, multi-head self-attention is divided into two parts, obtaining relevant information at the joint and part levels, and integrating the information captured from these two parts to obtain multi-grained contextual features. In addition, an inter-clip focus module is used to capture the key information of several consecutive sub-actions, which will help to distinguish similar actions. On two large-scale benchmarks for skeleton-based action recognition, our method achieves the most advanced performance, and its effectiveness has been verified.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323008853",
    "keywords": [
      "Action (physics)",
      "Action recognition",
      "Architectural engineering",
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Discrete mathematics",
      "Engineering",
      "Focus (optics)",
      "Granularity",
      "Joint (building)",
      "Key (lock)",
      "Mathematics",
      "Operating system",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Programming language",
      "Quantum mechanics",
      "Skeleton (computer programming)",
      "Tuple"
    ],
    "authors": [
      {
        "surname": "Qiu",
        "given_name": "Helei"
      },
      {
        "surname": "Hou",
        "given_name": "Biao"
      }
    ]
  },
  {
    "title": "Human Gait Recognition by using Two Stream Neural Network along with Spatial and Temporal Features",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.02.010",
    "abstract": "Human Gait Recognition (HGR) is referred to as a biometric tactic that is broadly used for the recognition of an individual by using the pattern of walking. There are some key factors such as angle variation, clothing variation, foot shadows, and carrying conditions that affect the human gait. In this work, a new approach is proposed for the HGR that contains five major steps. In the first step, the video data is converted into image frames. In the second step, RGB to GRAY conversion is carried out. After that, a two-stream network is designed by using a 55-layer CNN model called CNN-55 trained on CIFAR-100. The CNN-55 is designed from scratch and trained on the CIFAR-100 dataset by selecting hyperparameters. This pre-trained CNN-55 is used to build a two-stream network. In Stream-1 the optical flow frames are obtained by Horn and Schunk algorithm. These frames are fed into a CNN-55 to extract temporal features. In Stream-2 the GRAY frames are fed to the CNN-55 model for extraction of spatial features. After that, both vectors are serially fused. In the fourth step, the fused feature vector is fed into the Genetic Algorithm for optimization. Finally, the feature vector is fed into the One-Versus-All SVM classifier for recognition. The system is tested on all CASIA-B angles such as 000, 180, 360, 540, 720, 900, 1080, 1260, 1440, 1620, and 1800 which provides accuracy of 97.10%, 96.80%, 94.60%, 98.0%, 98.30%, 96.80%, 97.60, 96.90%, 99.60%, 96.80%, and 97.60%, respectively. The proposed method produces better outcomes compared to recent techniques.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524000436",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer vision",
      "Gait",
      "Medicine",
      "Pattern recognition (psychology)",
      "Physical medicine and rehabilitation"
    ],
    "authors": [
      {
        "surname": "Mehmood",
        "given_name": "Asif"
      },
      {
        "surname": "Amin",
        "given_name": "Javeria"
      },
      {
        "surname": "Sharif",
        "given_name": "Muhammad"
      },
      {
        "surname": "Kadry",
        "given_name": "Seifedine"
      }
    ]
  },
  {
    "title": "A full-scale hierarchical encoder-decoder network with cascading edge-prior for infrared and visible image fusion",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110192",
    "abstract": "Most existing deep learning-based infrared and visible image fusion methods always fail to consider the full-scale long-range correlation and the prior knowledge, resulting in the fused images with low-contrast salient objects and blurred edge details. To overcome these drawbacks, a full-scale hierarchical encoder-decoder network with cascading edge-prior for infrared and visible image fusion is proposed. First, a top-down encoder extracts the hierarchical representations from source image. Then, to inject edge priors into the network and capture the progressive semantic correlations, a triple fusion mechanism is proposed including edge image fusion based on maximum fusion strategy, single-scale shallow layer fusion and full-scale semantic layer fusion based on dual-attention fusion (DAF) strategy. The fused full-scale semantic features (F2SF) are obtained by capturing the long-range affinities of the full-scale. At the same time, a cascading edge-prior branch (CEPB) is designed to embed the fused edge knowledge into fused single-scale shallow features, jointly guiding the decoder to focus on abundant details layer-by-layer on the basis of F2SF, thus recovering the edge and texture details of the fused image well. Finally, a novel loss function consisting of SSIM, intensity and edge loss is constructed to further maintain the network with better edge representation and reconstruction capability. Compared with existing state-of-the-art fusion methods, the proposed method has better performance in terms of both visual evaluation and objective evaluation on public datasets. The source code is available at https://github.com/lxq-jnu/FSFusion.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323008890",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Encoder",
      "Enhanced Data Rates for GSM Evolution",
      "Fusion",
      "Fusion rules",
      "Image (mathematics)",
      "Image fusion",
      "Linguistics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Scale (ratio)"
    ],
    "authors": [
      {
        "surname": "Luo",
        "given_name": "Xiaoqing"
      },
      {
        "surname": "Wang",
        "given_name": "Juan"
      },
      {
        "surname": "Zhang",
        "given_name": "Zhancheng"
      },
      {
        "surname": "Wu",
        "given_name": "Xiao-jun"
      }
    ]
  },
  {
    "title": "Graph meets probabilistic generation model: A new perspective for graph disentanglement",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110153",
    "abstract": "Different from the existing graph disentanglement neural networks, we interpret the graph entanglement under a probabilistic generation framework in this paper. With this foundation, a Mixed Probabilistic Generation Model induced Graph Disentanglement Network (MPGD) is proposed. Considering the disentangled components corresponding to different factors as obeying specific distributions, a generalized probabilistic aggregation scheme among components is deduced theoretically. As a key part of the mixed probabilistic generative model, we provide a solution for estimating the mixture probabilities using self-attention and an in-depth analysis of its close connection with the classical EM parameter estimation method. Meanwhile, a way of probabilistic aggregation is formulated to obtain the node representation in embedding space. In addition, the prior mixture probabilities are formulated as an auxiliary factor-aware representation to facilitate the twin-branch prediction. A variety of experiments show that MPGD achieves more competitive performance than some existing state-of-the-art methods while having ideal disentangling effects. The code implementation is available in https://github.com/GiorgioPeng/MPGD.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323008506",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Decoding methods",
      "Embedding",
      "Factor graph",
      "Graph",
      "Law",
      "Political science",
      "Politics",
      "Probabilistic logic",
      "Representation (politics)",
      "Statistical model",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Peng",
        "given_name": "Zouzhang"
      },
      {
        "surname": "Zheng",
        "given_name": "Shuai"
      },
      {
        "surname": "Zhu",
        "given_name": "Zhenfeng"
      },
      {
        "surname": "Liu",
        "given_name": "Zhizhe"
      },
      {
        "surname": "Cheng",
        "given_name": "Jian"
      },
      {
        "surname": "Dong",
        "given_name": "Honghui"
      },
      {
        "surname": "Zhao",
        "given_name": "Yao"
      }
    ]
  },
  {
    "title": "Anomaly detection via gating highway connection for retinal fundus images",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110167",
    "abstract": "Since the labels for medical images are challenging to collect in real scenarios, especially for rare diseases, fully supervised methods cannot achieve robust performance for clinical anomaly detection. Recent research tried to tackle this problem by training the anomaly detection framework using only normal data. Reconstruction-based methods, e.g., auto-encoder, achieved impressive performances in the anomaly detection task. However, most existing methods adopted the straightforward backbone architecture (i.e., encoder-and-decoder) for image reconstruction. The design of a skip connection, which can directly transfer information between the encoder and decoder, is rarely used. Since the existing U-Net has demonstrated the effectiveness of skip connections for image reconstruction tasks, in this paper, we first use the dynamic gating strategy to achieve the usage of skip connections in existing reconstruction-based anomaly detection methods and then propose a novel gating highway connection module to adaptively integrate skip connections into the framework and boost its anomaly detection performance, namely GatingAno. Furthermore, we formulate an auxiliary task, namely histograms of oriented gradients (HOG) prediction, to encourage the framework to exploit contextual information from fundus images in a self-driven manner, which increases the robustness of feature representation extracted from the healthy samples. Last but not least, to improve the model generalization for anomalous data, we introduce an adversarial strategy for the training of our multi-task framework. Experimental results on the publicly available datasets, i.e., IDRiD and ADAM, validate the superiority of our method for detecting abnormalities in retinal fundus images. The source code is available at https://github.com/WentianZhang-ML/GatingAno.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323008646",
    "keywords": [
      "Anomaly detection",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Encoder",
      "Exploit",
      "Gene",
      "Operating system",
      "Pattern recognition (psychology)",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Wentian"
      },
      {
        "surname": "Liu",
        "given_name": "Haozhe"
      },
      {
        "surname": "Xie",
        "given_name": "Jinheng"
      },
      {
        "surname": "Huang",
        "given_name": "Yawen"
      },
      {
        "surname": "Zhang",
        "given_name": "Yu"
      },
      {
        "surname": "Li",
        "given_name": "Yuexiang"
      },
      {
        "surname": "Ramachandra",
        "given_name": "Raghavendra"
      },
      {
        "surname": "Zheng",
        "given_name": "Yefeng"
      }
    ]
  },
  {
    "title": "Adaptive watermarking with self-mutual check parameters in deep neural networks",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.02.018",
    "abstract": "Artificial Intelligence has found wide application, but also poses risks due to unintentional or malicious tampering during deployment. Regular checks are therefore necessary to detect and prevent such risks. Fragile watermarking is a technique used to identify tampering in AI models. However, previous methods have faced challenges including risks of omission, additional information transmission, and inability to locate tampering precisely. In this paper, we propose a method for detecting tampered parameters and bits, which can be used to detect, locate, and restore parameters that have been tampered with. We also propose an adaptive embedding method that maximizes information capacity while maintaining model accuracy. Our approach was tested on multiple neural networks subjected to attacks that modified weight parameters, and our results demonstrate that our method achieved great recovery performance when the modification rate was below 20%. Furthermore, for models where watermarking significantly affected accuracy, we utilized an adaptive bit technique to recover more than 15% of the accuracy loss of the model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524000515",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Digital watermarking",
      "Image (mathematics)",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Gao",
        "given_name": "Zhenzhe"
      },
      {
        "surname": "Yin",
        "given_name": "Zhaoxia"
      },
      {
        "surname": "Zhan",
        "given_name": "Hongjian"
      },
      {
        "surname": "Yin",
        "given_name": "Heng"
      },
      {
        "surname": "Lu",
        "given_name": "Yue"
      }
    ]
  },
  {
    "title": "Linearized alternating direction method of multipliers for elastic-net support vector machines",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110134",
    "abstract": "In many high-dimensional datasets, the phenomenon that features are relevant often occurs. Elastic-net regularization is widely used in support vector machines (SVMs) because it can automatically perform feature selection and encourage highly correlated features to be selected or removed together. Recently, some effective algorithms have been proposed to solve the elastic-net SVMs with different convex loss functions, such as hinge, squared hinge, huberized hinge, pinball and huberized pinball. In this paper, we develop a linearized alternating direction method of multipliers (LADMM) algorithm to solve above elastic-net SVMs. In addition, our algorithm can be applied to solve some new elastic-net SVMs such as elastic-net least squares SVM. Compared with some existing algorithms, our algorithm has comparable or better performances in terms of computational cost and accuracy. Under mild conditions, we prove the convergence and derive convergence rate of our algorithm. Furthermore, numerical experiments on synthetic and real datasets demonstrate the feasibility and validity of the proposed algorithm.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323008312",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Channel (broadcasting)",
      "Computer network",
      "Computer science",
      "Convergence (economics)",
      "Economic growth",
      "Economics",
      "Elastic net regularization",
      "Engineering",
      "Feature (linguistics)",
      "Feature selection",
      "Geometry",
      "Hinge",
      "Hinge loss",
      "Linguistics",
      "Mathematical optimization",
      "Mathematics",
      "Mechanical engineering",
      "Net (polyhedron)",
      "Philosophy",
      "Rate of convergence",
      "Regular polygon",
      "Regularization (linguistics)",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Liang",
        "given_name": "Rongmei"
      },
      {
        "surname": "Wu",
        "given_name": "Xiaofei"
      },
      {
        "surname": "Zhang",
        "given_name": "Zhimin"
      }
    ]
  },
  {
    "title": "Adaptive regularized ensemble for evolving data stream classification",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.02.026",
    "abstract": "Extracting knowledge from data streams requires fast incremental algorithms that are able to handle unlimited processing and ever-changing data with finite memory. A strategy for this challenge is the use of ensembles owing to their ability to tackle concept drift and achieve highly accurate predictions. However, ensembles often require a lot of computational resources. In this study, we propose a novel ensemble-based classification algorithm for data streams, Adaptive Regularized Ensemble (ARE), with low demand for computational resources. The algorithm combines strategies that contribute to high prediction accuracy using only incorrectly classified instances into the training step, random-sized feature subspace for each ensemble element and classifier selection for final ensemble voting. After an extensive experimental study, ARE exhibited high predictive performance and outperformed state-of-the-art ensembles on data streams for real and synthetic datasets while requiring a low processing time and memory consumption.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524000576",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Concept drift",
      "Data mining",
      "Data stream",
      "Data stream mining",
      "Ensemble learning",
      "Feature selection",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Random subspace method",
      "Subspace topology",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Paim",
        "given_name": "Aldo M."
      },
      {
        "surname": "Enembreck",
        "given_name": "Fabrício"
      }
    ]
  },
  {
    "title": "From heavy rain removal to detail restoration: A faster and better network",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110205",
    "abstract": "The profound accumulation of precipitation during intense rainfall events can markedly degrade the quality of images, leading to the erosion of textural details. Despite the improvements observed in existing learning-based methods specialized for heavy rain removal, it is discerned that a significant proportion of these methods tend to overlook the precise reconstruction of the intricate details. In this work, we introduce a simple dual-stage progressive enhancement network, denoted as DPENet, aiming to achieve effective deraining while preserving the structural accuracy of rain-free images. This approach comprises two key modules, a rain streaks removal network (R 2 Net) focusing on accurate rain removal, and a details reconstruction network (DRNet) designed to recover the textural details of rain-free images. Firstly, we introduce a dilated dense residual block (DDRB) within R 2 Net, enabling the aggregation of high-level and low-level features. Secondly, an enhanced residual pixel-wise attention block (ERPAB) is integrated into DRNet to facilitate the incorporation of contextual information. To further enhance the fidelity of our approach, we employ a comprehensive loss function that accentuates both the marginal and regional accuracy of rain-free images. Extensive experiments conducted on publicly available benchmarks demonstrates the noteworthy efficiency and effectiveness of our proposed DPENet. The source code and pre-trained models are currently available at https://github.com/chdwyb/DPENet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323009020",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Environmental science"
    ],
    "authors": [
      {
        "surname": "Wen",
        "given_name": "Yuanbo"
      },
      {
        "surname": "Gao",
        "given_name": "Tao"
      },
      {
        "surname": "Zhang",
        "given_name": "Jing"
      },
      {
        "surname": "Zhang",
        "given_name": "Kaihao"
      },
      {
        "surname": "Chen",
        "given_name": "Ting"
      }
    ]
  },
  {
    "title": "Improved channel attention methods via hierarchical pooling and reducing information loss",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110148",
    "abstract": "Channel attention has been demonstrated to improve performance of convolutional neural networks. Most existing channel attention methods lower channel dimension for reducing computational complexity. However, the dimension reduction causes information loss, thus resulting in performance loss. To alleviate the paradox of complexity and performance trade-off, we propose two novel channel attention methods named Grouping-Shuffle-Aggregation Channel Attention (GSACA) method and Mixed Encoding Channel Attention (MECA) method, respectively. Our GSACA method partitions channel variables into several groups and performs the independent matrix multiplication without the dimension reduction to each group. Our GSACA method enables interaction between all groups using a ”channel shuffle” operator. After these, our GSACA method performs the independent matrix multiplication each group again and aggregates all channel correlations. Our MECA method encodes channel information through dual path architectures to benefit from both path topology where one uses the multilayer perception with dimension reduction to encode channel information and the other uses channel information encoding method without dimension reduction. Furthermore, a novel pooling operator named hierarchical pooling is presented and applied to our GSACA and MECA methods. The experimental results showed that our GSACA method almost consistently outperformed most existing channel attention methods and that our MECA method consistently outperformed the existing channel attention methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323008452",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Channel (broadcasting)",
      "Chemistry",
      "Computer network",
      "Computer science",
      "Dimension (graph theory)",
      "Dimensionality reduction",
      "ENCODE",
      "Encoding (memory)",
      "Gene",
      "Geometry",
      "Mathematics",
      "Pooling",
      "Pure mathematics",
      "Reduction (mathematics)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Zhu",
        "given_name": "Meng"
      },
      {
        "surname": "Min",
        "given_name": "Weidong"
      },
      {
        "surname": "Han",
        "given_name": "Junwei"
      },
      {
        "surname": "Han",
        "given_name": "Qing"
      },
      {
        "surname": "Cui",
        "given_name": "Shimiao"
      }
    ]
  },
  {
    "title": "Attention based multi-task interpretable graph convolutional network for Alzheimer’s disease analysis",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.02.016",
    "abstract": "Alzheimer’s Disease impairs the memory and cognitive function of patients, and early intervention can effectively mitigate its deterioration. Most existing methods for Alzheimer’s analysis rely solely on medical images, ignoring the impact of some clinical indicators associated with the disease. Furthermore, these methods have thus far failed to identify the specific brain regions affected by the disease. To solve these limitations, we propose an attention-based multi-task Graph Convolutional Network (GNN) for Alzheimer’s disease analysis. Specifically, we first segment brain regions based on tissue types and randomly assign a learnable weight for each region. Then, we introduce multi-task attention units to jointly capture the shared feature information between brain regions and across different tasks, achieving cross-interactions between medical images and clinical indicators. Finally, we design task-specific layers for each task, allowing the model to predict Alzheimer’s Disease status and clinical scores. Experimental results on four Alzheimer’s Disease datasets show that our approach not only outperforms the state-of-the-art in terms of accuracy, but also explicitly identifies brain regions associated with the disease as well as provides reliable clinical scores.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524000497",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Economics",
      "Graph",
      "Management",
      "Natural language processing",
      "Network analysis",
      "Pattern recognition (psychology)",
      "Physics",
      "Power graph analysis",
      "Quantum mechanics",
      "Task (project management)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Jiang",
        "given_name": "Shunqin"
      },
      {
        "surname": "Feng",
        "given_name": "Qiyuan"
      },
      {
        "surname": "Li",
        "given_name": "Hengxin"
      },
      {
        "surname": "Deng",
        "given_name": "Zhenyun"
      },
      {
        "surname": "Jiang",
        "given_name": "Qinghong"
      }
    ]
  },
  {
    "title": "PDTE: Pyramidal deep Taylor expansion for optical flow estimation",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.03.009",
    "abstract": "Optical flow estimation is an important hot research in computer vision. Although existing methods had got a considerable progress in improving their performance, they still have drawbacks, such as heavily computational burden, inaccurate pixel-level offset estimation, and poor interpretability. To address these issues, this letter proposes a pyramidal deep Taylor expansion (PDTE) framework, including: First, we seriously interpreted the relationship between optical flow computation and Taylor expansion. Then, the proposed PDTE is constructed by employing global motion aggregation (GMA) to calculate each derivative part which contributes to the final estimated optical flow. Quantitative and qualitative results on the Sintel and KITTI datasets validate that the proposed PDTE scheme is effective and outperforms the state-of-the-art optical flow estimation methods. The results of extensive experiments in the ablation study demonstrate that PDTE performs well on shape preservation and the accuracy improvement of optical flow estimation, even pixel-level offset calculation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524000771",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Economics",
      "Estimation",
      "Flow (mathematics)",
      "Geology",
      "Geometry",
      "Image (mathematics)",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Optical flow",
      "Taylor series"
    ],
    "authors": [
      {
        "surname": "Zhu",
        "given_name": "Zifan"
      },
      {
        "surname": "An",
        "given_name": "Qing"
      },
      {
        "surname": "Huang",
        "given_name": "Chen"
      },
      {
        "surname": "Huang",
        "given_name": "Zhenghua"
      },
      {
        "surname": "Huang",
        "given_name": "Likun"
      },
      {
        "surname": "Fang",
        "given_name": "Hao"
      }
    ]
  },
  {
    "title": "SSIR: Spatial shuffle multi-head self-attention for Single Image Super-Resolution",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110195",
    "abstract": "Benefiting from the development of deep convolutional neural networks, CNN-based single-image super-resolution methods have achieved remarkable reconstruction results. However, the limited perceptual field of the convolutional kernel and the use of static weights in the inference process limit the performance of CNN-based methods. Recently, a few vision transformer-based image super-resolution methods have achieved excellent performance compared to CNN-based methods. These methods contain many parameters and require vast amounts of GPU memory for training. In this paper, we propose a spatial shuffle multi-head self-attention for single-image super-resolution that can significantly model long-range pixel dependencies without additional computational consumption. A local perception module is also proposed to combine convolutional neural networks’ local connectivity and translational invariance. Reconstruction results on five popular benchmarks show that the proposed method outperforms existing methods in both reconstruction accuracy and visual performance. The proposed method matches the performance of transformed-based methods but requires an inferior number of transformer blocks, which reduces the number of parameters by 40%, GPU memory by 30%, and inference time by 30% compared to transformer-based methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323008920",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Image (mathematics)",
      "Image resolution",
      "Inference",
      "Kernel (algebra)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Pixel",
      "Quantum mechanics",
      "Superresolution",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Liangliang"
      },
      {
        "surname": "Gao",
        "given_name": "Junyu"
      },
      {
        "surname": "Deng",
        "given_name": "Donghu"
      },
      {
        "surname": "Li",
        "given_name": "Xuelong"
      }
    ]
  },
  {
    "title": "GhostFormer: Efficiently amalgamated CNN-transformer architecture for object detection",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110172",
    "abstract": "The lightweight network model has gradually evolved into an important research direction in object detection. Network lightweight design has a variety of research methods, such as quantization, knowledge distillation, and neural architecture search. However, these methods either fail to break through the performance bottleneck of the model itself or require massive training costs. In order to solve these problems, a new object detection model based on CNN-Transformer hybrid feature extraction network called GhostFormer is proposed from the perspective of lightweight network structure design. GhostFormer makes full use of the advantages of local modeling of CNN and global modeling of Transformer, not only effectively reducing the complexity of the convolution model but also breaking through the limitation of Transformer’s lack of inductive bias. Finally, better transfer results are obtained in downstream tasks. Experiments show that the model is less than half as computationally expensive as YOLOv7 on the Pascal VOC dataset, with only about 3 % mAP@0.5 loss, and 9.7% mAP@0.5:0.95 improvement on the MS COCO dataset compared with GhostNet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323008695",
    "keywords": [
      "Architecture",
      "Art",
      "Artificial intelligence",
      "Bottleneck",
      "Classifier (UML)",
      "Computer engineering",
      "Computer science",
      "Convolutional neural network",
      "Electrical engineering",
      "Embedded system",
      "Engineering",
      "Feature extraction",
      "Machine learning",
      "Object detection",
      "Pascal (unit)",
      "Pattern recognition (psychology)",
      "Programming language",
      "Transformer",
      "Visual arts",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Xie",
        "given_name": "Xin"
      },
      {
        "surname": "Wu",
        "given_name": "Dengquan"
      },
      {
        "surname": "Xie",
        "given_name": "Mingye"
      },
      {
        "surname": "Li",
        "given_name": "Zixi"
      }
    ]
  },
  {
    "title": "Improvised contrastive loss for improved face recognition in open-set nature",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.03.004",
    "abstract": "Face recognition models often encounter various unseen domains and environments in real-world applications, leading to unsatisfactory performance due to the open-set nature of face recognition. Models trained on central datasets may exhibit poor generalization when faced with different candidates under varying illumination and blur conditions. In this paper, our goal is to enhance the generalization of face recognition models for diverse target conditions without relying on active or incremental learning. We propose an approach for face recognition that utilizes contrastive learning to synthesize positive and multiple negative samples. To address the combinatorial challenges posed by positive and negative samples, our framework incorporates a combination of contrastive regularizer loss and Arcface loss, along with an effective sampling strategy for batch model learning. We update the model weights by jointly back-propagating contrastive and ArcFace gradients. We validate our method on both generalized and standard face recognition benchmarks dataset namely IJB-B and IJB-C. Series of experimentation revealed the out-performance of proposed framework against other state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524000722",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Discrete mathematics",
      "Face (sociological concept)",
      "Facial recognition system",
      "Generalization",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Open set",
      "Pattern recognition (psychology)",
      "Programming language",
      "Set (abstract data type)",
      "Social science",
      "Sociology",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Khan",
        "given_name": "Zafran"
      },
      {
        "surname": "Boragule",
        "given_name": "Abhijeet"
      },
      {
        "surname": "d’Auriol",
        "given_name": "Brian J."
      },
      {
        "surname": "Jeon",
        "given_name": "Moongu"
      }
    ]
  },
  {
    "title": "Investigating the effectiveness of data augmentation from similarity and diversity: An empirical study",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110204",
    "abstract": "Data augmentation has emerged as a widely adopted technique for improving the generalization capabilities of deep neural networks. However, evaluating the effectiveness of data augmentation methods solely based on model training is computationally demanding and lacks interpretability. Moreover, the absence of quantitative standards hinders our understanding of the underlying mechanisms of data augmentation approaches and the development of novel techniques. To this end, we propose interpretable quantitative measures that decompose the effectiveness of data augmentation methods into two key dimensions: similarity and diversity. The proposed similarity measure describes the overall similarity between the original and augmented datasets, while the diversity measure quantifies the divergence in inherent complexity between the original and augmented datasets in terms of categories. Importantly, our proposed measures are model training-agnostic, ensuring efficiency in their calculation. Through experiments on several benchmark datasets, including MNIST, CIFAR-10, CIFAR-100, and ImageNet, we demonstrate the efficacy of our measures in evaluating the effectiveness of various data augmentation methods. Furthermore, although the proposed measures are straightforward, they have the potential to guide the design and parameter tuning of data augmentation techniques and enable the validation of data augmentation methods’ efficacy before embarking on large-scale model training.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323009019",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Computer science",
      "Computer security",
      "Data mining",
      "Divergence (linguistics)",
      "Generalization",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Interpretability",
      "Key (lock)",
      "Linguistics",
      "MNIST database",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Measure (data warehouse)",
      "Philosophy",
      "Similarity (geometry)",
      "Similarity measure"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Suorong"
      },
      {
        "surname": "Guo",
        "given_name": "Suhan"
      },
      {
        "surname": "Zhao",
        "given_name": "Jian"
      },
      {
        "surname": "Shen",
        "given_name": "Furao"
      }
    ]
  },
  {
    "title": "Deep federated learning hybrid optimization model based on encrypted aligned data",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110193",
    "abstract": "Federated learning can achieve multi-party data-collaborative applications while safeguarding personal privacy. However, the process often leads to a decline in the quality of sample data due to a substantial amount of missing encrypted aligned data, and there is a lack of research on how to improve the model learning effect by increasing the number of samples of encrypted aligned data in federated learning. Therefore, this paper integrates the functional characteristics of deep learning models and proposes a Variational AutoEncoder Gaussian Mixture Model Clustering Vertical Federated Learning Model (VAEGMMC-VFL), which leverages the feature extraction capability of the autoencoder and the clustering and pattern discovery capabilities of Gaussian mixture clustering on diverse datasets to further explore a large number of potentially usable samples. Firstly, the Variational AutoEncoder is used to achieve dimensionality reduction and sample feature reconstruction of high-dimensional data samples. Subsequently, Gaussian mixture clustering is further employed to partition the dataset into multiple potential Gaussian-distributed clusters and filter the sample data using thresholding. Additionally, the paper introduces a labeled sample attribute value finding algorithm to fill in attribute values for encrypted unaligned samples that meet the requirements, allowing for the full recovery of encrypted unaligned data. In the experimental section, the paper selects four sets of datasets from different industries and compares the proposed method with three federated learning clustering methods in terms of clustering loss, reconstruction loss, and other metrics. Tests on precision, accuracy, recall, ROC curve, and F1-score indicate that the proposed method outperforms similar approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323008907",
    "keywords": [
      "Artificial intelligence",
      "Autoencoder",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Deep learning",
      "Dimensionality reduction",
      "Encryption",
      "Image (mathematics)",
      "Machine learning",
      "Operating system",
      "Pattern recognition (psychology)",
      "Thresholding"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Zhongnan"
      },
      {
        "surname": "Liang",
        "given_name": "Xiaoliang"
      },
      {
        "surname": "Huang",
        "given_name": "Hai"
      },
      {
        "surname": "Wang",
        "given_name": "Kun"
      }
    ]
  },
  {
    "title": "Reducing communication in federated learning via efficient client sampling",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110122",
    "abstract": "Federated learning (FL) ameliorates privacy concerns in settings where a central server coordinates learning from data distributed across many clients; rather than sharing the data, the clients train locally and report the models they learn to the server. Aggregation of local models requires communicating massive amounts of information between the clients and the server, consuming network bandwidth. We propose a novel framework for updating the global model in communication-constrained FL systems by requesting input only from the clients with informative updates, and estimating the local updates that are not communicated. Specifically, describing the progression of the model’s weights by an Ornstein–Uhlenbeck process allows us to develop sampling strategy for selecting a subset of clients with significant weight updates; model updates of the clients not selected for communication are replaced by their estimates. We test this policy on realistic federated benchmark datasets and show that the proposed framework provides up to 50% reduction in communication while maintaining competitive or achieving superior performance compared to baselines. The proposed method represents a new line of strategies for communication-efficient FL that is orthogonal to the existing user-driven techniques, such as compression, thus complementing rather than aiming to replace those existing methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323008191",
    "keywords": [
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Benchmark (surveying)",
      "Computer network",
      "Computer science",
      "Data mining",
      "Distributed computing",
      "Federated learning",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Operating system",
      "Process (computing)"
    ],
    "authors": [
      {
        "surname": "Ribero",
        "given_name": "Mónica"
      },
      {
        "surname": "Vikalo",
        "given_name": "Haris"
      }
    ]
  },
  {
    "title": "Structure correspondence searching of CAD model using local feature-based description and indexing",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110126",
    "abstract": "The CAD model retrieval has played a significant role in various applications, including product development and knowledge mining. However, most existing retrieval methods compare 3D shape similarity from a global perspective, while detecting similar structures automatically for CAD models remains a challenging problem. Consequently, this study proposes a structure correspondence searching framework for CAD models to address the issues. According to the boundary representation (B-rep) information, the proposed method first segments a CAD model into a set of local features denoted as structural cells. Then, the descriptor of each structural cell is extracted using a weighted shape distribution vector and neighbor set. In order to speed up the matching of structural cells, an indexing and filtering mechanism is constructed based on the shape clustering and topological analysis. The matched structural cells determine the boundary of similar structures. Finally, similarity measurement is conducted to generate a ranking list by analyzing the quality of the matched structural cells. The rationality and efficiency of the proposed approach are demonstrated via an analysis of experimental results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323008233",
    "keywords": [
      "Artificial intelligence",
      "Boundary (topology)",
      "CAD",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Engineering",
      "Engineering drawing",
      "Feature (linguistics)",
      "Image (mathematics)",
      "Law",
      "Linguistics",
      "Matching (statistics)",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Programming language",
      "Ranking (information retrieval)",
      "Representation (politics)",
      "Search engine indexing",
      "Set (abstract data type)",
      "Similarity (geometry)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Ji",
        "given_name": "Baoning"
      },
      {
        "surname": "Zhang",
        "given_name": "Jie"
      },
      {
        "surname": "Li",
        "given_name": "Yuan"
      },
      {
        "surname": "Tang",
        "given_name": "Wenbin"
      }
    ]
  },
  {
    "title": "Kinship similarity for open sets",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110123",
    "abstract": "The aim of image-based kinship recognition methods is to determine the genetic relationship between people from their face images. Kinship recognition and related methods have many applications in computer vision. Most of the methods are very meaningful but are focused on closed sets. However, for real-life scenarios, kinship recognition is an open set problem. In contrast to previous methods, this paper considers kinship recognition for open sets. Further, the aim is to determine family relationships and their corresponding degrees of kinship. Our method is pairwise-based and is able to exploit mutual information from positive pairs. Large scale experiments and ablation studies show that our method (1) reaches SOTA performance on the FIW dataset, (2) is able to properly separate kinship categories using pairwise similarity and (3) generates uniform similarity distributions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323008208",
    "keywords": [
      "Anthropology",
      "Artificial intelligence",
      "Cartography",
      "Computer science",
      "Face (sociological concept)",
      "Facial recognition system",
      "Geography",
      "Image (mathematics)",
      "Kinship",
      "Mathematics",
      "Pairwise comparison",
      "Pattern recognition (psychology)",
      "Programming language",
      "Scale (ratio)",
      "Set (abstract data type)",
      "Similarity (geometry)",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Wei"
      },
      {
        "surname": "You",
        "given_name": "Shaodi"
      },
      {
        "surname": "Karaoglu",
        "given_name": "Sezer"
      },
      {
        "surname": "Gevers",
        "given_name": "Theo"
      }
    ]
  },
  {
    "title": "LC-MSM: Language-Conditioned Masked Segmentation Model for unsupervised domain adaptation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110201",
    "abstract": "Unsupervised domain adaptation (UDA) is an important research topic in semantic segmentation tasks, wherein pixel-wise annotations are often difficult to collect in a test environment due to their high labeling costs. Previous UDA-based studies trained their segmentation networks using labeled synthetic data and unlabeled realistic data as source and target domains, respectively. However, they often fail to distinguish semantically similar classes, such as person vs. rider and road vs. sidewalk, because these classes are prone to confusion in domain-shifted environments. In this paper, we introduce a Language-Conditioned Masked Segmentation Model (LC-MSM), which is a new framework for the joint learning of context relations and domain-agnostic information for domain-adaptive semantic segmentation. Specifically, we reconstruct semantic labels with masked image conditions on the generalized text embeddings of the corresponding semantic class from OpenCLIP, which contains domain-invariant knowledge from large-scale data. To this end, we correlate the generalized text embeddings onto the per-pixel image feature of a masked image that learned the spatial context to further append domain-agnostic language information to the semantic decoder. This facilitates the generalization of our model to the target domain via the learning of context information within individual training instances, while considering cross-domain representations spanning the entire dataset. LC-MSM achieves an unprecedented UDA performance of 71.8 and 62.8 mIoU on GTA → Cityscapes and SYNTHIA → Cityscapes, respectively, which corresponds to an improvement of +3.5 and +1.9 percent points over the baseline method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323008981",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Domain (mathematical analysis)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Natural language processing",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Kim",
        "given_name": "Young-Eun"
      },
      {
        "surname": "Lee",
        "given_name": "Yu-Won"
      },
      {
        "surname": "Lee",
        "given_name": "Seong-Whan"
      }
    ]
  },
  {
    "title": "TSVT: Token Sparsification Vision Transformer for robust RGB-D salient object detection",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110190",
    "abstract": "Visual transformer-based salient object detection (SOD) models have attracted increasing research attention. However, the existing transformer-based RGB-D SOD models usually operate on the full token sequences of RGB-D images and use an equal tokenization process to treat appearance and depth modalities, which leads to limited feature richness and inefficiency. To address these limitations, we present a novel token sparsification vision transformer architecture for RGB-D SOD, named TSVT, that explicitly extracts global-local multi-modality features with sparse tokens. The TSVT is an asymmetric encoder–decoder architecture with a dynamic sparse token encoder that adaptively selects and operates on sparse tokens, along with an multiple cascade aggregation decoder (MCAD) that predicts saliency results. Furthermore, we deeply investigate the differences and similarities between the appearance and depth modalities and develop an interactive diversity fusion module (IDFM) to integrate each pair of multi-modality tokens in different stages. Finally, to comprehensively evaluate the performance of the proposed model, we conduct extensive experiments on seven standard RGB-D SOD benchmarks in terms of five evaluation metrics. The experimental results reveal that the proposed model is more robust and effective than fifteen existing RGB-D SOD models. Moreover, the complexity of our model with the sparsification module is more than two times lower than that of the variant model without the dynamic sparse token module (DSTM).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323008877",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Electrical engineering",
      "Encoder",
      "Engineering",
      "Operating system",
      "Pattern recognition (psychology)",
      "RGB color model",
      "Security token",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Gao",
        "given_name": "Lina"
      },
      {
        "surname": "Liu",
        "given_name": "Bing"
      },
      {
        "surname": "Fu",
        "given_name": "Ping"
      },
      {
        "surname": "Xu",
        "given_name": "Mingzhu"
      }
    ]
  },
  {
    "title": "NODE-ImgNet: A PDE-informed effective and robust model for image denoising",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110176",
    "abstract": "Inspired by the traditional partial differential equation (PDE) approach for image denoising, we propose a novel neural network architecture, referred as NODE-ImgNet, that combines neural ordinary differential equations (NODEs) with convolutional neural network (CNN) blocks. NODE-ImgNet is intrinsically a PDE model, where the dynamic system is learned implicitly without the explicit specification of the PDE. This naturally circumvents the typical issues associated with introducing artifacts during the learning process. By invoking such a NODE structure, which can also be viewed as a continuous variant of a residual network (ResNet) and inherits its advantage in image denoising, our model achieves enhanced accuracy and parameter efficiency. In particular, our model exhibits consistent effectiveness in different scenarios, including denoising gray and color images perturbed by Gaussian noise, as well as real-noisy images, and demonstrates superiority in learning from small image datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323008737",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Engineering",
      "Gaussian noise",
      "Image (mathematics)",
      "Image denoising",
      "Mathematical analysis",
      "Mathematics",
      "Node (physics)",
      "Noise (video)",
      "Noise reduction",
      "Partial differential equation",
      "Pattern recognition (psychology)",
      "Residual",
      "Structural engineering"
    ],
    "authors": [
      {
        "surname": "Xie",
        "given_name": "Xinheng"
      },
      {
        "surname": "Wu",
        "given_name": "Yue"
      },
      {
        "surname": "Ni",
        "given_name": "Hao"
      },
      {
        "surname": "He",
        "given_name": "Cuiyu"
      }
    ]
  },
  {
    "title": "ICLR: Instance Credibility-Based Label Refinement for label noisy person re-identification",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110168",
    "abstract": "Person re-identification (Re-ID) has demonstrated remarkable performance when trained on accurately annotated data. However, in practical applications, the presence of annotation errors is unavoidable, which can undermine the accuracy and robustness of the Re-ID model training. To address the adverse impacts of label noise, especially in scenarios with limited training samples for each identity (ID), a common approach is to utilize all the available sample labels. Unfortunately, these labels contain incorrect labels, leading to the model being influenced by noise and compromising its performance. In this paper, we propose an Instance Credibility-based Label Refinement and Re-weighting (ICLR) framework to exploit partially credible labels to refine and re-weight incredible labels effectively. Specifically, the Label-Incredibility Optimization (LIO) module is proposed to optimize incredible labels before model training, which partitions the samples into credible and incredible samples and propagates credible labels to others. Furthermore, we design an Incredible Instance Re-weight (I 2 R) strategy, aiming to emphasize instances that contribute more significantly and dynamically adjust the weight of each instance. The proposed method seamlessly reinforces accuracy without requiring additional information or discarding any samples. Extensive experimental results conducted on Market-1501 and Duke-MTMC datasets demonstrate the effectiveness of our proposed method, leading to a substantial improvement in performance under both random noise and pattern noise settings. Code will be available at https://github.com/whut16/ReID-Label-Noise.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323008658",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Biology",
      "Botany",
      "Chemistry",
      "Computer science",
      "Computer security",
      "Credibility",
      "Data mining",
      "Exploit",
      "Gene",
      "Identification (biology)",
      "Image (mathematics)",
      "Law",
      "Machine learning",
      "Medicine",
      "Noise (video)",
      "Political science",
      "Radiology",
      "Robustness (evolution)",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Zhong",
        "given_name": "Xian"
      },
      {
        "surname": "Han",
        "given_name": "Xiyu"
      },
      {
        "surname": "Jia",
        "given_name": "Xuemei"
      },
      {
        "surname": "Huang",
        "given_name": "Wenxin"
      },
      {
        "surname": "Liu",
        "given_name": "Wenxuan"
      },
      {
        "surname": "Su",
        "given_name": "Shuaipeng"
      },
      {
        "surname": "Yu",
        "given_name": "Xiaohan"
      },
      {
        "surname": "Ye",
        "given_name": "Mang"
      }
    ]
  },
  {
    "title": "Devil in the details: Delving into accurate quality scoring for DensePose",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110197",
    "abstract": "How to score the quality of the network output is an essential but long-neglected problem in DensePose, which dramatically limits the potential of the existing methods. To fill the blank in the quality estimation of DensePose, we conduct rigorous experiments to clarify the key factors that accurately reflect the quality of DensePose results. We find that the accurate results already exist in the candidate pool but are mistakenly removed due to the inappropriate quality scores. To solve this problem, we proposed DensePose Scoring RCNN (DS RCNN), a simple and comprehensive quality estimation framework to learn the calibrated quality score and select high-quality results from the pool. DS RCNN introduces a quality scoring module (QSM) and a quality perception module (QPM) into the existing high-performance pipeline. The QSM scores the quality of DensePose results by fusing diverse quality information, and the QPM enhances the ability of quality perception by extracting instance-aware quality features guided by the predicted IUV maps. Benefiting from the superiority of QSM and QPM, DS RCNN outperforms baselines by up to 4.8 AP on the DensePose-COCO dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323008944",
    "keywords": [
      "Artificial intelligence",
      "Blank",
      "Computer science",
      "Computer security",
      "Data mining",
      "Engineering",
      "Epistemology",
      "Key (lock)",
      "Machine learning",
      "Mechanical engineering",
      "Metric (unit)",
      "Operations management",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pipeline (software)",
      "Programming language",
      "Quality (philosophy)",
      "Quality Score"
    ],
    "authors": [
      {
        "surname": "Sun",
        "given_name": "Junyao"
      },
      {
        "surname": "Liu",
        "given_name": "Qiong"
      }
    ]
  },
  {
    "title": "VPCFormer: A transformer-based multi-view finger vein recognition model and a new benchmark",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110170",
    "abstract": "In the past decade, finger vein authentication garners significant interest. However, most existing databases and algorithms predominantly focused on single-view finger vein recognition. The current projection of vein patterns actually maps a 3D network topology into a 2D plane, which inevitably leads to 3D feature loss and topological ambiguity in 2D images. Additionally, single-view based methods are sensitive to finger rotation and translation in practical applications. So far, there are currently few dedicated studies and public databases on multi-view finger vein recognition. To address these issues, we first establish a benchmark for future research by constructing the multi-view finger vein database, named Tsinghua Multi-View Finger Vein-3 Views (THUMVFV-3V) Database , which is collected over two sessions. THUMVFV-3V provides three types of Regions of Interest (ROIs) and includes unified preprocessing operations, catering to the majority of existing methods. Furthermore, we propose a novel Transformer-based model named Vein Pattern Constrained Transformer (VPCFormer) for multi-view finger vein recognition, primarily composed of multiple Vein Pattern Constrained Encoders (VPC-Encoders) and Neighborhood-Perspective Modules (NPMs). Specifically, the VPC-Encoder incorporates a novel Vein Pattern Attention Module (VPAM) and an Integrative Feed-Forward Network (IFFN). Motivated by the fact that the strong correlations veins exhibit across different views, we devise the VPAM. Assisted by a vein mask, VPAM is meticulously designed to exclusively extract intra- and inter-view dependencies between vein patterns. Further, we propose IFFN to efficiently aggregate the preceding attention and contextual information of VPAM. In addition, the NPM is utilized to capture the correlations within a single view, enhancing the final multi-view finger vein representation. Extensive experiments demonstrate the superiority of our VPCFormer. The THUMVFV-3V database is available at https://github.com/Pengyang233/THUMVFV-3V-Database.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323008671",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Electrical engineering",
      "Encoder",
      "Engineering",
      "Geodesy",
      "Geography",
      "Operating system",
      "Pattern recognition (psychology)",
      "Preprocessor",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Pengyang"
      },
      {
        "surname": "Song",
        "given_name": "Yizhuo"
      },
      {
        "surname": "Wang",
        "given_name": "Siqi"
      },
      {
        "surname": "Xue",
        "given_name": "Jing-Hao"
      },
      {
        "surname": "Zhao",
        "given_name": "Shuping"
      },
      {
        "surname": "Liao",
        "given_name": "Qingmin"
      },
      {
        "surname": "Yang",
        "given_name": "Wenming"
      }
    ]
  },
  {
    "title": "Random epipolar constraint loss functions for supervised optical flow estimation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110141",
    "abstract": "The majority of supervised models estimate optical flow through minimizing the numerical difference between the predicted flow and the ground truth, resulting in the loss of positional and geometric characteristics of the calculated flow fields. In addition, these models require a large number of parameters and high computational cost when computing optical flow. To address these issues, this paper presents a novel loss function and a lightweight framework for optical flow estimation. The proposed loss function, called the random epipolar constraint loss function (RECLoss), incorporates epipolar geometry into supervised optimization to transform the numerical difference into geometry constraint. The RECLoss can make the optical flow estimation models more effective and enhance their generalization abilities. Moreover, the design of RECLoss is more interpretable and the estimated optical flow fields from RECLoss have clearly defined mathematical meanings. A lightweight recurrent neural network for optical flow estimation (LRFlow) that balances computational cost and estimation accuracy is also proposed. The LRFlow, containing only 3.0M parameters, consists of a feature extractor, a correlation matching module, and an iterative update unit. The proposed lightweight network achieves state-of-the-art results compared to all other lightweight networks on the challenging MPI-Sintel and KITTI2015 datasets. The effectiveness of RECLoss in improving the accuracy of LRFlow and other state-of-the-art methods such as RAFT and GMA has been validated through extensive experiments. The source code of the project are available at https://github.com/Eryo-iPython/RECLoss.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323008385",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Constraint (computer-aided design)",
      "Epipolar geometry",
      "Evolutionary biology",
      "Feature (linguistics)",
      "Flow (mathematics)",
      "Function (biology)",
      "Geometry",
      "Ground truth",
      "Image (mathematics)",
      "Linguistics",
      "Mathematical optimization",
      "Mathematics",
      "Optical flow",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Fan",
        "given_name": "Zhengyuan"
      },
      {
        "surname": "Cai",
        "given_name": "Zemin"
      }
    ]
  },
  {
    "title": "Learning on sample-efficient and label-efficient multi-view cardiac data with graph transformer",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.03.001",
    "abstract": "Predicting cardiovascular disease has been a challenging task, as assessing samples based on a single view of information may be insufficient. Therefore, in this paper, we focus on the challenge of predicting cardiovascular disease using multi-view cardiac data. However, multi-view cardiac data is usually difficult to collect and label. Based on this motivation, learning an effective predictive model on sample-efficient and label-efficient multi-view cardiac data is urgently needed. To address the aforementioned issues, we propose a multi-view learning method: (i) our method utilizes graph learning to establish and extract relationships between data, enabling learning from a small number of labeled data and a small number of samples; (ii) our method integrates features from multiple views to utilize complementary information in the data; (iii) for data without a provided graph of relationships between samples, we utilize the mechanism of transformers to learn the relationships between samples in a data-driven manner. We validate the effectiveness of our method on real heart disease datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524000692",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Electrical engineering",
      "Engineering",
      "Graph",
      "Theoretical computer science",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Lujing"
      },
      {
        "surname": "Ma",
        "given_name": "Yunting"
      },
      {
        "surname": "Zhang",
        "given_name": "Wanqiu"
      },
      {
        "surname": "Zhao",
        "given_name": "Xiaoying"
      },
      {
        "surname": "Zhao",
        "given_name": "Xinxiang"
      }
    ]
  },
  {
    "title": "Hierarchical matrix factorization for interpretable collaborative filtering",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.03.003",
    "abstract": "Matrix factorization (MF) is a simple collaborative filtering technique that achieves superior recommendation accuracy by decomposing the user–item interaction matrix into user and item latent matrices. Because the model typically learns each interaction independently, it may overlook the underlying shared dependencies between users and items, resulting in less stable and interpretable recommendations. Based on these insights, we propose “Hierarchical Matrix Factorization” (HMF), which incorporates clustering concepts to capture the hierarchy, where leaf nodes and other nodes correspond to users/items and clusters, respectively. Central to our approach, called hierarchical embeddings, is the additional decomposition of the latent matrices (embeddings) into probabilistic connection matrices, which link the hierarchy, and a root cluster latent matrix. The embeddings are differentiable, allowing simultaneous learning of interactions and clustering using a single gradient descent method. Furthermore, the obtained cluster-specific interactions naturally summarize user–item interactions and provide interpretability. Experimental results on ratings and ranking predictions show that HMF outperforms existing MF methods, in particular achieving a 1.37 point improvement in RMSE for sparse interactions. Additionally, it was confirmed that the clustering integration of HMF has the potential for faster learning convergence and mitigation of overfitting compared to MF, and also provides interpretability through a cluster-centered case study.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524000710",
    "keywords": [
      "Algebra over a field",
      "Algorithm",
      "Artificial intelligence",
      "Collaborative filtering",
      "Composite material",
      "Computer science",
      "Eigenvalues and eigenvectors",
      "Factorization",
      "Machine learning",
      "Materials science",
      "Mathematics",
      "Matrix (chemical analysis)",
      "Matrix decomposition",
      "Pattern recognition (psychology)",
      "Physics",
      "Pure mathematics",
      "Quantum mechanics",
      "Recommender system"
    ],
    "authors": [
      {
        "surname": "Sugahara",
        "given_name": "Kai"
      },
      {
        "surname": "Okamoto",
        "given_name": "Kazushi"
      }
    ]
  },
  {
    "title": "Global routing between capsules",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110142",
    "abstract": "Current convolutional neural networks (CNNs) lack viewpoint equivariance. Hence, they perform poorly when dealing with viewpoints unseen during training procedures. CNN achieves invariance via pooling operations in image classification tasks. However, the pooling operation does not necessarily improve viewpoint generalization, rather relying on more data to achieve viewpoint equivariance. Capsule network (CapsNet) is proposed to tackle this issue, but it is inefficient and inaccurate when applied to complex datasets. We propose a novel CapsNet architecture called Global Routing CapsNet (GR-CapsNet) to solve this problem. Specifically, colored background in the input image can generate invalid background voting capsules to reduce the performance of CapsNet. Therefore, we first construct a dynamic linear unit (DLU), which avoids the generation of invalid background voting capsules. Then we present two extra learnable units: frequency domain unit (FDU) and spatial unit (SPU). The former is used to capture finer features in the frequency domain and aims to improve classification performance on complex datasets. The latter is applied to construct the spatial relationship between the voting capsules and component capsules and aims to enhance robustness to affine transformation. Finally, we propose a global routing mechanism to simplify the routing process for CapsNet, which obtains more feature information to improve the performance of CapsNet. Extensive experiments on nine datasets show that our method obtains better robustness and generalization and achieves SOTA performance compared to other related methods. And it has fewer the number of parameters and GPU memory consumption than these related methods. The source code is available on https://github.com/cwpl/GR-CapsNet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323008397",
    "keywords": [
      "Affine transformation",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Convolutional neural network",
      "Gene",
      "Law",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Pooling",
      "Pure mathematics",
      "Robustness (evolution)",
      "Voting"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Ran"
      },
      {
        "surname": "Shen",
        "given_name": "Hao"
      },
      {
        "surname": "Zhao",
        "given_name": "Zhong-Qiu"
      },
      {
        "surname": "Yang",
        "given_name": "Yi"
      },
      {
        "surname": "Zhang",
        "given_name": "Zhao"
      }
    ]
  },
  {
    "title": "Joint recognition of basic and compound facial expressions by mining latent soft labels",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110173",
    "abstract": "Previous works on facial expression recognition focus on basic emotions, while ignoring more complex compound expressions. However, both basic and compound emotions appear in the real-world environment. In this work, we aim to jointly recognize basic and compound expressions. Aiming at the Basic-Compound Facial Expression Recognition (BC-FER) task, we illustrate that traditional hard label training is not ideal due to great label dependencies. Therefore, we propose an expression soft label mining (ESLM) method to improve the performance. On the one hand, an iterated soft label mining (ISLM) algorithm assisted by teacher–student network is proposed to make the network generate soft targets automatically for learning. On the other hand, to explicitly leverage prior knowledge of label correlations, we propose an expression correlation score learning (ECSL) loss to regularize the predicted distributions. Extensive experimental results on CFEE, RAF-DB, and EmotioNet show that our method achieves state-of-the-art performance on BC-FER task.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323008701",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Engineering",
      "Expression (computer science)",
      "Facial expression",
      "Facial expression recognition",
      "Facial recognition system",
      "Iterated function",
      "Leverage (statistics)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Programming language",
      "Systems engineering",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Jiang",
        "given_name": "Jing"
      },
      {
        "surname": "Wang",
        "given_name": "Mei"
      },
      {
        "surname": "Xiao",
        "given_name": "Bo"
      },
      {
        "surname": "Hu",
        "given_name": "Jiani"
      },
      {
        "surname": "Deng",
        "given_name": "Weihong"
      }
    ]
  },
  {
    "title": "Confusable facial expression recognition with geometry-aware conditional network",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110174",
    "abstract": "Many facial expression recognition (FER) methods have now achieved satisfactory results. However, some facial expressions have similar muscle deformation, making them easy to confuse. These confusable facial expressions are a key challenge to accurately recognizing facial expressions. In addition, most current FER methods rely on the convolution operation, but convolution is a building block that processes one local neighborhood at a time; thus, it fails to capture the geometric patterns that are important for facial muscle deformation. To address this issue, considering the problems of pose variations and insufficient training data, this paper proposes a geometry-aware conditional network (GACN) that captures long-range dependencies for simultaneous pose-invariant facial expression editing and geometry-aware FER. Specifically, the GACN can complete a pose-invariant image editing task with long-range dependency by introducing conditional self-attention operations to a generative adversarial network. Moreover, the GACN presents non-local operations as building blocks of the classifier to capture the texture and geometry patterns simultaneously. Finally, these two tasks can further boost each other’s performances through our GACN, and confusable facial expressions can be effectively distinguished. And we overcome the effect of pose variations while expanding and enriching the training set. Our proposed algorithm is evaluated on both the in-the-lab and in-the-wild datasets and outperforms the state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323008713",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Composite material",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Expression (computer science)",
      "Face detection",
      "Face hallucination",
      "Facial expression",
      "Facial recognition system",
      "Generative grammar",
      "Generative model",
      "Invariant (physics)",
      "Materials science",
      "Mathematical physics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Programming language",
      "Range (aeronautics)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Tong"
      },
      {
        "surname": "Li",
        "given_name": "Jing"
      },
      {
        "surname": "Wu",
        "given_name": "Jia"
      },
      {
        "surname": "Du",
        "given_name": "Bo"
      },
      {
        "surname": "Wan",
        "given_name": "Jun"
      },
      {
        "surname": "Chang",
        "given_name": "Jun"
      }
    ]
  },
  {
    "title": "Non-negative Tucker decomposition with graph regularization and smooth constraint for clustering",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110207",
    "abstract": "Non-negative Tucker decomposition (NTD) and its graph regularized extensions are the most popular techniques for representing high-dimensional non-negative data, which are typically found in a low-dimensional sub-manifold of ambient space, from a geometric perspective. Therefore, the performance of the graph-based NTD methods relies heavily on the low-dimensional representation of the original data. However, most existing approaches treat the last factor matrix in NTD as a low-dimensional representation of the original data. This treatment leads to the loss of the original data’s multi-linear structure in the low-dimensional subspace. To remedy this defect, we propose a novel graph regularized L p smooth NTD (GSNTD) method for high-dimensional data representation by incorporating graph regularization and an L p smoothing constraint into NTD. The new graph regularization term constructed by the product of the core tensor and the last factor matrix in NTD, and it is used to uncover hidden semantics while maintaining the intrinsic multi-linear geometric structure of the data. The addition of the L p smoothing constraint to NTD may produce a more accurate and smoother solution to the optimization problem. The update rules and the convergence of the GSNTD method are proposed. In addition, a randomized variant of the GSNTD algorithm based on fiber sampling is proposed. Finally, the experimental results on four standard image databases show that the proposed method and its randomized variant have better performance than some other state-of-the-art graph-based regularization methods for image clustering.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323009044",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Cluster analysis",
      "Combinatorics",
      "Computer science",
      "Decomposition",
      "Graph",
      "Karush–Kuhn–Tucker conditions",
      "Mathematical optimization",
      "Mathematics",
      "Organic chemistry",
      "Pure mathematics",
      "Regularization (linguistics)",
      "Tensor (intrinsic definition)",
      "Tensor decomposition",
      "Tucker decomposition"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Qilong"
      },
      {
        "surname": "Lu",
        "given_name": "Linzhang"
      },
      {
        "surname": "Chen",
        "given_name": "Zhen"
      }
    ]
  },
  {
    "title": "Dual subspace manifold learning based on GCN for intensity-invariant facial expression recognition",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110157",
    "abstract": "Facial expression recognition (FER) is one of the most important computer vision tasks for understanding human inner emotions. However, the poor generation ability of the FER model limits its applicability due to tremendous intraclass variation. Especially for expressions of varying intensities, the appearance differences among weak expressions are subtle, which makes FER tasks challenging. In response to these issues, this paper presents a dual subspace manifold learning method based on a graph convolutional network (GCN) for intensity-invariant FER tasks. Our method treats the target task as a node classification problem and learns the manifold representation using two subspace analysis methods: locality preserving projection (LPP) and peak-piloted locality preserving projection (PLPP). Inspired by the classic LPP, which maintains local similarity among data, this paper introduces a novel PLPP that maintains the locality between peak expressions and non-peak expressions to enhance the representation of weak expressions. This paper also reports two subspace fusion methods, one based on a weighted adjacency matrix and another on a self-attention mechanism, that combine the LPP and PLPP to further improve FER performance. The second method achieves a recognition accuracy of 93.83% on the CK+, 74.86% on the Oulu-CASIA and 75.37% on the MMI for weak expressions, outperforming state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323008543",
    "keywords": [
      "Adjacency matrix",
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Facial expression",
      "Graph",
      "Invariant (physics)",
      "Linguistics",
      "Locality",
      "Mathematical physics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Projection (relational algebra)",
      "Subspace topology",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Jingying"
      },
      {
        "surname": "Shi",
        "given_name": "Jinxin"
      },
      {
        "surname": "Xu",
        "given_name": "Ruyi"
      }
    ]
  },
  {
    "title": "Improving Augmentation Consistency for Graph Contrastive Learning",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110182",
    "abstract": "Graph contrastive learning (GCL) enhances unsupervised graph representation by generating different contrastive views, in which properties of augmented nodes are required to be aligned with their anchors. However, we find that in some existing GCL methods, it is hard to inherit semantic and structural properties of graphs from anchor views due to inconsistent augmentation schemes, which may hurt node consistency in augmented views. In this paper, we present ConGCL to improve node consistency and enhance node classification. Specifically, we first consider context entailment, which integrates the semantic and structural properties to better mine the underlying consistency relationships of nodes. Beneficial from this, we then design a novel consistency improvement loss to maintain augmentation consistency agreement among positive node pairs under stochastic augmentation schemes. To investigate the effectiveness of ConGCL on improving augmentation consistency and enhancing node classification, we conduct empirical study and extensive experiments on benchmark datasets. The code is available at: https://github.com/brysonwx/ConGCL.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323008798",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Computer science",
      "Consistency (knowledge bases)",
      "Consistency model",
      "Context (archaeology)",
      "Data consistency",
      "Distributed computing",
      "Engineering",
      "Geodesy",
      "Geography",
      "Graph",
      "Machine learning",
      "Natural language processing",
      "Node (physics)",
      "Paleontology",
      "Sequential consistency",
      "Structural engineering",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Bu",
        "given_name": "Weixin"
      },
      {
        "surname": "Cao",
        "given_name": "Xiaofeng"
      },
      {
        "surname": "Zheng",
        "given_name": "Yizhen"
      },
      {
        "surname": "Pan",
        "given_name": "Shirui"
      }
    ]
  },
  {
    "title": "SPACE: Senti-Prompt As Classifying Embedding for sentiment analysis",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.02.022",
    "abstract": "In natural language processing, the general approach to sentiment analysis involves a pre-training and fine-tuning paradigm using pre-trained language models combined with classifier models. Recently, numerous studies have applied prompts not only to downstream generation but also to classification tasks as well. However, to fully utilize the advantages of prompts and incorporate the context-dependent meaning of class representations into the prompts, it is necessary for the prompts to be learned similarly to the sentiment representations of each class. To achieve this, We introduce a novel method to learn soft prompts during fine-tuning. In this method, the class prompts are initialized with sentiment-related embeddings and are trained by a denoising task, which replaces them with masked tokens, just like the conventional masked language model (MLM) approach. Furthermore, a novel attention pattern is designed to tune attention between class prompts effectively. As a result, we demonstrate that our approach outperforms state-of-the-art models through experiments on four common datasets, achieving superior performance on sentiment analysis.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524000552",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Classifier (UML)",
      "Computer science",
      "Economics",
      "Embedding",
      "Language model",
      "Machine learning",
      "Management",
      "Natural language processing",
      "Sentiment analysis",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Kim",
        "given_name": "Jinyoung"
      },
      {
        "surname": "Ko",
        "given_name": "Youngjoong"
      }
    ]
  },
  {
    "title": "visClust: A visual clustering algorithm based on orthogonal projections",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110136",
    "abstract": "We present a novel clustering algorithm, visClust, that is based on lower dimensional data representations and visual interpretation. Thereto, we design a transformation that allows the data to be represented by a binary integer array enabling the use of image processing methods to select a partition. Qualitative and quantitative analyses measured in accuracy and an adjusted Rand-Index show that the algorithm performs well while requiring low runtime as well and RAM. We compare the results to 6 state-of-the-art algorithms with available code, confirming the quality of visClust by superior performance in most experiments. Moreover, the algorithm asks for just one obligatory input parameter while allowing optimization via optional parameters. The code is made available on GitHub and straightforward to use.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323008336",
    "keywords": [
      "Algorithm",
      "Arithmetic",
      "Artificial intelligence",
      "Binary number",
      "Biochemistry",
      "Chemistry",
      "Cluster analysis",
      "Code (set theory)",
      "Combinatorics",
      "Computer science",
      "Data mining",
      "Gene",
      "Interpretation (philosophy)",
      "Mathematics",
      "Operating system",
      "Partition (number theory)",
      "Programming language",
      "Rand index",
      "Set (abstract data type)",
      "Source code",
      "Transformation (genetics)"
    ],
    "authors": [
      {
        "surname": "Breger",
        "given_name": "Anna"
      },
      {
        "surname": "Karner",
        "given_name": "Clemens"
      },
      {
        "surname": "Ehler",
        "given_name": "Martin"
      }
    ]
  },
  {
    "title": "Re-abstraction and perturbing support pair network for few-shot fine-grained image classification",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110158",
    "abstract": "The goal of few-shot fine-grained image classification (FSFGIC) is to distinguish subordinate-level categories with subtle visual differences such as the species of bird and models of car with only a few samples. In this work, we argue that a designed network that has the ability to better distinguish feature descriptors of different categories will effectively improve the performance of FSFGIC. We propose a re-abstraction and perturbing support pair network (RaPSPNet) for FSFGIC. Specifically, we first design a feature re-abstraction embedding (FRaE) module which can not only effectively amplify the difference between the feature information from different categories but also better extract the feature information from images. Furthermore, a novel perturbing support pair (PSP) based similarity measure module is designed which evaluates the relationships of feature information among a query image and two different categories of support images (a support pair) at the same time for guiding the designed FRaE module to find salient feature information from the same category of query and support images and find distinguishable feature information from the different categories of query and support images. Extensive experiments on FSFGIC tasks demonstrate the superiority of the proposed methods over state-of-the-art benchmarks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323008555",
    "keywords": [
      "Abstraction",
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Embedding",
      "Epistemology",
      "Feature (linguistics)",
      "Feature extraction",
      "Image (mathematics)",
      "Information retrieval",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Salient",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Weichuan"
      },
      {
        "surname": "Zhao",
        "given_name": "Yali"
      },
      {
        "surname": "Gao",
        "given_name": "Yongsheng"
      },
      {
        "surname": "Sun",
        "given_name": "Changming"
      }
    ]
  },
  {
    "title": "AttenGait: Gait recognition with attention and rich modalities",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110171",
    "abstract": "Current gait recognition systems employ different types of manual attention mechanisms, like horizontal cropping of the input data to guide the training process and extract useful gait signatures for people identification. Typically, these techniques are applied using silhouettes as input, which limits the learning capabilities of the models. Thus, due to the limited information provided by silhouettes, state-of-the-art gait recognition approaches must use very simple and manually designed mechanisms, in contrast to approaches proposed for other topics such as action recognition. To tackle this problem, we propose AttenGait, a novel model for gait recognition equipped with trainable attention mechanisms that automatically discover interesting areas of the input data. AttenGait can be used with any kind of informative modalities, such as optical flow, obtaining state-of-the-art results thanks to the richer information contained in those modalities. We evaluate AttenGait on two public datasets for gait recognition: CASIA-B and GREW; improving the previous state-of-the-art results on them, obtaining 95.8% and 70.7% average accuracy, respectively. Code will be available at https://github.com/fmcp/attengait .",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323008683",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Code (set theory)",
      "Computer science",
      "Computer vision",
      "Gait",
      "Identification (biology)",
      "Image (mathematics)",
      "Machine learning",
      "Medicine",
      "Modalities",
      "Modality (human–computer interaction)",
      "Operating system",
      "Optical flow",
      "Pattern recognition (psychology)",
      "Physical medicine and rehabilitation",
      "Process (computing)",
      "Programming language",
      "Set (abstract data type)",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Castro",
        "given_name": "Francisco M."
      },
      {
        "surname": "Delgado-Escaño",
        "given_name": "Rubén"
      },
      {
        "surname": "Hernández-García",
        "given_name": "Ruber"
      },
      {
        "surname": "Marín-Jiménez",
        "given_name": "Manuel J."
      },
      {
        "surname": "Guil",
        "given_name": "Nicolás"
      }
    ]
  },
  {
    "title": "SSPNet: Scale and spatial priors guided generalizable and interpretable pedestrian attribute recognition",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110194",
    "abstract": "Global feature based Pedestrian Attribute Recognition (PAR) models are often poorly localized when using Grad-CAM for attribute response analysis, which has a significant impact on the interpretability, generalizability and performance. Previous researches have attempted to improve generalization and interpretation through meticulous model design, yet they often have neglected or underutilized effective prior information crucial for PAR. To this end, a novel Scale and Spatial Priors Guided Network (SSPNet) is proposed for PAR, which is mainly composed of the Adaptive Feature Scale Selection (AFSS) and Prior Location Extraction (PLE) modules. The AFSS module learns to provide reasonable scale prior information for different attribute groups, allowing the model to focus on different levels of feature maps with varying semantic granularity. The PLE module reveals potential attribute spatial prior information, which avoids unnecessary attention on irrelevant areas and lowers the risk of model over-fitting. More specifically, the scale prior in AFSS is adaptively learned from different layers of feature pyramid with maximum accuracy, while the spatial priors in PLE can be revealed from part feature with different granularity (such as image blocks, human pose keypoint and sparse sampling points). Besides, a novel IoU based attribute localization metric is proposed for Weakly-supervised Pedestrian Attribute Localization (WPAL) based on the improved Grad-CAM for attribute response mask. The experimental results on the intra-dataset and cross-dataset evaluations demonstrate the effectiveness of our proposed method in terms of mean accuracy (mA). Furthermore, it also achieves superior performance on the PCS dataset for attribute localization in terms of IoU. Code will be released at https://github.com/guotengg/SSPNet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323008919",
    "keywords": [
      "Artificial intelligence",
      "Bayesian probability",
      "Computer science",
      "Data mining",
      "Economics",
      "Feature (linguistics)",
      "Feature extraction",
      "Generalization",
      "Geometry",
      "Granularity",
      "Interpretability",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Metric (unit)",
      "Operating system",
      "Operations management",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Prior probability",
      "Pyramid (geometry)",
      "Quantum mechanics",
      "Scale (ratio)"
    ],
    "authors": [
      {
        "surname": "Shen",
        "given_name": "Jifeng"
      },
      {
        "surname": "Guo",
        "given_name": "Teng"
      },
      {
        "surname": "Zuo",
        "given_name": "Xin"
      },
      {
        "surname": "Fan",
        "given_name": "Heng"
      },
      {
        "surname": "Yang",
        "given_name": "Wankou"
      }
    ]
  },
  {
    "title": "A siamese-based verification system for open-set architecture attribution of synthetic images",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.03.002",
    "abstract": "Despite the wide variety of methods developed for synthetic image attribution, most of them can only attribute images generated by models or architectures included in the training set and do not work with unknown architectures, hindering their applicability in real-world scenarios. In this paper, we propose a verification framework that relies on a Siamese Network to address the problem of open-set attribution of synthetic images to the architecture that generated them. We consider two different settings. In the first setting, the system determines whether two images have been produced by the same generative architecture or not. In the second setting, the system verifies a claim about the architecture used to generate a synthetic image, utilizing one or multiple reference images generated by the claimed architecture. The main strength of the proposed system is its ability to operate in both closed and open-set scenarios so that the input images, either the query and reference images, can belong to the architectures considered during training or not. Experimental evaluations encompassing various generative architectures such as GANs, diffusion models, and transformers, focusing on synthetic face image generation, confirm the excellent performance of our method in both closed and open-set settings, as well as its strong generalization capabilities.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524000709",
    "keywords": [
      "Architecture",
      "Art",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Discrete mathematics",
      "Face (sociological concept)",
      "Generalization",
      "Generative grammar",
      "Generative model",
      "Image (mathematics)",
      "Mathematical analysis",
      "Mathematics",
      "Open set",
      "Pattern recognition (psychology)",
      "Programming language",
      "Set (abstract data type)",
      "Social science",
      "Sociology",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Abady",
        "given_name": "Lydia"
      },
      {
        "surname": "Wang",
        "given_name": "Jun"
      },
      {
        "surname": "Tondi",
        "given_name": "Benedetta"
      },
      {
        "surname": "Barni",
        "given_name": "Mauro"
      }
    ]
  },
  {
    "title": "A learnable support selection scheme for boosting few-shot segmentation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110202",
    "abstract": "Upon reevaluating recent studies of Few-Shot Segmentation (FSS), a key observation is that the random selection of support images is not always the optimal. In this situation, the support images cannot provide the useful guidance for the segmentation task. Therefore, we argue that a similarity-based support selection scheme, which selects support images according to the similarity between the query and candidate support images, is able to boost the performance of an FSS network. To this end, we propose a Siamese Support Selection Network (SSSN) which can be end-to-end trained along with an FSS network. We also leverage the joint utilization of a Convolutional Neural Network (CNN) and a Transformer network on top of a new feature fusion method to further improve the performance. To our knowledge, none of the similarity-based support selection scheme and the dual-stream network have been utilized for the FSS task before. Experimental results show that our FSS approach outperforms its counterparts on three data sets. In particular, the SSSN is able to boost the performance of an FSS network. We believe that these promising results should be due to the ability of the SSSN to select the top similar support images, which are useful for the FSS task. 1 1 Code is available at https://indtlab.github.io/projects/SSSN.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323008993",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Boosting (machine learning)",
      "Computer science",
      "Convolutional neural network",
      "Data mining",
      "Feature selection",
      "Image (mathematics)",
      "Leverage (statistics)",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Segmentation",
      "Selection (genetic algorithm)",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Shao",
        "given_name": "Wenxuan"
      },
      {
        "surname": "Qi",
        "given_name": "Hao"
      },
      {
        "surname": "Dong",
        "given_name": "Xinghui"
      }
    ]
  },
  {
    "title": "Forensic analysis of AI-compression traces in spatial and frequency domain",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.02.015",
    "abstract": "The classical JPEG compression is a rich source of cues for forensic image analysis. However, this compression standard will in the near future be complemented by a new, highly efficient learning-based compression standard called JPEG AI. JPEG AI is fundamentally different from classical JPEG. Hence, its forensic traces can also be expected to be fundamentally different. We argue that there is a pressing need for image forensics research to investigate these traces. In this work, we characterize forensic compression traces of different AI compression algorithms. Our analysis investigates AI compression artifacts in frequency domain and in spatial domain. Both domains exhibit similar artifacts that likely stem from upsampling operations of the decoders. Additionally, we report for one AI codec another artifact in homogeneous regions. We also investigate the artifact detectability in several scenarios including unseen AI compression traces and postprocessing. Here, frequency and autocorrelation features are better on additive noise and classical JPEG post-compression, while RGB features perform better on blurred and downsampled images.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524000503",
    "keywords": [
      "Artificial intelligence",
      "Codec",
      "Composite material",
      "Compression (physics)",
      "Compression artifact",
      "Computer hardware",
      "Computer science",
      "Computer vision",
      "Data compression",
      "Discrete cosine transform",
      "Image (mathematics)",
      "Image compression",
      "Image processing",
      "JPEG",
      "JPEG 2000",
      "Lossless JPEG",
      "Lossy compression",
      "Materials science",
      "Pattern recognition (psychology)",
      "Upsampling"
    ],
    "authors": [
      {
        "surname": "Bergmann",
        "given_name": "Sandra"
      },
      {
        "surname": "Moussa",
        "given_name": "Denise"
      },
      {
        "surname": "Brand",
        "given_name": "Fabian"
      },
      {
        "surname": "Kaup",
        "given_name": "André"
      },
      {
        "surname": "Riess",
        "given_name": "Christian"
      }
    ]
  },
  {
    "title": "A coarse-to-fine pattern parser for mitigating the issue of drastic imbalance in pixel distribution",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110143",
    "abstract": "The significance of minute semantic components such as eyes and eyebrows tends to be overshadowed by larger components like skin and background, leading to inadequate graph attention from the model. Recent cropping-and-segmenting strategies comprise distinct stages and do not involve extensive interactions, thereby preventing joint optimization for collaborative perception. To mitigate this flaw, a coarse-to-fine pattern parsing network (CtFPPN) is proposed based on the capsule network (CapsNet). The CtFPPN incorporates a coarse-grained parser module, which generates binary coarse-scaled masks for larger components, and a fine-grained parser module, which performs fine-scaled parsing for smaller components using the coarse contexts as references. To establish a connection between two parsers, the discretization attention fragmentation mechanism (DAFM) is customized. The fine-grained parser has the option to aggregate projections from non-spatially-fixed collections of the coarse-grained parser, thereby embodying the principle of \"coarse-to-fine\" of CtFPPN. Under the premise of the existence of imbalanced pixel distribution, quantitative and ablation experiments of face and human parsing demonstrate the superiority of CtFPPN over the state-of-the-arts. Notably, CtFPPN excels in mitigating the pixel imbalance issues and accurately defining fine-scaled semantic boundaries of minuscule components.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323008403",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Dependency grammar",
      "Graph",
      "Natural language processing",
      "Parsing",
      "Pattern recognition (psychology)",
      "Pixel",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Lin",
        "given_name": "Zhongqi"
      },
      {
        "surname": "Jiang",
        "given_name": "Xudong"
      },
      {
        "surname": "Zheng",
        "given_name": "Zengwei"
      }
    ]
  },
  {
    "title": "PA-Pose: Partial point cloud fusion based on reliable alignment for 6D pose tracking",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110151",
    "abstract": "Learning-based 6-DOF (6D) pose tracking, serving as a basis for most real-time applications such as augmented reality and robot manipulation, receives attention transiting from 2D to 3D vision, with the popularity of depth sensors. However, the irregular nature of 3D point clouds challenges this task, especially since the lack of explicit alignments hinders the interaction and fusion between the observed point clouds. Therefore, this paper proposes a novel approach named PA-Pose to achieve 6D pose tracking in point clouds. It takes the forward-predicted dense correspondences within an overlap as reliable alignments, to guide the feature fusion of the partial-to-partial point clouds. Then, the relative transformation pose of adjacent observations is continuously regressed from the point-wisely fused features by confidence scoring, avoiding non-differentiable pose fitting. In addition, a shifted point convolution (SPConv) operation is introduced in the fusion process, to further promote the local context interaction of the observed point cloud pair in the expanded alignment field. Extensive experiments on two benchmark datasets (YCB-Video and YCBInEOAT) demonstrate that our method achieves state-of-the-art performance. Even though only 3D point clouds are taken as input, our PA-Pose is still competitive with those methods fully utilizing RGB-D information in the single view. Finally, experiments in the real scene for tracking industrial objects also validates the effectiveness of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323008488",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Gene",
      "Geodesy",
      "Geography",
      "Point cloud",
      "Pose",
      "RGB color model",
      "Rigid transformation",
      "Transformation (genetics)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Zhenyu"
      },
      {
        "surname": "Wang",
        "given_name": "Qide"
      },
      {
        "surname": "Liu",
        "given_name": "Daxin"
      },
      {
        "surname": "Tan",
        "given_name": "Jianrong"
      }
    ]
  },
  {
    "title": "Large-margin multiple kernel ℓ p -SVDD using Frank–Wolfe algorithm for novelty detection",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110189",
    "abstract": "Using a variable ℓ p ≥ 1 -norm penalty on the slacks, the recently introduced ℓ p -norm Support Vector Data Description ( ℓ p -SVDD) method has improved the performance in novelty detection over the baseline approach, sometimes remarkably. This work extends this modelling formalism in multiple aspects. First, a large-margin extension of the ℓ p -SVDD method is formulated to enhance generalisation capability by maximising the margin between the positive and negative samples. Second, based on the Frank–Wolfe algorithm, an efficient yet effective method with predictable accuracy is presented to optimise the convex objective function in the proposed method. Finally, it is illustrated that the proposed approach can effectively benefit from a multiple kernel learning scheme to achieve state-of-the-art performance. The proposed method is theoretically analysed using Rademacher complexities to link its classification error probability to the margin and experimentally evaluated on several datasets to demonstrate its merits against existing methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323008865",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Geometry",
      "Law",
      "Machine learning",
      "Margin (machine learning)",
      "Mathematics",
      "Norm (philosophy)",
      "Political science",
      "Regular polygon",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Rahimzadeh Arashloo",
        "given_name": "Shervin"
      }
    ]
  },
  {
    "title": "GBCA: Graph Convolution Network and BERT combined with Co-Attention for fake news detection",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.02.014",
    "abstract": "Social media has evolved into a widely influential information source in contemporary society. However, the widespread use of social media also enables the rapid spread of fake news, which can pose a significant threat to national and social stability. Current fake news detection methods primarily rely on graph neural network, which analyze the dissemination patterns of news articles. Nevertheless, these approaches frequently overlook the semantic characteristics of the news content itself. To address this problem, we propose a novel Graph Convolution Network and BERT combined with Co-Attention (GBCA) model. Initially, we conduct training for a graph classification task on the propagation structure of fake news. Subsequently, we employ the BERT model to extract semantic features in fake news. Finally, we utilize co-attention mechanism to integrate the two dimensions of propagation structure and semantic features, which enhances the effectiveness of fake news detection. Our model outperforms baseline methods in terms of accuracy and training time, as demonstrated by experiments on three public benchmark datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524000473",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Attention network",
      "Baseline (sea)",
      "Benchmark (surveying)",
      "Computer science",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Economics",
      "Fake news",
      "Geodesy",
      "Geography",
      "Geology",
      "Graph",
      "Information retrieval",
      "Internet privacy",
      "Machine learning",
      "Management",
      "Oceanography",
      "Social media",
      "Task (project management)",
      "Theoretical computer science",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Zhen"
      },
      {
        "surname": "Lv",
        "given_name": "Qiyun"
      },
      {
        "surname": "Jia",
        "given_name": "Xiyuan"
      },
      {
        "surname": "Yun",
        "given_name": "Wenhao"
      },
      {
        "surname": "Miao",
        "given_name": "Gongxun"
      },
      {
        "surname": "Mao",
        "given_name": "Zongqing"
      },
      {
        "surname": "Wu",
        "given_name": "Guohua"
      }
    ]
  },
  {
    "title": "Self-supervised video distortion correction algorithm based on iterative optimization",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110114",
    "abstract": "Wide-angle video frames typically contain more information, but they also exhibit distortion that degrades the visual quality, especially at the edges. To eliminate this distortion from videos, we propose a self-supervised iterative optimization method in this paper. Specifically, we construct a motion parameter estimation model utilizing two consecutive distorted frames, where motion parameters comprise affine transform and distortion parameters. We apply the Gauss–Newton algorithm to minimize the sum-of-squares error between frames and update parameters. Treating inter-frame motion as undistort-affine-distort transformations, frame alignment is achieved by continuously adjusting transform parameters. Ultimately, frames are corrected using the converged parameters. We generated a synthetic dataset with various distortion parameters for evaluation. Experiments demonstrate superior performance versus state-of-the-art methods on synthetic and real wide-angle videos. Our algorithm also achieves higher parameter estimation accuracy without sacrificing efficiency.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323008117",
    "keywords": [
      "Affine transformation",
      "Algorithm",
      "Amplifier",
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Distortion (music)",
      "Frame (networking)",
      "Mathematics",
      "Motion estimation",
      "Pure mathematics",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Ren",
        "given_name": "Zhihao"
      },
      {
        "surname": "Su",
        "given_name": "Ya"
      }
    ]
  },
  {
    "title": "Corrigendum to “GITGAN: Generative inter-subject transfer for EEG motor imagery analysis” [Pattern Recognition 146 (2024) 110015]",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110217",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323009147",
    "keywords": [
      "Artificial intelligence",
      "Brain–computer interface",
      "Computer science",
      "Electroencephalography",
      "Generative grammar",
      "Motor imagery",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Psychology",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Yin",
        "given_name": "Kang"
      },
      {
        "surname": "Lim",
        "given_name": "Elissa Yanting"
      },
      {
        "surname": "Lee",
        "given_name": "Seong-Whan"
      }
    ]
  },
  {
    "title": "Contrasting augmented features for domain adaptation with limited target domain data",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110145",
    "abstract": "Domain adaptation aims to alleviate distribution gaps between source and target domains. However, when the available target domain data are scarce for training, learning generalizable representations for domain adaptation is challenging. We propose a novel approach, dubbed Contrasting Augmented Features (CAF), to tackle the challenge of insufficient target domain data for domain adaptation, by generating and contrasting augmented features. We introduce a semantic feature generator to generate augmented features by replacing the instance-level feature statistics of one domain with another domain. With the augmented features, we further design the reweighted instance contrastive loss and category contrastive loss to improve feature discrimination and align feature distributions of source and target domains. CAF can be applied to few-shot domain adaptation and unsupervised domain adaptation with limited unlabeled target domain data. Despite its simplicity, extensive experiments show promising results for both applications. In addition, experiments demonstrate that CAF is more robust to the number of target domain data and also effective in vanilla unsupervised domain adaptation setting with full target domain data.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323008427",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Computer vision",
      "Domain (mathematical analysis)",
      "Domain adaptation",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Yu",
        "given_name": "Xi"
      },
      {
        "surname": "Gu",
        "given_name": "Xiang"
      },
      {
        "surname": "Sun",
        "given_name": "Jian"
      }
    ]
  },
  {
    "title": "GNN-LoFI: A novel graph neural network through localized feature-based histogram intersection",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110210",
    "abstract": "Graph neural networks are increasingly becoming the framework of choice for graph-based machine learning. In this paper, we propose a new graph neural network architecture that substitutes classical message passing with an analysis of the local distribution of node features. To this end, we extract the distribution of features in the egonet for each local neighbourhood and compare them against a set of learned label distributions by taking the histogram intersection kernel. The similarity information is then propagated to other nodes in the network, effectively creating a message passing-like mechanism where the message is determined by the ensemble of the features. We perform an ablation study to evaluate the network’s performance under different choices of its hyper-parameters. Finally, we test our model on standard graph classification and regression benchmarks, and we find that it outperforms widely used alternative approaches, including both graph kernels and graph neural networks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032300907X",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Graph",
      "Graph kernel",
      "Histogram",
      "Image (mathematics)",
      "Kernel method",
      "Pattern recognition (psychology)",
      "Polynomial kernel",
      "Support vector machine",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Bicciato",
        "given_name": "Alessandro"
      },
      {
        "surname": "Cosmo",
        "given_name": "Luca"
      },
      {
        "surname": "Minello",
        "given_name": "Giorgia"
      },
      {
        "surname": "Rossi",
        "given_name": "Luca"
      },
      {
        "surname": "Torsello",
        "given_name": "Andrea"
      }
    ]
  },
  {
    "title": "From patch, sample to domain: Capture geometric structures for few-shot learning",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110147",
    "abstract": "Few-shot learning aims to recognize novel concepts with only few samples by using prior knowledge learned from the seen concepts. In this paper, we address the problem of few-shot learning under domain shifts. Traditional few-shot learning methods are not directly applicable to cross-domain scenarios due to the large discrepancy of feature distributions across domains. To this end, we propose a novel Hierarchical Optimal Transport network with Attention (HOTA) for cross-domain few-shot learning. The underlying idea is to learn the transferable and discriminative embeddings by taking advantage of the hierarchical geometric structures among image data, ranging from patch, sample to domain. The HOTA framework utilizes a hierarchical optimal transport network to smooth the domain shifts by domain alignment while enhancing the discrimination and the transferability of the embeddings by aligning the patches of images. To further enhance the transferability, HOTA conducts a mix-up data augmentation based on cross-domain attention to capture the relationships of samples in different domains. The extensive experiments on a variety of few-shot benchmark scenarios demonstrate that HOTA outperforms the state-of-the-art methods under both supervised and unsupervised conditions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323008440",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Data mining",
      "Discriminative model",
      "Domain (mathematical analysis)",
      "Feature (linguistics)",
      "Geodesy",
      "Geography",
      "Linguistics",
      "Logit",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Sample (material)",
      "Transferability",
      "Variety (cybernetics)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Qiaonan"
      },
      {
        "surname": "Wen",
        "given_name": "Guihua"
      },
      {
        "surname": "Yang",
        "given_name": "Pei"
      }
    ]
  },
  {
    "title": "Dynamic semantic structure distillation for low-resolution fine-grained recognition",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110216",
    "abstract": "Low-resolution images are ubiquitous in real applications such as surveillance and mobile photography. However, existing fine-grained approaches usually suffer catastrophic failures when dealing with low-resolution inputs. This is because their learning strategy inherently depends on the semantic structure of the pre-trained model, resulting in poor robustness and generalization. To mitigate this limitation, we propose a dynamic semantic structure distillation learning framework. Our method first facilitates knowledge distillation of diverse semantic structures by perturbing the composition of semantic components and then utilizes a decoupled distillation objective to prevent the loss of primary semantic part relation knowledge. We evaluate our proposed approach on two knowledge distillation tasks: high-to-low resolution and large-to-small model. The experimental results show that our proposed approach significantly outperforms existing methods in low-resolution fine-grained image classification tasks. This indicates that it can effectively distill knowledge from high-resolution teacher models to low-resolution student models. Furthermore, we demonstrate the effectiveness of our approach in general image classification and standard knowledge distillation tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323009135",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Data mining",
      "Distillation",
      "Gene",
      "Generalization",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Resolution (logic)",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Liang",
        "given_name": "Mingjiang"
      },
      {
        "surname": "Huang",
        "given_name": "Shaoli"
      },
      {
        "surname": "Liu",
        "given_name": "Wei"
      }
    ]
  },
  {
    "title": "Offline handwritten mathematical expression recognition with graph encoder and transformer decoder",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110155",
    "abstract": "Handwritten mathematical expression recognition (HMER) has attracted extensive attention. Despite the significant progress achieved in recent years attributed to the development of deep learning approaches, HMER remains a challenge due to the complex spatial structure and variable writing styles. Encoder–decoder models with attention mechanism, which treats HMER as an image-to-sequence (i.e. LaTeX) generation task, have boosted the accuracy, but suffer from low interpretability in that the symbols are not segmented explicitly. Symbol segmentation is desired for facilitating post-processing and human interaction in real applications. In this paper, we formulate the mathematical expression as a graph and propose a Graph-Encoder-Transformer-Decoder (GETD) approach for HMER. For constructing the graph from input image, candidate symbols are first detected using an object detector and represented as the nodes of a graph, called symbol graph, and the edges of the graph encodes the between-symbol relationship. The spatial information is aggregated in a graph neural network (GNN), and a Transformer-based decoder is used to identify the symbol classes and structure from the graph. Experiments on public datasets demonstrate that our GETD model achieves competitive expression recognition performance while offering good interpretability compared with previous methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032300852X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Encoder",
      "Graph",
      "Interpretability",
      "Operating system",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Theoretical computer science",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Tang",
        "given_name": "Jia-Man"
      },
      {
        "surname": "Guo",
        "given_name": "Hong-Yu"
      },
      {
        "surname": "Wu",
        "given_name": "Jin-Wen"
      },
      {
        "surname": "Yin",
        "given_name": "Fei"
      },
      {
        "surname": "Huang",
        "given_name": "Lin-Lin"
      }
    ]
  },
  {
    "title": "IIOF: Intra- and Inter-feature orthogonal fusion of local and global features for music emotion recognition",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110200",
    "abstract": "In this paper, we propose intra- and inter-feature orthogonal fusion (IIOF) of local and global features obtained from MS-SincResNet or MS-SSincResNet (a variant of MS-SincResNet) for music emotion recognition (MER). Given a raw waveform of music signal, MS-SincResNet/MS-SSincResNet is first used to learn several 2D representations having different receptive fields and obtain embeddings with time-frequency information from different layers. Then, local and global features are extracted from these embeddings. IIOF consisting of intra-feature OF and inter-feature OF is further employed to integrate both local and global features to obtain a discriminative descriptor for MER. The intra-feature OF is used to enhance the diversity of the global feature, and the inter-feature OF is utilized to reduce redundancies and produce complementary information between local and global features. The experimental results have demonstrated that the representation discriminability can be enhanced by IIOF considering the feature orthogonality. Furthermore, extensive experimental results have shown that the proposed method outperforms other state-of-the-art methods in terms of regression and classification tasks on the well-known MER datasets, including the DEAM dataset and the PMEmo dataset. The codes are available at https://github.com/PeiChunChang/MS-SSincResNet_with_IIOF.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032300897X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Discriminative model",
      "Feature (linguistics)",
      "Feature learning",
      "Fusion",
      "Geometry",
      "Law",
      "Linguistics",
      "Mathematics",
      "Orthogonality",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Chang",
        "given_name": "Pei-Chun"
      },
      {
        "surname": "Chen",
        "given_name": "Yong-Sheng"
      },
      {
        "surname": "Lee",
        "given_name": "Chang-Hsing"
      }
    ]
  },
  {
    "title": "Frame-part-activated deep reinforcement learning for Action Prediction",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.02.024",
    "abstract": "In this paper, we propose a frame-part-activated deep reinforcement learning (FPA-DRL) for action prediction. Most existing methods for action prediction utilize the evolution of whole frames to model actions, which cannot avoid the noise of the current action, especially in the early prediction. Moreover, the loss of structural information of human body diminishes the capacity of features to describe actions. To address this, we design a FPA-DRL to exploit the structure of the human body by extracting skeleton proposals and reduce the redundancy of frames under a deep reinforcement learning framework. Specifically, we extract features from different parts of the human body individually, activate the action-related parts in features and the action-related frames in videos to enhance the representation. Our method not only exploits the structure information of the human body, but also considers the attention frame for expressing actions. We evaluate our method on three popular action prediction datasets: UT-Interaction, BIT-Interaction and UCF101. Our experimental results demonstrate that our method achieves the very competitive performance with state-of-the-arts.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524000606",
    "keywords": [
      "Action (physics)",
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Exploit",
      "Frame (networking)",
      "Law",
      "Machine learning",
      "Operating system",
      "Pattern recognition (psychology)",
      "Physics",
      "Political science",
      "Politics",
      "Quantum mechanics",
      "Redundancy (engineering)",
      "Reinforcement learning",
      "Representation (politics)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Lei"
      },
      {
        "surname": "Song",
        "given_name": "Zhanjie"
      }
    ]
  },
  {
    "title": "Continual learning for adaptive social network identification",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.02.020",
    "abstract": "The popularity of social networks as primary mediums for sharing visual content has made it crucial for forensic experts to identify the original platform of multimedia content. Various methods address this challenge, but the constant emergence of new platforms and updates to existing ones often render forensic tools ineffective shortly after release. This necessitates the regular updating of methods and models, which can be particularly cumbersome for techniques based on neural networks which cannot quickly adapt to new classes without sacrificing performance on previously learned ones – a phenomenon known as catastrophic forgetting. Recently, researchers aimed at mitigating this problem via a family of techniques known as continual learning. In this paper we study the applicability of continual learning techniques to the social network identification task by evaluating two relevant forensic scenarios: Incremental Social Platform Classification, for handling newly introduced social media platforms, and Incremental Social Version Classification, for addressing updated versions of a set of existing social networks. We perform an extensive experimental evaluation of a variety of continual learning approaches applied to these two scenarios. Experimental results demonstrate that, although Continual Social Network Identification remains a difficult problem, catastrophic forgetting can be significantly mitigated in both scenarios by retaining only a fraction of the image patches from past task training samples or by employing previous tasks prototypes.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524000540",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Botany",
      "Computer science",
      "Data science",
      "Engineering",
      "Forgetting",
      "Identification (biology)",
      "Linguistics",
      "Machine learning",
      "Philosophy",
      "Popularity",
      "Programming language",
      "Psychology",
      "Set (abstract data type)",
      "Social media",
      "Social network (sociolinguistics)",
      "Social psychology",
      "Systems engineering",
      "Task (project management)",
      "Variety (cybernetics)",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Magistri",
        "given_name": "Simone"
      },
      {
        "surname": "Baracchi",
        "given_name": "Daniele"
      },
      {
        "surname": "Shullani",
        "given_name": "Dasara"
      },
      {
        "surname": "Bagdanov",
        "given_name": "Andrew D."
      },
      {
        "surname": "Piva",
        "given_name": "Alessandro"
      }
    ]
  },
  {
    "title": "Oracle character recognition using unsupervised discriminative consistency network",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110180",
    "abstract": "Ancient history relies on the study of ancient characters. However, real-world scanned oracle characters are difficult to collect and annotate, posing a major obstacle for oracle character recognition (OrCR). Besides, serious abrasion and inter-class similarity also make OrCR more challenging. In this paper, we propose a novel unsupervised domain adaptation method for OrCR, which enables to transfer knowledge from labeled handprinted oracle characters to unlabeled scanned data. We leverage pseudo-labeling to incorporate the semantic information into adaptation and constrain augmentation consistency to make the predictions of scanned samples consistent under different perturbations, leading to the model robustness to abrasion, stain and distortion. Simultaneously, an unsupervised transition loss is proposed to learn more discriminative features on the scanned domain by optimizing both between-class and within-class transition probability. Extensive experiments show that our approach achieves state-of-the-art result on Oracle-241 dataset and substantially outperforms the recently proposed structure-texture separation network by 15.1%.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323008774",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Boosting (machine learning)",
      "Chemistry",
      "Classifier (UML)",
      "Computer science",
      "Discriminative model",
      "Domain adaptation",
      "Gene",
      "Leverage (statistics)",
      "Machine learning",
      "Oracle",
      "Pattern recognition (psychology)",
      "Robustness (evolution)",
      "Software engineering"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Mei"
      },
      {
        "surname": "Deng",
        "given_name": "Weihong"
      },
      {
        "surname": "Su",
        "given_name": "Sen"
      }
    ]
  },
  {
    "title": "Mathematical formula detection in document images: A new dataset and a new approach",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110212",
    "abstract": "Most of existing mathematical formula detectors focus on detecting formula entities through object detection or instance segmentation techniques. However, these methods often fail to convey complete messages due to the absence of the contextual and layout information of mathematical formulas. For a more comprehensive understanding of mathematical formulas in document images, it is preferable to detect logical formula blocks that include one or multiple formula entities arranged in their natural reading order. These logical formula blocks enable the transmission of complete contextual messages of mathematical formulas and aid in the reconstruction of layout information of the document images, resulting in a more accurate mathematical formula detection. In this paper, we present a novel perspective on mathematical formula detection by framing it as a joint task of formula entity detection and formula relation extraction for identifying logical formula blocks. To this end, we introduce a new, large-scale dataset, called ArxivFormula, that includes well-annotated formula entity bounding boxes and formula relationships. We also propose a new approach, called FormulaDet, to address these two sub-tasks simultaneously. FormulaDet first employs a dynamic convolution-based formula entity detector, named DynFormula, to detect formula entities. It then uses a multi-modal transformer-based relation extraction method, named RelFormer, to group these detected formula entities into logical formula blocks. Extensive experiments on standard benchmarks in this field and the proposed dataset demonstrate that our FormulaDet can achieve significantly improved performance on formula entity detection and formula relation extraction compared to previous state-of-the-art methods. The joint detection and relation extraction approach provides a more thorough understanding of mathematical formulas in document images and effectively supports downstream tasks such as document layout analysis and scientific document digitization. The ArxivFormula dataset is publicly available at https://github.com/microsoft/ArxivFormula.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323009093",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Bounding overwatch",
      "Computer science",
      "Data mining",
      "Mathematics",
      "Pascal (unit)",
      "Pattern recognition (psychology)",
      "Programming language",
      "Relation (database)",
      "Relationship extraction"
    ],
    "authors": [
      {
        "surname": "Hu",
        "given_name": "Kai"
      },
      {
        "surname": "Zhong",
        "given_name": "Zhuoyao"
      },
      {
        "surname": "Sun",
        "given_name": "Lei"
      },
      {
        "surname": "Huo",
        "given_name": "Qiang"
      }
    ]
  },
  {
    "title": "Transformer-based stroke relation encoding for online handwriting and sketches",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110131",
    "abstract": "Stroke classification for online handwriting and sketches, aimed at grouping strokes into different semantic categories, has drawn considerable attention due to its wide applications. This task is challenging since individual strokes look similar and are easily confused with each other. The key is to consider both the individual strokes and the contextual information jointly for making prediction. Previous methods are insufficient in modeling and exploiting complex contextual information of strokes. To overcome this limitation, we propose a Transformer-based model for Online Handwriting and Sketches (T-OHS), with novel relation encoding schemes to take advantage of temporal and spatial information in stroke sequence. Particularly, we introduce a coarse-to-fine hierarchical encoding approach based on the polar coordinate system for precisely modeling spatial relations between strokes. Experiments on three types of handwriting data, including online handwritten documents, diagrams, and sketches, demonstrate that our method is universal and provides state-of-the-art performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323008282",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Data mining",
      "Encoding (memory)",
      "Feature extraction",
      "Handwriting",
      "Handwriting recognition",
      "Key (lock)",
      "Natural language processing",
      "Physics",
      "Quantum mechanics",
      "Relation (database)",
      "Spatial relation",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Jing-Yu"
      },
      {
        "surname": "Zhang",
        "given_name": "Yan-Ming"
      },
      {
        "surname": "Yin",
        "given_name": "Fei"
      },
      {
        "surname": "Liu",
        "given_name": "Cheng-Lin"
      }
    ]
  },
  {
    "title": "Coresets for fast causal discovery with the additive noise model",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110149",
    "abstract": "Causal discovery reveals the true causal relationships behind data and discovering causal relationships from observed data is a particularly challenging problem, especially in large-scale datasets. The functional causal model is an effective method for causal discovery, but its time efficiency cannot be guaranteed. How to efficiently apply it to massive data still needs to be solved. In this paper, we propose a coreset construction for the additive noise model to accelerate causal discovery. According to the asymmetry characteristic of causality, samples were assigned different weights to construct the coreset. With the constructed coreset, we propose a Fast causal discovery algorithm based on the Additive Noise Model (FANM) to improve the time efficiency of the functional causal model while ensuring the result performance of causal discovery. Experiments on synthetic data and real-world data show that our proposed algorithm is much more time-efficient than the methods based on the functional causal model, and the runtime of FANM remains consistent as sample size increases while maintaining or exceeding the accuracy of the original nonlinear additive noise model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323008464",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Causal inference",
      "Causal model",
      "Causal structure",
      "Causality (physics)",
      "Computer science",
      "Data mining",
      "Econometrics",
      "Image (mathematics)",
      "Machine learning",
      "Mathematics",
      "Noise (video)",
      "Physics",
      "Quantum mechanics",
      "Statistics",
      "Synthetic data"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Boxiang"
      },
      {
        "surname": "Wang",
        "given_name": "Shuliang"
      },
      {
        "surname": "Chi",
        "given_name": "Lianhua"
      },
      {
        "surname": "Yuan",
        "given_name": "Hanning"
      },
      {
        "surname": "Yuan",
        "given_name": "Ye"
      },
      {
        "surname": "Li",
        "given_name": "Qi"
      },
      {
        "surname": "Geng",
        "given_name": "Jing"
      },
      {
        "surname": "Zhang",
        "given_name": "Shao-Liang"
      }
    ]
  },
  {
    "title": "Dynamic Graph Contrastive Learning via Maximize Temporal Consistency",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110144",
    "abstract": "Graph contrastive learning (GCL) is one of the most powerful self-supervised representation learning frameworks. Existing GCL methods have achieved impressive performance. However, it is still challenging to capture the evolution of nodes or edges, where the interaction of nodes or edges is stable in a short time but changeable at long time intervals. Therefore, it is crucial to capture the temporal consistency in dynamic graph. In this paper, we propose a novel Dynamic Graph Contrastive Learning framework, DyGCL, which learns node representation by maximizing the temporal consistency in a short time and discriminating the non-consistency in a long term. More specifically, DyGCL consists of two parts: GCL Trainer and Auxiliary Trainer. GCL Trainer focus on distinguishing temporal consistency and non-consistency. And the Auxiliary Trainer aims to improve the generalization ability with less labeled data as auxiliary supervision. Finally, we demonstrate the effectiveness and superiority of DyGCL by applying it to three datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323008415",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Consistency (knowledge bases)",
      "Generalization",
      "Graph",
      "Law",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Political science",
      "Politics",
      "Programming language",
      "Representation (politics)",
      "Theoretical computer science",
      "Trainer"
    ],
    "authors": [
      {
        "surname": "Bao",
        "given_name": "Peng"
      },
      {
        "surname": "Li",
        "given_name": "Jianian"
      },
      {
        "surname": "Yan",
        "given_name": "Rong"
      },
      {
        "surname": "Liu",
        "given_name": "Zhongyi"
      }
    ]
  },
  {
    "title": "Unsupervised feature selection by learning exponential weights",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110183",
    "abstract": "Unsupervised feature selection has gained considerable attention for extracting valuable features from unlabeled datasets. Existing approaches typically rely on sparse mapping matrices to preserve local neighborhood structures. However, this strategy favors large-weight features, potentially overlooking smaller yet valuable ones and distorting data distribution and feature structure. Besides, some methods focus on local structure information, failing to explore global information. To address these limitations, we introduce an exponential weighting mechanism to induce a rational feature distribution and explore data structure in the feature subspace. Specifically, we propose a unified framework incorporating local structure learning and exponentially weighted sparse regression for optimal feature combinations, preserving global and local information. Experimental results demonstrate the superiority of our approach over existing unsupervised feature selection methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323008804",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Feature (linguistics)",
      "Feature learning",
      "Feature selection",
      "Focus (optics)",
      "Linguistics",
      "Machine learning",
      "Medicine",
      "Minimum redundancy feature selection",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Radiology",
      "Subspace topology",
      "Unsupervised learning",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Chenchen"
      },
      {
        "surname": "Wang",
        "given_name": "Jun"
      },
      {
        "surname": "Gu",
        "given_name": "Zhichen"
      },
      {
        "surname": "Wei",
        "given_name": "Jin-Mao"
      },
      {
        "surname": "Liu",
        "given_name": "Jian"
      }
    ]
  },
  {
    "title": "Interpretable multidisease diagnosis and label noise detection based on a matching network and self-paced learning",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110178",
    "abstract": "With the extensive use of information systems in hospitals, a large quantity of electronic medical record data has been accumulated, which makes it possible to train clinical decision support systems based on the data. However, electronic medical records are written by doctors of different levels, which easily introduces label noise into the datasets. The lack of interpretability of current auxiliary diagnosis methods is another problem. To address these challenges, we introduce a matching network based on medical guidelines and build an auxiliary diagnosis model based on self-paced learning. The matching network based on guidelines can provide medical knowledge beyond medical records and a certain degree of interpretability. Additionally, self-paced learning can help the model identify the label noise and prevent the model from being misled. The experiments show that our method outperforms the baselines in a Chinese medical multi-disease diagnosis dataset and the MIMIC-III dataset and has good performance in the label noise detection task.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323008750",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Deep learning",
      "Economics",
      "Image (mathematics)",
      "Interpretability",
      "Machine learning",
      "Management",
      "Matching (statistics)",
      "Medical record",
      "Medicine",
      "Noise (video)",
      "Pathology",
      "Radiology",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Long",
        "given_name": "Jiawei"
      },
      {
        "surname": "Ren",
        "given_name": "Jiangtao"
      }
    ]
  },
  {
    "title": "Joint facial action unit recognition and self-supervised optical flow estimation",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.03.022",
    "abstract": "Facial action unit (AU) recognition and optical flow estimation are two highly correlated tasks, since optical flow can provide motion information of facial muscles to facilitate AU recognition. However, most existing AU recognition methods handle the two tasks independently by offline extracting optical flow as auxiliary information or directly ignoring the use of optical flow. In this paper, we propose a novel end-to-end joint framework of AU recognition and optical flow estimation, in which the two tasks contribute to each other. Moreover, due to the lack of optical flow annotations in AU datasets, we propose to estimate optical flow in a self-supervised manner. To regularize the self-supervised estimation of optical flow, we propose an identical mapping constraint for the optical flow guided image warping process, in which the estimated optical flow between two same images is required to not change the image during warping. Experiments demonstrate that our framework (i) outperforms most of the state-of-the-art AU recognition methods on the challenging BP4D and GFT benchmarks, and (ii) also achieves competitive self-supervised optical flow estimation performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524000928",
    "keywords": [
      "Action (physics)",
      "Architectural engineering",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Engineering",
      "Facial recognition system",
      "Flow (mathematics)",
      "Geometry",
      "Image (mathematics)",
      "Joint (building)",
      "Mathematics",
      "Mathematics education",
      "Optical flow",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Speech recognition",
      "Unit (ring theory)"
    ],
    "authors": [
      {
        "surname": "Shao",
        "given_name": "Zhiwen"
      },
      {
        "surname": "Zhou",
        "given_name": "Yong"
      },
      {
        "surname": "Li",
        "given_name": "Feiran"
      },
      {
        "surname": "Zhu",
        "given_name": "Hancheng"
      },
      {
        "surname": "Liu",
        "given_name": "Bing"
      }
    ]
  },
  {
    "title": "Network characteristics adaption and hierarchical feature exploration for robust object recognition",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110240",
    "abstract": "Recent advances in deep networks have achieved appealing performances on object recognition tasks, due to their robust feature learning abilities. Besides the generated deep features, other network characteristics, e.g., inter-layer weight matrix and their back-propagated derivatives, may behave complementarily in feature learning in terms of generalization and robustness performances. However, characteristics adaptivity to different databases is not well studied. Meanwhile, current algorithms are apt to explore the most salient features for better generalization performance, while the hierarchically-salient features that may be beneficial for network robustness are not fully explored. Thus, we propose an attention module to make network characteristics adaptive to different training tasks, which can be further combined with the dynamic dropout algorithm to suppress salient neurons to explore more SndMS (Second Most Salient) features for robust recognition. The proposed algorithm has two main merits. First, the complementarity of network characteristics is taken into account when conducting training on different databases; Second, with the exploration of more SndMS neurons for hierarchically-salient feature representation learning, the network robustness against adversarial perturbations or fine-grained differences can be enhanced. The extensive experiments on seven public databases show that the proposed attention-based dropout largely improves the network robustness, without compromising the generalization performance, compared with related variants and state-of-the-art (SOTA) algorithms. Algorithm codes are available at https://github.com/lingjivoo/ACAD.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323009378",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Cognitive neuroscience of visual object recognition",
      "Computer science",
      "Feature (linguistics)",
      "Feature extraction",
      "Feature learning",
      "Gene",
      "Generalization",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Robustness (evolution)",
      "Salient"
    ],
    "authors": [
      {
        "surname": "Xie",
        "given_name": "Weicheng"
      },
      {
        "surname": "Luo",
        "given_name": "Cheng"
      },
      {
        "surname": "Wang",
        "given_name": "Gui"
      },
      {
        "surname": "Shen",
        "given_name": "Linlin"
      },
      {
        "surname": "Lai",
        "given_name": "Zhihui"
      },
      {
        "surname": "Song",
        "given_name": "Siyang"
      }
    ]
  },
  {
    "title": "iCausalOSR: invertible Causal Disentanglement for Open-set Recognition",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110243",
    "abstract": "Despite recent developments that allowed neural networks to achieve impressive performance in a wide range of recognition, these models are intrinsically challenged by real-world applications. Open-set recognition is introduced to facilitate the development of recognition systems towards real-world applications, for this it has to deal with the issue caused by discriminative features. Such features arise from closed-set training and tend to partition the full input space into the closed set of target classes. To reduce the issue, we present an invertible model, iCausalOSR, to learn invertible causal disentanglement to reveal the essence of classes for open-set recognition. The invertible model consists of an encoder and class functions, wherein the class functions are responsible to model the known classes, and the encoder is responsible for progressive signal separation and contraction. A contrast strategy is designed to couple the encoder and class functions to learn invertible causal disentanglement. The dual properties of the model, causal disentanglement and invertibility, constitute the key elements in revealing the class essence. Experiments on widely-used standard datasets in open-set recognition demonstrate the superior performance of our model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323009408",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Discrete mathematics",
      "Discriminative model",
      "Encoder",
      "Invertible matrix",
      "Mathematics",
      "Open set",
      "Operating system",
      "Pattern recognition (psychology)",
      "Programming language",
      "Pure mathematics",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Fenglei"
      },
      {
        "surname": "Li",
        "given_name": "Baomin"
      },
      {
        "surname": "Han",
        "given_name": "Jingling"
      }
    ]
  },
  {
    "title": "Multi-Source Domain Adaptation with Mixture of Joint Distributions",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110295",
    "abstract": "The goal of Multi-Source Domain Adaptation (MSDA) is to train a model (e.g., neural network) with minimal target loss, utilizing training data from multiple source domains (source joint distributions) and a target domain (target joint distribution). The challenge in this problem is that the multiple source joint distributions are different from the target joint distribution. In this paper, we develop a theory that shows a neural network’s target loss is upper bounded by both its source mixture loss (i.e., the loss concerning the source mixture joint distribution) and the Pearson χ 2 divergence between the source mixture joint distribution and the target joint distribution. Here, the source mixture joint distribution is the mixture of multiple source joint distributions with mixing weights. Accordingly, we propose an algorithm that optimizes both the mixing weights and the neural network to minimize the estimated source mixture loss and the estimated Pearson χ 2 divergence. To estimate the Pearson χ 2 divergence, we rewrite it as the maximal value of a quadratic functional, exploit a linear-in-parameter function as the functional’s input, and solve the resultant optimization problem with an analytic solution. This analytic solution allows us to explicitly express the estimated divergence as a loss of the mixing weights and the network’s feature extractor. Finally, we conduct experiments on popular image classification datasets, and the results show that our algorithm statistically outperforms the comparison algorithms. PyTorch code is available at https://github.com/sentaochen/Mixture-of-Joint-Distributions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324000463",
    "keywords": [
      "Adaptation (eye)",
      "Algorithm",
      "Architectural engineering",
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Domain (mathematical analysis)",
      "Domain adaptation",
      "Engineering",
      "Joint (building)",
      "Joint probability distribution",
      "Mathematical analysis",
      "Mathematics",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Psychology",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Sentao"
      }
    ]
  },
  {
    "title": "A guided-based approach for deepfake detection: RGB-depth integration via features fusion",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.03.025",
    "abstract": "Deep fake technology paves the way for a new generation of super realistic artificial content. While this opens the door to extraordinary new applications, the malicious use of deepfakes allows for far more realistic disinformation attacks than ever before. In this paper, we start from the intuition that generating fake content introduces possible inconsistencies in the depth of the generated images. This extra information provides valuable spatial and semantic cues that can reveal inconsistencies facial generative methods introduce. To test this idea, we evaluate different strategies for integrating depth information into an RGB detector and we propose an attention mechanism that makes it possible to integrate information from depth effectively. In addition to being more accurate than an RGB model, our Masked Depthfake Network method is + 3 . 2 % more robust against common adversarial attacks on average than a typical RGB detector. Furthermore, we show how this technique allows the model to learn more discriminative features than RGB alone.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524000990",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Detector",
      "Discriminative model",
      "Epistemology",
      "Gene",
      "Generative adversarial network",
      "Intuition",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy",
      "RGB color model",
      "Robustness (evolution)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Leporoni",
        "given_name": "Giorgio"
      },
      {
        "surname": "Maiano",
        "given_name": "Luca"
      },
      {
        "surname": "Papa",
        "given_name": "Lorenzo"
      },
      {
        "surname": "Amerini",
        "given_name": "Irene"
      }
    ]
  },
  {
    "title": "Multi-scale hypergraph-based feature alignment network for cell localization",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110260",
    "abstract": "Cell localization in medical image analysis is a challenging task due to the significant variation in cell shape, size and color. Existing localization methods continue to tackle these challenges separately, frequently facing complications where these difficulties intersect and adversely impact model performance. In this paper, these challenges are first reframed as issues of feature misalignment between cell images and location maps, which are then collectively addressed. Specifically, we propose a feature alignment model based on a multi-scale hypergraph attention network. The model considers local regions in the feature map as nodes and utilizes a learnable similarity metric to construct hypergraphs at various scales. We then utilize a hypergraph convolutional network to aggregate the features associated with the nodes and achieve feature alignment between the cell images and location maps. Furthermore, we introduce a stepwise adaptive fusion module to fuse features at different levels effectively and adaptively. The comprehensive experimental results demonstrate the effectiveness of our proposed multi-scale hypergraph attention module in addressing the issue of feature misalignment, and our model achieves state-of-the-art performance across various cell localization datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324000116",
    "keywords": [
      "Aggregate (composite)",
      "Artificial intelligence",
      "Composite material",
      "Computer science",
      "Construct (python library)",
      "Data mining",
      "Discrete mathematics",
      "Economics",
      "Electrical engineering",
      "Engineering",
      "Feature (linguistics)",
      "Fuse (electrical)",
      "Hypergraph",
      "Image (mathematics)",
      "Linguistics",
      "Materials science",
      "Mathematics",
      "Metric (unit)",
      "Operations management",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Programming language",
      "Quantum mechanics",
      "Scale (ratio)",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Bo"
      },
      {
        "surname": "Zhang",
        "given_name": "Yong"
      },
      {
        "surname": "Zhang",
        "given_name": "Chengyang"
      },
      {
        "surname": "Piao",
        "given_name": "Xinglin"
      },
      {
        "surname": "Hu",
        "given_name": "Yongli"
      },
      {
        "surname": "Yin",
        "given_name": "Baocai"
      }
    ]
  },
  {
    "title": "Confidence-based dynamic cross-modal memory network for image aesthetic assessment",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110227",
    "abstract": "Image aesthetic assessment (IAA) aims to design algorithms that can make human-like aesthetic decisions. Due to its high subjectivity and complexity, visual information alone is limited to fully predict the aesthetic quality of an image. More and more researchers try to use complementary information from user comments. However, user comments are not always available due to various technical and practical reasons. Therefore, it is necessary to find a way to reconstruct the missing textual information for aesthetic prediction with visual information only. This paper solves this problem by proposing a Confidence-based Dynamic Cross-modal Memory Network (CDCM-Net). Specifically, the proposed CDCM-Net consists of two key components: Visual and Textual Memory (VTM) network and Confidence-based Dynamical Multi-modal Fusion module (CDMF). VTM is based on the key–value memory network. It consists of a visual key memory and a textual value memory. The visual key memory learns the visual information. While the textual value memory learns to remember the textual feature and align them with the corresponding visual features. During inference, textual information can be reconstructed using only visual features. Furthermore, a CDMF module is introduced to perform trustworthy fusion. CDMF evaluates modality-level informativeness and then dynamically integrates reliable information. Extensive experiments are performed to demonstrate the superiority of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032300924X",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Computer security",
      "Feature (linguistics)",
      "Inference",
      "Information retrieval",
      "Key (lock)",
      "Linguistics",
      "Machine learning",
      "Modal",
      "Modality (human–computer interaction)",
      "Philosophy",
      "Polymer chemistry",
      "Visualization"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Xiaodan"
      },
      {
        "surname": "Xiao",
        "given_name": "Yuan"
      },
      {
        "surname": "Peng",
        "given_name": "Jinye"
      },
      {
        "surname": "Gao",
        "given_name": "Xinbo"
      },
      {
        "surname": "Hu",
        "given_name": "Bo"
      }
    ]
  },
  {
    "title": "3D human pose estimation with single image and inertial measurement unit (IMU) sequence",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110175",
    "abstract": "Three-dimensional human pose estimation plays an important role in the field of computer vision, such as in healthcare, sports, activity recognition, motion capture, and augmented reality. However, monocular image or video based methods are sensitive to occlusions, while multi-view methods usually require enormous computation resources. Currently, inertial measurement unit (IMU)-based methods have begun to overcome the occlusion problem and can potentially achieve real-time inference. Yet, they still suffer from insufficient precision and scale drift error over time. In this paper, we propose a novel, efficient framework to fuse a single image with temporal sequence from IMU sensors to estimate human poses and reconstruct human shapes. Our method achieves 46 mm Mean Per Joint Positional Error (MPJPE) on the Total Capture dataset with 30 frames time segment, and surpasses state-of-the-art pure IMU-based methods. Moreover, in comparison with other vision-based methods, the proposed method shows great advantage in reducing computing floating point operations per second (FLOPS) quota while still achieving competitive estimation precision. Our method achieves 74 FPS on an IPhone 12 for offline processing. In addition, our method can easily be generalized for outdoor cases.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323008725",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer vision",
      "Genetics",
      "Inertial frame of reference",
      "Inertial measurement unit",
      "Physics",
      "Pose",
      "Quantum mechanics",
      "Sequence (biology)",
      "Single camera",
      "Units of measurement"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Liujun"
      },
      {
        "surname": "Yang",
        "given_name": "Jiewen"
      },
      {
        "surname": "Lin",
        "given_name": "Ye"
      },
      {
        "surname": "Zhang",
        "given_name": "Peixuan"
      },
      {
        "surname": "Zhang",
        "given_name": "Lihua"
      }
    ]
  },
  {
    "title": "A contrastive variational graph auto-encoder for node clustering",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110209",
    "abstract": "Variational Graph Auto-Encoders (VGAEs) have been widely used to solve the node clustering task. However, the state-of-the-art methods have numerous challenges. First, existing VGAEs do not account for the discrepancy between the inference and generative models after incorporating the clustering inductive bias. Second, current models are prone to degenerate solutions that make the latent codes match the prior independently of the input signal (i.e., Posterior Collapse). Third, existing VGAEs overlook the effect of the noisy clustering assignments (i.e., Feature Randomness) and the impact of the strong trade-off between clustering and reconstruction (i.e., Feature Drift). To address these problems, we formulate a variational lower bound in a contrastive setting. Our lower bound is a tighter approximation of the log-likelihood function than the corresponding Evidence Lower BOund (ELBO). Thanks to a newly identified term, our lower bound can escape Posterior Collapse and has more flexibility to account for the difference between the inference and generative models. Additionally, our solution has two mechanisms to control the trade-off between Feature Randomness and Feature Drift. Extensive experiments show that the proposed method achieves state-of-the-art clustering results on several datasets. We provide strong evidence that this improvement is attributed to four aspects: integrating contrastive learning and alleviating Feature Randomness, Feature Drift, and Posterior Collapse.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323009068",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Encoder",
      "Feature (linguistics)",
      "Inference",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Randomness",
      "Statistics",
      "Upper and lower bounds"
    ],
    "authors": [
      {
        "surname": "Mrabah",
        "given_name": "Nairouz"
      },
      {
        "surname": "Bouguessa",
        "given_name": "Mohamed"
      },
      {
        "surname": "Ksantini",
        "given_name": "Riadh"
      }
    ]
  },
  {
    "title": "Hypergraph modeling and hypergraph multi-view attention neural network for link prediction",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110292",
    "abstract": "Hypergraph neural networks are widely used in link prediction because of their ability to learn the high-order structure relationship. However, most existing hypergraph modeling relies on the attribute information of nodes. And as for the link prediction, missing links are not utilized when training link predictors, so conventional transductive hypergraph learning are generally not consistent with link prediction tasks. To address these limitations, we propose the Network Structure Linear Representation (NSLR) method to model hypergraph for general networks without node attribute information and the inductive hypergraph learning method Hypergraph Multi-view Attention Neural Network (HMANN) that learns the rich high-order structure information from node-level and hyperedge-level. Also, this paper put forwards a novel NSLR-HMANN link prediction algorithm based on NSLR and HMANN methods. Extensive comparison and ablation experiments show that the NSLR-HMANN link prediction algorithm achieves state-of-the-art performance on link prediction and has better performance on robustness.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324000438",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Computer network",
      "Computer science",
      "Data mining",
      "Discrete mathematics",
      "Engineering",
      "Gene",
      "Hypergraph",
      "Law",
      "Link (geometry)",
      "Machine learning",
      "Mathematics",
      "Node (physics)",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Robustness (evolution)",
      "Structural engineering",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Chai",
        "given_name": "Lang"
      },
      {
        "surname": "Tu",
        "given_name": "Lilan"
      },
      {
        "surname": "Wang",
        "given_name": "Xianjia"
      },
      {
        "surname": "Su",
        "given_name": "Qingqing"
      }
    ]
  },
  {
    "title": "Exploring low-resource medical image classification with weakly supervised prompt learning",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110250",
    "abstract": "Most advances in medical image recognition supporting clinical auxiliary diagnosis meet challenges due to the low-resource situation in the medical field, where annotations are highly expensive and professional. This low-resource problem can be alleviated by leveraging the transferable representations of large-scale pre-trained vision-language models like CLIP. After being pre-trained using large-scale unlabeled medical images and texts (such as medical reports), the vision-language models can learn transferable representations and support flexible downstream clinical tasks such as medical image classification via relevant medical text prompts. However, existing pre-trained vision-language models require domain experts (clinicians) to carefully design the medical text prompts based on different datasets when applied to specific medical image tasks, which is extremely time-consuming and greatly increases the burden on clinicians. To address this problem, we propose a weakly supervised prompt learning method M e d P r o m p t for automatically generating medical prompts, which includes an unsupervised pre-trained vision-language model and a weakly supervised prompt learning model. The unsupervised pre-trained vision-language model adopts large-scale medical images and texts for pre-training, utilizing the natural correlation between medical images and corresponding medical texts without manual annotations. The weakly supervised prompt learning model only utilizes the classes of images in the dataset to guide the learning of the specific class vector in the prompt, while the learning of other context vectors in the prompt does not require any manual annotations for guidance. To the best of our knowledge, this is the first model to automatically generate medical prompts. With the assistance of these prompts, the pre-trained vision-language model can be freed from the strong expert dependency of manual annotation and manual prompt design, thus achieving end-to-end, low-cost medical image classification. Experimental results show that the model using our automatically generated prompts outperforms all its hand-crafted prompts counterparts in full-shot learning on all four datasets, and achieves superior accuracy on zero-shot image classification and few-shot learning in three of the four medical benchmark datasets and comparable accuracy in the remaining one. In addition, the proposed prompt generator is lightweight and therefore has the potential to be embedded into any network architecture.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324000013",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer network",
      "Computer science",
      "Context (archaeology)",
      "Deep learning",
      "Field (mathematics)",
      "Machine learning",
      "Mathematics",
      "Natural language processing",
      "Paleontology",
      "Physics",
      "Pure mathematics",
      "Quantum mechanics",
      "Resource (disambiguation)",
      "Scale (ratio)",
      "Supervised learning",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Zheng",
        "given_name": "Fudan"
      },
      {
        "surname": "Cao",
        "given_name": "Jindong"
      },
      {
        "surname": "Yu",
        "given_name": "Weijiang"
      },
      {
        "surname": "Chen",
        "given_name": "Zhiguang"
      },
      {
        "surname": "Xiao",
        "given_name": "Nong"
      },
      {
        "surname": "Lu",
        "given_name": "Yutong"
      }
    ]
  },
  {
    "title": "Dual residual attention network for image denoising",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110291",
    "abstract": "In image denoising, deep convolutional neural networks (CNNs) can obtain favorable performance on removing spatially invariant noise. However, many of these networks cannot perform well on removing the real noise (i.e. spatially variant noise) that is generated during image acquisition or transmission, which severely impedes their application in practical image denoising tasks. In this paper, we propose a novel Dual-branch Residual Attention Network (DRANet) for image denoising, which has both the merits of a wide model architecture and the attention-guided feature learning. The proposed DRANet includes two different parallel branches, which can capture complementary features to enhance the learning ability of the model. We designed a new residual attention block (RAB) and a novel hybrid dilated residual attention block (HDRAB) for the upper and lower branches, respectively. The RAB and HDRAB can capture rich local features through multiple skip connections between different convolutional layers, and the unimportant features can be dropped. Meanwhile, the long skip connections in each branch and the global feature fusion between the two parallel branches can effectively capture the global features as well. Extensive experiments demonstrate that compared with other state-of-the-art denoising methods, our DRANet can produce competitive denoising performance both on the synthetic and real-world noise removal. The code for DRANet is accessible at https://github.com/WenCongWu/DRANet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324000426",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Feature (linguistics)",
      "Geometry",
      "Image (mathematics)",
      "Linguistics",
      "Mathematics",
      "Noise (video)",
      "Noise reduction",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Residual"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Wencong"
      },
      {
        "surname": "Liu",
        "given_name": "Shijie"
      },
      {
        "surname": "Xia",
        "given_name": "Yuelong"
      },
      {
        "surname": "Zhang",
        "given_name": "Yungang"
      }
    ]
  },
  {
    "title": "A Max-Relevance-Min-Divergence criterion for data discretization with applications on naive Bayes",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110236",
    "abstract": "In many classification models, data is discretized to better estimate its distribution. Existing discretization methods often target at maximizing the discriminant power of discretized data, while overlooking the fact that the primary target of data discretization in classification is to improve the generalization performance. As a result, the data tend to be over-split into many small bins since the data without discretization retain the maximal discriminant information. Thus, we propose a Max-Dependency-Min-Divergence (MDmD) criterion that maximizes both the discriminant information and generalization ability of the discretized data. More specifically, the Max-Dependency criterion maximizes the statistical dependency between the discretized data and the classification variable while the Min-Divergence criterion explicitly minimizes the JS-divergence between the training data and the validation data for a given discretization scheme. The proposed MDmD criterion is technically appealing, but it is difficult to reliably estimate the high-order joint distributions of attributes and the classification variable. We hence further propose a more practical solution, Max-Relevance-Min-Divergence (MRmD) discretization scheme, where each attribute is discretized separately, by simultaneously maximizing the discriminant information and the generalization ability of the discretized data. The proposed MRmD is compared with the state-of-the-art discretization algorithms under the naive Bayes classification framework on 45 benchmark datasets. It significantly outperforms all the compared methods on most of the datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323009330",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Dependency (UML)",
      "Discretization",
      "Discretization error",
      "Discretization of continuous features",
      "Divergence (linguistics)",
      "Generalization",
      "Linear discriminant analysis",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Naive Bayes classifier",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Shihe"
      },
      {
        "surname": "Ren",
        "given_name": "Jianfeng"
      },
      {
        "surname": "Bai",
        "given_name": "Ruibin"
      },
      {
        "surname": "Yao",
        "given_name": "Yuan"
      },
      {
        "surname": "Jiang",
        "given_name": "Xudong"
      }
    ]
  },
  {
    "title": "A tree-based model with branch parallel decoding for handwritten mathematical expression recognition",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110220",
    "abstract": "Handwritten mathematical expression recognition (HMER) is a challenging task in the field of computer vision due to the complex two-dimensional spatial structure and diverse handwriting styles of mathematical expressions (MEs). Recent mainstream approach treats MEs as objects with tree structures, modeled by sequence decoders or tree decoders. These decoders recognize the symbols and relationships between symbols in MEs in depth-first order, resulting in long decoding steps that can harm their performance, particularly for MEs with complex structures. In this paper, we propose a novel tree-based model with branch parallel decoding for HMER, which parses the structures of ME trees by explicitly predicting the relationships between symbols. In addition, a query constructing module is proposed to assist the decoder in decoding the branches of ME trees in parallel, thus reducing the number of decoding time steps and alleviating the problem of long sequence attention decoding. As a result, our model outperforms existing models on three widely-used benchmarks and demonstrates significant improvements in HMER performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323009172",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Binary tree",
      "Biology",
      "Block code",
      "Computer science",
      "Decoding methods",
      "Expression (computer science)",
      "Field (mathematics)",
      "Genetics",
      "Handwriting",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Programming language",
      "Pure mathematics",
      "Sequence (biology)",
      "Sequential decoding",
      "Tree (set theory)",
      "Tree structure"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Zhe"
      },
      {
        "surname": "Yang",
        "given_name": "Wentao"
      },
      {
        "surname": "Qi",
        "given_name": "Hengnian"
      },
      {
        "surname": "Jin",
        "given_name": "Lianwen"
      },
      {
        "surname": "Huang",
        "given_name": "Yichao"
      },
      {
        "surname": "Ding",
        "given_name": "Kai"
      }
    ]
  },
  {
    "title": "Single image super-resolution based on trainable feature matching attention network",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110289",
    "abstract": "Convolutional Neural Networks (CNNs) have been widely employed for image Super-Resolution (SR) in recent years. Various techniques enhance SR performance by altering CNN structures or incorporating improved self-attention mechanisms. Interestingly, these advancements share a common trait. Instead of explicitly learning high-frequency details, they learn an implicit feature processing mode that utilizes weighted sums of a feature map’s own elements for reconstruction, akin to convolution and non-local. In contrast, early dictionary-based approaches learn feature decompositions explicitly to match and rebuild Low-Resolution (LR) features. Building on this analysis, we introduce Trainable Feature Matching (TFM) to amalgamate this explicit feature learning into CNNs, augmenting their representation capabilities. Within TFM, trainable feature sets are integrated to explicitly learn features from training images through feature matching. Furthermore, we integrate non-local and channel attention into our proposed Trainable Feature Matching Attention Network (TFMAN) to further enhance SR performance. To alleviate the computational demands of non-local operations, we propose a streamlined variant called Same-size-divided Region-level Non-Local (SRNL). SRNL conducts non-local computations in parallel on blocks uniformly divided from the input feature map. The efficacy of TFM and SRNL is validated through ablation studies and module explorations. We employ a recurrent convolutional network as the backbone of our TFMAN to optimize parameter utilization. Comprehensive experiments on benchmark datasets demonstrate that TFMAN achieves superior results in most comparisons while using fewer parameters. The code is available at https://github.com/qizhou000/tfman.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324000402",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Feature matching",
      "Image (mathematics)",
      "Linguistics",
      "Matching (statistics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Resolution (logic)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Qizhou"
      },
      {
        "surname": "Shao",
        "given_name": "Qing"
      }
    ]
  },
  {
    "title": "Gradient aggregation based fine-grained image retrieval: A unified viewpoint for CNN and Transformer",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110248",
    "abstract": "The gradients of CNN are traditionally utilized for optimization and visualization. In this paper, we find that a discriminative representation hides in the gradients of convolution filters. Based on this, we propose a corresponding feature extraction and aggregation method for fine-grained image retrieval (FGIR). Firstly, we propose a metric to evaluate manually-designed loss functions and design a loss function originating from Grad-CAM in the testing phase based on it to extract the gradients of the convolution filters. Secondly, we take the gradients as the new features and design a succinct approach to aggregate them into a compact vector, which is named as Convolution Filters Gradient Aggregation (CFGA) feature. CFGA features can be extracted from pre-trained and fine-tuned CNN models. Extensive experiments are conducted on FGIR to verify the effectiveness of our proposed CFGA approach, compared with five supervised state-of-the-art methods and two unsupervised methods on two standard fine-grained retrieval datasets. Moreover, we generalize the CFGA method designed for CNN to Swin Transformer, and propose the Transformer parameter gradients aggregation (TPGA) method, which proves the applicability of the core idea of CFGA/TPGA to mainstream feature extraction models. We achieve state-of-the-art FGIR performance on CUB-200-2011 dataset and CARS196 dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323009457",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Discriminative model",
      "Economics",
      "Feature extraction",
      "Feature learning",
      "Feature vector",
      "Granularity",
      "Image (mathematics)",
      "Image retrieval",
      "Metric (unit)",
      "Operating system",
      "Operations management",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Yu",
        "given_name": "Han"
      },
      {
        "surname": "Lu",
        "given_name": "Huibin"
      },
      {
        "surname": "Zhao",
        "given_name": "Min"
      },
      {
        "surname": "Li",
        "given_name": "Zhuoyi"
      },
      {
        "surname": "Gu",
        "given_name": "Guanghua"
      }
    ]
  },
  {
    "title": "Class-Aware Mask-guided feature refinement for scene text recognition",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110244",
    "abstract": "Scene text recognition is a rapidly developing field that faces numerous challenges due to the complexity and diversity of scene text, including complex backgrounds, diverse fonts, flexible arrangements, and accidental occlusions. In this paper, we propose a novel approach called Class-Aware Mask-guided feature refinement (CAM) to address these challenges. Our approach introduces canonical class-aware glyph masks generated from a standard font to effectively suppress background and text style noise, thereby enhancing feature discrimination. Additionally, we design a feature alignment and fusion module to incorporate the canonical mask guidance for further feature refinement for text recognition. By enhancing the alignment between the canonical mask feature and the text feature, the module ensures more effective fusion, ultimately leading to improved recognition performance. We first evaluate CAM on six standard text recognition benchmarks to demonstrate its effectiveness. Furthermore, CAM exhibits superiority over the state-of-the-art method by an average performance gain of 4.1% across six more challenging datasets, despite utilizing a smaller model size. Our study highlights the importance of incorporating canonical mask guidance and aligned feature refinement techniques for robust scene text recognition. Code will be available at https://github.com/MelosY/CAM.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032300941X",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Code (set theory)",
      "Computer science",
      "Feature (linguistics)",
      "Feature extraction",
      "Field (mathematics)",
      "Glyph (data visualization)",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Pure mathematics",
      "Set (abstract data type)",
      "Visualization"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Mingkun"
      },
      {
        "surname": "Yang",
        "given_name": "Biao"
      },
      {
        "surname": "Liao",
        "given_name": "Minghui"
      },
      {
        "surname": "Zhu",
        "given_name": "Yingying"
      },
      {
        "surname": "Bai",
        "given_name": "Xiang"
      }
    ]
  },
  {
    "title": "YOLO2U-Net: Detection-guided 3D instance segmentation for microscopy",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.03.015",
    "abstract": "Microscopy imaging techniques are instrumental for characterization and analysis of biological structures. As these techniques typically render 3D visualization of cells by stacking 2D projections, issues such as out-of-plane excitation and low resolution in the z -axis may pose challenges (even for human experts) to detect individual cells in 3D volumes as these non-overlapping cells may appear as overlapping. A comprehensive method for accurate 3D instance segmentation of cells in the brain tissue is introduced here. The proposed method combines the 2D YOLO detection method with a multi-view fusion algorithm to construct a 3D localization of the cells. Next, the 3D bounding boxes along with the data volume are input to a 3D U-Net network that is designed to segment the primary cell in each 3D bounding box, and in turn, to carry out instance segmentation of cells in the entire volume. The promising performance of the proposed method is shown in comparison with current deep learning-based 3D instance segmentation methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524000837",
    "keywords": [
      "3d model",
      "Artificial intelligence",
      "Biology",
      "Bounding overwatch",
      "Computer science",
      "Computer vision",
      "Connectome",
      "Connectomics",
      "Deep learning",
      "Functional connectivity",
      "Image (mathematics)",
      "Image segmentation",
      "Minimum bounding box",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Segmentation",
      "Visualization",
      "Volume (thermodynamics)"
    ],
    "authors": [
      {
        "surname": "Ziabari",
        "given_name": "Amirkoushyar"
      },
      {
        "surname": "Rose",
        "given_name": "Derek C."
      },
      {
        "surname": "Shirinifard",
        "given_name": "Abbas"
      },
      {
        "surname": "Solecki",
        "given_name": "David"
      }
    ]
  },
  {
    "title": "Multi-label contrastive hashing",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110239",
    "abstract": "Joint learning of image representation and hash encoding represents a dominant solution to approximate nearest neighbor search for large-scale image retrieval. Despite significant advances in deep learning to hash in multi-label setting, optimization of semantic similarity-preserving representations with minimal quantization error remains challenging. Motivated by the recent success of contrastive representation learning in various vision tasks, this article introduces a Multi-label Contrastive Hashing (MCH) method for large-scale multi-label image retrieval. We extend the image similarity modeling in existing supervised contrastive loss from binary to multi-level structure, by which multi-level semantic similarity between multi-label images can be well modeled and captured in learning to hash. Despite the appealing properties of contrastive learning, directly adapting it into multi-label hashing is non-trivial, as the quantization loss may restricts the optimization of the multi-level contrastive loss, degrading the multi-level similarity-preserving hashing. To this end, we design a curriculum strategy to adaptively adjust the weight of quantization loss by leveraging the historical quantization deviations during training, such that the multi-level semantic similarity can be well preserved with progressively reduced quantization deviation. We conduct extensive experiments on three benchmark datasets including MirFlickr25k, NUS-WIDE, and IAPRTC-12. The results indicate the effectiveness of our approach, outperforming several state-of-the-art solutions for hashing-based multi-label image retrieval.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323009366",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Double hashing",
      "Feature hashing",
      "Hash function",
      "Hash table",
      "Image (mathematics)",
      "Image retrieval",
      "Locality-sensitive hashing",
      "Machine learning",
      "Nearest neighbor search",
      "Pattern recognition (psychology)",
      "Quantization (signal processing)",
      "Semantic similarity",
      "Similarity (geometry)",
      "Universal hashing"
    ],
    "authors": [
      {
        "surname": "Wei",
        "given_name": "Zeqiang"
      },
      {
        "surname": "Jin",
        "given_name": "Kai"
      },
      {
        "surname": "Zhang",
        "given_name": "Zheng"
      },
      {
        "surname": "Zhou",
        "given_name": "Xiuzhuang"
      }
    ]
  },
  {
    "title": "On the representation and methodology for wide and short range head pose estimation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110263",
    "abstract": "Head pose estimation (HPE) is a problem of interest in computer vision to improve the performance of face processing tasks in semi-frontal or profile settings. Recent applications require the analysis of faces in the full 360° rotation range. Traditional approaches to solve the semi-frontal and profile cases are not directly amenable for the full rotation case. In this paper we analyze the methodology for short- and wide-range HPE and discuss which representations and metrics are adequate for each case. We show that the popular Euler angles representation is a good choice for short-range HPE, but not at extreme rotations. However, the Euler angles’ gimbal lock problem prevents them from being used as a valid metric in any setting. We also revisit the current cross-data set evaluation methodology and note that the lack of alignment between the reference systems of the training and test data sets negatively biases the results of all articles in the literature. We introduce a procedure to quantify this misalignment and a new methodology for cross-data set HPE that establishes new, more accurate, SOTA for the 300W-LP/Biwi benchmark. We also propose a generalization of the geodesic angular distance metric that enables the construction of a loss that controls the contribution of each training sample to the optimization of the model. Finally, we introduce a wide range HPE benchmark based on the CMU Panoptic data set. code:https://github.com/pcr-upm/opal23_headpose",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324000141",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Composite material",
      "Computer science",
      "Computer vision",
      "Data set",
      "Economics",
      "Euler angles",
      "Generalization",
      "Geodesic",
      "Geodesy",
      "Geography",
      "Geometry",
      "Law",
      "Materials science",
      "Mathematical analysis",
      "Mathematics",
      "Metric (unit)",
      "Operations management",
      "Political science",
      "Politics",
      "Pose",
      "Programming language",
      "Range (aeronautics)",
      "Representation (politics)",
      "Rotation (mathematics)",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Cobo",
        "given_name": "Alejandro"
      },
      {
        "surname": "Valle",
        "given_name": "Roberto"
      },
      {
        "surname": "Buenaposada",
        "given_name": "José M."
      },
      {
        "surname": "Baumela",
        "given_name": "Luis"
      }
    ]
  },
  {
    "title": "GaFL: Geometric-aware Feature Learning for universal 3D models recognition",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110214",
    "abstract": "Existing methods of 3D model recognition mainly depend on deep learning algorithms. However, the extracted deep feature lacks the geometric information of the models. Thus, these methods fail to identify rigid and non-rigid 3D models simultaneously, i.e., universal 3D models recognition. In this work, we propose a novel method, Geometric-aware Feature Learning (GaFL), to further investigate the combination mechanism of geometric feature and deep learning in universal 3D models recognition. In GaFL, we design the Layer-Projected and Ray-Projected feature extraction policies to obtain depth values, which contain rich geometric information. Furthermore, the sphere convolution is proposed to guarantee the continuity and integrity of the ray feature when feeding into a deep network and the feature inactivation fusion module is designed to achieve the complementarity between layer and ray feature. Finally, the final merged feature vector contains enough geometric information as well as high-level semantic information, which are critical to universal 3D models recognition. In the experiments, GaFL achieves 95.0% and 95.2% classification accuracy in the rigid 3D models dataset ModelNet40 and the non-rigid 3D models dataset SHREC16, respectively, indicating that GaFL is powerful in universal 3D models recognition. Moreover, its significant advantage over state-of-the-art methods has also been validated on three other datasets, i.e., ShapeNet Core55, ScanObjectNN and SHREC15.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323009111",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Convolution (computer science)",
      "Deep learning",
      "Feature (linguistics)",
      "Feature extraction",
      "Feature learning",
      "Geometric modeling",
      "Geometry",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Yan"
      },
      {
        "surname": "Sun",
        "given_name": "Huajie"
      },
      {
        "surname": "Zhang",
        "given_name": "Huaidong"
      },
      {
        "surname": "Xu",
        "given_name": "Xuemiao"
      },
      {
        "surname": "Yi",
        "given_name": "Chang’an"
      },
      {
        "surname": "Ye",
        "given_name": "Dewang"
      },
      {
        "surname": "Zhou",
        "given_name": "Yuexia"
      },
      {
        "surname": "Liu",
        "given_name": "Xiangyu"
      }
    ]
  },
  {
    "title": "Multi-modal brain tumor segmentation via disentangled representation learning and region-aware contrastive learning",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110282",
    "abstract": "Brain tumors are threatening the life and health of people in the world. Automatic brain tumor segmentation using multiple MR images is challenging in medical image analysis. It is known that accurate segmentation relies on effective feature learning. Existing methods address the multi-modal MR brain tumor segmentation by explicitly learning a shared feature representation. However, these methods fail to capture the relationship between MR modalities and the feature correlation between different target tumor regions. In this paper, I propose a multi-modal brain tumor segmentation network via disentangled representation learning and region-aware contrastive learning. Specifically, a feature fusion module is first designed to learn the valuable multi-modal feature representation. Subsequently, a novel disentangled representation learning is proposed to decouple the fused feature representation into multiple factors corresponding to the target tumor regions. Furthermore, contrastive learning is presented to help the network extract tumor region-related feature representations. Finally, the segmentation results are obtained using the segmentation decoders. Quantitative and qualitative experiments conducted on the public datasets, BraTS 2018 and BraTS 2019, justify the importance of the proposed strategies, and the proposed approach can achieve better performance than other state-of-the-art approaches. In addition, the proposed strategies can be extended to other deep neural networks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324000335",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Deep learning",
      "Feature (linguistics)",
      "Feature learning",
      "Image segmentation",
      "Law",
      "Linguistics",
      "Machine learning",
      "Modal",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Polymer chemistry",
      "Representation (politics)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Tongxue"
      }
    ]
  },
  {
    "title": "Towards fair and personalized federated recommendation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110234",
    "abstract": "Recommender systems have gained immense popularity in recent years for predicting users’ interests by learning embeddings. The majority of existing recommendation approaches, represented by graph neural network-based recommendation algorithms, rely on centralized storage of user-item graphs for model learning, which raises privacy issues in the process of collecting and sharing user data. Federated recommendation can mitigate privacy concerns by preventing the server from collecting sensitive data from clients while there still exist unfairness and personalization issues. To address these challenges, we propose a novel framework named Fair and Personalized Federated Recommendation (FPFR). On the client-side, the soft attention mechanism is designed to learn the representation of user/item by combining interaction and attribute information, and the filter network is combined to better characterize user preferences. On the server-side, we cluster users into different groups and learn personalized models for each user. Then, we select representative users from each group to participate in the global model parameters update. Finally, the fairness of federated recommendation is implemented by adding the fairness constraint to recommendation loss. We conduct experiments on five real-world recommendation datasets, and the results demonstrate that the proposed FPFR not only balances group fairness and recommendation accuracy but also improves personalization.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323009317",
    "keywords": [
      "Collaborative filtering",
      "Computer science",
      "Information retrieval",
      "Personalization",
      "Popularity",
      "Psychology",
      "Recommender system",
      "Social psychology",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Shanfeng"
      },
      {
        "surname": "Tao",
        "given_name": "Hao"
      },
      {
        "surname": "Li",
        "given_name": "Jianzhao"
      },
      {
        "surname": "Ji",
        "given_name": "Xinyuan"
      },
      {
        "surname": "Gao",
        "given_name": "Yuan"
      },
      {
        "surname": "Gong",
        "given_name": "Maoguo"
      }
    ]
  },
  {
    "title": "Paired relation feature network for spatial relation recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.03.019",
    "abstract": "Recognizing relations between objects in an image is challenging for neural networks because some relations may not have obvious dedicated visual features. This paper proposes a Paired Relation Feature Network (PRFN), where all spatial and semantic features are extracted from the subject–object pair jointly, without using any hand-crafted features. PRFN includes a paired 2D spatial feature module that can learn the representative features from a pair of bounding boxes. By focusing on the paired depth feature between the subject and object, the problem of depth feature extraction is simplified to the recognition of a ternary relation {−1, 0, 1}, which is much easier to learn from training data. Experimental results demonstrate the effectiveness of PRFN for both the cases of RGB-D images and RGB images with estimated depth.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524000874",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Bounding overwatch",
      "Cognitive neuroscience of visual object recognition",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Feature (linguistics)",
      "Feature extraction",
      "Image (mathematics)",
      "Linguistics",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "RGB color model",
      "Relation (database)",
      "Spatial relation"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Nanxi"
      },
      {
        "surname": "Wang",
        "given_name": "Xu"
      },
      {
        "surname": "Sun",
        "given_name": "Qi"
      },
      {
        "surname": "Li",
        "given_name": "Jiamao"
      },
      {
        "surname": "Zhang",
        "given_name": "Xiaolin"
      }
    ]
  },
  {
    "title": "Multimodal prediction of student performance: A fusion of signed graph neural networks and large language models",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.03.007",
    "abstract": "In online education platforms, accurately predicting student performance is essential for timely dropout prevention and interventions for at-risk students. This task is made difficult by the prevalent use of Multiple-Choice Questions (MCQs) in learnersourcing platforms, where noise in student-generated content and the limitations of existing unsigned graph-based models, specifically their inability to distinguish the semantic meaning between correct and incorrect responses, hinder accurate performance predictions. To address these issues, we introduce the Large Language Model enhanced Signed Bipartite graph Contrastive Learning (LLM-SBCL) model—a novel Multimodal Model utilizing Signed Graph Neural Networks (SGNNs) and a Large Language Model (LLM). Our model uses a signed bipartite graph to represent students’ answers, with positive and negative edges denoting correct and incorrect responses, respectively. To mitigate noise impact, we apply contrastive learning to the signed graphs, combined with knowledge point embeddings from the LLM to further enhance our model’s predictive performance. Upon evaluating our model on five real-world datasets, it demonstrates superior accuracy and stability, exhibiting an average F1 improvement of 3.7% over the best baseline models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524000758",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Fusion",
      "Graph",
      "Linguistics",
      "Machine learning",
      "Natural language processing",
      "Philosophy",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Sijie"
      },
      {
        "surname": "Ni",
        "given_name": "Lin"
      },
      {
        "surname": "Zhang",
        "given_name": "Zeyu"
      },
      {
        "surname": "Li",
        "given_name": "Xiaoxuan"
      },
      {
        "surname": "Zheng",
        "given_name": "Xianda"
      },
      {
        "surname": "Liu",
        "given_name": "Jiamou"
      }
    ]
  },
  {
    "title": "Uncovering the authorship: Linking media content to social user profiles",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.03.008",
    "abstract": "The extensive spread of fake news on social networks is carried out by a diverse range of users, encompassing private individuals, newspapers, and organizations. With widely accessible image and video editing tools, malicious users can easily create manipulated media. They can then distribute this content through multiple fake profiles, aiming to maximize its social impact. To tackle this problem effectively, it is crucial to possess the ability to analyze shared media to identify the originators of fake news. To this end, multimedia forensics research has advanced tools that examine traces in media, revealing valuable insights into its origins. While combining these tools has proven to be highly efficient in creating profiles of image and video creators, it is important to note that most of these tools are not specifically designed to function effectively in the complex environment of content exchange on social networks. In this paper, we introduce the problem of establishing associations between images and their source profiles as a means to tackle the spread of disinformation on social platforms. To this end, we assembled SocialNews, an extensive image dataset comprising more than 12,000 images sourced from 21 user profiles across Facebook, Instagram, and Twitter, and we propose three increasingly realistic and challenging experimental scenarios. We present two simple yet effective techniques as benchmarks, one based on statistical analysis of Discrete Cosine Transform (DCT) coefficients and one employing a neural network model based on ResNet, and we compare their performance against the state of the art. Experimental results show that the proposed approaches exhibit superior performance in accurately classifying the originating user profiles.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552400076X",
    "keywords": [
      "Computer science",
      "Content (measure theory)",
      "Information retrieval",
      "Mathematical analysis",
      "Mathematics",
      "Social media",
      "User-generated content",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Baracchi",
        "given_name": "Daniele"
      },
      {
        "surname": "Shullani",
        "given_name": "Dasara"
      },
      {
        "surname": "Iuliani",
        "given_name": "Massimo"
      },
      {
        "surname": "Giani",
        "given_name": "Damiano"
      },
      {
        "surname": "Piva",
        "given_name": "Alessandro"
      }
    ]
  },
  {
    "title": "EgoCap and EgoFormer: First-person image captioning with context fusion",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.03.012",
    "abstract": "First-person captioning is significant because it provides veracious descriptions of egocentric scenes in a unique perspective. Also, there is a need to caption the scene, a.k.a. life-logging, for patients, travellers, and emergency responders in an egocentric narrative. Ego-captioning is indeed non-trivial since (1) Ego-images can be noisy due to motion and angles; (2) Describing a scene in a first-person narrative involves drastically different semantics; (3) Empirical implications have to be made on top of visual appearance because the cameraperson is often outside the field of view. We note we humans make good sense out of casual footage thanks to our contextual awareness in judging when and where the event unfolds, and whom the cameraperson is interacting with. This inspires the infusion of such “contexts” for situation-aware captioning. We create EgoCap which contains 2.1K ego-images, over 10K ego-captions, and 6.3K contextual labels, to close the gap of lacking ego-captioning datasets. We propose EgoFormer, a dual-encoder transformer-based network which fuses both contextual and visual features. The context encoder is pre-trained on ImageNet before fine tuning with context classification tasks. Similar to visual attention, we exploit stacked multi-head attention layers in the captioning decoder to reinforce attention to the context features. The EgoFormer has realized state-of-the-art performance on EgoCap achieving a CIDEr score of 125.52. The EgoCap dataset and EgoFormer are publicly available at https://github.com/zdai257/EgoCap-EgoFormer.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524000801",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Closed captioning",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Fusion",
      "Geography",
      "Image (mathematics)",
      "Image fusion",
      "Linguistics",
      "Natural language processing",
      "Philosophy",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Dai",
        "given_name": "Zhuangzhuang"
      },
      {
        "surname": "Tran",
        "given_name": "Vu"
      },
      {
        "surname": "Markham",
        "given_name": "Andrew"
      },
      {
        "surname": "Trigoni",
        "given_name": "Niki"
      },
      {
        "surname": "Rahman",
        "given_name": "M. Arif"
      },
      {
        "surname": "Wijayasingha",
        "given_name": "L.N.S."
      },
      {
        "surname": "Stankovic",
        "given_name": "John"
      },
      {
        "surname": "Li",
        "given_name": "Chen"
      }
    ]
  },
  {
    "title": "DA-Tran: Multiphase liver tumor segmentation with a domain-adaptive transformer network",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110233",
    "abstract": "Accurate liver tumor segmentation from multiphase CT images is a prerequisite for data-driven tumor analysis. This study presents a domain-adaptive transformer (DA-Tran) network to segment liver tumors from each CT phase. First, a DA module is designed to produce domain-adapted feature maps from noncontrast-enhanced (NC)-phase, arterial (ART)-phase, portal venous (PV)-phase, and delay-phase (DP) images. Then, these domain-adapted feature maps are integrated using 3D transformer blocks to catch patch-structured similarity information and global context attention. Finally, the attention fusion decoder (AFD) integrates features from different branches to generate a more refined prediction. Extensive experimental results demonstrate that DA-Tran achieves state-of-the-art tumor segmentation results, i.e., a Dice similarity coefficient (DSC) of 87.00% and a 95% Hausdorff distance (HD95) of 5.10 mm on a clinical dataset (DB1). Additionally, DA-Tran consistently outperforms other cutting-edge methods on another multiphase liver tumor dataset (DB2). The DA module and transformer blocks can boost the co-segmentation performance and make DA-Tran an ideal solution for multiphase liver tumor segmentation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323009305",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Segmentation",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Ni",
        "given_name": "Yangfan"
      },
      {
        "surname": "Chen",
        "given_name": "Geng"
      },
      {
        "surname": "Feng",
        "given_name": "Zhan"
      },
      {
        "surname": "Cui",
        "given_name": "Heng"
      },
      {
        "surname": "Metaxas",
        "given_name": "Dimitris"
      },
      {
        "surname": "Zhang",
        "given_name": "Shaoting"
      },
      {
        "surname": "Zhu",
        "given_name": "Wentao"
      }
    ]
  },
  {
    "title": "Latent Linear Discriminant Analysis for feature extraction via Isometric Structural Learning",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110218",
    "abstract": "Linear discriminant analysis (LDA) is one of the most successful feature extraction methods, which projects high-dimensional data to a low-dimensional space with discriminative features. However, there are problems in the existing LDAs: (1) the effect of hidden data is not exploited in LDA, (2) the LDAs cannot preserve the local isometric structure, (3) there is no consideration for structural consistency that unifies the supervised global and unsupervised local information. In this paper, we propose a brand-new LDA method, namely, Latent Linear Discriminant Analysis with Isometric Structural Learning ( L 2 DA-ISL). We formulate LDA to a latent representation framework that considers both the discriminability from observed data and hidden data. Then, we propose isometric structural learning to capture the intrinsic local structural information. Lastly, we establish the concept of structural consistency in LDA framework. Extensive experiments and comparisons show that L 2 DA-ISL achieves a promising performance with structural consistency and stronger robustness in feature extraction.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323009159",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Consistency (knowledge bases)",
      "Discriminant",
      "Discriminative model",
      "Feature extraction",
      "Feature learning",
      "Gene",
      "Isometric exercise",
      "Linear discriminant analysis",
      "Machine learning",
      "Mathematics",
      "Medicine",
      "Pattern recognition (psychology)",
      "Physical therapy",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Jianhang"
      },
      {
        "surname": "Zhang",
        "given_name": "Qi"
      },
      {
        "surname": "Zeng",
        "given_name": "Shaoning"
      },
      {
        "surname": "Zhang",
        "given_name": "Bob"
      },
      {
        "surname": "Fang",
        "given_name": "Leyuan"
      }
    ]
  },
  {
    "title": "Pan-sharpening via intrinsic decomposition knowledge distillation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110247",
    "abstract": "Existing deep-learning-based pan-sharpening strategies mainly involve the fusion of panchromatic and multispectral (MS) information at both the pixel and feature levels. In this paper, we hypothesize that the MS image can be expressed as the multiplication of reflectance and illumination components, and that the reflection components of low-resolution (LR) MS and high-resolution (HR) MS images are invariant. Here, the spectral reflection component can effectively describe the spectral response of an object, while the illumination component can effectively describe its texture. Based on this hypothesis, we propose a novel and concise pan-sharpening framework called intrinsic decomposition knowledge distillation. Specifically, the teacher network decomposes the HR MS image into reflectance and illumination components, which are then combined in the student network with the reflectance component and the enhanced illumination component from LR MS to reconstruct the pan-sharpened image. To approximate the component distributions from the teacher network, we introduce a novel three-stage knowledge distillation strategy that can transfer knowledge about the relationships between components and constrain the student network. Our quantitative and qualitative comparisons demonstrate the reasonableness of our hypothesis and the effectiveness of our proposed method in significantly improving perception quality.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323009445",
    "keywords": [
      "Artificial intelligence",
      "Component (thermodynamics)",
      "Computer science",
      "Computer vision",
      "Multispectral image",
      "Panchromatic film",
      "Pattern recognition (psychology)",
      "Physics",
      "Sharpening",
      "Thermodynamics"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Jiaming"
      },
      {
        "surname": "Zhou",
        "given_name": "Qiang"
      },
      {
        "surname": "Huang",
        "given_name": "Xiao"
      },
      {
        "surname": "Zhang",
        "given_name": "Ruiqian"
      },
      {
        "surname": "Chen",
        "given_name": "Xitong"
      },
      {
        "surname": "Lu",
        "given_name": "Tao"
      }
    ]
  },
  {
    "title": "Promote knowledge mining towards open-world semi-supervised learning",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110259",
    "abstract": "Deep learning models often rely on a large number of labeled data to achieve good performance. However, labeling such a large number of data requires exhaustive labor efforts. In recent years, a pivotal research direction is to generalize deep learning models to learn from not only unlabeled data of seen classes but also data of novel classes which are not predefined, known as open-world semi-supervised learning (open-world SSL). Existing works tackled this challenging task by manually designing different optimizations for labeled/unlabeled data and seen/novel classes. In this paper, we propose a simple unified framework that can be applied to all images and all classes in the same form. In this framework, we exploit the Sinkhorn–Knopp algorithm to overcome the overconfidence issue of pseudo labels on seen classes and thus lead to a more balanced distribution of seen and novel classes. To reduce the intra-class variance and avoid model collapse, we take as input two different views of an image and regard one’s prediction as the other’s pseudo label. However, in a unified framework, the model converges much faster on the seen classes than those novel classes. To balance them and encourage knowledge transfer from seen classes to novel classes, we further propose mixing up any two training images during our unified optimization. Extensive experiments on three benchmarks (i.e., CIFAR-10, CIFAR-100, and ImageNet-100) show that our unified framework achieved comparable performance with existing state-of-the-art methods. Our code is available on https://github.com/happytianhao/OWSSL.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324000104",
    "keywords": [
      "Accounting",
      "Artificial intelligence",
      "Business",
      "Class (philosophy)",
      "Code (set theory)",
      "Computer science",
      "Computer security",
      "Economics",
      "Exploit",
      "Labeled data",
      "Machine learning",
      "Management",
      "Programming language",
      "Set (abstract data type)",
      "Task (project management)",
      "Transfer of learning",
      "Variance (accounting)"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Tianhao"
      },
      {
        "surname": "Lin",
        "given_name": "Yutian"
      },
      {
        "surname": "Wu",
        "given_name": "Yu"
      },
      {
        "surname": "Du",
        "given_name": "Bo"
      }
    ]
  },
  {
    "title": "SEMv2: Table separation line detection based on instance segmentation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110279",
    "abstract": "Table structure recognition is an indispensable element for enabling machines to comprehend tables. Its primary purpose is to identify the internal structure of a table. Nevertheless, due to the complexity and diversity of their structure and style, it is highly challenging to parse the tabular data into a structured format that machines can comprehend. In this work, we adhere to the principle of the split-and-merge based methods and propose an accurate table structure recognizer, termed SEMv2 (SEM: Split, Embed and Merge). Unlike the previous works in the “split” stage, we aim to address the table separation line instance-level discrimination problem and introduce a table separation line detection strategy based on conditional convolution. Specifically, we design the “split” in a top-down manner that detects the table separation line instance first and then dynamically predicts the table separation line mask for each instance. The final table separation line shape can be accurately obtained by processing the table separation line mask in a row-wise/column-wise manner. To comprehensively evaluate the SEMv2, we also present a more challenging dataset for table structure recognition, dubbed iFLYTAB, which encompasses multiple style tables in various scenarios such as photos, scanned documents, etc. Extensive experiments on publicly available datasets (e.g. SciTSR, PubTabNet and iFLYTAB) demonstrate the efficacy of our proposed approach. The code and iFLYTAB dataset are available at https://github.com/ZZR8066/SEMv2",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032400030X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Decision table",
      "Information retrieval",
      "Line segment",
      "Merge (version control)",
      "Parsing",
      "Pattern recognition (psychology)",
      "Rough set",
      "Segmentation",
      "Table (database)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Zhenrong"
      },
      {
        "surname": "Hu",
        "given_name": "Pengfei"
      },
      {
        "surname": "Ma",
        "given_name": "Jiefeng"
      },
      {
        "surname": "Du",
        "given_name": "Jun"
      },
      {
        "surname": "Zhang",
        "given_name": "Jianshu"
      },
      {
        "surname": "Yin",
        "given_name": "Baocai"
      },
      {
        "surname": "Yin",
        "given_name": "Bing"
      },
      {
        "surname": "Liu",
        "given_name": "Cong"
      }
    ]
  },
  {
    "title": "Bimodal SegNet: Fused instance segmentation using events and RGB frames",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110215",
    "abstract": "Object segmentation enhances robotic grasping by aiding object identification. Complex environments and dynamic conditions pose challenges such as occlusion, low light conditions, motion blur and object size variance. To address these challenges, we propose a Bimodal SegNet that fuses two types of visual signals, event-based data and RGB frame data. The proposed Bimodal SegNet network has two distinct encoders — one for RGB signal input and another for Event signal input, in addition to an Atrous Pyramidal Feature Amplification module. Encoders capture and fuse the rich contextual information from different resolutions via a Cross-Domain Contextual Attention layer while the decoder obtains sharp object boundaries. The evaluation of the proposed method undertakes five unique image degradation challenges including occlusion, blur, brightness, trajectory and scale variance on the Event-based Segmentation (ESD) Dataset. The results show a 4%–6% MIOU score improvement over state-of-the-art methods in terms of mean intersection over the union and pixel accuracy. The source code, dataset and model are publicly available at: https://github.com/sanket0707/Bimodal-SegNet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323009123",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Encoder",
      "Event (particle physics)",
      "Feature (linguistics)",
      "Frame (networking)",
      "Image (mathematics)",
      "Linguistics",
      "Motion blur",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "RGB color model",
      "Segmentation",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Kachole",
        "given_name": "Sanket"
      },
      {
        "surname": "Huang",
        "given_name": "Xiaoqian"
      },
      {
        "surname": "Naeini",
        "given_name": "Fariborz Baghaei"
      },
      {
        "surname": "Muthusamy",
        "given_name": "Rajkumar"
      },
      {
        "surname": "Makris",
        "given_name": "Dimitrios"
      },
      {
        "surname": "Zweiri",
        "given_name": "Yahya"
      }
    ]
  },
  {
    "title": "An end-to-end model for multi-view scene text recognition",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110206",
    "abstract": "Due to the increasing applications of surveillance and monitoring such as person re-identification, vehicle re-identification and sports events tracking, the necessity of text detection and end-to-end recognition is also growing. Although the past deep learning-based models have addressed several challenges such as arbitrary-shaped text, multiple scripts, and variations in the geometric structure of characters, the scope of the models is limited to a single view. This paper presents an end-to-end model for text recognition through refining the multi-views of the same scene, which is called E2EMVSTR (End-to-End Model for Multi-View Scene Text Recognition). Considering the common characteristics shared in multi-view texts, we propose a cycle consistency pairwise similarity-based deep learning model to find texts more efficiently in three input views. Further, the extracted texts are supplied to a Siamese network and semi-supervised attention embedding combinational network for obtaining recognition results. The proposed model combines natural language processing and genetic algorithm models to restore missing character information and correct wrong recognition results. In experiments on our multi-view dataset and several benchmark datasets, the proposed method is proven effective compared to the state-of-the-art methods. The dataset and codes will be made available to the public upon acceptance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323009032",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Botany",
      "Computer science",
      "Consistency (knowledge bases)",
      "Deep learning",
      "End-to-end principle",
      "Geodesy",
      "Geography",
      "Identification (biology)",
      "Image (mathematics)",
      "Language model",
      "Machine learning",
      "Natural language processing",
      "Operating system",
      "Pairwise comparison",
      "Pattern recognition (psychology)",
      "Scripting language",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Banerjee",
        "given_name": "Ayan"
      },
      {
        "surname": "Shivakumara",
        "given_name": "Palaiahnakote"
      },
      {
        "surname": "Bhattacharya",
        "given_name": "Saumik"
      },
      {
        "surname": "Pal",
        "given_name": "Umapada"
      },
      {
        "surname": "Liu",
        "given_name": "Cheng-Lin"
      }
    ]
  },
  {
    "title": "Gaussian process classification bandits",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110224",
    "abstract": "Classification bandits are multi-armed bandit problems whose task is to classify a given set of arms into either positive or negative class depending on whether the rate of the arms with the expected reward of at least h is not less than w for given thresholds h and w . We study a special classification bandit problem in which arms correspond to points x in d -dimensional real space with expected rewards f ( x ) which are generated according to a Gaussian process prior. We develop a framework algorithm for the problem using various arm selection policies and propose policies called FCB (Farthest Confidence Bound) and FTSV (Farthest Thompson Sampling Value). We show a smaller sample complexity upper bound for FCB than that for the existing algorithm of the level set estimation, in which whether f ( x ) is at least h or not must be decided for every arm’s x . Arm selection policies depending on an estimated rate of arms with mean rewards of at least h are also proposed and shown to improve empirical sample complexity. According to our experimental results, the rate-estimation versions of FCB and FTSV, together with that of the popular active learning policy which selects the point with the maximum variance, outperform other policies for synthetic functions, and the rate-estimation version of FTSV is also the best performer for our real-world dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323009214",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Gaussian",
      "Gaussian process",
      "Machine learning",
      "Operating system",
      "Pattern recognition (psychology)",
      "Physics",
      "Process (computing)",
      "Quantum mechanics"
    ],
    "authors": [
      {
        "surname": "Hayashi",
        "given_name": "Tatsuya"
      },
      {
        "surname": "Ito",
        "given_name": "Naoki"
      },
      {
        "surname": "Tabata",
        "given_name": "Koji"
      },
      {
        "surname": "Nakamura",
        "given_name": "Atsuyoshi"
      },
      {
        "surname": "Fujita",
        "given_name": "Katsumasa"
      },
      {
        "surname": "Harada",
        "given_name": "Yoshinori"
      },
      {
        "surname": "Komatsuzaki",
        "given_name": "Tamiki"
      }
    ]
  },
  {
    "title": "OSPC: Online Sequential Photometric Calibration",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.03.005",
    "abstract": "Photometric calibration is essential to many computer vision applications. One of its key benefits is enhancing the performance of Visual SLAM, especially when it depends on a direct method for tracking, such as the standard KLT algorithm. Additionally, it proves valuable in extracting sensor irradiance values from measured intensities, serving as a pre-processing step for a number of vision algorithms such as shape-from-shading. Current photometric calibration systems rely on a joint optimization problem and encounter an ambiguity in the estimates, which can only be resolved using ground truth information. We propose a novel method that solves for photometric parameters using a sequential estimation approach. To enhance the decoupling of CRF and Vignette estimation, we strategically utilize keyframes with high exposure ratios and small displacements for the former, and keyframes with relatively large displacements for the latter. Our proposed method achieves high accuracy in estimating all parameters; furthermore, the formulations are linear and convex, which makes the solution fast and suitable for online applications. Experiments on a Visual Odometry system validate the proposed method and demonstrate its advantages.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524000734",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Calibration",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Ground truth",
      "Image (mathematics)",
      "Key (lock)",
      "Mathematics",
      "Mobile robot",
      "Odometry",
      "Photometric stereo",
      "Robot",
      "Statistics",
      "Visual odometry"
    ],
    "authors": [
      {
        "surname": "Haidar",
        "given_name": "Jawad"
      },
      {
        "surname": "Khalil",
        "given_name": "Douaa"
      },
      {
        "surname": "Asmar",
        "given_name": "Daniel"
      }
    ]
  },
  {
    "title": "Keep DRÆMing: Discriminative 3D anomaly detection through anomaly simulation",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.03.018",
    "abstract": "Recent surface anomaly detection methods rely on pretrained backbone networks for efficient anomaly detection. On standard RGB anomaly detection benchmarks these methods achieve excellent results but fail on 3D anomaly detection due to a lack of pretrained backbones that suit this domain. Additionally, there is a lack of industrial depth data that would enable the backbone network training that could be used in 3D anomaly detection models. Discriminative anomaly detection methods do not require pretrained networks and are trained using simulated anomalies. The process of simulating anomalies that fit the domain of industrial depth data is not trivial and is necessary for training discriminative methods. We propose a novel 3D anomaly simulation process that follows the natural characteristics of industrial depth data and generates diverse deformations, making it suitable for training discriminative anomaly detection methods. We demonstrate its effectiveness by adapting the DRÆM method to work on 3D anomaly detection, thus obtaining 3DRÆM, a strong discriminative 3D anomaly detection model. The proposed approach achieves excellent results on the MVTec3D anomaly detection benchmark where it achieves state-of-the-art results on both 3D and RGB+3D problem setups, significantly outperforming competing methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524000862",
    "keywords": [
      "Anomaly (physics)",
      "Anomaly detection",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Condensed matter physics",
      "Discriminative model",
      "Geodesy",
      "Geology",
      "Operating system",
      "Pattern recognition (psychology)",
      "Physics",
      "Process (computing)",
      "RGB color model"
    ],
    "authors": [
      {
        "surname": "Zavrtanik",
        "given_name": "Vitjan"
      },
      {
        "surname": "Kristan",
        "given_name": "Matej"
      },
      {
        "surname": "Skočaj",
        "given_name": "Danijel"
      }
    ]
  },
  {
    "title": "Towards robust neural networks via orthogonal diversity",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110281",
    "abstract": "Deep Neural Networks (DNNs) are vulnerable to invisible perturbations on the images generated by adversarial attacks, which raises researches on the adversarial robustness of DNNs. A series of methods represented by the adversarial training and its variants have proven as one of the most effective techniques in enhancing the DNN robustness. Generally, adversarial training focuses on enriching the training data by involving perturbed data. Such data augmentation effect of the involved perturbed data in adversarial training does not contribute to the robustness of DNN itself and usually suffers from clean accuracy drop. Towards the robustness of DNN itself, we in this paper propose a novel defense that aims at augmenting the model in order to learn features that are adaptive to diverse inputs, including adversarial examples. More specifically, to augment the model, multiple paths are embedded into the network, and an orthogonality constraint is imposed on these paths to guarantee the diversity among them. A margin-maximization loss is then designed to further boost such DIversity via Orthogonality (DIO). In this way, the proposed DIO augments the model and enhances the robustness of DNN itself as the learned features can be corrected by these mutually-orthogonal paths. Extensive empirical results on various data sets, structures and attacks verify the stronger adversarial robustness of the proposed DIO utilizing model augmentation. Besides, DIO can also be flexibly combined with different data augmentation techniques (e.g., TRADES and DDPM), further promoting robustness gains.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324000323",
    "keywords": [
      "Adversarial system",
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Deep neural networks",
      "Gene",
      "Geometry",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Maximization",
      "Orthogonality",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Fang",
        "given_name": "Kun"
      },
      {
        "surname": "Tao",
        "given_name": "Qinghua"
      },
      {
        "surname": "Wu",
        "given_name": "Yingwen"
      },
      {
        "surname": "Li",
        "given_name": "Tao"
      },
      {
        "surname": "Cai",
        "given_name": "Jia"
      },
      {
        "surname": "Cai",
        "given_name": "Feipeng"
      },
      {
        "surname": "Huang",
        "given_name": "Xiaolin"
      },
      {
        "surname": "Yang",
        "given_name": "Jie"
      }
    ]
  },
  {
    "title": "Learning adversarial semantic embeddings for zero-shot recognition in open worlds",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110258",
    "abstract": "Zero-Shot Learning (ZSL) focuses on classifying samples of unseen classes with only their side semantic information presented during training. It cannot handle real-life, open-world scenarios where there are test samples of unknown classes for which neither samples (e.g., images) nor their side semantic information is known during training. Open-Set Recognition (OSR) is dedicated to addressing the unknown class issue, but existing OSR methods are not designed to model the semantic information of the unseen classes. To tackle this combined ZSL and OSR problem, we consider the case of “Zero-Shot Open-Set Recognition” (ZS-OSR), where a model is trained under the ZSL setting but it is required to accurately classify samples from the unseen classes while being able to reject samples from the unknown classes during inference. We perform large experiments on combining existing state-of-the-art ZSL and OSR models for the ZS-OSR task on four widely used datasets adapted from the ZSL task, and reveal that ZS-OSR is a non-trivial task as the simply combined solutions perform badly in distinguishing the unseen-class and unknown-class samples. We further introduce a novel approach specifically designed for ZS-OSR, in which our model learns to generate adversarial semantic embeddings of the unknown classes to train an unknowns-informed ZS-OSR classifier. Extensive empirical results show that our method 1) substantially outperforms the combined solutions in detecting the unknown classes while retaining the classification accuracy on the unseen classes and 2) achieves similar superiority under generalized ZS-OSR settings. Our code is available at https://github.com/lhrst/ASE.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324000098",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Class (philosophy)",
      "Classifier (UML)",
      "Computer science",
      "Discrete mathematics",
      "Engineering",
      "Inference",
      "Machine learning",
      "Mathematics",
      "Open set",
      "Pattern recognition (psychology)",
      "Programming language",
      "Set (abstract data type)",
      "Systems engineering",
      "Task (project management)",
      "Test set"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Tianqi"
      },
      {
        "surname": "Pang",
        "given_name": "Guansong"
      },
      {
        "surname": "Bai",
        "given_name": "Xiao"
      },
      {
        "surname": "Zheng",
        "given_name": "Jin"
      },
      {
        "surname": "Zhou",
        "given_name": "Lei"
      },
      {
        "surname": "Ning",
        "given_name": "Xin"
      }
    ]
  },
  {
    "title": "On the prediction of power outage length based on linear multifractional Lévy stable motion",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.03.013",
    "abstract": "In the power system, failure interruption hugely compromises the power system reliability. Therefore, an efficient method to correctly predict how long the power outage last would significantly improve the efficiency. To address this problem, we propose a method based on a suitable stochastic motion. First, we introduce linear multifractional Lévy stable motion (LMLSM). Then, by computing the global and local fractal characteristics we show that the LMLSM is multifractal and possesses the long-range dependence (LRD) property. Besides, the non-Gaussian characteristics of the LMLSM are analyzed, which means it can better describe the constant peak values in the stochastic time series. Furthermore, a discrete iterative prediction model is derived with fractional Black-Scholes differential equation. At last, a case study is provided based on the real failure interruption duration (FID) dataset in the power grid, by illustrating the validity of the proposed prediction method for power grid reliability.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524000813",
    "keywords": [
      "Algorithm",
      "Applied mathematics",
      "Brownian motion",
      "Composite material",
      "Computer science",
      "Fractal",
      "Fractional Brownian motion",
      "Gaussian",
      "Materials science",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Multifractal system",
      "Physics",
      "Power (physics)",
      "Quantum mechanics",
      "Range (aeronautics)",
      "Reliability (semiconductor)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Song",
        "given_name": "Wanqing"
      },
      {
        "surname": "Deng",
        "given_name": "Wujin"
      },
      {
        "surname": "Cattani",
        "given_name": "Piercarlo"
      },
      {
        "surname": "Qi",
        "given_name": "Deyu"
      },
      {
        "surname": "Yang",
        "given_name": "Xianhua"
      },
      {
        "surname": "Yao",
        "given_name": "Xuyin"
      },
      {
        "surname": "Chen",
        "given_name": "Dongdong"
      },
      {
        "surname": "Yan",
        "given_name": "Wenduan"
      },
      {
        "surname": "Zio",
        "given_name": "Enrico"
      }
    ]
  },
  {
    "title": "Progressive modality-complement aggregative multitransformer for domain multi-modal neural machine translation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110294",
    "abstract": "Domain-specific Multi-modal Neural Machine Translation (DMNMT) aims to translate domain-specific sentences from a source language to a target language by incorporating text-related visual information. Generally, domain-specific text-image data often complement each other and have the potential to collaboratively enhance the representation of domain-specific information. Unfortunately, there is a considerable modality gap between image and text in data format and semantic expression, which leads to distinctive challenges in domain-text translation tasks. Narrowing the modality gap and improving domain-aware representation are two critical challenges in DMNMT. To this end, this paper proposes a progressive modality-complement aggregative MultiTransformer, which aims to simultaneously narrow the modality gap and capture domain-specific multi-modal representation. We first adopt a bidirectional progressive cross-modal interactive strategy to effectively narrow the text-to-text, text-to-visual, and visual-to-text semantics in the multi-modal representation space by integrating visual and text information layer-by-layer. Subsequently, we introduce a modality-complement MultiTransformer based on progressive cross-modal interaction to extract the domain-related multi-modal representation, thereby enhancing machine translation performance. Experiment results on the Fashion-MMT and Multi-30k datasets are conducted, and the results show that the proposed approach outperforms the compared state-of-the-art (SOTA) methods on the En-Zh task in E-commerce domain, En-De, En-Fr and En-Cs tasks of Multi-30k in general domain. The in-depth analysis confirms the validity of the proposed modality-complement MultiTransformer and bidirectional progressive cross-modal interactive strategy for DMNMT.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324000451",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Complement (music)",
      "Complementation",
      "Computer science",
      "Domain (mathematical analysis)",
      "Gene",
      "Law",
      "Machine translation",
      "Mathematical analysis",
      "Mathematics",
      "Messenger RNA",
      "Modal",
      "Modality (human–computer interaction)",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Phenotype",
      "Political science",
      "Politics",
      "Polymer chemistry",
      "Programming language",
      "Representation (politics)",
      "Semantics (computer science)",
      "Translation (biology)"
    ],
    "authors": [
      {
        "surname": "Guo",
        "given_name": "Junjun"
      },
      {
        "surname": "Hou",
        "given_name": "Zhenyu"
      },
      {
        "surname": "Xian",
        "given_name": "Yantuan"
      },
      {
        "surname": "Yu",
        "given_name": "Zhengtao"
      }
    ]
  },
  {
    "title": "Loose to compact feature alignment for domain adaptive object detection",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.03.021",
    "abstract": "Recently, great achievements have been made for deep learning based object detection methods. But their performance drops significantly when domain shifts occur. To address this problem, in this work we propose a loose to compact feature alignment method under an unsupervised domain adaptation framework. The entire feature alignment is performed in a divide and conquer manner, so as to distribute the alignment difficulties at two steps. At the first step, we loosen the goal at both image and instance levels. At the image level, a new Mask Guided Foreground Alignment (MGFA) module is proposed to make the alignment focus more on easier foreground regions, leaving the more diverse and more difficult background regions to the second step; at the instance level, we propose a Class-Wise Instance Alignment (CWIA) module with separated domain classifiers for different categories so as to ease the alignment. At the second step, the alignment is performed per pixel and per instance, achieving a more compact and better aligned feature space. We conduct experiments on three different adaptation scenarios, where we achieve comparable results, demonstrating the effectiveness of our proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524000916",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Domain (mathematical analysis)",
      "Feature (linguistics)",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Yang"
      },
      {
        "surname": "Zhang",
        "given_name": "Shanshan"
      },
      {
        "surname": "Liu",
        "given_name": "Yunan"
      },
      {
        "surname": "Yang",
        "given_name": "Jian"
      }
    ]
  },
  {
    "title": "Unbiased and augmentation-free self-supervised graph representation learning",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110274",
    "abstract": "Graph Contrastive Learning (GCL) is a promising self-supervised method for learning node representations that combines graph convolutional networks (GCN) and contrastive learning. However, existing GCL methods heavily rely on graph structure data and augmentation schemes to learn invariant representations between different augmentation views. This can be problematic as the performance of GCNs may deteriorate when noisy connections are present in the original graph structure. Additionally, there is limited knowledge on how to significantly augment graphs without altering their labels. To address these issues, we propose a novel method called Unbiased and Augmentation-Free Self-Supervised Graph Contrastive Learning (USAF-GCL). We design graph learners and post-processing schemes to improve the structure of the original graph. Instead of using augmentation schemes, we generate contrastive views using global and local semantics. To ensure consistency between embedding similarity and original feature similarity, we introduce pseudo-homology to maximize the mutual information between predicted and true labels. Furthermore, we theoretically demonstrate that pseudo-homology maximization can enhance the upper bound of mutual information between predicted and true labels. USAF-GCL offers several advantages over existing GCL methods. Firstly, it uses an unbiased graph structure to reduce the impact of noise on model performance. Secondly, it saves computational resources by eliminating complex data expansion. Lastly, it integrates structural information, neighborhood information, and the consistency of embeddings and features in graph representation learning, effectively improving model performance. Extensive experiments on eight benchmark datasets confirm the remarkable effectiveness and efficiency of USAF-GCL.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324000256",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Feature learning",
      "Graph",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Ruyue"
      },
      {
        "surname": "Yin",
        "given_name": "Rong"
      },
      {
        "surname": "Liu",
        "given_name": "Yong"
      },
      {
        "surname": "Wang",
        "given_name": "Weiping"
      }
    ]
  },
  {
    "title": "Enhanced visible–infrared person re-identification based on cross-attention multiscale residual vision transformer",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110288",
    "abstract": "Visible–infrared (VI) person re-identification (Re-ID) is a critical identification task that involves retrieving and matching images of an individual using both infrared and visible imaging modalities. To improve the performance, researchers have developed methods to obtain implicit feature information; however, this degrades with fewer discriminative features. To address this issue, we propose a weighted fused cross-attention multi-scale residual vision transformer (WF-CAMReViT) approach to re-identify the appropriate person from visible–infrared modality images by integrating the cross-attention multi-scale residual vision transformer architecture with Opposition-based Dove Swarm Optimization (ODSO). The proposed framework aims to bridge the domain gap between the visible and infrared modalities and significantly improve the re-identification performance. RGB (visible) and infrared (IR) images of persons are gathered from standard datasets, subjected to a cross-attention multi-scale residual vision transformer network to collect features, and then fuse using minimal weight. We also propose Opposition-based DSO to find the minimal weight. The weighted fused features are then subjected to the final decoder layer of CAMReViT to perceive the characteristics of each modality. In this study, model-aware enhancement (MAE) loss is develop to improve the modality information capacity of modality-shared features. Then, the experimental results on the SYSU-MM01 and RegDB datasets are compared with state-of-the-art transformer-based visible–infrared person Re-ID tasks to verify the efficacy of the proposed model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324000396",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Electrical engineering",
      "Engineering",
      "Infrared",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Residual",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Sarker",
        "given_name": "Prodip Kumar"
      },
      {
        "surname": "Zhao",
        "given_name": "Qingjie"
      }
    ]
  },
  {
    "title": "AdvOps: Decoupling adversarial examples",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110252",
    "abstract": "Adversarial examples have a simple additive structure that the clean sample is added with delicate devised noise. Inspired by such an observation, we find that the prediction of the network on adversarial examples can also be decoupled into a simple additive structure, which is the sum of clean samples and adversarial perturbations in terms of the model prediction (called the decoupling principle). Thus, our findings can be served as a useful tool to gain insight into the underlying relationship between the inputs and the outputs of the model. However, although the adversarial examples generated by existing attack methods can satisfy the decoupling principle, the proportion is small. In this paper, we formulate the above issues as an optimization problem with multi-constrains, and we propose a generative model to generate adversarial examples that satisfy the decoupling principle and simultaneously obtain high attack performance. Specifically, we first adopt the adversarial loss to ensure the attack performance. Then, we devise a decouple loss to guarantee the decoupling principle. Moreover, we treat the Euclidean distances of perturbation as regularization terms to maintain visual quality. Extensive experiments against various networks on ImageNet and CIFAR10 show that the proposed method performs better than comparison methods in the comprehensive metric. Furthermore, transferability results suggested that adversarial examples that satisfy the decoupling principle show better transferability.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324000037",
    "keywords": [
      "Adversarial system",
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Control engineering",
      "Decoupling (probability)",
      "Engineering",
      "Generative grammar",
      "Logit",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Regularization (linguistics)",
      "Transferability"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Donghua"
      },
      {
        "surname": "Yao",
        "given_name": "Wen"
      },
      {
        "surname": "Jiang",
        "given_name": "Tingsong"
      },
      {
        "surname": "Chen",
        "given_name": "Xiaoqian"
      }
    ]
  },
  {
    "title": "FET-FGVC: Feature-enhanced transformer for fine-grained visual classification",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110265",
    "abstract": "The challenge of Fine-grained visual classification (FGVC) comes from the small variations between classes and the large variations within classes. Inspired by the fact that identifying bird species focuses not only on the global features of the subject area but also on the subtle details of the local area, we propose a feature-enhanced Transformer to improve the performance of FGVC. Our proposed method consists of a Dynamic Swin Transformer backbone for extracting comprehensive global image features through continuous attention aggregation, a GCN-based local branch for separating and enhancing local features in different regions, and a pairwise feature interaction (PFI) module for enhancing global features through interactions between image pairs. We conducted extensive experiments on five FGVC datasets to demonstrate the superiority of our method. By fusing the enhanced global and local features, our method achieves the best accuracy compared to existing methods. Our method has an advantage in terms of computational efficiency.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324000165",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Electrical engineering",
      "Engineering",
      "Feature (linguistics)",
      "Feature extraction",
      "Linguistics",
      "Machine learning",
      "Pairwise comparison",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Huazhen"
      },
      {
        "surname": "Zhang",
        "given_name": "Haimiao"
      },
      {
        "surname": "Liu",
        "given_name": "Chang"
      },
      {
        "surname": "An",
        "given_name": "Jianpeng"
      },
      {
        "surname": "Gao",
        "given_name": "Zhongke"
      },
      {
        "surname": "Qiu",
        "given_name": "Jun"
      }
    ]
  },
  {
    "title": "Tensor recovery based on Bivariate Equivalent Minimax-Concave Penalty",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110253",
    "abstract": "In tensor recovery problem, larger singular values often correspond to the primary information of the image, such as contours, sharp edges and smooth areas. The minimum–maximum concave penalty (MCP) function has been effective in preserving larger singular values and achieving good tensor recovery results. However, in the process of solving this problem, singular values may vary during iterations, and the fixed parameters of the MCP function may not sufficiently preserve all the larger singular values, which may hinder the attainment of optimal results in tensor recovery. To overcome this challenge, we propose the Bivariate Equivalent Minimax-Concave Penalty (BEMCP) theorem, which allows the parameters to adapt to the changes in singular values and more comprehensively preserve the larger singular values. For low-rank tensor completion and tensor robust principal component analysis problems, we propose BEMCP-based models. Finally, experiments with various real-world data demonstrate that the proposed methods outperform state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324000049",
    "keywords": [
      "Applied mathematics",
      "Biology",
      "Bivariate analysis",
      "Combinatorics",
      "Eigenvalues and eigenvectors",
      "Evolutionary biology",
      "Function (biology)",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Minimax",
      "Penalty method",
      "Physics",
      "Principal component analysis",
      "Pure mathematics",
      "Quantum mechanics",
      "Rank (graph theory)",
      "Robust principal component analysis",
      "Singular value",
      "Statistics",
      "Tensor (intrinsic definition)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Hongbing"
      },
      {
        "surname": "Fan",
        "given_name": "Hongtao"
      },
      {
        "surname": "Li",
        "given_name": "Yajing"
      }
    ]
  },
  {
    "title": "Deep neural networks for automatic speaker recognition do not learn supra-segmental temporal features",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.03.016",
    "abstract": "While deep neural networks have shown impressive results in automatic speaker recognition and related tasks, it is dissatisfactory how little is understood about what exactly is responsible for these results. Part of the success has been attributed in prior work to their capability to model supra-segmental temporal information (SST), i.e., learn rhythmic-prosodic characteristics of speech in addition to spectral features. In this paper, we (i) present and apply a novel test to quantify to what extent the performance of state-of-the-art neural networks for speaker recognition can be explained by modeling SST; and (ii) present several means to force respective nets to focus more on SST and evaluate their merits. We find that a variety of CNN- and RNN-based neural network architectures for speaker recognition do not model SST to any sufficient degree, even when forced. The results provide a highly relevant basis for impactful future research into better exploitation of the full speech signal and give insights into the inner workings of such networks, enhancing explainability of deep learning for speech technologies.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524000849",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Deep learning",
      "Deep neural networks",
      "Pattern recognition (psychology)",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Neururer",
        "given_name": "Daniel"
      },
      {
        "surname": "Dellwo",
        "given_name": "Volker"
      },
      {
        "surname": "Stadelmann",
        "given_name": "Thilo"
      }
    ]
  },
  {
    "title": "Leveraging tensor kernels to reduce objective function mismatch in deep clustering",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110229",
    "abstract": "Objective Function Mismatch (OFM) occurs when the optimization of one objective has a negative impact on the optimization of another objective. In this work we study OFM in deep clustering, and find that the popular autoencoder-based approach to deep clustering can lead to both reduced clustering performance, and a significant amount of OFM between the reconstruction and clustering objectives. To reduce the mismatch, while maintaining the structure-preserving property of an auxiliary objective, we propose a set of new auxiliary objectives for deep clustering, referred to as the Unsupervised Companion Objectives (UCOs). The UCOs rely on a kernel function to formulate a clustering objective on intermediate representations in the network. Generally, intermediate representations can include other dimensions, for instance spatial or temporal, in addition to the feature dimension. We therefore argue that the naïve approach of vectorizing and applying a vector kernel is suboptimal for such representations, as it ignores the information contained in the other dimensions. To address this drawback, we equip the UCOs with structure-exploiting tensor kernels, designed for tensors of arbitrary rank. The UCOs can thus be adapted to a broad class of network architectures. We also propose a novel, regression-based measure of OFM, allowing us to accurately quantify the amount of OFM observed during training. Our experiments show that the OFM between the UCOs and the main clustering objective is lower, compared to a similar autoencoder-based model. Further, we illustrate that the UCOs improve the clustering performance of the model, in contrast to the autoencoder-based approach. The code for our experiments is available at https://github.com/danieltrosten/tk-uco.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323009263",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Evolutionary biology",
      "Function (biology)",
      "Geometry",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Tensor (intrinsic definition)"
    ],
    "authors": [
      {
        "surname": "Trosten",
        "given_name": "Daniel J."
      },
      {
        "surname": "Løkse",
        "given_name": "Sigurd"
      },
      {
        "surname": "Jenssen",
        "given_name": "Robert"
      },
      {
        "surname": "Kampffmeyer",
        "given_name": "Michael"
      }
    ]
  },
  {
    "title": "CR-CAM: Generating explanations for deep neural networks by contrasting and ranking features",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110251",
    "abstract": "The class activation mapping (CAM) algorithm is a visual interpretation algorithm that identifies the most discriminative regions for the target class in a classification task. However, existing CAM algorithms do not consider the differences between different categories and regions that are irrelevant to the target class during the feature extraction process. This results in interference from similar categories and irrelevant regions on the edges of the target class, leading to distorted saliency maps. To address this issue, we propose the Contrast-Ranking Class Activation Mapping (CR-CAM), including Inter-class Mapping Contract Block (IMCB) and Ranking Block. To gradually eliminate the interference regions that are irrelevant to the target class, IMCB is designed to compare distances between features in manifold space in order to generate more accurate saliency maps. To account for the similarity between different categories, ranking blocks adopt a comparative approach to measure the distances of feature mappings in the manifold space, thereby reducing the weights of surrounding regions. CR-CAM can be used on both CNN and GCN without modifying the algorithm to generate the class activation map. Experiments on both ImageNet and NTU RGB+D 60 datasets show highly competitive performance. In particular, CR-CAM outperforms alternative approaches by an average decrease of 0.0811 in Ave Drop, while exhibiting an average improvement of 0.011 in Ave Inc for CNN.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324000025",
    "keywords": [
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Class (philosophy)",
      "Computer science",
      "Discriminative model",
      "Feature (linguistics)",
      "Geometry",
      "Image (mathematics)",
      "Linguistics",
      "Machine learning",
      "Margin (machine learning)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Ranking (information retrieval)",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Yanshan"
      },
      {
        "surname": "Liang",
        "given_name": "Huajie"
      },
      {
        "surname": "Zheng",
        "given_name": "Hongfang"
      },
      {
        "surname": "Yu",
        "given_name": "Rui"
      }
    ]
  },
  {
    "title": "HRNet: 3D object detection network for point cloud with hierarchical refinement",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110254",
    "abstract": "Recently, 3D object detection from LiDAR point clouds has advanced rapidly. Although the second stage can improve the detection performance significantly, prior works concern little about the essential differences among different stages for the performance enhancement. To address this, this paper proposes a Hierarchical Refinement Network (HRNet) with two novel strategies. Firstly, we build the detection head on multi-scale voxel features to optimize the regression branch progressively with an effective Scale-aware Attentive Propagation (SAP) module. Then, we propose a Dynamic Sample Selection (DSS) module for the recalculation of the IoU during each stage to obtain more balanced positive and negative sample selections. Experiments over benchmark datasets show the effectiveness of our HRNet, particularly for car detection in the sparse point clouds.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324000050",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Geodesy",
      "Geography",
      "Geometry",
      "Mathematics",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Physics",
      "Point (geometry)",
      "Point cloud",
      "Quantum mechanics",
      "Sample (material)",
      "Scale (ratio)",
      "Selection (genetic algorithm)",
      "Voxel"
    ],
    "authors": [
      {
        "surname": "Lu",
        "given_name": "Bin"
      },
      {
        "surname": "Sun",
        "given_name": "Yang"
      },
      {
        "surname": "Yang",
        "given_name": "Zhenyu"
      },
      {
        "surname": "Song",
        "given_name": "Ran"
      },
      {
        "surname": "Jiang",
        "given_name": "Haiyan"
      },
      {
        "surname": "Liu",
        "given_name": "Yonghuai"
      }
    ]
  },
  {
    "title": "Real-time 3-D image analysis via Jacobi moments",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.03.011",
    "abstract": "In this research, we have proposed the parallel GPU-accelerated algorithms to compute the Jacobi moments defined in a rectangular region with substantially improved computational efficiency and highly satisfied accuracy. In our algorithms, the parallel 3-D matrix multiplications are adopted to increase the computational efficiency, while the techniques of coalesced memory access, shared memory and heterogeneous computation are utilized to optimize the computing performance on a GPU platform. Our new GPU-accelerated system can provide any required moment computational accuracy without additional computing time. To verify the performance of our new parallel GPU-accelerated algorithms, we conducted a series of 2D and 3-D image analysis tests via the Jacobi moments with encouraging outcome, while the computing times for all these experimental tasks are in the level of milliseconds. It is expected that our new parallel GPU-accelerated algorithms will expedite the research in 3-D image analysis via the moment methods in the real-time range.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524000795",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Image (mathematics)",
      "Mathematics",
      "Optics",
      "Physics",
      "Velocity Moments",
      "Wavefront",
      "Zernike polynomials"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Puwei"
      },
      {
        "surname": "Liao",
        "given_name": "Simon"
      }
    ]
  },
  {
    "title": "Multi-scale architectures matter: Examining the adversarial robustness of flow-based lossless compression",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110242",
    "abstract": "Exact likelihood estimation on entropy coding makes flow-based models appealing for lossless image compression. However, the high fidelity storage cost is affected by the lossless compression ratio. The trade-off between efficiency and robustness of flow-based deep lossless compression models has not been fully explored. This paper characterizes the trade-off theoretically and empirically, revealing that flow-based models are susceptible to adversarial examples resulting in a significant change in compression ratio. The fragile robustness of flow-based models is due to their intrinsic multi-scale architectures lacking the Lipschitz property. Based on this insight, a stronger white-box attack, Auto-Weighted Projected Gradient Descent (AW-PGD), is developed to generate more universal adversarial examples. Additionally, a novel flow-based lossless compression model, Robust Integer Discrete Flow (R-IDF), is proposed to achieve comparable robustness to adversarial training without sacrificing compression efficiency. Experiments demonstrate that the PGD algorithm falls into local extreme values when attacking the compression model, but the proposed attack and defense methods effectively improve the invulnerability of the flow-based compression model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323009391",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Automotive engineering",
      "Biochemistry",
      "Chemistry",
      "Compression ratio",
      "Computer science",
      "Data compression",
      "Data compression ratio",
      "Engineering",
      "Entropy (arrow of time)",
      "Gene",
      "Image (mathematics)",
      "Image compression",
      "Image processing",
      "Internal combustion engine",
      "Lossless compression",
      "Lossy compression",
      "Physics",
      "Quantum mechanics",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Xia",
        "given_name": "Yichong"
      },
      {
        "surname": "Chen",
        "given_name": "Bin"
      },
      {
        "surname": "Feng",
        "given_name": "Yan"
      },
      {
        "surname": "Ge",
        "given_name": "Tianshuo"
      },
      {
        "surname": "Huang",
        "given_name": "Yujun"
      },
      {
        "surname": "Wang",
        "given_name": "Haoqian"
      },
      {
        "surname": "Wang",
        "given_name": "Yaowei"
      }
    ]
  },
  {
    "title": "Closed-loop unified knowledge distillation for dense object detection",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110235",
    "abstract": "Most of knowledge distillation methods for object detection are feature-based and have achieved competitive results. However, only distillating in feature imitation part does not take full advantage of more sophisticated detection head design for object detection, especially dense object detection. In this paper, a triple parallel distillation (TPD) is proposed which can efficiently transfer all the output response in detection head from teacher to student. Moreover, to overcome the drawback of simply combining the feature-based with the response-based distillation with limited effect enhancement. A hierarchical re-weighting attention distillation (HRAD) is proposed to make student learn more than the teacher in feature information, as well as reciprocal feedback between the classification-IoU joint representation of detection head and the attention-based feature. By jointly interacting the benefits of TPD and HRAD, a closed-loop unified knowledge distillation for dense object detection is proposed, which makes the feature-based and response-based distillation unified and complementary. Experiments on different benchmark datasets have shown that the proposed work is able to outperform other state-of-the-art distillation methods for dense object detection on both accuracy and robustness.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323009329",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Distillation",
      "Feature (linguistics)",
      "Feature extraction",
      "Gene",
      "Geodesy",
      "Geography",
      "Linguistics",
      "Machine learning",
      "Medicine",
      "Object (grammar)",
      "Object detection",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Radiology",
      "Robustness (evolution)",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Song",
        "given_name": "Yaoye"
      },
      {
        "surname": "Zhang",
        "given_name": "Peng"
      },
      {
        "surname": "Huang",
        "given_name": "Wei"
      },
      {
        "surname": "Zha",
        "given_name": "Yufei"
      },
      {
        "surname": "You",
        "given_name": "Tao"
      },
      {
        "surname": "Zhang",
        "given_name": "Yanning"
      }
    ]
  },
  {
    "title": "Adaptive instance similarity embedding for online continual learning",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110238",
    "abstract": "We study the online continual learning (CL) paradigm, where the learner must continually learn a sequence of tasks. In this setting, improving the learning ability of the model and mitigating catastrophic forgetting are two pivotal factors. Note that most existing approaches for online continual learning are based on the experience replay strategy. In this type of method, a memory buffer is applied to store a subset of previous tasks to prevent catastrophic forgetting. The samples from between the current task and the memory buffer are jointly trained to update the network parameters. Consider that most methods only generate the feature embeddings via a shared feature extractor and then train the network via cross-entropy loss. We argue that such methods fail to explore the feature embedding in its entirety and neglect the similar relations between samples, thus leading to lower discriminant performance, especially in an online learning setting. To this end, we propose the Adaptive Instance Similarity Embedding for Online Continual Learning (AISEOCL) framework, which further takes all the sample relations in a given batch into account. In detail, firstly, the experience replay strategy is used to avoid catastrophic forgetting. Then, during training, we apply the adaptive similar embedding to obtain additional valuable similar information from the current training samples composed of the current and previous tasks. Since not all samples are equally important to make a prediction, we further weigh the importance of each instance accordingly resorting to the attention mechanism. Importantly, we further impose a similarity distillation loss on the distributions of the similarity relationship between current and previous models. Such operation can transfer the similarity relationship between different samples from the old model to the current model to alleviate catastrophic forgetting. With this strategy, AISEOCL can further improve the learning ability of the model while enhancing the discriminant power, which is also beneficial to stably resist forgetting. The experiments on several existing benchmarks validate the effectiveness of our proposed approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323009354",
    "keywords": [
      "Adaptive learning",
      "Artificial intelligence",
      "Computer science",
      "Embedding",
      "Feature (linguistics)",
      "Feature learning",
      "Forgetting",
      "Image (mathematics)",
      "Linguistics",
      "Machine learning",
      "Philosophy",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Han",
        "given_name": "Ya-nan"
      },
      {
        "surname": "Liu",
        "given_name": "Jian-wei"
      }
    ]
  },
  {
    "title": "Graph contrastive learning with consistency regularization",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.03.014",
    "abstract": "Contrastive learning has actively been used for unsupervised graph representation learning owing to its success in computer vision. Most graph contrastive learning methods use instance discrimination. It treats each instance as a distinct class against a query instance as the pretext task. However, such methods inevitably cause a class collision problem because some instances may belong to the same class as the query. Thus, the similarity shared through instances from the same class cannot be reflected in the pre-training stage. To address this problem, we propose graph contrastive learning with consistency regularization (GCCR), which introduces a consistency regularization term to graph contrastive learning. Unlike existing methods, GCCR can obtain a graph representation that reflects intra-class similarity by introducing a consistency regularization term. To verify the effectiveness of the proposed method, we performed extensive experiments and demonstrated that GCCR improved the quality of graph representations for most datasets. Notably, experimental results in various settings show that the proposed method can learn effective graph representations with better robustness against transformations than other state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524000825",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Consistency (knowledge bases)",
      "Graph",
      "Mathematics",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Regularization (linguistics)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Lee",
        "given_name": "Soohong"
      },
      {
        "surname": "Lee",
        "given_name": "Sangho"
      },
      {
        "surname": "Lee",
        "given_name": "Jaehwan"
      },
      {
        "surname": "Lee",
        "given_name": "Woojin"
      },
      {
        "surname": "Son",
        "given_name": "Youngdoo"
      }
    ]
  },
  {
    "title": "PDRLRR: A novel low-rank representation with projection distance regularization via manifold optimization for clustering",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110198",
    "abstract": "The low-rank representation (LRR) method has attracted widespread attention due to its excellent performance in pattern recognition and machine learning. LRR-based variants have been proposed to solve the three existing problems in LRR: (1) the projection matrix is permanently fixed when dimensionality reduction techniques are adopted; (2) LRR fails to capture the local geometric structure; and (3) the solution deviates from the real low-rank solution. To address these problems, this paper proposes a low-rank representation with projection distance regularization (PDRLRR) via manifold optimization for clustering. In detail, we first introduce a low-dimensional projection matrix and a projection distance regularization term to fit the projected data automatically and capture the local structure of the data, respectively. Consequently, the projection matrix and representation matrix are obtained jointly. Then, we obtain a more accurate low-rank solution by minimizing the Schatten- p norm instead of the nuclear norm. Next, the projection matrix is optimized through a generalized Stiefel manifold. Extensive experiments demonstrate that our proposed method outperforms the state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323008956",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Cluster analysis",
      "Combinatorics",
      "Composite material",
      "Computer science",
      "Dimensionality reduction",
      "Eigenvalues and eigenvectors",
      "Low-rank approximation",
      "Materials science",
      "Mathematics",
      "Matrix (chemical analysis)",
      "Matrix norm",
      "Pattern recognition (psychology)",
      "Physics",
      "Projection (relational algebra)",
      "Pure mathematics",
      "Quantum mechanics",
      "Rank (graph theory)",
      "Regularization (linguistics)",
      "Stiefel manifold",
      "Tensor (intrinsic definition)"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Haoran"
      },
      {
        "surname": "Chen",
        "given_name": "Xu"
      },
      {
        "surname": "Tao",
        "given_name": "Hongwei"
      },
      {
        "surname": "Li",
        "given_name": "Zuhe"
      },
      {
        "surname": "Wang",
        "given_name": "Boyue"
      }
    ]
  },
  {
    "title": "Robust low tubal rank tensor recovery via L 2 E criterion",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110241",
    "abstract": "Tensor analysis has received enormous attention as the increasing prevalence of high-dimensional data in science research and engineering application. Tensor recovery is an important and meaningful problem for tensor analysis. It aims to complete a tensor from an observed subset of its entries disturbed by noise. However, classical methods either develop on the second-order statistics or Lasso-type penalty, leading to them not effectively dealing with gross or dense noise effectively. To address such issues, we propose a robust tensor recovery model for simultaneously completing a low tubal rank tensor with complex noise and missing data. Based on tensor–tensor product (t-product), we first develop a tensor factor Frobenius norm to exploit the low tubal rank property which is closely related to tensor nuclear norm and tubal rank. By utilizing the robust L 2 criterion, we derive the nonconvex objective function to accommodate the low tubal rank tensor. An implementable alternating minimization algorithm has been developed to estimate the low tubal rank tensor. It is worth noting that our method is able to jointly estimate the precision parameter to capture the hidden complex noise pattern. Furthermore, some convergence properties of the proposed algorithm are presented. A series of numerical experiments are conducted on both synthetic and real-world data to demonstrate the effectiveness and robustness of the proposed approach in comparison with the state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032300938X",
    "keywords": [
      "Combinatorics",
      "Computer science",
      "Mathematics",
      "Rank (graph theory)"
    ],
    "authors": [
      {
        "surname": "Song",
        "given_name": "Zihao"
      },
      {
        "surname": "Xu",
        "given_name": "Xiangjian"
      },
      {
        "surname": "Lian",
        "given_name": "Heng"
      },
      {
        "surname": "Zhao",
        "given_name": "Weihua"
      }
    ]
  },
  {
    "title": "TriSig: Evaluating the statistical significance of triclusters",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110231",
    "abstract": "Tensor data analysis allows researchers to uncover novel patterns and relationships that cannot be obtained from tabular data alone. The information inferred from multi-way patterns can offer valuable insights into disease progression, bioproduction processes, behavioral responses, weather fluctuations, or social dynamics. However, spurious patterns often hamper this process. This work aims at proposing a statistical frame to assess the probability of patterns in tensor data to deviate from null expectations, extending well-established principles for assessing the statistical significance of patterns in tabular data. A principled discussion on binomial testing to mitigate false positive discoveries is entailed at the light of: variable dependencies, temporal associations and misalignments, and multi-hypothesis correction. Results gathered from the application of triclustering algorithms over distinct real-world case studies in biotechnological domains confer validity to the proposed statistical frame while revealing vulnerabilities of reference triclustering searches. The proposed assessment can be incorporated into existing triclustering algorithms to minimize spurious occurrences, rank patterns, and further prune the search space, reducing their computational complexity.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323009287",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Frame (networking)",
      "Machine learning",
      "Mathematics",
      "Null (SQL)",
      "Null hypothesis",
      "Operating system",
      "Process (computing)",
      "Spurious relationship",
      "Statistical hypothesis testing",
      "Statistical model",
      "Statistics",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Alexandre",
        "given_name": "Leonardo"
      },
      {
        "surname": "Costa",
        "given_name": "Rafael S."
      },
      {
        "surname": "Henriques",
        "given_name": "Rui"
      }
    ]
  },
  {
    "title": "FedNN: Federated learning on concept drift data using weight and adaptive group normalizations",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110230",
    "abstract": "Federated Learning (FL) allows a global model to be trained without sharing private raw data. The major challenge in FL is client-wise data heterogeneity leading to different model convergence speed and accuracy. Despite the recent progress of FL, most methods verify their accuracy on prior probability shift (label distribution skew) dataset, while the concept drift problem (i.e., where each client has distinct styles of input while sharing the same labels) has not been explored. In real scenarios, concept drift is of paramount concern in FL since the client’s data is collected under extremely different conditions making FL optimization more challenging. Significant differences in inputs among clients exacerbate the heterogeneity of clients’ parameters compared to prior probability shift, ultimately resulting in failures for previous FL approaches. To address the challenge of concept drift, we use Weight Normalization (WN) and Adaptive Group Normalization (AGN) to alleviate conflicts during global model updates. WN re-parameterizes weights to have zero mean and unit variance while AGN adaptively selects the optimal mean and standard deviation for feature normalization based on the dataset. These two components significantly contribute to having consistent activations after global model updates reducing heterogeneity in concept drift data. Comprehensive experiments on seven datasets (with concept drift) demonstrate that our method outperforms five state-of-the-art FL methods and shows faster convergence speed compared to the previous FL methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323009275",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Concept drift",
      "Data mining",
      "Data stream mining",
      "Federated learning",
      "Group (periodic table)",
      "Machine learning",
      "Organic chemistry"
    ],
    "authors": [
      {
        "surname": "Kang",
        "given_name": "Myeongkyun"
      },
      {
        "surname": "Kim",
        "given_name": "Soopil"
      },
      {
        "surname": "Jin",
        "given_name": "Kyong Hwan"
      },
      {
        "surname": "Adeli",
        "given_name": "Ehsan"
      },
      {
        "surname": "Pohl",
        "given_name": "Kilian M."
      },
      {
        "surname": "Park",
        "given_name": "Sang Hyun"
      }
    ]
  },
  {
    "title": "U-Transformer-based multi-levels refinement for weakly supervised action segmentation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110199",
    "abstract": "Action segmentation is a research hotspot in human action analysis, which aims to split videos into segments of different actions. Recent algorithms have achieved great success in modeling based on temporal convolution, but these methods weight local or global timing information through additional modules, ignoring the existing long-term and short-term information connections between actions. This paper proposes a U-Transformer structure based on multi-level refinement, introduces neighborhood attention to learn the neighborhood information of adjacent frames, and aggregates video frame features to effectively process long-term sequence information. Then a loss optimization strategy is proposed to smooth the original classification effect and generate a more accurate calibration sequence by introducing a pairing similarity optimization method based on deep feature learning. In addition, we propose a timestamp supervised training method to generate complete information for actions based on pseudo-label predictions for action boundary predictions. Experiments on three challenging action segmentation datasets, 50Salads, GTEA, and Breakfast, show that our model performs state-of-the-art models, and our weakly supervised model also performs comparably to fully supervised performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323008968",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Segmentation",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Ke",
        "given_name": "Xiao"
      },
      {
        "surname": "Miao",
        "given_name": "Xin"
      },
      {
        "surname": "Guo",
        "given_name": "Wenzhong"
      }
    ]
  },
  {
    "title": "A diagnostic report supervised deep learning model training strategy for diagnosis of COVID-19",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110232",
    "abstract": "COVID-19 is a highly contagious infectious disease that necessitates timely assessment and effective diagnosis, although it is no longer a health emergency. Most existing computer-aided diagnosis systems for COVID-19 can achieve high accuracy, but they show insufficient generalization performance and weak interpretability. To address these issues, we propose diagnostic report supervised contrastive learning (DRSCL), a model training strategy that incorporates textual information from medical reports into model pretraining and then only transfers the pretrained image encoder into model inference. Due to the issue of data recurrence in medical diagnosis reports, which is common in the medical domain and can cause nonconvergence of the pretraining stage of DRSCL, we improve the loss function calculation of contrastive learning by integrating an operation to merge identical text or image features. In addition, for the fine-tuning and inference stage of DRSCL, we design a hierarchical fine-tuning strategy to better evaluate the pretraining performance and importance of each module. In case study, we build a medical image-text pair dataset of lung diseases for pretraining, with samples collected from hospitals in East China, and then conduct the fine-tuning and inference operation of DRSCL with a publicly available SARS-CoV-2 dataset. The comparative experimental results show that DRSCL helps all involved image encoders obtain better classification accuracy and superior generalization performance in the given COVID-19-related diagnostic application. This finding indicates that DRSCL enhances deep models to learn more deep information with supervision of medical textual information. Furthermore, we adopt the Grad-CAM method to visualize pretrained models, and the results demonstrate that the DRSCL strategy is advantageous for improving model interpretability.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323009299",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Deep learning",
      "Encoder",
      "Generalization",
      "Inference",
      "Information retrieval",
      "Interpretability",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Merge (version control)",
      "Operating system"
    ],
    "authors": [
      {
        "surname": "Deng",
        "given_name": "Shiqi"
      },
      {
        "surname": "Zhang",
        "given_name": "Xing"
      },
      {
        "surname": "Jiang",
        "given_name": "Shancheng"
      }
    ]
  },
  {
    "title": "Fusion-competition framework of local topology and global texture for head pose estimation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110285",
    "abstract": "RGB image and point cloud involve texture and geometric structure, which are widely used for head pose estimation. However, images lack of spatial information, and the quality of point cloud is easily affected by sensor noise. In this paper, a novel fusion-competition framework (FCF) is proposed to overcome the limitations of a single modality. The global texture information is extracted from image and the local topology information is extracted from point cloud to project heterogeneous data into a common feature subspace. The projected texture feature weighted by the channel attention mechanism is embedded into each local point cloud region with different topological features for fusion. The scoring mechanism creates competition among the regions involving local-global fused features to predict final pose with the highest score. According to the evaluation results on the public and our constructed datasets, the FCF improves the estimation accuracy and stability by an average of 13.6 % and 12.7 %, which is compared to nine state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324000360",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Image (mathematics)",
      "Linguistics",
      "Mathematics",
      "Noise (video)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Point cloud",
      "Pose",
      "RGB color model",
      "Subspace topology",
      "Topology (electrical circuits)"
    ],
    "authors": [
      {
        "surname": "Ma",
        "given_name": "Dongsheng"
      },
      {
        "surname": "Fu",
        "given_name": "Tianyu"
      },
      {
        "surname": "Yang",
        "given_name": "Yifei"
      },
      {
        "surname": "Cao",
        "given_name": "Kaibin"
      },
      {
        "surname": "Fan",
        "given_name": "Jingfan"
      },
      {
        "surname": "Xiao",
        "given_name": "Deqiang"
      },
      {
        "surname": "Song",
        "given_name": "Hong"
      },
      {
        "surname": "Gu",
        "given_name": "Ying"
      },
      {
        "surname": "Yang",
        "given_name": "Jian"
      }
    ]
  },
  {
    "title": "Class correlation correction for unbiased scene graph generation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110221",
    "abstract": "The long-tail distribution in the scene graph generation (SGG) task has spurred immense interest in unbiased SGG. However, current state-of-the-art debiasing techniques extract statistics from the independent category, while ignoring the correlation between categories. To address this issue, we propose a simple but effective method, class correlation correction, to aggregate dependency knowledge among various classes. Specifically, given biased predictions, two kinds of debiasing transformations are developed employing the class correlation aware label to recover the unbiased estimates. We also propose to retrain SGG models with the biasing transformations adapted to the biased data distribution. The proposed debiasing method is evaluated using several biased datasets that are constructed from CIFAR-10 and Fashion-MNIST, as well as the commonly used SGG dataset and Caltech 101 dataset. Multiple evaluation metrics are used to assess the debiasing performance, and the results of extensive experiments show the effectiveness of the proposed method. The Pytorch® implementations can be downloaded from an open-source GitHub project https://github.com/Dlut-lab-zmn/class-correlation-correction.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323009184",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Cognitive science",
      "Computer science",
      "Correlation",
      "Debiasing",
      "Deep learning",
      "Geometry",
      "Graph",
      "MNIST database",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Psychology",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Mengnan"
      },
      {
        "surname": "Kong",
        "given_name": "Yuqiu"
      },
      {
        "surname": "Zhang",
        "given_name": "Lihe"
      },
      {
        "surname": "Yin",
        "given_name": "Baocai"
      }
    ]
  },
  {
    "title": "SED: Searching Enhanced Decoder with switchable skip connection for semantic segmentation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110196",
    "abstract": "Neural architecture search (NAS) has shown excellent performance. However, existing semantic segmentation models rely heavily on pre-training on Image-Net or COCO and mainly focus on the designing of decoders. Directly training the encoder–decoder architecture search models from scratch to SOTA for semantic segmentation requires even thousands GPU days, which greatly limits the application of NAS. To address this issue, we propose a novel neural architecture Search framework for Enhanced Decoder (SED). Utilizing the pre-trained hand-designing backbone and the searching space composed of light-weight cells, SED searches for a decoder which can perform high-quality segmentation. Furthermore, we attach switchable skip connection operations to search space, expanding the diversity of possible network structure. The parameters of backbone and operations selected in searching phrase are copied to retraining process. As a result, searching, pruning and retraining can be done in just 1 day. The experimental results show that the SED proposed in this paper only needs 1/4 of the parameters and calculation in contrast to hand-designing decoder, and obtains higher segmentation accuracy on Cityscapes. Transferring the same decoder architecture to other datasets, such as: Pascal VOC 2012, Camvid, ADE20K proves the robustness of SED.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323008932",
    "keywords": [
      "Architecture",
      "Art",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Encoder",
      "Gene",
      "Operating system",
      "Pattern recognition (psychology)",
      "Robustness (evolution)",
      "Segmentation",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Xian"
      },
      {
        "surname": "Quan",
        "given_name": "Zhibin"
      },
      {
        "surname": "Li",
        "given_name": "Qiang"
      },
      {
        "surname": "Zhu",
        "given_name": "Dejun"
      },
      {
        "surname": "Yang",
        "given_name": "Wankou"
      }
    ]
  },
  {
    "title": "PVConvNet: Pixel-Voxel Sparse Convolution for multimodal 3D object detection",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110284",
    "abstract": "Current LiDAR-only 3D detection methods inevitably suffer from the sparsity of point clouds and insufficient semantic information. To alleviate this difficulty, recent proposals densify LiDAR points by depth completion and then perform feature fusion with image pixels at the data-level or result-level. However, these methods often suffer from poor fusion effects and insufficient use of image information for voxel feature-level fusion. Meanwhile, noises brought by inaccurate depth completion significantly degrade detection accuracy. In this paper, we propose PVConvNet, a unified framework for multi-modal feature fusion that cleverly combines LiDAR points, virtual points and image pixels. Firstly, we develop an efficient Pixel-Voxel Sparse Convolution (PVConv) to perform voxel-wise feature-level fusion of point clouds and images. Secondly, we design a Noise-Resistant Dilated Sparse Convolution (NRDConv) to encode the voxel features of virtual points, which effectively reduces the impact of noise. Finally, we propose a unified RoI pooling strategy, namely Multimodal Voxel-RoI Pooling, for improving proposal refinement accuracy. We evaluate PVConvNet on the widely used KITTI dataset and the more challenging nuScenes dataset. Experimental results show that our method outperforms state-of-the-art multi-modal based methods, achieving a moderate 3D AP of 86.92% on the KITTI test set.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324000359",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer vision",
      "Convolution (computer science)",
      "Feature (linguistics)",
      "Feature extraction",
      "Geography",
      "Image (mathematics)",
      "Lidar",
      "Linguistics",
      "Noise (video)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pixel",
      "Point cloud",
      "Pooling",
      "Remote sensing",
      "Voxel"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Huaijin"
      },
      {
        "surname": "Du",
        "given_name": "Jixiang"
      },
      {
        "surname": "Zhang",
        "given_name": "Yong"
      },
      {
        "surname": "Zhang",
        "given_name": "Hongbo"
      },
      {
        "surname": "Zeng",
        "given_name": "Jiandian"
      }
    ]
  },
  {
    "title": "Multi-view robust regression for feature extraction",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110219",
    "abstract": "Recently, Multi-view Discriminant Analysis (MVDA) has been proposed and achieves good performance in multi-view recognition tasks. However, as an extension of LDA, this method still suffers from the small-class problem and has the sensitivity to outliers. In order to address these drawbacks and achieve better performance on multi-view recognition tasks, we proposed Multi-view Robust Regression (MVRR) for multi-view feature extraction. MVRR is a regression based method that imposes L 2 , 1 norm as the metric of the loss function and the regularization term to improve robustness and obtain jointly sparse projection matrices for effective feature extraction. Moreover, we incorporate an orthogonal matrix to regress the extracted features to their scaled label to avoid the small-class problem. Therefore, MVRR guarantees the projection matrix to break through the restriction of the number of class for solving the small-class problem. We also propose an iterative algorithm to compute the optimal solution of MVRR and the convergence of MVRR is proved. Experiments are conducted on four databases to verify the performance of MVRR and the result illustrates that MVRR is robust on multi-view feature extraction.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323009160",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Feature extraction",
      "Gene",
      "Mathematics",
      "Outlier",
      "Pattern recognition (psychology)",
      "Regularization (linguistics)",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Lai",
        "given_name": "Zhihui"
      },
      {
        "surname": "Chen",
        "given_name": "Foping"
      },
      {
        "surname": "Wen",
        "given_name": "Jiajun"
      }
    ]
  },
  {
    "title": "Unsupervised spatial self-similarity difference-based change detection method for multi-source heterogeneous images",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110237",
    "abstract": "Multi-source heterogeneous change detection has been widely used in dynamic disaster monitoring, land cover updating, etc. Various methods have been proposed to make heterogeneous data comparable. However, heterogeneous images are difficult to compare directly and may be affected by noise. Most existing methods obtain change information through mapping and regression, lacking the utilisation of image spatial information and a comprehensive portrayal of the changes, which may affect change detection results. To address these challenges, we propose an unsupervised spatial self-similarity difference-based change detection (USSD) method for multi-source heterogeneous images to evaluate the similarity of spatial relationships in heterogeneous images. First, the images are divided into image blocks to construct spatial self-difference images between individual image blocks aiming to make the data comparable. Second, the change information is portrayed in terms of both the magnitude differences and similarity differences to obtain a more comprehensive spatial self-difference change magnitude map. Then, the spatial neighbourhood information of the spatial self-difference change magnitude map is considered to avoid noise. Experimental results on six open datasets indicate that the overall accuracy of the USSD method was approximately 85%–95%. This method improves the change magnitude map discrimination, better detects the change region, and avoids noise in synthetic aperture radar images.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323009342",
    "keywords": [
      "Artificial intelligence",
      "Change detection",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Geography",
      "Image (mathematics)",
      "Noise (video)",
      "Pattern recognition (psychology)",
      "Remote sensing",
      "Similarity (geometry)",
      "Spatial analysis"
    ],
    "authors": [
      {
        "surname": "Zhu",
        "given_name": "Linye"
      },
      {
        "surname": "Sun",
        "given_name": "Wenbin"
      },
      {
        "surname": "Fan",
        "given_name": "Deqin"
      },
      {
        "surname": "Xing",
        "given_name": "Huaqiao"
      },
      {
        "surname": "Liu",
        "given_name": "Xiaoqi"
      }
    ]
  },
  {
    "title": "Superpixel-based multi-scale multi-instance learning for hyperspectral image classification",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110257",
    "abstract": "Superpixels can define meaningful local regions within a hyperspectral image (HSI) and have become the building blocks of various HSI classification methods. The superpixels in HSIs consist of multiple spectral pixels, sharing a similar structure with the data in multi-instance learning (MIL). However, the potential of MIL methods in the field of HSI classification has been rarely explored. In this paper, we propose the superpixel-based multi-scale multi-instance learning (MSMIL) framework, enhancing the superpixel representation with MIL for the first time. Segmenting the HSIs with superpixels, MSMIL converts the HSI classification into MIL problems and extracts superpixel representations via the MIL method, namely multi-instance factor analysis (MIFA). Compared with the existing methods focusing exclusively on the local information, MIFA utilizes the deviations from an overall generative model to describe the superpixels, retaining both the local and the global information. Moreover, MSMIL introduces multi-scale superpixels and a spectral–spatial decision fusion strategy for further refinement, where the results of multi-scale superpixel maps are weighted according to prediction certainty and spatial consistency. The proposed method is evaluated on four benchmark datasets and achieves competitive results. For example, MSMIL outperforms the comparison methods with a margin of 5% overall accuracy on the Indian Pines dataset, when 2% pixels are selected as the training set.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324000086",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Cartography",
      "Computer science",
      "Geodesy",
      "Geography",
      "Hyperspectral imaging",
      "Machine learning",
      "Margin (machine learning)",
      "Pattern recognition (psychology)",
      "Pixel",
      "Scale (ratio)"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Shiluo"
      },
      {
        "surname": "Liu",
        "given_name": "Zheng"
      },
      {
        "surname": "Jin",
        "given_name": "Wei"
      },
      {
        "surname": "Mu",
        "given_name": "Ying"
      }
    ]
  },
  {
    "title": "Semi-supervised domain generalization with evolving intermediate domain",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110280",
    "abstract": "Domain Generalization (DG) aims to generalize a model trained on multiple source domains to an unseen target domain. The source domains always require precise annotations, which can be cumbersome or even infeasible to obtain in practice due to the vast amount of data involved. Web data, namely web-crawled images, offers an opportunity to access large amounts of unlabeled images with rich style information, which can be leveraged to improve DG. From this perspective, we introduce a novel paradigm of DG, termed as Semi-Supervised Domain Generalization (SSDG), to explore how the labeled and unlabeled source domains can interact, and establish two settings, including the close-set and open-set SSDG. The close-set SSDG is based on existing public DG datasets, while the open-set SSDG, built on the newly-collected web-crawled datasets, presents a novel yet realistic challenge that pushes the limits of current technologies. A natural approach of SSDG is to transfer knowledge from labeled data to unlabeled data via pseudo labeling, and train the model on both labeled and pseudo-labeled data for generalization. Since there are conflicting goals between domain-oriented pseudo labeling and out-of-domain generalization, we develop a pseudo labeling phase and a generalization phase independently for SSDG. Unfortunately, due to the large domain gap, the pseudo labels provided in the pseudo labeling phase inevitably contain noise, which has negative affect on the subsequent generalization phase. Therefore, to improve the quality of pseudo labels and further enhance generalizability, we propose a cyclic learning framework to encourage a positive feedback between these two phases, utilizing an evolving intermediate domain that bridges the labeled and unlabeled domains in a curriculum learning manner. Extensive experiments are conducted to validate the effectiveness of our method. It is worth highlighting that web-crawled images can promote domain generalization as demonstrated by the experimental results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324000311",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Domain (mathematical analysis)",
      "Generalizability theory",
      "Generalization",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Perspective (graphical)",
      "Programming language",
      "Set (abstract data type)",
      "Statistics",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Lin",
        "given_name": "Luojun"
      },
      {
        "surname": "Xie",
        "given_name": "Han"
      },
      {
        "surname": "Sun",
        "given_name": "Zhishu"
      },
      {
        "surname": "Chen",
        "given_name": "Weijie"
      },
      {
        "surname": "Liu",
        "given_name": "Wenxi"
      },
      {
        "surname": "Yu",
        "given_name": "Yuanlong"
      },
      {
        "surname": "Zhang",
        "given_name": "Lei"
      }
    ]
  },
  {
    "title": "Source-free domain adaptation with unrestricted source hypothesis",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110246",
    "abstract": "Domain adaptation aims to bridge the distribution discrepancy across different domains and improve the generalization ability of learning models on the target domain. The existing domain adaptation approaches align the distribution shift via adversarial training on the source and target data. In practice, however, the source data is usually unavailable due to the privacy factor. In this work, we mainly focus on the source-free domain adaptation setting, in which we are only accessible to the model trained on the source data and the unlabeled target data. To this end, we propose the Source-Free Adversarial Domain Adaptation (SFADA) approach to align the distribution of the target domain data in the absence of source domain data. In particular, we develop an effective metric to measure the domain discrepancy by introducing the proxy data of the source domain. To generate the proxy data, our approach retrieves target data which lie over the intersection of the supports of the source and target domains. We also derive the learning bound of the source-free domain adaptation theoretically and show that our proposed SFADA approach is capable of reducing the bound effectively. Additionally, instead of modifying the source model in previous source-free approaches, our SFADA does not require training the source model with specific restrictions (i.e., normalizing the classifier weight) for practice and privacy-related concerns. State-of-the-art results are achieved for different standard domain adaptation benchmarks. The code can be available from https://github.com/tiggers23/SFADA-main.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323009433",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Domain (mathematical analysis)",
      "Domain adaptation",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Multi-source",
      "Operating system",
      "Source code",
      "Statistics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "He",
        "given_name": "Jiujun"
      },
      {
        "surname": "Wu",
        "given_name": "Liang"
      },
      {
        "surname": "Tao",
        "given_name": "Chaofan"
      },
      {
        "surname": "Lv",
        "given_name": "Fengmao"
      }
    ]
  },
  {
    "title": "A more reliable local-global-guided network for correspondence pruning",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.03.006",
    "abstract": "The correspondence pruning task relies on both local and global contexts, which are considered to be essential in inferring the probability of inliers. Many previous approaches seek to devise various structures to make effective use of them, but they either use only a plain structure or base it on their own hypothetical relationships, which leads to some limitations remaining to be improved. Derived from this, we propose our LG-Net including a simple yet effective LGA block and a well-designed GPA block to extract local and global information respectively. Specifically, the LGA block combines local topology into the neighborhood and enhances the representative ability by enabling the interaction of the adjacency neighbors with a simple Softmax operation. Meanwhile, the GPA block prefers correspondences with higher inlier-probability to restrict the interference of outliers. With the guide of relatively reliable prior, it will facilitate the robustness of gathering rich global contextual information. As a consequence, our LG-Net takes both local and global context into account to help successfully recover correspondences. Extensive experiments have demonstrated the better performance of our method comparing with existing state-of-the-art methods on camera pose estimation and homography estimation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524000746",
    "keywords": [
      "Agronomy",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Correspondence problem",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pruning"
    ],
    "authors": [
      {
        "surname": "Peng",
        "given_name": "Chengli"
      },
      {
        "surname": "Yang",
        "given_name": "Zhenyu"
      },
      {
        "surname": "Lu",
        "given_name": "Yiwei"
      },
      {
        "surname": "Li",
        "given_name": "Zizhuo"
      },
      {
        "surname": "Jin",
        "given_name": "Qiwen"
      }
    ]
  },
  {
    "title": "A flexible non-monotonic discretization method for pre-processing in supervised learning",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.03.024",
    "abstract": "Discretization is one of the important pre-processing steps for supervised learning. Discretizing attributes helps to simplify the data and make it easier to understand and analyze by reducing the number of values. It can provide a better representation of knowledge and thus help improve the accuracy of a classifier. However, to minimize the information loss, it is important to consider the characteristics of the data. Most approaches assume that the values of a continuous attribute are monotone with respect to the probability of belonging to a particular class. In other words, it is assumed that increasing or decreasing the value of the attribute leads to a proportional increase or decrease in the classification score. This assumption may not always be valid for all attributes of data. In this study, we present entropy-based, flexible discretization strategies capable of capturing the non-monotonicity of the attribute values. The algorithm can adjust the number of cut points and values depending on the characteristics of the data. It does not require setting of any hyper-parameter or threshold. Extensive experiments on different datasets have shown that the proposed discretizers significantly improve the performance of classifiers, especially on complex and high-dimensional data sets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524000898",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Discretization",
      "Mathematical analysis",
      "Mathematics",
      "Monotonic function",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Şenozan",
        "given_name": "Hatice"
      },
      {
        "surname": "Soylu",
        "given_name": "Banu"
      }
    ]
  },
  {
    "title": "TFS-ViT: Token-level feature stylization for domain generalization",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110213",
    "abstract": "Standard deep learning models such as convolutional neural networks (CNNs) lack the ability of generalizing to domains which have not been seen during training. This problem is mainly due to the common but often wrong assumption of such models that the source and target data come from the same i.i.d. distribution. Recently, Vision Transformers (ViTs) have shown outstanding performance for a broad range of computer vision tasks. However, very few studies have investigated their ability to generalize to new domains. This paper presents a first Token-level Feature Stylization (TFS-ViT) approach for domain generalization, which improves the performance of ViTs to unseen data by synthesizing new domains. Our approach transforms token features by mixing the normalization statistics of images from different domains. We further improve this approach with a novel strategy for attention-aware stylization, which uses the attention maps of class (CLS) tokens to compute and mix normalization statistics of tokens corresponding to different image regions. The proposed method is flexible to the choice of backbone model and can be easily applied to any ViT-based architecture with a negligible increase in computational complexity. Comprehensive experiments show that our approach is able to achieve state-of-the-art performance on five challenging benchmarks for domain generalization, and demonstrate its ability to deal with different types of domain shifts. The implementation is available at this repository.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032300910X",
    "keywords": [
      "Anthropology",
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Convolutional neural network",
      "Feature (linguistics)",
      "Generalization",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Normalization (sociology)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Security token",
      "Sociology",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Noori",
        "given_name": "Mehrdad"
      },
      {
        "surname": "Cheraghalikhani",
        "given_name": "Milad"
      },
      {
        "surname": "Bahri",
        "given_name": "Ali"
      },
      {
        "surname": "Vargas Hakim",
        "given_name": "Gustavo A."
      },
      {
        "surname": "Osowiechi",
        "given_name": "David"
      },
      {
        "surname": "Ayed",
        "given_name": "Ismail Ben"
      },
      {
        "surname": "Desrosiers",
        "given_name": "Christian"
      }
    ]
  },
  {
    "title": "Multiresolution causality of Bitcoin on GCC stock markets: Utilizing EMD-Granger analytical methodology",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.03.027",
    "abstract": "This article employs an Empirical Mode Decomposition (EMD)-based multiresolution causality approach to explore the scale-by-scale interconnectedness between Bitcoin and the stock markets of Gulf Cooperation Council (GCC) countries. EMD is utilized to decompose signals into intrinsic mode functions (IMFs), which delineate variations across different frequency scales, thus facilitating the identification of distinct oscillations and trends within the signals. The study reveals a significant positive connectedness between Bitcoin and most GCC stock markets, albeit with some variations observed across countries and time periods. Employing multiresolutional causal analysis through EMD provides a valuable framework for examining the nonlinear relationships among financial assets, offering insights into Bitcoin’s potential as a diversification tool in certain periods within GCC stock markets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524001016",
    "keywords": [
      "Algorithm",
      "Archaeology",
      "Artificial intelligence",
      "Computer science",
      "Econometrics",
      "Economics",
      "Geography",
      "Granger causality",
      "Pattern recognition (psychology)",
      "Stock (firearms)"
    ],
    "authors": [
      {
        "surname": "Saâdaoui",
        "given_name": "Foued"
      },
      {
        "surname": "Rabbouch",
        "given_name": "Bochra"
      },
      {
        "surname": "Garg",
        "given_name": "Harish"
      }
    ]
  },
  {
    "title": "Semantic perceptive infrared and visible image fusion Transformer",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110223",
    "abstract": "Deep learning based fusion mechanisms have achieved sophisticated performance in the field of image fusion. However, most existing approaches focus on learning global and local features but seldom consider to modeling semantic information, which might result in inadequate source information preservation. In this work, we propose a semantic perceptive infrared and visible image fusion Transformer (SePT). The proposed SePT extracts local feature through convolutional neural network (CNN) based module and learns long-range dependency through Transformer based modules, and meanwhile designs two semantic modeling modules based on Transformer architecture to manage high-level semantic information. One semantic modeling module maps the shallow features of source images into deep semantic, the other learns the deep semantic information in different receptive fields. The final fused results are recovered from the combination of local feature, long-range dependency and semantic feature. Extensive comparison experiments demonstrate the superiority of SePT compare to other advanced fusion approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323009202",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Deep learning",
      "Electrical engineering",
      "Engineering",
      "Feature (linguistics)",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Semantic Web",
      "Semantic computing",
      "Semantic feature",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Xin"
      },
      {
        "surname": "Huo",
        "given_name": "Hongtao"
      },
      {
        "surname": "Li",
        "given_name": "Chang"
      },
      {
        "surname": "Liu",
        "given_name": "Xiaowen"
      },
      {
        "surname": "Wang",
        "given_name": "Wenxi"
      },
      {
        "surname": "Wang",
        "given_name": "Cheng"
      }
    ]
  },
  {
    "title": "A stochastic model based on Gaussian random fields to characterize the morphology of granular objects",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110255",
    "abstract": "The geometrical modeling of granular objects is a complex challenge that exists in many scientific fields, such as the modeling of granular materials or rocks and coarse aggregates with applications in civil, mechanical, and chemical engineering. In this paper, a model called SPHERE (Stochastic Process for Highly Effective Radial Expansion) is proposed, which is based on the deformation of an ellipsoid mesh using multiple 3D Gaussian random fields. The model is designed to be flexible (full control over 2D and 3D morphological properties of granular objects), ultra-fast (over 1000 aggregates in less than 5 s), and independent of the mesh and base shape used (as long as it is a star-shaped object). The flexibility of the model and its ability to reflect real data is illustrated using images of latex nanoparticle aggregates. Using 2D measurements on images from a morphogranulometer, a method based on the SPHERE model is proposed to estimate the 3D morphological properties of aggregates. A multiscale optimization process is applied, in particular using a partial reconstruction of 2D shapes from elliptic Fourier descriptors, in order to best reproduce the shape, angularity and texture of the aggregates using the SPHERE model. Validation of the method on 3D printed data shows relative errors of less than 3% for all measured 2D and 3D morphological characteristics, and validation on a population of synthetic objects shows relative errors of less than 6%. The results are compared and discussed with those obtained using other models based on overlapping spheres and show consistency with previous work. Finally, suggestions for improvement are given.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324000062",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Astronomy",
      "Biological system",
      "Biology",
      "Composite material",
      "Computer science",
      "Consistency (knowledge bases)",
      "Demography",
      "Ellipsoid",
      "Gaussian",
      "Granular material",
      "Image (mathematics)",
      "Materials science",
      "Mathematics",
      "Physics",
      "Population",
      "Quantum mechanics",
      "Random field",
      "SPHERES",
      "Sociology",
      "Statistical physics",
      "Statistics",
      "Stochastic modelling",
      "Texture (cosmology)"
    ],
    "authors": [
      {
        "surname": "Théodon",
        "given_name": "L."
      },
      {
        "surname": "Coufort-Saudejaud",
        "given_name": "C."
      },
      {
        "surname": "Debayle",
        "given_name": "J."
      }
    ]
  },
  {
    "title": "A simple and efficient filter feature selection method via document-term matrix unitization",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.02.025",
    "abstract": "Text processing tasks commonly grapple with the challenge of high dimensionality. One of the most effective solutions to this challenge is to preprocess text data through feature selection methods. Feature selection can select the most advantageous features for subsequent operations (e.g., classification) from the native feature space of the text. This process effectively trims the feature space’s dimensionality, enhancing subsequent operations’ efficiency and accuracy. This paper proposes a straightforward and efficient filter feature selection method based on document-term matrix unitization (DTMU) for text processing. Diverging from previous filter feature selection methods that concentrate on scoring criteria definition, our method achieves more optimal feature selection by unitizing each column of the document-term matrix. This approach mitigates feature-to-feature influence and reinforces the role of the weighting proportion within the features. Subsequently, our scoring criterion subtracts the sum of weights for negative samples from positive samples and takes the absolute value. We conduct numerical experiments to compare DTMU with four advanced filter feature selection methods: max–min ratio metric, proportional rough feature selector, least loss, and relative discrimination criterion, along with two classical filter feature selection methods: Chi-square and information gain. The experiments are performed on four ten-thousand-dimensional feature space datasets: b o o k , d v d , m u s i c , m o v i e and two thousand-dimensional feature space datasets: i m d b , a m a z o n _ c e l l s , sourced from Amazon product reviews and movie reviews. Experimental findings demonstrate that DTMU selects more advantageous features for subsequent operations and achieves a higher dimensionality reduction rate than those of the other six methods used for comparison. Moreover, DTMU exhibits robust generalization capabilities across various classifiers and dimensional datasets. Notably, the average CPU time for a single run of DTMU is measured at 1.455 s.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524000588",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Composite material",
      "Computer science",
      "Computer vision",
      "Epistemology",
      "Feature (linguistics)",
      "Feature selection",
      "Filter (signal processing)",
      "Linguistics",
      "Materials science",
      "Mathematics",
      "Matrix (chemical analysis)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Selection (genetic algorithm)",
      "Simple (philosophy)",
      "Term (time)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Qing"
      },
      {
        "surname": "Zhao",
        "given_name": "Shuai"
      },
      {
        "surname": "He",
        "given_name": "Tengjiao"
      },
      {
        "surname": "Wen",
        "given_name": "Jinming"
      }
    ]
  },
  {
    "title": "A gated cross-domain collaborative network for underwater object detection",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110222",
    "abstract": "Underwater object detection (UOD) plays a significant role in aquaculture and marine environmental protection. Considering the challenges posed by low contrast and low-light conditions in underwater environments, several underwater image enhancement (UIE) methods have been proposed to improve the quality of underwater images. However, only using the enhanced images does not improve the performance of UOD, since it may unavoidably remove or alter critical patterns and details of underwater objects. In contrast, we believe that exploring the complementary information from the two domains is beneficial for UOD. The raw image preserves the natural characteristics of the scene and texture information of the objects, while the enhanced image improves the visibility of underwater objects. Based on this perspective, we propose a Gated Cross-domain Collaborative Network (GCC-Net) to address the challenges of poor visibility and low contrast in underwater environments, which comprises three dedicated components. Firstly, a real-time UIE method is employed to generate enhanced images, which can improve the visibility of objects in low-contrast areas. Secondly, a cross-domain feature interaction module is introduced to facilitate the interaction and mine complementary information between raw and enhanced image features. Thirdly, to prevent the contamination of unreliable generated results, a gated feature fusion module is proposed to adaptively control the fusion ratio of cross-domain information. Our method presents a new UOD paradigm from the perspective of cross-domain information interaction and fusion. Experimental results demonstrate that the proposed GCC-Net achieves state-of-the-art performance on four underwater datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323009196",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Contrast (vision)",
      "Domain (mathematical analysis)",
      "Feature (linguistics)",
      "Feature extraction",
      "Geography",
      "Image (mathematics)",
      "Image fusion",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Meteorology",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Perspective (graphical)",
      "Philosophy",
      "Underwater",
      "Visibility"
    ],
    "authors": [
      {
        "surname": "Dai",
        "given_name": "Linhui"
      },
      {
        "surname": "Liu",
        "given_name": "Hong"
      },
      {
        "surname": "Song",
        "given_name": "Pinhao"
      },
      {
        "surname": "Liu",
        "given_name": "Mengyuan"
      }
    ]
  },
  {
    "title": "SCGTracker: Spatio-temporal correlation and graph neural networks for multiple object tracking",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110249",
    "abstract": "Multiple Object Tracking is an important vitally important fundamental task in computer vision. Visual tracking becomes challenging when objects move in groups and are obscured from each other. There are two mainstream solution strategies for these group models. One is to transform the data association problem into a graph matching problem for solving, while the other is to apply the social power model as an advanced constraint for group tracking. In the former case, the solving difficulty geometric growth as the number of tracked objects increases, and the computing efficiency for real-time tracking demand cannot be met. The latter strategy tends to set up fixed-size groups or offline training rules, resulting in a lack of flexibility that limits their scenario generalization. According to the shortcomings of existing methods, this paper proposes a novel multiple object tracking method with spatio-temporal correlation and graph neural networks. Firstly, the relational features of the historical trajectories are extracted through the spatio-temporal relationship learning module, which models the spatio-temporal correlations of the objects and dynamically constructs the group structure online. Then, the graph neural network is combined with appearance and motion information, and the similarity between each detection and tracklet is used as a weight in node feature aggregation to make powerful distinctions between node features. Meanwhile, the spatio-temporal correlation method is also used to solve target loss issues caused by occlusion. Even collocated with linearly assigned data association method, good tracking results are still achieved, with a low computational complexity. Experiments on three challenging public datasets, namely MOT16, MOT17, and MOT20, validated the accuracy and efficiency of the proposed tracking method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323009469",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer vision",
      "Correlation",
      "Geometry",
      "Graph",
      "Image (mathematics)",
      "Matching (statistics)",
      "Mathematics",
      "Motion blur",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Statistics",
      "Theoretical computer science",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Yajuan"
      },
      {
        "surname": "Liang",
        "given_name": "Yongquan"
      },
      {
        "surname": "Leng",
        "given_name": "Jiaxu"
      },
      {
        "surname": "Wang",
        "given_name": "Zhihui"
      }
    ]
  },
  {
    "title": "Learning self-target knowledge for few-shot segmentation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110266",
    "abstract": "Few-shot semantic segmentation uses a few annotated data of a specific class in the support set to segment the target of the same class in the query set. Most existing approaches fail to perform well when there are significant intra-class variances. This paper alleviates the problem by concentrating on mining the query image and using the support set as supplementary information. First, it proposes a Query Prototype Generation Module to generate a query foreground prototype from the query features. Specifically, we use both prototype-level and pixel-level similarity matching to generate two complementary initial prototypes, which we then integrate to create a discriminative query foreground prototype. Second, we propose a Support Auxiliary Refinement Module to further guide the final precise prediction of the query image by leveraging the target category information of the support set through step-by-step mining. Specifically, we generate a query-support mixture prototype based on the support prototype representation obtained using the attention mechanism. Then we generate a support supplement prototype to complement the missing information by encoding over the foreground regions that the query-support mixture prototype fails to segment out. Extensive experiments on PASCAL- 5 i and COCO- 2 0 i demonstrate that our model outperforms the prior works of few-shot segmentation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324000177",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Data mining",
      "Discriminative model",
      "Information retrieval",
      "Matching (statistics)",
      "Mathematics",
      "Pascal (unit)",
      "Pattern recognition (psychology)",
      "Programming language",
      "Segmentation",
      "Set (abstract data type)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Yadang"
      },
      {
        "surname": "Chen",
        "given_name": "Sihan"
      },
      {
        "surname": "Yang",
        "given_name": "Zhi-Xin"
      },
      {
        "surname": "Wu",
        "given_name": "Enhua"
      }
    ]
  },
  {
    "title": "Multimodal multiscale dynamic graph convolution networks for stock price prediction",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110211",
    "abstract": "Predicting directional future stock price movements is very challenging due to the complex, stochastic, and evolving nature of the financial markets. Existing literature either neglects other timely and granular alternative data, such as media text data, or fails to extract and distill predictive multimodal features from the data. Moreover, the time-varying cross-sectional relations beyond sequential dependencies of stock prices are informative for forecasting price fluctuations, for which the modelling flexibility, however, is not adequate in most of the previous studies. In this paper, we propose a novel Multiscale Multimodal Dynamic Graph Convolution Network (Melody-GCN) to address these issues in stock price prediction. It contains three core modules: (1) multimodal fusing-diffusing blocks that effectively integrate and align the numerical and textual features; (2) a multiscale architecture that extracts and refines temporal features via a fine-to-coarse descending path and a coarse-to-fine ascending path progressively; and (3) dynamic spatio-temporal graph convolutional layers that learn the complex and evolving stock relations not only in between industries and individual companies but also across time horizons. Extensive experimental results and trading simulations on two real-world datasets demonstrate the superior performance of our proposed approach beyond other state-of-the-art models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323009081",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Econometrics",
      "Engineering",
      "Graph",
      "Machine learning",
      "Mathematics",
      "Mechanical engineering",
      "Stock (firearms)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Ruirui"
      },
      {
        "surname": "Liu",
        "given_name": "Haoxian"
      },
      {
        "surname": "Huang",
        "given_name": "Huichou"
      },
      {
        "surname": "Song",
        "given_name": "Bo"
      },
      {
        "surname": "Wu",
        "given_name": "Qingyao"
      }
    ]
  },
  {
    "title": "mEBAL2 database and benchmark: Image-based multispectral eyeblink detection",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.04.011",
    "abstract": "This work introduces a new multispectral database and framework to train and evaluate eyeblink detection in RGB and Near-Infrared (NIR). Our contributed dataset (mEBAL2, multimodal EyeBlink and Attention Level estimation, Version 2) is the largest existing eyeblink database, representing a great opportunity to improve data-driven multispectral approaches for blink detection and related applications (e.g., attention level estimation). mEBAL2 includes 21,100 image sequences from 180 different students (more than 2 million labeled images in total) while conducting a number of e-learning tasks of varying difficulty or taking a real course on HTML initiation through the edX MOOC platform. mEBAL2 uses multiple sensors, including two Near-Infrared (NIR) and one RGB camera to capture facial gestures during the execution of the tasks, as well as an Electroencephalogram (EEG) band to get the cognitive activity of the user and blinking events. Furthermore, this work proposes 3 data-driven approaches as benchmarks for blink detection on mEBAL2, where the architecture based on Convolutional Long Short-Term Memory (ConvLSTM) achieved performances of up to 99%. The experiments explored whether combining RGB and NIR spectrum data improves blink detection in training and architectures that merge both types of data. Experiments showed that the NIR spectrum enhances results, even when only RGB images are available during inference. Finally, the generalization capacity of the proposed eyeblink detectors, along with state-of-the-art eyeblink detection implementations, is validated in wilder and more challenging environments like the HUST-LEBW dataset to show the usefulness of mEBAL2 to train a new generation of data-driven approaches for eyeblink detection.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524001120",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Geodesy",
      "Geography",
      "Multispectral image",
      "Pattern recognition (psychology)",
      "RGB color model"
    ],
    "authors": [
      {
        "surname": "Daza",
        "given_name": "Roberto"
      },
      {
        "surname": "Morales",
        "given_name": "Aythami"
      },
      {
        "surname": "Fierrez",
        "given_name": "Julian"
      },
      {
        "surname": "Tolosana",
        "given_name": "Ruben"
      },
      {
        "surname": "Vera-Rodriguez",
        "given_name": "Ruben"
      }
    ]
  },
  {
    "title": "Towards better small object detection in UAV scenes: Aggregating more object-oriented information",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.04.002",
    "abstract": "Security, transportation, and rescue applications require fully analyzing the visual data interpretation via drone platforms. While various aspects of object detection research are expanding at a rapid pace, the detection of small objects in drone platforms continues to pose significant challenges. Specifically, targets in drone-captured scenarios are notoriously hard to detect due to factors such as low resolution, feature indistinguishability, occlusion, scale variation, among others. To address this, our work provides abundant object-oriented information to enhance the recognition ability of small objects in large scale and is structured as follows: Firstly, we devise a method for small object detection in UAV captured scenes, incorporating enhanced object-oriented information. This involves an improvement of the fundamental convolutional feature transformation to yield more discriminative contexts for small objects. Secondly, we represent the input tokens of the vision Multilayer Perceptron (MLP) as a wave function. In order to acquire more effective global representation, we compute the amplitude and phase of these tokens using a local-maximum approach, facilitating dynamic aggregation tailored to the object’s unique semantic information. Lastly, we propose a fusion method transitioning from local to global, devised for comprehensive learning of object features. Experimental evidence substantiates the efficacy of our model, achieving a mean Average Precision (mAP) of 30.6% on the VisDrone dataset. This precision, maintained with consistent input size, outperforms other state-of-the-art methods, underscoring our model’s reliability for drone-captured small object detection.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552400103X",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Discriminative model",
      "Drone",
      "Feature (linguistics)",
      "Genetics",
      "Law",
      "Linguistics",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Political science",
      "Politics",
      "Quantum mechanics",
      "Representation (politics)",
      "Scale (ratio)"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Chenyue"
      },
      {
        "surname": "Cao",
        "given_name": "Yichao"
      },
      {
        "surname": "Lu",
        "given_name": "Xiaobo"
      }
    ]
  },
  {
    "title": "Spatiotemporal Progressive Inward-Outward Aggregation Network for skeleton-based action recognition",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110262",
    "abstract": "Previous works have realized that spatio-temporal entanglement features can not be ignored in skeleton-based motion recognition tasks, then they have not broken away from the barriers of traditional GCN (The entanglement feature is still modeled by the extended single-frame adjacency matrix). We introduce a new joint-correlations determination mechanism that uses a non-linear transformation of the distance between joints in multiple frames to construct the connection relationship. The proposed method results in improved accuracy while significantly reducing the number of parameters. Meanwhile, recent works have alleviated the problem of most actions being only related to the dynamic characteristics of local joints by aggregating features of different parts of the human body in parallel, while interacting with different features still remains at a lower level of concatenation or addition. We propose a progressive inward-outward structure (PIS) that allows joint features corresponding to the action to be extracted while taking into account the lightweight link between this part of the joints and the rest. Integrating the above two designs, we propose a Spatiotemporal Progressive Inward-Outward Aggregation Network (SPIANet) to model the complex spatiotemporal entanglement between joints in the process of human motion, which is validated on three public datasets (NTU-RGB+D60, NTU-RGB+D120, and UESTC varying-view) and outperforms state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032400013X",
    "keywords": [
      "Action (physics)",
      "Adjacency matrix",
      "Algorithm",
      "Architectural engineering",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Classical mechanics",
      "Combinatorics",
      "Computer science",
      "Concatenation (mathematics)",
      "Construct (python library)",
      "Engineering",
      "Feature (linguistics)",
      "Frame (networking)",
      "Gene",
      "Graph",
      "Joint (building)",
      "Kinematics",
      "Linguistics",
      "Mathematics",
      "Motion (physics)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Process (computing)",
      "Programming language",
      "Quantum mechanics",
      "RGB color model",
      "Skeleton (computer programming)",
      "Telecommunications",
      "Theoretical computer science",
      "Transformation (genetics)",
      "Transformation matrix"
    ],
    "authors": [
      {
        "surname": "Yin",
        "given_name": "Xinpeng"
      },
      {
        "surname": "Zhong",
        "given_name": "Jianqi"
      },
      {
        "surname": "Lian",
        "given_name": "Deliang"
      },
      {
        "surname": "Cao",
        "given_name": "Wenming"
      }
    ]
  },
  {
    "title": "RCsearcher: Reaction center identification in retrosynthesis via deep Q-learning",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110318",
    "abstract": "The reaction center consists of atoms in the product with local properties that differ from those in the reactants. Previous studies focused on identifying the reaction center using semi-templated retrosynthesis methods, which are limited to single reaction center identification. In reality, however, many reaction centers involve multiple bonds or atoms, referred to as multiple reaction centers. This paper introduces RCsearcher, a unified framework that exploits the strengths of graph neural networks and deep reinforcement learning for identifying both single and multiple reaction centers. The key insight of the framework is that the single or multiple reaction center must be a node-induced subgraph of the molecular product graph. At each step, RCsearcher selects a node in the molecular product graph and adds it to the explored node-induced subgraph as an action. Comprehensive experiments demonstrate that RCsearcher consistently outperforms other baselines, and is able to identify reaction center patterns not present in the training set. Ablation experiments confirm the effectiveness of individual components of RCsearcher, including the beam search and the one-hop constraint of the action space.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324000694",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Center (category theory)",
      "Chemistry",
      "Computer science",
      "Crystallography",
      "Identification (biology)",
      "Pattern recognition (psychology)",
      "Retrosynthetic analysis",
      "Stereochemistry",
      "Total synthesis"
    ],
    "authors": [
      {
        "surname": "Lan",
        "given_name": "Zixun"
      },
      {
        "surname": "Zeng",
        "given_name": "Zuo"
      },
      {
        "surname": "Hong",
        "given_name": "Binjie"
      },
      {
        "surname": "Liu",
        "given_name": "Zhenfu"
      },
      {
        "surname": "Ma",
        "given_name": "Fei"
      }
    ]
  },
  {
    "title": "Generalized and robust model for GAN-generated image detection",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.04.018",
    "abstract": "Generative adversarial network (GAN)-generated image detection is a crucial yet challenging problem. The advancement of diverse generative models makes it simpler to build realistic synthetic images, but it also increases the likelihood of using images maliciously. Recent techniques of fake image detection often overfit to specific GANs, incapable of generalizing to other GANs. Each GAN leaves its distinct fingerprint on the images it generates. We propose a simple and compact attention network that exploits high-frequency and noise residual patterns of images. We design two-channel image representations in the preprocessing phase, with one channel using a bilateral high pass filter (BiHPF) for high-frequency patterns and the other using photo response non-uniformity (PRNU) for noise residuals capturing from GAN-generated and real images. The feature maps pass through the attention network with a dense layer for classification tasks. We assess the generalizability and robustness of the model through extensive experiments. The model is evaluated on a benchmark dataset in various settings, including cross-category, cross-GAN models with varying class settings and different color manipulations. This study demonstrates that the proposed model outperforms state-of-the-art (SOTA) methods by approximately 5% in different experiment settings.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524001193",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Image (mathematics)",
      "Image processing",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Raj",
        "given_name": "Surbhi"
      },
      {
        "surname": "Mathew",
        "given_name": "Jimson"
      },
      {
        "surname": "Mondal",
        "given_name": "Arijit"
      }
    ]
  },
  {
    "title": "Bidirectional feature learning network for RGB-D salient object detection",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110304",
    "abstract": "RGB-D salient object detection aims to perform the pixel-wise localization of salient objects from both RGB and depth images, whose challenge mainly comes from how to learn complementary features from each modality. Existing works often use increasingly large models for performance enhancement, which need large memory and time consumption in practice. In this paper, we propose a simple yet effective Bidirectional Feature Learning Network (BFLNet) for RGB-D salient object detection under limited memory and time conditions. To achieve accurate performance with lightweight backbone networks, an effective Bidirectional Feature Fusion (BFF) module is designed to merge features from both RGB and depth streams, in which the cross-modal fusions and cross-scale fusions are jointly conducted to fuse the immediate features in multiple scales and multiple modals. What is more, a simple Dual Consistency Loss (DCL) function is designed to prompt cross-modal fusion by keeping the consistency between cross-modal target predictions. Extensive experiments on four benchmark datasets demonstrate that our method has achieved the state-of-the-art performance with high efficiency in RGB-D salient object detection. Code will be available at https://github.com/nightsky-nostar/BFLNet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324000554",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Feature learning",
      "Linguistics",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Philosophy",
      "RGB color model",
      "Salient"
    ],
    "authors": [
      {
        "surname": "Niu",
        "given_name": "Ye"
      },
      {
        "surname": "Zhou",
        "given_name": "Sanping"
      },
      {
        "surname": "Dong",
        "given_name": "Yonghao"
      },
      {
        "surname": "Wang",
        "given_name": "Le"
      },
      {
        "surname": "Wang",
        "given_name": "Jinjun"
      },
      {
        "surname": "Zheng",
        "given_name": "Nanning"
      }
    ]
  },
  {
    "title": "MHSAN: Multi-view hierarchical self-attention network for 3D shape recognition",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110315",
    "abstract": "Multi-view learning has demonstrated promising performance for 3D shape recognition. However, existing multi-view methods usually focus on fusing multiple views and ignore the structural and discriminative information carried by 2D views. In this paper, we propose a multi-view hierarchical self-attention network (MHSAN) to explore the geometric and discriminative information from complex 2D views. Specifically, MHSAN consists of two self-attention networks. First, a global self-attention network is adopted to exploit the structure information by embedding position information of views. Then, the discriminative self-attention network learns discriminative information from the views with high classification scores. Through the proposed MHSAN, the geometric and discriminative information is condensed as the novel representation of 3D shapes. To validate the effectiveness of our proposed method, extensive experiments have been conducted on three 3D shape benchmarks. Experimental results demonstrate that our method is generally superior to the state-of-the-art methods in 3D shape classification and retrieval tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324000669",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Discriminative model",
      "Embedding",
      "Exploit",
      "Focus (optics)",
      "Law",
      "Machine learning",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Political science",
      "Politics",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Cao",
        "given_name": "Jiangzhong"
      },
      {
        "surname": "Yu",
        "given_name": "Lianggeng"
      },
      {
        "surname": "Ling",
        "given_name": "Bingo Wing-Kuen"
      },
      {
        "surname": "Yao",
        "given_name": "Zijie"
      },
      {
        "surname": "Dai",
        "given_name": "Qingyun"
      }
    ]
  },
  {
    "title": "Pose-robust personalized facial expression recognition through unsupervised multi-source domain adaptation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110311",
    "abstract": "Pose-robust personalized facial expression recognition is rather challenging, as facial expressions are subject-related and pose-dependent. Multi-source domain adaptation tries to leverage knowledge from multiple source domains to boost the performance of the target domain. For pose-robust personalized facial expression recognition, the images of the source domain are from multiple sources since the images are under different poses. Thus, in this paper, we propose a novel unsupervised multi-source domain adaptation framework for pose-robust personalized facial expression recognition. The proposed framework consists of five components: a source encoder, a target encoder, an expression classifier, a view discriminator, and a domain discriminator. The source encoder and target encoder learn facial representations from facial images in the training and testing sets, respectively. The expression classifier recognizes expressions from the learned representations. The view discriminator classifies poses. The domain discriminator distinguishes the learned representations of the source domain from those of the target domain. The source encoder works cooperatively with the expression classifier and adversarially with the view discriminator. The target encoder aims to learn domain robust representations and fool the domain discriminator, while the domain discriminator tries to distinguish between the source and target domains. Through adversarial learning, the distribution of the learned representations from the source domain converges to that from the target domain. Thus, the feature representation extracted by the target encoder is pose-invariant and target subject-specific. Experimental results demonstrate the superiority of the proposed method compared to related works.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324000621",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Computer vision",
      "Detector",
      "Discriminator",
      "Encoder",
      "Facial expression",
      "Operating system",
      "Pattern recognition (psychology)",
      "Speech recognition",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Shangfei"
      },
      {
        "surname": "Chang",
        "given_name": "Yanan"
      },
      {
        "surname": "Li",
        "given_name": "Qiong"
      },
      {
        "surname": "Wang",
        "given_name": "Can"
      },
      {
        "surname": "Li",
        "given_name": "Guoming"
      },
      {
        "surname": "Mao",
        "given_name": "Meng"
      }
    ]
  },
  {
    "title": "IDC-Net: Breast cancer classification network based on BI-RADS 4",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110323",
    "abstract": "In the diagnosis of breast cancer, the 3 sub-categories 4a-4c of BI-RADS 4 are of great significance to doctors. However, low resolution of ultrasound image and high similarity between different category images pose great challenges to this task, which requires the network to be more capable of extracting image features. Therefore, in response to the efficient classification of BI-RADS 4a-4c in breast ultrasound images, we developed a lightweight classification network IDCNet, a neural network model combining the advantages of convolutional neural network(CNN) and CapsNet. In this model: Firstly, we proposed ID-Net based on CNN architecture and mainly constructed by ID block and DD block, which ensure the ID-Net deep and wide enough to extract sufficient local semantic information of image, and at the same time being lightweight. Secondly, we use the CapsNet to learn the position and posture information between the global features of the image, which makes up for the defects of CNN. Finally, two parallel paths of IDCNet and CapsNet are fused to enhance IDCNet's capability of feature extraction. To verify our method, experiments have been conducted on the breast ultrasound dataset of Yunnan cancer hospital and two public datasets. The classification results of our method have been compared with those obtained by five existing approaches. The experimental results show that the proposed method IDCNet has the highest Accuracy (98.54 %), Precision (98.54 %) and F1 score (98.54 %).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324000748",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Block (permutation group theory)",
      "Breast cancer",
      "Breast ultrasound",
      "Cancer",
      "Computer science",
      "Contextual image classification",
      "Convolutional neural network",
      "Deep learning",
      "Feature (linguistics)",
      "Feature extraction",
      "Geometry",
      "Image (mathematics)",
      "Internal medicine",
      "Linguistics",
      "Mammography",
      "Mathematics",
      "Medicine",
      "Net (polyhedron)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Yi",
        "given_name": "Sanli"
      },
      {
        "surname": "Chen",
        "given_name": "Ziyan"
      },
      {
        "surname": "She",
        "given_name": "Furong"
      },
      {
        "surname": "Wang",
        "given_name": "Tianwei"
      },
      {
        "surname": "Yang",
        "given_name": "Xuelian"
      },
      {
        "surname": "Chen",
        "given_name": "Dong"
      },
      {
        "surname": "Luo",
        "given_name": "Xiaomao"
      }
    ]
  },
  {
    "title": "GPONet: A two-stream gated progressive optimization network for salient object detection",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110330",
    "abstract": "The salient object detection task is to locate and detect salient regions in images, which is widely applied in various fields. In this paper, we propose a gated progressive optimization network (GPONet) for salient object detection. Firstly, to extract salient regions more accurately, we design a multi-level feature fusion module with a gating mechanism named gate fusion network (GFN). GFN focuses on the semantic information of high-level features as well as the detailed information of low-level features, enabling purposeful delivery of high-level features to low-level features. The gate fusion unit (GFU) is also able to maintain valid information and suppress redundant information in the fusion process. Secondly, while some existing methods have shown that the additional edge supervision can facilitate salient object detection, edge pixels are often much less common than non-edge pixels, leading to the challenge of class imbalance. To overcome this issue, we introduce detail labels that provide additional internal details as a supplementary supervisory signal. Combining these labels with proposed detail perception loss (DPL) enables our network to learn edge information of salient objects more effectively. To complement each other and guide information exchange between the two branches, we propose a cross guide module (CGM) to control the information flow transfer between them. Finally, we develop a simple and efficient attention fusion strategy to merge the prediction maps of the two branches to generate the final salient prediction map. Extensive experimental results validate that our method reaches optimal or comparable performance on several mainstream datasets. The code of GPONet is available from https://github.com/antonie-z/GPONet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324000815",
    "keywords": [
      "Artificial intelligence",
      "Backbone network",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Enhanced Data Rates for GSM Evolution",
      "Feature (linguistics)",
      "Linguistics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pixel",
      "Process (computing)",
      "Salient"
    ],
    "authors": [
      {
        "surname": "Yi",
        "given_name": "Yugen"
      },
      {
        "surname": "Zhang",
        "given_name": "Ningyi"
      },
      {
        "surname": "Zhou",
        "given_name": "Wei"
      },
      {
        "surname": "Shi",
        "given_name": "Yanjiao"
      },
      {
        "surname": "Xie",
        "given_name": "Gengsheng"
      },
      {
        "surname": "Wang",
        "given_name": "Jianzhong"
      }
    ]
  },
  {
    "title": "Leveraging spatio-temporal features using graph neural networks for human activity recognition",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110301",
    "abstract": "Unsupervised human activity recognition (HAR) algorithms working on motion capture (mocap) data often use spatial information and neglect the activity-specific information contained in the temporal sequences. In this work, we propose a new unsupervised algorithm for HAR from mocap data to leverage both spatial and temporal information embedded in activity sequences. For this, we employ a shallow graph neural network (GNN) comprising a graph convolutional network and a gated recurrent unit to aggregate the spatial and temporal features of the mocap sequences, respectively. Moreover, we encode the transformations of the human body through log-regularized kernel covariance descriptors linked to the trajectory movement maps of mocap frames. These descriptors are then fused with the GNN features for downstream activity recognition tasks. Finally, HAR is performed by a new unsupervised algorithm using a neighborhood Laplacian regularizer and a normalized dictionary learning approach. The generalizability of the proposed model is validated by training the GNN on a public dataset and testing on the other datasets. The performance of the proposed model is evaluated using six publicly available human mocap datasets. Compared to existing approaches, the proposed model improves activity recognition consistently by 12%–30% across different datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324000529",
    "keywords": [
      "Activity recognition",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Graph",
      "Pattern recognition (psychology)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Raj",
        "given_name": "M.S. Subodh"
      },
      {
        "surname": "George",
        "given_name": "Sudhish N."
      },
      {
        "surname": "Raja",
        "given_name": "Kiran"
      }
    ]
  },
  {
    "title": "OLCH: Online Label Consistent Hashing for streaming cross-modal retrieval",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110335",
    "abstract": "Cross-modal hashing has received growing interest to facilitating efficient retrieval across large-scale multi-modal data, and existing methods still face three challenges: 1) Most offline learning works are unsuitable for processing and training with streaming multi-modal data. 2) Current online learning methods rarely consider the potential interdependency between the label categories. 3) Existing supervised methods often utilize pairwise label similarities or adopt relaxation scheme to learn hash codes, which, respectively, require much computation time or accumulate large quantization loss during the learning process. To alleviate these challenges, this paper presents an efficient Online Label Consistent Hashing (OLCH) approach for streaming cross-modal retrieval. The proposed approach first exploits the relative similarity of semantic labels and utilizes the multi-class classification to derive the common semantic vector. Then, an online semantic representation learning framework is adaptively designed to preserve the semantic similarity across different modalities, and a mini-batch online gradient descent approach associated with forward–backward splitting is developed to discriminatively optimize the hash functions. Accordingly, the hash codes are incrementally learned with high discriminative capability, while avoiding high computation complexity to process the streaming data. Extensive experiments highlight the superiority of the proposed approach and show its very competitive performance in comparison with the state-of-the-arts.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324000864",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Computer security",
      "Hash function",
      "Information retrieval",
      "Modal",
      "Polymer chemistry"
    ],
    "authors": [
      {
        "surname": "Peng",
        "given_name": "Shu-Juan"
      },
      {
        "surname": "Yi",
        "given_name": "Jinhan"
      },
      {
        "surname": "Liu",
        "given_name": "Xin"
      },
      {
        "surname": "Cheung",
        "given_name": "Yiu-ming"
      },
      {
        "surname": "Cui",
        "given_name": "Zhen"
      },
      {
        "surname": "Li",
        "given_name": "Taihao"
      }
    ]
  },
  {
    "title": "ABAE: Auxiliary Balanced AutoEncoder for class-imbalanced semi-supervised learning",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.04.004",
    "abstract": "Semi-supervised learning has achieved extraordinary success in prevalent image-classification benchmarks. However, a class-balanced distribution that differs notably from real-world data distribution is required. In general, models trained under class-imbalanced semi-supervised learning conditions are severely biased towards the majority classes. To address this issue, we propose a novel framework called ABAE by implanting an Auxiliary Balanced AutoEncoder branch into existing semi-supervised learning algorithms. Considering that adaptive feature augmentation for different classes can alleviate confirmation bias, we devise a class-aware reconstruction loss to train the AutoEncoder module. To further smooth the output, we adopt a graph-based label propagation scheme at the end of the AutoEncoder. Extensive experiments on CIFAR-10/100-LT, SVHN-LT and Small ImageNet-127 demonstrate the effectiveness of ABAE.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524001053",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Autoencoder",
      "Class (philosophy)",
      "Computer science",
      "Deep learning",
      "Feature (linguistics)",
      "Feature learning",
      "Graph",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Scheme (mathematics)",
      "Supervised learning",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Tang",
        "given_name": "Qianying"
      },
      {
        "surname": "Wei",
        "given_name": "Xiang"
      },
      {
        "surname": "Su",
        "given_name": "Qi"
      },
      {
        "surname": "Zhang",
        "given_name": "Shunli"
      }
    ]
  },
  {
    "title": "EmoComicNet: A multi-task model for comic emotion recognition",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110261",
    "abstract": "The emotion and sentiment associated with comic scenes can provide potential information for inferring the context of comic stories, which is an essential pre-requisite for developing comics’ automatic content understanding tools. Here, we address this open area of comic research by exploiting the multi-modal nature of comics. The general assumptions for multi-modal sentiment analysis methods are that both image and text modalities are always present at the test phase. However, this assumption is not always satisfied for comics since comic characters’ facial expressions, gestures, etc., are not always clearly visible. Also, the dialogues between comic characters are often challenging to comprehend the underlying context. To deal with these constraints of comic emotion analysis, we propose a multi-task-based framework, namely EmoComicNet, to fuse multi-modal information (i.e., both image and text) if it is available. However, the proposed EmoComicNet is designed to perform even when any modality is weak or completely missing. The proposed method potentially improves the overall performance. Besides, EmoComicNet can also deal with the problem of weak or absent modality during the training phase.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324000128",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Chemistry",
      "Comics",
      "Computer science",
      "Context (archaeology)",
      "Engineering",
      "Gesture",
      "Modal",
      "Modalities",
      "Modality (human–computer interaction)",
      "Natural language processing",
      "Paleontology",
      "Polymer chemistry",
      "Social science",
      "Sociology",
      "Systems engineering",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Dutta",
        "given_name": "Arpita"
      },
      {
        "surname": "Biswas",
        "given_name": "Samit"
      },
      {
        "surname": "Das",
        "given_name": "Amit Kumar"
      }
    ]
  },
  {
    "title": "Transferable graph auto-encoders for cross-network node classification",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110334",
    "abstract": "Node classification is a popular and challenging task in graph neural networks, and existing approaches are mainly developed for a single network. With the advances in domain adaptation, researchers tend to leverage knowledge extracted from a fully-labeled source network to further improve the node classification performance in an unlabeled target network. This learning paradigm refers to cross-network node classification, which is the topic we studied in this paper. Specifically, we propose a novel model named Transferable Graph Auto-Encoders (TGAE), which first encodes the initial network data into latent representations and then decodes the learned features to preserve graph information. In the encoding phase, TGAE adopts the attentional mechanism to fuse the local and global information of nodes to discover latent node representations. To obtain transferable features between the source and target networks, TGAE aligns their distributions based on the learned representations by reducing marginal and conditional distribution differences. In the decoding phase, the latent representations are subjected to pairwise and reconstruction constraints, thus preserving structural proximity and graph topology information to learn discriminative features. Besides, a node classifier is trained to enhance the discriminant of the node representations further. Experimental results on several real-world datasets demonstrate that the proposed model achieves state-of-the-art performance in cross-network node classification tasks compared with existing methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324000852",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Cross-validation",
      "Encoder",
      "Engineering",
      "Graph",
      "Node (physics)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Structural engineering",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Hanrui"
      },
      {
        "surname": "Tian",
        "given_name": "Lei"
      },
      {
        "surname": "Wu",
        "given_name": "Yanxin"
      },
      {
        "surname": "Zhang",
        "given_name": "Jia"
      },
      {
        "surname": "Ng",
        "given_name": "Michael K."
      },
      {
        "surname": "Long",
        "given_name": "Jinyi"
      }
    ]
  },
  {
    "title": "Time to retire F1-binary score for action unit detection",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.04.016",
    "abstract": "Detecting action units is an important task in face analysis, especially in facial expression recognition. This is due, in part, to the idea that expressions can be decomposed into multiple action units. To evaluate systems that detect action units, F1-binary score is often used as the evaluation metric. In this paper, we argue that F1-binary score does not reliably evaluate these models due largely to class imbalance. Because of this, F1-binary score should be retired and a suitable replacement should be used. We justify this argument through a detailed evaluation of the negative influence of class imbalance on action unit detection. This includes an investigation into the influence of class imbalance in train and test sets and in new data (i.e., generalizability). We empirically show that F1-micro should be used as the replacement for F1-binary.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552400117X",
    "keywords": [
      "Action (physics)",
      "Arithmetic",
      "Artificial intelligence",
      "Binary classification",
      "Binary number",
      "Class (philosophy)",
      "Computer science",
      "Economics",
      "Generalizability theory",
      "Management",
      "Mathematics",
      "Metric (unit)",
      "Operations management",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Statistics",
      "Support vector machine",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Hinduja",
        "given_name": "Saurabh"
      },
      {
        "surname": "Nourivandi",
        "given_name": "Tara"
      },
      {
        "surname": "Cohn",
        "given_name": "Jeffrey F."
      },
      {
        "surname": "Canavan",
        "given_name": "Shaun"
      }
    ]
  },
  {
    "title": "Local topology similarity guided probabilistic sampling for mismatch removal",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110293",
    "abstract": "Feature point matching between two images is a fundamental and important process in machine vision. In many cases, mismatches are inevitable, and removing mismatches is an indispensable task. The existing methods attempt to find comprehensive constraints or sampling model to achieve better performance, which results in the increasingly complexity and may cause the weakness of the generality and scalability. To address this issue, a method called Local Topology similarity guided probabilistic Sampling consensus (LTS) is proposed. It constructs a topological network, then quantifies the mismatch probability in a concise approach based on comparing the topological relationship with neighbourhoods. Then, it detects and removes the mismatches by sampling guided by the mismatch probability. Compared with the state-of-the-art methods, LTS has an excellent performance in accuracy and robustness.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032400044X",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Database",
      "Filter (signal processing)",
      "Gene",
      "Generality",
      "Image (mathematics)",
      "Machine learning",
      "Matching (statistics)",
      "Mathematics",
      "Network topology",
      "Operating system",
      "Pattern recognition (psychology)",
      "Probabilistic logic",
      "Psychology",
      "Psychotherapist",
      "Robustness (evolution)",
      "Sampling (signal processing)",
      "Scalability",
      "Similarity (geometry)",
      "Statistics",
      "Topology (electrical circuits)"
    ],
    "authors": [
      {
        "surname": "He",
        "given_name": "Zaixing"
      },
      {
        "surname": "Shen",
        "given_name": "Chentao"
      },
      {
        "surname": "Zhao",
        "given_name": "Xinyue"
      }
    ]
  },
  {
    "title": "Integrating foreground–background feature distillation and contrastive feature learning for ultra-fine-grained visual classification",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110339",
    "abstract": "In pattern recognition, ultra-fine-grained visual classification (ultra-FGVC) has emerged as a paramount challenge, focusing on sub-category distinction within fine-grained objects. The near-indistinguishable similarities among such objects, combined with the dearth of sample data, intensify this challenge. In response, our FDCL-DA method is introduced, which integrates Foreground–background feature Distillation (FD) and Contrastive feature Learning (CL) with Dual Augmentation (DA). This method uses two different data augmentation techniques, standard and auxiliary augmentation, to enhance model performance and generalization ability. The FD module reduces superfluous features and augments the contrast between the principal entity and its backdrop, while the CL focuses on creating unique data imprints by reducing intra-class resemblances and enhancing inter-class disparities. Integrating this method with different architectures, such as ResNet-50, Vision Transformer, and Swin-Transformer (Swin-T), significantly improves these backbone networks, especially when used with Swin-T, leading to promising results on eight popular datasets for ultra-FGVC tasks. 1 1 The code is available at https://github.com/qpuchen/FDCL-DA.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324000906",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Electrical engineering",
      "Engineering",
      "Feature (linguistics)",
      "Feature extraction",
      "Generalization",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Qiupu"
      },
      {
        "surname": "Jiao",
        "given_name": "Lin"
      },
      {
        "surname": "Wang",
        "given_name": "Fenmei"
      },
      {
        "surname": "Du",
        "given_name": "Jianming"
      },
      {
        "surname": "Liu",
        "given_name": "Haiyun"
      },
      {
        "surname": "Wang",
        "given_name": "Xue"
      },
      {
        "surname": "Wang",
        "given_name": "Rujing"
      }
    ]
  },
  {
    "title": "Discovering causally invariant features for out-of-distribution generalization",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110338",
    "abstract": "Out-of-distribution (OOD) generalization aims to generalize a model trained on source domains to unseen target domains. Recently, causality-based generalization methods have focused on learning invariant causal relationships around the label variable, as causal mechanisms are robust across different domains. However, these methods would yield an inaccurate causal variable set due to the lack of heterogeneous domain data or a prior causal structure, which severely weakens their generalization capacity. To address this problem, we propose a Causally Invariant Features Discovery (CIFD) framework, which combines causal structure discovery and causal effect estimation for selecting a high-quality causal variable set and realizing better OOD generalization. Specifically, CIFD first identifies all potential causal variables by learning a double-layer-based local causal structure around the label variable. Secondly, CIFD uses a double-layer causal effect estimator for estimating the causality of potential causal variables and obtaining true causal variables. The comprehensive experiments on both regression and classification tasks clearly demonstrate the superiority of our framework over the state-of-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032400089X",
    "keywords": [
      "Artificial intelligence",
      "Causal model",
      "Causal structure",
      "Causality (physics)",
      "Computer science",
      "Estimator",
      "Generalization",
      "Invariant (physics)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematical physics",
      "Mathematics",
      "Physics",
      "Programming language",
      "Quantum mechanics",
      "Set (abstract data type)",
      "Statistics",
      "Variable (mathematics)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Yujie"
      },
      {
        "surname": "Yu",
        "given_name": "Kui"
      },
      {
        "surname": "Xiang",
        "given_name": "Guodu"
      },
      {
        "surname": "Cao",
        "given_name": "Fuyuan"
      },
      {
        "surname": "Liang",
        "given_name": "Jiye"
      }
    ]
  },
  {
    "title": "Fast and explainable clustering based on sorting",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110298",
    "abstract": "We introduce a fast and explainable clustering method called CLASSIX. It consists of two phases, namely a greedy aggregation phase of the sorted data into groups of nearby data points, followed by the merging of groups into clusters. The algorithm is controlled by two scalar parameters, namely a distance parameter for the aggregation and another parameter controlling the minimal cluster size. Extensive experiments are conducted to give a comprehensive evaluation of the clustering performance on synthetic and real-world datasets, with various cluster shapes and low to high feature dimensionality. Our experiments demonstrate that CLASSIX competes with state-of-the-art clustering algorithms. The algorithm has linear space complexity and achieves near linear time complexity on a wide range of problems. Its inherent simplicity allows for the generation of intuitive explanations of the computed clusters.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324000499",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Pattern recognition (psychology)",
      "Sorting"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Xinye"
      },
      {
        "surname": "Güttel",
        "given_name": "Stefan"
      }
    ]
  },
  {
    "title": "Deep-NFA: A deep a contrario framework for tiny object detection",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110312",
    "abstract": "The detection of tiny objects is a challenging task in computer vision. Conventional object detection methods have difficulties in finding the balance between high detection rate and low false alarm rate. In the literature, some methods have addressed this issue by enhancing the feature map responses for small objects, but without guaranteeing robustness with respect to the number of false alarms induced by background elements. To tackle this problem, we introduce an a contrario decision criterion into the learning process to take into account the unexpectedness of tiny objects. This statistic criterion enhances the feature map responses while controlling the number of false alarms (NFA) and can be integrated as an add-on into any semantic segmentation neural network. Our add-on NFA module not only allows us to obtain competitive results for small target, road crack and ship detection tasks respectively, but also leads to more robust and interpretable results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324000633",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Constant false alarm rate",
      "Economics",
      "False alarm",
      "Feature (linguistics)",
      "Gene",
      "Linguistics",
      "Management",
      "Mathematics",
      "Object detection",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Process (computing)",
      "Robustness (evolution)",
      "Segmentation",
      "Semantic feature",
      "Statistic",
      "Statistics",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Ciocarlan",
        "given_name": "Alina"
      },
      {
        "surname": "Le Hégarat-Mascle",
        "given_name": "Sylvie"
      },
      {
        "surname": "Lefebvre",
        "given_name": "Sidonie"
      },
      {
        "surname": "Woiselle",
        "given_name": "Arnaud"
      }
    ]
  },
  {
    "title": "Improving self-supervised action recognition from extremely augmented skeleton sequences",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110333",
    "abstract": "Self-supervised contrastive learning has been widely applied to skeleton-based action recognition due to its ability to learn discriminative features. However, directly applying the existing contrastive learning framework for 3D skeleton learning is limited by the well-designed augmentations and the simple multi-stream decision-level fusion. To deal with these drawbacks, we propose a three-stream contrastive learning framework utilizing abundant information mining for self-supervised action representation (3s-AimCLR++). For single-stream contrastive learning, extreme augmentation is first proposed to generate more movement patterns, which can introduce more movement patterns to improve the universality of the learned representations. Since directly using extreme augmentation can barely boost the performance due to the drastic changes in original identity, the Distributional Divergence Minimization (DDM) loss is proposed to utilize the extreme augmentation more gently. Moreover, the Single-Stream Nearest Neighbors Mining (SNNM) is proposed to expand positive samples to make the learning process more reasonable. For multi-stream, existing methods simply ensemble the results. Yet, considering the complementarity of information between different streams, we propose Multi-Stream Aggregation and Interaction (MSAI) strategy to better fuse multi-stream information. Extensive experiments on NTU-60, NTU-120, and PKU-MMD datasets have verified that our 3s-AimCLR++ can significantly perform favorably against state-of-the-art methods under a variety of evaluation protocols. The code and models are available at https://github.com/Levigty/AimCLR-v2.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324000840",
    "keywords": [
      "Action (physics)",
      "Action recognition",
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Computer vision",
      "Pattern recognition (psychology)",
      "Physics",
      "Programming language",
      "Quantum mechanics",
      "Skeleton (computer programming)"
    ],
    "authors": [
      {
        "surname": "Guo",
        "given_name": "Tianyu"
      },
      {
        "surname": "Liu",
        "given_name": "Mengyuan"
      },
      {
        "surname": "Liu",
        "given_name": "Hong"
      },
      {
        "surname": "Wang",
        "given_name": "Guoquan"
      },
      {
        "surname": "Li",
        "given_name": "Wenhao"
      }
    ]
  },
  {
    "title": "Reading order detection in visually-rich documents with multi-modal layout-aware relation prediction",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110314",
    "abstract": "Reading order detection aims to arrange the text logically, which is essential in understanding visual documents. Current methods mostly model the problem as a sequence generation task, which use insufficient modalities information ignore the various reading habits under different document layouts, leading to the lack of robustness for some complex scenarios. To address these challenges, we present a novel approach with the Multi-Modal Layout-Aware Relation Prediction. It employs a straightforward yet highly effective task formulation for predicting the order relation between text instances. Our model leverages visual, semantic, and positional features, with the positional features being adaptively generated through a layout-aware position embedding module. Then, different modality features are enhanced via a two-staged position-guided multi-modal fusion module. Additionally, we introduce two novel loss functions, Degree Loss and Cycle Loss, to effectively impose network constraints at multiple levels. Our experimental results, conducted on three real-world datasets, demonstrate that our proposed method achieves a new state-of-the-art level of performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324000657",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Economics",
      "Finance",
      "Information retrieval",
      "Linguistics",
      "Modal",
      "Natural language processing",
      "Order (exchange)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Polymer chemistry",
      "Reading (process)",
      "Relation (database)"
    ],
    "authors": [
      {
        "surname": "Qiao",
        "given_name": "Liang"
      },
      {
        "surname": "Li",
        "given_name": "Can"
      },
      {
        "surname": "Cheng",
        "given_name": "Zhanzhan"
      },
      {
        "surname": "Xu",
        "given_name": "Yunlu"
      },
      {
        "surname": "Niu",
        "given_name": "Yi"
      },
      {
        "surname": "Li",
        "given_name": "Xi"
      }
    ]
  },
  {
    "title": "Improving generalized zero-shot learning via cluster-based semantic disentangling representation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110320",
    "abstract": "Generalized Zero-Shot Learning (GZSL) aims to recognize both seen and unseen classes by training only the seen classes, in which the instances of unseen classes tend to be biased towards the seen class. In this paper, we propose a Cluster-based Semantic Disentangling Representation (CSDR) method to improve GZSL by alleviating the problems of domain shift and semantic gap. First, we cluster the seen data into multiple clusters, where the samples in each cluster belong to several original seen categories, so as to facilitate fine-grained semantic disentangling of visual feature vectors. Then, we introduce representation random swapping and contrastive learning based on the clustering results to realize the disentangling semantic representations of semantic-unspecific, class-shared, and class-unique. The fine-grained semantic disentangling representations show high intra-class similarity and inter-class discriminability, which improve the performance of GZSL by alleviating the problem of domain shift. Finally, we construct the visual-semantic embedding space by the variational auto-encoder and alignment module, which can bridge the semantic gap by generating strongly discriminative unseen class samples. Extensive experimental results on four public data sets prove that our method significantly outperforms state-of-the-art methods in generalized and conventional settings.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324000712",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Cluster analysis",
      "Computer science",
      "Discriminative model",
      "Domain (mathematical analysis)",
      "Embedding",
      "Encoder",
      "Feature (linguistics)",
      "Feature learning",
      "Feature vector",
      "Image (mathematics)",
      "Image retrieval",
      "Law",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Natural language processing",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Semantic gap",
      "Semantic similarity",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Gao",
        "given_name": "Yi"
      },
      {
        "surname": "Feng",
        "given_name": "Wentao"
      },
      {
        "surname": "Xiao",
        "given_name": "Rong"
      },
      {
        "surname": "He",
        "given_name": "Lihuo"
      },
      {
        "surname": "He",
        "given_name": "Zhenan"
      },
      {
        "surname": "Lv",
        "given_name": "Jiancheng"
      },
      {
        "surname": "Tang",
        "given_name": "Chenwei"
      }
    ]
  },
  {
    "title": "Sample diversity selection strategy based on label distribution morphology for active label distribution learning",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110322",
    "abstract": "Labeling a sample in label distribution learning is highly expensive because it involves several labels at the same time and also requires assigning an exact value as the significance of each label. Therefore, active learning, which lowers the labeling cost by actively querying the labels of the most useful data, becomes especially critical for label distribution learning. Most of the known active query algorithms are for multi-label learning, and applying them directly to label distribution learning will lose some key supervisory information and thus fail to yield satisfactory experimental results. In this paper, we propose a sample diversity selection strategy based on the label distribution morphology, which can select diverse samples with different distribution morphologies from a large number of unlabeled samples for active querying. First, we use the feature space to construct a dissimilarity matrix that describes the pairwise dissimilarity among the unlabeled samples in order to pick a subset of samples that are representative of the unlabeled dataset. Second, using the information about the label distribution morphologies provided by the predicted labels of the unlabeled samples, we design a diversity loss score for each unlabeled sample. This score reflects the degree of difference between the sample and the labeled training sample. Finally, we use a convex optimization method to select valuable samples that are diverse from the labeled samples and represent the distribution of the unlabeled samples. The results of the comparison experiments demonstrate the effectiveness of our approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324000736",
    "keywords": [
      "Active learning (machine learning)",
      "Artificial intelligence",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Distribution (mathematics)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pairwise comparison",
      "Pattern recognition (psychology)",
      "Sample (material)",
      "Sample size determination",
      "Selection (genetic algorithm)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Weiwei"
      },
      {
        "surname": "Qian",
        "given_name": "Wei"
      },
      {
        "surname": "Chen",
        "given_name": "Lei"
      },
      {
        "surname": "Jia",
        "given_name": "Xiuyi"
      }
    ]
  },
  {
    "title": "Comprehensive assessment of triclustering algorithms for three-way temporal data analysis",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110303",
    "abstract": "The analysis of temporal data has gained increasing attention in recent years, aiming to identify patterns and trends that change over time. Temporal triclustering is a promising approach for this purpose, as it allows for the simultaneous clustering of three dimensions of data: objects, attributes, and time. In this work, we present a comparative study and experimental evaluation of state-of-the-art temporal triclustering algorithms. Our study provides a comprehensive quantitative assessment of several triclustering algorithms to unravel their strengths and limitations. To this end, we consider synthetic data with varying sizes and regularities, where true solutions are planted with different coherence and quality criteria, in order to assess the algorithms’ performance in datasets with diverse characteristics and assess their capacity to retrieve specific types of hidden patterns. This provides a more comprehensive evaluation of the algorithms and allows for a better understanding of their capabilities and limitations. This study is the first to compare state-of-the-art triclustering algorithms inherently prepared to deal with temporal data and provides new benchmark results for the Temporal Triclustering task. Our results on the algorithms’ performance can guide practitioners in selecting the most appropriate algorithm for their specific application.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324000542",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science"
    ],
    "authors": [
      {
        "surname": "Soares",
        "given_name": "Diogo F."
      },
      {
        "surname": "Henriques",
        "given_name": "Rui"
      },
      {
        "surname": "C. Madeira",
        "given_name": "Sara"
      }
    ]
  },
  {
    "title": "Learning with incomplete labels of multisource datasets for ECG classification",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110321",
    "abstract": "The shortage of annotated ECG data presents a significant impediment, hampering the overall generalization capabilities of machine learning models tailored for automated ECG classification. The collective integration of multisource datasets presents a potential remedy for this challenge. However, it is crucial to underscore that the mere addition of supplementary data does not automatically guarantee performance enhancement, given the unresolved challenges associated with multisource data. In this research, we address one such challenge, namely, the issue of incomplete labels arising from the diversity of annotations within multi-source ECG datasets. First, we identified three distinct types of label missing: dataset-related label missing, supertype missing, and subtype missing. To address the supertype missing effectively, we introduce a novel approach known as offline category mapping which leverages the hierarchical relationships inherent within the categories to recover the missing supertype labels. Additionally, two complementary strategies, referred to as prediction masking and online category mapping, are proposed to mitigating the adverse effects of subtype and dataset-related label missing on model optimization. These strategies enhance the model's ability to identify missing subtypes under conditions of weak supervision. These pioneering methodologies are integrated into a deep learning-based framework designed for multilabel ECG classification. The performance of our proposed framework is rigorously evaluated using realistic multi-source datasets obtained from the PhysioNet/CinC challenge 2020/2021. The proposed learning framework exhibits a notable improvement in macro-average precision, surpassing the corresponding baseline model by more than 25 % on the test datasets. As a result, this research study makes a substantial contribution to the field of ECG classification by addressing the critical issue of incomplete labels in multisource datasets, ultimately enhancing the generalization capabilities of machine learning models in this domain.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324000724",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Economic shortage",
      "Generalization",
      "Government (linguistics)",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Missing data",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Qince"
      },
      {
        "surname": "Liu",
        "given_name": "Yang"
      },
      {
        "surname": "Zhang",
        "given_name": "Ze"
      },
      {
        "surname": "Liu",
        "given_name": "Jun"
      },
      {
        "surname": "Yuan",
        "given_name": "Yongfeng"
      },
      {
        "surname": "Wang",
        "given_name": "Kuanquan"
      },
      {
        "surname": "He",
        "given_name": "Runnan"
      }
    ]
  },
  {
    "title": "HDR light field imaging of dynamic scenes: A learning-based method and a benchmark dataset",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110313",
    "abstract": "Light field (LF) imaging is an effective way to enable immersive applications. However, limited by the potential well capacity of the image sensor, the acquired LF images suffer from low dynamic range and are thus prone to under-exposure or over-exposure. High dynamic range (HDR) LF imaging is an efficacious avenue to improve the LF imaging's dynamic range. Unfortunately, for dynamic scenes, existing methods are inclined to produce ghosting artifacts and lose details in the saturated regions, while potentially damaging the parallax structure of generated HDR LF images. To address the above challenges, in this paper, we propose a new ghost-free HDR LF imaging method based on a deformable aggregation and angular embedding network. Specifically, considering the four-dimensional geometric structure of the LF image, a deformable alignment module is first designed to handle dynamic regions in the spatial domain, and then the aligned spatial features are fully fused through an aggregation operation. Subsequently, an angular embedding module is constructed to explore angular information to enhance the aggregated spatial features. Based on this, the above two modules are cascaded in a multi-scale manner to achieve multi-level feature extraction and enhance the feature representation ability. Finally, a decoder is leveraged to recover the ghost-free HDR LF image from the enhanced multi-scale features. For performance evaluation, this paper establishes a large-scale benchmark dataset with multi-exposure inputs and ground truth images. Extensive experimental results show that the proposed method generates visually pleasing HDR LF images while preserving accurate angular consistency. Moreover, the proposed method surpasses the state-of-the-art methods in both quantitative and qualitative comparisons. The code and dataset will be available at https://github.com/YeyaoChen/HDRLFI.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324000645",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Computer vision",
      "Dynamic range",
      "Embedding",
      "Feature (linguistics)",
      "Geodesy",
      "Geography",
      "Ghosting",
      "High dynamic range",
      "High-dynamic-range imaging",
      "Linguistics",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Yeyao"
      },
      {
        "surname": "Jiang",
        "given_name": "Gangyi"
      },
      {
        "surname": "Yu",
        "given_name": "Mei"
      },
      {
        "surname": "Jin",
        "given_name": "Chongchong"
      },
      {
        "surname": "Xu",
        "given_name": "Haiyong"
      },
      {
        "surname": "Ho",
        "given_name": "Yo-Sung"
      }
    ]
  },
  {
    "title": "Kernel correlation–dissimilarity for Multiple Kernel k-Means clustering",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110307",
    "abstract": "The main objective of the Multiple Kernel k-Means (MKKM) algorithm is to extract non-linear information and achieve optimal clustering by optimizing base kernel matrices. Current methods enhance information diversity and reduce redundancy by exploiting interdependencies among multiple kernels based on correlations or dissimilarities. Nevertheless, relying solely on a single metric, such as correlation or dissimilarity, to define kernel relationships introduces bias and incomplete characterization. Consequently, this limitation hinders efficient information extraction, ultimately compromising clustering performance. To tackle this challenge, we introduce a novel method that systematically integrates both kernel correlation and dissimilarity. Our approach comprehensively captures kernel relationships, facilitating more efficient classification information extraction and improving clustering performance. By emphasizing the coherence between kernel correlation and dissimilarity, our method offers a more objective and transparent strategy for extracting non-linear information and significantly improving clustering precision, supported by theoretical rationale. We assess the performance of our algorithm on 13 challenging benchmark datasets, demonstrating its superiority over contemporary state-of-the-art MKKM techniques.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032400058X",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Cluster analysis",
      "Combinatorics",
      "Computer science",
      "Correlation",
      "Data mining",
      "Economics",
      "Geodesy",
      "Geography",
      "Geometry",
      "Kernel (algebra)",
      "Kernel method",
      "Machine learning",
      "Mathematics",
      "Metric (unit)",
      "Operating system",
      "Operations management",
      "Pattern recognition (psychology)",
      "Redundancy (engineering)",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Su",
        "given_name": "Rina"
      },
      {
        "surname": "Guo",
        "given_name": "Yu"
      },
      {
        "surname": "Wu",
        "given_name": "Caiying"
      },
      {
        "surname": "Jin",
        "given_name": "Qiyu"
      },
      {
        "surname": "Zeng",
        "given_name": "Tieyong"
      }
    ]
  },
  {
    "title": "Semi-supervised imbalanced multi-label classification with label propagation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110358",
    "abstract": "Multi-label learning tasks usually encounter the problem of the class-imbalance, where samples and their corresponding labels are non-uniformly distributed over multi-label data space. It has attracted increasing attention during the past decade, however, there is a lack of methods capable of handling the imbalanced problem in a semi-supervised setting. This study proposes a label propagation technique to settle the semi-supervised imbalanced multi-label issue. Specially, we first utilize a collaborative manner to exploit the correlations from labels and instances, and learn a label regularization matrix to overcome the imbalanced problem in the labeled instance. After that, we extend to semi-supervised learning and explore to represent the similarity of instances with weighted graphs on labeled and unlabeled data. Then, the data distribution information and label correlations are fully utilized to design the loss function under the consistency assumption manner. At last, we present an iterative scheme to settle the optimization issue, thereby achieving label propagation to address the imbalanced challenge. Experiments on a variety of multi-label data sets show the favorable performance of the proposed method against related comparing approaches. Notably, the proposed method is also validated to be robust with a limited number of training instances.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324001092",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer security",
      "Consistency (knowledge bases)",
      "Data mining",
      "Exploit",
      "Image (mathematics)",
      "Machine learning",
      "Multi-label classification",
      "Pattern recognition (psychology)",
      "Regularization (linguistics)",
      "Semi-supervised learning",
      "Similarity (geometry)",
      "Supervised learning"
    ],
    "authors": [
      {
        "surname": "Du",
        "given_name": "Guodong"
      },
      {
        "surname": "Zhang",
        "given_name": "Jia"
      },
      {
        "surname": "Zhang",
        "given_name": "Ning"
      },
      {
        "surname": "Wu",
        "given_name": "Hanrui"
      },
      {
        "surname": "Wu",
        "given_name": "Peiliang"
      },
      {
        "surname": "Li",
        "given_name": "Shaozi"
      }
    ]
  },
  {
    "title": "Cross-frame feature-saliency mutual reinforcing for weakly supervised video salient object detection",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110302",
    "abstract": "Scribble annotations have recently become popular in video salient object detection. Previous methods only focus on utilizing shallow feature consistency for more integral predictions. However, there is potential for consistency between cross-frame deep features to be used to help regularize saliency predictions better. Besides, we have observed that leveraging saliency predictions as pseudo-supervision signals yields notable improvements in extracting both intra-frame and cross-frame deep features. This, in turn, leads to more precise and detailed object structural information. Thus, we propose a cross-frame feature-saliency mutual reinforcing training process to assist scribble annotations for integral video saliency predictions. Specifically, we design a cross-frame feature regularization head, which leverages intra-frame and cross-frame deep feature consistency to regularize saliency predictions as auxiliary supervision. Then, to help obtain more accurate feature consistency, we design a cross-frame saliency regularization head, where predicted saliency values are used as pseudo-supervision signals to acquire better feature consistency. In this way, our cross-frame feature and saliency regularization heads can benefit from each other to help the network learn more accurately. Extensive experiments show that our method can achieve better performances than the previous best methods. The project is available at https://github.com/muchengxue0911/CFMR.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324000530",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Frame (networking)",
      "Linguistics",
      "Mutual information",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Salient",
      "Telecommunications",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Jian"
      },
      {
        "surname": "Yu",
        "given_name": "Siyue"
      },
      {
        "surname": "Zhang",
        "given_name": "Bingfeng"
      },
      {
        "surname": "Zhao",
        "given_name": "Xinqiao"
      },
      {
        "surname": "García-Fernández",
        "given_name": "Ángel F."
      },
      {
        "surname": "Lim",
        "given_name": "Eng Gee"
      },
      {
        "surname": "Xiao",
        "given_name": "Jimin"
      }
    ]
  },
  {
    "title": "DAT: A robust Discriminant Analysis-based Test of unimodality for unknown input distributions",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.04.021",
    "abstract": "This letter introduces a Discriminant Analysis-based unimodality Test (DAT) for automatically identifying whether a dataset is unimodal or multimodal, detecting deviations in time series datasets, estimating statistical parameters, and identifying the skewness of the data. DAT is effective in classifying datasets under both unimodal and multimodal conditions and is suitable for bi-classification applications. The performance of DAT was compared to two well-known unimodality tests, namely the dip and folding tests, and is shown to perform better. We then extended DAT to the development of a fault detection technique, which was tested against five different machine learning classifiers using data from the Case Western Reserve University (CWRU) ball bearing database. The results obtained show that the proposed approach is effective, achieving 99.999% accuracy for detecting small ball bearing fault sizes (0.007 inches). Our conclusion indicates a significant potential of the proposed test for improving anomaly detection in industrial and other related fields.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524001302",
    "keywords": [
      "Anomaly detection",
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Discriminant",
      "Linear discriminant analysis",
      "Mathematics",
      "Normality",
      "Pattern recognition (psychology)",
      "Skewness",
      "Statistical hypothesis testing",
      "Statistics",
      "Unimodality"
    ],
    "authors": [
      {
        "surname": "Gupta",
        "given_name": "Aditi"
      },
      {
        "surname": "Onumanyi",
        "given_name": "Adeiza J."
      },
      {
        "surname": "Ahlawat",
        "given_name": "Satyadev"
      },
      {
        "surname": "Prasad",
        "given_name": "Yamuna"
      },
      {
        "surname": "Singh",
        "given_name": "Virendra"
      },
      {
        "surname": "Abu-Mahfouz",
        "given_name": "Adnan M."
      }
    ]
  },
  {
    "title": "Prior based Pyramid Residual Clique Network for human body image super-resolution",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110352",
    "abstract": "Recent research in the analysis of human images, such as human parsing and pose estimation, usually requires input images to have a sufficiently high resolution. However, small images of people are commonly encountered in our daily lives, particularly in surveillance applications. This paper aims to ultra-resolve a tiny person image to its high-resolution counterpart by learning effective feature representations and exploiting useful human body prior knowledge. First, we propose the Residual Clique Block (RCB) to fully exploit compact feature representations for image Super-Resolution (SR). Second, a series of RCBs are cascaded in a coarse-to-fine manner to construct the Pyramid Residual Clique Network (PRCN), which simultaneously reconstructs multiple SR results (e.g. 2 × , 4 × , and 8 × ) in one feed-forward pass. Third, we utilize the human parsing map as the shape prior, and the high-frequency sub-bands of Uniform Discrete Curvelet Transform (UDCT) as the texture prior to enhance the details of reconstructed human body image. Experimental results demonstrate that our proposed method achieves state-of-the-art performance with superior visual quality and PSNR/SSIM scores. Moreover, we show that our results can considerably enhance the performance of human parsing and pose estimation tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324001031",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Clique",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Construct (python library)",
      "Feature (linguistics)",
      "Geometry",
      "Image (mathematics)",
      "Linguistics",
      "Mathematics",
      "Parsing",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Pyramid (geometry)",
      "Residual",
      "Subspace topology"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Simiao"
      },
      {
        "surname": "Sang",
        "given_name": "Yu"
      },
      {
        "surname": "Liu",
        "given_name": "Yunan"
      },
      {
        "surname": "Wang",
        "given_name": "Chunpeng"
      },
      {
        "surname": "Lu",
        "given_name": "Mingyu"
      },
      {
        "surname": "Sun",
        "given_name": "Jinguang"
      }
    ]
  },
  {
    "title": "Decomposition via elastic-band transform",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.04.013",
    "abstract": "In this paper, we propose a novel decomposition method using elastic-band transform (EBT), which mimics eye scanning and is utilized for multiscale analysis of signals. The proposed EBT-based method can efficiently extract the features of various signals with the following three advantages. First, it is a data-driven approach that extracts several important modes based solely on data without using predetermined basis functions. Second, it does not assume that the signal consists of (locally) sinusoidal intrinsic mode functions, which is a common assumption in existing methods. Therefore, the proposed method can handle a wide range of signals. Finally, it is robust to noise. A practical algorithm for decomposition is presented, along with some theoretical properties. Simulation examples and real data analysis results show promising empirical properties of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524001144",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Decomposition",
      "Geometry",
      "Mathematics",
      "Organic chemistry"
    ],
    "authors": [
      {
        "surname": "Choi",
        "given_name": "Guebin"
      },
      {
        "surname": "Oh",
        "given_name": "Hee-Seok"
      }
    ]
  },
  {
    "title": "Learning from feature and label spaces’ bias for uncertainty-adaptive facial emotion recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.04.015",
    "abstract": "Developing an accurate deep model for facial emotion recognition is a long-term challenge. It is because the uncertainty of emotions, stemming from the ambiguity of different emotional categories and the difference of subjective annotations, can ruin the ability of model to achieve the desired optimization. This paper constructs two distinct datasets, namely original sample set and ambiguous sample set, to explore an effective ambiguous knowledge transfer method to realize the adaptive awareness of uncertainty in facial emotion recognition. The original sample set is the weakly-augmented data with relatively low uncertainty, as most emotions are clean in reality. Meanwhile, the ambiguous sample set is strongly-augmented data that introduces feature and label bias with regard to emotion, which are with relatively high uncertainty. The proposed framework consists of two sub-nets, which are trained using the original set and the ambiguous set respectively. To achieve uncertainty-adaptive learning for two sub-nets, we introduce two modules. One is the cross-space attention consistency learning module that performs attention coupling across original and ambiguous feature spaces, achieving uncertainty-aware representation learning in feature granularity. The other is the soft-label learning module that models and utilizes uncertainty in label granularity, through aligning the posterior distributions between original label space and ambiguous label space. Experimental studies on public datasets indicate that our method is competitive with the state-of-the-art.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524001168",
    "keywords": [
      "Artificial intelligence",
      "Cognitive psychology",
      "Computer science",
      "Emotion recognition",
      "Facial expression",
      "Facial recognition system",
      "Feature (linguistics)",
      "Feature learning",
      "Linguistics",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Psychology",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Luhui"
      },
      {
        "surname": "Gan",
        "given_name": "Yanling"
      },
      {
        "surname": "Xia",
        "given_name": "Haiying"
      }
    ]
  },
  {
    "title": "DSCA: A Dual Semantic Correlation Alignment Method for domain adaptation object detection",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110329",
    "abstract": "In self-driving cars, adverse weather (e.g., fog, rain, snow, and cloud) or occlusion scenarios result in domain shift being unavoidable in object detection. Researchers have recently proposed Domain Adaptive Object Detection (DAOD), i.e., aligning the source and target domains at the image and instance levels distribution by utilizing the Unsupervised Domain Adaptation (UDA) method. However, the semantic correlation information is ignored leading to the effect of aligning not good, and low detection accuracy of objects in adverse weather or occlusion scenarios. Here, we propose a Dual Semantic Correlation Alignment (DSCA) method for DAOD to address the problem. The core idea behind DSCA is to make full use of semantic correlation information including context correlation semantic information and class correlation semantic information to align object semantic information in source and target domains, which supplement and enhance the missing information for target domains. It consists of a two-level semantic alignment: (1) context correlation semantic alignment is developed to obtain the context correlation semantic information of the object to align context semantic information at the image level; (2) class correlation semantic alignment is proposed to obtain the class correlation semantic information of the object to align class semantic information at the instance level. The two-level semantic alignment can effectively decrease negative transfer and complete object information to improve the detection accuracy of objects in different domains. Experiments on four challenging benchmarks show that our proposed DSCA method outperforms state-of-the-art DAOD methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324000803",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Class (philosophy)",
      "Computer science",
      "Context (archaeology)",
      "Correlation",
      "Geometry",
      "Mathematics",
      "Object (grammar)",
      "Object detection",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Programming language",
      "Semantic Web",
      "Semantic compression",
      "Semantic computing",
      "Semantic technology",
      "Semantics (computer science)"
    ],
    "authors": [
      {
        "surname": "Guo",
        "given_name": "Yinsai"
      },
      {
        "surname": "Yu",
        "given_name": "Hang"
      },
      {
        "surname": "Xie",
        "given_name": "Shaorong"
      },
      {
        "surname": "Ma",
        "given_name": "Liyan"
      },
      {
        "surname": "Cao",
        "given_name": "Xinzhi"
      },
      {
        "surname": "Luo",
        "given_name": "Xiangfeng"
      }
    ]
  },
  {
    "title": "Certainty weighted voting-based noise correction for crowdsourcing",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110325",
    "abstract": "In crowdsourcing scenarios, we can obtain each instance’s multiple noisy label set from different workers and then use a ground truth inference algorithm to infer its integrated label. Despite the effectiveness of ground truth inference algorithms, there is still a certain level of noise in integrated labels. To reduce the impact of noise, many noise correction algorithms have been proposed in recent years. To the best of our knowledge, almost all these algorithms assume that workers have the same labeling certainty on different classes and instances. However, it is rarely true in reality due to the differences in workers’ individual preferences and cognitive abilities. In this paper, we argue that the labeling certainty of a worker should be class-dependent and instance-dependent. Based on this premise, we propose a certainty weighted voting-based noise correction (CWVNC) algorithm. At first, we use the consistency between worker-labeled labels and integrated labels on different classes to estimate the class-dependent certainty. Then, we train a probability-based classifier on the instances labeled by each worker separately and use it to estimate the instance-dependent certainty. Finally, we correct the integrated label of each instance by weighted voting based on class-dependent certainty and instance-dependent certainty. When the proposed algorithm CWVNC is examined, the average noise ratio of CWVNC on 34 simulated datasets is equal to 15.08%, and on two real-world datasets “Income” and “Music_genre” the noise ratio is equal to 25.77% and 26.94%, respectively. The results show that CWVNC significantly outperforms all other state-of-the-art noise correction algorithms used for comparison.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324000761",
    "keywords": [
      "Artificial intelligence",
      "Certainty",
      "Computer science",
      "Crowdsourcing",
      "Data mining",
      "Geometry",
      "Image (mathematics)",
      "Law",
      "Majority rule",
      "Mathematics",
      "Noise (video)",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Voting",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Huiru"
      },
      {
        "surname": "Jiang",
        "given_name": "Liangxiao"
      },
      {
        "surname": "Li",
        "given_name": "Chaoqun"
      }
    ]
  },
  {
    "title": "Regularization and optimization in model-based clustering",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110310",
    "abstract": "Due to their conceptual simplicity, k-means algorithm variants have been extensively used for unsupervised cluster analysis. However, one main shortcoming of these algorithms is that they essentially fit a mixture of identical spherical Gaussians to data that vastly deviates from such a distribution. In comparison, general Gaussian Mixture Models (GMMs) can fit richer structures but require estimating a quadratic number of parameters per cluster to represent the covariance matrices. This poses two main issues: (i) the underlying optimization problems are challenging due to their larger number of local minima, and (ii) their solutions can overfit the data. In this work, we design search strategies that circumvent both issues. We develop more effective optimization algorithms for general GMMs, and we combine these algorithms with regularization strategies that avoid overfitting. Through extensive computational analyses, we observe that optimization or regularization in isolation does not substantially improve cluster recovery. However, combining these techniques permits a completely new level of performance previously unachieved by k-means algorithm variants, unraveling vastly different cluster structures. These results shed new light on the current status quo between GMM and k-means methods and suggest the more frequent use of general GMMs for data exploration. To facilitate such applications, we provide open-source code as well as Julia packages (UnsupervisedClustering.jl and RegularizedCovarianceMatrices.jl) implementing the proposed techniques.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032400061X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Cluster analysis",
      "Computer science",
      "Covariance",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Maxima and minima",
      "Mixture model",
      "Overfitting",
      "Regularization (linguistics)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Sampaio",
        "given_name": "Raphael Araujo"
      },
      {
        "surname": "Dias Garcia",
        "given_name": "Joaquim"
      },
      {
        "surname": "Poggi",
        "given_name": "Marcus"
      },
      {
        "surname": "Vidal",
        "given_name": "Thibaut"
      }
    ]
  },
  {
    "title": "Multi-granularity Cross Transformer Network for person re-identification",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110362",
    "abstract": "Person re-identification (Re-ID) aims to retrieve the same person in the gallery. Transformers have been introduced to the Re-ID task due to their excellent ability to model long-range dependency. However, due to the properties of the global attention mechanism, they are less effective in capturing the discriminative local semantics of pedestrians compared to convolutional operations. To address this issue, we present a Multi-granularity Cross Transformer Network (MCTN) that progressively learns salient features of different local structures in a global context. Specifically, we first utilize a Multi-granularity Convolutional Layer (MCL) to investigate salient pedestrian features at various granularities. On this basis, we propose a Pyramidal Cross Transformer learning layer (PCT), which contains a pyramidal division of pedestrian image feature maps, differentiated feature extraction of different parts of pedestrians, and cross attention to exploring the local–global relationship of the feature map. It allows effective mining of local information in the global structure from a coarse-to-fine perspective. Furthermore, to enhance the interaction between low-level detailed features and high-level semantic features, a Hierarchical Aggregation Strategy (HAS) is introduced to fuse features learned by cross attention learning at different stages. Pedestrian features learned in shallow layers will serve as global priors for semantics learning in deep layers. We evaluate our method on four large-scale Re-ID datasets, and the experimental results reveal that the proposed method outperforms the state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324001134",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Computer science",
      "Data mining",
      "Electrical engineering",
      "Engineering",
      "Granularity",
      "Identification (biology)",
      "Operating system",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Yanping"
      },
      {
        "surname": "Miao",
        "given_name": "Duoqian"
      },
      {
        "surname": "Zhang",
        "given_name": "Hongyun"
      },
      {
        "surname": "Zhou",
        "given_name": "Jie"
      },
      {
        "surname": "Zhao",
        "given_name": "Cairong"
      }
    ]
  },
  {
    "title": "MCPNet: Multi-space color correction and features prior fusion for single-image dehazing in non-homogeneous haze scenarios",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110290",
    "abstract": "When capturing images in non-homogeneous haze conditions, the degree of haze impact varies across the scene, and the information in certain parts of the scene are lost, making the scene nearly invisible. However, most existing dehazing convolutional neural networks (CNNs) are designed for homogeneous haze and do not consider the challenge of feature extraction arising from non-homogeneous haze distribution. Moreover, these networks primarily rely on the RGB color space for feature extraction, often fail to extract color and detail feature effectively, resulting in color distortion and loss of details in the output. To tackle this problem, we introduce image priors in the YCbCr and HSV color spaces, proposing a novel Multiple Color Space Prior Network (MCPNet) to enhance the dehazing performance specifically for non-homogeneous hazy images, while simultaneously correcting the color, preserving the visual quality of the output. Leveraging image priors, we designed two parallel sub-networks to extract color and detail features from the YCbCr and HSV color spaces. Moreover, to capitalize on these features and incorporate them effectively into the dehazed image, we introduce a Comprehensive Fusion Module (CFM). This module judiciously takes into account both the fusion of multiscale features and the interrelation among channels to optimize feature fusion. By employing a dual network architecture coupled with the CFM, our model proficiently amalgamates and exploits the mined features, accurately restoring the color and detail information of the image, especially for images containing non-homogeneous haze. Extensive experimental highlight the effectiveness of our model in addressing homogeneous and non-homogeneous hazy images, concurrently preserving the visual appeal of the dehazed outcomes. When compared with other SOTA models, our MCPNet demonstrates superior results in dehazing.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324000414",
    "keywords": [
      "Amplifier",
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Biology",
      "Color image",
      "Color space",
      "Combinatorics",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Distortion (music)",
      "Feature (linguistics)",
      "HSL and HSV",
      "Haze",
      "Homogeneous",
      "Image (mathematics)",
      "Image processing",
      "Linguistics",
      "Mathematics",
      "Meteorology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "RGB color model",
      "Virology",
      "Virus",
      "YCbCr"
    ],
    "authors": [
      {
        "surname": "Lyu",
        "given_name": "Zhiyu"
      },
      {
        "surname": "Chen",
        "given_name": "Yan"
      },
      {
        "surname": "Hou",
        "given_name": "Yimin"
      }
    ]
  },
  {
    "title": "Kinematics-aware spatial-temporal feature transform for 3D human pose estimation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110316",
    "abstract": "3D human pose estimation plays an important role in various human-machine interactive applications, but how to effectively extract and represent the kinematical features of human body structure in video has always been a challenge. This paper presents some inspiring observations on the human body properties that hold heuristic patterns of human poses: 1) There is distinct temporal coherence in any kind of human pose; 2) there exist evident spatial and temporal correlations among local joints even though the human is doing complex actions. According to the observed patterns, a locally structured feature encoder and a spatial–temporal feature transform are proposed for kinematics-aware feature extraction and enhancement. Unlike existing works directly projecting every bone joint to pose features without distinction, the proposed locally-structured feature encoder maps the local connection property of human body structure to kinematical features which are neural embeddings extracted from both local and global groups of human bone joints. Since the local and global bone-joint groups are pre-defined according to human body kinematics, the kinematical features are able to represent body kinematics. The kinematical features are then transformed by the proposed spatial–temporal feature transform to enhance the spatial and temporal correlations among human bone joints. The overall framework well promotes the representation of human body kinematics for 3D pose estimation. Extensive experimental results on commonly used datasets show that the mean per joint position error (MPJPE) is significantly reduced when compared with state-of-the-art methods under the same experimental condition. The improvement is expected to promote machines to better understand human poses for building superior human-centered automation systems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324000670",
    "keywords": [
      "Architectural engineering",
      "Artificial intelligence",
      "Classical mechanics",
      "Coherence (philosophical gambling strategy)",
      "Computer science",
      "Computer vision",
      "Encoder",
      "Engineering",
      "Feature (linguistics)",
      "Feature extraction",
      "Heuristic",
      "Joint (building)",
      "Kinematics",
      "Linguistics",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Pose",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Du",
        "given_name": "Songlin"
      },
      {
        "surname": "Yuan",
        "given_name": "Zhiwei"
      },
      {
        "surname": "Ikenaga",
        "given_name": "Takeshi"
      }
    ]
  },
  {
    "title": "Large-scale continual learning for ancient Chinese character recognition",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110283",
    "abstract": "Ancient Chinese character recognition is a challenging problem in the field of pattern recognition. It is difficult to collect all character classes during the training stage due to the numerous classes of ancient Chinese characters and the likelihood of discovering new characters over time. A solution to address this problem is continual learning. However, most continual learning methods are not well-suited for large-scale applications, making them insufficient for solving the problem of ancient Chinese character recognition. Although saving raw data for old classes is a good approach for continual learning to address large-scale problems, it is often infeasible due to the lack of data accessibility in reality. To solve these problems, we propose a large-scale continual learning framework based on the convolutional prototype network (CPN), which does not save raw data for old classes. In this paper, several basic strategies have been proposed for the initial training stage to enhance the feature extraction ability and robustness of the network, which can improve the performance of the model in continual learning. In addition, we propose two practical methods in varying feature space (parameters of feature extractor are changeable) and fixed feature space (parameters of feature extractor are fixed), which enable the model to carry out large-scale continual learning. The proposed method does not save the raw data of old classes and enables simultaneous classification of all existing classes without knowing the incremental batch number. Experiments on the CASIA-AHCDB dataset with 5000 character classes demonstrate the effectiveness and superiority of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324000347",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Character (mathematics)",
      "Chemistry",
      "Computer science",
      "Engineering",
      "Extractor",
      "Feature (linguistics)",
      "Feature extraction",
      "Gene",
      "Geometry",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Process engineering",
      "Programming language",
      "Quantum mechanics",
      "Raw data",
      "Robustness (evolution)",
      "Scale (ratio)"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Yue"
      },
      {
        "surname": "Zhang",
        "given_name": "Xu-Yao"
      },
      {
        "surname": "Zhang",
        "given_name": "Zhaoxiang"
      },
      {
        "surname": "Liu",
        "given_name": "Cheng-Lin"
      }
    ]
  },
  {
    "title": "Topological optimization of continuous action iterated dilemma based on finite-time strategy using DQN",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.04.010",
    "abstract": "In this paper, a finite-time convergent continuous action iterated dilemma (CAID) with topological optimization is proposed to overcome the limitations of traditional methods. Asymptotic stability in traditional CAID does not provide information about the rate of convergence or the dynamics of the system in the finite time. There are no effective methods to analyze its convergence time in previous works. We made some efforts to solve these problems. Firstly, CAID is proposed by enriching the players’ strategies as continuous, which means the player can choose an intermediate state between cooperation and defection. And discount rate is considered to imitate that players cannot learn accurately based on strategic differences. Then, to analyze the convergence time of CAID, a finite-time convergent analysis based on the Lyapunov function is introduced. Furthermore, the optimal communication topology generation method based on the Deep Q-learning (DQN) is proposed to explore a better game structure. At last, the simulation shows the effectiveness of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524001119",
    "keywords": [
      "Action (physics)",
      "Combinatorics",
      "Computer science",
      "Iterated function",
      "Mathematical analysis",
      "Mathematics",
      "Physics",
      "Quantum mechanics",
      "Topology (electrical circuits)"
    ],
    "authors": [
      {
        "surname": "Jin",
        "given_name": "Xiaoyue"
      },
      {
        "surname": "Li",
        "given_name": "Haojing"
      },
      {
        "surname": "Yu",
        "given_name": "Dengxiu"
      },
      {
        "surname": "Wang",
        "given_name": "Zhen"
      },
      {
        "surname": "Li",
        "given_name": "Xuelong"
      }
    ]
  },
  {
    "title": "Brain-driven facial image reconstruction via StyleGAN inversion with improved identity consistency",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110331",
    "abstract": "The reconstruction of visual stimuli from fMRI data represents a major technological and scientific challenge at the forefront of contemporary neuroscience research. Deep learning techniques have played a critical role in advancing decoding models for visual stimulus reconstruction from fMRI data. Particularly, the use of advanced GANs has resulted in significant improvements in the quality of image generation, providing a powerful tool for addressing the challenges of this complex task. However, none of these studies have taken into account the inherent characteristics of the stimulus contents themselves; This, in turn, leads to unsatisfactory outcomes, as demonstrated by the inconsistent identity between reconstructed faces and ground truth in the decoding of facial images. In order to tackle this challenge, we introduce a new framework aimed at enhancing the accuracy of reconstructing facial images from fMRI data. Our key innovation involves extracting and disentangling multi-level visual information from brain signals in the latent space and optimizing high-level features for facial identity control using identity loss. Specifically, our framework uses StyleGAN inversion to extract hierarchical latent codes from images, which are then bridged to fMRI data through transformation blocks. Additionally, we introduce a multi-stage refinement method to enhance the accuracy of reconstructed faces, which involves progressively updating fMRI latent codes with custom loss functions designed for both feature- and image-wise optimization. Our experimental results demonstrate that our proposed framework effectively achieves two critical objectives: (1) accurate facial image reconstruction from fMRI data and (2) preservation of identity characteristics with a high level of consistency.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324000827",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Cognitive psychology",
      "Computer science",
      "Computer vision",
      "Consistency (knowledge bases)",
      "Decoding methods",
      "Iterative reconstruction",
      "Neuroimaging",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Psychology",
      "Stimulus (psychology)"
    ],
    "authors": [
      {
        "surname": "Ren",
        "given_name": "Ziqi"
      },
      {
        "surname": "Li",
        "given_name": "Jie"
      },
      {
        "surname": "Wu",
        "given_name": "Lukun"
      },
      {
        "surname": "Xue",
        "given_name": "Xuetong"
      },
      {
        "surname": "Li",
        "given_name": "Xin"
      },
      {
        "surname": "Yang",
        "given_name": "Fan"
      },
      {
        "surname": "Jiao",
        "given_name": "Zhicheng"
      },
      {
        "surname": "Gao",
        "given_name": "Xinbo"
      }
    ]
  },
  {
    "title": "RASNet: Renal automatic segmentation using an improved U-Net with multi-scale perception and attention unit",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110336",
    "abstract": "For the treatment of renal disease, the application of radioactive equipment has become one of the important methods. Accurate segmentation of renal contour plays an important role in clinical diagnosis. However, manual renal contour drawing is not only inefficient but also prone to inaccurate outlining results due to different manual proficiency and fatigue caused by long-term work. There is little research on automatic renal segmentation with renal dynamic imaging. To address this issue, an improved model based on a deep neural network called Renal Automatic Segmentation Network (RASNet) is proposed, to aid in the automatic segmentation of renal contours. Besides, a multi-scale spatial perception module and a decoding module with attention connection are introduced to enrich the semantic information and further improve the accuracy of network segmentation. Extensive experiments were conducted on a renal dynamic medical image database established in this paper. Analysis results show the superiority of the proposed RASNet to several existing segmentation frameworks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324000876",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer vision",
      "Decoding methods",
      "Image segmentation",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Scale (ratio)",
      "Scale-space segmentation",
      "Segmentation",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Cao",
        "given_name": "Gaoyu"
      },
      {
        "surname": "Sun",
        "given_name": "Zhanquan"
      },
      {
        "surname": "Wang",
        "given_name": "Chaoli"
      },
      {
        "surname": "Geng",
        "given_name": "Hongquan"
      },
      {
        "surname": "Fu",
        "given_name": "Hongliang"
      },
      {
        "surname": "Yin",
        "given_name": "Zhong"
      },
      {
        "surname": "Pan",
        "given_name": "Minlan"
      }
    ]
  },
  {
    "title": "Integrated convolutional neural networks for joint super-resolution and classification of radar images",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110351",
    "abstract": "Deep learning techniques have been widely used for two-dimensional (2D) and three-dimensional (3D) computer vision problems, such as object detection, super-resolution (SR) and classification to name a few. Radar images suffer from poor resolution as compared to optical images, hence developing a high-accuracy model to solve computer vision problems, such as a classifier, is a challenge. This is because of the lack of high-frequency details in the input images which makes it difficult for the classifier model to generate accurate predictions. Ways of addressing this challenge include training the learning model with a large dataset or using a more complicated model, such as deeper layer architecture. However, employing such approaches might result in the overfitting of the model, where the model might not generalize well on previously unseen data. Also, generating a large dataset for training the model is a challenging task, especially in the case of radar images. An alternate solution for achieving high accuracy in radar classification problems is provided in this paper wherein a CNN-enabled super-resolution (SR) model is integrated with the classifier model. The SR model is trained to generate high-resolution (HR) millimeter-wave (mmW) images from any input low-resolution (LR) mmW images. These resolved images from the SR model will be used by the classifier model to classify the input images into appropriate classes, consisting of threat and non-threat objects. The training data for the dual CNN layers are generated using a numerical model of a near-field coded-aperture computational imaging (CI) system. This trained dual CNN model is tested with simulated data generated from the CI numerical model wherein a high classification accuracy of 95% and a fast inference time of 0.193 s are obtained, making it suitable for real-time automated threat classification applications. For fair comparison, the developed CNN model is also validated with experimentally generated reconstruction data, in which case, a classification accuracy of 94% is obtained.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032400102X",
    "keywords": [
      "Architectural engineering",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Engineering",
      "Joint (building)",
      "Pattern recognition (psychology)",
      "Radar",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Sharma",
        "given_name": "Rahul"
      },
      {
        "surname": "Deka",
        "given_name": "Bhabesh"
      },
      {
        "surname": "Fusco",
        "given_name": "Vincent"
      },
      {
        "surname": "Yurduseven",
        "given_name": "Okan"
      }
    ]
  },
  {
    "title": "A patch distribution-based active learning method for multiple instance Alzheimer's disease diagnosis",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110341",
    "abstract": "Medical data, particularly the complex brain imaging structures, acquisition presents significant difficulties and high diagnostic expenses, resulting in a scarcity of the trainable samples in the real-world scenarios. To overcome this limitation, we present an active learning-based sampling strategy that selects the most informative samples from the unlabeled candidate sample pool for expert annotation, leading to high classification performance with a reduced number of training samples. This study adopts a patch-level perspective and introduces a multi-instance learning framework for Alzheimer's Disease diagnosis. Initially, a patch pre-selection module is designed to identify pathology-prone regions while excluding background areas and irrelevant information. Subsequently, an inner-patch local attention mechanism block and an outer-patch global attention mechanism block are developed to enhance the extraction of discriminative local and global information by the network model. Finally, an active learning sampling strategy is devised to minimize the costs associated with data acquisition and expert annotation in medical domain. The effectiveness of the proposed network framework and active learning strategy was validated through four sets of control experiments on the ADNI dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032400092X",
    "keywords": [
      "Active learning (machine learning)",
      "Annotation",
      "Artificial intelligence",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Discriminative model",
      "Filter (signal processing)",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Sample (material)",
      "Sampling (signal processing)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Tianxiang"
      },
      {
        "surname": "Dai",
        "given_name": "Qun"
      }
    ]
  },
  {
    "title": "Enhancing identification for person search with multi-scale multi-grained representation learning",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110361",
    "abstract": "Person Search aims to simultaneously address Person Detection and Person Re-ID. There are various challenges in person search such as significant scale variations, occlusions, and partial instances. In this paper, we propose a Multi-Scale Multi-Grained (MSMG) sequential network for end-to-end person search, intended to alleviate these issues. To generate re-id representations robust to scale changes, MSMG leverages multi-scale RoI features and aggregates them with a proposed Multi-Scale feature Aggregation Encoder (MSAE). In this way, the aggregated multi-scale re-id features are enriched with more semantic information and detailed information, thereby being more discriminative for identification. Moreover, to produce re-id representations more robust to occlusions and partial instances, MSMG introduces a Multi-Grained feature Learning Decoder (MGLD) focused on multi-grained feature learning. MGLD adaptively decodes multi-grained re-id representations with more accurate semantic information through a regional deformable cross-attention module. Finally, the multi-scale multi-grained re-id representation substantially improves the identification accuracy under challenging cases. Through comprehensive experiments, we demonstrate that our method achieves state-of-the-art performance on two benchmark datasets. On the challenging PRW benchmark, MSMG obtains the best-reported results with a mean average precision (mAP) score of 61.3%.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324001122",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Cartography",
      "Computer science",
      "Geography",
      "Identification (biology)",
      "Law",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Scale (ratio)"
    ],
    "authors": [
      {
        "surname": "Han",
        "given_name": "Zhixiong"
      },
      {
        "surname": "Ma",
        "given_name": "Bingpeng"
      }
    ]
  },
  {
    "title": "Co–TES: Learning noisy labels with a Co-Teaching Exchange Student method",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.04.001",
    "abstract": "The performance of a machine-learning model is influenced by two main factors: the structure of the model, and the quality of the dataset it processes. As high-quality labeled data in substantial size is often difficult to obtain, there are ongoing efforts to develop machine learning algorithms that are robust with noisy datasets. Among these algorithms, multi-network learning utilizes learning from a noisy dataset by the selection and filtering of samples through multiple learning networks. We propose an improved co-teaching algorithm termed Co-TES that leverages different models with various architectures. Co-TES extracts different features from each iteration of data selection and makes the model more robust with the same quality dataset. Numerical results show that the proposed method can lead to faster performance gains in the early to mid-range.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524001028",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Machine learning",
      "Mathematics education",
      "Natural language processing",
      "Psychology",
      "Teaching method"
    ],
    "authors": [
      {
        "surname": "Shin",
        "given_name": "Chan Ho"
      },
      {
        "surname": "Oh",
        "given_name": "Seong-jun"
      }
    ]
  },
  {
    "title": "Adapt only once: Fast unsupervised person re-identification via relevance-aware guidance",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110360",
    "abstract": "Unsupervised domain adaptive person re-identification (UDA person reID) defines a task where labels in target domains are totally unknown while source domains are fully labeled. Assigning reliable labels quickly is a critical issue for UDA person reID that could be applied in the real-world scenarios. Recent studies focus on obtaining pseudo labels by clustering algorithms and then training the reID model with these labels. However, the main limitation of these methods is the high time complexity, which is caused by the calculation of all pair-wise similarities and multiple iterations in the clustering algorithm to obtain reliable results. When the data is very large or the feature dimensions are very high, the memory and time cost requirements of the clustering algorithm can increase rapidly. In this paper, we provide a fast unsupervised domain adaptive person reID framework (FUReID), which calculates the relevance between unlabeled samples only once to adapt to the new scenarios without any iterations in the stage of label generation. Especially, instead of pursuing accurate labels, FUReID considers constructing a lightweight paradigm to generate coarse labels and then refine these labels during the training stage. Therefore, FUReID designs a prototype-guided labeling method that only relies on calculating the relevance between the prototype vectors and the samples, and assigning coarse labels with noise. Then, to alleviate the issue of noise, FUReID designs a label-flexible training network with an adaptive selection strategy to refine those coarse labels progressively. For several widely-used person reID datasets, our method achieves 81.7%, 26.2%, and 87.7% in mAP on Market1501, MSMT17 and PersonX, respectively. Code is available at https://github.com/AILab90/FUReID.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324001110",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Computer science",
      "Identification (biology)",
      "Law",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Political science",
      "Relevance (law)"
    ],
    "authors": [
      {
        "surname": "Peng",
        "given_name": "Jinjia"
      },
      {
        "surname": "Yu",
        "given_name": "Jiazuo"
      },
      {
        "surname": "Wang",
        "given_name": "Chengjun"
      },
      {
        "surname": "Wang",
        "given_name": "Huibing"
      },
      {
        "surname": "Fu",
        "given_name": "Xianping"
      }
    ]
  },
  {
    "title": "Orientation-aware leg movement learning for action-driven human motion prediction",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110317",
    "abstract": "The task of action-driven human motion prediction aims to forecast future human motion based on the observed sequence while respecting the given action label. It requires modeling not only the stochasticity within human motion but the smooth yet realistic transition between multiple action labels. However, the fact that most datasets do not contain such transition data complicates this task. Existing work tackles this issue by learning a smoothness prior to simply promote smooth transitions, yet doing so can result in unnatural transitions especially when the history and predicted motions differ significantly in orientations. In this paper, we argue that valid human motion transitions should incorporate realistic leg movements to handle orientation changes, and cast it as an action-conditioned in-betweening (ACB) learning task to encourage transition naturalness. Because modeling all possible transitions is virtually unreasonable, our ACB is only performed on very few selected action classes with active gait motions, such as “Walk” or “Run”. Specifically, we follow a two-stage forecasting strategy by first employing the motion diffusion model to generate the target motion with a specified future action, and then producing the in-betweening to smoothly connect the observation and prediction to eventually address motion prediction. Our method is completely free from the labeled motion transition data during training. To show the robustness of our approach, we generalize our trained in-betweening learning model on one dataset to two unseen large-scale motion datasets to produce natural transitions. Extensive experimental evaluations on three benchmark datasets demonstrate that our method yields the state-of-the-art performance in terms of visual quality, prediction accuracy, and action faithfulness.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324000682",
    "keywords": [
      "Acoustics",
      "Action (physics)",
      "Action recognition",
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Computer vision",
      "Geometry",
      "Human motion",
      "Mathematics",
      "Motion (physics)",
      "Movement (music)",
      "Orientation (vector space)",
      "Physics",
      "Quantum mechanics"
    ],
    "authors": [
      {
        "surname": "Gu",
        "given_name": "Chunzhi"
      },
      {
        "surname": "Zhang",
        "given_name": "Chao"
      },
      {
        "surname": "Kuriyama",
        "given_name": "Shigeru"
      }
    ]
  },
  {
    "title": "Explainable AI for time series via Virtual Inspection Layers",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110309",
    "abstract": "The field of eXplainable Artificial Intelligence (XAI) has witnessed significant advancements in recent years. However, the majority of progress has been concentrated in the domains of computer vision and natural language processing. For time series data, where the input itself is often not interpretable, dedicated XAI research is scarce. In this work, we put forward a virtual inspection layer for transforming the time series to an interpretable representation and allows to propagate relevance attributions to this representation via local XAI methods. In this way, we extend the applicability of XAI methods to domains (e.g. speech) where the input is only interpretable after a transformation. In this work, we focus on the Fourier Transform which, is prominently applied in the preprocessing of time series, with Layer-wise Relevance Propagation (LRP) and refer to our method as DFT-LRP. We demonstrate the usefulness of DFT-LRP in various time series classification settings like audio and medical data. We showcase how DFT-LRP reveals differences in the classification strategies of models trained in different domains (e.g., time vs. frequency domain) or helps to discover how models act on spurious correlations in the data.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324000608",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Biology",
      "Chemistry",
      "Computer science",
      "Data mining",
      "Domain (mathematical analysis)",
      "Field (mathematics)",
      "Focus (optics)",
      "Gene",
      "Law",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Optics",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Physics",
      "Political science",
      "Politics",
      "Preprocessor",
      "Pure mathematics",
      "Relevance (law)",
      "Representation (politics)",
      "Series (stratigraphy)",
      "Spurious relationship",
      "Time series",
      "Transformation (genetics)"
    ],
    "authors": [
      {
        "surname": "Vielhaben",
        "given_name": "Johanna"
      },
      {
        "surname": "Lapuschkin",
        "given_name": "Sebastian"
      },
      {
        "surname": "Montavon",
        "given_name": "Grégoire"
      },
      {
        "surname": "Samek",
        "given_name": "Wojciech"
      }
    ]
  },
  {
    "title": "Wavelet-based Auto-Encoder for simultaneous haze and rain removal from images",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110370",
    "abstract": "Noise introduced due to weather can reduce the efficiency of computer vision applications as the visibility of the objects in images is greatly affected. Haze and rain are the most common weather conditions seen in nature. However, most of the algorithms found in the literature apply rain and haze removal approaches separately. To this end, in this paper, we propose a novel Wavelet-based deep Auto-encoder, called WAE, for simultaneously removing the haze and rain effects from images. The proposed network uses wavelet transformation and inverse wavelet transformation as an alternative to down-sampling and up-sampling operations, respectively, in order to add sparsity to the network. By training the model on both spatial and frequency domains, it learns non-stationary features that are found to be useful to remove haze and rain effects from images. The proposed model is tested on several rain and haze-affected image datasets, and it performs well in terms of standard evaluation metrics like structural similarity index measure and peak signal-to-noise ratio. The code can be found at : https://github.com/asfakali/WAE.git.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324001213",
    "keywords": [
      "Artificial intelligence",
      "Autoencoder",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Encoder",
      "Geography",
      "Geology",
      "Haze",
      "Meteorology",
      "Operating system",
      "Pattern recognition (psychology)",
      "Remote sensing",
      "Wavelet"
    ],
    "authors": [
      {
        "surname": "Ali",
        "given_name": "Asfak"
      },
      {
        "surname": "Sarkar",
        "given_name": "Ram"
      },
      {
        "surname": "Chaudhuri",
        "given_name": "Sheli Sinha"
      }
    ]
  },
  {
    "title": "Interpretable answer retrieval based on heterogeneous network embedding",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.03.023",
    "abstract": "Community question answering is a rising technology based on users' autonomous interactive behaviors, such as posting their issues, answering questions based on their experience, and commenting on existing questions. As a result of its use of natural language for communication and stimulation of user interest in information sharing, it has increasingly taken the place of other channels as the main way that people learn new things. Multi-type entity characteristics fusion and poor answer interpretability are the two major concerns that currently plague community answer prediction research. The Interpretable Answer Retrieval Method Based on Heterogeneous Network Embedding (IARHNE) is what we present in this work. It combines complex entity features and generates interpretable predicted answers. In order to incorporate the interactions of several kinds of individuals in answer social retrieval, we first build a heterogeneity graph. In order to acquire entity embeddings, we secondly use the heterogeneous graph neural network. We then adopt the vector distance to convert the entity matching problem in the heterogeneous information network into a homogeneous node similarity job. Finally, using entity correlation to predict answers, we provide a list of answers to the new query and interpret them using meta-paths. Comparative studies using three authentic datasets demonstrate the benefits of IARHNE for interpretative question-answering research.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524000904",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Embedding",
      "Graph",
      "Heterogeneous network",
      "Homogeneous",
      "Information retrieval",
      "Interpretability",
      "Machine learning",
      "Matching (statistics)",
      "Mathematics",
      "Physics",
      "Question answering",
      "Statistics",
      "Telecommunications",
      "Theoretical computer science",
      "Thermodynamics",
      "Wireless",
      "Wireless network"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Yongliang"
      },
      {
        "surname": "Pan",
        "given_name": "Xiao"
      },
      {
        "surname": "Li",
        "given_name": "Jinghui"
      },
      {
        "surname": "Dou",
        "given_name": "Shimao"
      },
      {
        "surname": "Wang",
        "given_name": "Xiaoxue"
      }
    ]
  },
  {
    "title": "Kinematics modeling network for video-based human pose estimation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110287",
    "abstract": "Estimating human poses from videos is critical in human–computer interaction. Joints cooperate rather than move independently during human movement. There are both spatial and temporal correlations between joints. Despite the positive results of previous approaches, most of them focus on modeling the spatial correlation between joints while only straightforwardly integrating features along the temporal dimension, which ignores the temporal correlation between joints. In this work, we propose a plug-and-play kinematics modeling module (KMM) to explicitly model temporal correlations between joints across different frames by calculating their temporal similarity. In this way, KMM can capture motion cues of the current joint relative to all joints in different time. Besides, we formulate video-based human pose estimation as a Markov Decision Process and design a novel kinematics modeling network (KIMNet) to simulate the Markov Chain, allowing KIMNet to locate joints recursively. Our approach achieves state-of-the-art results on two challenging benchmarks. In particular, KIMNet shows robustness to the occlusion. Code will be released at https://github.com/YHDang/KIMNet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324000384",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Classical mechanics",
      "Computer science",
      "Computer vision",
      "Gene",
      "Hidden Markov model",
      "Kinematics",
      "Machine learning",
      "Markov chain",
      "Physics",
      "Plug-in",
      "Pose",
      "Programming language",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Dang",
        "given_name": "Yonghao"
      },
      {
        "surname": "Yin",
        "given_name": "Jianqin"
      },
      {
        "surname": "Zhang",
        "given_name": "Shaojie"
      },
      {
        "surname": "Liu",
        "given_name": "Jiping"
      },
      {
        "surname": "Hu",
        "given_name": "Yanzhu"
      }
    ]
  },
  {
    "title": "Efficient image analysis with triple attention vision transformer",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110357",
    "abstract": "This paper introduces TrpViT, a novel triple attention vision transformer that efficiently captures both local and global features. The proposed architecture tackles global information acquisition by employing three complementary attention mechanisms in a unique attention block: Window, Dilated, and Channel attention. This attention block extracts spatially local features while expanding the receptive field to capture richer global context. By integrating this attention block with convolution, a new C-C-T-T architecture is formed. We rigorously evaluate TrpViT, demonstrating state-of-the-art performance on various computer vision tasks, including image classification, 2D and 3D object detection, instance segmentation, and low-level image colorization. Notably, TrpViT achieves strong accuracy across all parameter scales, highlighting its computational efficiency and effectiveness.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324001080",
    "keywords": [
      "Architecture",
      "Art",
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Cognitive neuroscience of visual object recognition",
      "Computer science",
      "Computer vision",
      "Geometry",
      "Image (mathematics)",
      "Mathematics",
      "Object (grammar)",
      "Object detection",
      "Optical flow",
      "Pattern recognition (psychology)",
      "Segmentation",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Gehui"
      },
      {
        "surname": "Zhao",
        "given_name": "Tongtong"
      }
    ]
  },
  {
    "title": "A categorical interpretation of state merging algorithms for DFA inference",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110326",
    "abstract": "We use Category Theory to interpret the family of algorithms for inference of DFAs that work by merging states. This interpretation allows us to characterize the structure of the search space and to define criteria for the convergence of these algorithms to the correct DFA. We also prove that the well-known EDSM algorithm does not identify DFAs in the limit.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324000773",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Categorical variable",
      "Computer science",
      "Convergence (economics)",
      "Economic growth",
      "Economics",
      "Inference",
      "Interpretation (philosophy)",
      "Limit (mathematics)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Programming language",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Vilar",
        "given_name": "Juan Miguel"
      }
    ]
  },
  {
    "title": "Multi-task meta label correction for time series prediction",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110319",
    "abstract": "Time series classification faces two unavoidable problems. One is partial feature information and the other is poor label quality, which may affect model performance. To address the above issues, we create a label correction method to time series data with meta-learning under a multi-task framework. There are three main contributions. First, we train the label correction model with a two-branch neural network in the outer loop. While in the model-agnostic inner loop, we use pre-existing classification models in a multi-task way and jointly update the meta-knowledge so as to help us achieve adaptive labeling on complex time series. Second, we devise new data visualization methods for both image patterns of the historical data and data in the prediction horizon. Finally, we test our method with various financial datasets, including XOM, S&P500, and SZ50. Results show that our method is more effective and accurate than some existing label correction techniques.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324000700",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer science",
      "Data mining",
      "Economics",
      "Feature (linguistics)",
      "Linguistics",
      "Machine learning",
      "Management",
      "Meta learning (computer science)",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Series (stratigraphy)",
      "Task (project management)",
      "Time series",
      "Visualization"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Luxuan"
      },
      {
        "surname": "Gao",
        "given_name": "Ting"
      },
      {
        "surname": "Wei",
        "given_name": "Wei"
      },
      {
        "surname": "Dai",
        "given_name": "Min"
      },
      {
        "surname": "Fang",
        "given_name": "Cheng"
      },
      {
        "surname": "Duan",
        "given_name": "Jinqiao"
      }
    ]
  },
  {
    "title": "Self-supervised multi-task learning for medical image analysis",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110327",
    "abstract": "Deep learning is crucial for preliminary screening and diagnostic assistance based on medical image analysis. However, limited annotated data and complex anatomical structures challenge existing models as they struggle to capture anatomical context information effectively. In response, we propose a novel self-supervised multi-task learning framework (SSMT), which integrates two key modules: a discriminative-based module and a generative-based module. These modules collaborate through multiple proxy tasks, encouraging models to learn global discriminative representations and local fine-grained representations. Additionally, we introduce an efficient uniformity regularization to further enhance the learned representations. To demonstrate the effectiveness of SSMT, we conduct extensive experiments on six public Chest X-ray image datasets. Our results highlight that SSMT not only outperforms existing state-of-the-art methods but also achieves comparable performance to the supervised model in challenging downstream tasks. The ablation study demonstrates collaboration between the key components of SSMT, showcasing its potential for advancing medical image analysis.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324000785",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Engineering",
      "Image (mathematics)",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Systems engineering",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Yu",
        "given_name": "Huihui"
      },
      {
        "surname": "Dai",
        "given_name": "Qun"
      }
    ]
  },
  {
    "title": "Attention-map augmentation for hypercomplex breast cancer classification",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.04.014",
    "abstract": "Breast cancer is the most widespread neoplasm among women and early detection of this disease is critical. Deep learning techniques have become of great interest to improve diagnostic performance. However, distinguishing between malignant and benign masses in whole mammograms poses a challenge, as they appear nearly identical to an untrained eye, and the region of interest (ROI) constitutes only a small fraction of the entire image. In this paper, we propose a framework, parameterized hypercomplex attention maps (PHAM), to overcome these problems. Specifically, we deploy an augmentation step based on computing attention maps. Then, the attention maps are used to condition the classification step by constructing a multi-dimensional input comprised of the original breast cancer image and the corresponding attention map. In this step, a parameterized hypercomplex neural network (PHNN) is employed to perform breast cancer classification. The framework offers two main advantages. First, attention maps provide critical information regarding the ROI and allow the neural model to concentrate on it. Second, the hypercomplex architecture has the ability to model local relations between input dimensions thanks to hypercomplex algebra rules, thus properly exploiting the information provided by the attention map. We demonstrate the efficacy of the proposed framework on both mammography images as well as histopathological ones. We surpass attention-based state-of-the-art networks and the real-valued counterpart of our approach. The code of our work is available at https://github.com/ispamm/AttentionBCS.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524001156",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Breast cancer",
      "Cancer",
      "Code (set theory)",
      "Computer science",
      "Deep learning",
      "Geometry",
      "Hypercomplex number",
      "Internal medicine",
      "Machine learning",
      "Mammography",
      "Mathematics",
      "Medicine",
      "Parameterized complexity",
      "Pattern recognition (psychology)",
      "Programming language",
      "Quaternion",
      "Region of interest",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Lopez",
        "given_name": "Eleonora"
      },
      {
        "surname": "Betello",
        "given_name": "Filippo"
      },
      {
        "surname": "Carmignani",
        "given_name": "Federico"
      },
      {
        "surname": "Grassucci",
        "given_name": "Eleonora"
      },
      {
        "surname": "Comminiello",
        "given_name": "Danilo"
      }
    ]
  },
  {
    "title": "FSDA: Frequency re-scaling in data augmentation for corruption-robust image classification",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110332",
    "abstract": "Modern convolutional neural networks (CNNs) are used in various applications, including computer vision, speech recognition, and robotics. However, practical usage in various applications requires large-scale datasets, and real-world data contains various corruptions that degrade the model’s performance owing to the inconsistencies in the training and testing distributions. In this study, we propose Frequency re-Scaling Data Augmentation (FSDA) to improve the classification performance, robustness against corruption, and localizability of classifiers trained on various image classification datasets. Our method consists of two processes: mask generation process (MGP) and pattern re-scaling process (PSP). MGP clusters the frequency domain spectra to produce similar frequency patterns, and then PSP scales frequency by learning rescaling parameters from frequency patterns. Because the CNN classifies images by focusing on their structural features highlighted with FSDA, CNN trained with the proposed method has more robustness against corruption than that with the other data augmentations (DAs). Our technique outperforms the existing DAs on four public image classification datasets, including the CIFAR-10/100, STL-10, and ImageNet. Particularly, our strategy increases the robustness of the classifier against the different corruption errors by an average of 5.04% over the baseline.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324000839",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Geometry",
      "Image (mathematics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Scaling"
    ],
    "authors": [
      {
        "surname": "Nam",
        "given_name": "Ju-Hyeon"
      },
      {
        "surname": "Lee",
        "given_name": "Sang-Chul"
      }
    ]
  },
  {
    "title": "Unsupervised incremental estimation of Gaussian mixture models with 1D split moves",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110306",
    "abstract": "In this paper, we propose a new type of split rule for incremental estimation of Gaussian Mixture Models with model selection. Split-based methods typically start with a mixture composed of a single component representing all data, and successively split and optimize components for a given model selection criterion. These algorithms are typically faster than alternatives, but depend critically on the component splitting method, since a good split rule promotes a faster convergence of the mixture optimization phase. We propose a new efficient and robust split rule that projects mixture components onto a 1D subspace and fits a two-component model to the projected data with the Expectation Maximization algorithm. The proposed approach is fast and robust to parameter tuning, being the ideal choice for applications that favor speed while still maintaining an acceptable accuracy. We illustrate the validity of the method through a series of experiments on synthetic and real datasets comparing the proposed method to alternatives of the state-of-the-art in terms of efficiency, accuracy, and sensitivity to parameter tuning.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324000578",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Engineering",
      "Estimation",
      "Gaussian",
      "Gaussian process",
      "Mathematics",
      "Mixture model",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Systems engineering"
    ],
    "authors": [
      {
        "surname": "Greggio",
        "given_name": "Nicola"
      },
      {
        "surname": "Bernardino",
        "given_name": "Alexandre"
      }
    ]
  },
  {
    "title": "LAFED: Towards robust ensemble models via Latent Feature Diversification",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2023.110225",
    "abstract": "Adversarial examples pose a significant challenge to the security of deep neural networks (DNNs). In order to defend against malicious attacks, adversarial training forces DNNs to learn more robust features by suppressing generalizable but non-robust features, which boosts the robustness while suffering from significant accuracy drops on clean images. Ensemble training, on the other hand, trains multiple sub-models to predict data for improved robustness and still achieves desirable accuracy on clean data. Despite these efforts, previous ensemble methods are still susceptible to attacks and fail to increase model diversity as the size of the ensemble group increases. In this work, we revisit the model diversity from the perspective of data and discover that high similarity between training batches decreases feature diversity and weakens ensemble robustness. To this end, we propose Latent Feature Diversification (LAFED), which reconstructs training sets with diverse features during the optimization, enhancing the overall robustness of an ensemble. For each sub-model, LAFED treats the vulnerability extracted from other sub-models as raw data, which is then combined with round-changed weights with a stochastic manner in the latent space. This results in the formation of new features, remarkably reducing the similarity of learned representations between the sub-models. Furthermore, LAFED enhances feature diversity within the ensemble model by utilizing hierarchical smoothed labels. Extensive experiments illustrate that LAFED significantly improves diversity among sub-models and enhances robustness against adversarial attacks compared to current methods. The code is publicly available at https://github.com/zhuangwz/LAFED.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320323009226",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Data mining",
      "Ensemble forecasting",
      "Ensemble learning",
      "Feature vector",
      "Gene",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Zhuang",
        "given_name": "Wenzi"
      },
      {
        "surname": "Huang",
        "given_name": "Lifeng"
      },
      {
        "surname": "Gao",
        "given_name": "Chengying"
      },
      {
        "surname": "Liu",
        "given_name": "Ning"
      }
    ]
  },
  {
    "title": "A Fast Multi-Network K-Dependence Bayesian Classifier for Continuous Features",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110299",
    "abstract": "One of the Bayesian network classifiers widely used in the classification is K-dependence Bayesian (KDB). However, most of the KDB classifiers build a single network on a class variable without considering dependencies between features in each class. Moreover, many KDB classifiers need the discretization process to handle continuous features. This paper aims to propose a fast Multi-Network K-Dependence Bayesian (MNKDB) classifier for continuous features. According to this aim, we propose a non-parametric approach that efficiently identifies dependencies between continuous features in each class with a low computational cost and without discretizing continuous features. The results indicate that the MNKDB classifier is more accurate than the state-of-the-art KDB classifiers, especially for datasets with more than three classes. The MNKDB classifier not only decreases the classification time but also deals with continuous variables without discretizing them. The results for K=2 show that the MNKDB classifier is 36.5, 31.8, and 14.2 times faster and 4.13%, 5.15%, and 5.48% more accurate than the state-of-the-art FKDB (Flexible KDB), KMM-KDB (Kernel Mixture Model based on KDB), and SKDB (Scalable KDB) classifiers, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324000505",
    "keywords": [
      "Artificial intelligence",
      "Bayesian network",
      "Bayesian probability",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Machine learning",
      "Mathematics",
      "Naive Bayes classifier",
      "Parametric statistics",
      "Pattern recognition (psychology)",
      "Statistics",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Khodayari-Samghabadi",
        "given_name": "Imaneh"
      },
      {
        "surname": "Mohammad-Khanli",
        "given_name": "Leyli"
      },
      {
        "surname": "Tanha",
        "given_name": "Jafar"
      }
    ]
  },
  {
    "title": "Robust spectral embedded bilateral orthogonal concept factorization for clustering",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110308",
    "abstract": "Concept factorization (CF), unlike nonnegative matrix factorization (NMF), can handle data with negative values by approximating the original data with two low-dimensional nonnegative matrices and itself. Nevertheless, existing CF-based methods continue to suffer from the two issues specified as follows: (1) Their effectiveness is reduced by the high degree of factorization freedom and the two-stage mismatch between factorization and category acquisition, and (2) their robustness drops significantly when dealing with complex noise. In response to the aforementioned issues, we propose a robust spectral-embedded bilateral orthogonal concept factorization (RSOCF) model for clustering. It constrains the factor matrices as orthogonal matrices to decrease the freedom and obtain samples’ categories directly after factorization, which can significantly improve clustering effectiveness. Moreover, correntropy is introduced into RSOCF to improve its robustness to complex noise. To optimize the non-convex RSOCF model, a half-quadratic-based algorithm is devised. Numerous experiments demonstrate that RSOCF surpasses other state-of-the-art methods in terms of clustering effectiveness and robustness.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324000591",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Factorization",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Spectral clustering"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Ben"
      },
      {
        "surname": "Wu",
        "given_name": "Jinghan"
      },
      {
        "surname": "Zhou",
        "given_name": "Yu"
      },
      {
        "surname": "Zhang",
        "given_name": "Xuetao"
      },
      {
        "surname": "Lin",
        "given_name": "Zhiping"
      },
      {
        "surname": "Nie",
        "given_name": "Feiping"
      },
      {
        "surname": "Chen",
        "given_name": "Badong"
      }
    ]
  },
  {
    "title": "Edge-labeling based modified gated graph network for few-shot learning",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110264",
    "abstract": "Accurate determination of similarity between samples is fundamental and critical for graph network based few-shot learning tasks. Previous approaches typically employ convolutional neural networks to obtain relations between nodes. However, these networks are not adept at handling node features in vector form. To overcome this limitation, we proposed a modified gated graph network (MGGN) that uniquely integrates graph networks and modified gated recurrent units (M-GRU) for few-shot classification. The introduced M-GRU mitigates the loss of label information from the initial graph and reduces computational complexity. The MGGN contains two modules that alternately update node and edge features. The node update module leverages a gating mechanism to integrate edge features into node update weights, fostering a learnable node aggregation process. The edge update component perceives the trend in edge feature changes and establishes long-term dependencies. Experimental results on two benchmark datasets demonstrate that our MGGN achieves comparable performance to state-of-the-art methods. The code is available at https://github.com/zpx16900/MGGN.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324000153",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Convolutional neural network",
      "Engineering",
      "Enhanced Data Rates for GSM Evolution",
      "Feature learning",
      "Geodesy",
      "Geography",
      "Graph",
      "Node (physics)",
      "Pattern recognition (psychology)",
      "Structural engineering",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Zheng",
        "given_name": "Peixiao"
      },
      {
        "surname": "Guo",
        "given_name": "Xin"
      },
      {
        "surname": "Chen",
        "given_name": "Enqing"
      },
      {
        "surname": "Qi",
        "given_name": "Lin"
      },
      {
        "surname": "Guan",
        "given_name": "Ling"
      }
    ]
  },
  {
    "title": "Kreĭn twin support vector machines for imbalanced data classification",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.03.017",
    "abstract": "Conventional classification assumes a balanced sample distribution among classes. However, such a premise leads to biased performance over the majority class (with the highest number of instances). The Twin Support Vector Machines (TWSVM) obtained great prominence due to their low computational burden compared to the standard SVM. Besides, traditional machine learning seeks methods whose solution depends on a convex problem or semi-positive definite similarity matrices. Yet, this kind of matrix cannot adequately represent many real-world applications. The above defines the need to use non-negative measures as an indefinite function in a Reproducing Kernel Kreĭn Space (RKKS). This paper proposes a novel approach called Kreĭn Twin Support Vector Machines (KTSVM), which appropriately incorporates indefinite kernels within a TWSVM-based gradient optimization. To code pertinent input patterns within an imbalanced data discrimination, our KTSVM employs an implicit mapping to a RKKS. Also, our approach takes advantage of the TWSVM scheme’s benefits by creating two parallel hyperplanes. This promotes the KTSVM optimization in a gradient-descent framework. Results obtained on synthetic and real-world datasets demonstrate that our solution performs better in terms of imbalanced data classification than state-of-the-art techniques.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524000850",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Combinatorics",
      "Composite material",
      "Computer science",
      "Geometry",
      "Gradient descent",
      "Hyperplane",
      "Image (mathematics)",
      "Kernel (algebra)",
      "Machine learning",
      "Materials science",
      "Mathematics",
      "Matrix (chemical analysis)",
      "Pattern recognition (psychology)",
      "Similarity (geometry)",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Jimenez-Castaño",
        "given_name": "C."
      },
      {
        "surname": "Álvarez-Meza",
        "given_name": "A."
      },
      {
        "surname": "Cárdenas-Peña",
        "given_name": "D."
      },
      {
        "surname": "Orozco-Gutíerrez",
        "given_name": "A."
      },
      {
        "surname": "Guerrero-Erazo",
        "given_name": "J."
      }
    ]
  },
  {
    "title": "Enhancing mass spectrometry data analysis: A novel framework for calibration, outlier detection, and classification",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.03.026",
    "abstract": "Mass spectrometry (MS) is a powerful analytical technique in metabolomics, enabling the identification and quantification of metabolites. However, analyzing MS data poses challenges such as batch effects, outliers, and high-dimensional data. In this paper, we propose a comprehensive framework for MS data analysis. The framework integrates data calibration, outlier detection, and automatic classification modules. Data calibration is performed using a deep autoencoder to remove batch effects. Outlier detection combines multiple algorithms through ensemble learning to identify and remove outliers. Automatic classification utilizes a transformer model to handle high-dimensional data and capture global feature relationships. Experimental results on myocardial infarction (MI) and coronary heart disease (CHD) datasets demonstrate the effectiveness of the framework. It outperforms traditional classification models and achieves higher accuracy. The proposed framework provides a robust solution for MS data analysis, facilitating more accurate classification and enabling reliable biological insights in metabolomics research.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524001004",
    "keywords": [
      "Anomaly detection",
      "Artificial intelligence",
      "Autoencoder",
      "Calibration",
      "Computer science",
      "Data mining",
      "Deep learning",
      "Mathematics",
      "Outlier",
      "Pattern recognition (psychology)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Peng",
        "given_name": "Weili"
      },
      {
        "surname": "Zhou",
        "given_name": "Tao"
      },
      {
        "surname": "Chen",
        "given_name": "Yuanyuan"
      }
    ]
  },
  {
    "title": "Separate first, then segment: An integrity segmentation network for salient object detection",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110328",
    "abstract": "Current methods aggregate multi-level features or introduce auxiliary information to get more refined saliency maps. However, little attention is paid to how to obtain complete salient objects in cluttered background. To address this problem, we propose an integrity segmentation network (ISNet) with a novel detection paradigm that first separates the targets completely and then segment them finely. Specifically, the ISNet architecture consists of a target separation (TS) branch and an object segmentation (OS) branch, trained using a hierarchical difference-aware (HDA) loss. The TS branch equipped with a fractal structure is utilized to produce saliency features with expanded boundary (SF w/ EB), which can enlarge the difference of border details to separate the target from background completely. Compared with the edge and skeleton information, the SF w/ EB contains a more complete structure, which can supplement the defect of salient objects. The OS branch is leveraged to generate complementary features, which gradually integrates the SF w/ EB and aggregated features to segment complete saliency maps. Moreover, we propose the HDA loss to further improve the structural integrity of prediction, which hierarchically assigns weight to pixels according to their differences. Hard pixels will be given more attention to discriminate the similar parts between foreground and background. Comprehensive experimental results on five datasets show that the proposed ISNet outperforms the state-of-the-art methods both quantitatively and qualitatively. Concretely, compared with three typical models, the average gain percentage reaches 2.6% in terms of F β , S m and MAE on two large complex datasets. The improvements demonstrate that the proposed ISNet are beneficial for improving the integrity of prediction. Besides, the ISNet is efficient and runs at a real-time speed of 39.5 FPS when processing an image with size of 320 × 320. Furthermore, the proposed model has better generalization, which can also be applied to other vision tasks to handle complex scenes. Codes are available at https://github.com/lesonly/ISNet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324000797",
    "keywords": [
      "Aggregate (composite)",
      "Artificial intelligence",
      "Boundary (topology)",
      "Composite material",
      "Computer science",
      "Computer vision",
      "Enhanced Data Rates for GSM Evolution",
      "Materials science",
      "Mathematical analysis",
      "Mathematics",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Pixel",
      "Salient",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Zhu",
        "given_name": "Ge"
      },
      {
        "surname": "Li",
        "given_name": "Jinbao"
      },
      {
        "surname": "Guo",
        "given_name": "Yahong"
      }
    ]
  },
  {
    "title": "Clustering-inspired channel selection method for weakly supervised object localization",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.04.005",
    "abstract": "Weakly Supervised Object Localization (WSOL) aims to utilize the features learned by a classifier on the image-level labels to locate target objects. However, these existing channel selection methods for WSOL still cannot effectively select the important channels and remove the unimportant ones. To address this issue, we propose a Clustering-inspired Channel Selection method based on Class Activation Maps (CCS-CAM). Compared with the traditional methods, the advantage of CCS-CAM is that it is very simple yet effective for channel selection due to the K-means clustering based on Class Activation Maps. It can effectively ensure both object localization and classification accuracy. The effectiveness of the proposed CCS-CAM method has been demonstrated using multiple public datasets, with GT-Know Loc reaching 87.9% and 63.71% on the CUB200-2011 and ImageNet-1k respectively, which is superior to the other state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524001065",
    "keywords": [
      "Artificial intelligence",
      "Channel (broadcasting)",
      "Class (philosophy)",
      "Classifier (UML)",
      "Cluster analysis",
      "Computer network",
      "Computer science",
      "Data mining",
      "Machine learning",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Selection (genetic algorithm)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Xiaofeng"
      },
      {
        "surname": "Liu",
        "given_name": "Zhe"
      },
      {
        "surname": "Qiao",
        "given_name": "Xiangru"
      },
      {
        "surname": "Li",
        "given_name": "Zhiquan"
      },
      {
        "surname": "Wu",
        "given_name": "Sidong"
      },
      {
        "surname": "Zhang",
        "given_name": "Jiao"
      },
      {
        "surname": "Liu",
        "given_name": "Yonghuai"
      },
      {
        "surname": "Li",
        "given_name": "Zhan"
      },
      {
        "surname": "Guo",
        "given_name": "Hongbo"
      },
      {
        "surname": "Zhang",
        "given_name": "Huaizhong"
      }
    ]
  },
  {
    "title": "LATFormer: Locality-Aware Point-View Fusion Transformer for 3D shape recognition",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110413",
    "abstract": "Recently, 3D shape understanding has achieved significant progress due to the advances of deep learning models on various data formats like images, voxels, and point clouds. Among them, point clouds and multi-view images are two complementary modalities of 3D objects, and learning representations by fusing both of them has been proven to be fairly effective. While prior works typically focus on exploiting global features of the two modalities, herein we argue that more discriminative features can be derived by modeling “where to fuse”. To investigate this, we propose a novel Locality-Aware Point-View Fusion Transformer (LATFormer) for 3D shape retrieval and classification. The core component of LATFormer is a module named Locality-Aware Fusion (LAF) which integrates the local features of correlated regions across the two modalities based on the co-occurrence scores. We further propose to filter out scores with low values to obtain salient local co-occurring regions, which reduces redundancy for the fusion process. In our LATFormer, we utilize the LAF module to fuse the multi-scale features of the two modalities both bidirectionally and hierarchically to obtain more informative features. Comprehensive experiments on four popular 3D shape benchmarks covering 3D object retrieval and classification validate its effectiveness.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032400164X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Discriminative model",
      "Electrical engineering",
      "Engineering",
      "Fuse (electrical)",
      "Fusion",
      "Linguistics",
      "Locality",
      "Modalities",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Point cloud",
      "Salient",
      "Social science",
      "Sociology",
      "Voxel"
    ],
    "authors": [
      {
        "surname": "He",
        "given_name": "Xinwei"
      },
      {
        "surname": "Cheng",
        "given_name": "Silin"
      },
      {
        "surname": "Liang",
        "given_name": "Dingkang"
      },
      {
        "surname": "Bai",
        "given_name": "Song"
      },
      {
        "surname": "Wang",
        "given_name": "Xi"
      },
      {
        "surname": "Zhu",
        "given_name": "Yingying"
      }
    ]
  },
  {
    "title": "UGNet: Uncertainty aware geometry enhanced networks for stereo matching",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110410",
    "abstract": "Stereo matching is a fundamental research area in the field of computer vision. In recent years, iterative methods based on Gated Recurrent Units (GRUs) have showcased remarkable achievements in this domain. Despite their high accuracy, these methods suffer from notable limitations such as a reliance on a large number of iterations and a tendency to lose high-frequency details. To address these issues, we propose a novel uncertainty-aware framework that combines 3D convolution and GRU-based iterations, aiming to improve efficiency and accuracy. Specifically, we first introduce a probabilistic method to jointly train the disparity map and its corresponding uncertainty map using 3D convolutions. Next, leveraging the uncertainty map as a guide, we develop a novel uncertainty reweighting iterative module to assist in identifying errors in the coarse disparity and cost volume, thereby refining the disparity estimation process and significantly improving the iteration efficiency. Moreover, we introduce a high-resolution refinement module that utilizes Pixel Difference Convolution (PDC) to incorporate additional gradient information. This module can fine-tune the disparity estimation to enhance accuracy. Finally, our network is evaluated on multiple widely-used benchmark datasets. The results demonstrate its proficiency in predicting precise boundaries and effectively reduce iterations. Our model achieves comparable performance to other state-of-the-art methods, ranking 1st on KITTI 2015, and 2nd on KITTI 2012. These results validate its strong performance and generalizability.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324001614",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Computer science",
      "Computer vision",
      "Convolution (computer science)",
      "Generalizability theory",
      "Geodesy",
      "Geography",
      "Matching (statistics)",
      "Mathematics",
      "Operating system",
      "Probabilistic logic",
      "Process (computing)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Qi",
        "given_name": "Zhengkai"
      },
      {
        "surname": "Zhang",
        "given_name": "Junkang"
      },
      {
        "surname": "Fang",
        "given_name": "Faming"
      },
      {
        "surname": "Wang",
        "given_name": "Tingting"
      },
      {
        "surname": "Zhang",
        "given_name": "Guixu"
      }
    ]
  },
  {
    "title": "Discriminative Regularized Input Manifold for multilayer perceptron",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110421",
    "abstract": "Multilayer perceptron (MLP) fails to discriminate the ambiguous inputs belonging to the overlapping regions of multiple classes, resulting in misclassification. To classify the input samples accurately according to their classes, removing the ambiguity that occurred due to the sharing of common input space is important. In this article, a novel neural network framework, called Discriminative Regularized Input Manifold MLP (DRIM-MLP) is proposed to reduce this ambiguity and improve the classification accuracy. The proposed framework consists of two different feed forward networks that are trained simultaneously: (i) DRIM and (ii) MLP. The proposed DRIM learns the input distribution during its training and the learnt information is incorporated during the training of MLP to reduce the ambiguity. Simultaneously, the class information learnt by MLP is incorporated into the DRIM to learn discriminative information of the input distribution. The hidden layer output of DRIM estimates (i) the input of DRIM using the weights of hidden and output layers of DRIM and (ii) the class output of MLP using the weights of hidden and output layers of MLP. Here, the estimated class output, learnt by the DRIM that contains information of the input distribution, is used as a regularizer for the MLP to minimize the difference between the estimated class output of DRIM and the estimated class output of MLP itself. The hidden layer of MLP also estimates: (i) the class output of MLP using the weights between hidden and output layers of MLP and (ii) the input of DRIM using the weights between hidden and output layers of DRIM. Here, the estimated input learnt by MLP that contains class information is used as a regularizer for DRIM to minimize the difference between the estimated input of DRIM itself and the estimated input of MLP. These two regularizers are respectively called the Input manifold Regularizer (ImR) and the Discriminative Regularizer (DiscR). The experimental results based on ten standard data sets strongly support the effectiveness of the proposed DRIM-MLP compared to conventional MLP, auto encoder based MLP (AE-MLP), denoising auto encoder based MLP (DAE-MLP), AE-MLP with KLD, DAE-MLP with KLD along with two recent works of state-of-the-art.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324001729",
    "keywords": [
      "Ambiguity",
      "Artificial intelligence",
      "Artificial neural network",
      "Class (philosophy)",
      "Computer science",
      "Discriminative model",
      "Engineering",
      "Manifold (fluid mechanics)",
      "Mathematics",
      "Mechanical engineering",
      "Multilayer perceptron",
      "Pattern recognition (psychology)",
      "Perceptron",
      "Programming language"
    ],
    "authors": [
      {
        "surname": "Mondal",
        "given_name": "Rahul"
      },
      {
        "surname": "Pal",
        "given_name": "Tandra"
      },
      {
        "surname": "Dey",
        "given_name": "Prasenjit"
      }
    ]
  },
  {
    "title": "Adaptive weighted dictionary representation using anchor graph for subspace clustering",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110350",
    "abstract": "Samples are commonly represented as sparse vectors in many dictionary representation algorithms. However, this method may result in loss of discriminatory information. Moreover, a redundant dictionary can increase the computational complexity of the algorithm. To tackle these challenges, we propose a novel method named Adaptive Weighted Dictionary Representation using Anchor Graph for Subspace Clustering (AWDR). First, AWDR constructs an anchor graph that encodes the classification information and establishes accurate connectivity components between anchors and clusters, thereby fully utilizing the discriminative information of the original samples. In addition, AWDR learns a complete-dictionary in the subspace to eliminate the noise and out-of-sample effects of the original sample space, while also improving computational efficiency. Finally, AWDR computes the coefficients for the samples in an adaptively weighted manner to find discriminative representation of the samples from the dictionary. Extensive experiments on real-world datasets demonstrate that our method is effective and efficient compared to the state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324001018",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Cluster analysis",
      "Computational complexity theory",
      "Computer science",
      "Discriminative model",
      "Graph",
      "K-SVD",
      "Law",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Sparse approximation",
      "Subspace topology",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Feng",
        "given_name": "Wenyi"
      },
      {
        "surname": "Wang",
        "given_name": "Zhe"
      },
      {
        "surname": "Xiao",
        "given_name": "Ting"
      },
      {
        "surname": "Yang",
        "given_name": "Mengping"
      }
    ]
  },
  {
    "title": "A distortion model guided adversarial surrogate for recaptured document detection",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110433",
    "abstract": "Due to the ever-growing need for e-business, document presentation attack detection (DPAD) is an important forensic task. With deep learning, the performances of DPAD methods have been improved significantly. However, the cross-domain performance under different types of document images is not yet satisfactory. In this work, we propose to study the document recapturing distortion model (DM), which is employed in guiding the training of an end-to-end surrogate distortion model. This surrogate model is incorporated into the DPAD scheme in an adversarial training fashion, yielding the proposed Adversarial Surrogate-based DPAD (AS-DPAD) scheme. The surrogate model actively generates adversarial samples with recapturing distortions that confuse the DPAD model. Meanwhile, the DPAD backbone learns a latent space that considers the distances between the generated and real recaptured samples to achieve a better generalization performance. A challenging cross-domain protocol is conducted in our experiment. The results confirm that our DM improves the efficacy of the trained surrogate distortion model and also validate that the proposed adversarial training strategy leads to a significant performance gain in the evaluation of document images with different contents.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324001845",
    "keywords": [
      "Adversarial system",
      "Algorithm",
      "Amplifier",
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Computer science",
      "Distortion (music)",
      "Econometrics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Statistics",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Changsheng"
      },
      {
        "surname": "Li",
        "given_name": "Xijin"
      },
      {
        "surname": "Chen",
        "given_name": "Baoying"
      },
      {
        "surname": "Li",
        "given_name": "Haodong"
      }
    ]
  },
  {
    "title": "Learning spatial-spectral dual adaptive graph embedding for multispectral and hyperspectral image fusion",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110365",
    "abstract": "Fusion of high spatial resolution multispectral (HR MS) and low spatial resolution hyperspectral (LR HS) images has become a significant way to produce high spatial resolution hyperspectral (HR HS) images. Though many methods have exploited the spatial nonlocal similarity (SNS) and spectral band correlation (SBC) in the HR HS image, it is difficult to model the priors explicitly because the HR HS image is unavailable in real scenes. As the low-dimensional degradation versions, HR MS and LR HS images inherit the SNS and SBC in the HR HS image, respectively. But these methods seldom consider the inheritance of SNS and SBC between the two source images and the HR HS image. In this paper, we propose a spatial–spectral dual adaptive graph embedding model (SDAGE) to exploit the SNS and SBC in HR MS and LR HS images for the regularization of their fusion. Specifically, spatial and spectral graphs are constructed adaptively to describe the SNS in the HR MS image and the SBC in the LR HS image. Then, the two graphs are embedded into the features for the reconstruction of the HR HS image. In this way, it is explicit to ensure the consistency of SNS and SBC between the source images and the HR HS image. Experiments on three benchmark datasets demonstrate the effectiveness of our SDAGE method. The code can be downloaded from https://github.com/RSMagneto/SDAGE.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032400116X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Embedding",
      "Hyperspectral imaging",
      "Image (mathematics)",
      "Image resolution",
      "Multispectral image",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Xuquan"
      },
      {
        "surname": "Zhang",
        "given_name": "Feng"
      },
      {
        "surname": "Zhang",
        "given_name": "Kai"
      },
      {
        "surname": "Wang",
        "given_name": "Weijie"
      },
      {
        "surname": "Dun",
        "given_name": "Xiong"
      },
      {
        "surname": "Sun",
        "given_name": "Jiande"
      }
    ]
  },
  {
    "title": "Global feature-based multimodal semantic segmentation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110340",
    "abstract": "Incorporating complementary modality into RGB branch can significantly improve the effectiveness of semantic segmentation. However, fusion between the two modalities faces huge challenge due to the difference of their optical dimensions. Existed fusion methods can't keep a balance between performance and efficiency in aggregating detailed features. To address this problem, we propose a global feature-based network (GFBN) for semantic segmentation that establishes mapping function and extraction relationship among the multi-modalities. The GFBN contains three important modules, which are used for feature correction, fusion and edge enhancement. Firstly, the cross-attention rectification module (CARM) adaptively extracts mapping relationships and rectifies the RGB and complementary features. Secondly, the cross-field fusion module (CFM) integrates long-range rectified features of two branches to obtain an optimal fusion feature. Finally, the boundary guidance module (BGM) sharpens the boundary information of the fused features to effectively improve the segmentation accuracy of object boundaries. We make the experiments of GFBN on the challenging MCubeS and ZJU-RGB-Ps datasets. The results show that GFBN outperforms state-of-the-art methods by at least 0.64 % and 0.7 % on mean intersection over union (mIoU), respectively. It demonstrates the performance and efficiency of our proposed method. The code corresponding to our method can be found at the following link: https://github.com/Sci-Epiphany/GFBNext.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324000918",
    "keywords": [
      "Artificial intelligence",
      "Boundary (topology)",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Feature extraction",
      "Fusion",
      "Linguistics",
      "Matching (statistics)",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "RGB color model",
      "Segmentation",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Gao",
        "given_name": "Suining"
      },
      {
        "surname": "Yang",
        "given_name": "Xiubin"
      },
      {
        "surname": "Jiang",
        "given_name": "Li"
      },
      {
        "surname": "Fu",
        "given_name": "Zongqiang"
      },
      {
        "surname": "Du",
        "given_name": "Jiamin"
      }
    ]
  },
  {
    "title": "Manifold information through neighbor embedding projection for image retrieval",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.04.022",
    "abstract": "Although studied for decades, constructing effective image retrieval remains an open problem in a wide range of relevant applications. Impressive advances have been made to represent image content, mainly supported by the development of Convolution Neural Networks (CNNs) and Transformer-based models. On the other hand, effectively computing the similarity between such representations is still challenging, especially in collections in which images are structured in manifolds. This paper introduces a novel solution to this problem based on dimensionality reduction techniques, often used for data visualization. The key idea consists in exploiting the spatial relationships defined by neighbor embedding data visualization methods, such as t-SNE and UMAP, to compute a more effective distance/similarity measure between images. Experiments were conducted on several widely-used datasets. Obtained results indicate that the proposed approach leads to significant gains in comparison to the original feature representations. Experiments also indicate competitive results in comparison with state-of-the-art image retrieval approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524001314",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Embedding",
      "Engineering",
      "Image (mathematics)",
      "Image retrieval",
      "Manifold (fluid mechanics)",
      "Mathematics",
      "Mechanical engineering",
      "Pattern recognition (psychology)",
      "Projection (relational algebra)"
    ],
    "authors": [
      {
        "surname": "Leticio",
        "given_name": "Gustavo Rosseto"
      },
      {
        "surname": "Kawai",
        "given_name": "Vinicius Sato"
      },
      {
        "surname": "Valem",
        "given_name": "Lucas Pascotti"
      },
      {
        "surname": "Pedronette",
        "given_name": "Daniel Carlos Guimarães"
      },
      {
        "surname": "da S. Torres",
        "given_name": "Ricardo"
      }
    ]
  },
  {
    "title": "Structural and positional ensembled encoding for Graph Transformer",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.05.006",
    "abstract": "In the Transformer architecture, positional encoding is a vital component because it provides the model with information about the structure and position of data. In Graph Transformer, there have been attempts to introduce different positional encodings and inject additional structural information. Therefore, in terms of integrating positional and structural information, we propose a Structural and Positional Ensembled Graph Transformer (SPEGT). We developed SPEGT by noting the different properties of structural and positional encodings of graphs and the similarity of their computational processes. We have set a unified component that integrates the functionalities: (i) Random Walk Positional Encoding, (ii) Shortest Path Distance between each node, and (iii) Hierarchical Cluster Encoding. We find a problem with a well-known positional encoding and experimentally verify that combining it with other encodings can solve their problem. In addition, SPEGT outperforms previous models on a variety of graph datasets. We also show that SPEGT using unified positional encoding, performs well on structurally indistinguishable graph data through error case analysis.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524001508",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Encoding (memory)",
      "Graph",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Theoretical computer science",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Yeom",
        "given_name": "Jeyoon"
      },
      {
        "surname": "Kim",
        "given_name": "Taero"
      },
      {
        "surname": "Chang",
        "given_name": "Rakwoo"
      },
      {
        "surname": "Song",
        "given_name": "Kyungwoo"
      }
    ]
  },
  {
    "title": "Uncertainty-driven active developmental learning",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110384",
    "abstract": "Existing machine learning models can well handle common classes but struggle to detect unfamiliar or unknown classes due to environmental variations. To address this challenge, we propose a new task called active developmental learning (ADL), which empowers models to actively determine what to learn in the open world, thereby progressively enhancing the capability of detecting unfamiliar and unknown classes. Considering the uncertain essence of the task, we design an uncertainty-driven method for ADL (UADL) that measures and utilizes uncertainty to evaluate unfamiliar known classes and unknown classes separately, which consists of two stages: (1) unfamiliar detection of known classes and (2) unknown detection of novel classes. In the first stage, UADL identifies unfamiliar samples of known classes via known-class uncertainty calculated by GMMs on detectors’ heads. In the second stage, UADL identifies samples containing unknown classes via unknown-class uncertainty computed by class-specific GMMs in feature space. In both stages, uncertainty is used to select a minimal number of unlabeled samples for manual labeling, facilitating the model’s active self-development. Experiments on multiple object detection benchmark datasets demonstrate the feasibility and significant performance of UADL and show its effectiveness against the ADL task compared to other state-of-the-art approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324001353",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Class (philosophy)",
      "Computer science",
      "Economics",
      "Feature (linguistics)",
      "Geodesy",
      "Geography",
      "Linguistics",
      "Machine learning",
      "Management",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Hu",
        "given_name": "Qinghua"
      },
      {
        "surname": "Ji",
        "given_name": "Luona"
      },
      {
        "surname": "Wang",
        "given_name": "Yu"
      },
      {
        "surname": "Zhao",
        "given_name": "Shuai"
      },
      {
        "surname": "Lin",
        "given_name": "Zhibin"
      }
    ]
  },
  {
    "title": "UnitModule: A lightweight joint image enhancement module for underwater object detection",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110435",
    "abstract": "Underwater object detection faces the problem of underwater image degradation, which affects the performance of the detector. Underwater object detection methods based on noise reduction and image enhancement usually do not provide images preferred by the detector or require additional datasets. In this paper, we propose a plug-and-play Underwater joint image enhancement Module (UnitModule) that provides the input image preferred by the detector. We design an unsupervised learning loss for the joint training of UnitModule with the detector without additional datasets to improve the interaction between UnitModule and the detector. Furthermore, a color cast predictor with the assisting color cast loss and a data augmentation called Underwater Color Random Transfer (UCRT) are designed to improve the performance of UnitModule on underwater images with different color casts. Extensive experiments are conducted on DUO for different object detection models, where UnitModule achieves the highest performance improvement of 2.6 AP for YOLOv5-S and gains the improvement of 3.3 AP on the brand-new test set ( URPC t e s t ). And UnitModule significantly improves the performance of all object detection models we test, especially for models with a small number of parameters. In addition, UnitModule with a small number of parameters of 31K has little effect on the inference speed of the original object detection model. Our quantitative and visual analysis also demonstrates the effectiveness of UnitModule in enhancing the input image and improving the perception ability of the detector for object features. The code is available at https://github.com/LEFTeyex/UnitModule.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324001869",
    "keywords": [
      "Architectural engineering",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Detector",
      "Engineering",
      "Geology",
      "Joint (building)",
      "Object detection",
      "Oceanography",
      "Pattern recognition (psychology)",
      "Telecommunications",
      "Underwater"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Zhuoyan"
      },
      {
        "surname": "Wang",
        "given_name": "Bo"
      },
      {
        "surname": "Li",
        "given_name": "Ye"
      },
      {
        "surname": "He",
        "given_name": "Jiaxian"
      },
      {
        "surname": "Li",
        "given_name": "Yunfeng"
      }
    ]
  },
  {
    "title": "Towards accurate unsupervised video captioning with implicit visual feature injection and explicit",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.05.012",
    "abstract": "In the realm of the video captioning field, acquiring large amounts of high-quality aligned video–text pairs remains laborious, impeding its practical applications. Therefore, we explore the modelling techniques for unsupervised video captioning. Using text inputs similar to the video representation to generate captions has been a successful unsupervised video captioning generation strategy in the past. However, this setting relies solely on the textual data for training, neglecting vital visual cues related to the spatio-temporal appearance within the video. The absence of visual information increases the risk of generating erroneous video captions. In view of this, we propose a novel unsupervised video captioning method that introduces visual information related to text features keywords to implicitly enhance training for text generation tasks. Simultaneously, our method incorporates sentence to explicitly augment the training process. our method injects additional implicit visual features and explicit keywords into the model, Which can inject the generated captions with more accurate semantics. the experimental analysis demonstrates the merit of the proposed formulation, achieving superior performance against the state-of-the-art unsupervised studies.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524001569",
    "keywords": [
      "Artificial intelligence",
      "Closed captioning",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Image (mathematics)",
      "Linguistics",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Yunjie"
      },
      {
        "surname": "Xu",
        "given_name": "Tianyang"
      },
      {
        "surname": "Song",
        "given_name": "Xiaoning"
      },
      {
        "surname": "Zhu",
        "given_name": "Xue-Feng"
      },
      {
        "surname": "Feng",
        "given_name": "Zhenghua"
      },
      {
        "surname": "Wu",
        "given_name": "Xiao-Jun"
      }
    ]
  },
  {
    "title": "Dual teachers for self-knowledge distillation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110422",
    "abstract": "We introduce an efficient self-knowledge distillation framework, Dual Teachers for Self-Knowledge Distillation (DTSKD), where the student receives self-supervisions by dual teachers from two substantially different fields, i.e., the past learning history and the current network structure. Specifically, DTSKD trains a considerably lightweight multi-branch network and acquires predictions from each, which are simultaneously supervised by a historical teacher from the previous epoch and a structural teacher under the current iteration. To our best knowledge, it is the first attempt to jointly conduct historical and structural self-knowledge distillation in a unified framework where they demonstrate complementary and mutual benefits. The Mixed Fusion Module (MFM) is further developed to bridge the semantic gap between deep stages and shallow branches by iteratively fusing multi-stage features based on the top-down topology. Extensive experiments prove the effectiveness of our proposed method, showing superior performance over related state-of-the-art self-distillation works on three datasets: CIFAR-100, ImageNet-2012, and PASCAL VOC.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324001730",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Cartography",
      "Chemistry",
      "Computer science",
      "Distillation",
      "Dual (grammatical number)",
      "Geography",
      "Literature",
      "Machine learning",
      "Organic chemistry",
      "Pascal (unit)",
      "Programming language",
      "Train"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Zheng"
      },
      {
        "surname": "Li",
        "given_name": "Xiang"
      },
      {
        "surname": "Yang",
        "given_name": "Lingfeng"
      },
      {
        "surname": "Song",
        "given_name": "Renjie"
      },
      {
        "surname": "Yang",
        "given_name": "Jian"
      },
      {
        "surname": "Pan",
        "given_name": "Zhigeng"
      }
    ]
  },
  {
    "title": "Overall positive prototype for few-shot open-set recognition",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110400",
    "abstract": "Few-shot open-set recognition (FSOR) is the task of recognizing samples in known classes with a limited number of annotated instances while also detecting samples that do not belong to any known class. This is a challenging problem because the models must learn to generalize from a small number of labeled samples and distinguish them from an unlimited number of potential negative examples. In this paper, we propose a novel approach called overall positive prototype to effectively improve performance. Conceptually, negative samples would distribute throughout the feature space and are hard to be described. From the opposite viewpoint, we propose to construct an overall positive prototype that acts as a cohesive representation for positive samples that distribute in a relatively smaller neighborhood. By measuring the distance between a query sample and the overall positive prototype, we can effectively classify it as either positive or negative. We show that this simple yet innovative approach provides the state-of-the-art FSOR performance in terms of accuracy and AUROC.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324001511",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Chromatography",
      "Class (philosophy)",
      "Computer science",
      "Construct (python library)",
      "Discrete mathematics",
      "Economics",
      "Feature (linguistics)",
      "Feature vector",
      "Law",
      "Linguistics",
      "Machine learning",
      "Management",
      "Mathematics",
      "Open set",
      "Operating system",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Programming language",
      "Representation (politics)",
      "Sample (material)",
      "Set (abstract data type)",
      "Shot (pellet)",
      "Space (punctuation)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Sun",
        "given_name": "Liang-Yu"
      },
      {
        "surname": "Chu",
        "given_name": "Wei-Ta"
      }
    ]
  },
  {
    "title": "Multi-label feature selection via latent representation learning and dynamic graph constraints",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110411",
    "abstract": "As an effective method to deal with the curse of dimensionality, multi-label feature selection aims to select the most representative subset of features by eliminating unfavorable features. Although great progress has been made in this field, how to mine adequate supervisory information from multi-label data remains a key challenge. Compared to the latent information of instances, the latent information of instance relevance contains both the basic information of instances and the latent relevance between instances. Base on this knowledge, we propose a novel multi-label feature selection method named LRDG that explores latent representation learning and dynamic graph constraints. Specifically, we introduce the latent representation of instance relevance as supervisory information for pseudo-label learning, and minimize information loss during pseudo-label learning by means of the label manifold, the non-negative constraints, and the minimization of the Frobenius norm between pseudo-labels and ground-truth labels. In addition, considering the shortcomings brought by traditional graph regularization, we propose to use the dynamic graph constructed from low-dimensional pseudo-labels to constrain feature weights. Extensive experiments on various multi-label datasets demonstrate the effectiveness of the proposed method. The code is available at https://github.com/yunbao520/LRDG.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324001626",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Curse of dimensionality",
      "Dimensionality reduction",
      "Feature learning",
      "Feature selection",
      "Graph",
      "Law",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Regularization (linguistics)",
      "Relevance (law)",
      "Representation (politics)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Yao"
      },
      {
        "surname": "Huo",
        "given_name": "Wei"
      },
      {
        "surname": "Tang",
        "given_name": "Jun"
      }
    ]
  },
  {
    "title": "Domain-incremental learning without forgetting based on random vector functional link networks",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110430",
    "abstract": "Incremental learning is a paradigm that extends knowledge by learning from new data, often used to add new classes to an existing model or to learn a new domain. It imposes strict limitations on the model’s access to data from previous tasks, making it similar to the human learning process. The main challenge of incremental learning is catastrophic forgetting, where previous knowledge is severely forgotten while learning new tasks. In this work, we propose a novel approach for domain-incremental learning. Inspired by the Normal Equation, we accumulate the Gram Matrix from each task’s hidden layer output to update a simplified RVFL model. This algorithm achieves performance comparable to joint training while strictly adhering to privacy restrictions. With issues such as forgetting, storage requirements and privacy protection be addressed, this algorithm has the potential to play a crucial role in the field of edge computing and other related fields.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032400181X",
    "keywords": [
      "Artificial intelligence",
      "Cognitive psychology",
      "Computer network",
      "Computer science",
      "Domain (mathematical analysis)",
      "Forgetting",
      "Link (geometry)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Psychology"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Chong"
      },
      {
        "surname": "Wang",
        "given_name": "Yi"
      },
      {
        "surname": "Li",
        "given_name": "Dong"
      },
      {
        "surname": "Wang",
        "given_name": "Xizhao"
      }
    ]
  },
  {
    "title": "AG-Meta: Adaptive graph meta-learning via representation consistency over local subgraphs",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110387",
    "abstract": "Graph meta-learning has recently received significantly increased attention by virtue of its potential to extract common and transferable knowledge from learning different tasks on a graph. Existing methods for graph meta-learning usually leverage local subgraphs to transfer subgraph-specific information. However, they inherently face the challenge of imbalanced subgraphs due to inconsistent node density and different label distributions over local subgraphs. This paper proposes an adaptive graph meta-learning framework (AG-Meta) for learning the consistent and transferable representation of a graph in a way that can adapt to imbalanced subgraphs. Specifically, AG-Meta first learns the structural representation of subgraphs with various degrees using an Adaptive Graph Cascade Diffusion Network (AGCDN). AG-Meta then employs a prototype-consistency classifier to produce more accurate transferable inductive representations (also called prototypes) under few-shot settings with different label distributions of a subgraph. In the context of optimizing a model-agnostic meta-learner, a novel metric loss is finally introduced to achieve structural representation and prototype consistency. Extensive experiments are conducted to compare AG-Meta against baselines on five real-world networks, which validates that AG-Meta outperforms the state-of-the-art approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324001389",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Feature learning",
      "Graph",
      "Leverage (statistics)",
      "Machine learning",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Yi"
      },
      {
        "surname": "Huang",
        "given_name": "Changqin"
      },
      {
        "surname": "Li",
        "given_name": "Ming"
      },
      {
        "surname": "Huang",
        "given_name": "Qionghao"
      },
      {
        "surname": "Wu",
        "given_name": "Xuemei"
      },
      {
        "surname": "Wu",
        "given_name": "Jia"
      }
    ]
  },
  {
    "title": "Cross-lingual few-shot sign language recognition",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110374",
    "abstract": "There are over 150 sign languages worldwide, each with numerous local variants and thousands of signs. However, collecting annotated data for each sign language to train a model is a laborious and expert-dependent task. To address this issue, this paper introduces the problem of few-shot sign language recognition (FSSLR) in a cross-lingual setting. The central motivation is to be able to recognize a novel sign, even if it belongs to a sign language unseen during training, based on a small set of examples. To tackle this problem, we propose a novel embedding-based framework that first extracts a spatio-temporal visual representation based on video and hand features, as well as hand landmark estimates. To establish a comprehensive test bed, we propose three meta-learning FSSLR benchmarks that span multiple languages, and extensively evaluate the proposed framework. The experimental results demonstrate the effectiveness and superiority of the proposed approach for few-shot sign language recognition in both monolingual and cross-lingual settings.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324001250",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Economics",
      "Embedding",
      "Law",
      "Linguistics",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Natural language processing",
      "Philosophy",
      "Political science",
      "Politics",
      "Programming language",
      "Representation (politics)",
      "Set (abstract data type)",
      "Sign (mathematics)",
      "Sign language",
      "Speech recognition",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Bilge",
        "given_name": "Yunus Can"
      },
      {
        "surname": "Ikizler-Cinbis",
        "given_name": "Nazli"
      },
      {
        "surname": "Cinbis",
        "given_name": "Ramazan Gokberk"
      }
    ]
  },
  {
    "title": "Efficient label-free pruning and retraining for Text-VQA Transformers",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.04.024",
    "abstract": "Recent advancements in Scene Text Visual Question Answering (Text-VQA) employ autoregressive Transformers, showing improved performance with larger models and pre-training datasets. Although various pruning frameworks exist to simplify Transformers, many are integrated into the time-consuming training process. Researchers have recently explored post-training pruning techniques, which separate pruning from training and reduce time consumption. Some methods use gradient-based importance scores that rely on labeled data, while others offer retraining-free algorithms that quickly enhance pruned model accuracy. This paper proposes a novel gradient-based importance score that only necessitates raw, unlabeled data for post-training structured autoregressive Transformer pruning. Additionally, we introduce a Retraining Strategy (ReSt) for efficient performance restoration of pruned models of arbitrary sizes. We evaluate our approach on TextVQA and ST-VQA datasets using TAP, TAP†† and SaL‡-Base where all utilize autoregressive Transformers. On TAP and TAP†† , our pruning approach achieves up to 60% reduction in size with less than a 2.4% accuracy drop and the proposed ReSt retraining approach takes only 3 to 34 min, comparable to existing retraining-free techniques. On SaL‡-Base , the proposed method achieves up to 50% parameter reduction with less than 2.9% accuracy drop requiring only 1.19 h of retraining using the proposed ReSt approach. The code is publicly accessible at https://github.com/soonchangAI/LFPR.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524001338",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Business",
      "Computer science",
      "Electrical engineering",
      "Engineering",
      "International trade",
      "Machine learning",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Pruning",
      "Retraining",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Poh",
        "given_name": "Soon Chang"
      },
      {
        "surname": "Chan",
        "given_name": "Chee Seng"
      },
      {
        "surname": "Lim",
        "given_name": "Chee Kau"
      }
    ]
  },
  {
    "title": "Corrigendum to “FGPNet: A weakly supervised fine-grained 3D point clouds classification network” [Pattern Recognition 139 (2023) 109509]",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110379",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324001304",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Geometry",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Point (geometry)",
      "Point cloud"
    ],
    "authors": [
      {
        "surname": "Shao",
        "given_name": "Huihui"
      },
      {
        "surname": "Bai",
        "given_name": "Jing"
      },
      {
        "surname": "Wu",
        "given_name": "Rusong"
      },
      {
        "surname": "Jiang",
        "given_name": "Jinzhe"
      },
      {
        "surname": "Liang",
        "given_name": "Hongbo"
      }
    ]
  },
  {
    "title": "Dual-path dehazing network with spatial-frequency feature fusion",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110397",
    "abstract": "With rapid improvement of deep learning, significant progress has been made in image dehazing, leading to favorable outcomes in many methods. However, a common challenge arises as most of these methods struggle to restore intricate details with vibrant colors in complex haze. In response to this challenge, we present a novel dual-path dehazing network with spatial-frequency feature fusion (DDN-SFF) to remove heterogeneous haze. The proposed dual-path network consists of a spatial-domain vanilla path and a frequency-domain frequency-guided path, effectively harnessing spatial-frequency knowledge. To maximize the versatility of the learned features, we introduce a relaxation dense feature fusion (RDFF) module in the vanilla path. This module can skillfully re-exploit features from non-adjacent levels and concurrently generate new features. In the frequency-guided path, we integrate the discrete wavelet transform (DWT) and introduce a frequency attention (FA) mechanism for the flexible handling of specific channels. More precisely, we deploy a channel attention (CA) and a dense feature fusion (DFF) module for low-frequency channels, whereas a pixel attention (PA) and a residual dense block (RDB) module are implemented for high-frequency channels. In summary, the deep dual-path network fuses sub-bands with specific spatial-frequency features, effectively eliminating the haze and restoring intricate details along with rich textures. Extensive experimental results demonstrate the superior performance of the proposed DDN-SFF over state-of-the-art dehazing algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324001481",
    "keywords": [
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Frequency domain",
      "Geometry",
      "Linguistics",
      "Mathematics",
      "Optics",
      "Path (computing)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Pixel",
      "Spatial frequency"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Li"
      },
      {
        "surname": "Dong",
        "given_name": "Hang"
      },
      {
        "surname": "Li",
        "given_name": "Ruyu"
      },
      {
        "surname": "Zhu",
        "given_name": "Chao"
      },
      {
        "surname": "Tao",
        "given_name": "Huibin"
      },
      {
        "surname": "Guo",
        "given_name": "Yu"
      },
      {
        "surname": "Wang",
        "given_name": "Fei"
      }
    ]
  },
  {
    "title": "HandFormer: Hand pose reconstructing from a single RGB image",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.05.019",
    "abstract": "We propose a multi-task progressive Transformer framework to reconstruct hand poses from a single RGB image to address challenges such as hand occlusion hand distraction, and hand shape bias. Our proposed framework comprises three key components: the feature extraction branch, palm segmentation branch, and parameter prediction branch. The feature extraction branch initially employs the progressive Transformer to extract multi-scale features from the input image. Subsequently, these multi-scale features are fed into a multi-layer perceptron layer (MLP) for acquiring palm alignment features. We employ an efficient fusion module to enhance the parameter prediction further features to integrate the palm alignment features with the backbone features. A dense hand model is generated using a pre-computed articulated mesh deformed hand model. We evaluate the performance of our proposed method on STEREO, FreiHAND, and HO3D datasets separately. The experimental results demonstrate that our approach achieves 3D mean error metrics of 10.92 mm, 12.33 mm and 9.6 mm for the respective datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524001612",
    "keywords": [
      "Artificial intelligence",
      "Computer graphics (images)",
      "Computer science",
      "Computer vision",
      "Image (mathematics)",
      "RGB color model"
    ],
    "authors": [
      {
        "surname": "Jiao",
        "given_name": "Zixun"
      },
      {
        "surname": "Wang",
        "given_name": "Xihan"
      },
      {
        "surname": "Li",
        "given_name": "Jingcao"
      },
      {
        "surname": "Gao",
        "given_name": "Rongxin"
      },
      {
        "surname": "He",
        "given_name": "Miao"
      },
      {
        "surname": "Liang",
        "given_name": "Jiao"
      },
      {
        "surname": "Xia",
        "given_name": "Zhaoqiang"
      },
      {
        "surname": "Gao",
        "given_name": "Quanli"
      }
    ]
  },
  {
    "title": "Multi-view clustering via pseudo-label guide learning and latent graph structure recovery",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110420",
    "abstract": "Multi-view clustering (MvC) accomplishes sample classification tasks by exploring information from different views. Currently, researchers have paid greater attention to graph-based MvC methods. However, most existing methods only consider the original graph structure and pay relatively little attention to the graph structure in the latent space. In addition, most methods need to pay more attention to the consistency of information on different labels. Otherwise, this may lead to the loss of label information. This paper presents a new multi-view clustering framework to address the above issues. The proposed method considers both the information in the latent space and the original data space, which firstly obtains the pseudo-label by latent representation learning and then lets the pseudo-label guide the learning of the complementary information between the raw data views. To ensure the integrity of the data structure, a latent graph structure recovery strategy is designed in the latent space. Finally, an enhanced label fusion strategy is designed to fusion the different types of labels, yielding an information-rich label matrix for clustering. Experimental results demonstrate the proposed method’s effectiveness compared to other advanced approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324001717",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Consistency (knowledge bases)",
      "Data mining",
      "Graph",
      "Latent class model",
      "Law",
      "Machine learning",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Cai",
        "given_name": "Ronggang"
      },
      {
        "surname": "Chen",
        "given_name": "Hongmei"
      },
      {
        "surname": "Mi",
        "given_name": "Yong"
      },
      {
        "surname": "Luo",
        "given_name": "Chuan"
      },
      {
        "surname": "Horng",
        "given_name": "Shi-Jinn"
      },
      {
        "surname": "Li",
        "given_name": "Tianrui"
      }
    ]
  },
  {
    "title": "Graph Representation and Prototype Learning for webly supervised fine-grained image recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.05.002",
    "abstract": "Webly supervised fine-grained image recognition (FGIR) learns to distinguish sub-ordinate categories based on webly-retrieved data, which can dramatically alleviate the dependency on manually annotated labels. This is quite a challenging task due to the heavy noise labels and the inherent dilemma of small inter-class variance and large intra-class variance. Current webly supervised algorithms learn holistic category prototypes to help correct noisy labels but ignore local features that can distinguish different sub-ordinate categories. In this work, we propose a Graph Representation and Prototype Learning (GRPL) framework to automatically mine discriminative local regions and their interactions with holistic image to learn instance graph representation both category graph prototype to help correct noisy labels and retrieve out-of-distribution (OOD) samples. Specifically, an attention-focused module is designed to extract the discriminative regions and then build a structured graph to correlate them with the holistic image for each instance and an identical graph to model holistic-local correlations for each category. Next, we apply two stacked graph convolution networks to explore holistic-local interaction within each graph and across two graphs to learn graph representation for each instance and graph prototype for each category. Finally, the similarities between the instance-level and prototype-level graph representation are learned to help correct noisy labels and exclude OOD samples. Extensive experiments conducted on several datasets show the proposed approach achieves superior performance compared with current leading algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524001405",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Graph",
      "Image (mathematics)",
      "Law",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Lin",
        "given_name": "Jiantao"
      },
      {
        "surname": "Chen",
        "given_name": "Tianshui"
      },
      {
        "surname": "Chen",
        "given_name": "Yingcong"
      },
      {
        "surname": "Yang",
        "given_name": "Zhijing"
      },
      {
        "surname": "Gao",
        "given_name": "YueFang"
      }
    ]
  },
  {
    "title": "Spatial–temporal hypergraph based on dual-stage attention network for multi-view data lightweight action recognition",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110427",
    "abstract": "For the problems of irrelevant frames and high model complexity in action recognition, we propose a Spatial–Temporal Hypergraph based on Dual-Stage Attention Network (STHG-DAN) for multi-view data lightweight action recognition. It includes two stages: Temporal Attention Mechanism based on Trainable Threshold (TAM-TT) and Hypergraph Convolution based on Dynamic Spatial–Temporal Attention Mechanism (HG-DSTAM). In the first stage, TAM-TT uses a learning threshold to extract keyframes from multi-view videos, with the multi-view data serving as a guarantee for providing more comprehensive information subsequently; In the second stage, HG-DSTAM divides the human joints into three parts: trunk, hand and leg to build spatial–temporal hypergraphs, extracts high-order features from spatial–temporal hypergraphs constructed of multi-view human body joints, inputs them into the dynamic spatial–temporal attention mechanism, and learns the intra frame correlation of multi-view data between the joint features of body parts, which can obtain the significant areas of action; We use multi-scale convolution operation and depth separable network, which can realize efficient action recognition with a few trainable parameters. We experiment on the NTU-RGB+D, NTU-RGB+D 120 and the imitating traffic police gesture dataset. The performance and accuracy of the model are better than the existing algorithms, effectively improving the machine and human body language interaction cognitive ability.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032400178X",
    "keywords": [
      "Action (physics)",
      "Art",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Convolution (computer science)",
      "Data mining",
      "Discrete mathematics",
      "Dual (grammatical number)",
      "Frame (networking)",
      "Hypergraph",
      "Literature",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "RGB color model",
      "Spatial analysis",
      "Statistics",
      "Telecommunications",
      "Temporal database"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Zhixuan"
      },
      {
        "surname": "Ma",
        "given_name": "Nan"
      },
      {
        "surname": "Wang",
        "given_name": "Cheng"
      },
      {
        "surname": "Xu",
        "given_name": "Cheng"
      },
      {
        "surname": "Xu",
        "given_name": "Genbao"
      },
      {
        "surname": "Li",
        "given_name": "Mingxing"
      }
    ]
  },
  {
    "title": "Patch-based probabilistic identification of plant roots using convolutional neural networks",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.05.010",
    "abstract": "Recently, computer vision and artificial intelligence are being used as enabling technologies for plant phenotyping studies, since they allow the analysis of large amounts of data gathered by the sensors. Plant phenotyping studies can be devoted to the evaluation of complex plant traits either on the aerial part of the plant as well as on the underground part, to extract meaningful information about the growth, development, tolerance, or resistance of the plant itself. All plant traits should be evaluated automatically and quantitatively measured in a non-destructive way. This paper describes a novel approach for identifying plant roots from images of the root system architecture using a convolutional neural network (CNN) that operates on small image patches calculating the probability that the center point of the patch is a root pixel. The underlying idea is that the CNN model should embed as much information as possible about the variability of the patches that can show chaotic and heterogeneous backgrounds. Results on a real dataset demonstrate the feasibility of the proposed approach, as it overcomes the current state of the art.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524001545",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Computer science",
      "Convolutional neural network",
      "Identification (biology)",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Probabilistic logic"
    ],
    "authors": [
      {
        "surname": "Cardellicchio",
        "given_name": "A."
      },
      {
        "surname": "Solimani",
        "given_name": "F."
      },
      {
        "surname": "Dimauro",
        "given_name": "G."
      },
      {
        "surname": "Summerer",
        "given_name": "S."
      },
      {
        "surname": "Renò",
        "given_name": "V."
      }
    ]
  },
  {
    "title": "Robust multi-view learning via M-estimator joint sparse representation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110355",
    "abstract": "Recently, multi-view learning has achieved extraordinary success in many research areas such as pattern recognition and data mining. Most existing multi-view methods mainly focus on exploring the correlation information between different views and their performance may severely degrade in the presence of heavy noises and outliers. In this paper, we put forward a robust multi-view joint sparse representation (RMJSR) method for multi-view learning. Firstly, we design a novel multi-view Cauchy estimator based loss function originating from robust statistics to address complex noises and outliers in reality. Based on this, we leverage the ℓ 1 , q norm to enhance our model by encouraging the learned representation of multiple views to share the same sparsity pattern. Secondly, to explore the optimal solution for the RMJSR model, we devise an effective optimization algorithm based on the half-quadratic (HQ) theory and the alternating direction method of multipliers (ADMM) framework. Thirdly, we provide the theoretical guarantee for revealing the theoretical condition for the success of the proposed method. Further, we have also provided extensive analysis of the proposed method, including the optimality condition, convergence analysis, and complexity analysis. Extensive experimental results validate the effectiveness and robustness of the proposed method in comparison with state-of-the-art competitors. The source code is available at https://github.com/Huyutao7/RMJSRC.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324001067",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Data mining",
      "Estimator",
      "Gene",
      "Law",
      "Leverage (statistics)",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Operating system",
      "Outlier",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Robust statistics",
      "Robustness (evolution)",
      "Source code",
      "Sparse approximation",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Hu",
        "given_name": "Yutao"
      },
      {
        "surname": "Wang",
        "given_name": "Yulong"
      },
      {
        "surname": "Li",
        "given_name": "Han"
      },
      {
        "surname": "Chen",
        "given_name": "Hong"
      }
    ]
  },
  {
    "title": "Noise level estimation using locality preserving natural image statistics",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110393",
    "abstract": "Natural images are known to have certain regular statistical properties. These properties get modified under any artificial change or distortion in natural images. Most common form of image degradation occurs in the form of noise. The amount of degradation in noisy images is measured by estimating the noise level. Many image processing applications such as denoising, restoration, segmentation, compression etc. use noise level information as a prior; inaccurate estimate of which may impact their performance. In this article, we explore natural image statistics in locality preserving transform domain. This property groups structurally similar images/image patches when projected in the transform domain. Image patches corrupted with similar noise level get projected close by in the locality preserving domain and show consistent coefficient behaviour. In particular, we use Two Dimensional Orthogonal Locality Preserving Projection (2DOLPP) as the domain transformation technique. 2DOLPP basis, representing natural images, are learnt in advance from a set of clean images, thereby reducing the computational time significantly. Features based on natural image statistics are extracted from 2DOLPP domain representation of input image patches. Mapping from feature space to noise level is carried out using support vector regression. The proposed noise estimation approach is at par with or surpasses the state-of-the-art techniques with much less computational time. Performance of this approach is stable across a wide range of noise levels and independent of the image structure.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324001444",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Engineering",
      "Estimation",
      "Geography",
      "Image (mathematics)",
      "Linguistics",
      "Locality",
      "Mathematics",
      "Natural (archaeology)",
      "Noise (video)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Statistics",
      "Systems engineering"
    ],
    "authors": [
      {
        "surname": "Shikkenawis",
        "given_name": "Gitam"
      },
      {
        "surname": "Mitra",
        "given_name": "Suman K."
      },
      {
        "surname": "Saxena",
        "given_name": "Ashutosh"
      }
    ]
  },
  {
    "title": "A randomized algorithm for clustering discrete sequences",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110388",
    "abstract": "Cluster analysis is one of the most important research issues in data mining and machine learning. To date, numerous clustering algorithms have been proposed to tackle the fixed-length vector data. In many real applications, we need to detect clusters from a set of discrete sequences in which each sequence is an ordered list of items. Due to the sequential and discrete nature, the discrete sequence clustering problem is more challenging and most of existing vector data clustering algorithms cannot be directly employed. In this paper, we present a stochastic algorithm for clustering discrete sequences. Our method first quickly generates a set of random partitions over the sequential data set and then merges these random clustering results via weighted graph construction and partition. We perform extensive empirical comparisons on real data sets to show that our method is comparable to those state-of-the-art clustering algorithms with respect to both accuracy and efficiency.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324001390",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "CURE data clustering algorithm",
      "Canopy clustering algorithm",
      "Cluster analysis",
      "Clustering high-dimensional data",
      "Combinatorics",
      "Computer science",
      "Consensus clustering",
      "Constrained clustering",
      "Correlation clustering",
      "Data mining",
      "Data stream clustering",
      "Fuzzy clustering",
      "Mathematics",
      "Partition (number theory)",
      "Single-linkage clustering"
    ],
    "authors": [
      {
        "surname": "Jiang",
        "given_name": "Mudi"
      },
      {
        "surname": "Hu",
        "given_name": "Lianyu"
      },
      {
        "surname": "Han",
        "given_name": "Xin"
      },
      {
        "surname": "Zhou",
        "given_name": "Yong"
      },
      {
        "surname": "He",
        "given_name": "Zengyou"
      }
    ]
  },
  {
    "title": "A three-stream fusion and self-differential attention network for multi-modal crowd counting",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.04.025",
    "abstract": "Multi-modal crowd counting aims at using multiple types of data, like RGB-Thermal and RGB-Depth, to count the number of people in crowded scenes. Current methods mainly focus on two-stream multi-modal information fusing in the encoder and single-scale semantic features in the decoder. In this paper, we propose an end-to-end three-stream fusion and self-differential attention network to simultaneously address the multi-modal fusion and scale variation problems for multi-modal crowd counting. Specifically, the encoder adopts three-stream fusion to fuse stage-wise modality-paired and modality-specific features. The decoder applies a self-differential attention mechanism on multi-level fused features to extract basic and differential information adaptively, and finally, the counting head predicts the density map. Experimental results on RGB-T and RGB-D benchmarks show the superiority of our proposed method compared with the state-of-the-art multi-modal crowd counting methods. Ablation studies and visualization demonstrate the advantages of the proposed modules in our model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552400134X",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Differential (mechanical device)",
      "Fusion",
      "Linguistics",
      "Modal",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Polymer chemistry",
      "Thermodynamics"
    ],
    "authors": [
      {
        "surname": "Tang",
        "given_name": "Haihan"
      },
      {
        "surname": "Wang",
        "given_name": "Yi"
      },
      {
        "surname": "Lin",
        "given_name": "Zhiping"
      },
      {
        "surname": "Chau",
        "given_name": "Lap-Pui"
      },
      {
        "surname": "Zhuang",
        "given_name": "Huiping"
      }
    ]
  },
  {
    "title": "Explainable AI in human motion: A comprehensive approach to analysis, modeling, and generation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110418",
    "abstract": "Extensive research has been conducted on analyzing human movements, driven by its diverse practical applications such as human–robot interaction, human learning, and clinical diagnosis. However, the current state-of-the-art still encounters scientific challenges when it comes to modeling human movements. There are two key aspects that need to be addressed. Firstly, new models should consider the stochastic nature of human movement and the physical structure of the human body to accurately predict the patterns in full-body motion descriptors over time. Secondly, while deep learning algorithms have been utilized, they lack explainability in terms of predicting body posture sequences, making it essential to improve their comprehensible representation of human movement. This paper aims to tackle these challenges by presenting three innovative methods for creating explainable representations of human movement. The study formulates human body movement as a state-space model based on the Gesture Operational Model (GOM). Model parameters are estimated through either one-shot training employing Kalman Filters or data-intensive training utilizing artificial neural networks. The trained models are utilized for analyzing the dexterity of expert professionals in full-body movements, enabling the identification of dynamic associations between body joints and gesture recognition. Additionally, these models are employed to generate artificial professional movements.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324001699",
    "keywords": [
      "Aesthetics",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Botany",
      "Computer science",
      "Gesture",
      "Hidden Markov model",
      "Human body",
      "Human–computer interaction",
      "Identification (biology)",
      "Law",
      "Machine learning",
      "Motion (physics)",
      "Motion capture",
      "Movement (music)",
      "Philosophy",
      "Political science",
      "Politics",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Olivas-Padilla",
        "given_name": "Brenda Elizabeth"
      },
      {
        "surname": "Manitsaris",
        "given_name": "Sotiris"
      },
      {
        "surname": "Glushkova",
        "given_name": "Alina"
      }
    ]
  },
  {
    "title": "Revisiting single-step adversarial training for robustness and generalization",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110356",
    "abstract": "Recently, single-step adversarial training has received high attention because it shows robustness and efficiency. However, a phenomenon referred to as “catastrophic overfitting” has been observed, which is prevalent in single-step defenses and may frustrate attempts to use FGSM adversarial training. To address this issue, we propose a novel method, Stable and Efficient Adversarial Training ( SEAT ). SEAT mitigates catastrophic overfitting by harnessing on local properties that differentiate a robust model from one prone to catastrophic overfitting. The proposed SEAT is underpinned by robust theoretical justifications, in that minimizing the SEAT loss is demonstrated to promote a smoother empirical risk, consequently enhancing robustness. Experimental results demonstrate that the proposed method successfully mitigates catastrophic overfitting, yielding superior performance amongst efficient defenses. Our single-step method can reach 51% robust accuracy for CIFAR-10 with l ∞ perturbations of radius 8/255 under a strong PGD-50 attack, matching the performance of a 10-step iterative method at merely 3% computational cost.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324001079",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Gene",
      "Generalization",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Meteorology",
      "Physics",
      "Robustness (evolution)",
      "Training (meteorology)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Zhuorong"
      },
      {
        "surname": "Yu",
        "given_name": "Daiwei"
      },
      {
        "surname": "Wu",
        "given_name": "Minghui"
      },
      {
        "surname": "Chan",
        "given_name": "Sixian"
      },
      {
        "surname": "Yu",
        "given_name": "Hongchuan"
      },
      {
        "surname": "Han",
        "given_name": "Zhike"
      }
    ]
  },
  {
    "title": "Parameter-free ensemble clustering with dynamic weighting mechanism",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110389",
    "abstract": "Ensemble clustering (EC) gains more and more attention because it can improve the effectiveness and robustness of single clustering methods. A popular ensemble approach is to construct a co-association matrix which represents the possibility that the sample pair is divided into different clusters by base clusterings. Then, some single clustering methods could be performed on it. However, this approach separates the construction of the co-association matrix from the generation of clustering results. Besides, the importance of base clusterings and clusters are often regarded as the same but their performance or quality is different actually. Although some weighted ensemble clustering methods have been proposed, typically, the weights are calculated based on certain criteria and then fixed. To cope with these issues, we propose a Parameter-Free Ensemble Clustering (PFEC) with dynamic weighting mechanism, which is capable of dynamically adjusting the weights of base clusterings and clusters. Firstly, the self-weighted framework is applied in our method to weight base clusterings automatically. Furthermore, an additional weight is assigned to each cluster in the base clustering, which can also be dynamically adjusted. This can help alleviate the issue of imbalanced clusters as well. Finally, a structured affinity graph is learned from the results of base clusterings through rank constraint and no post-processing is required. The experimental results on synthetic and real datasets illustrate the effectiveness of our proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324001407",
    "keywords": [
      "Acoustics",
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Mechanism (biology)",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Xie",
        "given_name": "Fangyuan"
      },
      {
        "surname": "Nie",
        "given_name": "Feiping"
      },
      {
        "surname": "Yu",
        "given_name": "Weizhong"
      },
      {
        "surname": "Li",
        "given_name": "Xuelong"
      }
    ]
  },
  {
    "title": "Deep anomaly detection on set data: Survey and comparison",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110381",
    "abstract": "Detecting anomalous samples in set data is a problem attracting increased interest due to novel modalities, such as point-cloud data produced by lidars. Novel methods including those based on deep neural networks are often tuned for a single purpose prohibiting intuition of how relevant they are for another purpose or application domains. The aim of this survey is to: (i) review elementary concepts of anomaly detection of set data, (ii) identify the building blocks of deep anomaly detectors, and (iii) analyze the impact of these blocks on performance. The impact is studied in a large experimental comparison on a variety of benchmark datasets. The results reveal that the main factor determining the performance is the type of anomalies in the dataset. While deep methods embedding the whole set to a single fixed vector perform well on point cloud data, the methods embedding each feature vector independently are better for datasets from multi-instance learning. Moreover, sophisticated methods utilizing transformer blocks are frequently inferior to simple models with properly optimized hyperparameters. An independent factor in performance is the cardinality of sets, the proper treatment of which remains an open problem, as the existing analytical solution was found to be inaccurate.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324001328",
    "keywords": [
      "Anomaly detection",
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Data set",
      "Deep learning",
      "Embedding",
      "Hyperparameter",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Point cloud"
    ],
    "authors": [
      {
        "surname": "Mašková",
        "given_name": "Michaela"
      },
      {
        "surname": "Zorek",
        "given_name": "Matěj"
      },
      {
        "surname": "Pevný",
        "given_name": "Tomáš"
      },
      {
        "surname": "Šmídl",
        "given_name": "Václav"
      }
    ]
  },
  {
    "title": "Camera-aware cluster-instance joint online learning for unsupervised person re-identification",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110359",
    "abstract": "Unsupervised person re-identification (re-ID) aims at learning discriminative feature representations for person retrieval without any annotations. Pseudo-label-based methods that iteratively perform pseudo-label generation and model training are currently the most popular approach to achieve this goal. However, distribution variations among cameras inevitably introduce noise in the generated pseudo-labels. Moreover, they are often assigned offline using relatively simple clustering criteria, which further accumulates the noise and limits the potential improvement in model performance. To address these issues, we propose a novel camera-aware cluster-instance joint online learning (CCIOL) framework that leverages the online inter-camera K-reciprocal nearest neighbors (OICKRNs) mined for each sample at every iteration to soften the traditional hard pseudo-labels at the cluster-level and generate multi-labels at the instance-level. Additionally, contrastive learning losses at two levels are employed to rectify the erroneous closeness between samples and promote intra-class aggregation and inter-class separation. Extensive experimental results on Market1501 and MSMT17 demonstrate the competitiveness of the proposed method compared to state-of-the-art unsupervised re-ID approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324001109",
    "keywords": [
      "Architectural engineering",
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Cluster (spacecraft)",
      "Computer science",
      "Computer vision",
      "Engineering",
      "Identification (biology)",
      "Joint (building)",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Programming language",
      "Unsupervised learning"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Zhaoru"
      },
      {
        "surname": "Fan",
        "given_name": "Zheyi"
      },
      {
        "surname": "Chen",
        "given_name": "Yiyu"
      },
      {
        "surname": "Zhu",
        "given_name": "Yixuan"
      }
    ]
  },
  {
    "title": "Deterministic attribute selection for isolation forest",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110395",
    "abstract": "Modern data mining techniques have been gained importance in recent years. In particular, anomaly detection algorithms, applied in key sectors of information technology, have been growing in popularity. One of the efficient and fast algorithms is Isolation Forest. The method consists of two separated stages: Forest formation and evaluation of elements. The first stage relies on forming a forest of isolation trees. Each tree is built in the same manner according to drawn samples and random divisions of data attributes. In this study, an innovative deterministic attribute selection method is proposed, maintaining its random value. New ideas based on imbalance, clustering, and a dispersion of values through non-linear transformation of elements are introduced and thoroughly analyzed. These novel anomaly detection approaches are applied to 25 real datasets, as well as our own artificially generated databases. The Area Under the ROC Curve and the Area Under the PR Curve are used as a measure of the outliers classification quality. The results of the numerical experiment have proven high efficiency and competitive evaluation speed of the proposals in comparison to other Isolation Forest-based approaches, as well as several other popular techniques.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324001468",
    "keywords": [
      "Anomaly detection",
      "Artificial intelligence",
      "Biochemistry",
      "Biology",
      "Chemistry",
      "Cluster analysis",
      "Computer science",
      "Computer security",
      "Data mining",
      "Epistemology",
      "Gene",
      "Isolation (microbiology)",
      "Key (lock)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Measure (data warehouse)",
      "Microbiology",
      "Outlier",
      "Philosophy",
      "Quality (philosophy)",
      "Random forest",
      "Selection (genetic algorithm)",
      "Transformation (genetics)",
      "Tree (set theory)"
    ],
    "authors": [
      {
        "surname": "Gałka",
        "given_name": "Łukasz"
      },
      {
        "surname": "Karczmarek",
        "given_name": "Paweł"
      }
    ]
  },
  {
    "title": "Global and local multi-modal feature mutual learning for retinal vessel segmentation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110376",
    "abstract": "Research on optical coherence tomography angiography (OCTA) images has received extensive attention in recent years since it provides more detailed information about retinal structures. The automatic segmentation of retinal vessel (RV) has become one of the key issues in the quantification of retinal indicators. To this end, there are various methods proposed with cutting-edge designs and techniques in the literature. However, most of them only learn features from single-modal data, despite the potential relation between data from different modalities. Clinically, 2D projection maps are more convenient for doctors to observe. Nevertheless, 3D volumes preserve the intrinsic retinal structure. We thus propose a novel multi-modal feature mutual learning framework that contains local mutual learning and global mutual learning capturing 2D and 3D information. In the framework, the 3D model and 2D model learn collaboratively and teach each other throughout the training process. Experimental results show that our method outperforms previous deep-learning methods in RV segmentation. The generalization experiments on the ROSE dataset demonstrate the portability and scalability of the proposed framework.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324001274",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Database",
      "Feature (linguistics)",
      "Linguistics",
      "Machine learning",
      "Medicine",
      "Modal",
      "Mutual information",
      "Operating system",
      "Ophthalmology",
      "Optical coherence tomography",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Polymer chemistry",
      "Process (computing)",
      "Programming language",
      "Scalability",
      "Segmentation",
      "Software portability"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Xin"
      },
      {
        "surname": "Zhang",
        "given_name": "Jing"
      },
      {
        "surname": "Li",
        "given_name": "Qiaozhe"
      },
      {
        "surname": "Zhao",
        "given_name": "Tengfei"
      },
      {
        "surname": "Li",
        "given_name": "Yi"
      },
      {
        "surname": "Wu",
        "given_name": "Zifeng"
      }
    ]
  },
  {
    "title": "Select & Enhance: Masked-based image enhancement through tree-search theory and deep reinforcement learning",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.05.013",
    "abstract": "The enhancement of low-quality images is both a challenging task and an essential endeavor in many fields including computer vision, computational photography, and image processing. In this paper, we propose a novel and fully explainable method for image enhancement that combines spatial selection and histogram equalization. Our approach leverages tree-search theory and deep reinforcement learning to iteratively select areas to be processed. Extensive experimentation on two datasets demonstrates the quality of our method compared to other state-of-the-art models. We also conducted a multi-user experiment which shows that our method can emulate a variety of enhancement styles. These results highlight the effectiveness and versatility of the proposed method in producing high-quality images through an explainable enhancement process.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524001570",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Engineering",
      "Image (mathematics)",
      "Image enhancement",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Reinforcement",
      "Reinforcement learning",
      "Structural engineering",
      "Tree (set theory)"
    ],
    "authors": [
      {
        "surname": "Cotogni",
        "given_name": "Marco"
      },
      {
        "surname": "Cusano",
        "given_name": "Claudio"
      }
    ]
  },
  {
    "title": "Improving license plate recognition via diverse stylistic plate generation",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.05.005",
    "abstract": "Deep learning-based license plate recognition heavily relies on the quantity and diversity of training images. However, manually collecting diverse license plate images is a highly time-consuming and labor-intensive task. Hence, generating license plate images to supplement training data emerges as a cost-effective solution for recognition enhancement. Previous generation methods treated the style of license plates as discrete features and extracted style features from a referenced image to generate multi-style license plate images. However, these methods are incapable of generating license plates with arbitrary new styles, as the style of the generated images is constrained by the existing images. To solve this problem, we propose to extract style features from randomly sampled vectors in continuous space for diverse stylistic license plate generation. This way, license plates with arbitrary styles can be generated by simply modifying the sampled style vectors. Besides, to provide controllable content information for the generator, we propose an orientation-aware content encoder to extract multi-oriented glyph content features from the input horizontal glyphs. Moreover, a dual-discriminator is proposed to conduct adversarial learning on both color images and image gradients, aiming to mitigate unnatural patterns and ensure realistic generated results. Finally, extensive experiments on multiple public license plate datasets demonstrate that our method can generate realistic and diverse license plates, and the generated images can significantly improve recognition performance by up to 11.5%.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524001491",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "License",
      "Natural language processing",
      "Operating system",
      "Pattern recognition (psychology)",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Qi"
      },
      {
        "surname": "Chen",
        "given_name": "Song-Lu"
      },
      {
        "surname": "Chen",
        "given_name": "Yu-Xiang"
      },
      {
        "surname": "Yin",
        "given_name": "Xu-Cheng"
      }
    ]
  },
  {
    "title": "The diversified-equal loss for image translation tasks",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.04.027",
    "abstract": "Image translation uses unpaired data to learn a multi-mode generator between multiple domains. Researchers use the cycle-consistent loss to train the generator from unpaired data, and design the mode-seeking loss to solve the model collapse problem. Although this divide-and-conquer strategy has achieved remarkable success, it sacrifices the simplicity of networks and objectives. We propose a simple yet effective diversified-equal loss and prove that our loss can be used to take the place of cycle-consistent loss and mode-seeking loss simultaneously. Both qualitative and quantitative results demonstrate the effectiveness of the proposed loss for improving baseline model performance in four image translation tasks: image generation, single-modal, multi-modal, and even single-image translation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524001363",
    "keywords": [
      "Arithmetic",
      "Artificial intelligence",
      "Biochemistry",
      "Biology",
      "Computer science",
      "Computer vision",
      "Gene",
      "Image (mathematics)",
      "Image translation",
      "Mathematics",
      "Messenger RNA",
      "Translation (biology)"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Qianhao"
      },
      {
        "surname": "Dai",
        "given_name": "Longquan"
      },
      {
        "surname": "Tang",
        "given_name": "Jinhui"
      }
    ]
  },
  {
    "title": "FApSH: An effective and robust local feature descriptor for 3D registration and object recognition",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110354",
    "abstract": "Three-dimensional (3D) local feature descriptor plays an important role in 3D computer vision because it is widely used to build point-to-point correspondences in many 3D vision applications. However, existing descriptors are difficult to have both high descriptiveness and strong robustness to various nuisances (e.g., noise and occlusion). To address this problem, a descriptor named Fully Attribute-pairs Statistical Histogram (FApSH) is proposed. FApSH is constructed on a local reference axis (LRA), and fully encodes the relevancy information of five attributes at each neighbor point by ten attribute-pair statistics. In this process, a priori distribution-based partition strategy is proposed for evenly distributing the attribute values of all neighbor points, and a radial distance-based histogram assignment method is proposed to improve the robustness to noise and outliers. The proposed methods are rigorously evaluated on six benchmark datasets with different application scenarios and nuisances. The results show that FApSH has high descriptiveness and strong robustness. It obviously outperforms the existing handcrafted descriptors, and is comparable to some superior learning-based descriptors. The results also show that the proposed priori distribution-based partition strategy significantly reduces the length and also improves the descriptiveness of FApSH, and the proposed radial distance-based histogram assignment method improves the robustness of FApSH on various datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324001055",
    "keywords": [
      "A priori and a posteriori",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Epistemology",
      "Gene",
      "Histogram",
      "Image (mathematics)",
      "Mathematics",
      "Outlier",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Bao"
      },
      {
        "surname": "Wang",
        "given_name": "Zihan"
      },
      {
        "surname": "Chen",
        "given_name": "Xiaobo"
      },
      {
        "surname": "Fang",
        "given_name": "Xianyong"
      },
      {
        "surname": "Jia",
        "given_name": "Zhaohong"
      }
    ]
  },
  {
    "title": "Deep Joint Semantic Adaptation Network for Multi-source Unsupervised Domain Adaptation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110409",
    "abstract": "Multi-source Unsupervised Domain Adaptation (MUDA) transfers knowledge learned from multiple labeled source domains to an unlabeled target domain by minimizing the domain shift between multiple source domains and the target domain. Recent studies on MUDA have focused on aligning the distribution of each pair of source and target domains in separate feature spaces to reduce their domain shift. However, these approaches suffer from two main shortcomings. First, they usually focus on the global domain shift which lacks consideration of the joint distribution of category-corresponded subdomains. Second, out-of-distribution samples far from the sample center are hard to align by the global domain alignment. Therefore, we propose a novel Deep Joint Semantic Adaptive Network (DJSAN) for MUDA. Specifically, a new maximum mean discrepancy-based metric, Joint Semantic Maximum Mean Discrepancy (JSMMD), is proposed, which can uniformly optimize the cross-domain joint distribution of category-corresponded subdomains on multiple task-specific layers. Moreover, to deal with the out-of-distribution hard samples, we propose an across-domain data augmentation method called Source-Target Domain Mixing (STDMix) to enhance the robustness of the model, which synthesizes the source domain and target domain into a new domain at a fixed ratio and utilizes information entropy to provide reliable pseudo-labels for samples in the target domain. Experimental results on three public datasets, i.e., Office-31, Digits-five, and Office-Home, show that our proposed method achieves improvements of 0.3%, 1.8%, and 2.7% in average accuracy, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324001602",
    "keywords": [
      "Architectural engineering",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Domain (mathematical analysis)",
      "Domain adaptation",
      "Engineering",
      "Gene",
      "Joint (building)",
      "Joint probability distribution",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Robustness (evolution)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Cheng",
        "given_name": "Zhiming"
      },
      {
        "surname": "Wang",
        "given_name": "Shuai"
      },
      {
        "surname": "Yang",
        "given_name": "Defu"
      },
      {
        "surname": "Qi",
        "given_name": "Jie"
      },
      {
        "surname": "Xiao",
        "given_name": "Mang"
      },
      {
        "surname": "Yan",
        "given_name": "Chenggang"
      }
    ]
  },
  {
    "title": "Deepfake face discrimination based on self-attention mechanism",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.02.019",
    "abstract": "With the rapid progress of deepfake technology, the improper use of manipulated images and videos presenting synthetic faces has arisen as a noteworthy concern, thereby posing threats to both daily life and national security. While numerous CNN based deepfake face detection methods were proposed, most of the existing approaches encounter challenges in effectively capturing the image contents across different scales and positions. In this paper, we present a novel two-branch structural network, referred to as the Self-Attention Deepfake Face Discrimination Network (SADFFD). Specifically, a branch incorporating cascaded multi self-attention mechanism (SAM) modules, is parallelly integrated with EfficientNet-B4 (EffB4). The multi SAM branch supplies additional features that concentrate on image regions essential for discriminating between real and fake. The EffB4 network is adopted because of its efficiency by adjusting the resolution, depth, and width of the network. According to our comprehensive experiments conducted on FaceForensics++, Celeb-DF, and our self-constructed SAMGAN3 datasets, the proposed SADFFD demonstrated the highest detection accuracy, averaging 99.01% in FaceForensics++, 98.65% in Celeb-DF, and an impressive 99.99% in SAMGAN3, surpassing other state-of-the-art (SOTA) methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524000527",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Epistemology",
      "Face (sociological concept)",
      "Mechanism (biology)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Shuai"
      },
      {
        "surname": "Zhu",
        "given_name": "Donghui"
      },
      {
        "surname": "Chen",
        "given_name": "Jian"
      },
      {
        "surname": "Bi",
        "given_name": "Jiangbo"
      },
      {
        "surname": "Wang",
        "given_name": "Wenyi"
      }
    ]
  },
  {
    "title": "LCSeg-Net: A low-contrast images semantic segmentation model with structural and frequency spectrum information",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110428",
    "abstract": "Semantic segmentation in low-contrast images is a challenging problem due to the ambiguous boundary of the segmented target and indistinguishable noise. Current models generally segment the target with local features, which leads to the lack of structural information, which reduces the performance. And they directly fuse multi-scale features to keep details, while it may integrate noise into the fused features. These problems reduce the performance of segmentation in the low-contrast image. To solve the above issues, we propose an image semantic segmentation model called Low-Contrast Segmentation Net (LCSeg-Net). The model enhances the structural information with the global context and reduces the noise of the fused features through the adaptive fusion way. Meanwhile, to improve the robustness of LCSeg-Net, we augment the low-frequency spectrum of the fused feature in the training phase. Extensive experiments are conducted on the five public datasets. Results show the proposed model achieves the best comprehensive performance in the low-contrast image.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324001791",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Contrast (vision)",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Segmentation",
      "Spectrum (functional analysis)"
    ],
    "authors": [
      {
        "surname": "Yuan",
        "given_name": "Haochen"
      },
      {
        "surname": "Peng",
        "given_name": "Junjie"
      }
    ]
  },
  {
    "title": "Decoupled representation for multi-view learning",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110377",
    "abstract": "Learning multi-view data is a central topic for advanced deep model applications. Existing efforts mainly focus on exploring shared information to maximize the consensus among all the views. However, after reasonably discarding superfluous task-irrelevant noise, the view-specific information is equally essential to downstream tasks. In this paper, we propose to decouple the multi-view representation learning into the shared and specific information extractions with parallel branches, and seamlessly adopt feature fusion in end-to-end models. The common feature is obtained based on the view-agnostic contrastive learning and view-discriminative training to minimize the discrepancy within the views. Simultaneously, the specific feature is learned with orthogonality constraints to minimize the view-level correlation. Besides, the semantic information in the features is reserved with supervised training. After disentangling the representations, we fuse the mutually complementary common and specific features for downstream tasks. Particularly, we provide a theoretical explanation for our method from an information bottleneck perspective. Compared with state-of-the-art multi-view models on benchmark datasets, we empirically demonstrate the advantage of our method in several downstream tasks, such as ordinary classification and few-shot learning. In addition, extensive experiments validate the robustness and transferability of our approach, when applying the learned representation on the source dataset to several target datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324001286",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Discriminative model",
      "Feature (linguistics)",
      "Feature learning",
      "Focus (optics)",
      "Gene",
      "Information bottleneck method",
      "Law",
      "Linguistics",
      "Machine learning",
      "Mutual information",
      "Optics",
      "Philosophy",
      "Physics",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Sun",
        "given_name": "Shiding"
      },
      {
        "surname": "Wang",
        "given_name": "Bo"
      },
      {
        "surname": "Tian",
        "given_name": "Yingjie"
      }
    ]
  },
  {
    "title": "Temporal pattern mining for knowledge discovery in the early prediction of septic shock",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110436",
    "abstract": "Temporal pattern mining can be employed to detect patterns and trends in a patient's health status as it evolves over time. However, these methods often produce an overwhelming number of patterns, impeding knowledge discovery and practical implementation in acute care settings. To address this, we propose a framework that focuses on identifying a concise set of relevant temporal patterns and static variables from electronic health records for the early prediction of septic shock. Sepsis is caused by an adverse immune response to infection that triggers widespread inflammation throughout the body, which can progress to septic shock and ultimately result in death if not treated promptly. The analysis of health state patterns in sepsis patients over time offers the potential to predict septic shock prior to its onset, enabling proactive healthcare interventions. Our framework incorporates a temporal pattern mining method and four feature selection techniques. We discover that selecting features based on a model-based wrapper approach yields the highest prediction performance among these techniques. On the other hand, the use of information value identifies more multi-state patterns with abnormal health states, providing healthcare providers with valuable indicators of patient deterioration.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324001870",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Data science",
      "Economic growth",
      "Economics",
      "Feature (linguistics)",
      "Feature selection",
      "Health care",
      "Immunology",
      "Intensive care medicine",
      "Internal medicine",
      "Knowledge extraction",
      "Linguistics",
      "Machine learning",
      "Medicine",
      "Philosophy",
      "Psychiatry",
      "Psychological intervention",
      "Sepsis",
      "Septic shock",
      "Shock (circulatory)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Ruoting"
      },
      {
        "surname": "Agor",
        "given_name": "Joseph K."
      },
      {
        "surname": "Özaltın",
        "given_name": "Osman Y."
      }
    ]
  },
  {
    "title": "Data filtering for efficient adversarial training",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110394",
    "abstract": "Adversarial training has been considered to be one of the most effective strategies to defend against adversarial attacks. Most existing adversarial training methods have shown a trade-off between training cost and robustness. This paper explores a new optimization direction from training data to reduce the computational cost of adversarial training without scarifying robustness. First, we show that some adversarial examples are less important, meaning that removing them does not hurt the robustness. Second, we propose a method to identify insignificant adversarial examples at a minimal cost. Third, we demonstrate that our approach can be integrated with other adversarial training frameworks with few modifications. The experimental results show that combined with previous works, our approach not only reduces about 20% of computational cost on the CIFAR10 and CIFAR100 datasets but also improves about 1.5% natural accuracy. With less computational cost, it achieves 58.22%, 30.68%, and 41.92% robust accuracy on CIFAR10, CIFAR100, and ImageNet datasets respectively, which are higher than those of the original methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324001456",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Gene",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Meteorology",
      "Physics",
      "Robustness (evolution)",
      "Training (meteorology)",
      "Training set"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Erh-Chung"
      },
      {
        "surname": "Lee",
        "given_name": "Che-Rung"
      }
    ]
  },
  {
    "title": "Coordinating explicit and implicit knowledge for knowledge-based VQA",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110368",
    "abstract": "Pre-trained models often generate plausible looking statements that are factually incorrect because of the inaccurate implicit knowledge contained in the model’s parameters. Related methods retrieve explicit knowledge from the external knowledge source to help improve the prediction performance and reliability. However, these methods often use weak training signals for the retriever, and require the model to make each prediction based on the retrieved knowledge, even when the retrieved knowledge is not reliable or the model can produce better prediction only using its implicit knowledge. Therefore, it is necessary to enable the pre-trained model to actively select more beneficial knowledge for producing better prediction. This work proposes a novel method to help the model to Coordinate Explicit and Implicit Knowledge (CEIK) for the knowledge-based visual question answering (VQA) task, which is an important direction of pre-trained models. Furthermore, a better training signal is proposed for the retriever according to whether the retrieved knowledge can correct the prediction. Experimental results demonstrate the effectiveness of our method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324001195",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Implicit knowledge",
      "Knowledge management"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Qunbo"
      },
      {
        "surname": "Liu",
        "given_name": "Jing"
      },
      {
        "surname": "Wu",
        "given_name": "Wenjun"
      }
    ]
  },
  {
    "title": "Improving ellipse fitting via multi-scale smoothing and key-point searching",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110432",
    "abstract": "Fast and efficient fitting of accurate ellipses from data points has many applications in pattern recognition, machine vision, and robotics. However, the fitting accuracy may significantly degrade in the existence of outliers, such as the least-squares-based approaches. Despite robust methods attaining more accurate results than the least-squares manner under the contamination of outliers, they typically require the careful tuning of the hyper-parameters for good results. To mitigate the outlier disturbance, in this paper, we propose a conceptually simple yet quite useful preprocessing framework for high-precision ellipse fitting. Firstly, we leverage multi-scale operators to shrink the input image, by which a large number of outliers can be removed, followed by the smoothing of the sub-image to further improve the data quality. Then, we propose a key-point searching method to enhance the fitting precision via the analysis of the discrete pixel data in images. We prove that key-point-based ellipse fitting gives the upper bound of the approximation error generated by other sampled points with the same ellipse. Based on the key-point pairs inside and outside the ellipse, we further calculate their barycentric points and then perform fitting on these points to attain high-precision ellipses. We conduct extensive experiments on synthetic and real-world images to validate the proposed method and compare it with representative state-of-the-art approaches. Quantitative and qualitative results demonstrate that our method has more accurate and robust performance than competitors. Additionally, we employ the proposed method to compared approaches as a preprocessing step. Experiments demonstrate that our method is effective to significantly improve their fitting accuracy. Our source code is freely available at https://github.com/ChengQian09/MSKPF.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324001833",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Ellipse",
      "Geometry",
      "Key (lock)",
      "Mathematical optimization",
      "Mathematics",
      "Physics",
      "Point (geometry)",
      "Quantum mechanics",
      "Scale (ratio)",
      "Smoothing"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Xiao-Diao"
      },
      {
        "surname": "Qian",
        "given_name": "Cheng"
      },
      {
        "surname": "Zhao",
        "given_name": "Mingyang"
      },
      {
        "surname": "Yong",
        "given_name": "Jun-Hai"
      },
      {
        "surname": "Yan",
        "given_name": "Dong-Ming"
      }
    ]
  },
  {
    "title": "BALQUE: Batch active learning by querying unstable examples with calibrated confidence",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110385",
    "abstract": "Active learning alleviates labeling costs by selecting and labeling the most informative examples from an unlabeled pool. However, most existing active learning approaches estimate informativeness with uncalibrated confidence, resulting in unreliable informativeness estimation. These approaches generally ignored two significant issues caused by uncalibrated confidence methods. Firstly, the average uncalibrated confidence generated by modern neural networks is usually higher than the accuracy. Secondly, examples located near the decision boundaries are unstable during prediction when the target model updates parameters in the last several epochs, even throughout the training process. This phenomenon, caused by the forgetting characteristic of neural networks, has a significant impact on some specific models that estimate the informativeness by predicted probability vectors or pseudo labels. To address these issues, in this paper, we propose a novel active learning approach to reliably estimate informativeness with calibrated confidence. Specifically, we integrate the intermediate predictions for each unlabeled example, generated by the target model during the training process, to generate calibrated confidence. The calibrated confidence can capture a tendentious label from an indecisive subset of the class space. We show that the calibrated confidence with tendentiousness can maintain the ability of correct predictions. The empirical results demonstrate that our approach outperforms the state-of-the-art active learning methods on image classification tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324001365",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Calibration",
      "Computer science",
      "Confidence interval",
      "Forgetting",
      "Linguistics",
      "Low Confidence",
      "Machine learning",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Process (computing)",
      "Psychology",
      "Social psychology",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Han",
        "given_name": "Yincheng"
      },
      {
        "surname": "Liu",
        "given_name": "Dajiang"
      },
      {
        "surname": "Shang",
        "given_name": "Jiaxing"
      },
      {
        "surname": "Zheng",
        "given_name": "Linjiang"
      },
      {
        "surname": "Zhong",
        "given_name": "Jiang"
      },
      {
        "surname": "Cao",
        "given_name": "Weiwei"
      },
      {
        "surname": "Sun",
        "given_name": "Hong"
      },
      {
        "surname": "Xie",
        "given_name": "Wu"
      }
    ]
  },
  {
    "title": "Relation fusion propagation network for transductive few-shot learning",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110367",
    "abstract": "Previous graph-based meta-learning approaches have explored pairwise feature similarity to learn instance-level relations of samples, however, the gap between the sample relations in feature and label spaces is often ignored. It is empirically observed that instances with different labels may display considerable similarity in visual characteristics, making it challenging to distinguish between them in the feature space. To this end, we propose a dual-branch Relation Fusion Propagation Network (RFPN) for transductive few-shot learning, which explicitly models both feature and label relations across support-query pairs. Specifically, we design a Relation Fusion Block (RFB) to fuse instance-level and class-level relations, thus obtaining more robust fusion-level relations to guide feature propagation. In addition to the feature propagation branch, we encode the label relations and construct a label shortcut branch for label propagation. To alleviate the error accumulation during propagation, which is caused by uncertain pseudo-labels for query samples, we propose a Label Shortcut Mechanism (LSM) to progressively update the sample relations with the initial labels. Our full method is plug-and-play and can be easily applied in existing graph-based approaches for transductive few-shot learning. Extensive experiments demonstrate that our proposed RFPN yields significant improvements over the baselines and achieves promising performance on four popular few-shot classification benchmarks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324001183",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Feature (linguistics)",
      "Feature vector",
      "Graph",
      "Image (mathematics)",
      "Linguistics",
      "Machine learning",
      "Pairwise comparison",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Relation (database)",
      "Similarity (geometry)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Yixiang"
      },
      {
        "surname": "Hao",
        "given_name": "Hongyu"
      },
      {
        "surname": "Ge",
        "given_name": "Weichao"
      },
      {
        "surname": "Cao",
        "given_name": "Yang"
      },
      {
        "surname": "Wu",
        "given_name": "Ming"
      },
      {
        "surname": "Zhang",
        "given_name": "Chuang"
      },
      {
        "surname": "Guo",
        "given_name": "Jun"
      }
    ]
  },
  {
    "title": "MOD-YOLO: Multispectral object detection based on transformer dual-stream YOLO",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.05.001",
    "abstract": "Multispectral object detection can effectively improve the precision of object detection in low-visibility scenes, which increases the reliability and stability of the object detection application in the open environment. Cross-Modality Fusion Transformer (CFT) can effectively fuse different spectral information, but this method relies on large models and expensive computing resources. In this paper, we propose multispectral object detection dual-stream YOLO (MOD-YOLO), based on Cross Stage Partial CFT (CSP-CFT), to address the issue that prior studies need heavy inference calculations from the recurrent fusing of multispectral features. This network can divide the fused feature map into two parts, respectively for cross stage output and combined with the next stage feature, to achieve the correct speed/memory/precision balance. To further improve the accuracy, SIoU was selected as the loss function. Ultimately, extensive experiments on multiple publicly available datasets demonstrate that our model, which achieves the smallest model size and excellent performance, produces better tradeoffs between accuracy and model size than other popular models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524001399",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Dual (grammatical number)",
      "Literature",
      "Multispectral image",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Shao",
        "given_name": "Yanhua"
      },
      {
        "surname": "Huang",
        "given_name": "Qimeng"
      },
      {
        "surname": "Mei",
        "given_name": "yanying"
      },
      {
        "surname": "Chu",
        "given_name": "hongyu"
      }
    ]
  },
  {
    "title": "Embrace sustainable AI: Dynamic data subset selection for image classification",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110392",
    "abstract": "Data selection is commonly used to reduce costs and energy usage by training on a subset of available data. However, determining the appropriate subset size requires extensive dataset knowledge and experimentation, limiting transferability. Varying the validation set also produces unstable results and wastes computational resources. In this paper, we propose a data selection method for dynamically determining subset ratios based on model performance using only a training set. The data search space is narrowed through weighted sampling, leveraging statistical selection patterns. Parallel analysis of class distributions identifies the most representative samples with high selection potential. Extensive experiments validate our approach and demonstrate improved training efficiency. Our method speeds up various subset ratios by up to 2.2x on CIFAR-10, 1.9x on CIFAR-100, 2.0x on TinyImageNet, and 2.3x on ImageNet with negligible accuracy drops.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324001432",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Data set",
      "Engineering",
      "Filter (signal processing)",
      "Image (mathematics)",
      "Limiting",
      "Logit",
      "Machine learning",
      "Mechanical engineering",
      "Pattern recognition (psychology)",
      "Programming language",
      "Sampling (signal processing)",
      "Selection (genetic algorithm)",
      "Set (abstract data type)",
      "Training set",
      "Transferability"
    ],
    "authors": [
      {
        "surname": "Yin",
        "given_name": "Zimo"
      },
      {
        "surname": "Pu",
        "given_name": "Jian"
      },
      {
        "surname": "Wan",
        "given_name": "Ru"
      },
      {
        "surname": "Xue",
        "given_name": "Xiangyang"
      }
    ]
  },
  {
    "title": "Joint negative–positive-learning based sample reweighting for hyperspectral image classification with label noise",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.04.028",
    "abstract": "Deep neural networks (DNNs) have been widely used for hyperspectral image (HSI) classification. However, the superior performance of DNNs requires accurately labeled samples. In various applications, label noise in large-scale HSI data is often unavoidable and adversely affects the performance of a classifier. Therefore, establishing a robust classification model from an existing DNN classifier for the HSI dataset with label noise is a significant and challenging problem. In this report, we propose a meta-learning reweighting framework based on a joint positive and negative learning (JPNL) method which can adaptively reweight labeled samples to improve the robustness of the classification model. Experimental results on two commonly used hyperspectral imaging (HSI) datasets contaminated with label noise demonstrate significant improvements in classification performance and robustness achieved by the proposed framework, as compared to existing classifiers.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524001387",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Classifier (UML)",
      "Computer science",
      "Contextual image classification",
      "Gene",
      "Hyperspectral imaging",
      "Image (mathematics)",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Liao",
        "given_name": "Qiming"
      },
      {
        "surname": "Zhao",
        "given_name": "Lin"
      },
      {
        "surname": "Luo",
        "given_name": "Wenqiang"
      },
      {
        "surname": "Li",
        "given_name": "Xinping"
      },
      {
        "surname": "Zhang",
        "given_name": "Guoyun"
      }
    ]
  },
  {
    "title": "A shared-private sentiment analysis approach based on cross-modal information interaction",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.05.009",
    "abstract": "To explore the heterogeneous sentiment information in each modal feature and improve the accuracy of sentiment analysis, this paper proposes a Multimodal Sentiment Analysis based on Text-Centric Sharing-Private Affective Semantics (TCSP). First, the Deep Canonical Time Wrapping (DCTW) algorithm is employed to effectively align the timing deviations of Audio and Picture modalities. Then, a cross-modal shared mask matrix is designed, and a mutual attention mechanism is introduced to compute the shared affective semantic features of Audio-picture-to-text. Following this, the private affective semantic features within Audio and Picture modalities are derived via the self-attention mechanism with LSTM. Finally, the Transformer Encoder structure is improved, achieving deep interaction and feature fusion of cross-modal emotional information, and conducting emotional analysis. Experiments are conducted on the IEMOCAP and MELD datasets. By comparing with current state-of-the-art models, the accuracy of the TCSP model reaches 82.02%, fully validating the effectiveness. In addition, the rationality of the design of each structure within the model is verified through ablation experiments.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524001533",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Information retrieval",
      "Modal",
      "Natural language processing",
      "Polymer chemistry",
      "Sentiment analysis",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Hou",
        "given_name": "Yilin"
      },
      {
        "surname": "Zhong",
        "given_name": "Xianjing"
      },
      {
        "surname": "Cao",
        "given_name": "Hui"
      },
      {
        "surname": "Zhu",
        "given_name": "Zheng"
      },
      {
        "surname": "Zhou",
        "given_name": "Yunfeng"
      },
      {
        "surname": "Zhang",
        "given_name": "Jie"
      }
    ]
  },
  {
    "title": "Medical image segmentation based on dynamic positioning and region-aware attention",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110375",
    "abstract": "Transformer has already proven its ability to model long-distance dependencies. However, medical images have strong local structures. Directly using Transformer to extract features would not only contain redundant information increasing the computational effort, but also be detrimental to extracting local details. Given these issues, we propose a network based on dynamic positioning and region-aware attention, which adopts a two-stage feature extraction strategy. In the shallow layer, we design Dynamic Positioning Attention (DPA). It will localize to the key feature information and construct a variable window for it, then perform attention calculation. DPA improves the learning ability of local details, reduces the amount of computation. At the deep level, Bi-Level Routing Attention (BRA) is used to discard irrelevant key–value pairs, achieve content-aware sparse attention for the deep dispersed semantic information, and improve computational efficiency. After several experiments, the results show that our method achieves advanced performance on different types of datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324001262",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Attention network",
      "Computation",
      "Computer network",
      "Computer science",
      "Computer security",
      "Construct (python library)",
      "Data mining",
      "Electrical engineering",
      "Engineering",
      "Feature (linguistics)",
      "Feature extraction",
      "Key (lock)",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Segmentation",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Zhongmiao"
      },
      {
        "surname": "Cheng",
        "given_name": "Shuli"
      },
      {
        "surname": "Wang",
        "given_name": "Liejun"
      }
    ]
  },
  {
    "title": "Graph Convolutional Network with elastic topology",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110364",
    "abstract": "Graph Convolutional Network (GCN) has drawn widespread attention in data mining on graphs due to its outstanding performance and rigor theoretical guarantee. However, some recent studies have revealed that GCN-based methods may mine latent information insufficiently owing to the underutilization of the feature space. Besides, the unlearnable topology also significantly imperils the performance of GCN-based methods. In this paper, we conduct experiments to investigate these issues, finding that GCN does not fully consider the potential structure in the feature space, and a fixed topology deteriorates the robustness of GCN. Thus, it is desired to distill node features and establish a learnable graph. Motivated by this goal, we propose a framework dubbed Graph Convolutional Network with elastic topology (GCNet 1 1 The source code is available at https://github.com/ZhihaoWu99/GCNet. ). With the analysis of the optimization for the proposed flexible Laplacian embedding, GCNet is naturally constructed by alternative graph convolutional layers and adaptive topology learning layers. GCNet aims to deeply explore the feature space and employ the mined information to construct a learnable topology, which leads to a more robust graph representation. In addition, a set-level orthogonal loss is utilized to meet the orthogonal constraint required by the flexible Laplacian embedding and promote better class separability. Moreover, comprehensive experiments indicate that GCNet achieves remarkable performance and generalization on several real-world datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324001158",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Combinatorics",
      "Computer science",
      "Embedding",
      "Feature learning",
      "Gene",
      "Graph",
      "Graph embedding",
      "Line graph",
      "Mathematics",
      "Network topology",
      "Operating system",
      "Robustness (evolution)",
      "Theoretical computer science",
      "Topological graph theory",
      "Topology (electrical circuits)",
      "Voltage graph"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Zhihao"
      },
      {
        "surname": "Chen",
        "given_name": "Zhaoliang"
      },
      {
        "surname": "Du",
        "given_name": "Shide"
      },
      {
        "surname": "Huang",
        "given_name": "Sujia"
      },
      {
        "surname": "Wang",
        "given_name": "Shiping"
      }
    ]
  },
  {
    "title": "Unsupervised face image deblurring via disentangled representation learning",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.04.020",
    "abstract": "Unsupervised face image deblurring aims to restore latent clear face images with explicit structure and facial details without relying on labels. Despite the recent progress, most existing approaches ignore the domain shift issue between clear and blur domains, leading to color distortion and loss of detail in deblurred images. To tackle this issue, we propose a symmetrical unsupervised framework including a decoupling stage and a reconstruction stage. In the decoupling stage, we employ disentangled representation and adversarial domain translation to establish discriminative boundaries between content-invariant domains and feature-shift domains. Besides, a clear feature encoder for clear images is introduced to obtain a latent feature space containing more detailed clarity, providing reliable input for subsequent reconstruction. In the reconstruction stage, similar modules are adopted to ensure the integrity of the mutual mapping between clear and blurred images, while domain latent supervision loss is introduced to ensure that the decoupled features and content information from both stages belong to the same domain. Extensive experimental results on popular benchmarks validate the effectiveness of our method by surpassing existing works.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524001296",
    "keywords": [
      "Artificial intelligence",
      "Autoencoder",
      "Computer science",
      "Computer vision",
      "Deblurring",
      "Deep learning",
      "Discriminative model",
      "Face (sociological concept)",
      "Feature (linguistics)",
      "Feature learning",
      "Image (mathematics)",
      "Image processing",
      "Image restoration",
      "Law",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Hu",
        "given_name": "Yufan"
      },
      {
        "surname": "Xia",
        "given_name": "Junyong"
      },
      {
        "surname": "Liu",
        "given_name": "Hongmin"
      },
      {
        "surname": "Wang",
        "given_name": "Xing"
      }
    ]
  },
  {
    "title": "Federated learning for medical image analysis: A survey",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110424",
    "abstract": "Machine learning in medical imaging often faces a fundamental dilemma, namely, the small sample size problem. Many recent studies suggest using multi-domain data pooled from different acquisition sites/centers to improve statistical power. However, medical images from different sites cannot be easily shared to build large datasets for model training due to privacy protection reasons. As a promising solution, federated learning, which enables collaborative training of machine learning models based on data from different sites without cross-site data sharing, has attracted considerable attention recently. In this paper, we conduct a comprehensive survey of the recent development of federated learning methods in medical image analysis. We have systematically gathered research papers on federated learning and its applications in medical image analysis published between 2017 and 2023. Our search and compilation were conducted using databases from IEEE Xplore, ACM Digital Library, Science Direct, Springer Link, Web of Science, Google Scholar, and PubMed. In this survey, we first introduce the background of federated learning for dealing with privacy protection and collaborative learning issues. We then present a comprehensive review of recent advances in federated learning methods for medical image analysis. Specifically, existing methods are categorized based on three critical aspects of a federated learning system, including client end, server end, and communication techniques. In each category, we summarize the existing federated learning methods according to specific research problems in medical image analysis and also provide insights into the motivations of different approaches. In addition, we provide a review of existing benchmark medical imaging datasets and software platforms for current federated learning research. We also conduct an experimental study to empirically evaluate typical federated learning methods for medical image analysis. This survey can help to better understand the current research status, challenges, and potential research opportunities in this promising research field.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324001754",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Data science",
      "Image (mathematics)"
    ],
    "authors": [
      {
        "surname": "Guan",
        "given_name": "Hao"
      },
      {
        "surname": "Yap",
        "given_name": "Pew-Thian"
      },
      {
        "surname": "Bozoki",
        "given_name": "Andrea"
      },
      {
        "surname": "Liu",
        "given_name": "Mingxia"
      }
    ]
  },
  {
    "title": "Motion-guided and occlusion-aware multi-object tracking with hierarchical matching",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110369",
    "abstract": "In the field of multi-target tracking, the widely embraced tracking-by-detection paradigm has rapidly progressed with the refinement of detectors and matching techniques. However, the paradigm of joint detection and tracking is relatively limited, and it is difficult to model complex scenes, such as the complexities introduced by camera motion and occlusion. In this work, a hierarchical joint detection and tracking framework is proposed, namely MSPNet. From a temporal concern, a motion-guided feature aggregation module is proposed to address the complexities of multi-frame variations. From a spatial concern, an occlusion-aware head and hierarchical spatial association are proposed to handle the challenges of occlusion. Extensive experiments on MOT challenging benchmarks demonstrate that the MSPNet can effectively reduce false negatives and improve the accuracy of tracking while outperforming a wide range of existing methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324001201",
    "keywords": [
      "Artificial intelligence",
      "Cardiology",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Field (mathematics)",
      "Frame (networking)",
      "Linguistics",
      "Matching (statistics)",
      "Mathematics",
      "Medicine",
      "Motion (physics)",
      "Object (grammar)",
      "Object detection",
      "Occlusion",
      "Pattern recognition (psychology)",
      "Pedagogy",
      "Philosophy",
      "Psychology",
      "Pure mathematics",
      "Statistics",
      "Telecommunications",
      "Tracking (education)",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Zheng",
        "given_name": "Yujin"
      },
      {
        "surname": "Qi",
        "given_name": "Hang"
      },
      {
        "surname": "Li",
        "given_name": "Lei"
      },
      {
        "surname": "Li",
        "given_name": "Shan"
      },
      {
        "surname": "Huang",
        "given_name": "Yan"
      },
      {
        "surname": "He",
        "given_name": "Chu"
      },
      {
        "surname": "Wang",
        "given_name": "Dingwen"
      }
    ]
  },
  {
    "title": "Object detection in drone video based on recurrent motion attention",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.04.029",
    "abstract": "In recent years, object detection in drone videos has garnered widespread attention. Conventional image-based detectors have encountered a performance bottleneck for drone images due to the severe appearance deterioration of targets and numerous background clutters. Video object detection (VOD) is a promising paradigm for such problems as it can effectively enhance target appearance by aggregating features from nearby frames in drone videos. However, struggling with the feature alignment problem which is particularly important for drone images where the background varies frequently. Following the VOD philosophy, we propose the Recurrent Motion Attention Network (RMA-Net) to extract spatial-temporal image features for object detection in drone vision. A recurrent fashion is designed to effectively and efficiently learn motion features in long sequences, only requiring a few frames of optical flows for initialization. Experiments demonstrate that the proposed method could easily promote existing object detectors to achieve high performance on drone videos. Specifically, our RMA adapted YOLOv7 outperforms the state-of-the-arts by a large margin on the challenging VisDrone2019-VID dataset, not only boosts the mean average precision by 6.99 % but also reaches a batch-1 inference speed of 31 frames per second.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524001375",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Bottleneck",
      "Computer science",
      "Computer vision",
      "Drone",
      "Embedded system",
      "Feature (linguistics)",
      "Genetics",
      "Initialization",
      "Linguistics",
      "Machine learning",
      "Margin (machine learning)",
      "Motion (physics)",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Zihao"
      },
      {
        "surname": "Yu",
        "given_name": "Xianguo"
      },
      {
        "surname": "Wang",
        "given_name": "Xiangke"
      }
    ]
  },
  {
    "title": "Bibimbap : Pre-trained models ensemble for Domain Generalization",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110391",
    "abstract": "This paper addresses a machine learning problem often challenged by differences in the distributions of training and real-world data. We propose a framework that addresses the problem of underfitting in the ensembling method using pre-trained models and improves the performance and robustness of deep learning models through ensemble diversity. For the naive weight ensembling framework, we discovered that the ensembled models could not lie in the same loss basin under extreme domain shift conditions, suggesting that a loss barrier may exist. We used a fine-tuning step after the weighted ensemble to address the underfitting problem caused by the loss barrier and stabilize the batch normalization running parameters. We also inferred through qualitative analysis that the diversity of ensemble models affects domain generalization. We validate our method on a large-scale image dataset (ImageNet-1K) and chemical molecule data, which is suitable for testing with domain shift problems due to its data-splitting method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324001420",
    "keywords": [
      "Anthropology",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Domain (mathematical analysis)",
      "Ensemble forecasting",
      "Ensemble learning",
      "Gene",
      "Generalization",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Normalization (sociology)",
      "Pattern recognition (psychology)",
      "Robustness (evolution)",
      "Sociology",
      "Training set"
    ],
    "authors": [
      {
        "surname": "Kang",
        "given_name": "Jinho"
      },
      {
        "surname": "Kim",
        "given_name": "Taero"
      },
      {
        "surname": "Kim",
        "given_name": "Yewon"
      },
      {
        "surname": "Oh",
        "given_name": "Changdae"
      },
      {
        "surname": "Jung",
        "given_name": "Jiyoung"
      },
      {
        "surname": "Chang",
        "given_name": "Rakwoo"
      },
      {
        "surname": "Song",
        "given_name": "Kyungwoo"
      }
    ]
  },
  {
    "title": "Structure-aware neural radiance fields without posed camera",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110419",
    "abstract": "The neural radiance fields (NeRF) for realistic novel view synthesis require camera poses to be pre-acquired by a structure-from-motion (SfM) approach. This two-stage strategy is not convenient to use and degrades the performance because the error in the pose extraction can propagate to the view synthesis. We integrate pose extraction and view synthesis into a jointly optimized process so that they can benefit from each other. For network training, only images are given without pre-known camera poses. The camera poses are obtained by the depth-consistent constraint in which the identical feature in different views has the same world coordinates transformed from the local camera coordinates according to the extracted poses. The depth-consistent constraint is jointly optimized with the pixel color constraint. The poses are represented by a CNN-based deep network, whose input is the related frames. This joint optimization enables NeRF to be aware of the scene’s structure, resulting in improved generalization performance. Experiments on three datasets demonstrate the effectiveness of camera pose estimation and novel view synthesis. Code is available at https://github.com/XTU-PR-LAB/SaNerf.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324001705",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer vision",
      "Geology",
      "Radiance",
      "Remote sensing"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Shu"
      },
      {
        "surname": "Zhang",
        "given_name": "Yang"
      },
      {
        "surname": "Xu",
        "given_name": "Yaxin"
      },
      {
        "surname": "Zou",
        "given_name": "Beiji"
      }
    ]
  },
  {
    "title": "Improving radial lens distortion correction with multi-task learning",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.05.008",
    "abstract": "With computer vision and machine learning, sports image analysis can enhance viewer engagement and contribute to the overall understanding of sports events. However, beneath the apparent simplicity of sports images lies a complex challenge of radial distortion, which can impede their accurate interpretation. The need for high precision and real-time performance is paramount. We present a new regression-based method for radial distortion correction that improves the accuracy of distortion model coefficient prediction by introducing a secondary learning task that compares the distortion level of two random training samples. The secondary task requires no additional annotation, and because it is also related to radial distortion, it encourages the common feature extractor component to learn more general and robust features while preventing overfitting and improving the efficiency of training data. We have evaluated our proposed method using two public datasets and compared it against five other methods. Our method surpassed them all, both in the accuracy of the radial distortion correction and speed as well. You can find the source code and trained models at https://vgg.fiit.stuba.sk/improving-radial.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524001521",
    "keywords": [
      "Algorithm",
      "Amplifier",
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Computer science",
      "Computer vision",
      "Distortion (music)",
      "Engineering",
      "Lens (geology)",
      "Optics",
      "Physics",
      "Systems engineering",
      "Task (project management)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Janos",
        "given_name": "Igor"
      },
      {
        "surname": "Benesova",
        "given_name": "Wanda"
      }
    ]
  },
  {
    "title": "Efficient high-resolution template matching with vector quantized nearest neighbour fields",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110386",
    "abstract": "Template matching is a fundamental problem in computer vision with applications in fields including object detection, image registration, and object tracking. Current methods rely on nearest-neighbour (NN) matching, where the query feature space is converted to NN space by representing each query pixel with its NN in the template. NN-based methods have been shown to perform better in occlusions, appearance changes, and non-rigid transformations; however, they scale poorly with high-resolution data and high feature dimensions. We present an NN-based method that efficiently reduces the NN computations and introduces filtering in the NN fields (NNFs). A vector quantization step is introduced before the NN calculation to represent the template with k features, and the filter response over the NNFs is used to compare the template and query distributions over the features. We show that state-of-the-art performance is achieved in low-resolution data, and our method outperforms previous methods at higher resolution.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324001377",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Geometry",
      "Image (mathematics)",
      "Matching (statistics)",
      "Mathematics",
      "Nearest neighbour",
      "Pattern matching",
      "Pattern recognition (psychology)",
      "Statistics",
      "Template matching",
      "Vector field",
      "Vector quantization",
      "k-nearest neighbors algorithm"
    ],
    "authors": [
      {
        "surname": "Gupta",
        "given_name": "Ankit"
      },
      {
        "surname": "Sintorn",
        "given_name": "Ida-Maria"
      }
    ]
  },
  {
    "title": "Neural Knitworks: Patched neural implicit representation networks",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110378",
    "abstract": "Optimizing images as output of a neural network has been shown to introduce a powerful prior for image inverse tasks, capable of producing solutions of reasonable quality in a fully internal learning context, where no external datasets are involved. Two potential technical approaches involve fitting a coordinate-based Multilayer Perceptron (MLP), or a Convolutional Neural Network to produce the result image as output. The aim of this work is to evaluate the two counterparts, as well as a new framework proposed here, named Neural Knitwork, which maps pixel coordinates to local texture patches rather than singular pixel values. The utility of the proposed technique is demonstrated on the tasks of image inpainting, super-resolution, and denoising. It is shown that the Neural Knitwork can outperform the standard coordinate-based MLP baseline for the tasks of inpainting and denoising, and perform comparably for the super-resolution task.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324001298",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Law",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Czerkawski",
        "given_name": "Mikolaj"
      },
      {
        "surname": "Cardona",
        "given_name": "Javier"
      },
      {
        "surname": "Atkinson",
        "given_name": "Robert"
      },
      {
        "surname": "Michie",
        "given_name": "Craig"
      },
      {
        "surname": "Andonovic",
        "given_name": "Ivan"
      },
      {
        "surname": "Clemente",
        "given_name": "Carmine"
      },
      {
        "surname": "Tachtatzis",
        "given_name": "Christos"
      }
    ]
  },
  {
    "title": "Dog identification based on textural features and spatial relation of noseprint",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110353",
    "abstract": "This study proposes dog identification technology based on dog noseprints, which are equivalent to human fingerprints and possess unique characteristics. The aim is to utilize this technology for identifying and managing stray animals. The study presents three processing stages. In the first stage, YOLOv3 detects the dog's nose and nostril regions. The second stage involves enhancing the image's contrast and the contour of the scaly block using the multi-scale line detector. Finally, in the third stage, the shape and spatial features of the scaly block are extracted and utilized for dog identification. The study included a dataset of 276 dogs from multiple animal families and public shelters in Taiwan. The dataset was randomly divided into three groups to determine the optimal parameters for matching the dog noseprint via experimentation. Each dog identification group achieved an accuracy (ACC) exceeding 97.83 %, demonstrating that the proposed parameter-matching method offers high stability. Furthermore, in an additional experimental dataset consisting of dog noseprint images used for dog identification, the proposed method achieved an ACC exceeding 90.22 % in the Top 1 and 94.57 % in the Top 3. The ACC results across different groups consistently demonstrate the proposed method's ability to achieve high accuracy in dog identification. The source code and trained models are publicly available at: https://github.com/Chuen-HorngLin/Dog-Identification-Noseprint.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324001043",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Block (permutation group theory)",
      "Botany",
      "Computer science",
      "Computer vision",
      "Geometry",
      "Identification (biology)",
      "Matching (statistics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Chan",
        "given_name": "Yung-Kuan"
      },
      {
        "surname": "Lin",
        "given_name": "Chuen-Horng"
      },
      {
        "surname": "Wang",
        "given_name": "Ching-Lin"
      },
      {
        "surname": "Tu",
        "given_name": "Keng-Chang"
      },
      {
        "surname": "Yang",
        "given_name": "Shu-Chun"
      },
      {
        "surname": "Tsai",
        "given_name": "Meng-Hsiun"
      },
      {
        "surname": "Yu",
        "given_name": "Shyr-Shen"
      }
    ]
  },
  {
    "title": "Multi-task hierarchical convolutional network for visual-semantic cross-modal retrieval",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110398",
    "abstract": "Bridging visual and textual representations plays a central role in delving into multimedia data understanding. The main challenge arises from that images and texts exist in heterogeneous spaces, leading to the difficulty to preserve the semantic consistency between both modalities. To narrow the modality gap, most recent methods resort to extra object detectors or parsers to obtain the hierarchical representations. In this work, we address this problem by introducing our Multi-Task Hierarchical Convolutional Neural Network (MT-HCN). It is characterized by mining the hierarchical semantic information without the aid of any extra supervisions. Firstly, from the perspective of representing architecture, we leverage the intrinsic hierarchical structure of Convolutional Neural Networks (CNNs) to decompose the representations of both modalities into two semantically complementary levels, i.e., exterior representations and concept representations. The former focuses on discovering the fine-grained low-level associations between both modalities, meanwhile the latter underlines capturing more high-level abstract semantics. Specifically, we present a Self-Supervised Clustering (SSC) loss to preserve more fine-grained semantic clues in exterior representations. It is constituted on the basis of viewing multiple image/text pairs with similar exterior as a category. In addition, a novel harmonious bidirectional triplet ranking (HBTR) loss is proposed, which mitigate the adverse effects brought about by the biased and noisy negative samples. Besides hardest negatives, it also imposes the constraints on the distance between the positive pairs and the centroid of negative pairs. Extensive experimental results on two popular cross-modal retrieval benchmarks demonstrate our proposed MT-HCN can achieve the competitive results compared with the state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324001493",
    "keywords": [
      "Artificial intelligence",
      "Bridging (networking)",
      "Cluster analysis",
      "Computer network",
      "Computer science",
      "Convolutional neural network",
      "Hierarchical clustering",
      "Image (mathematics)",
      "Image retrieval",
      "Information retrieval",
      "Leverage (statistics)",
      "Modalities",
      "Natural language processing",
      "Parsing",
      "Pattern recognition (psychology)",
      "Programming language",
      "Semantic gap",
      "Semantics (computer science)",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Ji",
        "given_name": "Zhong"
      },
      {
        "surname": "Lin",
        "given_name": "Zhigang"
      },
      {
        "surname": "Wang",
        "given_name": "Haoran"
      },
      {
        "surname": "Pang",
        "given_name": "Yanwei"
      },
      {
        "surname": "Li",
        "given_name": "Xuelong"
      }
    ]
  },
  {
    "title": "ISP-IRLNet: Joint optimization of interpretable sampler and implicit regularization learning network for accelerated MRI",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110412",
    "abstract": "Compressed Sensing Magnetic Resonance Imaging (CS-MRI) was proposed to accelerate data acquisition and reconstruct MR images from under-sampled data in k -space. However, the traditional approaches design the sampling patterns separately from the reconstruction process, which often leads to suboptimal reconstruction performance. To address this issue, we propose a joint optimization model dubbed as ISP-IRLNet to yield optimal recovery including an interpretable sampler and a reconstructor using the implicit regularization learning network. In particular, we introduce a probabilistic distribution model of the sampling incorporated into the prior of k -space locations, which provides a robust CS-MRI sample pattern with an explicit expression. To estimate the approximating gradient in backward propagation for more stable training, we employ a differentiable binarization strategy. In addition, we unroll the ADMM optimization for solving the regularized reconstruction model to be a deep network (i.e., IRLNet) with a learnable implicit regularization sub-network to learn a regularizer or prior term implicitly. The experiments on our collected brain and public knee datasets demonstrate that our method provides an optimal sample pattern and achieves superior image reconstruction performance compared with existing methods, especially at high acceleration rates.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324001638",
    "keywords": [
      "Architectural engineering",
      "Artificial intelligence",
      "Computer science",
      "Engineering",
      "Joint (building)",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Regularization (linguistics)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Xing"
      },
      {
        "surname": "Yang",
        "given_name": "Yan"
      },
      {
        "surname": "Zheng",
        "given_name": "Hairong"
      },
      {
        "surname": "Xu",
        "given_name": "Zongben"
      }
    ]
  },
  {
    "title": "Adaptive knowledge transfer for class incremental learning",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.05.011",
    "abstract": "Humans are excellent at adapting to constantly changing circumstances, but deep neural networks have catastrophic forgetting. Recently, significant progress has been made with class-incremental methods based on dynamic network structures. However, these methods combine individual networks in a simplistic coupled manner, neglecting the fusion capabilities between modules, leading to a decline in the overall prediction performance. To address this, we propose a class-incremental learning paradigm with adaptive knowledge transfer. This paradigm leverages crucial self-learning factors to transfer the importance knowledge of old classes as much as possible, allowing each module to integrate the optimal information from the current class. Experiments demonstrate that our designed adaptive knowledge transfer module effectively reduces the sharpness of decision boundaries, thereby significantly improving the final accuracy. Additionally, we have devised a compression module with supplementary learning to mitigate errors arising from long-term session sequences during model fusion. Extensive experiments conducted on benchmarks, Our approach achieved an average accuracy exceeding 2.72%, 1.30% in the CIFAR-100 and ImageNet-100/1000 achieves SOTA performance in both ordinary and challenging class-incremental settings.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524001557",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Machine learning",
      "Mathematics",
      "Parallel computing",
      "Transfer (computing)",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Feng",
        "given_name": "Zhikun"
      },
      {
        "surname": "Zhou",
        "given_name": "Mian"
      },
      {
        "surname": "Gao",
        "given_name": "Zan"
      },
      {
        "surname": "Stefanidis",
        "given_name": "Angelos"
      },
      {
        "surname": "Su",
        "given_name": "Jionglong"
      },
      {
        "surname": "Dang",
        "given_name": "Kang"
      },
      {
        "surname": "Li",
        "given_name": "Chuanhui"
      }
    ]
  },
  {
    "title": "Learning De-biased prototypes for Few-shot Medical Image Segmentation",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.05.003",
    "abstract": "Prototypical networks have emerged as the dominant method for Few-shot Medical image Segmentation (FSMIS). Despite their success, the commonly used Masked Average Pooling (MAP) approach in prototypical networks computes the mean of the masks, resulting in imprecise and inadequate prototypes that fail to capture the subtle nuances and variations in the data. To address this issue, we propose a simple yet effective module called De-biasing Masked Average Pooling (DMAP) to generate more accurate prototypes from filtered foreground support features. Specifically, our approach introduces a Learnable Threshold Generation (LTG) module that adaptively learns a threshold based on the extracted features from both support and query images, and then choose partial foreground pixels that have larger similarity than the threshold to generate prototypes. Our proposed method is evaluated on three popular medical image segmentation datasets, and the results demonstrate the superiority of our approach over some state-of-the-art methods. Code is available at https://github.com/YazhouZhu19/DMAP.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524001417",
    "keywords": [
      "Artificial intelligence",
      "Code (set theory)",
      "Computer science",
      "Computer vision",
      "Image (mathematics)",
      "Image segmentation",
      "Pattern recognition (psychology)",
      "Pixel",
      "Pooling",
      "Programming language",
      "Segmentation",
      "Set (abstract data type)",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Zhu",
        "given_name": "Yazhou"
      },
      {
        "surname": "Cheng",
        "given_name": "Ziming"
      },
      {
        "surname": "Wang",
        "given_name": "Shidong"
      },
      {
        "surname": "Zhang",
        "given_name": "Haofeng"
      }
    ]
  },
  {
    "title": "HAIC-NET: Semi-supervised OCTA vessel segmentation with self-supervised pretext task and dual consistency training",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110429",
    "abstract": "Optical Coherence Tomography Angiography(OCTA) vessel segmentation is a challenging task. On the one hand, the complex structure of the capillary networks presents significant obstacles to achieving accurate vessel segmentation. On the other hand, current research on OCTA vessel segmentation heavily relies on high-quality manual annotations, especially in fully-supervised approaches. In contrast, pixel-level annotation of OCTA vessels is time-consuming and labor-intensive. To address these issues, we propose a semi-supervised method called HAIC-Net, which integrates self-supervised learning with a homologous augmented image classification pretext task and dual consistency training with data perturbation consistency and topological connectivity consistency. Firstly, we design a self-supervised homologous augmented image classification pretext task that directs the model’s attention to similar vascular features in homologous augmented images, thereby extracting rich vessel information from unlabeled images and reducing the dependence on manual annotations. Secondly, we introduce a dual-consistency structure with topological connectivity consistency to provide constraint from a topological perspective, which is consistent with the topological characteristics of the vascular network, to enhance the segmentation network’s sensitivity to vessel connectivity and decrease the topological errors in segmentation results. We conduct experiments on two publicly available datasets and one private dataset and validate the state-of-the-art performance of the proposed method. On the ROSE-1 dataset, our method achieves 0.9143 accuracy and 0.7658 dice coefficient, surpassing other current semi-supervised methods and approaching the performance of state-of-the-art fully supervised methods. The same result can also be observed on OCTA500 and our private dataset, demonstrating the effectiveness and superiority of our approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324001808",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Co-training",
      "Computer science",
      "Consistency (knowledge bases)",
      "Dual (grammatical number)",
      "Engineering",
      "Geography",
      "Geometry",
      "Law",
      "Literature",
      "Machine learning",
      "Mathematics",
      "Meteorology",
      "Net (polyhedron)",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Pretext",
      "Segmentation",
      "Semi-supervised learning",
      "Systems engineering",
      "Task (project management)",
      "Training (meteorology)"
    ],
    "authors": [
      {
        "surname": "Shen",
        "given_name": "Hailan"
      },
      {
        "surname": "Tang",
        "given_name": "Zheng"
      },
      {
        "surname": "Li",
        "given_name": "Yajing"
      },
      {
        "surname": "Duan",
        "given_name": "Xuanchu"
      },
      {
        "surname": "Chen",
        "given_name": "Zailiang"
      }
    ]
  },
  {
    "title": "A thorough experimental comparison of multilabel methods for classification performance",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110342",
    "abstract": "Multilabel classification as a data mining task has recently attracted increasing interest from researchers. Many current data mining applications address problems with instances that belong to more than one class. These problems require the development of new, efficient methods. Advantageously using the correlation among different labels can provide better performance than methods that manage each label separately. In recent decades, many methods have been developed to deal with multilabel datasets, which makes it difficult to decide which method is the most appropriate for a given task. In this paper, we present the most comprehensive comparison carried out so far. We compare a total of 62 different methods and several configurations of each one for a total of 197 trained models. We also use a large set of problems comprising 65 datasets. In addition, we studied the efficiency of the methods considering six different classification performance metrics. Our results show that, although there are methods that repeatedly appear among the top-performing models, the best methods are closely related to the metric used for evaluating the performance. We also analyzed different aspects of the behavior of the methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324000931",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Data mining",
      "Economics",
      "Machine learning",
      "Management",
      "Metric (unit)",
      "Operations management",
      "Programming language",
      "Set (abstract data type)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "García-Pedrajas",
        "given_name": "Nicolás E."
      },
      {
        "surname": "Cuevas-Muñoz",
        "given_name": "José M."
      },
      {
        "surname": "Cerruela-García",
        "given_name": "Gonzalo"
      },
      {
        "surname": "de Haro-García",
        "given_name": "Aida"
      }
    ]
  },
  {
    "title": "Structural self-similarity pattern in global food prices: Utilizing a segmented multifractal detrended fluctuation analysis",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.06.002",
    "abstract": "This paper provides a comprehensive analysis of the structural self-similarity observed in global food prices, focusing specifically on key commodities such as olive oil, eggs, bread, chicken, and beef. Employing Segmented Multifractal Detrended Fluctuation Analysis (SMF-DFA), we investigate the multifractal intricacies within the price dynamics of these essential food items. SMF-DFA facilitates a detailed examination of piecewise self-similarity, delineating segments by change-points and offering a nuanced understanding of the complex structures inherent in global market prices. Furthermore, our proposal incorporates Levene’s test to examine whether the volatility differs significantly among the segments separated by change-points, thereby enhancing the robustness of this analytical stage. This study surpasses conventional methods, providing valuable insights into the multifractal characteristics of food prices across various scales. These findings contribute to a deeper comprehension of the intricate patterns governing global food prices, crucial for decision-making in agricultural economics, financial markets, and the dynamics of global trade.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524001661",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Detrended correspondence analysis",
      "Detrended fluctuation analysis",
      "Econometrics",
      "Fractal",
      "Geometry",
      "Image (mathematics)",
      "Mathematical analysis",
      "Mathematics",
      "Multifractal system",
      "Ordination",
      "Pattern recognition (psychology)",
      "Scaling",
      "Self-similarity",
      "Similarity (geometry)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Saâdaoui",
        "given_name": "Foued"
      }
    ]
  },
  {
    "title": "Patch-wise vector quantization for unsupervised medical anomaly detection",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.06.028",
    "abstract": "Radiography images inherently possess globally consistent structures while exhibiting significant diversity in local anatomical regions, making it challenging to model their normal features through unsupervised anomaly detection. Since unsupervised anomaly detection methods localize anomalies by utilizing discrepancies between learned normal features and input abnormal features, previous studies introduce a memory structure to capture the normal features of radiography images. However, these approaches store extremely localized image segments in their memory, causing the model to represent both normal and pathological features with the stored components. This poses a significant challenge in unsupervised anomaly detection by reducing the disparity between learned features and abnormal features. Furthermore, with the diverse settings in radiography imaging, the above issue is exacerbated: more diversity in the normal images results in stronger representation of pathological features. To resolve the issues above, we propose a novel pathology detection method called Patch-wise Vector Quantization (P-VQ). Unlike the previous methods, P-VQ learns vector-quantized representations of normal “patches” while preserving its spatial information by incorporating vector similarity metric. Furthermore, we introduce a novel method for selecting features in the memory to further enhance the robustness against diverse imaging settings. P-VQ even mitigates the “index collapse” problem of vector quantization by proposing top- k % dropout. Our extensive experiments on the BMAD benchmark demonstrate the superior performance of P-VQ against existing state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552400196X",
    "keywords": [
      "Algorithm",
      "Anomaly detection",
      "Artificial intelligence",
      "Computer science",
      "Learning vector quantization",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Quantization (signal processing)",
      "Vector quantization"
    ],
    "authors": [
      {
        "surname": "Kim",
        "given_name": "Taejune"
      },
      {
        "surname": "Lee",
        "given_name": "Yun-Gyoo"
      },
      {
        "surname": "Jeong",
        "given_name": "Inho"
      },
      {
        "surname": "Ham",
        "given_name": "Soo-Youn"
      },
      {
        "surname": "Woo",
        "given_name": "Simon S."
      }
    ]
  },
  {
    "title": "MTS2Graph: Interpretable multivariate time series classification with temporal evolving graphs",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110486",
    "abstract": "Conventional time series classification approaches based on bags of patterns or shapelets face significant challenges in dealing with a vast amount of feature candidates from high-dimensional multivariate data. In contrast, deep neural networks can learn low-dimensional features efficiently, and in particular, convolutional neural networks have shown promising results in classifying multivariate time series data. A key factor in the success of deep neural networks is this astonishing expressive power. However, this power comes at the cost of complex, black-boxed models, conflicting with the goals of building reliable and human-understandable models. In this work 1 1 The code can be found at https://github.com/raneeny/MTS2Graph. , we introduce a new interpretable framework for multivariate time series data that by extracting and clustering the input quantifies the contribution of time-varying input variables and each signal’s role to the classification. We construct a graph that captures the temporal relationship between the extracted patterns for each layer and propose an effective merging strategy to aggregate those graphs into one. Finally, a graph embedding algorithm generates new representations of the created interpretable time-series features. Our extensive experiments indicate the benefit of our time-aware graph-based representation in multivariate time series classification while enriching them with more interpretability.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324002371",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Geology",
      "Machine learning",
      "Multivariate statistics",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Series (stratigraphy)",
      "Time series"
    ],
    "authors": [
      {
        "surname": "Younis",
        "given_name": "Raneen"
      },
      {
        "surname": "Hakmeh",
        "given_name": "Abdul"
      },
      {
        "surname": "Ahmadi",
        "given_name": "Zahra"
      }
    ]
  },
  {
    "title": "RL-NBV: A deep reinforcement learning based next-best-view method for unknown object reconstruction",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.05.014",
    "abstract": "The Next-Best-View (NBV) algorithm is a key component in autonomous unknown object reconstruction. It iteratively determines the optimal sensor pose to capture the maximum information about the object under reconstruction. However, prevailing deep reinforcement learning (DRL) based NBV algorithms tend to transform point cloud, the raw sensor data, into different representations, thereby obscuring natural invariances of the data. In this work, we propose an innovative DRL-based method, denoted as RL-NBV, to learn NBV policy directly from the raw point cloud data. Specifically, we interpret the observation space as the current state of the reconstructed object represented by point clouds and current view selection states. Experimental results indicate that our method outperforms existing methods in terms of reconstruction performance. Moreover, our method significantly improves efficiency over ray-casting-based algorithms as time-consuming ray casting and data transformation are unnecessary.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524001582",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Machine learning",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Psychology",
      "Reinforcement",
      "Reinforcement learning",
      "Social psychology"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Tao"
      },
      {
        "surname": "Xi",
        "given_name": "Weibin"
      },
      {
        "surname": "Cheng",
        "given_name": "Yong"
      },
      {
        "surname": "Han",
        "given_name": "Hao"
      },
      {
        "surname": "Yang",
        "given_name": "Yang"
      }
    ]
  },
  {
    "title": "Classification of Childhood Obstructive Sleep Apnea based on X-ray images analysis by Quasi-conformal Geometry",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110454",
    "abstract": "Craniofacial profile is one of the anatomical causes of obstructive sleep apnea (OSA). By medical research, cephalometry provides information on patients’ skeletal structures and soft tissues. In this work, a novel approach to cephalometric analysis using quasi-conformal geometry based local deformation information was proposed for OSA classification. Our study was a retrospective analysis based on 60 case-control pairs with accessible lateral cephalometry and polysomnography (PSG) data (mean age: 8.9 ± 2.3 years). By using the quasi-conformal geometry to study the local deformation around 15 landmark points, and combining the results with three linear distances between landmark points, a total of 1218 information features were obtained per subject. A L 2 norm based classification model was built. Under experiments, our proposed model achieves high testing accuracy. The accurate classification tool provides us with a reliable and accurate screening tool for childhood OSA.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032400205X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Conformal map",
      "Geometry",
      "Internal medicine",
      "Mathematics",
      "Medicine",
      "Obstructive sleep apnea"
    ],
    "authors": [
      {
        "surname": "Chan",
        "given_name": "Hei-Long"
      },
      {
        "surname": "Yuen",
        "given_name": "Hoi-Man"
      },
      {
        "surname": "Au",
        "given_name": "Chun-Ting"
      },
      {
        "surname": "Chan",
        "given_name": "Kate Ching-Ching"
      },
      {
        "surname": "Li",
        "given_name": "Albert Martin"
      },
      {
        "surname": "Lui",
        "given_name": "Lok-Ming"
      }
    ]
  },
  {
    "title": "Student State-aware knowledge tracing based on attention mechanism: A cognitive theory view",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.06.009",
    "abstract": "Knowledge tracing evaluates students’ knowledge state and predicts future performance by analyzing their past interactions. Recent research integrates features of learning activities into knowledge tracing to enhance interpretability. Ausubel’s cognitive theory underscores the significance of cognitive accumulation in learning, perceiving it as a process in which new content is linked and integrated with students’ existing knowledge. Yet, current studies often overlook this cognitive property and its impact on performance prediction. Therefore, we propose an attention-based knowledge tracing model, named Student State-aware knowledge tracing (SSKT). To align with this cognitive process, we incorporate suitable Query, Key, and Value objects into the attention mechanism, effectively modeling how students extract, integrate, and apply information from their existing knowledge. Meanwhile, traditional RNN-based models encounter the issue of losing early learning data due to gradient vanishing in long sequences. Our hybrid model, which combines LSTM and Transformer, efficiently extracts early learning information using an attention mechanism. Extensive experiments on real-world datasets validate the effectiveness and interpretability of SSKT.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552400179X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Cognition",
      "Cognitive psychology",
      "Cognitive science",
      "Computer science",
      "Epistemology",
      "Mechanism (biology)",
      "Neuroscience",
      "Philosophy",
      "Programming language",
      "Psychology",
      "State (computer science)",
      "Tracing"
    ],
    "authors": [
      {
        "surname": "Qian",
        "given_name": "Liyin"
      },
      {
        "surname": "Zheng",
        "given_name": "Kaiwen"
      },
      {
        "surname": "Wang",
        "given_name": "Luqi"
      },
      {
        "surname": "Li",
        "given_name": "Sheng"
      }
    ]
  },
  {
    "title": "Reading QR Codes on challenging surfaces using thin-plate splines",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.06.004",
    "abstract": "In real world uses, QR Codes are printed or overlaid on top of complex surfaces, like cylindrical bottles or other daily objects with random topographies that pose big challenges to their readout with the conventional planar algorithms proposed in the standard ISO and implemented in most of the available codes. We propose here a general-purpose method to improve the readability of QR Codes placed on these challenging surfaces, by fitting the topography of the underlying arbitrary surface with thin-plate splines. Then, we compare this new method with other alternatives proposed in the literature, like affine, projective and cylindrical transformations. Results demonstrate that our new approach works well under a variety of arbitrary surface topographies including those assumed in former proposals, and improve their readability by a factor of 4, clearly outperforming state-of-the-art decoders.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524001673",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Linguistics",
      "Mathematics",
      "Philosophy",
      "Reading (process)"
    ],
    "authors": [
      {
        "surname": "Benito-Altamirano",
        "given_name": "Ismael"
      },
      {
        "surname": "Martínez-Carpena",
        "given_name": "David"
      },
      {
        "surname": "Lizarzaburu-Aguilar",
        "given_name": "Hanna"
      },
      {
        "surname": "Fàbrega",
        "given_name": "Cristian"
      },
      {
        "surname": "Prades",
        "given_name": "Joan Daniel"
      }
    ]
  },
  {
    "title": "Contrastive domain-adaptive graph selective self-training network for cross-network edge classification",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110448",
    "abstract": "The performance of graph Neural Network (GNNs) can degrade significantly when trained on the graphs with noisy edges connecting nodes from different classes. To mitigate the negative effect of noisy edges, previous studies have largely focused on predicting the label agreement between node pairs within a single network. So far, predicting noisy edges across different networks remains largely underexplored. To bridge this gap, our work studies a novel problem of cross-network homophilous and heterophilous edge classification (CNHHEC), aiming to predict the label agreement of edges in an unlabeled target network by transferring the knowledge from a labeled source network. A novel Contrastive Domain-adaptive Graph Self-training Network (CDGSN) is proposed. Firstly, CDGSN learns node and edge embeddings end-to-end by a GNN encoder with adaptive edge weights during neighborhood aggregation. Secondly, to facilitate knowledge transfer across networks, CDGSN employs an adversarial domain adaptation module to align edge embeddings across networks, and also designs a novel contrastive domain adaptation module to conduct class-aware cross-network alignment of node embeddings. As a result, the intra-class domain divergence can be mitigated while the inter-class domain discrepancy can be enlarged to yield network-invariant and label-discriminative node and edge embeddings. Moreover, CDGSN designs a selective positive and negative pseudo-labeling strategy to assign positive (negative) pseudo-labels to the target nodes with extremely high (low) prediction confidence of belonging to each specific class. Such pseudo-labeled target nodes would be employed to iteratively re-train the model in a self-training manner, so as to obtain more reliable target pseudo-labels progressively to guide class-aware domain alignment. Extensive experiments on benchmark datasets demonstrate the effectiveness of the proposed CDGSN on the CNHHEC problem. The performance of CDGSN is robust against various types of edge embeddings. When adopting three operators to construct edge embeddings, the proposed CDGSN can improve the state-of-the-art method for CNHHEC by an average of 0.5 %, 2.8 %, and 10.8 % in terms of AUC, and 1.3 %, 3.0 %, and 6.3 % in terms of AP, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324001997",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Discriminative model",
      "Domain (mathematical analysis)",
      "Encoder",
      "Engineering",
      "Enhanced Data Rates for GSM Evolution",
      "Graph",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Node (physics)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Structural engineering",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Shao",
        "given_name": "Mengqiu"
      },
      {
        "surname": "Xue",
        "given_name": "Peng"
      },
      {
        "surname": "Zhou",
        "given_name": "Xi"
      },
      {
        "surname": "Shen",
        "given_name": "Xiao"
      }
    ]
  },
  {
    "title": "Cross co-teaching for semi-supervised medical image segmentation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110426",
    "abstract": "Excellent performance has been achieved on semi-supervised medical image segmentation, but existing algorithms perform relatively poorly for objects with variable topologies and weak boundaries. In this paper, we propose a novel cross co-teaching framework, called Cross-structure-task Collaborative Teaching (CroCT), which not only can effectively handle variable topologies, but also strengthens the learning for weak boundaries of unlabeled data. Specifically, a new cross-structure-task collaborative teaching mechanism is developed based on our designed “E-Net” structure composed of a shared encoder and two decoder branches with distinct learning paradigms, which asks these two branches to regress topology-aware signed distance functions and densely-predicted segmentation masks for each other. Powered by the collaboration across different structural biases and sequence-related tasks, our CroCT can extract more discriminative yet complementary representations from abundant raw medical data to promote the consistency learning generalization, further boosting the performance for tackling highly diverse shapes and topological changes intra-/inter-slices. Besides, it guarantees the diversities from multi-levels, i.e., structure and task perspectives, to preclude prediction uncertainty. In addition, a novel adaptive boundary enhancing (ABE) module is proposed to introduce compact annularly enhanced boundary features into semi-supervised training, which significantly improves weak boundary perception ability for unlabeled data while facilitating collaborative teaching for efficiently propagating complementary knowledge across different branches. The extensive experiments on three challenging medical benchmarks, employing different labeled settings, demonstrate the superiority of our CroCT over recent state-of-the-art competitors.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324001778",
    "keywords": [
      "Artificial intelligence",
      "Boosting (machine learning)",
      "Computer science",
      "Discriminative model",
      "Economics",
      "Image segmentation",
      "Machine learning",
      "Management",
      "Network topology",
      "Operating system",
      "Pattern recognition (psychology)",
      "Segmentation",
      "Semi-supervised learning",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Fan"
      },
      {
        "surname": "Liu",
        "given_name": "Huiying"
      },
      {
        "surname": "Wang",
        "given_name": "Jinjiang"
      },
      {
        "surname": "Lyu",
        "given_name": "Jun"
      },
      {
        "surname": "Cai",
        "given_name": "Qing"
      },
      {
        "surname": "Li",
        "given_name": "Huafeng"
      },
      {
        "surname": "Dong",
        "given_name": "Junyu"
      },
      {
        "surname": "Zhang",
        "given_name": "David"
      }
    ]
  },
  {
    "title": "Robust multiple subspaces transfer for heterogeneous domain adaptation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110473",
    "abstract": "Heterogeneous domain adaptation (HDA) aims to execute knowledge transfer from a source domain to a heterogeneous target domain. Previous works typically inject knowledge from the source and target domain into a common subspace. However, this may lead to the ineffectiveness of knowledge transfer due to the existence of heterogeneity. To overcome this drawback, in this paper, we propose a robust multiple subspaces transfer method for heterogeneous domain adaptation. Specifically, knowledge of two domains is projected into a union of multiple subspaces via a self-expressive model, in which joint distribution alignment and dynamic Laplacian regularization on self-repressive coefficients are included in the loss for characterizing transferability. Moreover, we provide a comprehensive analysis of stability, complexity, generalization, and convergence guarantee for the proposed method. Experiments on benchmark vision and Language datasets verify effectiveness of the proposed approach for heterogeneous domain adaptation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324002243",
    "keywords": [
      "Adaptation (eye)",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Classifier (UML)",
      "Computer science",
      "Convergence (economics)",
      "Domain (mathematical analysis)",
      "Domain adaptation",
      "Economic growth",
      "Economics",
      "Generalization",
      "Geodesy",
      "Geography",
      "Geometry",
      "Knowledge management",
      "Knowledge transfer",
      "Linear subspace",
      "Mathematical analysis",
      "Mathematics",
      "Optics",
      "Physics",
      "Regularization (linguistics)",
      "Subspace topology",
      "Theoretical computer science",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Youfa"
      },
      {
        "surname": "Du",
        "given_name": "Bo"
      },
      {
        "surname": "Chen",
        "given_name": "Yongyong"
      },
      {
        "surname": "Zhang",
        "given_name": "Lefei"
      }
    ]
  },
  {
    "title": "Graph node matching for edit distance",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.05.020",
    "abstract": "Graphs are commonly used to model interactions between elements of a set, but computing the Graph Edit Distance between two graphs is an NP-complete problem that is particularly challenging for large graphs. To address this problem, we propose a supervised metric learning approach that combines Graph Neural Networks and optimal transport to learn an approximation of the GED in an end-to-end fashion. Our model consists of two siamese GNNs and a comparison block. Each graph pair’s nodes are augmented by positional encoding and embedded by multiple Graph Isomorphism Network layers. The obtained embeddings are then compared through a Multi-Layer Perceptron and Linear Sum Assignment Problem solver applied on a node-wise Euclidean metric defined in the embedding space. We show that our approach achieves state-of-the-art results on benchmark datasets and outperforms other similar works in the domain. Our approach also provides explainability through the extraction of an edit path from one graph to another and guarantees metric properties conservation during training and inference.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524001648",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Edit distance",
      "Embedding",
      "Graph",
      "Graph embedding",
      "Mathematics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Moscatelli",
        "given_name": "Aldo"
      },
      {
        "surname": "Piquenot",
        "given_name": "Jason"
      },
      {
        "surname": "Bérar",
        "given_name": "Maxime"
      },
      {
        "surname": "Héroux",
        "given_name": "Pierre"
      },
      {
        "surname": "Adam",
        "given_name": "Sébastien"
      }
    ]
  },
  {
    "title": "FDM: Document image seen-through removal via Fuzzy Diffusion Models",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.06.015",
    "abstract": "While scanning or shooting a document, factors like ink density and paper transparency may cause the content from the reverse side to become visible through the paper, resulting in a digital image with a ‘seen-through’ phenomenon, which will affect practical applications. In addition, document images can be affected by random factors during the imaging process, such as differences in the performance of camera equipment and variations in the physical properties of the document itself. These random factors increase the noise of the document image and may cause the seen-through phenomena to become more complex and diverse. To tackle this issue, we propose the Fuzzy Diffusion Model (FDM), which combines fuzzy logic with diffusion models. It effectively models complex seen-through effects and handles uncertainties in document images. Specifically, we gradually degrade the original image with mean-reverting stochastic differential equation(SDE) to transform it into seen-through mean state with fixed Gaussian noise version. Following this, fuzzy operations are introduced into the noise network. Which helps the model better learn noise and data distributions by reasoning about the affiliation relationship of each pixel point through fuzzy logic. Eventually, in the reverse process, the low-quality image is gradually restored by simulating the corresponding reverse-time SDE. Extensive quantitative and qualitative experiments conducted on various datasets demonstrate that the proposed method significantly removes the seen-through effects and achieves good results under several metrics. The proposed FDM effectively solves the seen-through effects of document images and obtains better visual quality.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524001855",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Diffusion",
      "Fuzzy logic",
      "Image (mathematics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Thermodynamics"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Yijie"
      },
      {
        "surname": "Xu",
        "given_name": "Jindong"
      },
      {
        "surname": "Liang",
        "given_name": "Zongbao"
      },
      {
        "surname": "Chong",
        "given_name": "Qianpeng"
      },
      {
        "surname": "Cheng",
        "given_name": "Xiang"
      }
    ]
  },
  {
    "title": "Label enhancement via manifold approximation and projection with graph convolutional network",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110447",
    "abstract": "Label enhancement (LE) aims to enrich logical labels into their corresponding label distributions. But existing LE algorithms fail to fully leverage the structural information in the feature space to improve LE learning. To address this key issue, we first apply manifold learning to map the relatedness between low-dimensional feature samples to the label space. Based on the smoothness assumption of manifolds, the implicit correlation between low-dimensional feature and label spaces effectively promotes the LE process, enabling the learning model to accurately capture the mapping relationship between feature and label manifolds. This leads to an LE based on feature representation (LEFR) algorithm. We also propose an LE algorithm based on graph convolutional network (GCN), called LE-GCN. Inspired by the relationship between threshold connections and label connections, we extend GCN to the LE field for the first time to fully exploit the hidden relationships between nodes and labels. By enhancing node information with threshold connections and label connections, the label learning accuracy reaches a new level. Experiments on real-world datasets show that our LEFR and LE-GCN outperform several state-of-the-art LE algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324001985",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Dimensionality reduction",
      "Engineering",
      "Graph",
      "Manifold (fluid mechanics)",
      "Manifold alignment",
      "Mathematics",
      "Mechanical engineering",
      "Nonlinear dimensionality reduction",
      "Pattern recognition (psychology)",
      "Projection (relational algebra)",
      "Topology (electrical circuits)"
    ],
    "authors": [
      {
        "surname": "Tan",
        "given_name": "Chao"
      },
      {
        "surname": "Chen",
        "given_name": "Sheng"
      },
      {
        "surname": "Geng",
        "given_name": "Xin"
      },
      {
        "surname": "Zhou",
        "given_name": "Yunyao"
      },
      {
        "surname": "Ji",
        "given_name": "Genlin"
      }
    ]
  },
  {
    "title": "Automatic calculation of step size and inertia parameter for convolutional dictionary learning",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110443",
    "abstract": "The convergent methods available for convolutional dictionary learning (CDL) are the proximal gradient method (PGM) and the inertial proximal gradient method (IPGM). However, it is not trivial and heuristic for IPGM to set the step size and inertia parameter, and IPGM produces local minima and oscillating solutions. To address these issues, in this paper, we introduce a dry friction, which has an oscillation-alleviating property. Specifically, the proposed IPGM with dry friction (IPGM-DF) generates the composite proximal mappings, whose construction and optimization solver are two challenges. An auxiliary function is designed to construct the composite proximal mappings, whose optimization problem is solved by the alternating direction method of multipliers. Fortunately, IPGM-DF obtains the formulas of the step size and inertia parameter. The finite convergence of IPGM-DF is proved. Experimental results of image reconstruction, separation, and fusion demonstrate the superiority of IPGM-DF over IPGM and the state-of-the-art methods. For image reconstruction, the objective function value of IPGM-DF is reduced by about 38.708% than that of IPGM. Throughout alleviating oscillations, IPGM-DF obtains a much lower value of the objective function than IPGM, which indicates that IPGM-DF jumps out of the local minima of IPGM. In addition, the average PSNR of IPGM-DF is about 3.5 dB larger than that of IPGM and the state-of-the-art methods. The code is available.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324001948",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Classical mechanics",
      "Computer science",
      "Convolutional neural network",
      "Dictionary learning",
      "Inertia",
      "Pattern recognition (psychology)",
      "Physics",
      "Sparse approximation",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Jinjia"
      },
      {
        "surname": "Li",
        "given_name": "Pengyu"
      },
      {
        "surname": "Zhang",
        "given_name": "Yali"
      },
      {
        "surname": "Li",
        "given_name": "Ze"
      },
      {
        "surname": "Xu",
        "given_name": "Jingchen"
      },
      {
        "surname": "Wang",
        "given_name": "Qian"
      },
      {
        "surname": "Li",
        "given_name": "Jing"
      }
    ]
  },
  {
    "title": "Ensemble clustering via synchronized relabelling",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.06.026",
    "abstract": "Ensemble clustering is an important problem in unsupervised learning that aims at aggregating multiple noisy partitions into a unique clustering solution. It can be formulated in terms of relabelling and voting, where relabelling refers to the task of finding optimal permutations that bring coherence among labels in input partitions. In this paper we propose a novel solution to the relabelling problem based on permutation synchronization. By effectively circumventing the need for a reference clustering, our method achieves superior performance than previous work under varying assumptions and scenarios, demonstrating its capability to handle diverse and complex datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524001971",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Alziati",
        "given_name": "Michele"
      },
      {
        "surname": "Amarù",
        "given_name": "Fiore"
      },
      {
        "surname": "Magri",
        "given_name": "Luca"
      },
      {
        "surname": "Arrigoni",
        "given_name": "Federica"
      }
    ]
  },
  {
    "title": "Semi-supervised pathological image segmentation via cross distillation of multiple attentions and Seg-CAM consistency",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110492",
    "abstract": "Segmentation of pathological images is a crucial step for accurate cancer diagnosis. However, acquiring dense annotations of such images for training is labor-intensive and time-consuming. To address this issue, Semi-Supervised Learning (SSL) has the potential for reducing the annotation cost, but it is challenged by a large number of unlabeled training images. In this paper, we propose a novel SSL method based on Cross Distillation of Multiple Attentions and Seg-CAM Consistency (CDMA+) to effectively leverage unlabeled images. First, we propose a Multi-attention Tri-decoder Network (MTNet) that consists of a shared encoder and three decoders, with each decoder using a different attention mechanism that calibrates features in different aspects to generate diverse outputs. Second, we introduce Cross Decoder Knowledge Distillation (CDKD) between the three decoders, allowing them to learn from each other’s soft labels to mitigate the negative impact of incorrect pseudo labels during training. Subsequently, motivated by the observation that the Class Activation Maps (CAMs) derived from the classification task could provide a rough segmentation, we employ an auxiliary classification head and introduce a consistency constraint between the CAM and segmentation results, i.e. Seg-CAM consistency. Additionally, uncertainty minimization is applied to the average prediction of the three decoders, which further regularizes predictions on unlabeled images and encourages inter-decoder consistency. Our proposed CDMA+ was compared with eight state-of-the-art SSL methods on two public pathological image datasets, and the experimental results showed that our method outperforms the other approaches under different annotation ratios. The code is available at https://github.com/HiLab-git/CDMA.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324002437",
    "keywords": [
      "Algorithm",
      "Annotation",
      "Artificial intelligence",
      "Computer science",
      "Consistency (knowledge bases)",
      "Decoding methods",
      "Encoder",
      "Leverage (statistics)",
      "Machine learning",
      "Operating system",
      "Pattern recognition (psychology)",
      "Pipeline (software)",
      "Programming language",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Zhong",
        "given_name": "Lanfeng"
      },
      {
        "surname": "Luo",
        "given_name": "Xiangde"
      },
      {
        "surname": "Liao",
        "given_name": "Xin"
      },
      {
        "surname": "Zhang",
        "given_name": "Shaoting"
      },
      {
        "surname": "Wang",
        "given_name": "Guotai"
      }
    ]
  },
  {
    "title": "Mutual Balancing in State-Object Components for Compositional Zero-Shot Learning",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110451",
    "abstract": "Compositional Zero-Shot Learning (CZSL) aims to recognize unseen compositions from seen states and objects. The disparity between the manually labeled semantic information and its actual visual features causes a significant imbalance of visual deviation in the distribution of various object classes and state classes, which is ignored by existing methods. To ameliorate these issues, we consider the CZSL task as an unbalanced multi-label classification task and propose a novel method called MUtual balancing in STate-object components (MUST) for CZSL, which provides a balancing inductive bias for the model. In particular, we split the classification of the composition classes into two consecutive processes to analyze the entanglement of the two components to get additional knowledge in advance, which reflects the degree of visual deviation between the two components. We use the knowledge gained to modify the model’s training process in order to generate more distinct class borders for classes with significant visual deviations. Extensive experiments demonstrate that our approach significantly outperforms the state-of-the-art on MIT-States, UT-Zappos, and C-GQA when combined with the basic CZSL frameworks, and it can improve various CZSL frameworks. Our code is available at https://github.com/LanchJL/MUST.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324002024",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Class (philosophy)",
      "Code (set theory)",
      "Computer science",
      "Economics",
      "Image (mathematics)",
      "Machine learning",
      "Management",
      "Mutual information",
      "Neuroscience",
      "Object (grammar)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Perception",
      "Process (computing)",
      "Programming language",
      "Set (abstract data type)",
      "Similarity (geometry)",
      "Task (project management)",
      "Visual Objects"
    ],
    "authors": [
      {
        "surname": "Jiang",
        "given_name": "Chenyi"
      },
      {
        "surname": "Ye",
        "given_name": "Qiaolin"
      },
      {
        "surname": "Wang",
        "given_name": "Shidong"
      },
      {
        "surname": "Shen",
        "given_name": "Yuming"
      },
      {
        "surname": "Zhang",
        "given_name": "Zheng"
      },
      {
        "surname": "Zhang",
        "given_name": "Haofeng"
      }
    ]
  },
  {
    "title": "Hyper-feature aggregation and relaxed distillation for class incremental learning",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110440",
    "abstract": "Although neural networks have been used extensively in pattern recognition scenarios, the pre-acquisition of datasets is still challenging. In most pattern recognition areas, preparing a training dataset that covers all data domains is difficult. Incremental learning was proposed to update neural networks in an online manner, but the catastrophic forgetting issue still needs to be studied. Class-incremental learning is one of the most challenging incremental learning contexts; it trains a unified model to classify all incrementally arriving classes learned thus far equally. Prior studies on class-incremental learning favor model stability over plasticity to realize old knowledge reservation and prevent catastrophic forgetting. Consequently, the model’s plasticity is omitted, leading to difficult generalization on new data. We propose a novel distillation-based method named Hyper-feature Aggregation and Relaxed Distillation (HARD) to realize balanced optimization of old and new knowledge. The aggregation of features is proposed to capture the global semantics while maintaining the diversity of the feature distribution after promoting representations of exemplars to higher dimensions. The proposed algorithm also introduces a relaxed restriction in the hyper-feature space to conditions the hyper-feature space through a normalized comparison of the relation matrices. Following generalization on more classes, the model is encouraged to rebuild the feature distribution when meeting new classes and to fine-tune the feature space to realize more distinct interclass boundaries. Extensive experiments were conducted on two benchmark datasets, and consistent improvements under diverse experimental settings demonstrated the effectiveness of the proposed approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324001912",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Class (philosophy)",
      "Computer science",
      "Feature (linguistics)",
      "Feature vector",
      "Forgetting",
      "Generalization",
      "Geodesy",
      "Geography",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Stability (learning theory)"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Ran"
      },
      {
        "surname": "Liu",
        "given_name": "Huanyu"
      },
      {
        "surname": "Yue",
        "given_name": "Zongcheng"
      },
      {
        "surname": "Li",
        "given_name": "Jun-Bao"
      },
      {
        "surname": "Sham",
        "given_name": "Chiu-Wing"
      }
    ]
  },
  {
    "title": "Improving image segmentation with contextual and structural similarity",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110489",
    "abstract": "Deep learning models for medical image segmentation are usually trained with voxel-wise losses, e.g., cross-entropy loss, focusing on unary supervision without considering inter-voxel relationships. This oversight potentially leads to semantically inconsistent predictions. Here, we propose a contextual similarity loss (CSL) and a structural similarity loss (SSL) to explicitly and efficiently incorporate inter-voxel relationships for improved performance. The CSL promotes consistency in predicted object categories for each image sub-region compared to ground truth. The SSL enforces compatibility between the predictions of voxel pairs by computing pair-wise distances between them, ensuring that voxels of the same class are close together whereas those from different classes are separated by a wide margin in the distribution space. The effectiveness of the CSL and SSL is evaluated using a clinical cone-beam computed tomography (CBCT) dataset of patients with various craniomaxillofacial (CMF) deformities and a public pancreas dataset. Experimental results show that the CSL and SSL outperform state-of-the-art regional loss functions in preserving segmentation semantics.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324002401",
    "keywords": [
      "Artificial intelligence",
      "Boosting (machine learning)",
      "Computer science",
      "Computer vision",
      "Consistency (knowledge bases)",
      "Ground truth",
      "Image (mathematics)",
      "Pattern recognition (psychology)",
      "Segmentation",
      "Similarity (geometry)",
      "Voxel"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Xiaoyang"
      },
      {
        "surname": "Liu",
        "given_name": "Qin"
      },
      {
        "surname": "Deng",
        "given_name": "Hannah H."
      },
      {
        "surname": "Kuang",
        "given_name": "Tianshu"
      },
      {
        "surname": "Lin",
        "given_name": "Henry Hung-Ying"
      },
      {
        "surname": "Xiao",
        "given_name": "Deqiang"
      },
      {
        "surname": "Gateno",
        "given_name": "Jaime"
      },
      {
        "surname": "Xia",
        "given_name": "James J."
      },
      {
        "surname": "Yap",
        "given_name": "Pew-Thian"
      }
    ]
  },
  {
    "title": "ACQ: Improving generative data-free quantization via attention correction",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110444",
    "abstract": "Data-free quantization aims to achieve model quantization without accessing any authentic sample. It is significant in an application-oriented context involving data privacy. Converting noise vectors into synthetic samples through a generator is a popular data-free quantization method, which is called generative data-free quantization. However, there is a difference in attention map between synthetic samples and authentic samples. This is always ignored and restricts the quantization performance. First, since synthetic samples of the same class are prone to have homogenous attention, the quantized network can only learn limited intra-class visual features. Second, synthetic samples in eval mode and training mode exhibit different attention. Hence, the statistical distribution matching tends to be inaccurate. ACQ is proposed in this paper to fix the attention of synthetic samples. Regarding intra-class attention homogeneity, we introduce an attention center matching loss aimed at achieving coarse-grained matching of attention of synthetic samples. Additionally, we have devised an adversarial loss based on pairs of samples with identical conditions. On one hand, this mechanism prevents the generator from mode collapse due to excessive attention on conditional information. On the other hand, it augments the separation between intra-class samples, thus further enhancing intra-class attention diversity. To improve the attention similarity of synthetic samples in different network modes, we introduce a consistency penalty to guarantee accurate statistical distribution matching. The experimental results demonstrate that ACQ effectively improves the attention problems of synthetic samples. Under various training settings, ACQ achieves the best quantization performance. For the 4-bit quantization of Resnet18 and Resnet50, ACQ reaches 67.55 % and 72.23 % accuracy, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032400195X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Generative grammar",
      "Pattern recognition (psychology)",
      "Quantization (signal processing)",
      "Synthetic data"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Jixing"
      },
      {
        "surname": "Guo",
        "given_name": "Xiaozhou"
      },
      {
        "surname": "Dai",
        "given_name": "Benzhe"
      },
      {
        "surname": "Gong",
        "given_name": "Guoliang"
      },
      {
        "surname": "Jin",
        "given_name": "Min"
      },
      {
        "surname": "Chen",
        "given_name": "Gang"
      },
      {
        "surname": "Mao",
        "given_name": "Wenyu"
      },
      {
        "surname": "Lu",
        "given_name": "Huaxiang"
      }
    ]
  },
  {
    "title": "Neural network based cognitive approaches from face perception with human performance benchmark",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.06.024",
    "abstract": "Artificial neural network models are able to achieve great performance at numerous computationally challenging tasks like face recognition. It is of significant importance to explore the difference between neural network models and human brains in terms of computational mechanism. This issue has become an experimental focus for some researchers in recent studies, and it is believed that using human behavior to understand neural network models can address this issue. This paper compares the neural network model performance with human performance on a classic yet important task: judging the ethnicity of a given face. This study uses Caucasian and East Asian faces to train 4 neural networks including AlexNet, VGG11, VGG13, and VGG16. Then, the ethnicity judgments of the neural networks are compared with human data using classical psychophysical methods by fitting psychometric curves. The results suggest that VGG11, followed by VGG16, shows a similar response pattern as humans, while simpler AlexNet and more complex VGG13 do not resemble human performance. Thus, this paper explores a new paradigm to compare neural networks and human brains.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524001946",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Cognition",
      "Computer science",
      "Face (sociological concept)",
      "Face perception",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Perception",
      "Psychology",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Yiyang"
      },
      {
        "surname": "Li",
        "given_name": "Yi-Fan"
      },
      {
        "surname": "Cheng",
        "given_name": "Chuanxin"
      },
      {
        "surname": "Ying",
        "given_name": "Haojiang"
      }
    ]
  },
  {
    "title": "Zigzag persistence for image processing: New software and applications",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.06.010",
    "abstract": "Topological image analysis is a powerful tool for understanding the structure and topology of images, being persistent homology one of its most popular methods. However, persistent homology requires a chain of inclusions of topological spaces, which can be challenging for digital images. In this article, we explore the use of zigzag persistence, a recent variant of traditional persistence, for digital image processing. To this end, new algorithms are developed to build a simplicial complex associated to a digital image and to compute the relationships between homology classes of a sequence of binary images via zigzag persistence. Additionally, we provide a simple software to use them. We demonstrate its effectiveness by applying it to a real-world problem of analyzing honey bee sperm videos.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524001806",
    "keywords": [
      "Artificial intelligence",
      "Computer graphics (images)",
      "Computer science",
      "Computer vision",
      "Engineering",
      "Geometry",
      "Geotechnical engineering",
      "Image (mathematics)",
      "Image processing",
      "Mathematics",
      "Persistence (discontinuity)",
      "Programming language",
      "Software",
      "Zigzag"
    ],
    "authors": [
      {
        "surname": "Divasón",
        "given_name": "Jose"
      },
      {
        "surname": "Romero",
        "given_name": "Ana"
      },
      {
        "surname": "Santolaria",
        "given_name": "Pilar"
      },
      {
        "surname": "Yániz",
        "given_name": "Jesús L."
      }
    ]
  },
  {
    "title": "E2F-Net: Eyes-to-face inpainting via StyleGAN latent space",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110442",
    "abstract": "Face inpainting, the technique of restoring missing or damaged regions in facial images, is pivotal for applications like face recognition in occluded scenarios and image analysis with poor-quality captures. This process not only needs to produce realistic visuals but also preserve individual identity characteristics. The aim of this paper is to inpaint a face given periocular region (eyes-to-face) through a proposed new Generative Adversarial Network (GAN)-based model called Eyes-to-Face Network (E2F-Net). The proposed approach extracts identity and non-identity features from the periocular region using two dedicated encoders have been used. The extracted features are then mapped to the latent space of a pre-trained StyleGAN generator to benefit from its state-of-the-art performance and its rich, diverse and expressive latent space without any additional training. We further improve the StyleGAN's output to find the optimal code in the latent space using a new optimization for GAN inversion technique. Our E2F-Net requires a minimum training process reducing the computational complexity as a secondary benefit. Through extensive experiments, we show that our method successfully reconstructs the whole face with high quality, surpassing current techniques, despite significantly less training and supervision efforts. We have generated seven eyes-to-face datasets based on well-known public face datasets for training and verifying our proposed methods. The code and datasets are publicly available. 1 1 https://github.com/fatemejamalii/E2F-Net",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324001936",
    "keywords": [
      "Acoustics",
      "Artificial intelligence",
      "Code (set theory)",
      "Computer science",
      "Computer vision",
      "Encoder",
      "Face (sociological concept)",
      "Geometry",
      "Identity (music)",
      "Image (mathematics)",
      "Inpainting",
      "Mathematics",
      "Net (polyhedron)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Physics",
      "Process (computing)",
      "Programming language",
      "Set (abstract data type)",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Hassanpour",
        "given_name": "Ahmad"
      },
      {
        "surname": "Jamalbafrani",
        "given_name": "Fatemeh"
      },
      {
        "surname": "Yang",
        "given_name": "Bian"
      },
      {
        "surname": "Raja",
        "given_name": "Kiran"
      },
      {
        "surname": "Veldhuis",
        "given_name": "Raymond"
      },
      {
        "surname": "Fierrez",
        "given_name": "Julian"
      }
    ]
  },
  {
    "title": "Video-based face outline recognition",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110482",
    "abstract": "We propose a novel approach for individual recognition that uses the motion traits from face outline-anonymised videos as the identity signatures; we call this type of signature a Temporal-iD. To extract a robust Temporal-iD, a highly lightweight transformer-based model, namely the Temporal-iD-ViT (TiDViT), is devised to capture and aggregate Temporal-iD features from videos. The TiDViT is equipped with a custom-designed multi-head temporal–spatial joint attention module that establishes interaction between the current frame input and the previous hidden state, thereby temporally aggregating the temporal features. The TiDViT processes the face video frame by frame instead of in a fixed batch-wise manner, which requires less computational memory. Moreover, the TiDViT can extract the temporal features of unconstrained face videos and considers ethical and privacy concerns. Extensive experimental results show that the proposed TiDViT model achieves a decent performance on this highly challenging task.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324002334",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Face (sociological concept)",
      "Face detection",
      "Facial recognition system",
      "Pattern recognition (psychology)",
      "Social science",
      "Sociology",
      "Three-dimensional face recognition"
    ],
    "authors": [
      {
        "surname": "Dong",
        "given_name": "Xingbo"
      },
      {
        "surname": "Yang",
        "given_name": "Jiewen"
      },
      {
        "surname": "Teoh",
        "given_name": "Andrew Beng Jin"
      },
      {
        "surname": "Yu",
        "given_name": "Dahai"
      },
      {
        "surname": "Li",
        "given_name": "Xiaomeng"
      },
      {
        "surname": "Jin",
        "given_name": "Zhe"
      }
    ]
  },
  {
    "title": "HARWE: A multi-modal large-scale dataset for context-aware human activity recognition in smart working environments",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.06.017",
    "abstract": "In recent years, deep neural networks (DNNs) have provided high performances for various tasks, such as human activity recognition (HAR), in view of their end-to-end training process between the input data and output labels. However, the performances of the DNNs are highly dependent on the availability of large-scale data for their training processes. In this paper, we propose a novel dataset for the task of HAR, in which the labels are specified for the working environments (WE). Our proposed dataset, namely HARWE, considers multiple signal modalities, including visual signal, audio signal, inertial sensor signals, and biological signals, that are acquired using four different electronic devices. Furthermore, our HARWE dataset is acquired from a large number of participants while considering the realistic disturbances that can occur in the wild. Our HARWE data is context-driven, which means there exist a number of labels in it that even though they are correlated with each other, they have contextual differences. A deep conventional multi-modal neural network provides an accuracy of 99.06% and 68.60%, for the cases of the easy and difficult settings of our dataset, respectively, which indicates its applicability for the task of human activity recognition.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524001879",
    "keywords": [
      "Activity recognition",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Chemistry",
      "Computer science",
      "Context (archaeology)",
      "Deep neural networks",
      "Economics",
      "Machine learning",
      "Management",
      "Modal",
      "Modalities",
      "Operating system",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Physics",
      "Polymer chemistry",
      "Process (computing)",
      "Programming language",
      "Quantum mechanics",
      "SIGNAL (programming language)",
      "Scale (ratio)",
      "Social science",
      "Sociology",
      "Speech recognition",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Esmaeilzehi",
        "given_name": "Alireza"
      },
      {
        "surname": "Khazaei",
        "given_name": "Ensieh"
      },
      {
        "surname": "Wang",
        "given_name": "Kai"
      },
      {
        "surname": "Kaur Kalsi",
        "given_name": "Navjot"
      },
      {
        "surname": "Ng",
        "given_name": "Pai Chet"
      },
      {
        "surname": "Liu",
        "given_name": "Huan"
      },
      {
        "surname": "Yu",
        "given_name": "Yuanhao"
      },
      {
        "surname": "Hatzinakos",
        "given_name": "Dimitrios"
      },
      {
        "surname": "Plataniotis",
        "given_name": "Konstantinos"
      }
    ]
  },
  {
    "title": "Learning to learn point signature for 3D shape geometry",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.06.021",
    "abstract": "Point signature is a representation that describes the structural geometry of a point within a neighborhood in 3D shapes. Conventional approaches apply a weight-sharing network, e.g., Graph Neural Network (GNN), to all neighborhoods of all points to directly generate point signatures and gain the generalization ability of the network by extensive training over amounts of samples from scratch. However, such approaches lack the flexibility to rapidly adapt to unseen neighborhood structures and thus cannot generalize well to new point sets. In this paper, we propose a novel meta-learning 3D point signature model, 3D meta point signature (MEPS) network, which is capable of learning robust 3D point signatures. Regarding each point signature learning process as a task, our method obtains an optimized model over the best performance on the distribution of all tasks, generating reliable signatures for new tasks, i.e., signatures of unseen point neighborhoods. Specifically, our MEPS consists of two modules: a base signature learner and a meta signature learner. During training, a base-learner is trained to perform specific signature learning tasks. Meanwhile, a meta-learner is trained to update the base-learner with optimal parameters. During testing, the meta-learner learned with the distribution of all tasks can adaptively change the base-learner parameters to accommodate unseen local neighborhoods. We evaluate our MEPS model on 3D shape correspondence and segmentation. Experimental results demonstrate that our method not only gains significant improvements over the baseline model to achieve state-of-the-art performance, but also is capable of handling unseen 3D geometry. Our implementation is available at https://github.com/hhuang-code/MEPS.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524001922",
    "keywords": [
      "Artificial intelligence",
      "Computational geometry",
      "Computer science",
      "Computer vision",
      "Geometry",
      "Mathematics",
      "Point (geometry)",
      "Signature (topology)"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Hao"
      },
      {
        "surname": "Wang",
        "given_name": "Lingjing"
      },
      {
        "surname": "Li",
        "given_name": "Xiang"
      },
      {
        "surname": "Yuan",
        "given_name": "Shuaihang"
      },
      {
        "surname": "Wen",
        "given_name": "Congcong"
      },
      {
        "surname": "Hao",
        "given_name": "Yu"
      },
      {
        "surname": "Fang",
        "given_name": "Yi"
      }
    ]
  },
  {
    "title": "Residual Feature-Reutilization Inception Network",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110439",
    "abstract": "Capturing feature information effectively is of great importance in the field of computer vision. With the development of convolutional neural networks, concepts like residual connection and multiple scales promote continual performance gains in diverse deep learning vision tasks. In this paper, novel residual feature-reutilization inception and split-residual feature-reutilization inception are proposed to improve performance on various vision tasks. It consists of four parallel branches, each with convolutional kernels of different sizes. These branches are interconnected by hierarchically organized channels, similar to residual connections, facilitating information exchange and rich dimensional variations at different levels. This structure enables the acquisition of features with varying granularity and effectively broadens the span of the receptive field in each network layer. Moreover, according to the network structure designed above, split-residual feature-reutilization inceptions can adjust the split ratio of the input information, thereby reducing the number of parameters and guaranteeing the model performance. Specifically, in image classification experiments based on popular vision datasets, such as CIFAR10 (97.94%), CIFAR100 (85.91%), Tiny Imagenet (70.54%) and ImageNet (80.83%), we obtain state-of-the-art results compared with other modern models under the premise that the models’ sizes are approximate and no additional data is used.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324001900",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Feature (linguistics)",
      "Field (mathematics)",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pure mathematics",
      "Residual"
    ],
    "authors": [
      {
        "surname": "He",
        "given_name": "Yuanpeng"
      },
      {
        "surname": "Song",
        "given_name": "Wenjie"
      },
      {
        "surname": "Li",
        "given_name": "Lijian"
      },
      {
        "surname": "Zhan",
        "given_name": "Tianxiang"
      },
      {
        "surname": "Jiao",
        "given_name": "Wenpin"
      }
    ]
  },
  {
    "title": "Fast main density peak clustering within relevant regions via a robust decision graph",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110458",
    "abstract": "Although Density Peak Clustering (DPC) can easily locate cluster centers by detecting density peaks in its decision graph, its allocation strategy may unadvisedly associate irrelevant points, its decision graph may mislead the cluster center selection, and its high computational complexity O ( n 2 ) shies itself away from large-scale data. Herein, a Fast Main Density Peak Clustering Within Relevant Regions Via A Robust Decision Graph (R-MDPC) is proposed. R-MDPC assigns points within the relevant regions to avoid the association of irrelevant points. With the removal of regional differences and the attenuation of satellite peaks, a robust decision graph is obtained. Moreover, based on the kNN distance of data points, R-MDPC is believed to be suitable for large-scale data. Experimental results demonstrated the high robustness of R-MDPC’s decision graph in identifying cluster centers, and its outstanding performance and fast running speed in recognizing complex-shaped clusters.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324002097",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Cluster (spacecraft)",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Data point",
      "Gene",
      "Graph",
      "Pattern recognition (psychology)",
      "Programming language",
      "Robustness (evolution)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Guan",
        "given_name": "Junyi"
      },
      {
        "surname": "Li",
        "given_name": "Sheng"
      },
      {
        "surname": "Zhu",
        "given_name": "Jinhui"
      },
      {
        "surname": "He",
        "given_name": "Xiongxiong"
      },
      {
        "surname": "Chen",
        "given_name": "Jiajia"
      }
    ]
  },
  {
    "title": "Zero-shot sketch-based image retrieval via adaptive relation-aware metric learning",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110452",
    "abstract": "Retrieving natural images with the query sketches under the zero-shot scenario is known as zero-shot sketch-based image retrieval (ZS-SBIR). Most of the best-performing methods adapt the triplet loss to learn projections that map natural images and sketches to a latent embedding space. They nevertheless neglect the modality gap between the hand-drawn sketches and the photos and consider no difference between any two incorrect classes, which limits their performance in real use cases. Towards this end, we put forward a simple and effective model, which adopts relation-aware metric learning to suppress the modality gap between the sketches and the photos. We also propose an adaptive margin that utilizes each anchor in embedding space to improve clustering ability in metric learning. Extensive experiments on the Sketchy and TU-Berlin datasets show the dominant position of our proposed model over SOTA competitors.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324002036",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Economics",
      "Image (mathematics)",
      "Image retrieval",
      "Linguistics",
      "Mathematics",
      "Metric (unit)",
      "Operations management",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Relation (database)",
      "Shot (pellet)",
      "Sketch",
      "Zero (linguistics)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Yang"
      },
      {
        "surname": "Dang",
        "given_name": "Yuhao"
      },
      {
        "surname": "Gao",
        "given_name": "Xinbo"
      },
      {
        "surname": "Han",
        "given_name": "Jungong"
      },
      {
        "surname": "Shao",
        "given_name": "Ling"
      }
    ]
  },
  {
    "title": "BSDP: Brain-inspired Streaming Dual-level Perturbations for Online Open World Object Detection",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110472",
    "abstract": "Humans can easily distinguish the known and unknown categories and can recognize the unknown object by learning it once instead of repeating it many times without forgetting the learned object. Hence, we aim to make deep learning models simulate the way people learn. We refer to such a learning manner as OnLine Open World Object Detection(OLOWOD). Existing OWOD approaches pay more attention to the identification of unknown categories, while the incremental learning part is also very important. Besides, some neuroscience research shows that specific noises allow the brain to form new connections and neural pathways which may improve learning speed and efficiency. In this paper, we take the dual-level information of old samples as perturbations on new samples to make the model good at learning new knowledge without forgetting the old knowledge. Therefore, we propose a simple plug-and-play method, called Brain-inspired Streaming Dual-level Perturbations(BSDP), to solve the OLOWOD problem. Specifically, (1) we first calculate the prototypes of previous categories and use the distance between samples and the prototypes as the sample selecting strategy to choose old samples for replay; (2) then take the prototypes as the streaming feature-level perturbations of new samples, so as to improve the plasticity of the model through revisiting the old knowledge; (3) and also use the distribution of the features of the old category samples to generate adversarial data in the form of streams as the data-level perturbations to enhance the robustness of the model to new categories. We empirically evaluate BSDP on PASCAL VOC and MS-COCO, and the excellent results demonstrate the promising performance of our proposed method and learning manner.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324002231",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Dual (grammatical number)",
      "Literature",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Yu"
      },
      {
        "surname": "Ma",
        "given_name": "Liyan"
      },
      {
        "surname": "Jing",
        "given_name": "Liping"
      },
      {
        "surname": "Yu",
        "given_name": "Jian"
      }
    ]
  },
  {
    "title": "Appearance debiased gaze estimation via stochastic subject-wise adversarial learning",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110441",
    "abstract": "Recently, appearance-based gaze estimation has been attracting attention in computer vision, and remarkable improvements have been achieved using various deep learning techniques. Despite such progress, most methods aim to infer gaze vectors from images directly, which causes overfitting to person-specific appearance factors. In this paper, we address these challenges and propose a novel framework: Stochastic subject-wise Adversarial gaZE learning (SAZE), which trains a network to generalize the appearance of subjects. We design a Face generalization Network (Fgen-Net) using a face-to-gaze encoder and face identity classifier and a proposed adversarial loss. The proposed loss generalizes face appearance factors so that the identity classifier inferences a uniform probability distribution. In addition, the Fgen-Net is trained by a learning mechanism that optimizes the network by reselecting a subset of subjects at every training step to avoid overfitting. Our experimental results verify the robustness of the method in that it yields state-of-the-art performance, achieving 3.89°and 4.42°on the MPIIFaceGaze and EyeDiap datasets, respectively. Furthermore, we demonstrate the positive generalization effect by conducting further experiments using face images involving different styles generated from the generative model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324001924",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Engineering",
      "Estimation",
      "Gaze",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Systems engineering"
    ],
    "authors": [
      {
        "surname": "Kim",
        "given_name": "Suneung"
      },
      {
        "surname": "Nam",
        "given_name": "Woo-Jeoung"
      },
      {
        "surname": "Lee",
        "given_name": "Seong-Whan"
      }
    ]
  },
  {
    "title": "Deep motion estimation through adversarial learning for gait recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.06.031",
    "abstract": "Gait recognition is a form of identity verification that can be performed over long distances without requiring the subject’s cooperation, making it particularly valuable for applications such as access control, surveillance, and criminal investigation. The essence of gait lies in the motion dynamics of a walking individual. Accurate gait-motion estimation is crucial for high-performance gait recognition. In this paper, we introduce two main designs for gait motion estimation. Firstly, we propose a fully convolutional neural network named W-Net for silhouette segmentation from video sequences. Secondly, we present an adversarial learning-based algorithm for robust gait motion estimation. Together, these designs contribute to a high-performance system for gait recognition and user authentication. In the experiment, two datasets, i.e., OU-IRIS and our own dataset, are used for performance evaluation. Experimental results show that, the W-Net achieves an accuracy of 89.46% in silhouette segmentation, and the proposed user-authentication method achieves over 99.6% and 93.8% accuracy on the two datasets, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524002010",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Engineering",
      "Estimation",
      "Gait",
      "Machine learning",
      "Medicine",
      "Motion (physics)",
      "Motion estimation",
      "Pattern recognition (psychology)",
      "Physical medicine and rehabilitation",
      "Systems engineering"
    ],
    "authors": [
      {
        "surname": "Yue",
        "given_name": "Yuanhao"
      },
      {
        "surname": "Shi",
        "given_name": "Laixiang"
      },
      {
        "surname": "Zheng",
        "given_name": "Zheng"
      },
      {
        "surname": "Chen",
        "given_name": "Long"
      },
      {
        "surname": "Wang",
        "given_name": "Zhongyuan"
      },
      {
        "surname": "Zou",
        "given_name": "Qin"
      }
    ]
  },
  {
    "title": "Self-supervised scheme for generalizing GAN image detection",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.06.030",
    "abstract": "Although the recent advancement in generative models brings diverse advantages to society, it can also be abused with malicious purposes, such as fraud, defamation, and fake news. To prevent such cases, vigorous research is conducted to distinguish the generated images from the real images, but challenges still remain to distinguish the generated images outside of the training settings. Such limitations occur due to data dependency arising from the model’s overfitting issue to the specific Generative Adversarial Networks (GANs) and categories of the training data. To overcome this issue, we adopt a self-supervised scheme. Our method is composed of the artificial artifact generator reconstructing the high-quality artificial artifacts of GAN images, and the GAN detector distinguishing GAN images by learning the reconstructed artificial artifacts. To improve the generalization of the artificial artifact generator, we build multiple autoencoders with different numbers of upconvolution layers. With numerous ablation studies, the robust generalization of our method is validated by outperforming the generalization of the previous state-of-the-art algorithms, even without utilizing the GAN images of the training dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524002009",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Image (mathematics)",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Scheme (mathematics)"
    ],
    "authors": [
      {
        "surname": "Jeong",
        "given_name": "Yonghyun"
      },
      {
        "surname": "Kim",
        "given_name": "Doyeon"
      },
      {
        "surname": "Kim",
        "given_name": "Pyounggeon"
      },
      {
        "surname": "Ro",
        "given_name": "Youngmin"
      },
      {
        "surname": "Choi",
        "given_name": "Jongwon"
      }
    ]
  },
  {
    "title": "VTHSC-MIR: Vision Transformer Hashing with Supervised Contrastive learning based medical image retrieval",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.06.003",
    "abstract": "In past few years, deep learning based medical image analysis technologies have significantly improved computer-assisted tasks like detecting, diagnosing, and predicting medical outcomes. The monitoring and diagnosis of ailments such as cancer and COVID-19 rely primarily on retrieving medical images. As the medical databases size is increasing rapidly it causes difficulty in managing and querying them. The main challenge lies in achieving superior retrieval performance while handling the intricacies of medical imaging datasets. To tackle this, we propose a novel end-to-end trainable framework for medical image retrieval using a vision transformer hashing with Supervised Contrastive (VTHSC) learning. Leveraging the self-attention mechanism of transformers and supervised contrastive loss, our model surpasses existing hashing-based retrieval methods. Focusing on the most recent COVID-19 and breast cancer datasets, we aim to efficiently retrieve relevant medical images from large datasets. Using the ImageNet-pretrained ViT as its backbone network, the VTHSC model incorporates a hashing head combined with a supervised contrastive loss and employs joint loss optimization. The VTHSC model is subsequently fine- tuned for a retrieval task, employing four different hashing frameworks: Deep Supervised Hashing (DSH), GreedyHash, Improved Deep Hashing Network (IDHN), and Deep Polarized Network (DPN). Our model achieves outstanding Mean Average Precision (MAP) scores of 98.9 for the BreakHis dataset and 96.03 for the COVID dataset. Notably, the VTHSC model surpasses several competing hashing-based retrieval methods by a substantial gain in terms of performance across various metrics such as precision, recall, and Top-6 retrieved images retrieval performance on the two benchmark medical image datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524001685",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Hash function",
      "Image (mathematics)",
      "Image retrieval",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Kumar",
        "given_name": "Mehul"
      },
      {
        "surname": "Singh",
        "given_name": "Rhythumwinder"
      },
      {
        "surname": "Mukherjee",
        "given_name": "Prerana"
      }
    ]
  },
  {
    "title": "Adaptive multi-text union for stable text-to-image synthesis learning",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110438",
    "abstract": "Generative Adversarial Networks (GANs) have significantly boosted the performance of text-to-image generation tasks in recent years. To train the generator, losses like reconstruction loss or adversarial loss between the generated image and ground truth image are widely adopted by recent works. These losses are all built over one assumption: the given text descriptions can describe the corresponding image perfectly. Unfortunately, this assumption is not satisfied in many cases, especially in datasets like COCO with complicated scenes due to the variance of annotator experience and focal point. This paper addresses this issue by proposing a multi-text-to-image training framework, which adaptively adjusts the weights of all the text descriptions corresponding to a specific image to generate the union description features. With the union description features, the generator can generate more visual-consistent images and mitigate the negative optimization caused by incomplete or inconsistent text descriptions. To better measure the similarity between generated images and multi-text descriptions, we also reformulate the process of multi-modal matching loss to better measure the similarity between image and multi-text descriptions. Extensive experiments on relevant benchmarks CUB and COCO prove the proposed method’s effectiveness and superiority compared to state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324001894",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Image (mathematics)",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Yan"
      },
      {
        "surname": "Qian",
        "given_name": "Jiechang"
      },
      {
        "surname": "Zhang",
        "given_name": "Huaidong"
      },
      {
        "surname": "Xu",
        "given_name": "Xuemiao"
      },
      {
        "surname": "Sun",
        "given_name": "Huajie"
      },
      {
        "surname": "Zeng",
        "given_name": "Fanzhi"
      },
      {
        "surname": "Zhou",
        "given_name": "Yuexia"
      }
    ]
  },
  {
    "title": "Cross-modal de-deviation for enhancing few-shot classification",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110475",
    "abstract": "Few-shot learning poses a critical challenge due to the deviation problem caused by the scarcity of available samples. In this work, we aim to address deviations in both feature representations and prototypes. To achieve this, we propose a cross-modal de-deviation framework that leverages class semantic information to provide robust prior knowledge for the samples. This framework begins with a visual-to-semantic autoencoder trained on the labeled samples to predict semantic features for the unlabeled samples. Then, we devise a binary linear programming model to match the initial prototypes with the cluster centers of the unlabeled samples. To circumvent potential mismatches between the cluster centers and the initial prototypes, we perform the label assignment process in the semantic space by transforming the cluster centers into semantic representations and utilizing the class ground truth semantic features as reference points. Moreover, we model a linear classifier with the concatenation of the refined prototypes and the class ground truth semantic features serving as the initial weights. Then we propose a novel optimization strategy based on the alternating least squares (ALS) model. From the ALS model, we can derive two closed-form solutions regarding to the features and weights, facilitating alternative optimization of them. Extensive experiments conducted on few-shot learning benchmarks demonstrate the competitive advantages of our CMDD method over the state-of-the-art approaches, confirming its effectiveness in reducing deviation. The code is available at: https://github.com/pmhDL/CMDD.git.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324002267",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Materials science",
      "Mathematics",
      "Metallurgy",
      "Modal",
      "Pattern recognition (psychology)",
      "Polymer chemistry",
      "Shot (pellet)"
    ],
    "authors": [
      {
        "surname": "Pan",
        "given_name": "Mei-Hong"
      },
      {
        "surname": "Shen",
        "given_name": "Hong-Bin"
      }
    ]
  },
  {
    "title": "Query-guided generalizable medical image segmentation",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.06.005",
    "abstract": "The practical implementation of deep neural networks in clinical settings faces hurdles due to variations in data distribution across different centers. While the incorporation of query-guided Transformer has improved performance across diverse tasks, the full scope of their generalization capabilities remains unexplored. Given the ability of the query-guided Transformer to dynamically adjust to individual samples, fulfilling the need for domain generalization, this paper explores the potential of query-based Transformer for cross-center generalization and introduces a novel Query-based Cross-Center medical image Segmentation mechanism (QuCCeS). By integrating a query-guided Transformer into a U-Net-like architecture, QuCCeS utilizes attribution modeling capability of query-guided Transformer decoder for segmentation in fluctuating scenarios with limited data. Additionally, QuCCeS incorporates an auxiliary task with adaptive sample weighting for coarse mask prediction. Experimental results demonstrate QuCCeS’s superior generalization on unseen domains.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524001752",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Image (mathematics)",
      "Image segmentation",
      "Information retrieval",
      "Pattern recognition (psychology)",
      "Scale-space segmentation",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Zhiyi"
      },
      {
        "surname": "Zhao",
        "given_name": "Zhou"
      },
      {
        "surname": "Gu",
        "given_name": "Yuliang"
      },
      {
        "surname": "Xu",
        "given_name": "Yongchao"
      }
    ]
  },
  {
    "title": "Global-local graph neural networks for node-classification",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.06.013",
    "abstract": "The task of graph node classification is often approached by utilizing a local Graph Neural Network (GNN), that learns only local information from the node input features and their adjacency. In this paper, we propose to improve the performance of node classification GNNs by utilizing both global and local information, specifically by learning label- and node- features. We therefore call our method Global-Local-GNN (GLGNN). To learn proper label features, for each label, we maximize the similarity between its features and nodes features that belong to the label, while maximizing the distance between nodes that do not belong to the considered label. We then use the learnt label features to predict the node classification map. We demonstrate our GLGNN using three different GNN backbones, and show that our approach improves baseline performance, revealing the importance of global information utilization for node classification.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524001831",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Engineering",
      "Graph",
      "Node (physics)",
      "Pattern recognition (psychology)",
      "Structural engineering",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Eliasof",
        "given_name": "Moshe"
      },
      {
        "surname": "Treister",
        "given_name": "Eran"
      }
    ]
  },
  {
    "title": "Multiple-environment Self-adaptive Network for aerial-view geo-localization",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110363",
    "abstract": "Aerial-view geo-localization tends to determine an unknown position through matching the drone-view image with the geo-tagged satellite-view image. This task is mostly regarded as an image retrieval problem. The key underpinning this task is to design a series of deep neural networks to learn discriminative image descriptors. However, existing methods meet large performance drops under realistic weather, such as rain and fog, since they do not take the domain shift between the training data and multiple test environments into consideration. To minor this domain gap, we propose a Multiple-environment Self-adaptive Network (MuSe-Net) to dynamically adjust the domain shift caused by environmental changing. In particular, MuSe-Net employs a two-branch neural network containing one multiple-environment style extraction network and one self-adaptive feature extraction network. As the name implies, the multiple-environment style extraction network is to extract the environment-related style information, while the self-adaptive feature extraction network utilizes an adaptive modulation module to dynamically minimize the environment-related style gap. Extensive experiments on three widely-used benchmarks, i.e., University-1652, SUES-200, and CVUSA, demonstrate that the proposed MuSe-Net achieves a competitive result for geo-localization in multiple environments. Furthermore, we observe that the proposed method also shows great potential to the unseen extreme weather, such as mixing the fog, rain and snow.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324001146",
    "keywords": [
      "Aerial image",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Convolutional neural network",
      "Discriminative model",
      "Domain (mathematical analysis)",
      "Drone",
      "Engineering",
      "Feature extraction",
      "Genetics",
      "Image (mathematics)",
      "Key (lock)",
      "Matching (statistics)",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Statistics",
      "Systems engineering",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Tingyu"
      },
      {
        "surname": "Zheng",
        "given_name": "Zhedong"
      },
      {
        "surname": "Sun",
        "given_name": "Yaoqi"
      },
      {
        "surname": "Yan",
        "given_name": "Chenggang"
      },
      {
        "surname": "Yang",
        "given_name": "Yi"
      },
      {
        "surname": "Chua",
        "given_name": "Tat-Seng"
      }
    ]
  },
  {
    "title": "Editorial: Special session on IbPRIA 2023",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.06.023",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524001934",
    "keywords": [
      "Computer science",
      "Session (web analytics)",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Gallego",
        "given_name": "Antonio Javier"
      },
      {
        "surname": "Marín-Jiménez",
        "given_name": "Manuel J."
      },
      {
        "surname": "Justo",
        "given_name": "Raquel"
      },
      {
        "surname": "Oliveira",
        "given_name": "Hélder"
      },
      {
        "surname": "Pertusa",
        "given_name": "Antonio"
      }
    ]
  },
  {
    "title": "Pairwise difference relational distillation for object re-identification",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110455",
    "abstract": "Most relationship knowledge distillation methods individually optimize pairwise similarities to improve the accuracy performance of a lightweight student network. However, this optimization approach may not be optimal for object re-identification (Re-ID) which prioritizes ranking. This is because it does not guarantee consistent ranking results between a lightweight student network and a large teacher network. For that, we propose a novel method called pairwise difference relational distillation (PDRD) for object Re-ID. First, we theoretically prove that minimizing the difference relationship between pairwise similarities resulting from student and teacher networks ensures consistent ranking results between the two networks. Second, based on this theoretical foundation, we combine non-linear activation functions on pairwise similarity discrepancies to create a non-linear pairwise difference rational knowledge loss function, which enhances knowledge transfer. Extensive experiments on four public datasets demonstrate that our method achieves state-of-the-art performance. For example, on Market-1501, using ResNet18 as a lightweight student network, our method acquires a rank-1 identification rate of 93.62%.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324002061",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Data mining",
      "Distillation",
      "Identification (biology)",
      "Mathematics",
      "Object (grammar)",
      "Pairwise comparison",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Xie",
        "given_name": "Yi"
      },
      {
        "surname": "Wu",
        "given_name": "Hanxiao"
      },
      {
        "surname": "Lin",
        "given_name": "Yihong"
      },
      {
        "surname": "Zhu",
        "given_name": "Jianqing"
      },
      {
        "surname": "Zeng",
        "given_name": "Huanqiang"
      }
    ]
  },
  {
    "title": "Latent spectral regularization for continual learning",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.06.020",
    "abstract": "While biological intelligence grows organically as new knowledge is gathered throughout life, Artificial Neural Networks forget catastrophically whenever they face a changing training data distribution. Rehearsal-based Continual Learning (CL) approaches have been established as a versatile and reliable solution to overcome this limitation; however, sudden input disruptions and memory constraints are known to alter the consistency of their predictions. We study this phenomenon by investigating the geometric characteristics of the learner’s latent space and find that replayed data points of different classes increasingly mix up, interfering with classification. Hence, we propose a geometric regularizer that enforces weak requirements on the Laplacian spectrum of the latent space, promoting a partitioning behavior. Our proposal, called Continual Spectral Regularizer for Incremental Learning (CaSpeR-IL), can be easily combined with any rehearsal-based CL approach and improves the performance of SOTA methods on standard benchmarks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524001909",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Consistency (knowledge bases)",
      "Latent variable",
      "Machine learning",
      "Operating system",
      "Regularization (linguistics)",
      "Space (punctuation)"
    ],
    "authors": [
      {
        "surname": "Frascaroli",
        "given_name": "Emanuele"
      },
      {
        "surname": "Benaglia",
        "given_name": "Riccardo"
      },
      {
        "surname": "Boschini",
        "given_name": "Matteo"
      },
      {
        "surname": "Moschella",
        "given_name": "Luca"
      },
      {
        "surname": "Fiorini",
        "given_name": "Cosimo"
      },
      {
        "surname": "Rodolà",
        "given_name": "Emanuele"
      },
      {
        "surname": "Calderara",
        "given_name": "Simone"
      }
    ]
  },
  {
    "title": "Distillation embedded absorbable pruning for fast object re-identification",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110437",
    "abstract": "Combining knowledge distillation (KD) and network pruning (NP) shows promise in learning a light network to accelerate object re-identification. However, KD requires an untrained student network to establish more critical connections in early epochs, but NP demands a well-trained student network to avoid destroying critical connections. This presents a dilemma, potentially leading to a collapse of the student network and harming object Re-ID performance. For that, we propose a distillation embedded absorbable pruning (DEAP) method. We design a pruner-convolution-pruner (PCP) unit to resolve the dilemma by loading NP’s sparse regularization on extra untrained pruners. Additionally, we propose an asymmetric relation knowledge distillation method. It readily transfers feature representation knowledge and asymmetric pairwise similarity knowledge without using additional adaptation modules. Finally, we apply re-parameterization to absorb pruners of PCP units to simplify student networks. Experiments demonstrate the superiority of DEAP, such as on the VeRi-776 dataset, with ResNet-101 as a teacher, DEAP saves 73.24% of model parameters and 71.98% of floating-point operations without sacrificing accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324001882",
    "keywords": [
      "Agronomy",
      "Artificial intelligence",
      "Biology",
      "Chemistry",
      "Computer science",
      "Distillation",
      "Feature (linguistics)",
      "Law",
      "Linguistics",
      "Machine learning",
      "Object (grammar)",
      "Organic chemistry",
      "Pairwise comparison",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Pruning",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Xie",
        "given_name": "Yi"
      },
      {
        "surname": "Wu",
        "given_name": "Hanxiao"
      },
      {
        "surname": "Zhu",
        "given_name": "Jianqing"
      },
      {
        "surname": "Zeng",
        "given_name": "Huanqiang"
      }
    ]
  },
  {
    "title": "An efficient ensemble explainable AI (XAI) approach for morphed face detection",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.06.014",
    "abstract": "Numerous deep neural convolutional architectures have been proposed in literature for face Morphing Attack Detection (MADs) to prevent such attacks and lessen the risks associated with them. Although, deep learning models achieved optimal results in terms of performance, it is difficult to understand and analyze these networks since they are black box/opaque in nature. As a consequence, incorrect judgments may be made. There is, however, a dearth of literature that explains decision-making methods of black box deep learning models for biometric Presentation Attack Detection (PADs) or MADs that can aid the biometric community to have trust in deep learning-based biometric systems for identification and authentication in various security applications such as border control, criminal database establishment etc. In this work, we present a novel visual explanation approach named Ensemble XAI integrating Saliency maps, Class Activation Maps (CAM) and Gradient-CAM (Grad-CAM) to provide a more comprehensive visual explanation for a deep learning prognostic model (EfficientNet-B1) that we have employed to predict whether the input presented to a biometric authentication system is morphed or genuine. The experimentations have been performed on three publicly available datasets namely Face Research Lab London (FRLL) dataset, Wide Multi-Channel Presentation Attack (WMCA) dataset, and Makeup Induced Face Spoofing (MIFS) dataset. The experimental evaluations affirms that the resultant visual explanations highlight more fine-grained details of image features/areas focused by EfficientNet-B1 to reach decisions along with appropriate reasoning.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524001843",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Face (sociological concept)",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Dwivedi",
        "given_name": "Rudresh"
      },
      {
        "surname": "Kothari",
        "given_name": "Pranay"
      },
      {
        "surname": "Chopra",
        "given_name": "Deepak"
      },
      {
        "surname": "Singh",
        "given_name": "Manjot"
      },
      {
        "surname": "Kumar",
        "given_name": "Ritesh"
      }
    ]
  },
  {
    "title": "Self-supervised learning with automatic data augmentation for enhancing representation",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.06.012",
    "abstract": "Self-supervised learning has become an increasingly popular method for learning effective representations from unlabeled data. One prominent approach in self-supervised learning is contrastive learning, which trains models to distinguish between similar and dissimilar sample pairs by pulling similar pairs closer and pushing dissimilar pairs farther apart. The key to the success of contrastive learning lies in the quality of the data augmentation, which increases the diversity of the data and helps the model learn more powerful and generalizable representations. While many studies have emphasized the importance of data augmentation, however, most of them rely on human-crafted augmentation strategies. In this paper, we propose a novel method, Self Augmentation on Contrastive Learning with Clustering (SACL), searching for the optimal data augmentation policy automatically using Bayesian optimization and clustering. The proposed approach overcomes the limitations of relying on domain knowledge and avoids the high costs associated with manually designing data augmentation rules. It automatically captures informative and useful features within the data by exploring augmentation policies. We demonstrate that the proposed method surpasses existing approaches that rely on manually designed augmentation rules. Our experiments show SACL outperforms manual strategies, achieving a performance improvement of 1.68% and 1.57% over MoCo v2 on the CIFAR10 and SVHN datasets, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552400182X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Law",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Semi-supervised learning"
    ],
    "authors": [
      {
        "surname": "Park",
        "given_name": "Chanjong"
      },
      {
        "surname": "Kim",
        "given_name": "Eunwoo"
      }
    ]
  },
  {
    "title": "Towards effective person search with deep learning: A survey from systematic perspective",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110434",
    "abstract": "Person search detects and retrieves simultaneously a query person across uncropped scene images captured by multiple non-overlapping cameras. In light of the deep learning advancement, person search has emerged as a promising research direction that demonstrates great potential for real-world applications. This paper presents a systematic survey of deep learning methods for person search. Different from existing categorizations, we propose a new taxonomy that dissects person search models into four major components i.e., proposal prediction, feature representation learning, training objectives, and ranking optimization. The most representative works in each component are summarized with highlighted contributions to this field. An in-depth analysis is provided upon evaluation performances of state-of-the-art person search models together with a summary of benchmark datasets. Despite that significant progress has been made to date, practical and extendable person search remains an open task. We conclude with discussions on those under-explored yet challenging datasets and learning mechanisms for real-world demands to inspire future research directions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324001857",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Data science",
      "Deep learning",
      "Engineering",
      "Feature learning",
      "Field (mathematics)",
      "Geodesy",
      "Geography",
      "Information retrieval",
      "Law",
      "Machine learning",
      "Mathematics",
      "Perspective (graphical)",
      "Political science",
      "Politics",
      "Pure mathematics",
      "Ranking (information retrieval)",
      "Representation (politics)",
      "Systems engineering",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Pengcheng"
      },
      {
        "surname": "Yu",
        "given_name": "Xiaohan"
      },
      {
        "surname": "Wang",
        "given_name": "Chen"
      },
      {
        "surname": "Zheng",
        "given_name": "Jin"
      },
      {
        "surname": "Ning",
        "given_name": "Xin"
      },
      {
        "surname": "Bai",
        "given_name": "Xiao"
      }
    ]
  },
  {
    "title": "Frozen is better than learning: A new design of prototype-based classifier for semantic segmentation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110431",
    "abstract": "Semantic segmentation models comprise an encoder to extract features and a classifier for prediction. However, the learning of the classifier suffers from the ambiguity which is caused by two factors: (1) the weights of a classifier for similar categories may have positive similarities lowing the performance for similar categories, named correlation ambiguity, and (2) the classifier is prone to predict the category with a larger ℓ 2 norm and vice versa, termed prior ambiguity. To comedy the issues, we propose Category-Basis Prototype (CBP), frozen and mutually orthogonalized prototypes with equal ℓ 2 norm. Orthogonalization prevents the prototypes from being similar to each other and the equality decouples the prediction from the ℓ 2 norm. To better shape the feature space, we propose Online Centroid Contrastive Loss (OCCL) equipped with centroid and category-level losses. Experiments show that our method yields compelling results over two widely applied benchmarks indicating the effectiveness of our methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324001821",
    "keywords": [
      "Algorithm",
      "Ambiguity",
      "Artificial intelligence",
      "Autoencoder",
      "Centroid",
      "Classifier (UML)",
      "Computer science",
      "Deep learning",
      "Machine learning",
      "Mathematics",
      "Orthogonalization",
      "Pattern recognition (psychology)",
      "Programming language",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Jialei"
      },
      {
        "surname": "Deguchi",
        "given_name": "Daisuke"
      },
      {
        "surname": "Zhang",
        "given_name": "Chenkai"
      },
      {
        "surname": "Zheng",
        "given_name": "Xu"
      },
      {
        "surname": "Murase",
        "given_name": "Hiroshi"
      }
    ]
  },
  {
    "title": "End-to-end latent fingerprint enhancement using multi-scale Generative Adversarial Network",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.06.022",
    "abstract": "Latent fingerprint enhancement is paramount as it dramatically influences matching accuracy. This process is often challenging due to varying structured noise and background patterns. The prints may be of arbitrary sizes and scales with a high degree of occlusion. There is a need for creating an end-to-end system that handles different conditions reliably to streamline this often lengthy and tricky process. In this work, we propose a Generative Adversarial Network (GAN) based architecture that effectively captures multi-scale context using Atrous Spatial Pyramid Pooling (ASPP). We have trained the network on a synthetically generated dataset, carefully designed to represent real-world latent prints. By avoiding the reconstruction of spurious ridges and only enhancing valid ridges, we avoid the generation of false minutiae, leading to better matching performance. We obtained state-of-the-art results in Sensor to Latent matching using the IIITD MOLF and Latent to Latent Matching using IIITD Latent datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524001910",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Cartography",
      "Computer science",
      "Deep learning",
      "End-to-end principle",
      "Fingerprint (computing)",
      "Generative adversarial network",
      "Generative grammar",
      "Geography",
      "Pattern recognition (psychology)",
      "Scale (ratio)"
    ],
    "authors": [
      {
        "surname": "R.N.",
        "given_name": "Pramukha"
      },
      {
        "surname": "P.",
        "given_name": "Akhila"
      },
      {
        "surname": "Koolagudi",
        "given_name": "Shashidhar G."
      }
    ]
  },
  {
    "title": "Interpretable medical deep framework by logits-constraint attention guiding graph-based multi-scale fusion for Alzheimer’s disease analysis",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110450",
    "abstract": "Deep learning using structural MRI has been widely applied to early diagnosis study of Alzheimer’s disease. Among existing methods, attention-based 3D subject-level methods can not only provide diagnosis results but also interpret the significant brain regions, thereby attracting considerable attention. However, the performance of previous attention-based methods might be still restricted by: (i) the gap between attention scores and semantic significant regions; (ii) using only single-scale features or simply fusing multi-scale information by addition or concatenation for classification decision-making. To overcome these two issues, we propose an innovative dual-branch model called LA-GMF, which consists of two major modules: logits-constraint attention (LA) and graph-based multi-scale fusion (GMF). The LA module is designed to guide the model to focus on key areas to enhance the diagnostic performance of local lesions, by reducing the inconsistency between attention scores and class prediction probabilities. Meanwhile, by combining the graph neural network and the self-attention mechanism, the GMF module not only introduces the interaction between patches, but also explores the correlation and complementarity between features at different scales, thereby extracting feature representations more comprehensively. Experiments on the popular ADNI and AIBL datasets validate the potential of our model in boosting early AD diagnosis accuracy. Additionally, our interpretation experiments demonstrate the superior interpretability performance of the proposed method over recent state-of-the-art attention-based methods. Our source codes are released at: https://github.com/nollexu/LA-GMF.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324002012",
    "keywords": [
      "Artificial intelligence",
      "Cartography",
      "Computer science",
      "Constraint (computer-aided design)",
      "Disease",
      "Geography",
      "Geometry",
      "Graph",
      "Machine learning",
      "Mathematics",
      "Medicine",
      "Pathology",
      "Scale (ratio)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Jinghao"
      },
      {
        "surname": "Yuan",
        "given_name": "Chenxi"
      },
      {
        "surname": "Ma",
        "given_name": "Xiaochuan"
      },
      {
        "surname": "Shang",
        "given_name": "Huifang"
      },
      {
        "surname": "Shi",
        "given_name": "Xiaoshuang"
      },
      {
        "surname": "Zhu",
        "given_name": "Xiaofeng"
      }
    ]
  },
  {
    "title": "Graph embedding orthogonal decomposition: A synchronous feature selection technique based on collaborative particle swarm optimization",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110453",
    "abstract": "In unsupervised feature selection, the clustering label matrix has the ability to distinguish between projection clusters. However, the latent geometric structure of the clustering labels is often ignored. In addition, the optimal sub-feature selection performance of feature selection techniques relies greatly on the choice of balanced parameters, and the selection range of most technical parameters is limited and fixed. To solve the above-mentioned problems, this paper proposes a synchronous feature selection technique based on graph-embedded cluster label orthogonal decomposition and collaborative particle swarm optimization (GOD-cPSO). First, GOD-cPSO extends the feature selection framework of clustering label orthogonal decomposition by graph embedding to retain the latent geometric structure of clustering labels, thus maintaining the correlation between clustered sample labels. Then, the l 2,1-2-norm with strong global convergence is extended to the graph embedding clustering label orthogonal decomposition framework. By imposing this non-convex constraint, GOD-cPSO can achieve low-dimensional sparse and low-redundant sub-features. In addition, the local structure preserving of low-dimensional manifolds is integrated into the graph-embedded clustering label orthogonal decomposition framework to obtain good cluster separation and effectively maintain the latent local structure of the data. Finally, to ensure the adaptive parameter selection over a large range, GOD-cPSO synchronously guides the graph-embedding clustering labeling orthogonal decomposition framework for feature selection through collaborative particle swarm optimization. GOD-cPSO has synchronous parameter optimization and feature selection and picks parameters in a larger range. Comprehensive numerical experiments are performed on nine datasets to test the validity of the GOD-cPSO. The experimental results demonstrate that the sub-features selected by the GOD-cPSO have stronger discriminative power and are superior to other techniques in the clustering assignments.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324002048",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Feature (linguistics)",
      "Feature selection",
      "Linguistics",
      "Mathematics",
      "Particle swarm optimization",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Zhong",
        "given_name": "Jingyu"
      },
      {
        "surname": "Shang",
        "given_name": "Ronghua"
      },
      {
        "surname": "Xu",
        "given_name": "Songhua"
      },
      {
        "surname": "Li",
        "given_name": "Yangyang"
      }
    ]
  },
  {
    "title": "Multi-target label backdoor attacks on graph neural networks",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110449",
    "abstract": "Graph neural networks have been shown to have characteristics that make them susceptible to backdoor attacks, and many recent works have proposed feasible graph backdoor attack methods. However, existing graph backdoor attack methods only target one-to-one attack types and lack graph backdoor attack methods that can address one-to-many attack requirements. This paper is the first research work on one-to-many type graph backdoor attacks and proposes the backdoor attack method MLGB, which can achieve multi-target label attacks for GNN node classification tasks. We designed encoding mechanisms to allow MLGB to customize triggers for different target labels and ensure differentiation between triggers for different target labels through loss functions. Additionally, we designed an innovative poisoned node selection method to improve the efficiency of MLGB’s attacks further. Extensive experiments were conducted to validate MLGB’s effectiveness across multiple datasets and model architectures, demonstrating its robustness against graph backdoor attack defense mechanisms. Furthermore, ablation experiments and explainability analyses were conducted to provide deeper insights into MLGB. Our work reveals that graph neural networks are also vulnerable to one-to-many type backdoor attacks, which is important for practitioners to understand model risks comprehensively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324002000",
    "keywords": [
      "Backdoor",
      "Computer science",
      "Computer security",
      "Graph",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Kaiyang"
      },
      {
        "surname": "Deng",
        "given_name": "Huaxin"
      },
      {
        "surname": "Xu",
        "given_name": "Yijia"
      },
      {
        "surname": "Liu",
        "given_name": "Zhonglin"
      },
      {
        "surname": "Fang",
        "given_name": "Yong"
      }
    ]
  },
  {
    "title": "Self-supervised assisted multi-task learning network for one-shot defect segmentation with fake defect generation",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.05.017",
    "abstract": "Object surface defect segmentation is extremely crucial for automatic quality inspection in the field of industrial production. However, following deficiencies of most current defect semantic algorithms such as over-reliance on large-scale labeled datasets, underutilization of unlabeled defect-free (normal) samples, and restrictions on the type of material have limited the application of these algorithms. To solve these problems, we propose a novel self-supervised assisted multi-task learning network (SAMLNet) to segment the defects of several different types of materials. Making full use of unlabeled defect-free samples, we introduce an efficient self-supervised segmentation (SSS) task not only to assist in training parameters of the backbone shared with the one-shot segmentation (OSS) task but also to predict the rough mask of the actual defect image through the semantic representation (background prototype). To extract the representation, we propose a two-stage training method to accelerate model convergence. Since small or slender defects are common, we propose a high-pixel feature attention module to reserve more details of the defect area. Extensive experiments show that the proposed method outperforms the state-of-the-art methods across several defect datasets, demonstrating the effectiveness of our method. Code is available at: https://github.com/qasonvera0307/OSS-SAMLNet",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524001636",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Engineering",
      "Machine learning",
      "Materials science",
      "Mechanical engineering",
      "Metallurgy",
      "One shot",
      "Pattern recognition (psychology)",
      "Segmentation",
      "Shot (pellet)",
      "Systems engineering",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Hu",
        "given_name": "Ziqiang"
      },
      {
        "surname": "Chu",
        "given_name": "Hao"
      },
      {
        "surname": "Zhang",
        "given_name": "Yunzhou"
      },
      {
        "surname": "Shan",
        "given_name": "Dexing"
      },
      {
        "surname": "Shen",
        "given_name": "You"
      }
    ]
  },
  {
    "title": "SceneFake: An initial dataset and benchmarks for scene fake audio detection",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110468",
    "abstract": "Many datasets have been designed to further the development of fake audio detection. However, fake utterances in previous datasets are mostly generated by altering timbre, prosody, linguistic content or channel noise of original audio. These datasets leave out a scenario, in which the acoustic scene of an original audio is manipulated with a forged one. It will pose a major threat to our society if some people misuse the manipulated audio with malicious purpose. Therefore, this motivates us to fill in the gap. This paper proposes such a dataset for scene fake audio detection named SceneFake, where a manipulated audio is generated by only tampering with the acoustic scene of an real utterance by using speech enhancement technologies. Some scene fake audio detection benchmark results on the SceneFake dataset are reported in this paper. In addition, an analysis of fake attacks with different speech enhancement technologies and signal-to-noise ratios are presented in this paper. The results indicate that scene fake utterances cannot be reliably detected by baseline models trained on the ASVspoof 2019 dataset. Although these models perform well on the SceneFake training set and seen testing set, their performance is poor on the unseen test set. The dataset 2 2 https://zenodo.org/record/7663324#.Y_XKMuPYuUk. and benchmark source codes 3 3 https://github.com/ADDchallenge/SceneFake. are publicly available.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032400219X",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Background noise",
      "Benchmark (surveying)",
      "Computer science",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Musical",
      "Noise (video)",
      "Programming language",
      "Set (abstract data type)",
      "Speech recognition",
      "Telecommunications",
      "Timbre",
      "Utterance",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Yi",
        "given_name": "Jiangyan"
      },
      {
        "surname": "Wang",
        "given_name": "Chenglong"
      },
      {
        "surname": "Tao",
        "given_name": "Jianhua"
      },
      {
        "surname": "Zhang",
        "given_name": "Chu Yuan"
      },
      {
        "surname": "Fan",
        "given_name": "Cunhang"
      },
      {
        "surname": "Tian",
        "given_name": "Zhengkun"
      },
      {
        "surname": "Ma",
        "given_name": "Haoxin"
      },
      {
        "surname": "Fu",
        "given_name": "Ruibo"
      }
    ]
  },
  {
    "title": "Test-time adaptation for 6D pose tracking",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110390",
    "abstract": "We propose a test-time adaptation for 6D object pose tracking that learns to adapt a pre-trained model to track the 6D pose of novel objects. We consider the problem of 6D object pose tracking as a 3D keypoint detection and matching task and present a model that extracts 3D keypoints. Given an RGB-D image and the mask of a target object for each frame, the proposed model consists of the self- and cross-attention modules to produce the features that aggregate the information within and across frames, respectively. By using the keypoints detected from the features for each frame, we estimate the pose changes between two frames, which enables 6D pose tracking when the 6D pose of a target object in the initial frame is given. Our model is first trained in a source domain, a category-level tracking dataset where the ground truth 6D pose of the object is available. To deploy this pre-trained model to track novel objects, we present a test-time adaptation strategy that trains the model to adapt to the target novel object by self-supervised learning. Given an RGB-D video sequence of the novel object, the proposed self-supervised losses encourage the model to estimate the 6D pose changes that can keep the photometric and geometric consistency of the object. We validate our method on the NOCS-REAL275 dataset and our collected dataset, and the results show the advantages of tracking novel objects. The collected dataset and visualisation of tracking results are available: https://qm-ipalab.github.io/TA-6DT/",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324001419",
    "keywords": [
      "Adaptation (eye)",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Frame (networking)",
      "Matching (statistics)",
      "Mathematics",
      "Object (grammar)",
      "Optics",
      "Pattern recognition (psychology)",
      "Pedagogy",
      "Physics",
      "Pose",
      "Psychology",
      "RGB color model",
      "Statistics",
      "Telecommunications",
      "Tracking (education)",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Tian",
        "given_name": "Long"
      },
      {
        "surname": "Oh",
        "given_name": "Changjae"
      },
      {
        "surname": "Cavallaro",
        "given_name": "Andrea"
      }
    ]
  },
  {
    "title": "Text-based person search via cross-modal alignment learning",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110481",
    "abstract": "Text-based person search aims to use text descriptions to search for corresponding person images. However, due to the obvious pattern differences in image and text modalities, it is still a challenging problem to align the two modalities. Most existing approaches only consider semantic alignment within a global context or partial parts, lacking consideration of how to match image and text in terms of differences in model information. Therefore, in this paper, we propose an efficient Modality-Aligned Person Search network (MAPS) to address this problem. First, we suppress image-specific information by image feature style normalization to achieve modality knowledge alignment and reduce information differences between text and image. Second, we design a multi-granularity modal feature fusion and optimization method to enrich the modal features. To address the problem of useless and redundant information in the multi-granularity fused features, we propose a Multi-granularity Feature Self-optimization Module (MFSM) to adaptively adjust the corresponding contributions of different granularities in the fused features of the two modalities. Finally, to address the problem of information inconsistency in the training and inference stages, we propose a Cross-instance Feature Alignment (CFA) to help the network enhance category-level generalization ability and improve retrieval performance. Extensive experiments demonstrate that our MAPS achieves state-of-the-art performance on all text-based person search datasets, and significantly outperforms other existing methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324002322",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Modal",
      "Pattern recognition (psychology)",
      "Polymer chemistry"
    ],
    "authors": [
      {
        "surname": "Ke",
        "given_name": "Xiao"
      },
      {
        "surname": "Liu",
        "given_name": "Hao"
      },
      {
        "surname": "Xu",
        "given_name": "Peirong"
      },
      {
        "surname": "Lin",
        "given_name": "Xinru"
      },
      {
        "surname": "Guo",
        "given_name": "Wenzhong"
      }
    ]
  },
  {
    "title": "Transforming gradient-based techniques into interpretable methods",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.06.006",
    "abstract": "The explication of Convolutional Neural Networks (CNN) through xAI techniques often poses challenges in interpretation. The inherent complexity of input features, notably pixels extracted from images, engenders complex correlations. Gradient-based methodologies, exemplified by Integrated Gradients (IG), effectively demonstrate the significance of these features. Nevertheless, the conversion of these explanations into images frequently yields considerable noise. Presently, we introduce GAD (Gradient Artificial Distancing) as a supportive framework for gradient-based techniques. Its primary objective is to accentuate influential regions by establishing distinctions between classes. The essence of GAD is to limit the scope of analysis during visualization and, consequently reduce image noise. Empirical investigations involving occluded images have demonstrated that the identified regions through this methodology indeed play a pivotal role in facilitating class differentiation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524001764",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Rodrigues",
        "given_name": "Caroline Mazini"
      },
      {
        "surname": "Boutry",
        "given_name": "Nicolas"
      },
      {
        "surname": "Najman",
        "given_name": "Laurent"
      }
    ]
  },
  {
    "title": "Community detection in the stochastic block model by mixed integer programming",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110487",
    "abstract": "The Degree-Corrected Stochastic Block Model (DCSBM) is a popular model to generate random graphs with community structure given an expected degree sequence. The standard approach of community detection based on the DCSBM is to search for the model parameters that are the most likely to have produced the observed network data through maximum likelihood estimation (MLE). Current techniques for the MLE problem are heuristics, and therefore do not guarantee convergence to the optimum. We present mathematical programming formulations and exact solution methods that can provably find the model parameters and community assignments of maximum likelihood given an observed graph. We compare these exact methods with classical heuristic algorithms based on expectation–maximization (EM). The solutions given by exact methods give us a principled way of measuring the experimental performance of classical heuristics and comparing different variations thereof.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324002383",
    "keywords": [
      "Acoustics",
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Block (permutation group theory)",
      "Cluster analysis",
      "Computer science",
      "Convergence (economics)",
      "Degree (music)",
      "Economic growth",
      "Economics",
      "Expectation–maximization algorithm",
      "Genetics",
      "Geometry",
      "Graph",
      "Heuristic",
      "Heuristics",
      "Integer programming",
      "Mathematical optimization",
      "Mathematics",
      "Maximization",
      "Maximum likelihood",
      "Physics",
      "Random graph",
      "Sequence (biology)",
      "Statistics",
      "Stochastic block model",
      "Stochastic programming",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Serrano",
        "given_name": "Breno"
      },
      {
        "surname": "Vidal",
        "given_name": "Thibaut"
      }
    ]
  },
  {
    "title": "Augmented skeleton sequences with hypergraph network for self-supervised group activity recognition",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110478",
    "abstract": "Contrastive learning has been widely applied to self-supervised skeleton-based single-person action recognition. However, directly employing single-person contrastive learning techniques for multi-person skeleton-based Group Activity Recognition (GAR) suffers from some challenges. Firstly, single-person data augmentation strategies struggle to capture complex collaborations between actors in multi-person scenarios, resulting in poor generalization. Secondly, real-world uncertainties in the number of people make single-person methods fail to capture changing high-order actor relations. Finally, single-person methods treat each actor with equal importance for recognition, struggling to distinguish imbalanced contributions between individual and group activities. To this end, the coarse-to-fine Augmented Hypergraph Network (AHNet) is proposed for effective self-supervised GAR. Specifically, we introduce multi-person augmentation strategies to enhance the generalization of the model under complex actor collaboration scenarios. Moreover, a knowledge-masked hypergraph network is employed to enhance the adaptability of the model to capture varied high-order actor relations. Finally, coarse-to-fine contrast among key actors is conducted to mitigate the imbalanced contributions between individual and group levels. Extensive experiments on multiple datasets demonstrate that our AHNet achieves substantial improvements over state-of-the-art methods with various backbone architectures. Our code is available at https://github.com/WGQ109/AHNet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324002292",
    "keywords": [
      "Adaptability",
      "Artificial intelligence",
      "Biology",
      "Chemistry",
      "Code (set theory)",
      "Computer science",
      "Computer security",
      "Discrete mathematics",
      "Ecology",
      "Generalization",
      "Group (periodic table)",
      "Hypergraph",
      "Key (lock)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Programming language",
      "Set (abstract data type)",
      "Skeleton (computer programming)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Guoquan"
      },
      {
        "surname": "Liu",
        "given_name": "Mengyuan"
      },
      {
        "surname": "Liu",
        "given_name": "Hong"
      },
      {
        "surname": "Guo",
        "given_name": "Peini"
      },
      {
        "surname": "Wang",
        "given_name": "Ti"
      },
      {
        "surname": "Guo",
        "given_name": "Jingwen"
      },
      {
        "surname": "Fan",
        "given_name": "Ruijia"
      }
    ]
  },
  {
    "title": "DGFormer: Dynamic graph transformer for 3D human pose estimation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110446",
    "abstract": "Despite the significant progress for monocular 3D human pose estimation, it still faces challenges due to self-occlusions and depth ambiguities. To tackle those issues, we propose a novel Dynamic Graph Transformer (DGFormer) to exploit local and global relationships between skeleton joints for pose estimation. Specifically, the proposed DGFormer mainly consists of three core modules: Transformer Encoder (TE), immobile Graph Convolutional Network (GCN), and dynamic GCN. TE module leverages the self-attention mechanism to learn the complex global relationships among skeleton joints. The immobile GCN is responsible for capturing the local physical connections between human joints, while the dynamic GCN concentrates on learning the sparse dynamic K-nearest neighbor interactions according to different action poses. By building the adequately global long-range, local physical, and sparse dynamic dependencies of human joints, experiments on Human3.6M and MPI-INF-3DHP datasets demonstrate that our method can predict 3D pose with lower errors outperforming the recent state-of-the-art image-based performance. Furthermore, experiments on in-the-wild videos demonstrate the impressive generalization abilities of our method. Code will be available at: https://github.com/czmmmm/DGFormer.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324001973",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Encoder",
      "Exploit",
      "Generalization",
      "Graph",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Neural coding",
      "Operating system",
      "Pattern recognition (psychology)",
      "Physics",
      "Pose",
      "Quantum mechanics",
      "Theoretical computer science",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Zhangmeng"
      },
      {
        "surname": "Dai",
        "given_name": "Ju"
      },
      {
        "surname": "Bai",
        "given_name": "Junxuan"
      },
      {
        "surname": "Pan",
        "given_name": "Junjun"
      }
    ]
  },
  {
    "title": "Conditional Information Gain Trellis",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.06.018",
    "abstract": "Conditional computing processes an input using only part of the neural network’s computational units. Learning to execute parts of a deep convolutional network by routing individual samples has several advantages: This can facilitate the interpretability of the model, reduce the model complexity, and reduce the computational burden during training and inference. Furthermore, if similar classes are routed to the same path, that part of the network learns to discriminate between finer differences and better classification accuracies can be attained with fewer parameters. Recently, several papers have exploited this idea to select a particular child of a node in a tree-shaped network or to skip parts of a network. In this work, we follow a Trellis-based approach for generating specific execution paths in a deep convolutional neural network. We have designed routing mechanisms that use differentiable information gain-based cost functions to determine which subset of features in a convolutional layer will be executed. We call our method Conditional Information Gain Trellis (CIGT). We show that our conditional execution mechanism achieves comparable or better model performance compared to unconditional baselines, using only a fraction of the computational resources. We provide our code and model checkpoints used in the paper at: https://github.com/ufukcbicici/cigt/tree/prl/prl_scripts.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524001880",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Decoding methods",
      "Information gain",
      "Trellis (graph)"
    ],
    "authors": [
      {
        "surname": "Bicici",
        "given_name": "Ufuk Can"
      },
      {
        "surname": "Meral",
        "given_name": "Tuna Han Salih"
      },
      {
        "surname": "Akarun",
        "given_name": "Lale"
      }
    ]
  },
  {
    "title": "Neighbors selective Graph Convolutional Network for homophily and heterophily",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.06.001",
    "abstract": "Graph Convolutional Networks (GCNs) gain remarkable success in graph-related tasks under homophily graph assumption—most connected nodes have the same label. However, this assumption is fragile since heterophily is common in real-world networks, where most linked nodes have different labels. Some methods address heterophily using graph-level weighted combinations of multi-hop neighbor representations. However, they introduce noise and irrelevant information from two aspects: (1) higher-order neighborhoods may include more neighbors with different labels than those with the same label; (2) graph-level weighted combination fails to capture distinct node properties. To address these issues, firstly, a Neighborhood Distribution-induced Similarity (NDS) measure is developed to identify potential similar neighbors for nodes. Secondly, a node-level fusion mechanism Selective-Neighbors Gated Unit (SNGU) is designed to adaptively aggregate potential neighbors, first-hop neighbors, and the node itself by learning distinct weights for nodes. By combining the above two designs, this work proposes a novel Neighbors Selective Graph Convolutional Network (NSGCN), which allows nodes to selectively receive relevant neighbor information for better node representations, effectively modeling homophily and heterophily. Experiments on 10 widely used real-world datasets with varying properties demonstrate the superiority of the NSGCN, surpassing a strong baseline with an absolute increase of 9.83% in classification accuracy. The code is available at https://github.com/GGA23/NSGCN.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552400165X",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Graph",
      "Homophily",
      "Mathematics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Ai",
        "given_name": "Guoguo"
      },
      {
        "surname": "Gao",
        "given_name": "Yuan"
      },
      {
        "surname": "Wang",
        "given_name": "Huan"
      },
      {
        "surname": "Li",
        "given_name": "Xin"
      },
      {
        "surname": "Wang",
        "given_name": "Jin"
      },
      {
        "surname": "Yan",
        "given_name": "Hui"
      }
    ]
  },
  {
    "title": "Weight Saliency search with Semantic Constraint for Neural Machine Translation attacks",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.05.018",
    "abstract": "Text adversarial attack is an effective way to improve the robustness of Neural Machine Translation (NMT) models. Existing NMT attack tasks are often completed by replacing words. However, most of previous works pursue a high attack success rate but produce semantic inconsistency sentences, leading to wrong translations even for humans. In this paper, we propose a Weight Saliency search with Semantic Constraint (WSSC) algorithm to make semantic consistency word modifications to the input sentence for black-box NMT attacks. Specifically, our WSSC has two major merits. First, it optimizes the word substitution with a word saliency method, which is helpful to reduce word replacement rate. Second, it constrains the objective function with a semantic similarity loss, ensuring every modification does not lead to significant semantic changes. We evaluate the effectiveness of the proposed WSSC by attacking three popular NMT models, i.e., T5, Marian, and BART, on three widely used datasets, i.e., WMT14, WMT16, and TED. Experimental results validate that our WSSC improves Attack Success Rate (ASR) by 4.02% and Semantic Similarity score (USE) by 1.28% on average. Besides, our WSSC also shows good properties in keeping grammar correctness and transfer attack.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524001624",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Constraint (computer-aided design)",
      "Gene",
      "Geometry",
      "Machine learning",
      "Machine translation",
      "Mathematics",
      "Messenger RNA",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Theoretical computer science",
      "Translation (biology)"
    ],
    "authors": [
      {
        "surname": "Han",
        "given_name": "Wen"
      },
      {
        "surname": "Yang",
        "given_name": "Xinghao"
      },
      {
        "surname": "Liu",
        "given_name": "Baodi"
      },
      {
        "surname": "Zhang",
        "given_name": "Kai"
      },
      {
        "surname": "Liu",
        "given_name": "Weifeng"
      }
    ]
  },
  {
    "title": "CoReFace: Sample-guided Contrastive Regularization for Deep Face Recognition",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110483",
    "abstract": "The discriminability of the feature representation is crucial for face recognition. However, previous methods rely solely on the learnable weights of the classification layer, which represent the identities. This reliance could be problematic as the evaluation process depends on the similarity between pairs of face images and requires minimal identity information learned during training. As a result, there is an inconsistency between the training and evaluation processes, which can confuse the feature encoder and hinder the effectiveness of identity-based methods. To address this problem, we propose a novel approach namely Contrastive Regularization for Face Recognition (CoReFace), which applies sample-level regularization in feature representation learning. Specifically, we employ sample-guided contrastive learning to directly regularize the training based on the sample-sample relationship and thus align it with the evaluation process. To avoid image quality degradation, we augment the embeddings instead of the images in order to integrate contrastive learning into face recognition. Additionally, we introduce a new contrastive loss function for the regularization of representation distribution. This function incorporates an adaptive margin and a supervised contrastive mask to ensure stable loss values and prevent interference with the identity supervision signals. Finally, we explore new pair-coupling protocols in order to overcome the problem of semantically repetitive signals in contrastive learning. Extensive experiments demonstrate the efficacy and efficiency of our CoReFace approach, which achieves competitive results compared to state-of-the-art methods. Code could be found https://github.com/IsidoreSong/CoreFace here.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324002346",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Facial recognition system",
      "Feature (linguistics)",
      "Feature learning",
      "Linguistics",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Regularization (linguistics)"
    ],
    "authors": [
      {
        "surname": "Song",
        "given_name": "Youzhe"
      },
      {
        "surname": "Wang",
        "given_name": "Feng"
      }
    ]
  },
  {
    "title": "WRD-Net: Water Reflection Detection using a parallel attention transformer",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110467",
    "abstract": "In contrast to symmetry detection, Water Reflection Detection (WRD) is less studied. We treat this topic as a Symmetry Axis Point Prediction task which outputs a set of points by implicitly learning Gaussian heat maps and explicitly learning numerical coordinates. We first collect a new data set, namely, the Water Reflection Scene Data Set (WRSD). Then, we introduce a novel Water Reflection Detection Network, i.e., WRD-Net. This network is built on top of a series of Parallel Attention Vision Transformer blocks with the Atrous Spatial Pyramid (ASP-PAViT) that we deliberately design. Each block captures both the local and global features at multiple scales. To our knowledge, neither the WRSD nor the WRD-Net has been used for water reflection detection before. To derive the axis of symmetry, we perform Principal Component Analysis (PCA) on the points predicted. Experimental results show that the WRD-Net outperforms its counterparts and achieves the true positive rate of 0.823 compared with the human annotation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324002188",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Geometry",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Principal component analysis",
      "Programming language",
      "Quantum mechanics",
      "Reflection (computer programming)",
      "Reflection symmetry",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Dong",
        "given_name": "Huijie"
      },
      {
        "surname": "Qi",
        "given_name": "Hao"
      },
      {
        "surname": "Zhou",
        "given_name": "Huiyu"
      },
      {
        "surname": "Dong",
        "given_name": "Junyu"
      },
      {
        "surname": "Dong",
        "given_name": "Xinghui"
      }
    ]
  },
  {
    "title": "DSR-Diff: Depth map super-resolution with diffusion model",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.06.025",
    "abstract": "Color-guided depth map super-resolution (CDSR) improves the spatial resolution of a low-quality depth map with the corresponding high-quality color map, benefiting various applications such as 3D reconstruction, virtual reality, and augmented reality. While conventional CDSR methods typically rely on convolutional neural networks or transformers, diffusion models (DMs) have demonstrated notable effectiveness in high-level vision tasks. In this work, we present a novel CDSR paradigm that utilizes a diffusion model within the latent space to generate guidance for depth map super-resolution. The proposed method comprises a guidance generation network (GGN), a depth map super-resolution network (DSRN), and a guidance recovery network (GRN). The GGN is specifically designed to generate the guidance while managing its compactness. Additionally, we integrate a simple but effective feature fusion module and a transformer-style feature extraction module into the DSRN, enabling it to leverage the guiding prior in the extraction, fusion, and reconstruction of multi-model images. Taking into account both accuracy and efficiency, our proposed method has shown superior performance in extensive experiments compared to state-of-the-art methods. Specifically, our proposed DSR-Diff demonstrates remarkable improvements of 18.7% and 15.9% in terms of RMSE at scales 8 and 16 on the NYU v2 dataset while saving nearly 30% of the runtime, compared to existing SOTA method. Our codes will be made available at https://github.com/shiyuan7/DSR-Diff.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524001958",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Cartography",
      "Computer graphics (images)",
      "Computer science",
      "Computer vision",
      "Diffusion",
      "Geography",
      "Geology",
      "Geometry",
      "Mathematics",
      "Physics",
      "Resolution (logic)",
      "Thermodynamics"
    ],
    "authors": [
      {
        "surname": "Shi",
        "given_name": "Yuan"
      },
      {
        "surname": "Cao",
        "given_name": "Huiyun"
      },
      {
        "surname": "Xia",
        "given_name": "Bin"
      },
      {
        "surname": "Zhu",
        "given_name": "Rui"
      },
      {
        "surname": "Liao",
        "given_name": "Qingmin"
      },
      {
        "surname": "Yang",
        "given_name": "Wenming"
      }
    ]
  },
  {
    "title": "MFIFusion: An infrared and visible image enhanced fusion network based on multi-level feature injection",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110445",
    "abstract": "Infrared and visible image fusion aims to integrate complementary information from both types of images. Existing deep learning-based fusion methods rely solely on the final output of the feature extraction network, which may overlook valuable information presented in the middle layers of the network, ultimately reducing the richness of the fusion results, i.e., detailed texture information might not be fully extracted and integrated into the fused image. This study proposes a multi-level feature injection method based on an image decomposition model for infrared and visible image fusion, termed as MFIFusion. On the one hand, we introduce an attention-guided multi-level feature injection module designed to mitigate information loss during the feature extraction stage of the image scale decomposition process. More specifically, the proposed method integrates multiple fusion branches in the encoder network and employs an attention mechanism to guide the feature fusion process. On the other hand, based on the characteristics that superficial features retain image detail information and profound features are more suitable for extracting semantic information from images, we use distinct fusion strategies in these two phases to adaptively control the intensity distribution of the salient targets and to preserve the texture information in the background region. Qualitative and quantitative results demonstrate that our proposed approach produces fused images that are more visually salient to the target and contain richly detailed textures.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324001961",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Encoder",
      "Feature (linguistics)",
      "Feature extraction",
      "Fusion",
      "Fusion rules",
      "Image (mathematics)",
      "Image fusion",
      "Linguistics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Process (computing)",
      "Salient"
    ],
    "authors": [
      {
        "surname": "Dong",
        "given_name": "Aimei"
      },
      {
        "surname": "Wang",
        "given_name": "Long"
      },
      {
        "surname": "Liu",
        "given_name": "Jian"
      },
      {
        "surname": "Lv",
        "given_name": "Guohua"
      },
      {
        "surname": "Zhao",
        "given_name": "Guixin"
      },
      {
        "surname": "Cheng",
        "given_name": "Jinyong"
      }
    ]
  },
  {
    "title": "Flexible image denoising model with multi-layer conditional feature modulation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110372",
    "abstract": "For flexible non-blind image denoising, existing deep networks usually concatenate noisy image and noise level map as the input for handling various noise levels with a single model. However, in this kind of solution, the noise variance (i.e., noise level) is only deployed to modulate the first layer of convolution feature with channel-wise shifting, which is limited in balancing noise removal and detail preservation. In this paper, we present a novel flexible image denoising network (CFMNet) by equipping a U-Net backbone with multi-layer conditional feature modulation (CFM) modules. In comparison to channel-wise shifting only in the first layer, CFMNet can make better use of noise level information by deploying multiple layers of CFM. Moreover, each CFM module takes convolutional features from both noisy image and noise level map as input for better trade-off between noise removal and detail preservation. Experimental results show that our CFMNet is effective in exploiting noise level information for flexible non-blind denoising, and performs favorably against the existing deep image denoising methods in terms of both quantitative metrics and visual quality.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324001237",
    "keywords": [
      "Acoustics",
      "Artificial intelligence",
      "Composite material",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Image (mathematics)",
      "Image denoising",
      "Layer (electronics)",
      "Linguistics",
      "Materials science",
      "Modulation (music)",
      "Noise reduction",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics"
    ],
    "authors": [
      {
        "surname": "Du",
        "given_name": "Jiazhi"
      },
      {
        "surname": "Qiao",
        "given_name": "Xin"
      },
      {
        "surname": "Yan",
        "given_name": "Zifei"
      },
      {
        "surname": "Zhang",
        "given_name": "Hongzhi"
      },
      {
        "surname": "Zuo",
        "given_name": "Wangmeng"
      }
    ]
  },
  {
    "title": "Social domain integrated semantic self-discovery method for recommendation",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.05.015",
    "abstract": "Recommender systems effectively improve the convenience for users to access interesting information from vast amounts of data resources. However, the issue of data sparsity significantly impacts recommendation performance. Heterogeneous information network (HIN), as a form of heterogeneous graph data representation, remarkably enhances recommendation performance. Despite this, current HIN-based recommendations insufficiently consider the social attributes and multidimensional interaction information of users. Additionally, the problem of expert experience dependence on extracting semantic features leads to limited generalization performance. To address these issues, we propose the social domain integrated semantic self-discovery method for recommendation (SSRec). This method constructs a social domain information fusion network to fuse multidimensional interaction information of users in the social domain and designs a multidimensional semantic knowledge mining method to extract semantic knowledge of different meta-paths. We also propose a semantic relationship self-discovery method of heterogeneous nodes that utilizes deep reinforcement learning to address the limited efficiency of traditional manual meta-paths selection. Simulation results show that the SSRec can effectively improve the recommendation performance compared with baseline methods, achieving high-precision recommendations.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524001594",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data science",
      "Domain (mathematical analysis)",
      "Information retrieval",
      "Mathematical analysis",
      "Mathematics",
      "Natural language processing"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Dapeng"
      },
      {
        "surname": "Fan",
        "given_name": "Xiaming"
      },
      {
        "surname": "Zhang",
        "given_name": "Puning"
      },
      {
        "surname": "Fu",
        "given_name": "Miao"
      }
    ]
  },
  {
    "title": "Edge-preserving image restoration based on a weighted anisotropic diffusion model",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.06.007",
    "abstract": "Partial differential equation-based methods have been widely applied in image restoration. The anisotropic diffusion model has a good noise removal capability without affecting significant edges. However, existing anisotropic diffusion-based models closely depend on the diffusion coefficient function and threshold parameter. This paper proposes a new weighted anisotropic diffusion coefficient model with multiple scales, and it has a higher speed of closing to X-axis and exploits adaptive threshold parameters. Meanwhile, the proposed algorithm is verified to be suitable for multiple types of noise. Numerical metrics and visual comparison of simulation experiments show the proposed model has significant superiority in edge-preserving and staircase artifacts reducing over the existing anisotropic diffusion-based techniques.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524001776",
    "keywords": [
      "Algorithm",
      "Anisotropic diffusion",
      "Anisotropy",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Diffusion",
      "Enhanced Data Rates for GSM Evolution",
      "Image (mathematics)",
      "Image processing",
      "Image restoration",
      "Mathematics",
      "Optics",
      "Physics",
      "Thermodynamics"
    ],
    "authors": [
      {
        "surname": "Qi",
        "given_name": "Huiqing"
      },
      {
        "surname": "Li",
        "given_name": "Fang"
      },
      {
        "surname": "Chen",
        "given_name": "Peng"
      },
      {
        "surname": "Tan",
        "given_name": "Shengli"
      },
      {
        "surname": "Luo",
        "given_name": "Xiaoliu"
      },
      {
        "surname": "Xie",
        "given_name": "Ting"
      }
    ]
  },
  {
    "title": "r-FACE: Reference guided face component editing",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110425",
    "abstract": "Although recent studies have made significant processes in face portrait editing, simple and accurate face component editing remains a challenge. Face components, such as eyes, nose, and mouth, have a shape style that is difficult to transfer. Existing methods either (1) manipulate pre-defined binary attribute labels, which is difficult to edit the shape of face components with observable changes, or (2) control the shape change by manually editing the intermediate representation (e.g., precise masks or sketches) of face components, which is time-consuming and requires painting skills. To break these limitations, we propose a simple and effective framework for diverse and controllable face component editing with geometric changes, which utilizes an inpainting model to learn the shape of face components from reference images without any manual annotations. In order to guide generated images to learn the shape style of reference face components, an example-guided attention module is designed to help the network focus on target face component regions. Moreover, a novel domain verification discriminator is introduced to pull the realism of the generated facial component close to the source face. Experimental results demonstrate that the proposed method outperforms conventional methods in image quality, editing accuracy, and diversity of results (see Video Demo).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324001766",
    "keywords": [
      "Artificial intelligence",
      "Component (thermodynamics)",
      "Computer science",
      "Computer vision",
      "Face (sociological concept)",
      "Focus (optics)",
      "Image (mathematics)",
      "Image editing",
      "Inpainting",
      "Optics",
      "Physics",
      "Social science",
      "Sociology",
      "Thermodynamics",
      "Video editing"
    ],
    "authors": [
      {
        "surname": "Deng",
        "given_name": "Qiyao"
      },
      {
        "surname": "Cao",
        "given_name": "Jie"
      },
      {
        "surname": "Liu",
        "given_name": "Yunfan"
      },
      {
        "surname": "Li",
        "given_name": "Qi"
      },
      {
        "surname": "Sun",
        "given_name": "Zhenan"
      }
    ]
  },
  {
    "title": "HDRfeat: A feature-rich network for high dynamic range image reconstruction",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.06.019",
    "abstract": "A major challenge for high dynamic range (HDR) image reconstruction from multi-exposed low dynamic range (LDR) images, especially with dynamic scenes, is the extraction and merging of relevant contextual features in order to suppress any ghosting and blurring artifacts from moving objects. To tackle this, in this work we propose a novel network for HDR image reconstruction with deep and rich feature extraction layers, including residual attention blocks with sequential channel and spatial attention. For the compression of the rich-features to the HDR domain, a residual feature distillation block (RFDB) based architecture is adopted. In contrast to earlier deep-learning methods for HDR, the above contributions shift focus from merging/compression to feature extraction, the added value of which we demonstrate with ablation experiments. We present qualitative and quantitative comparisons on public benchmark datasets, showing that our proposed method outperforms the state-of-the-art. The code is available at: https://github.com/CAiM-lab/HDRfeat.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524001892",
    "keywords": [
      "Aerospace engineering",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Dynamic range",
      "Engineering",
      "Feature (linguistics)",
      "High dynamic range",
      "Image (mathematics)",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Range (aeronautics)"
    ],
    "authors": [
      {
        "surname": "Zhu",
        "given_name": "Lingkai"
      },
      {
        "surname": "Zhou",
        "given_name": "Fei"
      },
      {
        "surname": "Liu",
        "given_name": "Bozhi"
      },
      {
        "surname": "Goksel",
        "given_name": "Orcun"
      }
    ]
  },
  {
    "title": "Mirrored X-Net: Joint classification and contrastive learning for weakly supervised GA segmentation in SD-OCT",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110507",
    "abstract": "Deep learning achieves impressive performance in medical image segmentation, but the training phase requires a large amount of annotated data with precise clinical definitions. Weakly supervised lesion segmentation aims to produce pixel-level masks by learning discriminatory information from weak annotations. In this paper, Mirrored X-Net, a novel segmentation model only supervised by image-level category labels, is proposed to segment the Geographic Atrophy (GA) regions in en-face projection of Spectral-Domain Optical Coherence Tomography (SD-OCT) data. Characterized by the dimension-asymmetric information in SD-OCT images, a novel Anisotropic Downsampling (ADS) is proposed to augment feature shapes. To extract the regions of normal retina for both images with and without lesions, we propose a contrastive learning module to assimilate the deep representation of normal tissue in the SD-OCT images and improve the class-wise difference of deep representation. Based on the contrastive learning module, Anomalous Probability Map (APM) can be obtained to draw the distribution of difference with normal tissue in images. We jointly train the image classification and contrastive learning module, and the final GA segmentation is refined based on the en-face projection of APM. The experimental results on two independent GA datasets demonstrate that the proposed weakly supervised model can produce satisfactory results, and obtain even higher accuracy than fully supervised approaches. The source code is available at https://github.com/maxiao0234/Mirrored-X-Net-pytorch.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324002589",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Deep learning",
      "Feature (linguistics)",
      "Image (mathematics)",
      "Linguistics",
      "Medicine",
      "Ophthalmology",
      "Optical coherence tomography",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Projection (relational algebra)",
      "Segmentation",
      "Upsampling"
    ],
    "authors": [
      {
        "surname": "Ji",
        "given_name": "Zexuan"
      },
      {
        "surname": "Ma",
        "given_name": "Xiao"
      },
      {
        "surname": "Leng",
        "given_name": "Theodore"
      },
      {
        "surname": "Rubin",
        "given_name": "Daniel L."
      },
      {
        "surname": "Chen",
        "given_name": "Qiang"
      }
    ]
  },
  {
    "title": "EXACT: How to train your accuracy",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.06.033",
    "abstract": "Classification tasks are typically evaluated based on accuracy. However, due to the discontinuous nature of accuracy, it cannot be directly optimized using gradient-based methods. The conventional approach involves minimizing surrogate losses such as cross-entropy or hinge loss, which may result in suboptimal performance. In this paper, we introduce a novel optimization technique that incorporates stochasticity into the model’s output and focuses on optimizing the expected accuracy, defined as the accuracy of the stochastic model. Comprehensive experimental evaluations demonstrate that our proposed optimization method significantly enhances performance across various classification tasks, including SVHN, CIFAR-10, CIFAR-100, and ImageNet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524002034",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision"
    ],
    "authors": [
      {
        "surname": "Karpukhin",
        "given_name": "Ivan"
      },
      {
        "surname": "Dereka",
        "given_name": "Stanislav"
      },
      {
        "surname": "Kolesnikov",
        "given_name": "Sergey"
      }
    ]
  },
  {
    "title": "A general framework for implementing distances for categorical variables",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110547",
    "abstract": "The degree to which objects differ from each other with respect to observations on a set of variables, plays an important role in many statistical methods. Many data analysis methods require a quantification of differences in the observed values which we can call distances. An appropriate definition of a distance depends on the nature of the data and the problem at hand. For distances between numerical variables, there exist many definitions that depend on the size of the observed differences. For categorical data, the definition of a distance is more complex as there is no straightforward quantification of the size of the observed differences. In this paper, we introduce a flexible framework for efficiently computing distances between categorical variables, supporting existing and new formulations tailored to specific contexts. In supervised classification, it enhances performance by integrating relationships between response and predictor variables. This framework allows measuring differences among objects across diverse data types and domains.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032400298X",
    "keywords": [
      "Artificial intelligence",
      "Categorical variable",
      "Computer science",
      "Data mining",
      "Data set",
      "Data type",
      "Machine learning",
      "Mathematics",
      "Programming language",
      "Set (abstract data type)",
      "Variables"
    ],
    "authors": [
      {
        "surname": "Velden",
        "given_name": "Michel van de"
      },
      {
        "surname": "D’Enza",
        "given_name": "Alfonso Iodice"
      },
      {
        "surname": "Markos",
        "given_name": "Angelos"
      },
      {
        "surname": "Cavicchia",
        "given_name": "Carlo"
      }
    ]
  },
  {
    "title": "Robust deep fuzzy K -means clustering for image data",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110504",
    "abstract": "Image clustering is a difficult task with important application value in computer vision. The key to this task is the quality of images features. Most of current clustering methods encounter the challenge. That is, the process of feature learning and clustering operates independently. To address this problem, several researchers have been dedicated to performing feature learning and deep clustering together. However, the obtained features lack discriminability to address high-dimensional data successfully. To deal with this issue, we propose a novel model named as robust deep fuzzy K -means clustering (RD-FKC), which efficiently projects image samples into a representative embedding space and precisely learns membership degrees into a combined framework. Specifically, RD-FKC introduces Laplacian regularization technique to preserve locality properties of data. Moreover, by using an adaptive loss function, the model becomes more robust to diverse types of outliers. Furthermore, to avoid the latent space being distorted and make the extracted features retain the original information as much as possible, the model introduces reconstruction error and adds regularization to network parameters. Finally, an effective algorithm is derived to solve the optimization model. Numerous experiments have been conducted, illustrating the advantages and superiority of RD-FKC over existing clustering approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324002553",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Feature (linguistics)",
      "Feature vector",
      "Fuzzy logic",
      "Linguistics",
      "Machine learning",
      "Outlier",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Regularization (linguistics)"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Xiaoling"
      },
      {
        "surname": "Yu",
        "given_name": "Yu-Feng"
      },
      {
        "surname": "Chen",
        "given_name": "Long"
      },
      {
        "surname": "Ding",
        "given_name": "Weiping"
      },
      {
        "surname": "Wang",
        "given_name": "Yingxu"
      }
    ]
  },
  {
    "title": "Towards robust and sparse linear discriminant analysis for image classification",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110512",
    "abstract": "Linear discriminant analysis (LDA) is a popular dimensionality reduction technique that has been widely used in pattern recognition. However, there exist a large number of redundant features and corrupted noise in real-world applications, which makes the performance of existing LDA methods degrade and thus leads to a decrease in classification accuracy. To address the above issues, we propose a novel robust and sparse LDA formulation dubbed RSLDA+. The key idea is introducing the mixed sparse regularization, i.e., ℓ 0 -norm plus ℓ 2 , 0 -norm, for feature representation and enforce ℓ 0 -norm for noise reduction. Furthermore, an optimization algorithm based on the alternating direction method of multipliers (ADMM) is developed in combination with hard thresholding operators. Extensive experiments on six common image datasets verify that the proposed RSLDA+ outperforms state-of-the-art LDA variants in classification accuracy. In addition, the ablation, robustness, convergence, stability, and sparsity are analyzed in detail. The results suggest that the proposed RSLDA+ provides an effective and robust method for image classification.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324002632",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Contextual image classification",
      "Image (mathematics)",
      "Linear discriminant analysis",
      "Mathematics",
      "Optimal discriminant analysis",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Jingjing"
      },
      {
        "surname": "Feng",
        "given_name": "Manlong"
      },
      {
        "surname": "Xiu",
        "given_name": "Xianchao"
      },
      {
        "surname": "Liu",
        "given_name": "Wanquan"
      }
    ]
  },
  {
    "title": "Hybrid federated learning with brain-region attention network for multi-center Alzheimer's disease detection",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110423",
    "abstract": "Identifying reproducible and interpretable biomarkers for Alzheimer's disease (AD) detection remains a challenge. AD detection using multi-center datasets can expand the sample size to improve robustness but might lead to a data privacy problem. Moreover, due to the high cost of labeling data, a lot of unlabeled data in each center is not fully utilized. To address this, a hybrid FL (HFL) framework is proposed that not only uses unlabeled data to train deep learning networks, but also achieves data privacy protection. We propose a novel Brain-region Attention Network (BANet), which highlights important regions via attention to represent the region of interest (ROIs).Specifically, we use a brain template to extract ROI signals from the preprocessed structure magnetic resonance imaging (sMRI) data. In addition, we add a self-supervised loss to the current loss to guide the attention map generation to learn the representations from unlabeled data. Finally, we evaluate our method on a multi-center database which is constructed using five AD datasets. The experimental results show that the proposed method performs better than state-of-the-art methods, achieving mean accuracy rates of 85.69 %, 63.34 %, and 69.89 % on the AD vs. NC, MCI vs. NC, and AD vs. MCI respectively. The source code is available for reproducibility at: https://github.com/yuliangCarmelo/HFL.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324001742",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Code (set theory)",
      "Computer science",
      "Deep learning",
      "Gene",
      "Labeled data",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Programming language",
      "Region of interest",
      "Robustness (evolution)",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Lei",
        "given_name": "Baiying"
      },
      {
        "surname": "Liang",
        "given_name": "Yu"
      },
      {
        "surname": "Xie",
        "given_name": "Jiayi"
      },
      {
        "surname": "Wu",
        "given_name": "You"
      },
      {
        "surname": "Liang",
        "given_name": "Enmin"
      },
      {
        "surname": "Liu",
        "given_name": "Yong"
      },
      {
        "surname": "Yang",
        "given_name": "Peng"
      },
      {
        "surname": "Wang",
        "given_name": "Tianfu"
      },
      {
        "surname": "Liu",
        "given_name": "ChuanMing"
      },
      {
        "surname": "Du",
        "given_name": "Jichen"
      },
      {
        "surname": "Xiao",
        "given_name": "Xiaohua"
      },
      {
        "surname": "Wang",
        "given_name": "Shuqiang"
      }
    ]
  },
  {
    "title": "Enhancing performance of vision transformers on small datasets through local inductive bias incorporation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110510",
    "abstract": "Vision transformers (ViTs) achieve remarkable performance on large datasets, but tend to perform worse than convolutional neural networks (CNNs) when trained from scratch on smaller datasets, possibly due to a lack of local inductive bias in the architecture. Recent studies have therefore added locality to the architecture and demonstrated that it can help ViTs achieve performance comparable to CNNs in the small-size dataset regime. Existing methods, however, are architecture-specific or have higher computational and memory costs. Thus, we propose a module called Local InFormation Enhancer (LIFE) that extracts patch-level local information and incorporates it into the embeddings used in the self-attention block of ViTs. Our proposed module is memory and computation efficient, as well as flexible enough to process auxiliary tokens such as the classification and distillation tokens. Empirical results show that the addition of the LIFE module improves the performance of ViTs on small image classification datasets. We further demonstrate how the effect can be extended to downstream tasks, such as object detection and semantic segmentation. In addition, we introduce a new visualization method, Dense Attention Roll-Out, specifically designed for dense prediction tasks, allowing the generation of class-specific attention maps utilizing the attention maps of all tokens. The code for this project is available on Github (https://github.com/NeurAI-Lab/LIFEhttps://github.com/NeurAI-Lab/LIFE).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324002619",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Electrical engineering",
      "Engineering",
      "Inductive bias",
      "Multi-task learning",
      "Pattern recognition (psychology)",
      "Systems engineering",
      "Task (project management)",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Akkaya",
        "given_name": "Ibrahim Batuhan"
      },
      {
        "surname": "Kathiresan",
        "given_name": "Senthilkumar S."
      },
      {
        "surname": "Arani",
        "given_name": "Elahe"
      },
      {
        "surname": "Zonooz",
        "given_name": "Bahram"
      }
    ]
  },
  {
    "title": "A sharper definition of alignment for Panoptic Quality",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.07.005",
    "abstract": "The Panoptic Quality metric, developed by Kirillov et al. in 2019, makes object-level precision, recall and F1 measures available for evaluating image segmentation, and more generally any partitioning task, against a gold standard. Panoptic Quality is based on partial isomorphisms between hypothesized and true segmentations. Kirillov et al. desire that functions defining these one-to-one matchings should be simple, interpretable and effectively computable. They show that for t and h , true and hypothesized segments, the condition stating that there are more correct than wrongly predicted pixels, formalized as I o U ( t , h ) > . 5 or equivalently as | t ∩ h | > . 5 | t ∪ h | has these properties. We show that a weaker function, requiring that more than half of the pixels in the hypothesized segment are in the true segment and vice-versa, formalized as | t ∩ h | > . 5 | t | and | t ∩ h | > . 5 | h | , is not only sufficient but also necessary. With a small proviso, every function defining a partial isomorphism satisfies this condition. We theoretically and empirically compare the two conditions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524002083",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Chemistry",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Crystal structure",
      "Crystallography",
      "Discrete mathematics",
      "Economics",
      "Epistemology",
      "Evolutionary biology",
      "Function (biology)",
      "Image (mathematics)",
      "Isomorphism (crystallography)",
      "Law",
      "Management",
      "Mathematics",
      "Metric (unit)",
      "Object (grammar)",
      "Operations management",
      "Panopticon",
      "Philosophy",
      "Pixel",
      "Political science",
      "Politics",
      "Quality (philosophy)",
      "Segmentation",
      "Simple (philosophy)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "van Heusden",
        "given_name": "Ruben"
      },
      {
        "surname": "Marx",
        "given_name": "Maarten"
      }
    ]
  },
  {
    "title": "PARDet: Dynamic point set alignment for rotated object detection",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110534",
    "abstract": "Due to the inherent mismatch between rotated objects and horizontal features, feature point misalignment has been a challenge in the task of Rotated Object Detection (ROD). Specifically, considering the pattern of convolution, foreground features are often mixed with background noise, which can confuse the model and affect the model from feature point alignment during the training phase. To mitigate this issue, previous methods concentrate on fixed positions derived from predicted boxes by additionally introducing a refinement stage. However, merely learning fixed position priors during training can result in suboptimal alignment and inefficiency during inference. This paper introduces a dynamic point alignment detector to concurrently address issues associated with feature misalignment and inference inefficiency. The method is made up of two components: the fine-grained points generator (FPG) captures key information, and the point alignment module (PAM) derives precise feature representations. Both modules empower the detector with the capability to dynamically perceive rotated objects, extracting more comprehensive and reasoned feature contents from the feature maps. In general, our method enables the model to independently identify and prioritize the valuable features during the training process. Subsequently, during the inference stage, the results can be directly predicted without additional alignment operations. The experimental results demonstrate that our method can achieve competitive and superior results with average precision (AP) values of 79.33%, 95.73%, and 63.37% on the DOTA, HRSC2016, and DIOR-R datasets, respectively. Codes will be publicly available at https://github.com/Xuyihaoby/PARDet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324002851",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Geometry",
      "Mathematics",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Point (geometry)",
      "Programming language",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Yihao"
      },
      {
        "surname": "Shen",
        "given_name": "Jifeng"
      },
      {
        "surname": "Dai",
        "given_name": "Ming"
      },
      {
        "surname": "Yang",
        "given_name": "Wankou"
      }
    ]
  },
  {
    "title": "Scene recovery: Combining visual enhancement and resolution improvement",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110529",
    "abstract": "Visibility enhancement of outdoor images under complex imaging conditions has been a crucial task for computer vision and received growing attention. However, existing image enhancement methods could result in typical block-like artifacts or color distortion. The undesirable impurities might also be significantly magnified after the enhancement task, further reducing the image quality. For enhancing and super-resolving complex real-world degradation, we propose a simultaneous visual enhancement and resolution improvement (VERI) variational scene recovery model for jointly enhancing image visibility and improving the resolution of the degraded image. Particularly, we estimate the scattering light map for degradation images to achieve clean scene radiance and simultaneously seek a high-quality image through a deep super-resolution network. The semi-proximal alternating direction method of multipliers (sPADMM) algorithm is employed for efficiently solving the minimization problems in the proposed model. Extensive experiments illustrate the effectiveness and robustness of the proposed method in dealing with various scenes, such as haze, sandstorm, underwater or low illumination.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324002802",
    "keywords": [
      "Amplifier",
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Biochemistry",
      "Block (permutation group theory)",
      "Chemistry",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Distortion (music)",
      "Gene",
      "Geometry",
      "Haze",
      "Image (mathematics)",
      "Image enhancement",
      "Image processing",
      "Image quality",
      "Image restoration",
      "Mathematics",
      "Meteorology",
      "Optics",
      "Physics",
      "Robustness (evolution)",
      "Visibility"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Hao"
      },
      {
        "surname": "Qi",
        "given_name": "Te"
      },
      {
        "surname": "Zeng",
        "given_name": "Tieyong"
      }
    ]
  },
  {
    "title": "SurvivalLVQ: Interpretable supervised clustering and prediction in survival analysis via Learning Vector Quantization",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110497",
    "abstract": "Identifying subgroups with similar survival outcomes is a pivotal challenge in survival analysis. Traditional clustering methods often neglect the outcome variable, potentially leading to inaccurate representation of risk profiles. To address this, we present SurvivalLVQ, a novel interpretable method that adapts Learning Vector Quantization (LVQ) to survival analysis. Unlike traditional classification uses of LVQ, SurvivalLVQ groups individuals by survival probabilities and assigns a unique survival curve to each cluster, representing the collective survival behavior within that group. Moreover, it can predict individual survival curves using weighted averages from nearby clusters. When tested on 76 benchmark datasets, it outperformed other clustering methods and showed competitive prediction performance. SurvivalLVQ bridges the gap between clustering techniques and outcome-oriented methods. Its strong clustering performance, coupled with competitive prediction capabilities and with easy to interpret outcomes, make it a promising tool for various applications within survival analysis.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324002486",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Cluster (spacecraft)",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Geodesy",
      "Geography",
      "Learning vector quantization",
      "Machine learning",
      "Mathematical economics",
      "Mathematics",
      "Outcome (game theory)",
      "Pattern recognition (psychology)",
      "Programming language",
      "Statistics",
      "Survival analysis",
      "Vector quantization"
    ],
    "authors": [
      {
        "surname": "de Boer",
        "given_name": "Jasper"
      },
      {
        "surname": "Dedja",
        "given_name": "Klest"
      },
      {
        "surname": "Vens",
        "given_name": "Celine"
      }
    ]
  },
  {
    "title": "A semantic guidance-based fusion network for multi-label image classification",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.08.020",
    "abstract": "Multi-label image classification (MLIC), a fundamental task assigning multiple labels to each image, has been seen notable progress in recent years. Considering simultaneous appearances of objects in the physical world, modeling object correlations is crucial for enhancing classification accuracy. This involves accounting for spatial image feature correlation and label semantic correlation. However, existing methods struggle to establish these correlations due to complex spatial location and label semantic relationships. On the other hand, regarding the fusion of image feature relevance and label semantic relevance, existing methods typically learn a semantic representation in the final CNN layer to combine spatial and label semantic correlations. However, different CNN layers capture features at diverse scales and possess distinct discriminative abilities. To address these issues, in this paper we introduce the Semantic Guidance-Based Fusion Network (SGFN) for MLIC. To model spatial image feature correlation, we leverage the advanced TResNet architecture as the backbone network and employ the Feature Aggregation Module for capturing global spatial correlation. For label semantic correlation, we establish both local and global semantic correlation. We further enrich model features by learning semantic representations across multiple convolutional layers. Our method outperforms current state-of-the-art techniques on PASCAL VOC (2007, 2012) and MS-COCO datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524002526",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Fusion",
      "Image (mathematics)",
      "Image fusion",
      "Linguistics",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Jiuhang"
      },
      {
        "surname": "Tang",
        "given_name": "Hongying"
      },
      {
        "surname": "Luo",
        "given_name": "Shanshan"
      },
      {
        "surname": "Yang",
        "given_name": "Liqi"
      },
      {
        "surname": "Liu",
        "given_name": "Shusheng"
      },
      {
        "surname": "Hong",
        "given_name": "Aoping"
      },
      {
        "surname": "Li",
        "given_name": "Baoqing"
      }
    ]
  },
  {
    "title": "GCNet: Probing self-similarity learning for Generalized Counting Network",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110513",
    "abstract": "The class-agnostic counting (CAC) problem has garnered significant attention recently due to its broad societal applications and formidable challenges. Existing approaches to counting objects of various categories typically rely on user-provided exemplars, which are challenging to obtain and limit their generality. In this paper, our goal is to empower the framework to recognize adaptive exemplars within entire images. To achieve this, we introduce a zero-shot Generalized Counting Network (GCNet), which utilizes a pseudo-Siamese structure to automatically and efficiently learn pseudo exemplar cues from inherent repetition patterns. In addition, a weakly-supervised scheme is presented to reduce the burden of laborious density maps required by all contemporary CAC models, allowing GCNet to be trained using count-level supervisory signals in an end-to-end manner. Without providing any spatial location hints, GCNet is capable of adaptively capturing them through a carefully-designed self-similarity learning strategy. Extensive experiments and ablation studies on the prevailing benchmark FSC147 for zero-shot CAC demonstrate the superiority of our GCNet. It performs on par with existing exemplar-dependent methods and shows stunning cross-dataset generality on crowd-specific datasets, e.g., ShanghaiTech Part A, Part B and UCF_QNRF.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324002644",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Class (philosophy)",
      "Computer science",
      "Generality",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Limit (mathematics)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Psychology",
      "Psychotherapist",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Mingjie"
      },
      {
        "surname": "Li",
        "given_name": "Yande"
      },
      {
        "surname": "Zhou",
        "given_name": "Jun"
      },
      {
        "surname": "Taylor",
        "given_name": "Graham W."
      },
      {
        "surname": "Gong",
        "given_name": "Minglun"
      }
    ]
  },
  {
    "title": "DynamicKD: An effective knowledge distillation via dynamic entropy correction-based distillation for gap optimizing",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110545",
    "abstract": "The knowledge distillation uses a high-performance teacher network to guide the student network. However, the performance gap between the teacher and student networks can affect the student’s training. This paper proposes a novel knowledge distillation algorithm based on dynamic entropy correction, which adjusts the student instead of the teacher to reduce the gap. Firstly, the effect of changing the output entropy (short for output information entropy) on the distillation loss in the student is analyzed in theory. This paper shows that correcting the output entropy can reduce the gap. Then, a knowledge distillation algorithm based on dynamic entropy correction is created, which can correct the output entropy in real-time with an entropy controller updated dynamically by the distillation loss. The proposed algorithm is validated on the CIFAR100, ImageNet, and PASCAL VOC 2007. The comparison with various state-of-the-art distillation algorithms shows impressive results, especially in the experiment on the CIFAR100 regarding teacher–student pair resnet32x4–resnet8x4. The proposed algorithm raises 2.64 points over the traditional distillation algorithm and 0.87 points over the state-of-the-art algorithm CRD in classification accuracy, demonstrating its effectiveness and efficiency.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324002966",
    "keywords": [
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Distillation",
      "Engineering",
      "Entropy (arrow of time)",
      "Physics",
      "Process engineering",
      "Thermodynamics"
    ],
    "authors": [
      {
        "surname": "Zhu",
        "given_name": "Songling"
      },
      {
        "surname": "Shang",
        "given_name": "Ronghua"
      },
      {
        "surname": "Yuan",
        "given_name": "Bo"
      },
      {
        "surname": "Zhang",
        "given_name": "Weitong"
      },
      {
        "surname": "Li",
        "given_name": "Wenjie"
      },
      {
        "surname": "Li",
        "given_name": "Yangyang"
      },
      {
        "surname": "Jiao",
        "given_name": "Licheng"
      }
    ]
  },
  {
    "title": "RFL-CDNet: Towards accurate change detection via richer feature learning",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110515",
    "abstract": "Change Detection is a crucial but extremely challenging task of remote sensing image analysis, and much progress has been made with the rapid development of deep learning. However, most existing deep learning-based change detection methods mainly focus on intricate feature extraction and multi-scale feature fusion, while ignoring the insufficient utilization of features in the intermediate stages, thus resulting in sub-optimal results. To this end, we propose a novel framework, named RFL-CDNet, that utilizes richer feature learning to boost change detection performance. Specifically, we first introduce deep multiple supervision to enhance intermediate representations, thus unleashing the potential of backbone feature extractor at each stage. Furthermore, we design the Coarse-To-Fine Guiding (C2FG) module and the Learnable Fusion (LF) module to further improve feature learning and obtain more discriminative feature representations. The C2FG module aims to seamlessly integrate the side prediction from previous coarse-scale into the current fine-scale prediction in a coarse-to-fine manner, while LF module assumes that the contribution of each stage and each spatial location is independent, thus designing a learnable module to fuse multiple predictions. Experiments on several benchmark datasets show that our proposed RFL-CDNet achieves state-of-the-art performance on WHU cultivated land dataset and CDD dataset, and the second best performance on WHU building dataset. The source code and models are publicly available at https://github.com/Hhaizee/RFL-CDNet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324002668",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Change detection",
      "Code (set theory)",
      "Computer science",
      "Deep learning",
      "Discriminative model",
      "Electrical engineering",
      "Engineering",
      "Extractor",
      "Feature (linguistics)",
      "Feature extraction",
      "Feature learning",
      "Focus (optics)",
      "Fuse (electrical)",
      "Geodesy",
      "Geography",
      "Linguistics",
      "Machine learning",
      "Operating system",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Process engineering",
      "Programming language",
      "Quantum mechanics",
      "Scale (ratio)",
      "Set (abstract data type)",
      "Source code"
    ],
    "authors": [
      {
        "surname": "Gan",
        "given_name": "Yuhang"
      },
      {
        "surname": "Xuan",
        "given_name": "Wenjie"
      },
      {
        "surname": "Chen",
        "given_name": "Hang"
      },
      {
        "surname": "Liu",
        "given_name": "Juhua"
      },
      {
        "surname": "Du",
        "given_name": "Bo"
      }
    ]
  },
  {
    "title": "Feature selection by Universum embedding",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110514",
    "abstract": "Feature selection in classification is an important task in machine learning. Inspired by the success of Universum support vector machine proposed by Weston et al. on improving the classification ability of classical support vector machine, this paper considers a special type of Universum and further lets it play its role in both useful feature identification and separating hyperplane construction, aiming to improve both the feature selection ability and classification performance of Universum support vector machine. By introducing this special Universum, a redundant feature can be identified by observing whether some Universum sample is useful. In fact, we prove that by observing the dual solution of the optimization problem, useful features can be selected from a set satisfying some properties. Due to the introduction of these extra Universum samples, it needs to cope with a large-scale optimization problem. To improve the training efficiency, we modify the sequential minimal optimization algorithm and further combine it with the coordinate descent technique to solve the proposed model. Experimental results on artificial datasets, benchmark datasets, and text classification datasets demonstrate that the proposed method improves the classification performance of support vector machine and Universum support vector machine, and also has good feature selection ability.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324002656",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Feature (linguistics)",
      "Feature selection",
      "Feature vector",
      "Geodesy",
      "Geography",
      "Geometry",
      "Hyperplane",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Relevance vector machine",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Chun-Na"
      },
      {
        "surname": "Huang",
        "given_name": "Ling-Wei"
      },
      {
        "surname": "Shao",
        "given_name": "Yuan-Hai"
      },
      {
        "surname": "Guo",
        "given_name": "Tingting"
      },
      {
        "surname": "Mao",
        "given_name": "Yu"
      }
    ]
  },
  {
    "title": "Masked face recognition using domain adaptation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110574",
    "abstract": "Wearing facial masks has become a must in our daily life due to the global COVID-19 pandemic. However, the performance of a face recognition system is severely degraded due to the fact that the face images in the gallery are unmasked faces while the probe face images captured by the camera are masked faces, making the probe face images different from gallery face images in the activated region and the distribution domain. In this paper, we propose a novel face recognition system to address the issue. The system is integrated with a domain adaptation layer and a feature refinement layer. The feature refinement layer is based on the structure of the self-attention mechanism to align activated regions of unmasked faces with those of masked faces. The domain adaptation layer works by adapting the system from the unmasked face domain to the synthetically masked face domain and the real- world masked face domain. The system is tested on real-world data through face verification and face identification. The face verification accuracy is improved by 6.83% for the RMFD_FV dataset and 4.2% for the MFR2 dataset, and the face identification accuracy is improved by 15.43% for the MFRFI dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032400325X",
    "keywords": [
      "Adaptation (eye)",
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Domain (mathematical analysis)",
      "Face (sociological concept)",
      "Face detection",
      "Facial recognition system",
      "Feature (linguistics)",
      "Identification (biology)",
      "Layer (electronics)",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Neuroscience",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Psychology",
      "Social science",
      "Sociology",
      "Three-dimensional face recognition"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Yu-Chieh"
      },
      {
        "surname": "Rahardjo",
        "given_name": "David Akas Bedjo"
      },
      {
        "surname": "Shiue",
        "given_name": "Ren-Hau"
      },
      {
        "surname": "Chen",
        "given_name": "Homer H."
      }
    ]
  },
  {
    "title": "Boosting sharpness-aware training with dynamic neighborhood",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110496",
    "abstract": "Learning algorithms motivated by minimizing the sharpness of loss surface is a hot research topic in improving generalization. The existing methods usually solve a constrained min–max problem to minimize sharpness and find flat minima. However, most constraints (i.e., the neighborhood of the sharpness) are inappropriate, leading to sub-optimal results. This paper theoretically explores the optimal neighborhood from the view of Probably Approximately Correct-Bayesian (PAC-Bayesian) framework. A closed form of the optimal neighborhood is provided. This neighborhood is determined by the Hessian matrix and the scales of parameters. Then a generalization bound is derived that serves as a guiding principle in the design of the sharpness minimization algorithm. The Dynamic neighborhood-based Sharpness-Aware Minimization algorithm is proposed, which can adaptively adjust the neighborhood during the training process to gain better performance. Also, the algorithm is proved can convergent at the rate O ( log T / T ) . Experimental results demonstrate that the proposed algorithm outperforms the other methods (e.g., accuracy ＋2.86% over baseline on CIFAR-100 for VGG-16).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324002474",
    "keywords": [
      "Artificial intelligence",
      "Boosting (machine learning)",
      "Computer science",
      "Machine learning",
      "Meteorology",
      "Pattern recognition (psychology)",
      "Physics",
      "Training (meteorology)",
      "Training set"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Junhong"
      },
      {
        "surname": "Li",
        "given_name": "Hong"
      },
      {
        "surname": "Chen",
        "given_name": "C.L. Philip"
      }
    ]
  },
  {
    "title": "An efficient training-from-scratch framework with BN-based structural compressor",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110546",
    "abstract": "Channel pruning is an effective way of compressing convolutional neural networks (CNNs) under constrained resources. The current pruning methods follow a progressive pretrain-prune-finetune pipeline, which is inefficient and computationally expensive. In this paper, we bypass the pretrain-prune-finetune pipeline and propose a novel and efficient model training framework based on online channel pruning, which automatically produces a compact well-performed sub-network in one training-from-scratch pass under a given budget condition. Specifically, we introduce a novel BN-based indicator and a sparsity regularization strategy in the early training stage to iteratively and greedily shrink the model layers, which encourages a high-quality architecture with low channel redundancy. To ensure training stability and promote the generalization ability of the resultant pruned network, we also skillfully incorporate a simple self-distillation framework into our training and pruning pipeline. Extensive experiments indicate that our method can effectively achieve competitive performance on the image classification task compared with the state-of-the-arts.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324002978",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Engineering",
      "Gas compressor",
      "Mechanical engineering",
      "Meteorology",
      "Operating system",
      "Physics",
      "Scratch",
      "Training (meteorology)"
    ],
    "authors": [
      {
        "surname": "Hu",
        "given_name": "Fuyi"
      },
      {
        "surname": "Zhang",
        "given_name": "Jin"
      },
      {
        "surname": "Gao",
        "given_name": "Song"
      },
      {
        "surname": "Lin",
        "given_name": "Yu"
      },
      {
        "surname": "Zhou",
        "given_name": "Wei"
      },
      {
        "surname": "Wang",
        "given_name": "Ruxin"
      }
    ]
  },
  {
    "title": "Depth-aware guidance with self-estimated depth representations of diffusion models",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110474",
    "abstract": "Diffusion models have recently shown significant advancement in the generative models with their impressive fidelity and diversity. The success of these models can be often attributed to their use of sampling guidance techniques, such as classifier or classifier-free guidance, which provide effective mechanisms to trade-off between fidelity and diversity. However, these methods are not capable of guiding a generated image to be aware of its geometric configuration, e.g., depth, which hinders their application to downstream tasks such as scene understanding that require a certain level of depth awareness. To overcome this limitation, we propose a novel sampling guidance method for diffusion models that uses self-predicted depth information derived from the rich intermediate representations of diffusion models. Concretely, we first present a label-efficient depth estimation framework using internal representations of diffusion models. Subsequently, we propose the incorporation of two guidance techniques during the sampling phase. These methods involve using pseudo-labeling and depth-domain diffusion prior to self-condition the generated image using the estimated depth map. Experiments and comprehensive ablation studies demonstrate the effectiveness of our method in guiding the diffusion models toward the generation of geometrically plausible images. Our project page is available at https://ku-cvlab.github.io/DAG/.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324002255",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Depth perception",
      "Diffusion",
      "Neuroscience",
      "Perception",
      "Physics",
      "Psychology",
      "Thermodynamics"
    ],
    "authors": [
      {
        "surname": "Kim",
        "given_name": "Gyeongnyeon"
      },
      {
        "surname": "Jang",
        "given_name": "Wooseok"
      },
      {
        "surname": "Lee",
        "given_name": "Gyuseong"
      },
      {
        "surname": "Hong",
        "given_name": "Susung"
      },
      {
        "surname": "Seo",
        "given_name": "Junyoung"
      },
      {
        "surname": "Kim",
        "given_name": "Seungryong"
      }
    ]
  },
  {
    "title": "SIAM: A parameter-free, Spatial Intersection Attention Module",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110509",
    "abstract": "Attention mechanisms have been shown to play a crucial role in enhancing visual perception tasks. However, in most existing approaches, channel and spatial attention maps are estimated separately without considering the varying importance of each other. This results in a coarse attention weight for objects of interest from a holistic 3-D perspective. To address this issue, we propose a novel Parameter-free Spatial Intersection Attention Module (SIAM), which estimates 3D attention maps with spatial intersection using a parameter-free way. Specifically, SIAM first generates two independent mean queries from two spatial axes and views input as keys. Then, by computing a dot product between these mean queries and keys, SIAM generates two cross-dimension (channel and spatial) attention maps from two spatial directions and combines them into 3-D attention maps. By doing so, the produced attention maps reason important areas with spatial intersection, which can capture location-aware information to facilitate difficult objects’ location in the images. We evaluate our method in image classification, object detection, and object segmentation tasks. Extensive experimental results consistently demonstrate our approach is superior to its counterparts.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324002607",
    "keywords": [
      "Artificial intelligence",
      "Cartography",
      "Channel (broadcasting)",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Dimension (graph theory)",
      "Geography",
      "Intersection (aeronautics)",
      "Mathematics",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Perspective (graphical)",
      "Pure mathematics",
      "Segmentation",
      "Spatial analysis",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Han",
        "given_name": "Gaoge"
      },
      {
        "surname": "Huang",
        "given_name": "Shaoli"
      },
      {
        "surname": "Zhao",
        "given_name": "Fang"
      },
      {
        "surname": "Tang",
        "given_name": "Jinglei"
      }
    ]
  },
  {
    "title": "Advancements in point cloud data augmentation for deep learning: A survey",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110532",
    "abstract": "Deep learning (DL) has become one of the mainstream and effective methods for point cloud analysis tasks such as detection, segmentation and classification. To reduce overfitting during training DL models and improve model performance especially when the amount and/or diversity of training data are limited, augmentation is often crucial. Although various point cloud data augmentation methods have been widely used in different point cloud processing tasks, there are currently no published systematic surveys or reviews of these methods. Therefore, this article surveys these methods, categorizing them into a taxonomy framework that comprises basic and specialized point cloud data augmentation methods. Through a comprehensive evaluation of these augmentation methods, this article identifies their potentials and limitations, serving as a useful reference for choosing appropriate augmentation methods. In addition, potential directions for future research are recommended. This survey contributes to providing a holistic overview of the current state of point cloud data augmentation, promoting its wider application and development.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324002838",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Cloud computing",
      "Computer science",
      "Data mining",
      "Data science",
      "Deep learning",
      "Machine learning",
      "Operating system",
      "Overfitting",
      "Point cloud",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Zhu",
        "given_name": "Qinfeng"
      },
      {
        "surname": "Fan",
        "given_name": "Lei"
      },
      {
        "surname": "Weng",
        "given_name": "Ningxin"
      }
    ]
  },
  {
    "title": "Enhancing zero-shot object detection with external knowledge-guided robust contrast learning",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.08.003",
    "abstract": "Zero-shot object detection aims to identify objects from unseen categories not present during training. Existing methods rely on category labels to create pseudo-features for unseen categories, but they face limitations in exploring semantic information and lack robustness. To address these issues, we introduce a novel framework, EKZSD, enhancing zero-shot object detection by incorporating external knowledge and contrastive paradigms. This framework enriches semantic diversity, enhancing discriminative ability and robustness. Specifically, we introduce a novel external knowledge extraction module that leverages attribute and relationship prompts to enrich semantic information. Moreover, a novel external knowledge contrastive learning module is proposed to enhance the model’s discriminative and robust capabilities by exploring pseudo-visual features. Additionally, we use cycle consistency learning to align generated visual features with original semantic features and adversarial learning to align visual features with semantic features. Collaboratively trained with contrast learning loss, cycle consistency loss, adversarial learning loss, and classification loss, our framework outperforms superior performance on the MSCOCO and Ship-43 datasets, as demonstrated in experimental results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524002356",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Consistency (knowledge bases)",
      "Contrast (vision)",
      "Discriminative model",
      "Gene",
      "Machine learning",
      "Object detection",
      "Pattern recognition (psychology)",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Duan",
        "given_name": "Lijuan"
      },
      {
        "surname": "Liu",
        "given_name": "Guangyuan"
      },
      {
        "surname": "En",
        "given_name": "Qing"
      },
      {
        "surname": "Liu",
        "given_name": "Zhaoying"
      },
      {
        "surname": "Gong",
        "given_name": "Zhi"
      },
      {
        "surname": "Ma",
        "given_name": "Bian"
      }
    ]
  },
  {
    "title": "Understanding the vulnerability of skeleton-based Human Activity Recognition via black-box attack",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110564",
    "abstract": "Human Activity Recognition (HAR) has been employed in a wide range of applications, e.g. self-driving cars, where safety and lives are at stake. Recently, the robustness of skeleton-based HAR methods have been questioned due to their vulnerability to adversarial attacks. However, the proposed attacks require the full-knowledge of the attacked classifier, which is overly restrictive. In this paper, we show such threats indeed exist, even when the attacker only has access to the input/output of the model. To this end, we propose the very first black-box adversarial attack approach in skeleton-based HAR called BASAR. BASAR explores the interplay between the classification boundary and the natural motion manifold. To our best knowledge, this is the first time data manifold is introduced in adversarial attacks on time series. Via BASAR, we find on-manifold adversarial samples are extremely deceitful and rather common in skeletal motions, in contrast to the common belief that adversarial samples only exist off-manifold. Through exhaustive evaluation, we show that BASAR can deliver successful attacks across classifiers, datasets, and attack modes. By attack, BASAR helps identify the potential causes of the model vulnerability and provides insights on possible improvements. Finally, to mitigate the newly identified threat, we propose a new adversarial training approach by leveraging the sophisticated distributions of on/off-manifold adversarial samples, called mixed manifold-based adversarial training (MMAT). MMAT can successfully help defend against adversarial attacks without compromising classification accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324003157",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Classifier (UML)",
      "Computer science",
      "Computer security",
      "Engineering",
      "Gene",
      "Machine learning",
      "Manifold (fluid mechanics)",
      "Mechanical engineering",
      "Robustness (evolution)",
      "Vulnerability (computing)"
    ],
    "authors": [
      {
        "surname": "Diao",
        "given_name": "Yunfeng"
      },
      {
        "surname": "Wang",
        "given_name": "He"
      },
      {
        "surname": "Shao",
        "given_name": "Tianjia"
      },
      {
        "surname": "Yang",
        "given_name": "Yongliang"
      },
      {
        "surname": "Zhou",
        "given_name": "Kun"
      },
      {
        "surname": "Hogg",
        "given_name": "David"
      },
      {
        "surname": "Wang",
        "given_name": "Meng"
      }
    ]
  },
  {
    "title": "Composite convolution: A flexible operator for deep learning on 3D point clouds",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110557",
    "abstract": "Deep neural networks require specific layers to process point clouds, as the scattered and irregular location of 3D points prevents the use of conventional convolutional filters. We introduce the composite layer, a flexible and general alternative to the existing convolutional operators that process 3D point clouds. We design our composite layer to extract and compress the spatial information from the 3D coordinates of points and then combine this with the feature vectors. Compared to mainstream point-convolutional layers such as ConvPoint and KPConv, our composite layer guarantees greater flexibility in network design and provides an additional form of regularization. To demonstrate the generality of our composite layers, we define both a convolutional composite layer and an aggregate version that combines spatial information and features in a nonlinear manner, and we use these layers to implement CompositeNets. Our experiments on synthetic and real-world datasets show that, in both classification, segmentation, and anomaly detection, our CompositeNets outperform ConvPoint, which uses the same sequential architecture, and achieve similar results as KPConv, which has a deeper, residual architecture. Moreover, our CompositeNets achieve state-of-the-art performance in anomaly detection on point clouds. Our code is publicly available at https://github.com/sirolf-otrebla/CompositeNet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032400308X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Composite number",
      "Computer science",
      "Convolution (computer science)",
      "Deep learning",
      "Gene",
      "Geometry",
      "Mathematics",
      "Operator (biology)",
      "Point (geometry)",
      "Point cloud",
      "Repressor",
      "Transcription factor"
    ],
    "authors": [
      {
        "surname": "Floris",
        "given_name": "Alberto"
      },
      {
        "surname": "Frittoli",
        "given_name": "Luca"
      },
      {
        "surname": "Carrera",
        "given_name": "Diego"
      },
      {
        "surname": "Boracchi",
        "given_name": "Giacomo"
      }
    ]
  },
  {
    "title": "CSTrans: Correlation-guided Self-Activation Transformer for Counting Everything",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110556",
    "abstract": "Counting everything, also named few-shot counting, requires a model to be able to count objects with any novel (unseen) category giving few exemplar boxes. However, the existing few-shot counting methods are sub-optimal due to weak feature representation, such as the correlation between the exemplar patch and query feature, and contextual dependencies in density map prediction. In this paper, we propose a very simple but effective method, CSTrans, consisting of a Correlation-guided Self-Activation (CSA) module and a Local Dependency Transformer (LDT) module, to mitigate the above two issues, respectively. The CSA utilizes the correlation map to activate the semantic features and suppress the noisy influence of the query features, aiming at mining the potential relation while enriching correlation representation. Furthermore, the LDT incorporates a Transformer to explore local contextual dependencies and predict the density map. Our method achieves competitive performance on FSC-147 and CARPK datasets. We hope its simple implementation and superior performance can serve as a new and strong baseline for few-shot counting tasks and attract more interest in designing simple but effective models in future studies. Our code for CSTrans is available at https://github.com/gaobb/CSTrans.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324003078",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Correlation",
      "Electrical engineering",
      "Engineering",
      "Geometry",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Gao",
        "given_name": "Bin-Bin"
      },
      {
        "surname": "Huang",
        "given_name": "Zhongyi"
      }
    ]
  },
  {
    "title": "Fingerprint membership and identity inference against generative adversarial networks",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.07.018",
    "abstract": "Generative models are gaining significant attention as potential catalysts for a novel industrial revolution. Since automated sample generation can be useful to solve privacy and data scarcity issues that usually affect learned biometric models, such technologies became widely spread in this field. In this paper, we assess the vulnerabilities of generative machine learning models concerning identity protection by designing and testing an identity inference attack on fingerprint datasets created by means of a generative adversarial network. Experimental results show that the proposed solution proves to be effective under different configurations and easily extendable to other biometric measurements.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524002216",
    "keywords": [
      "Adversarial system",
      "Aesthetics",
      "Artificial intelligence",
      "Computer science",
      "Fingerprint (computing)",
      "Generative grammar",
      "Identity (music)",
      "Inference",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Cavasin",
        "given_name": "Saverio"
      },
      {
        "surname": "Mari",
        "given_name": "Daniele"
      },
      {
        "surname": "Milani",
        "given_name": "Simone"
      },
      {
        "surname": "Conti",
        "given_name": "Mauro"
      }
    ]
  },
  {
    "title": "Semantic-aware hyper-space deformable neural radiance fields for facial avatar reconstruction",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.08.004",
    "abstract": "High-fidelity facial avatar reconstruction from monocular videos is a prominent research problem in computer graphics and computer vision. Recent advancements in the Neural Radiance Field (NeRF) have demonstrated remarkable proficiency in rendering novel views and garnered attention for its potential in facial avatar reconstruction. However, previous methodologies have overlooked the complex motion dynamics present across the head, torso, and intricate facial features. Additionally, a deficiency exists in a generalized NeRF-based framework for facial avatar reconstruction adaptable to either 3DMM coefficients or audio input. To tackle these challenges, we propose an innovative framework that leverages semantic-aware hyper-space deformable NeRF, facilitating the reconstruction of high-fidelity facial avatars from either 3DMM coefficients or audio features. Our framework effectively addresses both localized facial movements and broader head and torso motions through semantic guidance and a unified hyper-space deformation module. Specifically, we adopt a dynamic weighted ray sampling strategy to allocate varying degrees of attention to distinct semantic regions, enhancing the deformable NeRF framework with semantic guidance to capture fine-grained details across diverse facial regions. Moreover, we introduce a hyper-space deformation module that enables the transformation of observation space coordinates into canonical hyper-space coordinates, allowing for the learning of natural facial deformation and head-torso movements. Extensive experiments validate the superiority of our framework over existing state-of-the-art methods, demonstrating its effectiveness in producing realistic and expressive facial avatars. Our code is available at https://github.com/jematy/SAHS-Deformable-Nerf.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524002368",
    "keywords": [
      "Anatomy",
      "Animation",
      "Artificial intelligence",
      "Avatar",
      "Computer animation",
      "Computer facial animation",
      "Computer graphics (images)",
      "Computer science",
      "Computer vision",
      "Human–computer interaction",
      "Medicine",
      "Rendering (computer graphics)",
      "Torso"
    ],
    "authors": [
      {
        "surname": "Jin",
        "given_name": "Kaixin"
      },
      {
        "surname": "Gu",
        "given_name": "Xiaoling"
      },
      {
        "surname": "Wang",
        "given_name": "Zimeng"
      },
      {
        "surname": "Kuang",
        "given_name": "Zhenzhong"
      },
      {
        "surname": "Wu",
        "given_name": "Zizhao"
      },
      {
        "surname": "Tan",
        "given_name": "Min"
      },
      {
        "surname": "Yu",
        "given_name": "Jun"
      }
    ]
  },
  {
    "title": "Lit me up: A reference free adaptive low light image enhancement for in-the-wild conditions",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110490",
    "abstract": "Images captured with different devices in uneven conditions (e.g., invariable lighting, low lighting, weather changes, exposure time, etc.) often lead to low image visibility and poor color and contrast, affecting the performance of computer vision and pattern recognition applications. The pre-trained convolutional neural networks (CNNs) solely rely on the training data and lack adaptation due to the uncertainty in the lighting conditions. Moreover, capturing large-scale datasets to train CNNs also raises the computational complexity and overall cost. This work integrates the knowledge and data and proposes a two-stage Uneven-to-Enliven network (U2E-Net), which rapidly learns to see in uneven conditions. A multiple-layered Uneven network learns to distinguish the reflection and illumination in the input images, and an encoder–decoder-based Enliven-Net contextualizes the illumination information. A key component in such ill-posed problems is to obtain information from priors and pairs; however, we present the compelling idea of information trade-off followed by decomposition consistency, thereby progressively improving the visual quality with the subsequent enhancement operations. To this end, we proposed a two-faceted framework that can work without depending on the data type. A novel color and contrast preservation strategy (CPS) is proposed following the decomposition of input data. CPS is integrated within the network to extract contrast in the darkest background regions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324002413",
    "keywords": [
      "Adaptation (eye)",
      "Artificial intelligence",
      "Component (thermodynamics)",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Consistency (knowledge bases)",
      "Contrast (vision)",
      "Convolutional neural network",
      "Encoder",
      "Image (mathematics)",
      "Image quality",
      "Key (lock)",
      "Operating system",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Thermodynamics",
      "Visibility"
    ],
    "authors": [
      {
        "surname": "Khan",
        "given_name": "Rizwan"
      },
      {
        "surname": "Mehmood",
        "given_name": "Atif"
      },
      {
        "surname": "Shahid",
        "given_name": "Farah"
      },
      {
        "surname": "Zheng",
        "given_name": "Zhonglong"
      },
      {
        "surname": "Ibrahim",
        "given_name": "Mostafa M."
      }
    ]
  },
  {
    "title": "OpenInst: A simple query-based method for open-world instance segmentation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110570",
    "abstract": "Open-world instance segmentation has recently gained significant popularity due to its importance in many real-world applications, such as autonomous driving, robot perception, and remote sensing. However, previous methods have either produced unsatisfactory results or relied on complex systems and paradigms. We wonder if there is a simple way to obtain state-of-the-art results. Fortunately, we have identified two observations that help us achieve the best of both worlds: (1) query-based methods demonstrate superiority over dense proposal-based methods in open-world instance segmentation, and (2) learning localization cues is sufficient for open-world instance segmentation. Based on these observations, we propose a simple query-based method named OpenInst for open-world instance segmentation. OpenInst leverages advanced query-based methods like QueryInst and focuses on learning localization cues. Notably, OpenInst is an extremely simple and straightforward framework without any auxiliary modules or post-processing, yet achieves state-of-the-art results on multiple benchmarks. Specifically, in the COCO → UVO scenario, OpenInst achieves a mask Average Recall (AR) of 53.3, outperforming the previous best methods by 2.0 AR with a simpler structure. We hope that OpenInst can serve as a solid baseline for future research in this area. The source codes are available at https://github.com/hustvl/OpenInst.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324003212",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Epistemology",
      "Philosophy",
      "Segmentation",
      "Simple (philosophy)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Cheng"
      },
      {
        "surname": "Wang",
        "given_name": "Guoli"
      },
      {
        "surname": "Zhang",
        "given_name": "Qian"
      },
      {
        "surname": "Guo",
        "given_name": "Peng"
      },
      {
        "surname": "Liu",
        "given_name": "Wenyu"
      },
      {
        "surname": "Wang",
        "given_name": "Xinggang"
      }
    ]
  },
  {
    "title": "Rescaling large datasets based on validation outcomes of a pre-trained network",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.07.001",
    "abstract": "In fact, several categories in a large dataset are not difficult for recent advanced deep neural networks to recognize. Eliminating them for a challenging smaller subset will assist the early network proposals in taking a quick trial of verification. To this end, we propose an efficient rescaling method based on the validation outcomes of a pre-trained model. Firstly, we will take out the sensitive images of the lowest-accuracy classes of the validation outcomes. Each of such images is then considered to identify which label it was confused with. Gathering the lowest-accuracy classes along with the most confused ones can produce a smaller subset with a higher challenge for quick validation of an early network draft. Finally, a rescaling application is introduced to rescale two popular large datasets (ImageNet and Places365) for different tiny subsets (i.e., ReIN Ω and RePL Ω respectively). Experiments for image classification have proved that neural networks obtaining good performance on the original datasets also achieve good results on their rescaled subsets. For instance, MobileNetV1 and MobileNetV2 with 70.6% and 72% on ImageNet respectively obtained 46.53% and 47.47% on its small subset ReIN 30 , which only contains about 39000 images. It can be observed that the better performance of MobileNetV2 on ImageNet correspondingly leads to the better rate on its rescaled subset. Appropriately, utilizing these rescaled sets would help researchers save time and computational costs in the way of designing deep neural architectures. All codes related to the rescaling proposal and the resultant subsets are available at http://github.com/nttbdrk25/ImageNetPlaces365.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524002046",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Data mining",
      "Deep neural networks",
      "Image (mathematics)",
      "Machine learning",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Nguyen",
        "given_name": "Thanh Tuan"
      },
      {
        "surname": "Nguyen",
        "given_name": "Thanh Phuong"
      }
    ]
  },
  {
    "title": "Incremental convolutional transformer for baggage threat detection",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110493",
    "abstract": "Detecting cluttered and overlapping contraband items from baggage scans is one of the most challenging tasks, even for human experts. Recently, considerable literature has grown up around the theme of deep learning-based X-ray screening for localizing contraband data. However, the existing threat detection systems are still vulnerable to high occlusion, clutter, and concealment. Furthermore, they require exhaustive training routines on large-scale and well-annotated data in order to produce accurate results. To overcome the above-mentioned limitations, this paper presents a novel convolutional transformer system that recognizes different overlapping instances of prohibited objects in complex baggage X-ray scans via a distillation-driven incremental instance segmentation scheme. Furthermore, unlike its competitors, the proposed framework allows an incremental integration of new item instances while avoiding costly training routines. In addition to this, the proposed framework also outperforms state-of-the-art approaches by achieving a mean average precision score of 0.7896, 0.5974, and 0.7569 on publicly available GDXray, SIXray, and OPIXray datasets for detecting concealed and cluttered baggage threats.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324002449",
    "keywords": [
      "Artificial intelligence",
      "Clutter",
      "Computer science",
      "Computer vision",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Radar",
      "Segmentation",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Hassan",
        "given_name": "Taimur"
      },
      {
        "surname": "Hassan",
        "given_name": "Bilal"
      },
      {
        "surname": "Owais",
        "given_name": "Muhammad"
      },
      {
        "surname": "Velayudhan",
        "given_name": "Divya"
      },
      {
        "surname": "Dias",
        "given_name": "Jorge"
      },
      {
        "surname": "Ghazal",
        "given_name": "Mohammed"
      },
      {
        "surname": "Werghi",
        "given_name": "Naoufel"
      }
    ]
  },
  {
    "title": "Fall detection algorithm based on global and local feature extraction",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.07.003",
    "abstract": "Falls have become one of the main causes of injury and death among the elderly. A high-accuracy fall detection method can effectively detect falls in the elderly, thereby reducing the probability of injury and mortality. This paper proposes a fall detection algorithm based on global and local feature extraction. Specifically, we design a dual-stream network, with one branch composed of a convolutional neural network and a regional attention module for extracting local features from images. The other branch consists of an improved Transformer for extracting global features from images. The local and global features are then fused using a feature fusion module for classification, enabling fall detection. Experimental results show that the proposed approach achieves accuracies of 99.55% and 99.75% when tested with UP-Fall Detection Dataset and Le2i Fall Detection Dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552400206X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Feature (linguistics)",
      "Feature extraction",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Bin"
      },
      {
        "surname": "Li",
        "given_name": "Jiangjiao"
      },
      {
        "surname": "Wang",
        "given_name": "Peng"
      }
    ]
  },
  {
    "title": "Distributed statistical learning algorithm for nonlinear regression with autoregressive errors",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110551",
    "abstract": "The growing size of modern data brings challenges to statistical learning, and substantial distributed algorithms have been proposed. However, most of them need the homogeneity assumption that the distribution of the local data is the same as that of the global data. This is seldom in practice, and the learning performance deteriorates seriously if this assumption is not satisfied. Moreover, they are only for independent data, and cannot incorporate the serial correlations between data. To solve these issues, we propose a novel distributed statistical learning framework for the nonlinear regression with autoregressive errors, which realizes communication-efficient distributed optimization, and overcomes the homogeneity assumption. The theoretical results also guarantee that the new distributed framework is equivalent to the global one. Numerical experiments also illustrate the good performance of the new method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324003029",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Autoregressive model",
      "Computer science",
      "Machine learning",
      "Mathematics",
      "Nonlinear regression",
      "Nonlinear system",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Regression",
      "Regression analysis",
      "Statistical learning",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Shaomin"
      },
      {
        "surname": "Sun",
        "given_name": "Xiaofei"
      },
      {
        "surname": "Wang",
        "given_name": "Kangning"
      }
    ]
  },
  {
    "title": "HC-MVSNet: A probability sampling-based multi-view-stereo network with hybrid cascade structure for 3D reconstruction",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.07.008",
    "abstract": "Multi-view stereo (MVS) is one of the ways to obtain the 3D structure from 2D images. Deep learning is an effective end-to-end method for MVS. In previous MVS methods based on deep learning, the depth interval is deeply coupled with the feature map resolution, resulting in more accurate depth intervals accompanied by higher computational cost. This paper proposes a new deep neural network HC-MVSNet which utilizes a hybrid cascade structures for depth estimation of MVS. Different from the previous MVS methods, the new coarse-to-fine depth estimation method decouples the two processes of resolution increase and depth interval reduction through a simple operation, achieving higher reconstruction accuracy and completeness for minimal additional computational cost. In addition, an efficient depth sampling strategy based on probability distribution is introduced, which allocates higher hypothesis density for regions with a high probability of ground truth. This novel sampling method makes full use of redundant information that was previously neglected and significantly improves the textural detail of the results. Extensive experiments are conducted on DTU datasets, Tanks and Temples benchmark, and BlendedMVS datasets. The results show that the proposed method exhibits superior performance and better generalization behavior than existing MVS methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524002113",
    "keywords": [
      "Artificial intelligence",
      "Cascade",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Computer vision",
      "Filter (signal processing)",
      "Pattern recognition (psychology)",
      "Sampling (signal processing)"
    ],
    "authors": [
      {
        "surname": "Gao",
        "given_name": "Tianxiang"
      },
      {
        "surname": "Hong",
        "given_name": "Zijian"
      },
      {
        "surname": "Tan",
        "given_name": "Yixing"
      },
      {
        "surname": "Sun",
        "given_name": "Lizhuo"
      },
      {
        "surname": "Wei",
        "given_name": "Yichen"
      },
      {
        "surname": "Ma",
        "given_name": "Jianwei"
      }
    ]
  },
  {
    "title": "Introduction to the special section “Advances trends of pattern recognition for intelligent systems applications” (SS:ISPR23)",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.08.005",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552400237X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Engineering",
      "Engineering physics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Section (typography)",
      "Special section"
    ],
    "authors": [
      {
        "surname": "Bennour",
        "given_name": "Akram"
      },
      {
        "surname": "Ensari",
        "given_name": "Tolga"
      },
      {
        "surname": "Al-Shabi",
        "given_name": "Mohammed"
      }
    ]
  },
  {
    "title": "Towards robust image matching in low-luminance environments: Self-supervised keypoint detection and descriptor-free cross-fusion matching",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110572",
    "abstract": "Image keypoint detection and feature matching are fundamental steps in computer vision tasks. However, variations in environment, time, and viewpoint pose a challenge to the stability of image keypoint detection and matching. Most traditional and deep learning-based methods cannot accurately and efficiently extract highly repeatable keypoints and robust match pairs in low-luminance environments. Therefore, we propose a two-step ‘detection + matching’ framework, which consists of deep neural networks in each step. Firstly, we design a self-supervised robust keypoint detection network, which utilizes multi-scale, multi-angle, and multi-luminance transformation techniques to create pseudo-labeled datasets to improve the model’s keypoint detection repeatability and luminance invariance. Secondly, we propose a descriptor-free cross-fusion matching network, which uses the cross-fusion attention mechanism to establish connections between keypoint-centered image patches and converts the feature-matching task into an image patch assignment task to improve the accuracy and efficiency of matching. Thirdly, the proposed framework is used to replace traditional SIFT in SfM. Experimental results on testing datasets show that the self-supervised robust keypoint detection network achieves higher keypoint repeatability in low luminance environments compared to SIFT, ORB, LIFT, and Superpoint. The descriptor-free cross-fusion matching network’s mean matching accuracy and efficiency are higher than the mainstream Superglue algorithm. Also, SfM achieves better performance regarding the number of sparse point clouds and accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324003236",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Feature extraction",
      "Luminance",
      "Matching (statistics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Scale-invariant feature transform",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Sikang"
      },
      {
        "surname": "Wei",
        "given_name": "Yida"
      },
      {
        "surname": "Wen",
        "given_name": "Zhichao"
      },
      {
        "surname": "Guo",
        "given_name": "Xueli"
      },
      {
        "surname": "Tu",
        "given_name": "Zhigang"
      },
      {
        "surname": "Li",
        "given_name": "You"
      }
    ]
  },
  {
    "title": "DualGroup for 3D instance and panoptic segmentation",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.07.014",
    "abstract": "Existing 3D instance segmentation methods usually learn the offsets (also known as center-shifted vectors) from points to their instance center for clustering and generating segmentation results. However, due to the instances with different scales, direct regression offsets will make the model pay more attention to the larger instances and ignore the smaller instances. Besides, the clustering also may fail because a single bandwidth for point grouping is insufficient for instances with different scales. To address these two problems, we propose a new framework (DualGroup) for 3D instance segmentation. For the first issue, different from directly learning the offsets, we propose an encoded center-shifted vector learning (ECSVL), which effectively compresses the range of the regression center-shifted vectors for more conducive learning of smaller instances. Second, to handle the instances with different scales in clustering, we propose a dual hierarchical grouping (DHG) to better group all points into different instances. The cooperation of these two components leads to the success of indoor instance segmentation. Moreover, the DualGroup is extended to the 3D panoptic segmentation by fusing the semantic predictions and instance results. Experimental results on the ScanNet v2 and S3DIS datasets demonstrate the effectiveness and superiority of the DualGroup.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524002174",
    "keywords": [
      "Artificial intelligence",
      "Computer graphics (images)",
      "Computer science",
      "Computer vision",
      "Law",
      "Panopticon",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Lin"
      },
      {
        "surname": "Chen",
        "given_name": "Sijia"
      },
      {
        "surname": "Tang",
        "given_name": "Xu"
      },
      {
        "surname": "Tao",
        "given_name": "Wenbing"
      }
    ]
  },
  {
    "title": "Focus on informative graphs! Semi-supervised active learning for graph-level classification",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110567",
    "abstract": "Graph-level classification is a critical problem in social analysis and bioinformatics. Since annotated labels are typically costly, we intend to study this challenging task in semi-supervised scenarios with limited budgets. Inspired by the fact that active learning is capable of interactively querying an oracle to annotate a small number of informative examples in the unlabeled dataset, we develop a novel Semi-supervised active learning framework termed GraphSpa for graph-level classification. To make the most of labeling budgets, we propose an effective unlabeled data selection strategy that takes both local similarity and global semantic structure into account. Specifically, we first construct an adaptive queue with labeled samples and select informative samples that have a low degree of similarity to the queue using the Min-Max principle from the local view. Further, we introduce class prototypes and select samples with a large predictive loss discrepancy from the global view. To harness the full potential of unlabeled data, we develop a semi-supervised active learning framework on the basis of our fusion selection strategy coupled with graph contrastive learning during active learning. The effectiveness of our GraphSpa is validated against state-of-the-art methods through experimental results on diverse real-world benchmark datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324003182",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Computer science",
      "Geodesy",
      "Geography",
      "Graph",
      "Leverage (statistics)",
      "Machine learning",
      "Oracle",
      "Semi-supervised learning",
      "Software engineering",
      "Supervised learning",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Ju",
        "given_name": "Wei"
      },
      {
        "surname": "Mao",
        "given_name": "Zhengyang"
      },
      {
        "surname": "Qiao",
        "given_name": "Ziyue"
      },
      {
        "surname": "Qin",
        "given_name": "Yifang"
      },
      {
        "surname": "Yi",
        "given_name": "Siyu"
      },
      {
        "surname": "Xiao",
        "given_name": "Zhiping"
      },
      {
        "surname": "Luo",
        "given_name": "Xiao"
      },
      {
        "surname": "Fu",
        "given_name": "Yanjie"
      },
      {
        "surname": "Zhang",
        "given_name": "Ming"
      }
    ]
  },
  {
    "title": "Recent Advances in Deep Learning Model Security",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.08.018",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524002502",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Deep learning"
    ],
    "authors": [
      {
        "surname": "Feng",
        "given_name": "Guorui"
      },
      {
        "surname": "Li",
        "given_name": "Sheng"
      },
      {
        "surname": "Zhao",
        "given_name": "Jian"
      },
      {
        "surname": "Wang",
        "given_name": "Zheng"
      }
    ]
  },
  {
    "title": "Synthetic unknown class learning for learning unknowns",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110560",
    "abstract": "This paper addresses the open set recognition (OSR) problem, where the goal is to correctly classify samples of known classes while detecting unknown samples to reject. In the OSR problem, “unknown” is assumed to have infinite possibilities because we have no knowledge about unknowns until they emerge. Intuitively, the more an OSR system explores the possibilities of unknowns, the more likely it is to detect unknowns. Even though several generative OSR models have been proposed to explore more by generating synthetic samples and learning them as unknowns, the generated samples are limited to a small subspace of the known classes. Thus, this paper proposes a novel synthetic unknown class learning method that constantly generates unknown-like samples while maintaining diversity between the generated samples. By learning the unknown-like samples and known samples in an alternating manner, the proposed method can not only experience diverse synthetic unknowns but also reduce overgeneralization with respect to known classes. Experiments on several benchmark datasets show that the proposed method significantly outperforms other state-of-the-art approaches by generating diverse realistic unknown samples.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032400311X",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Chemistry",
      "Chromatography",
      "Class (philosophy)",
      "Computer science",
      "Geodesy",
      "Geography",
      "MNIST database",
      "Machine learning",
      "Operating system",
      "Pattern recognition (psychology)",
      "Process (computing)",
      "Programming language",
      "Sample (material)",
      "Set (abstract data type)",
      "Synthetic data"
    ],
    "authors": [
      {
        "surname": "Jang",
        "given_name": "Jaeyeon"
      }
    ]
  },
  {
    "title": "Graph neural collaborative filtering with medical content-aware pre-training for treatment pattern recommendation",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.08.014",
    "abstract": "Recently, considering the advancement of information technology in healthcare, electronic medical records (EMRs) have become the repository of patients’ treatment processes in hospitals, including the patient’s treatment pattern (standard treatment process), the patient’s medical history, the patient’s admission diagnosis, etc. In particular, EMRs-based treatment recommendation systems have become critical for optimizing clinical decision-making. EMRs contain complex relationships between patients and treatment patterns. Recent studies have shown that graph neural collaborative filtering can effectively capture the complex relationships in EMRs. However, none of the existing methods take into account the impact of medical content such as the patient’s admission diagnosis, and medical history on treatment recommendations. In this work, we propose a graph neural collaborative filtering model with medical content-aware pre-training (CAPRec) for learning initial embeddings with medical content to improve recommendation performance. First the model constructs a patient-treatment pattern interaction graph from EMRs data. Then we attempt to use the medical content for pre-training learning and transfer the learned embeddings to a graph neural collaborative filtering model. Finally, the learned initial embedding can support the downstream task of graph collaborative filtering. Extensive experiments on real world datasets have consistently demonstrated the effectiveness of the medical content-aware training framework in improving treatment recommendations.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524002460",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Collaborative filtering",
      "Computer science",
      "Graph",
      "Information retrieval",
      "Machine learning",
      "Meteorology",
      "Pattern recognition (psychology)",
      "Physics",
      "Recommender system",
      "Theoretical computer science",
      "Training (meteorology)"
    ],
    "authors": [
      {
        "surname": "Min",
        "given_name": "Xin"
      },
      {
        "surname": "Li",
        "given_name": "Wei"
      },
      {
        "surname": "Han",
        "given_name": "Ruiqi"
      },
      {
        "surname": "Ji",
        "given_name": "Tianlong"
      },
      {
        "surname": "Xie",
        "given_name": "Weidong"
      }
    ]
  },
  {
    "title": "Dynamic image super-resolution via progressive contrastive self-distillation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110502",
    "abstract": "Convolutional neural networks (CNNs) are highly successful for image super-resolution (SR). However, they often require sophisticated architectures with high memory cost and computational overhead, significantly restricting their practical deployments on resource-limited devices. In this paper, we propose a novel dynamic contrastive self-distillation (Dynamic-CSD) framework to simultaneously compress and accelerate various off-the-shelf SR models, and explore using the trained model for dynamic inference. In particular, to build a compact student network, a channel-splitting super-resolution network (CSSR-Net) can first be constructed from a target teacher network. Then, we propose a novel contrastive loss to improve the quality of SR images via explicit knowledge transfer. Furthermore, progressive CSD (Pro-CSD) is developed to extend the two-branch CSSR-Net into multi-branch, leading to a switchable model at runtime. Finally, a difficulty-aware branch selection strategy for dynamic inference is given. Extensive experiments demonstrate that the proposed Dynamic-CSD scheme effectively compresses and accelerates several standard SR models such as EDSR, RCAN and CARN.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032400253X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Convolutional neural network",
      "Distillation",
      "Image (mathematics)",
      "Inference",
      "Operating system",
      "Organic chemistry",
      "Overhead (engineering)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Zhizhong"
      },
      {
        "surname": "Xie",
        "given_name": "Yuan"
      },
      {
        "surname": "Zhang",
        "given_name": "Chong"
      },
      {
        "surname": "Wang",
        "given_name": "Yanbo"
      },
      {
        "surname": "Qu",
        "given_name": "Yanyun"
      },
      {
        "surname": "Lin",
        "given_name": "Shaohui"
      },
      {
        "surname": "Ma",
        "given_name": "Lizhuang"
      },
      {
        "surname": "Tian",
        "given_name": "Qi"
      }
    ]
  },
  {
    "title": "Weakly privileged learning with knowledge extraction",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110517",
    "abstract": "Learning using privileged information (LUPI) has shown promise in improving supervised learning by embedding additional knowledge. However, its reliance on the assumption of readily available privileged information may not hold true in practical scenarios due to limitations in access or confidentiality. To address these challenges, this paper presents a novel weakly privileged learning (WPL) framework, integrating knowledge extraction methods within the LUPI context. An effective strategy is proposed to implement the WPL framework, where knowledge extraction techniques generate a weight matrix as weak privileged information. Extensive experiments employing various existing knowledge extraction techniques demonstrate that the proposed WPL outperforms traditional supervised learning and approaches the performance of standard privileged learning where privileged information is given in advance. This research establishes WPL as a promising learning paradigm, addressing limitations in privileged information availability and advancing the field of machine learning in practical settings.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324002681",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Extraction (chemistry)",
      "Machine learning",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Fu",
        "given_name": "Saiji"
      },
      {
        "surname": "Dong",
        "given_name": "Tianyi"
      },
      {
        "surname": "Wang",
        "given_name": "Zhaoxin"
      },
      {
        "surname": "Tian",
        "given_name": "Yingjie"
      }
    ]
  },
  {
    "title": "Granular3D: Delving into multi-granularity 3D scene graph prediction",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110562",
    "abstract": "This paper addresses the significant challenges in 3D Semantic Scene Graph (3DSSG) prediction, essential for understanding complex 3D environments. Traditional approaches, primarily using PointNet and Graph Convolutional Networks, struggle with effectively extracting multi-grained features from intricate 3D scenes, largely due to a focus on global scene processing and single-scale feature extraction. To overcome these limitations, we introduce Granular3D, a novel approach that shifts the focus towards multi-granularity analysis by predicting relation triplets from specific sub-scenes. One key is the Adaptive Instance Enveloping Method (AIEM), which establishes an approximate envelope structure around irregular instances, providing shape-adaptive local point cloud sampling, thereby comprehensively covering the contextual environments of instances. Moreover, Granular3D incorporates a Hierarchical Dual-Stage Network (HDSN), which differentiates and processes features of instances and their pairs at varying scales, leading to a targeted prediction of instance categories and their relationships. To advance the perception of sub-scene in HDSN, we design a Gather Point Transformer structure (GaPT) that enables the combinatorial interaction of local information from multiple point cloud sets, achieving a more comprehensive local contextual feature extraction. Extensive evaluations on the challenging 3DSSG benchmark demonstrate that our methods provide substantial improvements, establishing a new state-of-the-art in 3DSSG prediction, boosting the top-50 triplet accuracy by ＋2.8%.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324003133",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Feature extraction",
      "Focus (optics)",
      "Granularity",
      "Graph",
      "Machine learning",
      "Operating system",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Point cloud",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Kaixiang"
      },
      {
        "surname": "Yang",
        "given_name": "Jingru"
      },
      {
        "surname": "Wang",
        "given_name": "Jin"
      },
      {
        "surname": "He",
        "given_name": "Shengfeng"
      },
      {
        "surname": "Wang",
        "given_name": "Zhan"
      },
      {
        "surname": "He",
        "given_name": "Haiyan"
      },
      {
        "surname": "Zhang",
        "given_name": "Qifeng"
      },
      {
        "surname": "Lu",
        "given_name": "Guodong"
      }
    ]
  },
  {
    "title": "Video anomaly detection guided by clustering learning",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110550",
    "abstract": "With the fuzzy boundary between normal and abnormal video data, which cannot be well distinguished by most methods, anomaly detection in video requires better characterization of the data. First we give a convolution-enhanced self-attentive video auto-encoder based on the U-Net architecture, which can extract richer image features. Secondly we design a dual-scale feature clustering structure for this encoder, which simultaneously compresses the channel and spatial structure features of the image to represent the features to obtain good coding characteristics and expand the boundary between normal and abnormal data. We also verify that our approach is equivalent to a class of auto-encoders for memory-guided learning. Finally, in the reconstruction task, since video auto-encoders are capable of triggering temporal time leakage phenomena that can lead to network performance degradation, we propose an anomaly score computation paradigm for video auto-encoders that utilizes the average frame anomaly score of a video clip to compute the first frame anomaly score in that video clip. Extensive experiments on three benchmark datasets show that our method outperforms most existing methods on large datasets with complex patterns. The code will be published at the following link: Anomaly-detection-guided-by-clustering-learning",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324003017",
    "keywords": [
      "Anomaly detection",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Cluster analysis",
      "Computer science",
      "Computer vision",
      "Encoder",
      "Feature (linguistics)",
      "Feature learning",
      "Frame (networking)",
      "Geodesy",
      "Geography",
      "Linguistics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Qiu",
        "given_name": "Shaoming"
      },
      {
        "surname": "Ye",
        "given_name": "Jingfeng"
      },
      {
        "surname": "Zhao",
        "given_name": "Jiancheng"
      },
      {
        "surname": "He",
        "given_name": "Lei"
      },
      {
        "surname": "Liu",
        "given_name": "Liangyu"
      },
      {
        "surname": "E.",
        "given_name": "Bicong"
      },
      {
        "surname": "Huang",
        "given_name": "Xinchen"
      }
    ]
  },
  {
    "title": "Seizure detection via deterministic learning feature extraction",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110466",
    "abstract": "Epileptic seizures have a significant impact on the well-being of a large number of individuals worldwide. Utilizing electroencephalographic (EEG) signals for automatic seizure detection proves to be a valuable solution. However, dealing with raw EEG signals is inherently complex, necessitating a preliminary step of feature extraction prior to detection. Traditional feature extraction methods often amalgamate various types of features for seizure detection, as each type typically captures specific properties. In contrast, this paper focuses on detecting seizures by analyzing the system dynamics. The proposed Deterministic Learning Feature Extraction (DLFE) method extracts a single type of nonlinear dynamical feature rooted in the EEG system dynamics. DLFE employs deterministic learning to discern the inherent system dynamics of the EEG under both seizure and normal conditions. Through the feature extraction process, the infinite-dimensional system dynamics are transformed into feature vectors, exhibiting distinct distributions in seizure and normal states. This disparity can be effectively utilized for classification using standard classifiers. The performance of the proposed seizure detection method was assessed using the CHB-MIT and Bonn datasets. The average classification accuracy was found to be 98.63% with a specificity of 99.19% and a sensitivity of 98.06% on CHB-MIT dataset. Compared with the latest similar methods, the accuracy, specificity and sensitivity are improved by 0.31%, 0.21% and 0.05% respectively. Moreover, the performance was achieved with the short-time interval EEG signals within a few channels. The average classification accuracy was found to be 99.90% with a 0.22% improvement on Bonn dataset, which indicates the good generalization performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324002176",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Extraction (chemistry)",
      "Feature (linguistics)",
      "Feature extraction",
      "Linguistics",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Zirui"
      },
      {
        "surname": "Wu",
        "given_name": "Weiming"
      },
      {
        "surname": "Sun",
        "given_name": "Chen"
      },
      {
        "surname": "Wang",
        "given_name": "Cong"
      }
    ]
  },
  {
    "title": "A ranking-based problem transformation method for weakly supervised multi-label learning",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110505",
    "abstract": "Problem transformation is a simple yet effective framework for multi-label learning, where the original multi-label problem can be transformed into a series of single-label subproblems. However, the existing problem transformation methods have difficulties in handling label defect issues in real applications, e.g. multi-label learning with missing labels, partial multi-label learning and noisy multi-label learning. To deal with these issues, we propose a novel problem transformation method named EPR (i.e., Ensemble of Pairwise Ranking learners) applicable to various multi-label tasks. In EPR, the weakly supervised multi-label problem is converted into an ensemble of supervised single-label patterns due to pairwise label ranking, which successfully enhances label correlation exploration and improves the utilization of instances with defect labels. Moreover, an ensemble pruning mechanism is presented to heuristically balance the model performance and efficiency. Extensive experiments demonstrate the effectiveness of EPR against state-of-the-art algorithms in diverse multi-label learning scenarios.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324002565",
    "keywords": [
      "Agronomy",
      "Artificial intelligence",
      "Biochemistry",
      "Biology",
      "Chemistry",
      "Computer science",
      "Ensemble learning",
      "Gene",
      "Learning to rank",
      "Machine learning",
      "Multi-label classification",
      "Pairwise comparison",
      "Pattern recognition (psychology)",
      "Pruning",
      "Ranking (information retrieval)",
      "Transformation (genetics)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Jiaxuan"
      },
      {
        "surname": "Zhu",
        "given_name": "Xiaoyan"
      },
      {
        "surname": "Zhang",
        "given_name": "Weichu"
      },
      {
        "surname": "Wang",
        "given_name": "Jiayin"
      }
    ]
  },
  {
    "title": "Feature-consistent coplane-pair correspondence- and fusion-based point cloud registration",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.08.001",
    "abstract": "It is an important and challenging task to register two point clouds, and the estimated registration solution can be applied in 3D vision. In this paper, an outlier removal method is first proposed to delete redundant coplane-pair correspondences for constructing three feature-consistent coplane-pair correspondence subsets. Next, Rodrigues’ formula and a scoring-based method are adopted to solve the representative registration solution of each correspondence subset. Then, a robust fusion method is proposed to fuse the three representative solutions as the final registration solution. Based on typical testing datasets, comprehensive experimental results demonstrated that with good registration accuracy, our registration algorithm achieves significant execution time reduction effect when compared with the state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524002332",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Correspondence problem",
      "Economics",
      "Feature (linguistics)",
      "Fusion",
      "Geometry",
      "Image (mathematics)",
      "Image registration",
      "Iterative closest point",
      "Linguistics",
      "Management",
      "Mathematics",
      "Outlier",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Point (geometry)",
      "Point cloud",
      "Point set registration",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Chung",
        "given_name": "Kuo-Liang"
      },
      {
        "surname": "Hsu",
        "given_name": "Chia-Chi"
      },
      {
        "surname": "Hsieh",
        "given_name": "Pei-Hsuan"
      }
    ]
  },
  {
    "title": "SaliencyCut: Augmenting plausible anomalies for anomaly detection",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110508",
    "abstract": "Anomaly detection under the open-set scenario is a challenging task that requires learning discriminative features to detect anomalies that were even unseen during training. As a cheap yet effective approach, data augmentation has been widely used to create pseudo anomalies for better training of such models. Recent wisdom of augmentation methods focuses on generating random pseudo instances that may lead to a mixture of augmented instances with seen anomalies, or out of the typical range of anomalies. To address this issue, we propose a novel saliency-guided data augmentation method, SaliencyCut, to produce pseudo but more common anomalies that tend to stay in the plausible range of anomalies. Furthermore, we deploy a two-head learning strategy consisting of normal and anomaly learning heads to learn the anomaly score of each sample. Theoretical analyses show that this mechanism offers a more tractable and tighter lower bound of the data log-likelihood. We then design a novel patch-wise residual module in the anomaly learning head to extract and assess anomaly features from each sample, facilitating the learning of discriminative representations of anomaly instances. Extensive experiments conducted on six real-world anomaly detection datasets demonstrate the superiority of our method to competing methods under various settings. Codes are available at: https://github.com/yjnanan/SaliencyCut.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324002590",
    "keywords": [
      "Aerospace engineering",
      "Anomaly (physics)",
      "Anomaly detection",
      "Artificial intelligence",
      "Computer science",
      "Condensed matter physics",
      "Data set",
      "Discriminative model",
      "Engineering",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Physics",
      "Programming language",
      "Range (aeronautics)",
      "Set (abstract data type)",
      "Systems engineering",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Ye",
        "given_name": "Jianan"
      },
      {
        "surname": "Hu",
        "given_name": "Yijie"
      },
      {
        "surname": "Yang",
        "given_name": "Xi"
      },
      {
        "surname": "Wang",
        "given_name": "Qiu-Feng"
      },
      {
        "surname": "Huang",
        "given_name": "Chao"
      },
      {
        "surname": "Huang",
        "given_name": "Kaizhu"
      }
    ]
  },
  {
    "title": "Rebalancing network with knowledge stability for class incremental learning",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110506",
    "abstract": "Class incremental learning (CIL) has been proposed to solve the problem of learning to classify new classes while maintaining the performance on old classes. A typical strategy is to update the old classification model with entire new class data and a few old exemplars, which face serious performance degradation on old classes. The main reasons come down to class imbalance between old and new classes along with catastrophic forgetting towards old classes. Most existing CIL methods have proposed solving the above two issues in classification space, ignoring their adverse effects of overlapping between old and new classes and confusion among old classes in feature space. In this paper, we propose a rebalancing network with knowledge stability (RNKS), aiming to adequately retain the model performance on old classes in CIL by solving class imbalance and catastrophic forgetting in feature and classification space simultaneously. In detail, the proposed RNKS mainly consists of multi-proxies rebalancing (MPR) and hybrid knowledge distillation (HKD). MPR, focusing on class imbalance, employs multi-proxies metric learning to decrease the feature overlapping between old and new classes, together with balanced data sampling to correct the skewed decision boundary. HKD, coping with catastrophic forgetting, encourages the updated model to reproduce identical feature topologies and predictions of old classes as the old model via feature relation-based and response-based distillations. Experiments on CIFAR-100 and ILSVRC datasets demonstrate the effectiveness of this work against the state-of-the-art approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324002577",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Machine learning",
      "Stability (learning theory)"
    ],
    "authors": [
      {
        "surname": "Song",
        "given_name": "Jialun"
      },
      {
        "surname": "Chen",
        "given_name": "Jian"
      },
      {
        "surname": "Du",
        "given_name": "Lan"
      }
    ]
  },
  {
    "title": "Re-decoupling the classification branch in object detectors for few-class scenes",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110541",
    "abstract": "Few-class object detection is a critical task in numerous scenes, such as autonomous driving and intelligent surveillance. The current researches mainly focus on the correlation or decoupling between classification and regression subtasks in object detection. However, they rarely take advantage of the potential of re-decoupling the classification subtask. In this paper, we propose to re-decouple the classification branch in object detection for few-class scenes by reducing multi-classification features to multiple binary-classification features. Since multi-classification losses cannot supervise the network to learn decoupled binary-classification features, we introduce a single-class loss to supervise decoupled multiple binary-classification branches. In particular, we propose a basic feature degradation head (FD-Head) structure that decouples the classification branches and applies binary-classification loss to encourage each branch to learn only the degraded single-class features. In addition, based on the mutual exclusion between classes, we propose a mutual exclusion constraint (FD-Head-M) module to constrain the scores of all classes, promoting the detector performance. Finally, we replace the original convolution with more powerful feature extraction modules to form the enhanced FD-Head (FD-Head-E). Notably, our method can be used as a universal module and embedded into the existing object detectors to boost their performance. When applying our method to typical object detectors, it experimentally achieves performance gains of 1.2–2.2%, 1.7–2.5% on the KITTI-3, SeaShips datasets respectively. When using ResNet50 as the backbone network, our method gains an accuracy of 45.9% on the MS COCO dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324002929",
    "keywords": [
      "Arithmetic",
      "Artificial intelligence",
      "Binary classification",
      "Binary number",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Control engineering",
      "Decoupling (probability)",
      "Detector",
      "Engineering",
      "Feature (linguistics)",
      "Feature extraction",
      "Gene",
      "Linguistics",
      "Mathematics",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Robustness (evolution)",
      "Support vector machine",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Hua",
        "given_name": "Jie"
      },
      {
        "surname": "Wang",
        "given_name": "Zhongyuan"
      },
      {
        "surname": "Zou",
        "given_name": "Qin"
      },
      {
        "surname": "Xiao",
        "given_name": "Jinsheng"
      },
      {
        "surname": "Tian",
        "given_name": "Xin"
      },
      {
        "surname": "Zhang",
        "given_name": "Yufei"
      }
    ]
  },
  {
    "title": "Meta-learners for few-shot weakly-supervised medical image segmentation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110471",
    "abstract": "Most uses of Meta-Learning in visual recognition are very often applied to image classification, with a relative lack of work in other tasks such as segmentation and detection. We propose a new generic Meta-Learning framework for few-shot weakly supervised segmentation in medical imaging domains. The proposed approach includes a meta-training phase that uses a meta-dataset. It is deployed on an out-of-distribution few-shot target task, where a single highly generalizable model, trained via a selective supervised loss function, is used as a predictor. The model can be trained in several distinct ways, such as second-order optimization, metric learning, and late fusion. Some relevant improvements of existing methods that are part of the proposed approach are presented. We conduct a comparative analysis of meta-learners from distinct paradigms adapted to few-shot image segmentation in different sparsely annotated radiological tasks. The imaging modalities include 2D chest, mammographic, and dental X-rays, as well as 2D slices of volumetric tomography and resonance images. Our experiments consider in total 9 meta-learners, 4 backbones, and multiple target organ segmentation tasks. We explore small-data scenarios in radiology with varying weak annotation styles and densities. Our analysis shows that metric-based meta-learning approaches achieve better segmentation results in tasks with smaller domain shifts compared to the meta-training datasets, while some gradient- and fusion-based meta-learners are more generalizable to larger domain shifts. Guidelines learned from the comparative performance assessment of the analyzed methods are summarized to support those readers interested in the field.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032400222X",
    "keywords": [
      "Annotation",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer vision",
      "Economics",
      "Image segmentation",
      "Machine learning",
      "Management",
      "Medical imaging",
      "Meta learning (computer science)",
      "Metric (unit)",
      "Modalities",
      "Operations management",
      "Pattern recognition (psychology)",
      "Segmentation",
      "Social science",
      "Sociology",
      "Supervised learning",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Oliveira",
        "given_name": "Hugo"
      },
      {
        "surname": "Gama",
        "given_name": "Pedro H.T."
      },
      {
        "surname": "Bloch",
        "given_name": "Isabelle"
      },
      {
        "surname": "Cesar",
        "given_name": "Roberto Marcondes"
      }
    ]
  },
  {
    "title": "Adversarial self-training for robustness and generalization",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.07.020",
    "abstract": "Adversarial training is currently one of the most promising ways to achieve adversarial robustness of deep models. However, even the most sophisticated training methods is far from satisfactory, as improvement in robustness requires either heuristic strategies or more annotated data, which might be problematic in real-world applications. To alleviate these issues, we propose an effective training scheme that avoids prohibitively high cost of additional labeled data by adapting self-training scheme to adversarial training. In particular, we first use the confident prediction for a randomly-augmented image as the pseudo-label for self-training. Then we enforce the consistency regularization by targeting the adversarially-perturbed version of the same image at the pseudo-label, which implicitly suppresses the distortion of representation in latent space. Despite its simplicity, extensive experiments show that our regularization could bring significant advancement in adversarial robustness of a wide range of adversarial training methods and helps the model to generalize its robustness to larger perturbations or even against unseen adversaries.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552400223X",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Gene",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Regularization (linguistics)",
      "Robustness (evolution)",
      "Training set"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Zhuorong"
      },
      {
        "surname": "Wu",
        "given_name": "Minghui"
      },
      {
        "surname": "Jin",
        "given_name": "Canghong"
      },
      {
        "surname": "Yu",
        "given_name": "Daiwei"
      },
      {
        "surname": "Yu",
        "given_name": "Hongchuan"
      }
    ]
  },
  {
    "title": "Generating neural architectures from parameter spaces for multi-agent reinforcement learning",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.07.013",
    "abstract": "We explore a data-driven approach to generating neural network parameters to determine whether generative models can capture the underlying distribution of a collection of neural network checkpoints. We compile a dataset of checkpoints from neural networks trained within the multi-agent reinforcement learning framework, thus potentially producing previously unseen combinations of neural network parameters. In particular, our generative model is a conditional transformer-based variational autoencoder that, when provided with random noise and a specified performance metric – in our context, returns – predicts the appropriate distribution over the parameter space to achieve the desired performance metric. Our method successfully generates parameters for a specified optimal return without further fine-tuning. We also show that the parameters generated using this approach are more constrained and less variable and, most importantly, perform on par with those trained directly under the multi-agent reinforcement learning framework. We test our method on the neural network architectures commonly employed in the most advanced state-of-the-art algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524002162",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Autoencoder",
      "Computer science",
      "Economics",
      "Machine learning",
      "Metric (unit)",
      "Operations management",
      "Physics",
      "Quantum mechanics",
      "Reinforcement learning",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Artaud",
        "given_name": "Corentin"
      },
      {
        "surname": "De-Silva",
        "given_name": "Varuna"
      },
      {
        "surname": "Pina",
        "given_name": "Rafael"
      },
      {
        "surname": "Shi",
        "given_name": "Xiyu"
      }
    ]
  },
  {
    "title": "Early event detection for facial expression based on infinite mixture prototypes",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110527",
    "abstract": "In early event detection, the ability to detect events at an earlier stage means to have more ample time for processing. The purpose of this study is to apply few-shot learning to early event detection based on infinite mixture prototypes. we present a design to distribute the early event detection issues across the loss function and the data task setting in few-shot learning, and the adoption of infinite mixture models helps to resolve the multi-model distribution of the pattern of early events. The proposed algorithm is evaluated on the CK＋database. The experimental results demonstrate not only feasible performance in terms of accuracy and detection time but also alignment with our hypothesis through the observation of generated clusters.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324002784",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Event (particle physics)",
      "Expression (computer science)",
      "Facial expression",
      "Facial expression recognition",
      "Facial recognition system",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Programming language",
      "Quantum mechanics"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Zhi-Fang"
      },
      {
        "surname": "Chiu",
        "given_name": "Dai-Yi"
      }
    ]
  },
  {
    "title": "Self-reconstruction network for fine-grained few-shot classification",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110485",
    "abstract": "Metric-based methods are one of the most common methods to solve the problem of few-shot image classification. However, traditional metric-based few-shot methods suffer from overfitting and local feature misalignment. The recently proposed feature reconstruction-based approach, which reconstructs query image features from the support set features of a given class and compares the distance between the original query features and the reconstructed query features as the classification criterion, effectively solves the feature misalignment problem. However, the issue of overfitting still has not been considered. To this end, we propose a self-reconstruction metric module for diversifying query features and a restrained cross-entropy loss for avoiding over-confident predictions. By introducing them, the proposed self-reconstruction network can effectively alleviate overfitting. Extensive experiments on five benchmark fine-grained datasets demonstrate that our proposed method achieves state-of-the-art performance on both 5-way 1-shot and 5-way 5-shot classification tasks. Code is available at https://github.com/liz-lut/SRM-main.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032400236X",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Chemistry",
      "Class (philosophy)",
      "Code (set theory)",
      "Computer science",
      "Contextual image classification",
      "Data mining",
      "Economics",
      "Feature (linguistics)",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Linguistics",
      "Machine learning",
      "Metric (unit)",
      "Operating system",
      "Operations management",
      "Organic chemistry",
      "Overfitting",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Set (abstract data type)",
      "Shot (pellet)",
      "Source code"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Xiaoxu"
      },
      {
        "surname": "Li",
        "given_name": "Zhen"
      },
      {
        "surname": "Xie",
        "given_name": "Jiyang"
      },
      {
        "surname": "Yang",
        "given_name": "Xiaochen"
      },
      {
        "surname": "Xue",
        "given_name": "Jing-Hao"
      },
      {
        "surname": "Ma",
        "given_name": "Zhanyu"
      }
    ]
  },
  {
    "title": "DECA-Net: Dual encoder and cross-attention fusion network for surgical instrument segmentation",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.07.019",
    "abstract": "Minimally invasive surgery is now widely used to reduce surgical risks, and automatic and accurate instrument segmentation from endoscope videos is crucial for computer-assisted surgical guidance. However, given the rapid development of CNN-based surgical instrument segmentation methods, challenges like motion blur and illumination issues can still cause erroneous segmentation. In this work, we propose a novel dual encoder and cross-attention network (DECA-Net) to overcome these limitations with enhanced context representation and irrelevant feature fusion. Our approach introduces a CNN and Transformer based dual encoder unit for local features and global context information extraction and hence strength the model’s robustness against various illumination conditions. Then an attention fusion module is utilized to combine local feature and global context information and to select instrument-related boundary features. To bridge the semantic gap between encoder and decoder, we propose a parallel dual cross-attention (DCA) block to capture the channel and spatial dependencies across multi-scale encoder to build the enhanced skip connection. Experimental results show that the proposed method achieves state-of-the-art performance on Endovis2017 and Kvasir-instrument datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524002228",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Dual (grammatical number)",
      "Dual purpose",
      "Encoder",
      "Engineering",
      "Fusion",
      "Linguistics",
      "Literature",
      "Mechanical engineering",
      "Medicine",
      "Operating system",
      "Philosophy",
      "Segmentation",
      "Surgery",
      "Surgical instrument"
    ],
    "authors": [
      {
        "surname": "Liang",
        "given_name": "Sixin"
      },
      {
        "surname": "Zhang",
        "given_name": "Jianzhou"
      },
      {
        "surname": "Bian",
        "given_name": "Ang"
      },
      {
        "surname": "You",
        "given_name": "Jiaying"
      }
    ]
  },
  {
    "title": "Orthogonal subspace exploration for matrix completion",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110456",
    "abstract": "Matrix completion, aiming at restoring a low-rank matrix from observed entries, indicates the connection with the subspace clustering due to the low-rank property. However, it is expensive to incorporate subspace learning into the pervasive surrogate of matrix completion, the nuclear norm. In this paper, we design an orthogonal subspace exploration model for matrix completion, which can be easily integrated due to the succinct formulation. Then, we propose a non-convex surrogate with tractable solutions for low-rank matrix completion, so that the subspace exploration can be performed simultaneously. Compared with the existing surrogates (e.g., nuclear norm, Schatten- p norm, max norm, etc.), the proposed surrogate is differential such that the optimization is still simple even after the subspace exploration is incorporated. Although the surrogate is non-convex, a parameter-free algorithm that is proved to converge into the global optimum is developed. The optimization consists of closed-form solutions so that the orthogonal subspace exploration will not distinctly bring additional costs and the algorithm empirically converges within dozens of iterations. Experiments illustrate the efficiency and superiority of our model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324002073",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Combinatorics",
      "Completion (oil and gas wells)",
      "Composite material",
      "Computer science",
      "Computer vision",
      "Gaussian",
      "Geology",
      "Machine learning",
      "Materials science",
      "Mathematical optimization",
      "Mathematics",
      "Matrix (chemical analysis)",
      "Matrix completion",
      "Orthogonal array",
      "Orthogonal basis",
      "Orthogonal matrix",
      "Petroleum engineering",
      "Physics",
      "Quantum mechanics",
      "Subspace topology",
      "Taguchi methods"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Hongyuan"
      },
      {
        "surname": "Jiao",
        "given_name": "Ziheng"
      },
      {
        "surname": "Li",
        "given_name": "Xuelong"
      }
    ]
  },
  {
    "title": "A survey on handwritten mathematical expression recognition: The rise of encoder-decoder and GNN models",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110531",
    "abstract": "Recognition of handwritten mathematical expressions (HMEs) has attracted growing interest due to steady progress in handwriting recognition techniques and the rapid emergence of pen- and touch-based devices. Math formula recognition may be understood as a generalization of text recognition: formulas represent mathematical statements using a two dimensional arrangement of symbols on writing lines that are organized hierarchically. This survey provides an overview of techniques published in the last decade, including those taking input from handwritten strokes (i.e., ‘online’, as captured by a pen/touch device), raster images (i.e., ‘offline,’ from pixels), or both. Traditionally, HMEs were recognized by performing four structural pattern recognition tasks in separate steps: (1) symbol segmentation, (2) symbol classification, (3) spatial relationship classification, and (4) structural analysis, which identifies the arrangement of symbols on writing lines (e.g., in a Symbol Layout Tree (SLT) or LaTeX string). Recently, encoder–decoder neural network models and Graph Neural Network (GNN) approaches have greatly increased HME recognition accuracy. These newer approaches perform all recognition tasks simultaneously, and utilize contextual features across tasks (e.g., using neural self-attention models). We also discuss evaluation techniques and benchmarks, and explore some implicit dependencies among the four key recognition tasks. Finally, we identify limitations of current systems, and present suggestions for future work, such as using two-dimensional language models rather than the one-dimensional models commonly used in encoder–decoder models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324002826",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Encoder",
      "Generalization",
      "Gesture",
      "Gesture recognition",
      "Handwriting",
      "Mathematical analysis",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Programming language",
      "Segmentation",
      "Sketch recognition",
      "Speech recognition",
      "Symbol (formal)"
    ],
    "authors": [
      {
        "surname": "Truong",
        "given_name": "Thanh-Nghia"
      },
      {
        "surname": "Nguyen",
        "given_name": "Cuong Tuan"
      },
      {
        "surname": "Zanibbi",
        "given_name": "Richard"
      },
      {
        "surname": "Mouchère",
        "given_name": "Harold"
      },
      {
        "surname": "Nakagawa",
        "given_name": "Masaki"
      }
    ]
  },
  {
    "title": "Uncertainty-aware hierarchical labeling for face forgery detection",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110526",
    "abstract": "In this paper, we propose an uncertainty-aware hierarchical labeling method for face forgery detection, which aims to simultaneously explore the traces of forgery contents from a hierarchical level. Unlike existing face forgery detection methods that usually focus on local regions or entire images, our proposed approach takes advantage of hierarchical labeling as auxiliary supervised signals to capture hybrid information from pixel-level, patch-level and image-level. Moreover, we utilize Transformer architecture to extract the patch-level information to build a bridge between the pixel-level and the image-level information. However, the face forgery ground truth always carries data uncertainty due to the existence of the blending step and the compression process during face image manipulation. Thus we introduce an uncertainty learning method to formulate and leverage the data uncertainty. Extensive experimental results on five public datasets demonstrate that our approach not only achieves very competitive performance but also improves the generalization ability.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324002772",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Embedding",
      "Face (sociological concept)",
      "Face detection",
      "Facial recognition system",
      "Generalization",
      "Image (mathematics)",
      "Image editing",
      "Leverage (statistics)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pixel",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Yu",
        "given_name": "Bingyao"
      },
      {
        "surname": "Li",
        "given_name": "Wanhua"
      },
      {
        "surname": "Li",
        "given_name": "Xiu"
      },
      {
        "surname": "Zhou",
        "given_name": "Jie"
      },
      {
        "surname": "Lu",
        "given_name": "Jiwen"
      }
    ]
  },
  {
    "title": "Decoupled and boosted learning for skeleton-based dynamic hand gesture recognition",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110536",
    "abstract": "With the development of cost-effective depth sensors, skeleton-based dynamic hand gesture recognition has made significant progress. Existing methods mostly utilize a single model to learn all spatial–temporal features. Meanwhile, they cannot effectively boost key features and make use of multi-scale features. In this paper, we propose a lightweight dual-stream framework, which consists of a temporal mutual boosted stream (TMB-Stream) and a spatial self-boosted stream (SSB-Stream). In the TMB-Stream, we design a hybrid attention module (HAM) to boost important motion features from temporal sequences, which is composed of a multi-scale multi-head attention module (MMAM) and a spatial–temporal attention module (STAM). In the SSB-Stream, we present a self-boosted learning manner to promote the performance of the spatial stream. Specifically, we design a multi-scale auto-encoder (MAE), which can use limited skeleton data to extract and boost spatial latent features by minimizing the gap between original and reconstructed skeleton images. In addition, we propose a multi-scale fusion module (MFM) to effectively fuse multi-scale features in stages. Experimental results show that our lightweight framework yields satisfactory performance on SHREC’17 Track and DHG-14/28 datasets, as well as very competitive performance on FPHA dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324002875",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Gesture",
      "Gesture recognition",
      "Pattern recognition (psychology)",
      "Programming language",
      "Skeleton (computer programming)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Yangke"
      },
      {
        "surname": "Wei",
        "given_name": "Guangshun"
      },
      {
        "surname": "Desrosiers",
        "given_name": "Christian"
      },
      {
        "surname": "Zhou",
        "given_name": "Yuanfeng"
      }
    ]
  },
  {
    "title": "Affine Collaborative Normalization: A shortcut for adaptation in medical image analysis",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110528",
    "abstract": "The paradigm of “pretraining-then-finetuning” (PT-FT) has been extensively explored to enhance the performance of clinical applications with limited annotations. A major impediment to applying such workflow to various medical imaging modalities is the lack of parameter-free approaches boosting the transferability against notable domain shifts. The success of normalization techniques in domain adaptation provides a promising solution, while inaccessible source data in finetuning (FT) poses new challenges. In this paper, we revisit the Batch Normalization module in PT-FT and propose a unified framework for both fast transferability estimation and transferability-aware finetuning. We discovered that the vanilla FT suffers from the issue that feature patterns of corresponding channels could be misaligned across domains. Hence, we present an Affine Collaborative Normalization (AC-Norm) to dynamically recalibrate the channels in the target model according to the cross-domain channel-wise correlations without any source data and extra parameters. AC-Norm provides very competitive results compared to state-of-the-art FT methods in six classification/segmentation tasks. We also demonstrate the capability of AC-Norm in estimating the transferability of pretrained models in only single-step backpropagation. Our code is available at https://github.com/EndoluminalSurgicalVision-IMR/ACNorm.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324002796",
    "keywords": [
      "Affine transformation",
      "Anthropology",
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Database",
      "Domain adaptation",
      "Executable",
      "Logit",
      "Machine learning",
      "Mathematics",
      "Normalization (sociology)",
      "Pattern recognition (psychology)",
      "Programming language",
      "Pure mathematics",
      "Segmentation",
      "Sociology",
      "Source code",
      "Transferability",
      "Workflow"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Chuyan"
      },
      {
        "surname": "Yang",
        "given_name": "Yuncheng"
      },
      {
        "surname": "Zheng",
        "given_name": "Hao"
      },
      {
        "surname": "Huang",
        "given_name": "Yawen"
      },
      {
        "surname": "Zheng",
        "given_name": "Yefeng"
      },
      {
        "surname": "Gu",
        "given_name": "Yun"
      }
    ]
  },
  {
    "title": "Irregular text block recognition via decoupling visual, linguistic, and positional information",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110516",
    "abstract": "Scene text recognition has made great progress in regular formats, and the recent research has focused on irregular text recognition. In this work, we investigate a new challenge problem of recognizing a text block instance with irregular arrangement of characters, which is referred to as irregular text block recognition (ITBR). This problem is prevalent in daily scenarios, especially with the increasing use of rich text designs in signboards, logos, posters, and other mediums. The primary challenge arises from the weakened position clues and the highly complex reading order, which can often only be deciphered by a heavy reliance on understanding the intrinsic linguistic information. Hence, conventional recognition methods that employ inflexible character grouping rules, coupled with positional information, or constrained by vocabulary reliance, may struggle with the ITBR problem. To this end, we propose a progressive layout reasoning network (PLRN) to recognize the irregular text block by decoupling visual, linguistic, and positional information. PLRN comprises a character spotting module that recognizes the character set based solely on visual features with a new TopK-rank decoding mechanism, and a linkage reasoning module to interpret the character relationships within this set with a progressive refinement strategy. The linkages are initially reasoned by linguistic information and then progressively refined through the incorporation of proximity and tendency information, allowing for explicit decoupling and improved reasoning accuracy. To assess the effectiveness of the proposed method, we construct a new dataset called TextBlock600. This dataset consists of 600 images of irregular text blocks, each with complete sequence annotations. Experimental results demonstrate that PLRN shows promising performance in ITBR, opening up possibilities for further research in this field. Code and datasets will be available at https://github.com/eezyli/PLRN.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032400267X",
    "keywords": [
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Computer science",
      "Construct (python library)",
      "Geometry",
      "Mathematics",
      "Natural language processing",
      "Programming language",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Ziyan"
      },
      {
        "surname": "Jin",
        "given_name": "Lianwen"
      },
      {
        "surname": "Zhang",
        "given_name": "Chengquan"
      },
      {
        "surname": "Zhang",
        "given_name": "Jiaxin"
      },
      {
        "surname": "Xie",
        "given_name": "Zecheng"
      },
      {
        "surname": "Lyu",
        "given_name": "Pengyuan"
      },
      {
        "surname": "Yao",
        "given_name": "Kun"
      }
    ]
  },
  {
    "title": "Brain tumor segmentation in MRI with multi-modality spatial information enhancement and boundary shape correction",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110553",
    "abstract": "Brain tumor segmentation is currently of a priori guiding significance in medical research and clinical diagnosis. Brain tumor segmentation techniques can accurately partition different tumor areas on multi-modality images captured by magnetic resonance imaging (MRI). Due to the unpredictable pathological process of brain tumor generation and growth, brain tumor images often show irregular shapes and uneven internal gray levels. Existing neural network-based segmentation methods with an encoding/decoding structure can perform image segmentation to some extent. However, they ignore issues such as differences in multi-modality information, loss of spatial information, and under-utilization of boundary information, thereby limiting the further improvement of segmentation accuracy. This paper proposes a multimodal spatial information enhancement and boundary shape correction method consisting of a modality information extraction (MIE) module, a spatial information enhancement (SIE) module, and a boundary shape correction (BSC) module. The above three modules act on the input, backbone, and loss functions of deep convolutional networks (DCNN), respectively, and compose an end-to-end 3D brain tumor segmentation model. The three proposed modules can solve the low utilization rate of effective modality information, the insufficient spatial information acquisition ability, and the improper segmentation of key boundary positions can be solved. The proposed method was validated on BraTS2017, 2018, and 2019 datasets. Comparative experimental results confirmed the effectiveness and superiority of the proposed method over state-of-the-art segmentation methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324003042",
    "keywords": [
      "Artificial intelligence",
      "Boundary (topology)",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Image segmentation",
      "Mathematical analysis",
      "Mathematics",
      "Modality (human–computer interaction)",
      "Pattern recognition (psychology)",
      "Segmentation",
      "Spatial analysis",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Zhu",
        "given_name": "Zhiqin"
      },
      {
        "surname": "Wang",
        "given_name": "Ziyu"
      },
      {
        "surname": "Qi",
        "given_name": "Guanqiu"
      },
      {
        "surname": "Mazur",
        "given_name": "Neal"
      },
      {
        "surname": "Yang",
        "given_name": "Pan"
      },
      {
        "surname": "Liu",
        "given_name": "Yu"
      }
    ]
  },
  {
    "title": "Robust pedestrian detection via constructing versatile pedestrian knowledge bank",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110539",
    "abstract": "Pedestrian detection is a crucial field of computer vision research which can be adopted in various real-world applications (e.g., self-driving systems). However, despite noticeable evolution of pedestrian detection, pedestrian representations learned within a detection framework are usually limited to particular scene data in which they were trained. Therefore, in this paper, we propose a novel approach to construct versatile pedestrian knowledge bank containing representative pedestrian knowledge which can be applicable to various detection frameworks and adopted in diverse scenes. We extract generalized pedestrian knowledge from a large-scale pretrained model, and we curate them by quantizing most representative features and guiding them to be distinguishable from background scenes. Finally, we construct versatile pedestrian knowledge bank which is composed of such representations, and then we leverage it to complement and enhance pedestrian features within a pedestrian detection framework. Through comprehensive experiments, we validate the effectiveness of our method, demonstrating its versatility and outperforming state-of-the-art detection performances.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324002905",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Construct (python library)",
      "Engineering",
      "Field (mathematics)",
      "Leverage (statistics)",
      "Machine learning",
      "Mathematics",
      "Pedestrian",
      "Pedestrian detection",
      "Programming language",
      "Pure mathematics",
      "Transport engineering"
    ],
    "authors": [
      {
        "surname": "Park",
        "given_name": "Sungjune"
      },
      {
        "surname": "Kim",
        "given_name": "Hyunjun"
      },
      {
        "surname": "Ro",
        "given_name": "Yong Man"
      }
    ]
  },
  {
    "title": "Semi-supervised vanishing point detection with contrastive learning",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110518",
    "abstract": "Vanishing point (VP) detection in road images plays an important role in driving scenes for advanced driver assistance systems (ADAS) and autonomous vehicles. Because it is still a challenging problem to obtain expensive annotated datasets in deep-learning-based supervised training, in this paper, we propose a semi-supervised VP detection method in road images. Our proposed model first extracts high-resolution VP heatmaps of the road images by fusing multi-level and global–local contrastive learning. Then, we apply the π model as a semi-supervised learning framework and feed the whole global network with a small number of training samples to detect VPs. In the experiment, we used Kong’s dataset for the unstructured road test and the KITTI-VP dataset for the structured road test. Compared with the state-of-the-art methods, our model achieved high accuracy and robustness in road VP detection.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324002693",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Gene",
      "Geometry",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Point (geometry)",
      "Robustness (evolution)",
      "Supervised learning"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Yukun"
      },
      {
        "surname": "Gu",
        "given_name": "Shuo"
      },
      {
        "surname": "Liu",
        "given_name": "Yinbo"
      },
      {
        "surname": "Kong",
        "given_name": "Hui"
      }
    ]
  },
  {
    "title": "A micro Reinforcement Learning architecture for Intrusion Detection Systems",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.07.010",
    "abstract": "This paper proposes an Intrusion Detection System (IDS) that utilizes Deep Reinforcement Learning (DRL) in a fine-grained manner to enhance the performance of binary and multiclass intrusion classification tasks. The proposed system, named Micro Reinforcement Learning Classifier (MRLC), is evaluated using three standard datasets. MRLC architecture utilizes a fine-grained learning approach to enhance IDS accuracy. Simulation studies demonstrate that MRLC has a high efficiency in discriminating different intrusion classes, outperforming state-of-the-art RL-based methods. The average accuracy of MRLC is 99.56%, 99.99%, 99.01% for NSL-KDD, CIC-IDS2018, and UNSW-NB15 datasets respectively. The implementation codes are available at https://github.com/boshradarabi/MICRO-RL-IDS.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524002137",
    "keywords": [
      "Architecture",
      "Art",
      "Artificial intelligence",
      "Computer science",
      "Engineering",
      "Intrusion detection system",
      "Machine learning",
      "Reinforcement",
      "Reinforcement learning",
      "Structural engineering",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Darabi",
        "given_name": "Boshra"
      },
      {
        "surname": "Bag-Mohammadi",
        "given_name": "Mozafar"
      },
      {
        "surname": "Karami",
        "given_name": "Mojtaba"
      }
    ]
  },
  {
    "title": "Memory-Adaptive Vision-and-Language Navigation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110511",
    "abstract": "Vision-and-Language Navigation (VLN) requests an agent to navigate in 3D environments following given instructions, where history is critical for decision-making in dynamic navigation process. Particularly, a memory bank storing histories is widely used in existing methods to incorporate with multimodel representations in current scenes for better decision-making. However, by weighting each history with a simple scalar, those methods cannot purely utilize the informative cues that co-exist with detrimental contents in each history, thereby inevitably introducing noises into decision-making. To that end, we propose a novel Memory-Adaptive Model (MAM) that can dynamically restrain the detrimental contents in histories for retaining contents that benefit navigation only. Specifically, two key modules, Visual and Textual Adaptive Modules, are designed to restrain history noises based on scene-related vision and text, respectively. A Reliability Estimator Module is further introduced to refine above adaptation operations. Our experiments on the widely used RxR and R2R datasets show that MAM outperforms its baseline method by 4.0%/2.5% and 2%/1% on the validation unseen/test split, respectively, wrt the SR metric.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324002620",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Natural language processing"
    ],
    "authors": [
      {
        "surname": "He",
        "given_name": "Keji"
      },
      {
        "surname": "Jing",
        "given_name": "Ya"
      },
      {
        "surname": "Huang",
        "given_name": "Yan"
      },
      {
        "surname": "Lu",
        "given_name": "Zhihe"
      },
      {
        "surname": "An",
        "given_name": "Dong"
      },
      {
        "surname": "Wang",
        "given_name": "Liang"
      }
    ]
  },
  {
    "title": "Sparse and robust support vector machine with capped squared loss for large-scale pattern classification",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110544",
    "abstract": "Support vector machine (SVM), being considered one of the most efficient tools for classification, has received widespread attention in various fields. However, its performance is hindered when dealing with large-scale pattern classification tasks due to high memory requirements and running very slow. To address this challenge, we construct a novel sparse and robust SVM based on our newly proposed capped squared loss (named as L c s l -SVM). To solve L c s l -SVM, we first focus on establishing optimality theory of L c s l -SVM via our defined proximal stationary point, which is convenient for us to efficiently characterize the L c s l support vectors of L c s l -SVM. We subsequently demonstrate that the L c s l support vectors comprise merely a minor fraction of entire training data. This observation leads us to introduce the concept of the working set. Furthermore, we design a novel subspace fast algorithm with working set (named as L c s l -ADMM) for solving L c s l -SVM, which is proven that L c s l -ADMM has both global convergence and relatively low computational complexity. Finally, numerical experiments show that L c s l -ADMM has excellent performances in terms of getting the best classification accuracy, using the shortest time and presenting the smallest numbers of support vectors when solving large-scale pattern classification problems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324002954",
    "keywords": [
      "Artificial intelligence",
      "Cartography",
      "Computer science",
      "Geography",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Relevance vector machine",
      "Scale (ratio)",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Huajun"
      },
      {
        "surname": "Zhang",
        "given_name": "Hongwei"
      },
      {
        "surname": "Li",
        "given_name": "Wenqian"
      }
    ]
  },
  {
    "title": "Self-supervised Domain Adaptation with Significance-Oriented Masking for Pelvic Organ Prolapse detection",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.07.012",
    "abstract": "Pelvic Organ Prolapse (POP) is a common disease in middle-aged and elderly women. The detection of POP is a challenging task, and using deep learning for detection has its practical significance. However, medical image detection tasks always face many problems, i.e., small sample size, data imbalance, and unobvious pathological characteristics. In this paper, we propose a new training framework, called self-supervised Domain Adaptation with Significance-Oriented Masking (DASOM), to address these problems and improve the performance of POP intelligent detection. DASOM includes a new pre-training process based on the masked image modeling task, and redesigns the masking strategy, bringing the local induction capability required for detection to the model. Meanwhile, we also adopt the data process method fitting the pelvic floor ultrasonic dataset to effectively solve the problem of data shortage and imbalance. Extensive experimental results and analysis confirm that the proposed method significantly improves the performance and reliability of POP detection.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524002150",
    "keywords": [
      "Adaptation (eye)",
      "Art",
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Computer vision",
      "Domain (mathematical analysis)",
      "Domain adaptation",
      "Masking (illustration)",
      "Mathematical analysis",
      "Mathematics",
      "Medicine",
      "Neuroscience",
      "Psychology",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Shichang"
      },
      {
        "surname": "Wu",
        "given_name": "Hongjie"
      },
      {
        "surname": "Tang",
        "given_name": "Chenwei"
      },
      {
        "surname": "Chen",
        "given_name": "Dongdong"
      },
      {
        "surname": "Chen",
        "given_name": "Yueyue"
      },
      {
        "surname": "Mei",
        "given_name": "Ling"
      },
      {
        "surname": "Yang",
        "given_name": "Fan"
      },
      {
        "surname": "Lv",
        "given_name": "Jiancheng"
      }
    ]
  },
  {
    "title": "Robust clustering algorithm: The use of soft trimming approach",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.06.032",
    "abstract": "The presence of noise or outliers in data sets may heavily affect the performance of clustering algorithms and lead to unsatisfactory results. The majority of conventional clustering algorithms are sensitive to noise and outliers. Robust clustering algorithms often overcome difficulties associated with noise and outliers and find true cluster structures. We introduce a soft trimming approach for the hard clustering problem where its objective is modeled as a sum of the cluster function and a function represented as a composition of the algebraic and distance functions. We utilize the composite function to estimate the degree of the significance of each data point in clustering. A robust clustering algorithm based on the new model and a procedure for generating starting cluster centers is developed. We demonstrate the performance of the proposed algorithm using some synthetic and real-world data sets containing noise and outliers. We also compare its performance with that of some well-known clustering techniques. Results show that the new algorithm is robust to noise and outliers and finds true cluster structures.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524002022",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "CURE data clustering algorithm",
      "Canopy clustering algorithm",
      "Cluster analysis",
      "Computer science",
      "Correlation clustering",
      "Data mining",
      "Data stream clustering",
      "Determining the number of clusters in a data set",
      "Image (mathematics)",
      "Noise (video)",
      "Operating system",
      "Outlier",
      "Pattern recognition (psychology)",
      "Trimming",
      "k-medians clustering"
    ],
    "authors": [
      {
        "surname": "Taheri",
        "given_name": "Sona"
      },
      {
        "surname": "Bagirov",
        "given_name": "Adil M."
      },
      {
        "surname": "Sultanova",
        "given_name": "Nargiz"
      },
      {
        "surname": "Ordin",
        "given_name": "Burak"
      }
    ]
  },
  {
    "title": "Information filtering and interpolating for semi-supervised graph domain adaptation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110498",
    "abstract": "Graph domain adaptation, which falls under the umbrella of graph transfer learning, involves transferring knowledge from a labeled source graph to improve prediction accuracy on an unlabeled target graph, where both graphs have identical label spaces but exhibit distribution discrepancies due to temporal data shifts or distinct data collection methods. This adaptation is complicated by the challenges of graph-specific domain discrepancies and cross-graph label scarcity. This paper proposes a semi-supervised Graph domain adaptation method via Information Filtering and Interpolating (GIFI). Specifically, GIFI utilizes a parameterized graph reduction module and a variational information bottleneck to adequately filter out irrelevant information from the source and target graphs to eliminate distribution discrepancy. GIFI also introduces an interpolation-enhanced pseudo-labeling strategy for cross-graph semi-supervised learning, which can mitigate model over-fitting on domain-specific features and limited labeled nodes, thus improving the model’s adaptation and discriminative capability. Experimental results on various graph domain adaptation benchmarks demonstrate GIFI’s superior performance over state-of-the-art methods. Our code is available at https://github.com/joe817/GIFI.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324002498",
    "keywords": [
      "Adaptation (eye)",
      "Algorithm",
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Domain (mathematical analysis)",
      "Domain adaptation",
      "Graph",
      "Mathematical analysis",
      "Mathematics",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Psychology",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Qiao",
        "given_name": "Ziyue"
      },
      {
        "surname": "Xiao",
        "given_name": "Meng"
      },
      {
        "surname": "Guo",
        "given_name": "Weiyu"
      },
      {
        "surname": "Luo",
        "given_name": "Xiao"
      },
      {
        "surname": "Xiong",
        "given_name": "Hui"
      }
    ]
  },
  {
    "title": "Joint magnetic resonance imaging artifacts and noise reduction on discrete shape space of images",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110495",
    "abstract": "Magnetic resonance (MR) images can be corrupted by artifacts and noise, potentially leading to misinterpretation of the images. In this paper, we propose a novel approach based on the discrete shape space of images (DSSI) to jointly reduce artifacts and noise in MR images. The proposed method restores MR images in multiple domains based on the distinct generation mechanisms of noise and artifacts. The images in multiple domains are analyzed in a non-Euclidean space. The DSSI is constructed as a Riemannian manifold to measure the intrinsic properties of images. Images are considered shapes from a geometric perspective, and the impact of similarity transformations (e.g., rotation, scaling, and translation) on image analysis is eliminated. The patch-based rank-ordered difference (PROD) detector is defined in k-space within the framework of DSSI to detect and remove sparse outliers that cause artifacts. In addition, a novel similarity function for images is defined using the DSSI and be used to design the improved filter. Finally, the convergence of the improved filter is theoretically analyzed, indicating that our method offers an effective estimator of the ideal image. The experimental results of various MR images demonstrate that the proposed approach outperforms classical and state-of-the-art methods for artifact correction and noise removal, both qualitatively and quantitatively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324002462",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Filter (signal processing)",
      "Image (mathematics)",
      "Mathematics",
      "Noise (video)",
      "Noise reduction",
      "Outlier",
      "Pattern recognition (psychology)",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Xiangyuan"
      },
      {
        "surname": "Wu",
        "given_name": "Zhongke"
      },
      {
        "surname": "Wang",
        "given_name": "Xingce"
      },
      {
        "surname": "Liu",
        "given_name": "Quansheng"
      },
      {
        "surname": "Pozo",
        "given_name": "Jose M."
      },
      {
        "surname": "Frangi",
        "given_name": "Alejandro F."
      }
    ]
  },
  {
    "title": "Dense center-direction regression for object counting and localization with point supervision",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110540",
    "abstract": "Object counting and localization problems are commonly addressed with point-supervised learning, which allows the use of less labor-intensive point annotations. However, learning based on point annotations poses challenges due to the high imbalance between the sets of annotated and unannotated pixels, which is often treated with Gaussian smoothing of point annotations and focal loss. However, these approaches still focus on the pixels in the immediate vicinity of the point annotations and exploit the rest of the data only indirectly. In this work, we propose a novel approach termed CeDiRNet for point-supervised learning that uses a dense regression of directions pointing towards the nearest object centers, i.e. center-directions. This provides greater support for each center point arising from many surrounding pixels pointing towards the object center. We propose a formulation of center-directions that allows the problem to be split into the domain-specific dense regression of center-directions and the final localization task based on a small, lightweight, and domain-agnostic localization network that can be trained with synthetic data completely independent of the target domain. We demonstrate the performance of the proposed method on six different datasets for object counting and localization and show that it outperforms the existing state-of-the-art methods. The code is accessible on GitHub at https://github.com/vicoslab/CeDiRNet.git.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324002917",
    "keywords": [
      "Artificial intelligence",
      "Code (set theory)",
      "Computer science",
      "Computer vision",
      "Domain (mathematical analysis)",
      "Focus (optics)",
      "Geometry",
      "Mathematical analysis",
      "Mathematics",
      "Object (grammar)",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Pixel",
      "Point (geometry)",
      "Programming language",
      "Regression",
      "Set (abstract data type)",
      "Smoothing",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Tabernik",
        "given_name": "Domen"
      },
      {
        "surname": "Muhovič",
        "given_name": "Jon"
      },
      {
        "surname": "Skočaj",
        "given_name": "Danijel"
      }
    ]
  },
  {
    "title": "PSVMLP: Point and Shifted Voxel MLP for 3D deep learning",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.05.016",
    "abstract": "We propose a high-performance 3D feature extraction deep learning network based on point cloud and shifted voxel, named Point and Shifted Voxel MLP (PSVMLP). The main component of PSVMLP is simple Multi-Layer Perceptron (MLP) structure. PSVMLP achieves effective extraction of multi-scale features from 3D data. Specifically, we combine point cloud and voxel-based feature extraction methods. In voxel representation learning, we propose a wide-range geometric feature extraction method based on axial shifting operations and simple MLP structure. The axial shifting operations allow shifting voxels in the depth, height, and width directions, capturing more geometric information. In point cloud representation learning, we use simple MLP structure to extract local features, and we also extract global features by combining transformer structure. By combining point cloud and voxel feature extraction methods, we obtain rich feature representations from different scales, enhancing the model’s expressive power and generalization performance. Applying our designed model to basic geometric feature learning tasks, we achieve excellent results. Despite being built primarily on a simple MLP framework, our model demonstrates remarkable performance on both shape classification and shape part segmentation tasks. Our code is available at https://github.com/hitxraz/psvmlp.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524001600",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Geometry",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Point (geometry)",
      "Voxel"
    ],
    "authors": [
      {
        "surname": "Xie",
        "given_name": "Guanghu"
      },
      {
        "surname": "Liu",
        "given_name": "Yang"
      },
      {
        "surname": "Ji",
        "given_name": "Yiming"
      },
      {
        "surname": "Xie",
        "given_name": "Zongwu"
      },
      {
        "surname": "Cao",
        "given_name": "Baoshi"
      }
    ]
  },
  {
    "title": "Gated Siamese Fusion Network based on multimodal deep and hand-crafted features for personality traits assessment",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.07.004",
    "abstract": "People tend to judge others assessing their personality traits relying on life experience. This fact is especially evident when making an informed hiring decision, which should consider not only skills, but also match a company’s values and culture. Based on this assumption, we use the Siamese Network (SN) for assessing five personality traits by pairwise analyzing and comparing people simultaneously. For this, we propose the OCEAN-AI framework based on Gated Siamese Fusion Network (GSFN), which comprises six modules and enables the fusion of hand-crafted and deep features across three modalities (video, audio, and text). We use the ChaLearn First Impressions v2 (FIv2) and Multimodal Personality Traits Assessment (MuPTA) corpora and identify that all six feature sets and their combinations due to different information content allow the framework to adjust to heterogeneous input data flexibly. The experimental results show that the pairwise comparison of people with the same or different Personality Traits (PT) during the training enhances the proposed framework performance. The framework outperforms the State-of-the-Art (SOTA) systems based on three modalities (video-face, audio and text) by the relative value of 1.3% (0.928 vs. 0.916) in terms of the mean accuracy (mACC) on the FIv2 corpus. We also outperform the SOTA system in terms of the Concordance Correlation Coefficient (CCC) by the relative value of 8.6% (0.667 vs. 0.614) using two modalities (video and audio) on the MuPTA corpus. We make our framework publicly available to integrate it into various applications such as recruitment, education, and healthcare.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524002071",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Deep learning",
      "Fusion",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Personality",
      "Philosophy",
      "Psychology",
      "Social psychology"
    ],
    "authors": [
      {
        "surname": "Ryumina",
        "given_name": "Elena"
      },
      {
        "surname": "Markitantov",
        "given_name": "Maxim"
      },
      {
        "surname": "Ryumin",
        "given_name": "Dmitry"
      },
      {
        "surname": "Karpov",
        "given_name": "Alexey"
      }
    ]
  },
  {
    "title": "CAST: Cross-Modal Retrieval and Visual Conditioning for image captioning",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110555",
    "abstract": "Image captioning requires not only accurate recognition of objects and corresponding relationships, but also full comprehension of the scene information. However, existing models suffer from partial understanding and object hallucination. In this paper, a Cross-modal retrievAl and viSual condiTioning model (CAST) is proposed to address the above issues for image captioning with three key modules: an image–text retriever, an image & memory comprehender and a dual attention decoder. Aiming at a comprehensive understanding, we propose to exploit cross-modal retrieval to mimic human cognition, i.e., to trigger retrieval of contextual information (called episodic memory) about a specific event. Specifically, the image–text retriever searches the top n relevant sentences which serve as episodic memory for each input image. Then the image & memory comprehender encodes an input image and enriches episodic memory by self-attention and relevance attention respectively, which can encourage CAST to comprehend the scene thoroughly and support decoding more effectively. Finally, such image representation and memory are integrated into our dual attention decoder, which performs visual conditioning by re-weighting image and text features to alleviate object hallucination. Extensive experiments are conducted on MS COCO and Flickr30k datasets, which demonstrate that our CAST achieves state-of-the-art performance. Our model also has a promising performance even in low-resource scenarios (i.e. 0.1%, 0.5% and 1% of MS COCO training set).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324003066",
    "keywords": [
      "Artificial intelligence",
      "Closed captioning",
      "Computer science",
      "Computer vision",
      "Image (mathematics)",
      "Image retrieval",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Programming language",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Cao",
        "given_name": "Shan"
      },
      {
        "surname": "An",
        "given_name": "Gaoyun"
      },
      {
        "surname": "Cen",
        "given_name": "Yigang"
      },
      {
        "surname": "Yang",
        "given_name": "Zhaoqilin"
      },
      {
        "surname": "Lin",
        "given_name": "Weisi"
      }
    ]
  },
  {
    "title": "Ta-Adapter: Enhancing few-shot CLIP with task-aware encoders",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110559",
    "abstract": "Contrastive Language-Image Pre-training (CLIP) has shown impressive zero-shot transfer capabilities, but its potential for specific downstream tasks is not fully utilized. To further enhance CLIP’s few-shot capability for specific datasets, some subsequent works have been proposed, such as methods based on lightweight adapters and prompt learning. However, since CLIP is pretrained on a diverse collection of image and text pairs sourced from the internet, it is difficult to sufficiently tune models to specific datasets using only lightweight adaptions. In this paper, we argue that largely modifying the internal representations within CLIP’s encoders can yield better results on downstream datasets. In this work, we introduce Ta-Adapter, a method that equips both the visual and textual encoders of CLIP with task-specific prompts. These prompts are generated using a collaborative prompt learning approach, which allows the encoders to produce representations that are better aligned with specific downstream datasets. Then, we initialize an adapter module using the optimized features generated by the task-aware visual encoder for further feature alignment, and this module can also be further fine-tuned. Our extensive experiments on image classification datasets show that compared to the state-of-the-art few-shot methods Tip-Adapter-F and MaPLe, our model exhibits good performance and obtains an average absolute gain of 2.04% and 1.62% on 11 different image recognition datasets, respectively. In conclusion, this work presents a unique and effective approach to unlocking the full potential of CLIP’s few-shot learning capabilities.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324003108",
    "keywords": [
      "Adapter (computing)",
      "Artificial intelligence",
      "Computer hardware",
      "Computer science",
      "Computer vision",
      "Economics",
      "Encoder",
      "Image (mathematics)",
      "Management",
      "Operating system",
      "Pattern recognition (psychology)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Wenbo"
      },
      {
        "surname": "Zhang",
        "given_name": "Yifan"
      },
      {
        "surname": "Deng",
        "given_name": "Yuyang"
      },
      {
        "surname": "Zhang",
        "given_name": "Wenlong"
      },
      {
        "surname": "Lin",
        "given_name": "Jianfeng"
      },
      {
        "surname": "Huang",
        "given_name": "Binqiang"
      },
      {
        "surname": "Zhang",
        "given_name": "Jinlu"
      },
      {
        "surname": "Yu",
        "given_name": "Wenhao"
      }
    ]
  },
  {
    "title": "Saliency-based video summarization for face anti-spoofing",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.08.008",
    "abstract": "With the growing availability of databases for face presentation attack detection, researchers are increasingly focusing on video-based face anti-spoofing methods that involve hundreds to thousands of images for training the models. However, there is currently no clear consensus on the optimal number of frames in a video to improve face spoofing detection. Inspired by the visual saliency theory, we present a video summarization method for face anti-spoofing detection that aims to enhance the performance and efficiency of deep learning models by leveraging visual saliency. In particular, saliency information is extracted from the differences between the Laplacian and Wiener filter outputs of the source images, enabling the identification of the most visually salient regions within each frame. Subsequently, the source images are decomposed into base and detail images, enhancing the representation of the most important information. Weighting maps are then computed based on the saliency information, indicating the importance of each pixel in the image. By linearly combining the base and detail images using the weighting maps, the method fuses the source images to create a single representative image that summarizes the entire video. The key contribution of the proposed method lies in demonstrating how visual saliency can be used as a data-centric approach to improve the performance and efficiency for face presentation attack detection. By focusing on the most salient images or regions within the images, a more representative and diverse training set can be created, potentially leading to more effective models. To validate the method’s effectiveness, a simple CNN–RNN deep learning architecture was used, and the experimental results showcased state-of-the-art performance on four challenging face anti-spoofing datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552400240X",
    "keywords": [
      "Artificial intelligence",
      "Automatic summarization",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Face (sociological concept)",
      "Pattern recognition (psychology)",
      "Social science",
      "Sociology",
      "Spoofing attack"
    ],
    "authors": [
      {
        "surname": "Muhammad",
        "given_name": "Usman"
      },
      {
        "surname": "Oussalah",
        "given_name": "Mourad"
      },
      {
        "surname": "Laaksonen",
        "given_name": "Jorma"
      }
    ]
  },
  {
    "title": "Points2NeRF: Generating Neural Radiance Fields from 3D point cloud",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.07.002",
    "abstract": "Neural Radiance Fields (NeRFs) offers a state-of-the-art quality in synthesizing novel views of complex 3D scenes from a small subset of base images. For NeRFs to perform optimally, the registration of base images has to follow certain assumptions, including maintaining a constant distance between the camera and the object. We can address this limitation by training NeRFs with 3D point clouds instead of images, yet a straightforward substitution is impossible due to the sparsity of 3D clouds in the under-sampled regions, which leads to incomplete reconstruction output by NeRFs. To solve this problem, here we propose an auto-encoder-based architecture that leverages a hypernetwork paradigm to transfer 3D points with the associated color values through a lower-dimensional latent space and generate weights of NeRF model. This way, we can accommodate the sparsity of 3D point clouds and fully exploit the potential of point cloud data. As a side benefit, our method offers an implicit way of representing 3D scenes and objects that can be employed to condition NeRFs and hence generalize the models beyond objects seen during training. The empirical evaluation confirms the advantages of our method over conventional NeRFs and proves its superiority in practical applications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524002058",
    "keywords": [
      "Artificial intelligence",
      "Cloud computing",
      "Computer graphics (images)",
      "Computer science",
      "Computer vision",
      "Geology",
      "Geometry",
      "Mathematics",
      "Operating system",
      "Point (geometry)",
      "Point cloud",
      "Radiance",
      "Remote sensing"
    ],
    "authors": [
      {
        "surname": "Zimny",
        "given_name": "Dominik"
      },
      {
        "surname": "Waczyńska",
        "given_name": "Joanna"
      },
      {
        "surname": "Trzciński",
        "given_name": "Tomasz"
      },
      {
        "surname": "Spurek",
        "given_name": "Przemysław"
      }
    ]
  },
  {
    "title": "Swin-chart: An efficient approach for chart classification",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.08.012",
    "abstract": "Charts are a visualization tool used in scientific documents to facilitate easy comprehension of complex relationships underlying data and experiments. Researchers use various chart types to convey scientific information, so the problem of data extraction and subsequent chart understanding becomes very challenging. Many studies have been taken up in the literature to address the problem of chart mining, whose motivation is to facilitate the editing of existing charts, carry out extrapolative studies, and provide a deeper understanding of the underlying data. The first step towards chart understanding is chart classification, for which traditional ML and CNN-based deep learning models have been used in the literature. In this paper, we propose Swin-Chart, a Swin transformer-based deep learning approach for chart classification, which generalizes well across multiple datasets with a wide range of chart categories. Swin-Chart comprises a pre-trained Swin Transformer, a finetuning component, and a weight averaging component. The proposed approach is tested on a five-chart image benchmark dataset. We observed that the Swin-Chart model outperformers existing state-of-the-art models on all the datasets. Furthermore, we also provide an ablation study of the Swin-Chart model with all five datasets to understand the importance of various sub-parts such as the back-bone Swin transformer model, the value of several best weights selected for the weight averaging component, and the presence of the weight averaging component itself. The Swin-Chart model also received first position in the chart classification task on the latest dataset in the CHART Infographics competition at ICDAR 2023 - chartinfo.github.io.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524002447",
    "keywords": [
      "Artificial intelligence",
      "Chart",
      "Computer science",
      "Data mining",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Radar chart",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Dhote",
        "given_name": "Anurag"
      },
      {
        "surname": "Javed",
        "given_name": "Mohammed"
      },
      {
        "surname": "Doermann",
        "given_name": "David S."
      }
    ]
  },
  {
    "title": "Soft independence guided filter pruning",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110488",
    "abstract": "Filter pruning (FP) is an effective method for reducing the computational costs of convolutional neural networks, and herein, the most critical task involves evaluating the significance of each convolutional filter and eliminating the less important ones while minimizing performance degradation. Most existing FP methods consider only local information, which may prevent them from accurately recognizing the most important filters. To address this limitation, we propose the soft filter independence (SFI) method, which leverages global information to identify the most important filters using their magnitude and correlation information in different functional layers. The SFI criterion measures the replaceability of filters from a global perspective in a network. Filters with low independence can be represented effectively by others, so their information can be accurately conveyed by other filters. In addition, we introduce a novel SFI-based asymptotic pruning ratio, which improves training and pruning stability. Compared to the most advanced FP methods, our method enables CNNs to achieve higher pruning rates and better classification performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324002395",
    "keywords": [
      "Agronomy",
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Filter (signal processing)",
      "Independence (probability theory)",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pruning",
      "Stability (learning theory)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Liu"
      },
      {
        "surname": "Gu",
        "given_name": "Shiqiao"
      },
      {
        "surname": "Shen",
        "given_name": "Chenyang"
      },
      {
        "surname": "Zhao",
        "given_name": "Xile"
      },
      {
        "surname": "Hu",
        "given_name": "Qinghua"
      }
    ]
  },
  {
    "title": "Bidirectional correlation-driven inter-frame interaction Transformer for referring video object segmentation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110535",
    "abstract": "Referring video object segmentation (RVOS) aims to segment the target object in a video sequence described by a language expression. Typical multimodal Transformer based RVOS approaches process video sequence in a frame-independent manner to reduce the high computational cost, which however restricts the performance due to the lack of inter-frame interaction for temporal coherence modeling and spatio-temporal representation learning of the referred object. Besides, the absence of sufficient cross-modal interactions results in weak correlation between the visual and linguistic features, which increases the difficulty of decoding the target information and limits the performance of the model. In this paper, we propose a bidirectional correlation-driven inter-frame interaction Transformer, dubbed BIFIT, to address these issues in RVOS. Specifically, we design a lightweight and plug-and-play inter-frame interaction module in the Transformer decoder to efficiently learn the spatio-temporal features of the referred object, so as to decode the object information in the video sequence more precisely and generate more accurate segmentation results. Moreover, a bidirectional multi-level vision-language interaction module is implemented before the multimodal Transformer to enhance the correlation between the linguistic and multi-level visual features, thus facilitating the language queries to decode more precise object information from visual features and ultimately improving the segmentation performance. Extensive experimental results on four benchmarks validate the superiority of our BIFIT over state-of-the-art methods and the effectiveness of our proposed modules. The code is available in https://github.com/LANMNG/BIFIT.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324002863",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Correlation",
      "Electrical engineering",
      "Engineering",
      "Frame (networking)",
      "Geometry",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Segmentation",
      "Telecommunications",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Lan",
        "given_name": "Meng"
      },
      {
        "surname": "Rong",
        "given_name": "Fu"
      },
      {
        "surname": "Li",
        "given_name": "Zuchao"
      },
      {
        "surname": "Yu",
        "given_name": "Wei"
      },
      {
        "surname": "Zhang",
        "given_name": "Lefei"
      }
    ]
  },
  {
    "title": "Self-supervised multi-echo point cloud denoising in snowfall",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.07.007",
    "abstract": "Snowfall can cause noise to light detection and ranging (LiDAR) data. This is a problem since it is used in many outdoor applications, e.g., autonomous driving. We propose the task of multi-echo denoising, where the goal is to pick the echo that represents the objects of interest and discard other echoes. Thus, the idea is to pick points from alternative echoes unavailable in standard strongest echo point clouds. Intuitively, we are trying to see through the snowfall. We propose a novel self-supervised deep learning method and the characteristics similarity regularization to achieve this goal. The characteristics similarity regularization utilizes noise characteristics to increase performance. The experiments with a real-world multi-echo snowfall dataset prove the efficacy of multi-echo denoising and superior performance to the baseline. Moreover, based on extensive experiments on a semi-synthetic dataset, our method achieves superior performance compared to the state-of-the-art in self-supervised snowfall denoising. Our work enables more reliable point cloud acquisition in snowfall. The code is available at https://github.com/alvariseppanen/SMEDen.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524002101",
    "keywords": [
      "Artificial intelligence",
      "Cloud computing",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Echo (communications protocol)",
      "Geography",
      "Geology",
      "Geometry",
      "Mathematics",
      "Meteorology",
      "Noise reduction",
      "Operating system",
      "Pattern recognition (psychology)",
      "Point (geometry)",
      "Point cloud",
      "Remote sensing",
      "Snow"
    ],
    "authors": [
      {
        "surname": "Seppänen",
        "given_name": "Alvari"
      },
      {
        "surname": "Ojala",
        "given_name": "Risto"
      },
      {
        "surname": "Tammi",
        "given_name": "Kari"
      }
    ]
  },
  {
    "title": "Contrastive Learning for Lane Detection via cross-similarity",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.08.007",
    "abstract": "Detecting lane markings in road scenes poses a significant challenge due to their intricate nature, which is susceptible to unfavorable conditions. While lane markings have strong shape priors, their visibility is easily compromised by varying lighting conditions, adverse weather, occlusions by other vehicles or pedestrians, road plane changes, and fading of colors over time. The detection process is further complicated by the presence of several lane shapes and natural variations, necessitating large amounts of high-quality and diverse data to train a robust lane detection model capable of handling various real-world scenarios. In this paper, we present a novel self-supervised learning method termed Contrastive Learning for Lane Detection via Cross-Similarity (CLLD) to enhance the resilience and effectiveness of lane detection models in real-world scenarios, particularly when the visibility of lane markings are compromised. CLLD introduces a novel contrastive learning (CL) method that assesses the similarity of local features within the global context of the input image. It uses the surrounding information to predict lane markings. This is achieved by integrating local feature contrastive learning with our newly proposed operation, dubbed cross-similarity. The local feature CL concentrates on extracting features from small patches, a necessity for accurately localizing lane segments. Meanwhile, cross-similarity captures global features, enabling the detection of obscured lane segments based on their surroundings. We enhance cross-similarity by randomly masking portions of input images in the process of augmentation. Extensive experiments on TuSimple and CuLane benchmark datasets demonstrate that CLLD consistently outperforms state-of-the-art contrastive learning methods, particularly in visibility-impairing conditions like shadows, while it also delivers comparable results under normal conditions. When compared to supervised learning, CLLD still excels in challenging scenarios such as shadows and crowded scenes, which are common in real-world driving.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524002393",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Image (mathematics)",
      "Pattern recognition (psychology)",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Zoljodi",
        "given_name": "Ali"
      },
      {
        "surname": "Abadijou",
        "given_name": "Sadegh"
      },
      {
        "surname": "Alibeigi",
        "given_name": "Mina"
      },
      {
        "surname": "Daneshtalab",
        "given_name": "Masoud"
      }
    ]
  },
  {
    "title": "Robformer: A robust decomposition transformer for long-term time series forecasting",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110552",
    "abstract": "Transformer-based forecasting methods have been widely applied to forecast long-term multivariate time series, which achieves significant improvements on extending the forecasting time. However, their performance can degenerate terribly when abrupt trend shift and seasonal fluctuation arise in long-term time series. Hence, we identify two bottlenecks of previous Transformers architecture: (1) the robustless decomposition module and (2) trend shifting problem. These result in a different distribution between the trend prediction and ground truth in the long-term multivariate series forecasting. Towards these bottlenecks, we design Robformer as a novel decomposition-based Transformer, which consists of three new inner module to enhance the predictability of Transformers. Concretely, we renew the decomposition module and add a seasonal component adjustment module to tackle the unstationarized series. Further, we propose a novel inner trend forecasting architecture inspired by polynomial fitting method, which outperforms previous design in accuracy and robustness. Our empirical studies show that Robformer can achieve 17% and 10% relative improvements than state-of-the-art Autoformer and FEDformer baselines under the fair long-term multivariate setting on six benchmarks, covering five mainstream time series forecasting applications: energy, economics, traffic, weather, and disease. The code will be released upon publication.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324003030",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Decomposition",
      "Ecology",
      "Electrical engineering",
      "Engineering",
      "Geology",
      "Machine learning",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Series (stratigraphy)",
      "Term (time)",
      "Time series",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Yu",
        "given_name": "Yang"
      },
      {
        "surname": "Ma",
        "given_name": "Ruizhe"
      },
      {
        "surname": "Ma",
        "given_name": "Zongmin"
      }
    ]
  },
  {
    "title": "Recent advances in behavioral and hidden biometrics for personal identification",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.07.016",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524002198",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Biometrics",
      "Botany",
      "Computer science",
      "Identification (biology)"
    ],
    "authors": [
      {
        "surname": "Orrù",
        "given_name": "Giulia"
      },
      {
        "surname": "Rattani",
        "given_name": "Ajita"
      },
      {
        "surname": "Rida",
        "given_name": "Imad"
      },
      {
        "surname": "Marcel",
        "given_name": "Sébastien"
      }
    ]
  },
  {
    "title": "ScopeViT: Scale-Aware Vision Transformer",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110470",
    "abstract": "Multi-scale features are essential for various vision tasks, such as classification, detection, and segmentation. Although Vision Transformers (ViTs) show remarkable success in capturing global features within an image, how to leverage multi-scale features in Transformers is not well explored. This paper proposes a scale-aware vision Transformer called ScopeViT that efficiently captures multi-granularity representations. Two novel attention with lightweight computation are introduced: Multi-Scale Self-Attention (MSSA) and Global-Scale Dilated Attention (GSDA). MSSA embeds visual tokens with different receptive fields into distinct attention heads, allowing the model to perceive various scales across the network. GSDA enhances model understanding of the global context through token-dilation operation, which reduces the number of tokens involved in attention computations. This dual attention method enables ScopeViT to “see” various scales throughout the entire network and effectively learn inter-object relationships, reducing heavy quadratic computational complexity. Extensive experiments demonstrate that ScopeViT achieves competitive complexity/accuracy trade-offs compared to existing networks across a wide range of visual tasks. On the ImageNet-1K dataset, ScopeViT achieves a top-1 accuracy of 81.1%, using only 7.4M parameters and 2.0G FLOPs. Our approach outperforms Swin (ViT-based) by 1.9% accuracy while saving 42% of the parameters, outperforms MobileViTv2 (Hybrid-based) with a 0.7% accuracy gain while using 50% of the computations, and also beats ConvNeXt V2 (ConvNet-based) by 0.8% with fewer parameters.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324002218",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Electrical engineering",
      "Engineering",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Nie",
        "given_name": "Xuesong"
      },
      {
        "surname": "Jin",
        "given_name": "Haoyuan"
      },
      {
        "surname": "Yan",
        "given_name": "Yunfeng"
      },
      {
        "surname": "Chen",
        "given_name": "Xi"
      },
      {
        "surname": "Zhu",
        "given_name": "Zhihang"
      },
      {
        "surname": "Qi",
        "given_name": "Donglian"
      }
    ]
  },
  {
    "title": "PLoPS: Localization-aware person search with prototypical normalization",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110479",
    "abstract": "Person search involves localizing and re-identifying persons of interest captured by multiple, non-overlapping cameras. Recent approaches to person search are typically built on object detection frameworks to learn joint person representations for detection and re-identification. To this end, the features extracted from pedestrian proposals are projected onto a unit hypersphere using L2 normalization, and positive proposals that sufficiently overlap with the ground truth are equally incorporated for training by exploiting an external lookup table (LUT). We have found that (1) using the L2 normalization technique, without considering feature distributions, can degenerate the discriminative power of person representations, (2) positive proposals often depict distracting details, such as background clutter and person overlaps, and (3) person features in the LUT are not often updated during training. To address these limitations, we propose a novel framework for person search, dubbed PLoPS, using a prototypical normalization layer, ProtoNorm, that calibrates features while considering the long-tail distribution across person IDs. PLoPS also entails a localization-aware learning scheme that prioritizes better-aligned proposals w.r.t the ground truth. We further introduce a LUT calibration technique to continuously adjust the person features in the LUT. Experimental results and analysis on standard benchmarks demonstrate the effectiveness of PLoPS.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324002309",
    "keywords": [
      "Anthropology",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Normalization (sociology)",
      "Pattern recognition (psychology)",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Lee",
        "given_name": "Sanghoon"
      },
      {
        "surname": "Oh",
        "given_name": "Youngmin"
      },
      {
        "surname": "Baek",
        "given_name": "Donghyeon"
      },
      {
        "surname": "Lee",
        "given_name": "Junghyup"
      },
      {
        "surname": "Ham",
        "given_name": "Bumsub"
      }
    ]
  },
  {
    "title": "Single image dehazing based on multi-label graph cuts",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.07.015",
    "abstract": "Haze blurs image information and reduces the visibility of objects in the image, which seriously affects the performance of computer vision applications in a hazy environment. We propose an improved dehazing model based on multi-label graph cuts. A hazy image is modeled as an undirected graph. The multi-label graph cuts algorithm divides the image into subregions according to the functions of brightness and saturation. A subregion is selected to estimate atmospheric light based on saturation. Under the similarity of transmission in the same subregion, transmission is estimated by the distance between the pixel and atmospheric light in RGB space. Finally, the transmission map is regularized to recover a haze-free image. Experiments in different scenarios demonstrate the effectiveness of the proposed method than the state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524002186",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Cut",
      "Graph",
      "Image (mathematics)",
      "Image segmentation",
      "Pattern recognition (psychology)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Qin",
        "given_name": "Minshen"
      },
      {
        "surname": "Jiang",
        "given_name": "Junzheng"
      },
      {
        "surname": "Zhou",
        "given_name": "Fang"
      }
    ]
  },
  {
    "title": "PlaneAC: Line-guided planar 3D reconstruction based on self-attention and convolution hybrid model",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110519",
    "abstract": "Planar 3D reconstruction aims to simultaneously extract plane instances and reconstruct the local 3D model through the estimated plane parameters. Existing methods achieve promising results either through self-attention or convolution neural network (CNNs), but usually ignore the complementary properties of them. In this paper, we propose a line-guided planar 3D reconstruction method PlaneAC, which leverages the advantage of self-attention and CNNs to capture long-range dependencies and alleviate the computational burden. In addition, explicit connection between two adjacent attention layers is built for better leveraging the transferable knowledge and facilitating the information flow between tokenized feature from different layers. Therefore, the subsequent attention layer can directly interact with previous results. Finally, a line segment filtering method is presented to remove irrelevant guiding information from indistinctive line segments extracted from the image. Extensive experiments on ScanNet and NYUv2 public datasets demonstrate the preferable performance of our proposed method, and the results show that PlaneAC achieves a better trade-off between accuracy and computation cost compared with other state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032400270X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computation",
      "Computer graphics (images)",
      "Computer science",
      "Computer vision",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Feature (linguistics)",
      "Feature extraction",
      "Geometry",
      "Line (geometry)",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Planar",
      "Plane (geometry)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Jiahui"
      },
      {
        "surname": "Yang",
        "given_name": "Jinfu"
      },
      {
        "surname": "Fu",
        "given_name": "Fuji"
      },
      {
        "surname": "Ma",
        "given_name": "Jiaqi"
      }
    ]
  },
  {
    "title": "Transductive zero-shot learning with generative model-driven structure alignment",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110561",
    "abstract": "Zero-shot learning (ZSL) facilitates the transfer of knowledge from seen to unseen categories through high-dimensional vectors that capture both known and unknown class names. However it encounters challenges with domain shift arising from a lack of sufficient labeled data. Although transductive zero-shot learning (TZSL) addresses this bias by including samples from unseen classes, it still faces obstacles in enhancing TZSL performance. In this study, We introduce the Structure Alignment Variational Autoencoder Generative Adversarial Network (SA-VAEGAN), a novel approach that enhances the alignment between visual and auxiliary spaces. We delved into the underlying causes of domain shift and introduced a structural alignment (SA) strategy to tackle these challenges. The SA model thoroughly accounts for both inter-class and intra-class dynamics, designed to leverage the model’s comprehension of high-level semantic relations to disambiguate confusion among similar classes and mitigate intra-class confusion by penalizing atypical visual samples within classes. Assessed across four benchmark datasets, SA-VAEGAN has established a new performance standard, underscoring its efficiency in addressing the domain shift challenge within TZSL tasks, and achieving high accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324003121",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Autoencoder",
      "Benchmark (surveying)",
      "Class (philosophy)",
      "Computer science",
      "Confusion",
      "Deep learning",
      "Generative grammar",
      "Generative model",
      "Geodesy",
      "Geography",
      "Leverage (statistics)",
      "Machine learning",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Psychoanalysis",
      "Psychology"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Yang"
      },
      {
        "surname": "Tao",
        "given_name": "Keda"
      },
      {
        "surname": "Tian",
        "given_name": "Tianhui"
      },
      {
        "surname": "Gao",
        "given_name": "Xinbo"
      },
      {
        "surname": "Han",
        "given_name": "Jungong"
      },
      {
        "surname": "Shao",
        "given_name": "Ling"
      }
    ]
  },
  {
    "title": "Graph correlated discriminant embedding for multi-source domain adaptation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110538",
    "abstract": "As a main branch of domain adaptation (DA), multi-source DA (MSDA) has attracted increasing attention for exploiting information from multi-source domain data. However, how to effectively explore useful information from each source domain for target tasks is still a key problem. In this paper, to fully explore multiple information of different domain data, we propose a graph correlated discriminant embedding (GCDE) method for MSDA. In GCDE, the category-discriminative information, manifold structure, and correlation learning are fully considered. Specifically, GCDE encodes the within- and between- class information of each domain data, preserves the local and global structure information of the data, and extracts the maximization correlative features from different domains by designing a novel correlative learning scheme. We also extend GCDE to a nonlinear case and obtain kernel GCDE (KGCDE). We have conducted extensive experiments on four public data benchmarks to verify the performance of GCDE and KGCDE. The promising performance on the databases prove the efficiency of our methods with the comparison of the advanced approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324002899",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Discriminant",
      "Domain adaptation",
      "Embedding",
      "Graph",
      "Graph embedding",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Wong",
        "given_name": "Wai Keung"
      },
      {
        "surname": "Lu",
        "given_name": "Yuwu"
      },
      {
        "surname": "Lai",
        "given_name": "Zhihui"
      },
      {
        "surname": "Li",
        "given_name": "Xuelong"
      }
    ]
  },
  {
    "title": "A lightweight attention-driven distillation model for human pose estimation",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.08.009",
    "abstract": "Currently, research on human pose estimation tasks primarily focuses on heatmap-based and regression-based methods. However, the increasing complexity of heatmap models and the low accuracy of regression methods are becoming significant barriers to the advancement of the field. In recent years, researchers have begun exploring new methods to transfer knowledge from heatmap models to regression models. Recognizing the limitations of existing approaches, our study introduces a novel distillation model that is both lightweight and precise. In the feature extraction phase, we design the Channel-Attention-Unit (CAU), which integrates group convolution with an attention mechanism to effectively reduce redundancy while maintaining model accuracy with a decreased parameter count. During distillation, we develop the attention loss function, L A , which enhances the model’s capacity to locate key points quickly and accurately, emulating the effect of additional transformer layers and boosting precision without the need for increased parameters or network depth. Specifically, on the CrowdPose test dataset, our model achieves 71.7% mAP with 4.3M parameters, 2.2 GFLOPs, and 51.3 FPS. Experimental results demonstrates the model’s strong capabilities in both accuracy and efficiency, making it a viable option for real-time posture estimation tasks in real-world environments.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524002411",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Computer vision",
      "Distillation",
      "Engineering",
      "Estimation",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Pose",
      "Systems engineering"
    ],
    "authors": [
      {
        "surname": "Wei",
        "given_name": "Falai"
      },
      {
        "surname": "Hu",
        "given_name": "Xiaofang"
      }
    ]
  },
  {
    "title": "Generative data augmentation by conditional inpainting for multi-class object detection in infrared images",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110501",
    "abstract": "Multi-class object detection in infrared images is important in military and civilian use. Deep learning methods can obtain high accuracy but require a large-scale dataset. We propose a generative data augmentation framework DOCI-GAN, for infrared multi-class object detection with limited data. Contributions of this paper are four-folds. Firstly, DOCI-GAN is designed as a conditional image inpainting framework, yielding paired infrared multi-class object image and annotation. Secondly, a text-to-image converter is formulated to transform text-format object annotations to bounding box mask images, leading the augmentation to be mask-image-to-raw-image translation. Thirdly, a multiscale morphological erosion-based loss is created to alleviate the intensity inconsistency between inpainted local backgrounds and global background. Finally, for generating diverse images, artificial multi-class object annotations are integrated with real ones during augmentation. Experimental results demonstrated that DOCI-GAN augments dataset with high-quality infrared multi-class object images, consequently improving the accuracy of object detection baselines.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324002528",
    "keywords": [
      "Annotation",
      "Artificial intelligence",
      "Biochemistry",
      "Bounding overwatch",
      "Chemistry",
      "Class (philosophy)",
      "Computer science",
      "Computer vision",
      "Gene",
      "Image (mathematics)",
      "Inpainting",
      "Messenger RNA",
      "Minimum bounding box",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Translation (biology)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Peng"
      },
      {
        "surname": "Ma",
        "given_name": "Zhe"
      },
      {
        "surname": "Dong",
        "given_name": "Bo"
      },
      {
        "surname": "Liu",
        "given_name": "Xiuhua"
      },
      {
        "surname": "Ding",
        "given_name": "Jishiyu"
      },
      {
        "surname": "Sun",
        "given_name": "Kewu"
      },
      {
        "surname": "Chen",
        "given_name": "Ying"
      }
    ]
  },
  {
    "title": "Unified multi-level neighbor clustering for Source-Free Unsupervised Domain Adaptation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110533",
    "abstract": "Source-Free Unsupervised Domain Adaptation (SFUDA) transfers knowledge from the source domain to the target domain with using the source domain model instead of the source data, as the source data cannot be accessed in data privacy scenarios. Our method is based on a clustering assumption: although there is a domain shift, target data with similar semantic still form a cluster in the source feature space. We identify two levels of clustering. One is class-level neighbor clustering: data with the same label tend to form a large cluster. The other is instance-level neighbor clustering: data and its neighbors tend to share the same label. Previous methods only consider one level, and we consider that both are complementary. We propose a new SFUDA method called unified multi-level neighbor clustering to address class and instance consistency in a complementary way. Our proposed method achieves competitive results on three domain adaptation benchmark datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032400284X",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Class (philosophy)",
      "Cluster analysis",
      "Computer science",
      "Consistency (knowledge bases)",
      "Correlation clustering",
      "Data mining",
      "Domain (mathematical analysis)",
      "Geodesy",
      "Geography",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Xiao",
        "given_name": "Yuzhe"
      },
      {
        "surname": "Xiao",
        "given_name": "Guangyi"
      },
      {
        "surname": "Chen",
        "given_name": "Hao"
      }
    ]
  },
  {
    "title": "EHIR: Energy-based Hierarchical Iterative Image Registration for Accurate PCB Defect Detection",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.06.027",
    "abstract": "Printed Circuit Board (PCB) Surface defect detection is crucial to ensure the quality of electronic products in manufacturing industry. Detection methods can be divided into non-referential and referential methods. Non-referential methods employ designed rules or learned data distribution without template images but are difficult to address the uncertainty and subjectivity issues of defects. In contrast, referential methods use templates to achieve better performance but rely on precise image registration. However, image registration is especially challenging in feature extracting and matching for PCB images with defective, reduplicated or less features. To address these issues, we propose a novel Energy-based Hierarchical Iterative Image Registration method (EHIR) to formulate image registration as an energy optimization problem based on the edge points rather than finite features. Our framework consists of three stages: Edge-guided Energy Transformation (EET), EHIR and Edge-guided Energy-based Defect Detection (EEDD). The novelty is that the consistency of contours contributes to aligning images and the difference is highlighted for defect location. Extensive experiments show that this method has high accuracy and strong robustness, especially in the presence of defect feature interference, where our method demonstrates an overwhelming advantage over other methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524001983",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Energy (signal processing)",
      "Image (mathematics)",
      "Image processing",
      "Image registration",
      "Iterative method",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Deng",
        "given_name": "Shuixin"
      },
      {
        "surname": "Deng",
        "given_name": "Lei"
      },
      {
        "surname": "Meng",
        "given_name": "Xiangze"
      },
      {
        "surname": "Sun",
        "given_name": "Ting"
      },
      {
        "surname": "Chen",
        "given_name": "Baohua"
      },
      {
        "surname": "Chen",
        "given_name": "Zhixiang"
      },
      {
        "surname": "Hu",
        "given_name": "Hao"
      },
      {
        "surname": "Xie",
        "given_name": "Yusen"
      },
      {
        "surname": "Yin",
        "given_name": "Hanxi"
      },
      {
        "surname": "Yu",
        "given_name": "Shijie"
      }
    ]
  },
  {
    "title": "IBVC: Interpolation-driven B-frame video compression",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110465",
    "abstract": "Learned B-frame video compression aims to adopt bi-directional motion estimation and motion compensation (MEMC) coding for middle frame reconstruction. However, previous learned approaches often directly extend neural P-frame codecs to B-frame relying on bi-directional optical-flow estimation or video frame interpolation. They suffer from inaccurate quantized motions and inefficient motion compensation. To address these issues, we propose a simple yet effective structure called Interpolation-driven B-frame Video Compression (IBVC). Our approach only involves two major operations: video frame interpolation and artifact reduction compression. IBVC introduces a bit-rate free MEMC based on interpolation, which avoids optical-flow quantization and additional compression distortions. Later, to reduce duplicate bit-rate consumption and focus on unaligned artifacts, a residual guided masking encoder is deployed to adaptively select the meaningful contexts with interpolated multi-scale dependencies. In addition, a conditional spatio-temporal decoder is proposed to eliminate location errors and artifacts instead of using MEMC coding in other methods. The experimental results on B-frame coding demonstrate that IBVC has significant improvements compared to the relevant state-of-the-art methods. Meanwhile, our approach can save bit rates compared with the random access (RA) configuration of H.266 (VTM). The code will be available at https://github.com/ruhig6/IBVC.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324002164",
    "keywords": [
      "Artificial intelligence",
      "Block-matching algorithm",
      "Codec",
      "Compression artifact",
      "Computer hardware",
      "Computer science",
      "Computer vision",
      "Data compression",
      "Encoder",
      "Frame (networking)",
      "Image (mathematics)",
      "Image compression",
      "Image processing",
      "Inter frame",
      "Motion compensation",
      "Motion interpolation",
      "Operating system",
      "Reference frame",
      "Residual frame",
      "Telecommunications",
      "Video compression picture types",
      "Video processing",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Chenming"
      },
      {
        "surname": "Liu",
        "given_name": "Meiqin"
      },
      {
        "surname": "Yao",
        "given_name": "Chao"
      },
      {
        "surname": "Lin",
        "given_name": "Weisi"
      },
      {
        "surname": "Zhao",
        "given_name": "Yao"
      }
    ]
  },
  {
    "title": "SiamMAF: A multipath and feature-enhanced thermal infrared tracker",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.09.003",
    "abstract": "Thermal infrared (TIR) images are visually blurred and low in information content. Some TIR trackers focus on enhancing the semantic information of TIR features, neglecting the equally important detailed information for TIR tracking. After target localization, detailed information can assist the tracker in generating accurate prediction boxes. In addition, simple element-wise addition is not a way to fully utilize and fuse multiple response maps. To address these issues, this study proposes a multipath and feature-enhanced Siamese tracker (SiamMAF) for TIR tracking. We design a feature-enhanced module (FEM) based on complementarity, which can highlight the key semantic information of the target and preserve the detailed information of objects. Furthermore, we introduce a response fusion module (RFM) that can adaptively fuse multiple response maps. Extensive experimental results on two challenging benchmarks show that SiamMAF outperforms many existing state-of-the-art TIR trackers and runs at a steady 31FPS.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524002599",
    "keywords": [
      "Artificial intelligence",
      "BitTorrent tracker",
      "Channel (broadcasting)",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Electrical engineering",
      "Engineering",
      "Eye tracking",
      "Feature (linguistics)",
      "Focus (optics)",
      "Fuse (electrical)",
      "Fusion",
      "Linguistics",
      "Multipath propagation",
      "Optics",
      "Pattern recognition (psychology)",
      "Pedagogy",
      "Philosophy",
      "Physics",
      "Psychology",
      "Tracking (education)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Weisheng"
      },
      {
        "surname": "Fang",
        "given_name": "Yuhao"
      },
      {
        "surname": "Lv",
        "given_name": "Lanbing"
      },
      {
        "surname": "Chen",
        "given_name": "Shunping"
      }
    ]
  },
  {
    "title": "C2FResMorph: A high-performance framework for unsupervised 2D medical image registration",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110615",
    "abstract": "Deformable medical image registration is an important precursor task for surgical automation, while enhancing the registration performance of 2D medical images remains a challenging work. Existing methods primarily minimize the similarity loss between image pairs as the main optimization objective, leading to limited registration accuracy and a lack of pixel matching. Moreover, the scarcity of informative features in 2D images often results in overfitting on the training set, hampering generalization. To address these issues, we propose C2FResMorph, a learning-based deformable registration algorithm specifically designed for 2D medical images. C2FResMorph employs a two-stage framework that improves registration accuracy and preserves topology during deformation in a coarse-to-fine manner. Inside the framework, by leveraging the convolutional neural network’s locality and the multi-head self-attention mechanism’s globality, a ResMorph registration network is designed. Additionally, the integration of residual image knowledge addresses deformation folding in 2D image registration, enhancing the preservation of local structures and improving generalization. Experimental evaluations on three datasets demonstrate that C2FResMorph outperforms existing learning-based methods in terms of accuracy, generalization ability for 2D medical image registration, and also retains the efficiency advantages.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324003662",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Image (mathematics)",
      "Image registration",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Ding",
        "given_name": "Yi"
      },
      {
        "surname": "Bu",
        "given_name": "Junjian"
      },
      {
        "surname": "Qin",
        "given_name": "Zhen"
      },
      {
        "surname": "You",
        "given_name": "Li"
      },
      {
        "surname": "Cao",
        "given_name": "Mingsheng"
      },
      {
        "surname": "Qin",
        "given_name": "Zhiguang"
      },
      {
        "surname": "Pang",
        "given_name": "Minghui"
      }
    ]
  },
  {
    "title": "Enhancing abusive language detection: A domain-adapted approach leveraging BERT pre-training tasks",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.05.007",
    "abstract": "The widespread adoption of deep learning approaches in natural language processing is largely attributed to their exceptional performance across diverse tasks. Notably, Transformer-based models, such as BERT, have gained popularity for their remarkable efficacy and their ease of adaptation (via fine-tuning) across various domains. Despite their success, fine-tuning these models for informal language, particularly instances involving offensive expressions, presents a major challenge due to limitations in vocabulary coverage and contextual information for such tasks. To address these challenges, we propose the domain adaptation of the BERT language model for the task of detecting abusive language. Our approach involves constraining the language model with the adaptation and paradigm shift of two default pre-trained tasks, the design of two datasets specifically engineered to support the adapted pre-training tasks, and the proposal of a dynamic weighting loss function. The evaluation of these adapted configurations on six datasets dedicated to abusive language detection reveals promising outcomes, with a significant enhancement observed compared to the base model. Furthermore, our proposed methods yield competitive results when compared to state-of-the-art approaches, establishing a robust and easily trainable model for the effective identification of abusive language.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552400151X",
    "keywords": [
      "Abusive supervision",
      "Artificial intelligence",
      "Computer science",
      "Domain (mathematical analysis)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Meteorology",
      "Natural language processing",
      "Physics",
      "Psychology",
      "Social psychology",
      "Training (meteorology)",
      "Training set"
    ],
    "authors": [
      {
        "surname": "Jarquín-Vásquez",
        "given_name": "Horacio"
      },
      {
        "surname": "Escalante",
        "given_name": "Hugo Jair"
      },
      {
        "surname": "Montes-y-Gómez",
        "given_name": "Manuel"
      }
    ]
  },
  {
    "title": "Cross-attention based dual-similarity network for few-shot learning",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.08.019",
    "abstract": "Few-shot classification is a challenging task to recognize unseen classes with limited data. Following the success of Vision Transformer in various large-scale datasets image recognition domains, recent few-shot classification methods employ transformer-style. However, most of them focus only on cross-attention between support and query sets, mainly considering channel-similarity. To address this issue, we introduce dual-similarity network (DSN) in which attention maps for the same target within a class are made identical. With the network, a way of effective training through the integration of the channel-similarity and the map-similarity has been sought. Our method, while focused on N -way K -shot scenarios, also demonstrates strong performance in 1-shot settings through augmentation. The experimental results verify the effectiveness of DSN on widely used benchmark datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524002514",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Attention network",
      "Chemistry",
      "Computer science",
      "Dual (grammatical number)",
      "Engineering",
      "Image (mathematics)",
      "Literature",
      "Mechanical engineering",
      "One shot",
      "Organic chemistry",
      "Shot (pellet)",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Sim",
        "given_name": "Chan"
      },
      {
        "surname": "Kim",
        "given_name": "Gyeonghwan"
      }
    ]
  },
  {
    "title": "Visual speech recognition using compact hypercomplex neural networks",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.09.002",
    "abstract": "Recent progress in visual speech recognition systems due to advances in deep learning and large-scale public datasets has led to impressive performance compared to human professionals. The potential applications of these systems in real-life scenarios are numerous and can greatly benefit the lives of many individuals. However, most of these systems are not designed with practicality in mind, requiring large-size models and powerful hardware, factors which limit their applicability in resource-constrained environments and other real-world tasks. In addition, few works focus on developing lightweight systems that can be deployed in such conditions. Considering these issues, we propose compact networks that take advantage of hypercomplex layers that utilize a sum of Kronecker products to reduce overall parameter demands and model sizes. We train and evaluate our proposed models on the largest public dataset for single word speech recognition for English. Our experiments show that high compression rates are achievable with a minimal accuracy drop, indicating the method’s potential for practical applications in lower-resource environments. Code and models are available at https://github.com/jpanagos/vsr_phm.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524002587",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Code (set theory)",
      "Computer network",
      "Computer science",
      "Deep learning",
      "Focus (optics)",
      "Geometry",
      "Hypercomplex number",
      "Machine learning",
      "Mathematics",
      "Optics",
      "Physics",
      "Programming language",
      "Quaternion",
      "Resource (disambiguation)",
      "Set (abstract data type)",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Panagos",
        "given_name": "Iason Ioannis"
      },
      {
        "surname": "Sfikas",
        "given_name": "Giorgos"
      },
      {
        "surname": "Nikou",
        "given_name": "Christophoros"
      }
    ]
  },
  {
    "title": "Rethinking unsupervised domain adaptation for semantic segmentation",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.09.022",
    "abstract": "Unsupervised domain adaptation (UDA) adapts a model trained on one domain (called source) to a novel domain (called target) using only unlabeled data. Due to its high annotation cost, researchers have developed many UDA methods for semantic segmentation, which assume no labeled sample is available in the target domain. We question the practicality of this assumption for two reasons. First, after training a model with a UDA method, we must somehow verify the model before deployment. Second, UDA methods have at least a few hyper-parameters that need to be determined. The surest solution to these is to evaluate the model using validation data, i.e., a certain amount of labeled target-domain samples. This question about the basic assumption of UDA leads us to rethink UDA from a data-centric point of view. Specifically, we assume we have access to a minimum level of labeled data. Then, we ask how much is necessary to find good hyper-parameters of existing UDA methods. We then consider what if we use the same data for supervised training of the same model, e.g., finetuning. We conducted experiments to answer these questions with popular scenarios, {GTA5, SYNTHIA} → Cityscapes. We found that i) choosing good hyper-parameters needs only a few labeled images for some UDA methods whereas a lot more for others; and ii) simple finetuning works surprisingly well; it outperforms many UDA methods if only several dozens of labeled images are available.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524002836",
    "keywords": [
      "Adaptation (eye)",
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Domain (mathematical analysis)",
      "Domain adaptation",
      "Mathematical analysis",
      "Mathematics",
      "Natural language processing",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Psychology",
      "Segmentation",
      "Unsupervised learning"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Zhijie"
      },
      {
        "surname": "Suganuma",
        "given_name": "Masanori"
      },
      {
        "surname": "Okatani",
        "given_name": "Takayuki"
      }
    ]
  },
  {
    "title": "A high-precision ellipse detection method based on quadrant representation and top-down fitting",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110603",
    "abstract": "Ellipse detection is a basic task in many computer-vision related problems. While widely studied in recent years, accurate and efficient detection in real-world images is still a challenge. In this paper, a novel ellipse detector, with high accuracy and efficiency, is proposed. The detector models edge by block sequences, and extracts a set of elliptical arcs, which are classified into four sets. Then top-down ellipse fitting strategy that also makes the method able to detect small and flat ellipses is designed. A two-level validation process is used to select highly probable potential ellipses, especially for fragmented ellipses. Experiments on four synthetic datasets show that the proposed method performs far better than existing methods. In images with severe cluttering and occlusion, the F-measure can still be around 0.9. On four real image datasets the proposed method achieves better F-measure scores with competitive speed than state-of-the-art techniques.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324003546",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Ellipse",
      "Geometry",
      "Law",
      "Mathematics",
      "Medicine",
      "Pathology",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Quadrant (abdomen)",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Hongxia"
      },
      {
        "surname": "Han",
        "given_name": "Lixin"
      },
      {
        "surname": "Zhu",
        "given_name": "Shaojun"
      },
      {
        "surname": "Yan",
        "given_name": "Hong"
      }
    ]
  },
  {
    "title": "A recurrent graph neural network for inductive representation learning on dynamic graphs",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110577",
    "abstract": "Graph representation learning has recently garnered significant attention due to its wide applications in graph analysis tasks. It is well-known that real-world networks are dynamic, with edges and nodes evolving over time. This presents unique challenges that are distinct from those of static networks. However, most graph representation learning methods are either designed for static graphs, or address only partial challenges associated with dynamic graphs. They overlook the intricate interplay between topology and temporality in the evolution of dynamic graphs and the complexity of sequence modeling. Therefore, we propose a new dynamic graph representation learning model, called as R-GraphSAGE, which takes comprehensive considerations for embedding dynamic graphs. By incorporating a recurrent structure into GraphSAGE, the proposed R-GraphSAGE explores structural and temporal patterns integrally to capture more fine-grained evolving patterns of dynamic graphs. Additionally, it offers a lightweight architecture to decrease the computational costs for handling snapshot sequences, achieving a balance between performance and complexity. Moreover, it can inductively process the addition of new nodes and adapt to the situations without labels and node attributes. The performance of the proposed R-GraphSAGE is evaluated across various downstream tasks with both synthetic and real-world networks. The experimental results demonstrate that it outperforms state-of-the-art baselines by a significant margin in most cases.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324003285",
    "keywords": [
      "Artificial intelligence",
      "Computer network",
      "Computer science",
      "Dynamic network analysis",
      "Embedding",
      "Feature learning",
      "Graph",
      "Graph embedding",
      "Law",
      "Line graph",
      "Machine learning",
      "Operating system",
      "Pathwidth",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Snapshot (computer storage)",
      "Theoretical computer science",
      "Topological graph theory"
    ],
    "authors": [
      {
        "surname": "Yao",
        "given_name": "Hong-Yu"
      },
      {
        "surname": "Zhang",
        "given_name": "Chun-Yang"
      },
      {
        "surname": "Yao",
        "given_name": "Zhi-Liang"
      },
      {
        "surname": "Chen",
        "given_name": "C.L. Philip"
      },
      {
        "surname": "Hu",
        "given_name": "Junfeng"
      }
    ]
  },
  {
    "title": "Learning a complex network representation for shape classification",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110566",
    "abstract": "Shape contour is a key low-level characteristic, making shape description an important aspect in many computer vision problems, with several challenges such as variations in scale, rotation, and noise. In this paper, we introduce an approach for shape analysis and classification from binary images based on representations learned by applying Randomized Neural Networks (RNNs) on feature maps derived from a Complex Network (CN) framework. Our approach models the contour in a complex network and computes their topological measures using a dynamic evolution strategy. This evolution of the CN provides significant information into the physical aspects of the shape’s contour. Therefore, we propose embedding the topological measures computed from the dynamics of the CN evolution into a matrix representation, which we have named the Topological Feature Map (TFM). Then, we employ the RNN to learn representations from the TFM through a sliding window strategy. The proposed representation is formed by the learned weights between the hidden and output layers of the RNN. Our experimental results show performance improvements in shape classification using the proposed method across two generic shape datasets. We also applied our approach to the recognition of plant leaves, achieving high performance in this challenging task. Furthermore, the proposed approach has demonstrated robustness to noise and invariance to transformations in scale and orientation of the shapes.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324003170",
    "keywords": [
      "Active shape model",
      "Algorithm",
      "Arithmetic",
      "Artificial intelligence",
      "Artificial neural network",
      "Binary number",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Embedding",
      "Feature (linguistics)",
      "Gene",
      "Law",
      "Linguistics",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Programming language",
      "Recurrent neural network",
      "Representation (politics)",
      "Robustness (evolution)",
      "Segmentation",
      "Shape analysis (program analysis)",
      "Sliding window protocol",
      "Static analysis",
      "Topological skeleton",
      "Window (computing)"
    ],
    "authors": [
      {
        "surname": "Ribas",
        "given_name": "Lucas C."
      },
      {
        "surname": "Bruno",
        "given_name": "Odemir M."
      }
    ]
  },
  {
    "title": "Collaborative compensative transformer network for salient object detection",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110600",
    "abstract": "Salient object detection (SOD) is of high significance for various computer vision applications but is a challenging task due to the complicated scenes in real-world images. Most state-of-the-art SOD methods aim to build long-range dependency for improving global contrast modeling in complicated scenes. However, most of them suffer from the prior assumption of treating image patches as visual tokens for building long-range dependency. This is because this assumption leads to localizing salient regions with uncertain boundaries due to the lost object structure information. In this paper, to address this issue, we re-construct the prior assumption of treating both patches and superpixels as visual tokens for building long-range dependency, which takes into account the properties of superpixels and patches in preserving detailed structural-aware information and local context information, respectively. Based on the re-constructed prior assumption, we propose a Collaborative Compensative Transformer Network (CCTNet) for the SOD task. CCTNet firstly alternates the computation within the same kind of vision tokens and among different vision tokens to build their dependencies. By this means, the relationship between multi-level global context and detailed structure representation can be explicitly modeled for consistent semantic and object structure understanding. Then, CCTNet performs feature joint decoding for SOD by fusing the complementary global context and detailed structure for locating objects with certain boundaries. Extensive experiments were conducted to validate the effectiveness of the proposed modules. Furthermore, the experiments on ten benchmark datasets demonstrated the state-of-the-art performance of CCTNet on both RGB and RGB-D SOD.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324003510",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Context model",
      "Feature (linguistics)",
      "Geodesy",
      "Geography",
      "Linguistics",
      "Object (grammar)",
      "Object detection",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Salient"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Jun"
      },
      {
        "surname": "Zhang",
        "given_name": "Heye"
      },
      {
        "surname": "Gong",
        "given_name": "Mingming"
      },
      {
        "surname": "Gao",
        "given_name": "Zhifan"
      }
    ]
  },
  {
    "title": "Design of a differentiable L-1 norm for pattern recognition and machine learning",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.09.020",
    "abstract": "In various applications of pattern recognition, feature selection, and machine learning, L-1 norm is used as either an objective function or a regularizer. Mathematically, L-1 norm has unique characteristics that make it attractive in machine learning, feature selection, optimization, and regression. Computationally, however, L-1 norm presents a hurdle as it is non-differentiable, making the process of finding a solution difficult. Existing approach therefore relies on numerical approaches. In this work we designed an L-1 norm that is differentiable and, thus, has an analytical solution. The differentiable L-1 norm removes the absolute sign in the conventional definition and is everywhere differentiable. The new L-1 norm is almost everywhere linear, a desirable feature that is also present in the conventional L-1 norm. The only limitation of the new L-1 norm is that near zero, its behavior is not linear, hence we consider the new L-1 norm quasi-linear. Being differentiable, the new L-1 norm and its quasi-linear variation make them amenable to analytic solutions. Hence, it can facilitate the development and implementation of many algorithms involving L-1 norm. Our tests validate the capability of the new L-1 norm in various applications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524002812",
    "keywords": [
      "Algebra over a field",
      "Artificial intelligence",
      "Computer science",
      "Differentiable function",
      "Law",
      "Machine learning",
      "Mathematics",
      "Norm (philosophy)",
      "Pattern recognition (psychology)",
      "Political science",
      "Pure mathematics"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Min"
      },
      {
        "surname": "Wang",
        "given_name": "Yiming"
      },
      {
        "surname": "Chen",
        "given_name": "Hongyu"
      },
      {
        "surname": "Li",
        "given_name": "Taihao"
      },
      {
        "surname": "Liu",
        "given_name": "Shupeng"
      },
      {
        "surname": "Gu",
        "given_name": "Xianfeng"
      },
      {
        "surname": "Xu",
        "given_name": "Xiaoyin"
      }
    ]
  },
  {
    "title": "DeepSet SimCLR: Self-supervised deep sets for improved pathology representation learning",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.09.005",
    "abstract": "Often, applications of self-supervised learning to 3D medical data opt to use 3D variants of successful 2D network architectures. Although promising approaches, they are significantly more computationally demanding to train, and thus reduce the widespread applicability of these methods away from those with modest computational resources. Thus, in this paper, we aim to improve standard 2D SSL algorithms by modelling the inherent 3D nature of these datasets implicitly. We propose two variants that build upon a strong baseline model and show that both of these variants often outperform the baseline in a variety of downstream tasks. Importantly, in contrast to previous works in both 2D and 3D approaches for 3D medical data, both of our proposals introduce negligible additional overhead in terms of parameter complexity. Although data loading overhead increases over the baseline SimCLR model (which we can show can be somewhat mitigated through parallelisation), our proposed models are still significantly more efficient than previous approaches based on sequence modelling. Overall, our proposed methods help improve the democratisation of these approaches for medical applications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524002617",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Deep learning",
      "Humanities",
      "Law",
      "Machine learning",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Self representation"
    ],
    "authors": [
      {
        "surname": "Torpey",
        "given_name": "David"
      },
      {
        "surname": "Klein",
        "given_name": "Richard"
      }
    ]
  },
  {
    "title": "Joint Variational Inference Network for domain generalization",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110587",
    "abstract": "Domain generalization is a machine learning task that involves training a model on a set of domains with the goal of achieving high performance on unseen domains. While most domain generalization methods focus on extracting domain-invariant features, they may ignore domain-specific information beyond object styles which is label-relevant for classification. In this paper, we first propose a two-step data generation process in which the domain label and observed data are sampled from two distributions sequentially, and accordingly develop an end-to-end Joint Variational Inference Network (JVINet) for domain generalization. JVINet is a framework that admits a variational-based discriminative structure, which the domain-specific latent variable is learned and integrated to enhance the feature for classification. When testing on unseen target domains, the potential beneficial domain information is hence utilized to improve generalization ability. The overall objective is to optimize the variational lower bound of the conditional joint likelihood functions for both class and domain labels. We provide theoretical proof that JVINet can achieve an optimal lower bound using a variational-based discriminative approach. To evaluate the effectiveness of our method, we compare it with state-of-the-art methods on both simulated and real-world datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324003388",
    "keywords": [
      "Algorithm",
      "Architectural engineering",
      "Artificial intelligence",
      "Computer science",
      "Domain (mathematical analysis)",
      "Engineering",
      "Generalization",
      "Inference",
      "Joint (building)",
      "Mathematical analysis",
      "Mathematics"
    ],
    "authors": [
      {
        "surname": "Chu",
        "given_name": "Jun-Zheng"
      },
      {
        "surname": "Pan",
        "given_name": "Bin"
      },
      {
        "surname": "Xu",
        "given_name": "Xia"
      },
      {
        "surname": "Shi",
        "given_name": "Tian-Yang"
      },
      {
        "surname": "Shi",
        "given_name": "Zhen-Wei"
      },
      {
        "surname": "Li",
        "given_name": "Tao"
      }
    ]
  },
  {
    "title": "Evaluation of visual SLAM algorithms in unstructured planetary-like and agricultural environments",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.09.025",
    "abstract": "Given the significant advance in visual SLAM (VSLAM), it might be assumed that the location and mapping problem has been solved. Nevertheless, VSLAM algorithms may exhibit poor performance in unstructured environments. This paper addresses the problem of VSLAM in unstructured planetary-like and agricultural environments. A performance study of state-of-the-art algorithms in these environments was conducted to evaluate their robustness. Quantitative and qualitative results of the study are reported, which exposes that the impressive performance of most state-of-the-art VSLAM algorithms is not generally reflected in unstructured planetary-like and agricultural environments. Statistical scene analysis was performed on datasets from well-known structured environments as well as planetary-like and agricultural datasets to identify visual differences between structured and unstructured environments, which cause VSLAM algorithms to fail. In addition, strategies to overcome the VSLAM algorithm limitations in unstructured planetary-like and agricultural environments are suggested to guide future research on VSLAM in these environments.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552400285X",
    "keywords": [
      "Agriculture",
      "Algorithm",
      "Archaeology",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Geography"
    ],
    "authors": [
      {
        "surname": "Romero-Bautista",
        "given_name": "Víctor"
      },
      {
        "surname": "Altamirano-Robles",
        "given_name": "Leopoldo"
      },
      {
        "surname": "Díaz-Hernández",
        "given_name": "Raquel"
      },
      {
        "surname": "Zapotecas-Martínez",
        "given_name": "Saúl"
      },
      {
        "surname": "Sanchez-Medel",
        "given_name": "Nohemí"
      }
    ]
  },
  {
    "title": "CAST: Clustering self-Attention using Surrogate Tokens for efficient transformers",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.08.024",
    "abstract": "The Transformer architecture has shown to be a powerful tool for a wide range of tasks. It is based on the self-attention mechanism, which is an inherently computationally expensive operation with quadratic computational complexity: memory usage and compute time increase quadratically with the length of the input sequences, thus limiting the application of Transformers. In this work, we propose a novel Clustering self-Attention mechanism using Surrogate Tokens (CAST), to optimize the attention computation and achieve efficient transformers. CAST utilizes learnable surrogate tokens to construct a cluster affinity matrix, used to cluster the input sequence and generate novel cluster summaries. The self-attention from within each cluster is then combined with the cluster summaries of other clusters, enabling information flow across the entire input sequence. CAST improves efficiency by reducing the complexity from O ( N 2 ) to O ( α N ) where N is the sequence length, and α is constant according to the number of clusters and samples per cluster. We show that CAST performs better than or comparable to the baseline Transformers on long-range sequence modeling tasks, while also achieving higher results on time and memory efficiency than other efficient transformers.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524002563",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Electrical engineering",
      "Engineering",
      "Pattern recognition (psychology)",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "van Engelenhoven",
        "given_name": "Adjorn"
      },
      {
        "surname": "Strisciuglio",
        "given_name": "Nicola"
      },
      {
        "surname": "Talavera",
        "given_name": "Estefanía"
      }
    ]
  },
  {
    "title": "Deep graph layer information mining convolutional network",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110593",
    "abstract": "Graph convolution network is a powerful method of deep learning of graph structure data. Existing methods usually adjust the neighborhood information aggregation mode or optimize the graph topology layer by layer for improving the graph convolution network. However, these methods seldom consider the discriminative information about hierarchical characteristics nodes (some special nodes only can be correctly classified in one layer and are the misclassification nodes in the other layers of deep graph convolutional networks) in the different layers for complementing the neighborhood topology information. To further find these information, a deep graph layer information mining convolutional network (GLIM) can alternately measure the neighborhood ranking information on topology structure and update the residual identity mapping node information on the different layers for enhancing the model classification performance. Moreover, GLIM can construct a unified framework with the various hyper-parameters for the different graph learning method based on graph convolution network. Experiments show GLIM outperforms the state-of-the-art methods for semi-supervised node classification in three cite datasets (Cora, CiteSeer,and PubMed) and three image datasets (MNIST, Cifar10 and Cifar100).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324003443",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Deep learning",
      "Discriminative model",
      "Graph",
      "MNIST database",
      "Pattern recognition (psychology)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Lin",
        "given_name": "Guangfeng"
      },
      {
        "surname": "Wei",
        "given_name": "Wenchao"
      },
      {
        "surname": "Kang",
        "given_name": "Xiaobing"
      },
      {
        "surname": "Liao",
        "given_name": "Kaiyang"
      },
      {
        "surname": "Zhang",
        "given_name": "Erhu"
      }
    ]
  },
  {
    "title": "Adaptive propagation deep graph neural networks",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110607",
    "abstract": "Graph neural networks (GNNs) with adaptive propagation combinations represent a specialized deep learning paradigm, engineered to capture complex nodal interconnections within graph data. The primary challenge of this model lies in distilling and representing features extracted over varying nodal distances. This paper delves into an array of adaptive propagation strategies, with a focus on the influence of nodal distances and information aggregation on model efficacy. Our investigation identifies a critical performance drop in scenarios featuring overly brief propagation paths or an insufficient number of layers. Addressing this, we propose an innovative adaptive propagation technique in deep graph neural networks, named AP-DGNN, aimed at reconstructing high-order graph convolutional neural networks (GCNs). The AP-DGNN model assigns unique aggregation combination weights to each node and category, culminating in a final model representation through a process of weighted aggregation. Notably, these weights are capable of assimilating both subjective and objective information characteristics within the network. To substantiate our model’s effectiveness and scalability, we employed often-used benchmark datasets for experimental validation. A notable aspect of our AP-DGNN model is its minimal training parameter requirement and reduced computational demand. Furthermore, we demonstrate the model’s enhanced performance, which remains consistent across various hyperparameter configurations. This aspect was rigorously tested under diverse hyperparameter settings. Our findings contribute significantly to the evolution of graph neural networks, potentially revolutionizing their application across multiple domains. The research presented herein not only advances the understanding of GNNs but also paves the way for their robust application in varied scenarios. Codes are available at https://github.com/CW112/AP_DGNN.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324003583",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Graph",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Wei"
      },
      {
        "surname": "Yan",
        "given_name": "Wenxu"
      },
      {
        "surname": "Wang",
        "given_name": "Wenyuan"
      }
    ]
  },
  {
    "title": "Low-cost orthogonal basis-core extraction for classification and reconstruction using tensor ring",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110548",
    "abstract": "Tensor based methods have gained popularity for being able to represent multi-aspect real world data in a lower dimensional space. Among them, methods with orthogonal factors perform relatively better in classification. However, most of them cannot handle higher order data. Recently, Tensor Ring (TR) based methods are proposed to combat with the higher order issue more effectively focusing on both classification and reconstruction. A TR-based method with orthogonal cores performs reasonably well for a given error. However, its computational complexity is very high and might produce extra features. To solve these issues, in this paper, we propose a method named as Orthogonal basis-core extraction using Tensor Ring (OTR) that can facilitate better discrimination and reconstruction at a lower cost. To maintain the ring property, we also show, theoretically, that reshaping of the product of semi-orthogonal reshaped cores remains semi-orthogonal. Rigorous experiments over eighteen benchmark datasets from different fields demonstrate the superiority of OTR over state-of-the-art methods in terms of classification and reconstruction.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324002991",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Basis (linear algebra)",
      "Benchmark (surveying)",
      "Chemistry",
      "Computer science",
      "Core (optical fiber)",
      "Geodesy",
      "Geography",
      "Geometry",
      "Mathematics",
      "Organic chemistry",
      "Orthogonal basis",
      "Pattern recognition (psychology)",
      "Physics",
      "Pure mathematics",
      "Quantum mechanics",
      "Ring (chemistry)",
      "Telecommunications",
      "Tensor (intrinsic definition)",
      "Tensor product"
    ],
    "authors": [
      {
        "surname": "Akhter",
        "given_name": "Suravi"
      },
      {
        "surname": "Alam",
        "given_name": "Muhammad Mahbub"
      },
      {
        "surname": "Islam",
        "given_name": "Md. Shariful"
      },
      {
        "surname": "Momen",
        "given_name": "M. Arshad"
      },
      {
        "surname": "Islam",
        "given_name": "Md. Shariful"
      },
      {
        "surname": "Shoyaib",
        "given_name": "Mohammad"
      }
    ]
  },
  {
    "title": "OA-Pose: Occlusion-aware monocular 6-DoF object pose estimation under geometry alignment for robot manipulation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110576",
    "abstract": "Object pose estimation is the fundamental technology of robot manipulation systems. Recently, various learning-based monocular pose estimation methods have achieved outstanding performance by establishing sparse/dense 2D–3D correspondences. However, in cluttered environments, occlusion has been a challenging problem for pose estimation due to limited information provided by visible parts. In this work, we propose an efficient occlusion-aware monocular pose estimation method, called OA-Pose, to learn geometric feature information of occluded objects from cluttered scenes. Our framework takes RGB images as input and generates 2D–3D correspondences of visible and invisible parts based on the proposed Occlusion-aware Geometry Alignment Module. Extensive experiments show that our method is superior and competitive with state-of-the-art on multiple public datasets. We also conduct grasping experiments with different degrees of object occlusion, demonstrating the usability of our algorithm to deploy on robots in unstructured environments.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324003273",
    "keywords": [
      "3D pose estimation",
      "Articulated body pose estimation",
      "Artificial intelligence",
      "Cardiology",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Linguistics",
      "Medicine",
      "Monocular",
      "Monocular vision",
      "Object (grammar)",
      "Occlusion",
      "Philosophy",
      "Pose",
      "RGB color model",
      "Robot"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Jikun"
      },
      {
        "surname": "Luo",
        "given_name": "Luqing"
      },
      {
        "surname": "Liang",
        "given_name": "Weixiang"
      },
      {
        "surname": "Yang",
        "given_name": "Zhi-Xin"
      }
    ]
  },
  {
    "title": "Positional diffusion: Graph-based diffusion models for set ordering",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.10.010",
    "abstract": "Positional reasoning is the process of ordering an unsorted set of parts into a consistent structure. To address this problem, we present Positional Diffusion, a plug-and-play graph formulation with Diffusion Probabilistic Models. Using a diffusion process, we add Gaussian noise to the set elements’ position and map them to a random position in a continuous space. Positional Diffusion learns to reverse the noising process and recover the original positions through an Attention-based Graph Neural Network. To evaluate our method, we conduct extensive experiments on three different tasks and seven datasets, comparing our approach against the state-of-the-art methods for visual puzzle-solving, sentence ordering, and room arrangement, demonstrating that our method outperforms long-lasting research on puzzle solving with up to + 17 % compared to the second-best deep learning method, and performs on par against the state-of-the-art methods on sentence ordering and room rearrangement. Our work highlights the suitability of diffusion models for ordering problems and proposes a novel formulation and method for solving various ordering tasks. We release our code at https://github.com/IIT-PAVIS/Positional_Diffusion.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524002988",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Diffusion",
      "Diffusion map",
      "Dimensionality reduction",
      "Graph",
      "Mathematics",
      "Nonlinear dimensionality reduction",
      "Physics",
      "Programming language",
      "Set (abstract data type)",
      "Theoretical computer science",
      "Thermodynamics"
    ],
    "authors": [
      {
        "surname": "Giuliari",
        "given_name": "Francesco"
      },
      {
        "surname": "Scarpellini",
        "given_name": "Gianluca"
      },
      {
        "surname": "Fiorini",
        "given_name": "Stefano"
      },
      {
        "surname": "James",
        "given_name": "Stuart"
      },
      {
        "surname": "Morerio",
        "given_name": "Pietro"
      },
      {
        "surname": "Wang",
        "given_name": "Yiming"
      },
      {
        "surname": "Del Bue",
        "given_name": "Alessio"
      }
    ]
  },
  {
    "title": "Matrix normal PCA for interpretable dimension reduction and graphical noise modeling",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110591",
    "abstract": "Principal component analysis (PCA) is one of the most widely used dimension reduction and multivariate statistical techniques. From a probabilistic perspective, PCA seeks a low-dimensional representation of data in the presence of independent identical Gaussian noise. Probabilistic PCA (PPCA) and its variants have been extensively studied for decades. Most of them assume the underlying noise follows a certain independent identical distribution. However, the noise in the real world is usually complicated and structured. To address this challenge, some variants of PCA for non-IID data have been proposed. However, most of the existing methods only assume that the noise is correlated in the feature space while there may exist two-way structured noise. To this end, we propose a powerful and intuitive PCA method (MN-PCA) through modeling the graphical noise by the matrix normal distribution, which enables us to explore the structure of noise in both the feature space and the sample space. MN-PCA obtains a low-rank representation of data and the structure of noise simultaneously. And it can be explained as approximating data over the generalized Mahalanobis distance. We first solve this model by a standard approach – maximizing the regularized likelihood – and then develop a novel algorithm that exploits the Wasserstein distance, which is more robust. Extensive experiments on various data demonstrate their effectiveness.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032400342X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Combinatorics",
      "Composite material",
      "Computer science",
      "Dimension (graph theory)",
      "Dimensionality reduction",
      "Geometry",
      "Graphical model",
      "Image (mathematics)",
      "Materials science",
      "Mathematics",
      "Matrix (chemical analysis)",
      "Noise (video)",
      "Noise reduction",
      "Pattern recognition (psychology)",
      "Principal component analysis",
      "Reduction (mathematics)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Chihao"
      },
      {
        "surname": "Gai",
        "given_name": "Kuo"
      },
      {
        "surname": "Zhang",
        "given_name": "Shihua"
      }
    ]
  },
  {
    "title": "An efficient blur kernel estimation method for blind image Super-Resolution",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110590",
    "abstract": "Many existing space-variant blind image Super-Resolution (SR) methods require the generation of blur kernels for all input pixels. However, the existing methods overlook the possibility of similar blur kernel patterns among adjacent pixels, leading to spatial incoherence and wasteful computational resources. In this study, we introduce an efficient two-stage method for estimating blur kernels. Instead of generating pixel-wise kernels, a limited set of kernels at pixels with distinct information are generated first and the remaining kernels are reconstructed based on this initial set. This novel method, referred to as Anchor Detection and Kernel Reconstruction, comprises two main components: an Anchor Detection Module (ADM) and a Kernel Reconstruction Module (KRM). The objective of ADM is to identify pixels in a given Low-Resolution image that contain rich information, which are called anchors. The corresponding blur kernels, denoted as anchor kernels, are then generated for these identified pixels using a complete backbone network. The remaining blur kernels are reconstructed using KRM with a lightweight interpolation method based on the anchor kernels to enhance spatial consistency among the reconstructed pixels. Extensive experiments demonstrate that the proposed ADKR method maintains comparable performance while estimating only 50% of blur kernels with a full backbone network, reaching approximately 20% reduction in FLOPs. The code has been made available at https://github.com/xuyimin0926/ADKR.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324003418",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Image (mathematics)",
      "Interpolation (computer graphics)",
      "Kernel (algebra)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pixel"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Yimin"
      },
      {
        "surname": "Gao",
        "given_name": "Nanxi"
      },
      {
        "surname": "Chao",
        "given_name": "Fei"
      },
      {
        "surname": "Ji",
        "given_name": "Rongrong"
      }
    ]
  },
  {
    "title": "Looking beyond input frames: Self-supervised adaptation for video super-resolution",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110602",
    "abstract": "Recent test-time adaptive video super-resolution (VSR) methods have elevated the performance by exploiting the self-similar patches within the low-resolution (LR) frames to adapt to given input frames. However, the LR frames contain a limited amount of such patches, limiting the performance of such adaptation methods, especially for a challenging scale factor (e.g., × 4). In this work, we propose to explore beyond the input LR frames. In particular, we observe that a greater amount of self-similar patches across various scales can be found from estimated high-resolution (HR) frames (i.e., initially restored frames) produced by a pre-trained VSR network. Upon the observation, we propose a new self-supervision test-time adaptation approach via self-distillation to exploit such rich amount of self-similar patches from initially restored frames. Specifically, we perform self-distillation by exploiting multi-scale relationship: distilling knowledge from larger patches to smaller ones with similar semantics. Our framework is flexible and effective as the knowledge can be distilled either from the network itself or the larger one. Furthermore, our framework demonstrates the robustness, being able to recover from undesirable artifacts present in initially restored frames. Extensive evaluation with various VSR networks on numerous datasets reveals that our algorithm consistently improves the restoration quality by a large margin without ground-truth HR video frames. Code is available at: https://github.com/jinsuyoo/bissa.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324003534",
    "keywords": [
      "Adaptation (eye)",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Neuroscience",
      "Psychology",
      "Resolution (logic)"
    ],
    "authors": [
      {
        "surname": "Yoo",
        "given_name": "Jinsu"
      },
      {
        "surname": "Nam",
        "given_name": "Jihoon"
      },
      {
        "surname": "Baik",
        "given_name": "Sungyong"
      },
      {
        "surname": "Kim",
        "given_name": "Tae Hyun"
      }
    ]
  },
  {
    "title": "Uncertainty quantification metrics for deep regression",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.09.011",
    "abstract": "When deploying deep neural networks on robots or other physical systems, the learned model should reliably quantify predictive uncertainty. A reliable uncertainty allows downstream modules to reason about the safety of its actions. In this work, we address metrics for uncertainty quantification. Specifically, we focus on regression tasks, and investigate Area Under Sparsification Error (AUSE), Calibration Error (CE), Spearman’s Rank Correlation, and Negative Log-Likelihood (NLL). Using multiple datasets, we look into how those metrics behave under four typical types of uncertainty, their stability regarding the size of the test set, and reveal their strengths and weaknesses. Our results indicate that Calibration Error is the most stable and interpretable metric, but AUSE and NLL also have their respective use cases. We discourage the usage of Spearman’s Rank Correlation for evaluating uncertainties and recommend replacing it with AUSE.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524002733",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Regression",
      "Regression analysis",
      "Statistics",
      "Uncertainty quantification"
    ],
    "authors": [
      {
        "surname": "Kristoffersson Lind",
        "given_name": "Simon"
      },
      {
        "surname": "Xiong",
        "given_name": "Ziliang"
      },
      {
        "surname": "Forssén",
        "given_name": "Per-Erik"
      },
      {
        "surname": "Krüger",
        "given_name": "Volker"
      }
    ]
  },
  {
    "title": "Prototype learning based generic multiple object tracking via point-to-box supervision",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110588",
    "abstract": "Generic multiple object tracking aims to recover the trajectories for generic moving objects of the same category. This task relies on the ability of effectively extracting representative features of the target objects. To this end, we propose a novel prototype learning based model, PLGMOT, that can explore the template features of an exemplar object and extend to more objects to acquire their prototype. Their prototype features can be continuously updated during the video, in favor of generalization to all the target objects with different appearances. More importantly, on the public benchmark GMOT-40, our method achieves more than 14% advantage over the state-of-the-art methods, with less than 0.5% of the training data that is not even completely annotated in the form of bounding boxes, thanks to our proposed point-to-box label refinement training algorithm and hierarchical motion-aware association algorithm.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032400339X",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Computer vision",
      "Economics",
      "Generalization",
      "Geodesy",
      "Geography",
      "Geometry",
      "Image (mathematics)",
      "Machine learning",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Minimum bounding box",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Pedagogy",
      "Point (geometry)",
      "Psychology",
      "Task (project management)",
      "Tracking (education)",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Wenxi"
      },
      {
        "surname": "Lin",
        "given_name": "Yuhao"
      },
      {
        "surname": "Li",
        "given_name": "Qi"
      },
      {
        "surname": "She",
        "given_name": "Yinhua"
      },
      {
        "surname": "Yu",
        "given_name": "Yuanlong"
      },
      {
        "surname": "Pan",
        "given_name": "Jia"
      },
      {
        "surname": "Gu",
        "given_name": "Jason"
      }
    ]
  },
  {
    "title": "Ensemble filter-transfer learning algorithm",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110581",
    "abstract": "Transfer learning algorithms are capable to apply previously learned knowledge in source domain, which alleviates much expensive efforts of knowledge recollection in target domain. But the knowledge in source domain is always imperfect due to redundant or contaminated information. To solve this problem, an ensemble filter-transfer learning (EFTL) algorithm based on the source knowledge reconstruction is proposed in this paper. First, a knowledge partition strategy based on model is developed to classify the source knowledge into different types. Then, the positive knowledge can be identified, which contributes to target domain with a rejection of the negative transfer. Second, a knowledge filter algorithm is introduced to filter out the redundant information in non-positive knowledge. Then, the non-positive knowledge can be reconstructed by this algorithm to prevent the loss of available information. Third, an ensemble transfer mechanism is established to realize the synchronous transfer of omnidirectional knowledge for the target domain. Finally, comparative experiments on model prediction in practical applications are provided to illustrate the dependability of EFTL.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324003327",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Ensemble learning",
      "Filter (signal processing)",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Han",
        "given_name": "Honggui"
      },
      {
        "surname": "Li",
        "given_name": "Mengmeng"
      },
      {
        "surname": "Yang",
        "given_name": "Hongyan"
      },
      {
        "surname": "Wu",
        "given_name": "Xiaolong"
      },
      {
        "surname": "Han",
        "given_name": "Huayun"
      }
    ]
  },
  {
    "title": "LDDMM-Face: Large deformation diffeomorphic metric learning for cross-annotation face alignment",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110569",
    "abstract": "We propose an innovative, flexible, and consistent cross-annotation face alignment framework, LDDMM-Face, the key contribution of which is a deformation layer that naturally embeds facial geometry in a diffeomorphic way. This enables and solves cross-annotation face alignment tasks that were impossible in the existing works. Instead of predicting facial landmarks via a heatmap or coordinate regression, we formulate the face alignment task in a diffeomorphic registration manner and predict momenta that uniquely parameterize the deformation between the initial boundary and true boundary. We then perform large deformation diffeomorphic metric mapping (LDDMM) simultaneously for curve and landmark to localize the facial landmarks. The novel embedding of LDDMM into a deep network allows LDDMM-Face to consistently annotate facial landmarks without ambiguity and flexibly handle various annotation schemes, and can even predict dense annotations from sparse ones. To the best of our knowledge, this is the first study to leverage learning-based diffeomorphic mapping for face alignment. Our method can be easily integrated into various face alignment networks. We extensively evaluate LDDMM-Face on five benchmark datasets: 300W, WFLW, HELEN, COFW-68, and AFLW. LDDMM-Face distinguishes itself with outstanding performance when dealing with within-dataset cross-annotation learning (sparse-to-dense) and cross-dataset learning (different training and testing datasets). In addition, LDDMM-Face shows promising results on the most challenging task of cross-dataset cross-annotation learning (different training and testing datasets with different annotations). Our codes are available at https://github.com/CRazorback/LDDMM-Face.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324003200",
    "keywords": [
      "Annotation",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Deformation (meteorology)",
      "Diffeomorphism",
      "Economics",
      "Face (sociological concept)",
      "Geology",
      "Mathematics",
      "Metric (unit)",
      "Oceanography",
      "Operations management",
      "Pattern recognition (psychology)",
      "Pure mathematics",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Huilin"
      },
      {
        "surname": "Lyu",
        "given_name": "Junyan"
      },
      {
        "surname": "Cheng",
        "given_name": "Pujin"
      },
      {
        "surname": "Tam",
        "given_name": "Roger"
      },
      {
        "surname": "Tang",
        "given_name": "Xiaoying"
      }
    ]
  },
  {
    "title": "Publisher's note",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/S0167-8655(24)00334-9",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524003349",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Information retrieval"
    ],
    "authors": []
  },
  {
    "title": "Bending classification from interference signals of a fiber optic sensor using shallow learning and convolutional neural networks",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.06.029",
    "abstract": "Bending monitoring is critical in engineering applications, as it helps determine any structural deformation caused by load action or fatigue effect. While strain gauges and accelerometers were previously used to measure bending magnitude, optical fiber sensors have emerged as a reliable alternative. In this work, a machine-learning-based model is proposed to analyze the interference signal of an interferometric fiber sensor system and characterize the bending magnitude and direction. In particular, shallow learning-based and convolutional neural network-based (CNN) models have been implemented to perform this task. Furthermore, given the repeatability of the interference signals, a synthetic dataset was created to train the models, whereas real interferometric signals were used to evaluate the models’ performance. Experiments were conducted on a flexible rod in fixed–free and fixed–fixed ends configurations for bending monitoring. Although both models achieved mean accuracies above 91%, only the CNN-based model reached a mean accuracy above 98%. This confirms that monitoring bending movements through interference signal analysis by means of a CNN-based model is a viable approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524001995",
    "keywords": [
      "Accelerometer",
      "Acoustics",
      "Artificial intelligence",
      "Bending",
      "Channel (broadcasting)",
      "Computer science",
      "Convolutional neural network",
      "Engineering",
      "Fiber optic sensor",
      "Interference (communication)",
      "Interferometry",
      "Operating system",
      "Optical fiber",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Programming language",
      "SIGNAL (programming language)",
      "Strain gauge",
      "Structural engineering",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Valentín-Coronado",
        "given_name": "Luis M."
      },
      {
        "surname": "Martínez-Manuel",
        "given_name": "Rodolfo"
      },
      {
        "surname": "Esquivel-Hernández",
        "given_name": "Jonathan"
      },
      {
        "surname": "Martínez-Guerrero",
        "given_name": "Maria de los Angeles"
      },
      {
        "surname": "LaRochelle",
        "given_name": "Sophie"
      }
    ]
  },
  {
    "title": "On the effects of obfuscating speaker attributes in privacy-aware depression detection",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.10.016",
    "abstract": "Detection of depressive symptoms from spoken content has emerged as an efficient Artificial Intelligence (AI) tool for diagnosing this serious mental health condition. Since speech is a highly sensitive form of data, privacy-enhancing measures need to be in place for this technology to be useful. A common approach to enhance speech privacy is by using adversarial learning that involves concealing speaker’s specific attributes/identity while maintaining performance of the primary task. Although this technique works well for applications such as speech recognition, they are often ineffective for depression detection due to the interplay between certain speaker attributes and the performance of depression detection. This paper studies such interplay through a systematic study on how obfuscating specific speaker attributes (age, education) through adversarial learning impact the performance of a depression detection model. We highlight the relevance of two previously unexplored speaker attributes to depression detection, while considering a multimodal (audio-lexical) setting to highlight the relative vulnerabilities of the modalities under obfuscation. Results on a publicly available, clinically validated, depression detection dataset shows that attempts to disentangle age/education attributes through adversarial learning result in a large drop in depression detection accuracy, especially for the text modality. This calls for a revisit to how privacy mitigation should to be achieved for depression detection and any human-centric applications for that matter.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524003040",
    "keywords": [
      "Computer science",
      "Computer security",
      "Depression (economics)",
      "Economics",
      "Macroeconomics",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Aloshban",
        "given_name": "Nujud"
      },
      {
        "surname": "Esposito",
        "given_name": "Anna"
      },
      {
        "surname": "Vinciarelli",
        "given_name": "Alessandro"
      },
      {
        "surname": "Guha",
        "given_name": "Tanaya"
      }
    ]
  },
  {
    "title": "Measuring student behavioral engagement using histogram of actions",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.11.002",
    "abstract": "In this work, we propose a novel method for assessing students’ behavioral engagement by representing student’s actions and their frequencies over an arbitrary time interval as a histogram of actions. This histogram and the student’s gaze are utilized as input to a classifier that determines whether the student is engaged or not. For action recognition, we use students’ skeletons to model their postures and upper body movements. To learn the dynamics of a student’s upper body, a 3D-CNN model is developed. The trained 3D-CNN model recognizes actions within every 2-minute video segment then these actions are used to build the histogram of actions. To evaluate the proposed framework, we build a dataset consisting of 1414 video segments annotated with 13 actions and 963 2-minute video segments annotated with two engagement levels. Experimental results indicate that student actions can be recognized with top-1 accuracy 86.32% and the proposed framework can capture the average engagement of the class with a 90% F1-score.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524003088",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Histogram",
      "Image (mathematics)",
      "Mathematics education",
      "Pattern recognition (psychology)",
      "Psychology",
      "Student engagement"
    ],
    "authors": [
      {
        "surname": "Abdelkawy",
        "given_name": "Ahmed"
      },
      {
        "surname": "Farag",
        "given_name": "Aly"
      },
      {
        "surname": "Alkabbany",
        "given_name": "Islam"
      },
      {
        "surname": "Ali",
        "given_name": "Asem"
      },
      {
        "surname": "Foreman",
        "given_name": "Chris"
      },
      {
        "surname": "Tretter",
        "given_name": "Thomas"
      },
      {
        "surname": "Hindy",
        "given_name": "Nicholas"
      }
    ]
  },
  {
    "title": "A histogram-based approach to calculate graph similarity using graph neural networks",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.10.015",
    "abstract": "Deep learning has revolutionized the field of pattern recognition and machine learning by exhibiting exceptional efficiency in recognizing patterns. The success of deep learning can be seen in a wide range of applications including speech recognition, natural language processing, video processing, and image classification. Moreover, it has also been successful in recognizing structural patterns, such as graphs. Graph Neural Networks (GNNs) are models that employ message passing between nodes in a graph to capture its dependencies. These networks memorize a state that approximates graph information with greater depth compared to traditional neural networks. Although training a GNN can be challenging, recent advances in GNN variants, including Graph Convolutional Neural Networks, Gated Graph Neural Networks, and Graph Attention Networks, have shown promising results in solving various problems. In this work, we present a GNN-based approach for computing graph similarity and demonstrate its application to a classification problem. Our proposed method converts the similarity of two graphs into a score, and experiments on state-of-the-art datasets show that the proposed technique is effective and efficient. Results are summarized using a confusion matrix and mean square error metric, demonstrating the accuracy of our proposed technique.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524003039",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Graph",
      "Histogram",
      "Image (mathematics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Similarity (geometry)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Kajla",
        "given_name": "Nadeem Iqbal"
      },
      {
        "surname": "Missen",
        "given_name": "Malik Muhammad Saad"
      },
      {
        "surname": "Coustaty",
        "given_name": "Mickael"
      },
      {
        "surname": "Badar",
        "given_name": "Hafiz Muhammad Sanaullah"
      },
      {
        "surname": "Pasha",
        "given_name": "Maruf"
      },
      {
        "surname": "Belbachir",
        "given_name": "Faiza"
      }
    ]
  },
  {
    "title": "Collaborative brightening and amplification of low-light imagery via bi-level adversarial learning",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110558",
    "abstract": "Poor light conditions constrain the high pursuit of clarity and visible quality of photography especially smartphone devices. Admittedly, existing specific image processing methods, whether super-resolution methods or low-light enhancement methods, can hardly simultaneously enhance the resolution and brightness of low-light images at the same time. This paper dedicates a specialized enhancer with a dual-path modulated-interactive structure to recover high-quality sharp images in conditions of near absence of light, dubbed CollaBA, which learns the direct mapping from low-resolution dark-light images to their high-resolution normal sharp version. Specifically, we construct the generative modulation prior, serving as illuminance attention information, to regulate the exposure level of the neighborhood range. In addition, we construct an interactive degradation removal branch that progressively embeds the generated intrinsic prior to recover high-frequency detail and contrast at the feature level. We also introduce a multi-substrate up-scaler to integrate multi-scale sampling features, effectively addressing artifact-related problems. Rather than adopting the naive time-consuming learning strategy, we design a novel bi-level implicit adversarial learning mechanism as our fast training strategy. Extensive experiments on benchmark datasets — demonstrate our model’s wide-ranging applicability in various ultra-low-light scenarios, across 8 key performance metrics with significant improvements, notably achieving a 35.8% improvement in LPIPS and a 23.1% increase in RMSE. The code will be available at https://github.com/moriyaya/CollaBA.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324003091",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Computer science",
      "Environmental science",
      "Geography",
      "Geology",
      "Meteorology",
      "Remote sensing"
    ],
    "authors": [
      {
        "surname": "Gao",
        "given_name": "Jiaxin"
      },
      {
        "surname": "Liu",
        "given_name": "Yaohua"
      },
      {
        "surname": "Yue",
        "given_name": "Ziyu"
      },
      {
        "surname": "Fan",
        "given_name": "Xin"
      },
      {
        "surname": "Liu",
        "given_name": "Risheng"
      }
    ]
  },
  {
    "title": "Denoising diffusion model with adversarial learning for unsupervised anomaly detection on brain MRI images",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.10.007",
    "abstract": "This paper proposes the Adversarial Denoising Diffusion Model (ADDM). Diffusion models excel at generating high-quality samples, outperforming other generative models. These models also achieve outstanding medical image anomaly detection (AD) results due to their strong sampling ability. However, the performance of the diffusion model-based methods is highly varied depending on the sampling frequency, and the time cost to generate good-quality samples is significantly higher than that of other generative models. We propose the ADDM, a diffusion model-based AD method trained with adversarial learning that can maintain high-quality sample generation ability and significantly reduce the number of sampling steps. The proposed adversarial learning is achieved by classifying model-based denoised samples and samples to which random Gaussian noise is added to a specific sampling step. Compared with the loss function of diffusion models, defined under the noise space to minimise the predicted noise and scheduled noise, the diffusion model can explicitly learn semantic information about the sample space since adversarial learning is defined based on the sample space. Our experiment demonstrated that adversarial learning helps achieve a data sampling performance similar to the DDPM with much fewer sampling steps. Experimental results show that the proposed ADDM outperformed existing unsupervised AD methods on Brain MRI images. In particular, in the comparison using 22 T1-weighted MRI scans provided by the Centre for Clinical Brain Sciences from the University of Edinburgh, the ADDM achieves similar performance with 50% fewer sampling steps than other DDPM-based AD methods, and it shows 6.2% better performance about the Dice metric with the same number of sampling steps.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524002952",
    "keywords": [
      "Adversarial system",
      "Anomaly detection",
      "Artificial intelligence",
      "Computer science",
      "Diffusion",
      "Diffusion MRI",
      "Magnetic resonance imaging",
      "Medicine",
      "Noise reduction",
      "Pattern recognition (psychology)",
      "Physics",
      "Radiology",
      "Thermodynamics",
      "Unsupervised learning"
    ],
    "authors": [
      {
        "surname": "Yu",
        "given_name": "Jongmin"
      },
      {
        "surname": "Oh",
        "given_name": "Hyeontaek"
      },
      {
        "surname": "Lee",
        "given_name": "Younkwan"
      },
      {
        "surname": "Yang",
        "given_name": "Jinhong"
      }
    ]
  },
  {
    "title": "EDS: Exploring deeper into semantics for video captioning",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.09.017",
    "abstract": "Efficiently leveraging semantic information is crucial for advancing video captioning in recent years. But, prevailing approaches that involve designing various Part-of-Speech (POS) tags as prior information lack essential linguistic knowledge guidance throughout the training procedure, particularly in the context of POS and initial description generation. Furthermore, the restriction to a single source of semantic information ignores the potential for varied interpretations inherent in each video. To solve these problems, we propose the Exploring Deeper into Semantics (EDS) method for video captioning. EDS comprises three feasible modules that focus on semantic information. Specifically, we propose the Semantic Supervised Generation (SSG) module. It integrates semantic information as a prior, and facilitates enriched interrelations among words for POS supervision. A novel Similarity Semantic Extension (SSE) module is proposed to employ a query-based semantic expansion for collaboratively generating fine-grained content. Additionally, the proposed Input Semantic Enhancement (ISE) module provides a strategy for mitigating the information constraints faced during the initial phase of word generation. The experiments conducted show that, by exploiting semantic information through supervision, extension, and enhancement, EDS not only yields promising results but also underlines the effectiveness. Code will be available at https://github.com/BradenJoson/EDS.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524002824",
    "keywords": [
      "Artificial intelligence",
      "Closed captioning",
      "Computer science",
      "Image (mathematics)",
      "Multimedia",
      "Natural language processing",
      "Programming language",
      "Semantics (computer science)"
    ],
    "authors": [
      {
        "surname": "Lou",
        "given_name": "Yibo"
      },
      {
        "surname": "Zhang",
        "given_name": "Wenjie"
      },
      {
        "surname": "Song",
        "given_name": "Xiaoning"
      },
      {
        "surname": "Hua",
        "given_name": "Yang"
      },
      {
        "surname": "Wu",
        "given_name": "Xiao-Jun"
      }
    ]
  },
  {
    "title": "FAM: Adaptive federated meta-learning for MRI data",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.09.018",
    "abstract": "Federated learning enables multiple clients to collaborate to train a model without sharing data. Clients with insufficient data or data diversity participate in federated learning to learn a model with superior performance. MRI data suffers from inadequate data and different data distribution due to differences in MRI scanners and client characteristics. Also, privacy concerns preclude data sharing. In this work, we propose a novel adaptive federated meta-learning (FAM) mechanism for collaboratively learning a single global model, which is personalized locally on individual clients. The learnt sparse global model captures the common features in the MRI data across clients. This model is grown on each client to learn a personalized model by capturing additional client-specific parameters from local data. Experimental results on multiple data sets show that the personalization process at each client quickly converges using a limited number of epochs. The personalized client models outperformed the locally trained models, demonstrating the efficacy of the FAM mechanism. Additionally, the FAM-based sparse global model has fewer parameters that require less communication overhead during federated learning. This makes the model viable for networks with limited resources.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524002848",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Economics",
      "Machine learning",
      "Management",
      "Meta learning (computer science)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Sinha",
        "given_name": "Indrajeet Kumar"
      },
      {
        "surname": "Verma",
        "given_name": "Shekhar"
      },
      {
        "surname": "Singh",
        "given_name": "Krishna Pratap"
      }
    ]
  },
  {
    "title": "CvFormer: Cross-view transFormers with pre-training for fMRI analysis of human brain",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.09.010",
    "abstract": "In recent years, functional magnetic resonance imaging (fMRI) has been widely utilized to diagnose neurological disease, by exploiting the region of interest (RoI) nodes as well as their connectivities in human brain. However, most of existing works only rely on either RoIs or connectivities, neglecting the potential for complementary information between them. To address this issue, we study how to discover the rich cross-view information in fMRI data of human brain. This paper presents a novel method for cross-view analysis of fMRI data of the human brain, called Cross-view transFormers (CvFormer). CvFormer employs RoI and connectivity encoder modules to generate two separate views of the human brain, represented as RoI and sub-connectivity tokens. Then, basic transformer modules can be used to process the RoI and sub-connectivity tokens, and cross-view modules integrate the complement information across two views. Furthermore, CvFormer uses a global token for each branch as a query to exchange information with other branches in cross-view modules, which only requires linear time for both computational and memory complexity instead of quadratic time. To enhance the robustness of the proposed CvFormer, we propose a two-stage strategy to train its parameters. To be specific, RoI and connectivity views can be firstly utilized as self-supervised information to pre-train the CvFormer by combining it with contrastive learning and then fused to finetune the CvFormer using label information. Experiment results on two public ABIDE and ADNI datasets can show clear improvements by the proposed CvFormer, which can validate its effectiveness and superiority.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524002721",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Electrical engineering",
      "Engineering",
      "Geography",
      "Human brain",
      "Meteorology",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Psychology",
      "Training (meteorology)",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Meng",
        "given_name": "Xiangzhu"
      },
      {
        "surname": "Wei",
        "given_name": "Wei"
      },
      {
        "surname": "Liu",
        "given_name": "Qiang"
      },
      {
        "surname": "Wang",
        "given_name": "Yu"
      },
      {
        "surname": "Li",
        "given_name": "Min"
      },
      {
        "surname": "Wang",
        "given_name": "Liang"
      }
    ]
  },
  {
    "title": "A novel HMM distance measure with state alignment",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.10.018",
    "abstract": "In this paper, we introduce a novel distance measure that conforms to the definition of a semi-distance, for quantifying the similarity between Hidden Markov Models (HMMs). This distance measure is not only easier to implement, but also accounts for state alignment before distance calculation, ensuring correctness and accuracy. Our proposed distance measure presents a significant advancement in HMM comparison, offering a more practical and accurate solution compared to existing measures. Numerical examples that demonstrate the utility of the proposed distance measure are given for HMMs with continuous state probability densities. In real-world data experiments, we employ HMM to represent the evolution of financial time series or music. Subsequently, leveraging the proposed distance measure, we conduct HMM-based unsupervised clustering, demonstrating promising results. Our approach proves effective in capturing the inherent difference in dynamics of financial time series, showcasing the practicality and success of the proposed distance measure.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524003064",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Hidden Markov model",
      "Measure (data warehouse)",
      "Pattern recognition (psychology)",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Nan"
      },
      {
        "surname": "Leung",
        "given_name": "Cheuk Hang"
      },
      {
        "surname": "Yan",
        "given_name": "Xing"
      }
    ]
  },
  {
    "title": "Consistent graph learning for multi-view spectral clustering",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110598",
    "abstract": "Given the heterogeneous information of multiple views and the possible noise embedded in multi-view data, it is difficult to directly learn a consistent representation from multiple graphs to depict the intrinsic structure of all views. We propose a consistent graph learning method by exploiting both the high-order correlations underlying multiple views and the global structure of each single view. First, we calculate a transition probability matrix from each view. Second, a tensor is constructed by stacking each transition matrix as its frontal slice and then decomposed into the latent and error tensors. For the latent tensor, after rotated, a weighted tensor nuclear norm is used to encourage the rotate tensor to fully exploit the high-order correlations underlying multiple views. Furthermore, each frontal slice of the latent tensor is regularized by the nuclear norm to capture the global structure of each single view, and is restricted by probability constraints. Besides, we adopt the Frobenius-norm-based regularization to directly learn a common affinity matrix from the latent tensor. The established model is readily optimized by the alternating Lagrangian method. Extensive experiments on six real world datasets demonstrate that our method outperforms state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324003492",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Graph",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Spectral clustering",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Xie",
        "given_name": "Deyan"
      },
      {
        "surname": "Gao",
        "given_name": "Quanxue"
      },
      {
        "surname": "Zhao",
        "given_name": "Yougang"
      },
      {
        "surname": "Yang",
        "given_name": "Fan"
      },
      {
        "surname": "Song",
        "given_name": "Wei"
      }
    ]
  },
  {
    "title": "Efficient multi-task progressive learning for semantic segmentation and disparity estimation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110601",
    "abstract": "Scene understanding is an important area in robotics and autonomous driving. To accomplish these tasks, the 3D structures in the scene have to be inferred to know what the objects and their locations are. To this end, semantic segmentation and disparity estimation networks are typically used, but running them individually is inefficient since they require high-performance resources. A possible solution is to learn both tasks together using a multi-task approach. Some current methods address this problem by learning semantic segmentation and monocular depth together. However, monocular depth estimation from single images is an ill-posed problem. A better solution is to estimate the disparity between two stereo images and take advantage of this additional information to improve the segmentation. This work proposes an efficient multi-task method that jointly learns disparity and semantic segmentation. Employing a Siamese backbone architecture for multi-scale feature extraction, the method integrates specialized branches for disparity estimation and coarse and refined segmentations, leveraging progressive task-specific feature sharing and attention mechanisms to enhance accuracy for solving both tasks concurrently. The proposal achieves state-of-the-art results for joint segmentation and disparity estimation on three distinct datasets: Cityscapes, TrimBot2020 Garden, and S-ROSeS, using only 1 / 3 of the parameters of previous approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324003522",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Economics",
      "Estimation",
      "Machine learning",
      "Management",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Segmentation",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Cuevas-Velasquez",
        "given_name": "Hanz"
      },
      {
        "surname": "Galán-Cuenca",
        "given_name": "Alejandro"
      },
      {
        "surname": "Fisher",
        "given_name": "Robert B."
      },
      {
        "surname": "Gallego",
        "given_name": "Antonio Javier"
      }
    ]
  },
  {
    "title": "Table Transformers for imputing textual attributes",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.09.023",
    "abstract": "Missing data in tabular dataset is a common issue as the performance of downstream tasks usually depends on the completeness of the training dataset. Previous missing data imputation methods focus on numeric and categorical columns, but we propose a novel end-to-end approach called Table Transformers for Imputing Textual Attributes (TTITA) based on the transformer to impute unstructured textual columns using other columns in the table. We conduct extensive experiments on three datasets, and our approach shows competitive performance outperforming baseline models such as recurrent neural networks and Llama2. The performance improvement is more significant when the target sequence has a longer length. Additionally, we incorporate multi-task learning to simultaneously impute for heterogeneous columns, boosting the performance for text imputation. We also qualitatively compare with ChatGPT for realistic applications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524002782",
    "keywords": [
      "Arithmetic",
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Electrical engineering",
      "Engineering",
      "Information retrieval",
      "Mathematics",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Table (database)",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Wei",
        "given_name": "Ting-Ruen"
      },
      {
        "surname": "Wang",
        "given_name": "Yuan"
      },
      {
        "surname": "Inoue",
        "given_name": "Yoshitaka"
      },
      {
        "surname": "Wu",
        "given_name": "Hsin-Tai"
      },
      {
        "surname": "Fang",
        "given_name": "Yi"
      }
    ]
  },
  {
    "title": "WBNet: Weakly-supervised salient object detection via scribble and pseudo-background priors",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110579",
    "abstract": "Weakly supervised salient object detection (WSOD) methods endeavor to boost sparse labels to get more salient cues in various ways. Among them, an effective approach is using pseudo labels from multiple unsupervised self-learning methods, but inaccurate and inconsistent pseudo labels could ultimately lead to detection performance degradation. To tackle this problem, we develop a new multi-source WSOD framework, WBNet, that can effectively utilize pseudo-background (non-salient region) labels combined with scribble labels to obtain more accurate salient features. We first design a comprehensive salient pseudo-mask generator from multiple self-learning features. Then, we pioneer the exploration of generating salient pseudo-labels via point-prompted and box-prompted Segment Anything Models (SAM). Then, WBNet leverages a pixel-level Feature Aggregation Module (FAM), a mask-level Transformer-decoder (TFD), and an auxiliary Boundary Prediction Module (EPM) with a hybrid loss function to handle complex saliency detection tasks. Comprehensively evaluated with state-of-the-art methods on five widely used datasets, the proposed method significantly improves saliency detection performance. The code and results are publicly available at https://github.com/yiwangtz/WBNet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324003303",
    "keywords": [
      "Artificial intelligence",
      "Bayesian probability",
      "Code (set theory)",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Generator (circuit theory)",
      "Linguistics",
      "Object detection",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Pixel",
      "Power (physics)",
      "Prior probability",
      "Programming language",
      "Quantum mechanics",
      "Salient",
      "Set (abstract data type)",
      "Source code"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Yi"
      },
      {
        "surname": "Wang",
        "given_name": "Ruili"
      },
      {
        "surname": "He",
        "given_name": "Xiangjian"
      },
      {
        "surname": "Lin",
        "given_name": "Chi"
      },
      {
        "surname": "Wang",
        "given_name": "Tianzhu"
      },
      {
        "surname": "Jia",
        "given_name": "Qi"
      },
      {
        "surname": "Fan",
        "given_name": "Xin"
      }
    ]
  },
  {
    "title": "Contrastive representation enhancement and learning for handwritten mathematical expression recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.08.021",
    "abstract": "Handwritten mathematical expression recognition (HMER) is an appealing task due to its wide applications and research challenges. Previous deep learning-based methods used string decoder to emphasize on expression symbol awareness and achieved considerable recognition performance. However, these methods still meet an obstacle in recognizing handwritten symbols with varying appearance, in which huge appearance variations significantly lead to the ambiguity of symbol representation. To this end, our intuition is to employ printed expressions with unified appearance to serve as the template of handwritten expressions, alleviating the effects brought by varying symbol appearance. In this paper, we propose a contrastive learning method, where handwritten symbols with identical semantic are clustered together through the guidance of printed symbols, leading model to enhance the robustness of symbol semantic representations. Specifically, we propose an anchor generation scheme to obtain printed expression images corresponding with handwritten expressions. We propose a contrastive learning objective, termed Semantic-NCE Loss, to pull together printed and handwritten symbols with identical semantic. Moreover, we employ a string decoder to parse the calibrated semantic representations, outputting satisfactory expression symbols. The experiment results on benchmark datasets CROHME 14/16/19 demonstrate that our method noticeably improves recognition accuracy of handwritten expressions and outperforms the standard string decoder methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524002538",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Expression (computer science)",
      "Facial expression recognition",
      "Facial recognition system",
      "Law",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Programming language",
      "Representation (politics)",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Lin",
        "given_name": "Zihao"
      },
      {
        "surname": "Li",
        "given_name": "Jinrong"
      },
      {
        "surname": "Dai",
        "given_name": "Gang"
      },
      {
        "surname": "Chen",
        "given_name": "Tianshui"
      },
      {
        "surname": "Huang",
        "given_name": "Shuangping"
      },
      {
        "surname": "Lin",
        "given_name": "Jianmin"
      }
    ]
  },
  {
    "title": "Joint learning of latent subspace and structured graph for multi-view clustering",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110592",
    "abstract": "Most existing multi-view clustering methods rely solely on subspace clustering or graph-based clustering. Subspace clustering reduces the redundant information in high dimensional data, but it neglects the intrinsic structural dependencies among samples. Graph-based clustering can model the similarity among samples but tends to suffer from redundant information. In this paper, a novel framework jointing subspace learning and structured graph learning for multi-view clustering (SSMC) is proposed, which benefits from the merits of both subspace learning and structured graph learning. SSMC utilizes graph regularized subspace learning to obtain low dimensional consensus features, where the embedded features are ensured to have maximized correlation to reduce the redundant information, and the graph regularization forces embedded features to preserve their sample similarities. Meanwhile, an adaptive structured graph is learned based on the consensus features in the embedded feature space, avoiding the curse of dimensionality in the graph learning procedure. A rank constraint forces the learned graph to have exactly the same number of connected components as the number of clusters, to obtain a more reliable structured graph. Moreover, an effective algorithm is proposed to optimize the SSMC, where the graph regularized subspace learning part and the structured graph learning part are jointly optimized in a mutual reinforcement manner. The experimental results on real-world benchmark datasets show that the SSMC outperforms the state-of-the-arts in multi-view clustering tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324003431",
    "keywords": [
      "Architectural engineering",
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Engineering",
      "Graph",
      "Joint (building)",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Subspace topology",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Yinuo"
      },
      {
        "surname": "Guo",
        "given_name": "Yu"
      },
      {
        "surname": "Wang",
        "given_name": "Zheng"
      },
      {
        "surname": "Wang",
        "given_name": "Fei"
      }
    ]
  },
  {
    "title": "Polynomial kernel learning for interpolation kernel machines with application to graph classification",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.08.022",
    "abstract": "Since all training data is interpolated, interpolating classifiers have zero training error. However, recent work provides compelling reasons to investigate these classifiers, including their significance for ensemble methods. Interpolation kernel machines, which belong to the class of interpolating classifiers, are capable of good generalization and have proven to be an effective substitute for support vector machines, particularly for graph classification. In this work, we further enhance their performance by studying multiple kernel learning. To this end, we propose a general scheme of polynomial combined kernel functions, employing both quadratic and cubic kernel combinations in our experimental work. Our findings demonstrate that this approach improves performance compared to individual graph kernels. Our work supports the use of interpolation kernel machines as an alternative to support vector machines, thereby contributing to greater methodological diversity.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552400254X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Discrete mathematics",
      "Generalization",
      "Graph",
      "Graph kernel",
      "Kernel (algebra)",
      "Kernel embedding of distributions",
      "Kernel method",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Polynomial kernel",
      "Radial basis function kernel",
      "String kernel",
      "Support vector machine",
      "Theoretical computer science",
      "Tree kernel",
      "Variable kernel density estimation"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Jiaqi"
      },
      {
        "surname": "Liu",
        "given_name": "Cheng-Lin"
      },
      {
        "surname": "Jiang",
        "given_name": "Xiaoyi"
      }
    ]
  },
  {
    "title": "Personalized Federated Learning on long-tailed data via knowledge distillation and generated features",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.09.024",
    "abstract": "Personalized Federated Learning (PFL) offers a novel paradigm for distributed learning, which aims to learn a personalized model for each client through collaborative training of all distributed clients in a privacy-preserving manner. However, the performance of personalized models is often compromised by data heterogeneity and the challenges of long-tailed distributions, both of which are common in real-world applications. In this paper, we explore the joint problem of data heterogeneity and long-tailed distribution in PFL and propose a corresponding solution called Personalized Federated Learning with Distillation and generated Features (PFLDF). Specifically, we employ a lightweight generator trained on the server to generate a balanced feature set for each client that can supplement local minority class information with global class information. This augmentation mechanism is a robust countermeasure against the adverse effects of data imbalance. Subsequently, we use knowledge distillation to transfer the knowledge of the global model to personalized models to improve their generalization performance. Extensive experimental results show the superiority of PFLDF compared to other state-of-the-art PFL methods with long-tailed data distribution.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524002800",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Distillation",
      "Machine learning"
    ],
    "authors": [
      {
        "surname": "Lv",
        "given_name": "Fengling"
      },
      {
        "surname": "Qian",
        "given_name": "Pinxin"
      },
      {
        "surname": "Lu",
        "given_name": "Yang"
      },
      {
        "surname": "Wang",
        "given_name": "Hanzi"
      }
    ]
  },
  {
    "title": "Robust affine point matching via quadratic assignment on Grassmannians",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.09.016",
    "abstract": "Robust Affine Matching with Grassmannians (RoAM) is a new algorithm to perform affine registration of point clouds. The algorithm is based on minimizing the Frobenius distance between two elements of the Grassmannian. For this purpose, an indefinite relaxation of the Quadratic Assignment Problem (QAP) is used, and several approaches to affine feature matching are studied and compared. Experiments demonstrate that RoAM is more robust to noise and point discrepancy than previous methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524002794",
    "keywords": [
      "Affine combination",
      "Affine transformation",
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Geometry",
      "Matching (statistics)",
      "Mathematical optimization",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Point (geometry)",
      "Point set registration",
      "Pure mathematics",
      "Quadratic equation",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Kolpakov",
        "given_name": "Alexander"
      },
      {
        "surname": "Werman",
        "given_name": "Michael"
      }
    ]
  },
  {
    "title": "Explainable hypergraphs for gait based Parkinson classification",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.09.026",
    "abstract": "Parkinson Disease (PD) classification using Vertical Ground Reaction Force (VGRF) sensors can help in unobtrusive detection and monitoring of PD patients. State-of-the-art (SOTA) research in PD classification reveals that Deep Learning (DL), at the expense of explainability, performs better than Shallow Learning (SL). In this paper, we introduce a novel explainable weighted hypergraph, where the interconnections of the SOTA features are exploited, leading to more discriminative derived features, and thereby, forming an SL arm. In parallel, we create a DL arm consisting of ResNet architecture to learn the spatio-temporal patterns of the VGRF signals. Probabilities of PD classification scores from the SL and the DL arms are adaptively fused to create a hybrid pipeline. The pipeline achieves an AUC value of 0.979 on the Physionet Parkinson Dataset. This AUC value is found to be superior to the SL as well as the DL arm used in isolation, yielding respective AUCs of 0.878 and 0.852. The proposed pipeline demonstrates explainability through improved permutation feature importance and contrasting examples of use cases, where incorrect misclassification of the DL arm gets rectified by the SL arm and vice versa. We further demonstrate that our solution achieves comparable performance with SOTA methods. To the best of our knowledge, this is the first approach to analyze PD classification with a hypergraph based xAI (Explainable Artificial Intelligence).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524002885",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Disease",
      "Gait",
      "Gait analysis",
      "Medicine",
      "Parkinson's disease",
      "Pathology",
      "Pattern recognition (psychology)",
      "Physical medicine and rehabilitation"
    ],
    "authors": [
      {
        "surname": "Choudhury",
        "given_name": "Anirban Dutta"
      },
      {
        "surname": "Chowdhury",
        "given_name": "Ananda S."
      }
    ]
  },
  {
    "title": "Contrastive visual clustering for improving instance-level contrastive learning as a plugin",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110631",
    "abstract": "Contrastive learning has achieved remarkable success in computer vision, however it is built on instance-level discrimination which leaves the valuable intra-class correlation in dataset unexploited. Current semantic clustering methods are proven to be helpful but they would suffer from the error accumulated in the iteration process without ground-truth guidance. In an attempt to remedy the clustering error accumulation when utilizing intra-class correlation for contrastive learning, we propose an online Contrastive Visual Clustering (CVC) method with two actions: gathering instances with highly similar feature embeddings, and penalizing instances being clustered with low confidence. CVC can integrate with not only contrastive learning but also arbitrary self-supervised learning frameworks simply as a plugin. Under various experiment settings, we show that CVC improves the linear classification performance by a large margin for models pre-trained with self-supervised representation learning, in both image and video scenarios. The code is available at https://github.com/yliu1229/CVC.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324003820",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Plug-in",
      "Programming language"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Yue"
      },
      {
        "surname": "Zan",
        "given_name": "Xiangzhen"
      },
      {
        "surname": "Li",
        "given_name": "Xianbin"
      },
      {
        "surname": "Liu",
        "given_name": "Wenbin"
      },
      {
        "surname": "Fang",
        "given_name": "Gang"
      }
    ]
  },
  {
    "title": "Motion-guided small MAV detection in complex and non-planar scenes",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.09.013",
    "abstract": "In recent years, there has been a growing interest in the visual detection of micro aerial vehicles (MAVs) due to its importance in numerous applications. However, the existing methods based on either appearance or motion features encounter difficulties when the background is complex or the MAV is too small. In this paper, we propose a novel motion-guided MAV detector that can accurately identify small MAVs in complex and non-planar scenes. This detector first exploits a motion feature enhancement module to capture the motion features of small MAVs. Then it uses multi-object tracking and trajectory filtering to eliminate false positives caused by motion parallax. Finally, an appearance-based classifier and an appearance-based detector that operates on the cropped regions are used to achieve precise detection results. Our proposed method can effectively and efficiently detect extremely small MAVs from dynamic and complex backgrounds because it aggregates pixel-level motion features and eliminates false positives based on the motion and appearance features of MAVs. Experiments on the ARD-MAV dataset demonstrate that the proposed method could achieve high performance in small MAV detection under challenging conditions and outperform other state-of-the-art methods across various metrics.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524002757",
    "keywords": [
      "Artificial intelligence",
      "Computer graphics (images)",
      "Computer science",
      "Computer vision",
      "Motion (physics)",
      "Planar"
    ],
    "authors": [
      {
        "surname": "Guo",
        "given_name": "Hanqing"
      },
      {
        "surname": "Zheng",
        "given_name": "Canlun"
      },
      {
        "surname": "Zhao",
        "given_name": "Shiyu"
      }
    ]
  },
  {
    "title": "Innovative multi-stage matching for counting anything",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.09.014",
    "abstract": "Few-shot counting (FSC) is the task of counting the number of objects in an image that belong to the same category, by using a provided exemplar pattern. By replacing the exemplar, we can effectively count anything, even in cases where we have no prior knowledge of that category’s exemplar. However, due to the variations within the same category and the impact of inter-class similarity, it is challenging to achieve accurate intra-class similarity matching using conventional similarity comparison methods. To tackle these issues, we propose a novel few-shot counting method called Multi-stage Exemplar Attention Match Network (MEAMNet), which increases the accuracy of matching, reduces the impact of noise, and enhances similarity feature matching. Specifically, we propose a multi-stage matching strategy to obtain more stable and effective matching results by acquiring similar feature in different feature spaces. In addition, we propose a novel feature matching module called Exemplar Attention Match (EAM). With this module, the intra-class similarity representation in each stage will be enhanced to achieve a better matching of the key feature. Experimental results indicate that our method not only significantly surpasses the state-of-the-art (SOTA) methods in most evaluation metrics on the FSC-147 dataset but also achieves comprehensive superiority on the CARPK dataset. This highlights the outstanding accuracy and stability of our matching performance, as well as its exceptional transferability. We will release the code at https://github.com/hzg0505/MEAMNet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524002769",
    "keywords": [
      "Arithmetic",
      "Artificial intelligence",
      "Computer science",
      "Geology",
      "Matching (statistics)",
      "Mathematics",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Stage (stratigraphy)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Shihui"
      },
      {
        "surname": "Huang",
        "given_name": "Zhigang"
      },
      {
        "surname": "Zhan",
        "given_name": "Sheng"
      },
      {
        "surname": "Li",
        "given_name": "Ping"
      },
      {
        "surname": "Cui",
        "given_name": "Zhiguo"
      },
      {
        "surname": "Li",
        "given_name": "Feiyu"
      }
    ]
  },
  {
    "title": "GANN: Graph Alignment Neural Network for semi-supervised learning",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110484",
    "abstract": "Graph neural networks (GNNs) have been widely investigated in the field of semi-supervised graph machine learning. Most methods fail to exploit adequate graph information when labeled data is limited, leading to the problem of oversmoothing. To overcome this issue, we propose the Graph Alignment Neural Network (GANN), a simple and effective graph neural architecture. A unique learning algorithm with three alignment rules is proposed to thoroughly explore hidden information for insufficient labels. Firstly, to better investigate attribute specifics, we suggest the feature alignment rule to align the inner product of both the attribute and embedding matrices. Secondly, to properly utilize the higher-order neighbor information, we propose the cluster center alignment rule, which involves aligning the inner product of the cluster center matrix with the unit matrix. Finally, to get reliable prediction results with few labels, we establish the minimum entropy alignment rule by lining up the prediction probability matrix with its sharpened result. Extensive studies on graph benchmark datasets demonstrate that GANN can achieve considerable benefits in semi-supervised node classification and outperform state-of-the-art competitors.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324002358",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Graph",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Song",
        "given_name": "Linxuan"
      },
      {
        "surname": "Tu",
        "given_name": "Wenxuan"
      },
      {
        "surname": "Zhou",
        "given_name": "Sihang"
      },
      {
        "surname": "Zhu",
        "given_name": "En"
      }
    ]
  },
  {
    "title": "Non-convex tensorial multi-view clustering by integrating ℓ 1 -based sliced-Laplacian regularization and ℓ 2 , p -sparsity",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110605",
    "abstract": "Consider the recent upswing in interest around multi-view clustering procedures. Such methods aim to boost clustering efficiency by leveraging information from numerous perspectives. Much research has been devoted to tensorial representation to exploit high-order correlations underlying disparate views while preserving the local geometric structure inside each view. Our research introduces a novel multi-view clustering approach. This approach creates a 3rd-order tensor, assimilating features from all perspectives. We use the t-product in the tensor space to generate the self-representation tensor from the tensorial data. We incorporate the ℓ 1 -based sliced-Laplacian regularization to increase our model’s resilience and introduce a fresh column-wise sparse norm: the ℓ 2 , p -norm with 0 < p < 1 . This norm displays attributes of invariance, continuity, and differentiability. We present a closed-form answer to the ℓ 2 , p -regularized shrinkage problem, broadening its relevance to other generalized problems. Simultaneously, we propose a tensorial arctan -function as an improved surrogate for the tensor rank. This function has proven more proficient at assessing consistency across multiple viewpoints. By integrating these two components, we formulate an effective algorithm that refines our suggested model, ensuring that the constructed sequence gravitates toward the stationary KKT point. Our team conducts extensive experiments on various datasets to evaluate our model’s effectiveness, spanning diverse situations and scales. Results from these experiments emphasize that our approach establishes a novel performance standard.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032400356X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Geometry",
      "Mathematics",
      "Regular polygon"
    ],
    "authors": [
      {
        "surname": "Xie",
        "given_name": "Deyan"
      },
      {
        "surname": "Yang",
        "given_name": "Ming"
      },
      {
        "surname": "Gao",
        "given_name": "Quanxue"
      },
      {
        "surname": "Song",
        "given_name": "Wei"
      }
    ]
  },
  {
    "title": "Hierarchical Glocal Attention Pooling for Graph Classification",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.09.009",
    "abstract": "Graph pooling is an essential operation in Graph Neural Networks that reduces the size of an input graph while preserving its core structural properties. Existing pooling methods find a compressed representation considering the Global Topological Structures (e.g., cliques, stars, clusters) or Local information at node level (e.g., top- k informative nodes). However, an effective graph pooling method does not hierarchically integrate both Global and Local graph properties. To this end, we propose a dual-fold Hierarchical Global Local Attention Pooling (HGLA-Pool) layer that exploits the aforementioned graph properties, generating more robust graph representations. Exhaustive experiments on nine publicly available graph classification benchmarks under standard metrics show that HGLA-Pool significantly outperforms eleven state-of-the-art models on seven datasets while being on par for the remaining two.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552400271X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Globalization",
      "Glocalization",
      "Graph",
      "Law",
      "Pattern recognition (psychology)",
      "Political science",
      "Pooling",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Ali",
        "given_name": "Waqar"
      },
      {
        "surname": "Vascon",
        "given_name": "Sebastiano"
      },
      {
        "surname": "Stadelmann",
        "given_name": "Thilo"
      },
      {
        "surname": "Pelillo",
        "given_name": "Marcello"
      }
    ]
  },
  {
    "title": "Use estimated signal and noise to adjust step size for image restoration",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.09.006",
    "abstract": "Image deblurring is a challenging inverse problem, especially when there is additive noise to the observation. To solve such an inverse problem in an iterative manner, it is important to control the step size for achieving a stable and robust performance. We designed a method that controls the progress of iterative process in solving the inverse problem without the need for a user-specified step size. The method searches for an optimal step size under the assumption that the signal and noise are two independent stochastic processes. Experiments show that the method can achieve good performance in the presence of noise and imperfect knowledge about the blurring kernel. Tests also show that, for different blurring kernels and noise levels, the difference between two consecutive estimates given by the new method tends to remain more stable and stay in a smaller range, as compared to those given by some existing techniques. This stable feature makes the new method more robust in the sense that it is easier to select a stopping threshold for the new method to use in different scenarios.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524002629",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Image (mathematics)",
      "Image processing",
      "Image restoration",
      "Mathematics",
      "Noise (video)",
      "Pattern recognition (psychology)",
      "Programming language",
      "SIGNAL (programming language)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Min"
      },
      {
        "surname": "Liu",
        "given_name": "Shupeng"
      },
      {
        "surname": "Li",
        "given_name": "Taihao"
      },
      {
        "surname": "Chen",
        "given_name": "Huai"
      },
      {
        "surname": "Xu",
        "given_name": "Xiaoyin"
      }
    ]
  },
  {
    "title": "CRTrack: Learning Correlation-Refine network for visual object tracking",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110582",
    "abstract": "Conducting reliable feature interaction plays a critical role in the visual tracking community, especially in recent dominated Siamese-based tracking paradigm. In general, there are two primary approaches for fusing representations from template and search area in the Siamese setting, i.e., cross-correlation and transformer modeling. The former provides a straightforward interaction solution, which may have limitations in handling complex scenarios, such as appearance variations and occlusion. While the latter offers an effective interaction mechanism, albeit with higher computation complexity and model cost. In contrast to traditional Siamese-based trackers which rely on two mentioned feature cross-correlation operators, this paper proposes a novel Correlation-Refine network to address the issue of lacking semantic information caused by local linear matching in correlation, from both spatial and channel perspectives. Correlation-Refine network (named CR) is solely built on top of fully convolutional layers, without employing intricate transformer mechanisms or complex methods to fuse features from multiple scales. Moreover, we present a concise yet effective convolutional tracking framework based on the correlation-refine network. CR network can increase the discriminative ability of semantic information in a coarse-to-fine manner: it gradually learns the semantic features of the target to be tracked and suppresses interference from similar objects by stacking multiple CR layers. Extensive experiments and comparisons with recent competitive trackers in challenging large-scale benchmarks demonstrate that, our tracker outperforms all previous convolutional trackers and has competitive results with transformer-based method. The code will be made available.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324003339",
    "keywords": [
      "Artificial intelligence",
      "BitTorrent tracker",
      "Computer science",
      "Correlation",
      "Discriminative model",
      "Eye tracking",
      "Feature (linguistics)",
      "Geometry",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Transformer",
      "Visualization",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Wenkang"
      },
      {
        "surname": "Xie",
        "given_name": "Fei"
      },
      {
        "surname": "Xu",
        "given_name": "Tianyang"
      },
      {
        "surname": "Zhai",
        "given_name": "Jiang"
      },
      {
        "surname": "Yang",
        "given_name": "Wankou"
      }
    ]
  },
  {
    "title": "Local Reference Feature Transfer (LRFT): A simple pre-processing step for image enhancement",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.10.013",
    "abstract": "Low-light, nighttime haze, and underwater images captured in harsh environments typically exhibit color deviations and reduced visibility due to light scattering and absorption. Additionally, we observe an almost complete loss of information in at least one color channel in these degraded images. To repair the lost information in each channel, we present an image preprocessing strategy called Local Reference Feature Transfer (LRFT), which employs the local feature to compensate for the color loss automatically. Specifically, we design a dedicated reference image by fusing the detail, salience, and uniform grayscale images of the raw image that ensures a balanced chromaticity distribution. Subsequently, we employ the local reference feature transfer strategy to migrate the local mean and variance of the reference image to the raw image to get a color-corrected image. Extensive evaluation experiments demonstrate that our proposed LRFT method has good preprocessing performance for the subsequent enhancement of images of different degradation types. The code is publicly available at: https://www.researchgate.net/publication/383528251_2024-LRFT.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524003015",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Epistemology",
      "Feature (linguistics)",
      "Feature detection (computer vision)",
      "Image (mathematics)",
      "Image enhancement",
      "Image processing",
      "Linguistics",
      "Parallel computing",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Simple (philosophy)",
      "Transfer (computing)"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Ling"
      },
      {
        "surname": "Zhang",
        "given_name": "Weidong"
      },
      {
        "surname": "Zheng",
        "given_name": "Yuchao"
      },
      {
        "surname": "Wang",
        "given_name": "Jianping"
      },
      {
        "surname": "Zhao",
        "given_name": "Wenyi"
      }
    ]
  },
  {
    "title": "Label-noise learning via uncertainty-aware neighborhood sample selection",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.09.012",
    "abstract": "Existing deep learning methods often require a large amount of high-quality labeled data. Yet, the presence of noisy labels in the real-world training data seriously affects the generalization ability of the model. Sample selection techniques, the current dominant approach to mitigating the effects of noisy labels on models, use the consistency of sample predictions and observed labels to make clean selections. However, these methods rely heavily on the accuracy of the sample predictions and inevitably suffer when the model predictions are unstable. To address these issues, we propose an uncertainty-aware neighborhood sample selection method. Especially, it calibrates for sample prediction by neighbor prediction and reassigns model attention to the selected samples based on sample uncertainty. By alleviating the influence of prediction bias on sample selection and avoiding the occurrence of prediction bias, our proposed method achieves excellent performance in extensive experiments. In particular, we achieved an average of 5% improvement in asymmetric noise scenarios.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524002745",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Image (mathematics)",
      "Machine learning",
      "Noise (video)",
      "Pattern recognition (psychology)",
      "Sample (material)",
      "Selection (genetic algorithm)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Yiliang"
      },
      {
        "surname": "Lu",
        "given_name": "Yang"
      },
      {
        "surname": "Wang",
        "given_name": "Hanzi"
      }
    ]
  },
  {
    "title": "MLENet: Multi-Level Extraction Network for video action recognition",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110614",
    "abstract": "Human action recognition is a well-established task in the field of computer vision. However, accurately representing spatio-temporal information remains a challenge due to the complex interplay between human actions, video timing, and scene changes. To address this challenge and improve the efficiency of temporal modeling in videos, we propose MLENet, a novel approach that eliminates contextual data and eliminates the need for laborious optical flow extraction.MLENet incorporates a Temporal Feature Refinement Extraction Module (TFREM) that utilizes Optical Flow Guided Features to enhance attention to local deep detail information. This refinement process significantly enhances the network’s capacity for feature learning and expression. Moreover, MLENet is designed to be trained end-to-end, facilitating seamless integration into existing frameworks. Additionally, our model adopts a temporal segmentation structure for sampling, effectively reducing redundant information and improving computational efficiency. Compared to existing video-based action recognition models that require optical flow or other modalities, MLENet achieves substantial performance enhancements while requiring fewer inputs. We validate the effectiveness of our proposed approach on benchmark datasets, including Something-Something V1&V2, UCF-101, and HMDB-51, where MLENet consistently outperforms state-of-the-art models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324003650",
    "keywords": [
      "Action (physics)",
      "Action recognition",
      "Artificial intelligence",
      "Chemistry",
      "Chromatography",
      "Class (philosophy)",
      "Computer science",
      "Computer vision",
      "Extraction (chemistry)",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Fan"
      },
      {
        "surname": "Li",
        "given_name": "Xinke"
      },
      {
        "surname": "Xiong",
        "given_name": "Han"
      },
      {
        "surname": "Mo",
        "given_name": "Haofan"
      },
      {
        "surname": "Li",
        "given_name": "Yongming"
      }
    ]
  },
  {
    "title": "Main genes in breast cancer primary tumor and first metastasis in lymph nodes revealed by information-theory-based genetic networks pattern analysis",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.07.006",
    "abstract": "In this paper, we use pattern analysis in genetic networks to identify differentially expressed genes in primary breast cancer tumors and their first metastasis in lymph nodes, using human biopsies from the GEO and GDCDP databases. By applying Information-Theory-based algorithms to process gene expression profile matrices, we obtained the genetic networks of the following tissues: (1) breast cancer-free, (2) primary breast cancer tumors, and (3) first metastasis of breast cancer in lymph nodes. Topological analysis of the genetic networks delves for identifying patterns of pairs of genes with higher mutual information than a threshold; then, among these genes, the ones with highest degree are elected. We propose the plausible hypothesis that the elected genes, having principal roles in each network, could be relevant as biomarkers regarding the genetic information. A subsequent gene ontology-based analysis of the molecular and functional characteristics of these genes reveals specific signaling pathways signatures in cancer-free tissue and in the tumor microenvironment associated with primary and metastatic requirements. Furthermore, a state-of-the-art review of the functional roles of genes reveals tumor suppressor genes in cancer-free tissue and proliferation- and migration-associated genes in cancer.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524002095",
    "keywords": [
      "Bioinformatics",
      "Biology",
      "Breast cancer",
      "Cancer",
      "Cancer research",
      "Computational biology",
      "Gene",
      "Genetics",
      "Metastasis",
      "Primary tumor"
    ],
    "authors": [
      {
        "surname": "Martínez Vargas",
        "given_name": "Irving Ulises"
      },
      {
        "surname": "León Pineda",
        "given_name": "Moises Omar"
      },
      {
        "surname": "Alvarado Mentado",
        "given_name": "Matías"
      }
    ]
  },
  {
    "title": "Robust feature selection via central point link information and sparse latent representation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110617",
    "abstract": "Before conducting unsupervised feature selection, it is usually assumed that these data are independent of each other. On the contrary, real data will influence each other. Therefore, traditional feature selection methods may lose information related to each other between data. This can lead to inaccurately generated pseudo-label information and may result in poor feature selection results. To find solutions to this issue, this paper proposes robust feature selection via central point link information and sparse latent representation (CPSLR). Firstly, structure a link graph by calculating the center matrix to store the distance information from the sample to the center point. If two samples have similar distances to the center point, it can be determined that they belong to the same class. Therefore, the similarity between samples is preserved, and more accurate pseudo-label information is obtained. Secondly, CPSLR uses data graph and link graph to form a dual graph structure. It can not only retain the link information between samples but also retain the manifold structures of the samples. Then, CPSLR saves the interconnection contents between samples by sparse latent representation. That is, the constraint l 2,1-norm is exerted on the expression of latent representation, and sparse non-redundant interconnection information is preserved. And by combining central point link information with sparse latent representation makes the interconnections between data reserved more comprehensive. That is to say, the pseudo-labels obtained are more like the real labels of the classes. Finally, CPSLR constrains the feature transformation matrix by l 2,1/2-norm constraint so as to select robust and sparse features. CPSLR uses l 2,1/2-norm constraint to assure that the feature transformation matrix is sparse, selecting more discriminative features, thereby obtaining the feature selection that can improve its efficiency. The experiments demonstrate that the clustering result of CPSLR outperform six classical or latest compared algorithms on eight datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324003686",
    "keywords": [
      "Artificial intelligence",
      "Computer network",
      "Computer science",
      "Feature (linguistics)",
      "Feature selection",
      "Law",
      "Linguistics",
      "Link (geometry)",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Sparse approximation"
    ],
    "authors": [
      {
        "surname": "Kong",
        "given_name": "Jiarui"
      },
      {
        "surname": "Shang",
        "given_name": "Ronghua"
      },
      {
        "surname": "Zhang",
        "given_name": "Weitong"
      },
      {
        "surname": "Wang",
        "given_name": "Chao"
      },
      {
        "surname": "Xu",
        "given_name": "Songhua"
      }
    ]
  },
  {
    "title": "Color matching in the wild",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110575",
    "abstract": "We present a method that, given two different views of the same scene taken by two cameras with unknown settings and internal parameters, corrects the colors of one of the images making it look as if it was captured under the other camera settings. Our method is able to deal with any standard non-linear encoded images (gamma-corrected, logarithmic-encoded, or any other) without requiring any previous knowledge of the encoding. To this end, our method makes use of two important observations. First, the camera imaging pipeline from RAW to sRGB can be well approximated by considering just a per-pixel shading and a color transformation matrix, and second, for correcting the images we only need to estimate a single matrix –that will contain information from both of the original images– and an approximation of the shading term (that emulates the non-linearity). Our proposed method is fast and the results have no spurious artifacts. The method outperforms the state-of-the-art when compared with other methods that do not require knowledge of the encoding used. It is also able to compete with –and even surpass in some cases– methods that consider information about image encoding.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324003261",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Classical mechanics",
      "Composite material",
      "Computer science",
      "Computer vision",
      "Encoding (memory)",
      "Gene",
      "Image (mathematics)",
      "Kinematics",
      "Logarithm",
      "Machine learning",
      "Matching (statistics)",
      "Materials science",
      "Mathematical analysis",
      "Mathematics",
      "Matrix (chemical analysis)",
      "Pattern recognition (psychology)",
      "Physics",
      "Pipeline (software)",
      "Pixel",
      "Programming language",
      "Spurious relationship",
      "Statistics",
      "Transformation (genetics)",
      "Transformation matrix"
    ],
    "authors": [
      {
        "surname": "Gil Rodríguez",
        "given_name": "Raquel"
      },
      {
        "surname": "Vazquez-Corral",
        "given_name": "Javier"
      },
      {
        "surname": "Bertalmío",
        "given_name": "Marcelo"
      },
      {
        "surname": "Finlayson",
        "given_name": "Graham D."
      }
    ]
  },
  {
    "title": "LuminanceGAN: Controlling the brightness of generated images for various night conditions",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.10.014",
    "abstract": "There are diverse datasets available for training deep learning models utilized in autonomous driving. However, most of these datasets are composed of images obtained in day conditions, leading to a data imbalance issue when dealing with night condition images. Several day-to-night image translation models have been proposed to resolve the insufficiency of the night condition dataset, but these models often generate artifacts and cannot control the brightness of the generated image. In this study, we propose a LuminanceGAN, for controlling the brightness degree in night conditions to generate realistic night image outputs. The proposed novel Y-control loss converges the brightness degree of the output image to a specific luminance value. Furthermore, the implementation of the self-attention module effectively reduces artifacts in the generated images. Consequently, in qualitative comparisons, our model demonstrates superior performance in day-to-night image translation. Additionally, a quantitative evaluation was conducted using lane detection models, showing that our proposed method improves performance in night lane detection tasks. Moreover, the quality of the generated indoor dark images was assessed using an evaluation metric. It can be proven that our model generates images most similar to real dark images compared to other image translation models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524003027",
    "keywords": [
      "Artificial intelligence",
      "Brightness",
      "Computer graphics (images)",
      "Computer science",
      "Computer vision",
      "Optics",
      "Physics"
    ],
    "authors": [
      {
        "surname": "Seo",
        "given_name": "Junghyun"
      },
      {
        "surname": "Wang",
        "given_name": "Sungjun"
      },
      {
        "surname": "Jeon",
        "given_name": "Hyeonjae"
      },
      {
        "surname": "Kim",
        "given_name": "Taesoo"
      },
      {
        "surname": "Jin",
        "given_name": "Yongsik"
      },
      {
        "surname": "Kwon",
        "given_name": "Soon"
      },
      {
        "surname": "Kim",
        "given_name": "Jeseok"
      },
      {
        "surname": "Lim",
        "given_name": "Yongseob"
      }
    ]
  },
  {
    "title": "Online probabilistic knowledge distillation on cryptocurrency trading using Deep Reinforcement Learning",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.10.005",
    "abstract": "Leveraging Deep Reinforcement Learning (DRL) for training agents for financial trading has gained significant attention in recent years. However, training these agents in noisy financial environments remains challenging and unstable, significantly impacting their performance as trading agents, as the recent literature has also showcased. This paper introduces a novel distillation method for DRL agents, aiming to improve the training stability of DRL agents. The proposed method transfers knowledge from a teacher ensemble to a student model, incorporating both the action probability distribution knowledge from the output layer, as well as the knowledge from the intermediate layers of the teacher’s network. Furthermore, the proposed method also works in an online fashion, allowing for eliminating the separate teacher training process typically involved in many DRL distillation pipelines, simplifying the distillation process. The proposed method is extensively evaluated on a large-scale cryptocurrency trading setup, demonstrating its ability to both lead to significant improvements in trading accuracy and obtained profit, as well as increase the stability of the training process.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524002939",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Cryptocurrency",
      "Distillation",
      "Machine learning",
      "Probabilistic logic",
      "Reinforcement learning",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Moustakidis",
        "given_name": "Vasileios"
      },
      {
        "surname": "Passalis",
        "given_name": "Nikolaos"
      },
      {
        "surname": "Tefas",
        "given_name": "Anastasios"
      }
    ]
  },
  {
    "title": "Analysis of systems’ performance in natural language processing competitions",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.03.010",
    "abstract": "Collaborative competitions have gained popularity in the scientific and technological fields. These competitions involve defining tasks, selecting evaluation scores, and devising result verification methods. In the standard scenario, participants receive a training set and are expected to provide a solution for a held-out dataset kept by organizers. An essential challenge for organizers arises when comparing algorithms’ performance, assessing multiple participants, and ranking them. Statistical tools are often used for this purpose; however, traditional statistical methods often fail to capture decisive differences between systems’ performance. This manuscript describes an evaluation methodology for statistically analyzing competition results and competition. The methodology is designed to be universally applicable; however, it is illustrated using eight natural language competitions as case studies involving classification and regression problems. The proposed methodology offers several advantages, including off-the-shell comparisons with correction mechanisms and the inclusion of confidence intervals. Furthermore, we introduce metrics that allow organizers to assess the difficulty of competitions. Our analysis shows the potential usefulness of our methodology for effectively evaluating competition results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524000783",
    "keywords": [
      "Archaeology",
      "Computer science",
      "History",
      "Natural (archaeology)",
      "Natural language processing"
    ],
    "authors": [
      {
        "surname": "Nava-Muñoz",
        "given_name": "Sergio"
      },
      {
        "surname": "Graff",
        "given_name": "Mario"
      },
      {
        "surname": "Escalante",
        "given_name": "Hugo Jair"
      }
    ]
  },
  {
    "title": "NCART: Neural Classification and Regression Tree for tabular data",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110578",
    "abstract": "Deep learning models have become popular in the analysis of tabular data, as they address the limitations of decision trees and enable valuable applications like semi-supervised learning, online learning, and transfer learning. However, these deep-learning approaches often encounter a trade-off. On one hand, they can be computationally demanding when dealing with large-scale or high-dimensional datasets. On the other hand, they may lack interpretability and may not be suitable for small-scale datasets. In this study, we propose a novel interpretable neural network called Neural Classification and Regression Tree (NCART) to overcome these challenges. NCART is a modified version of Residual Networks that replaces fully-connected layers with multiple differentiable oblivious decision trees. By integrating decision trees into the architecture, NCART maintains its interpretability while benefiting from the end-to-end capabilities of neural networks. The simplicity of the NCART architecture makes it well-suited for datasets of varying sizes and reduces computational costs compared to state-of-the-art deep learning models. Extensive numerical experiments demonstrate the superior performance of NCART compared to existing deep learning models, establishing it as a strong competitor to tree-based models. The code is available at https://github.com/Luojiaqimath/NCART.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324003297",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Data mining",
      "Decision tree",
      "Deep learning",
      "Interpretability",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Physics",
      "Quantum mechanics",
      "Scale (ratio)",
      "Tree (set theory)"
    ],
    "authors": [
      {
        "surname": "Luo",
        "given_name": "Jiaqi"
      },
      {
        "surname": "Xu",
        "given_name": "Shixin"
      }
    ]
  },
  {
    "title": "One-index vector quantization based adversarial attack on image classification",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.09.001",
    "abstract": "To improve storage and transmission, images are generally compressed. Vector quantization (VQ) is a popular compression method as it has a high compression ratio that suppresses other compression techniques. Despite this, existing adversarial attack methods on image classification are mostly performed in the pixel domain with few exceptions in the compressed domain, making them less applicable in real-world scenarios. In this paper, we propose a novel one-index attack method in the VQ domain to generate adversarial images by a differential evolution algorithm, successfully resulting in image misclassification in victim models. The one-index attack method modifies a single index in the compressed data stream so that the decompressed image is misclassified. It only needs to modify a single VQ index to realize an attack, which limits the number of perturbed indexes. The proposed method belongs to a semi-black-box attack, which is more in line with the actual attack scenario. We apply our method to attack three popular image classification models, i.e., Resnet, NIN, and VGG16. On average, 55.9 % and 77.4 % of the images in CIFAR-10 and Fashion MNIST, respectively, are successfully attacked, with a high level of misclassification confidence and a low level of image perturbation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524002575",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Contextual image classification",
      "Image (mathematics)",
      "Index (typography)",
      "Learning vector quantization",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Quantization (signal processing)",
      "Vector quantization",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Fan",
        "given_name": "Haiju"
      },
      {
        "surname": "Qin",
        "given_name": "Xiaona"
      },
      {
        "surname": "Chen",
        "given_name": "Shuang"
      },
      {
        "surname": "Shum",
        "given_name": "Hubert P． H．"
      },
      {
        "surname": "Li",
        "given_name": "Ming"
      }
    ]
  },
  {
    "title": "MMIFR: Multi-modal industry focused data repository",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.11.001",
    "abstract": "In the rapidly advancing field of industrial automation, the availability of robust and diverse datasets is crucial for the development and evaluation of machine learning models. The data repository consists of four distinct versions of datasets: MMIFR-D, MMIFR-FS, MMIFR-OD and MMIFR-P. The MMIFR-D dataset comprises a comprehensive assemblage of 5907 images accompanied by corresponding textual descriptions, notably facilitating the application of industrial equipment classification. In contrast, the MMIFR-FS dataset serves as an alternative variant characterized by the inclusion of 129 distinct classes and 5907 images, specifically catering to the task of few-shot learning within the industrial domain. MMIFR-OD is another alternative variant, comprising 8,839 annotation instances across 128 distinct categories, is predominantly utilized for object detection tasks. Additionally, the MMIFR-P dataset consists of 142 textual–visual information pairs, making it suitable for detecting pairs of industrial equipment. Furthermore, we conduct a comprehensive comparative analysis of our dataset in relation to other datasets used in industrial settings. Benchmark performances for different industrial tasks on our data repository are provided. The proposed multimodal dataset, MMIFR, can be utilized for research in industrial automation, quality control, safety monitoring, and other relevant domains.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524003076",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Materials science",
      "Modal",
      "Polymer chemistry"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Mingxuan"
      },
      {
        "surname": "Li",
        "given_name": "Shiqi"
      },
      {
        "surname": "Wei",
        "given_name": "Xujun"
      },
      {
        "surname": "Song",
        "given_name": "Jiacheng"
      }
    ]
  },
  {
    "title": "Observation weights matching approach for causal inference",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110549",
    "abstract": "This study introduces a novel method integrating pattern recognition models with causal inference methodologies to adeptly identify and manage overlapping regions between treatment and control groups. Our approach, Observation Weights Matching (OWM), addresses the intrinsic challenges in observational studies—specifically, the fixed sample size and the lack of complete overlap in pretreatment variables. Through ensemble learning, OWM effectively retains examples within these critical overlapping regions, systematically generating weighted data distributions that aid in the precise identification of these instances. By prioritizing hard-to-classify observations and employing a novel metric of critical values for matched samples, our approach optimizes matching performance and provides greater robustness in causal analysis. Through empirical and simulation studies, we demonstrate OWM's notable advantage over traditional matching methods, enhancing causal inference in observational research. Furthermore, we show that OWM provides richer balance scores than propensity scores, ensuring unbiased estimations and advancing the field significantly.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324003005",
    "keywords": [
      "Artificial intelligence",
      "Causal inference",
      "Computer science",
      "Inference",
      "Matching (statistics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Lee",
        "given_name": "Kangbok"
      },
      {
        "surname": "Han",
        "given_name": "Sumin"
      },
      {
        "surname": "Baik",
        "given_name": "Hyeoncheol"
      },
      {
        "surname": "Jeong",
        "given_name": "Yeasung"
      },
      {
        "surname": "Park",
        "given_name": "Young Woong"
      }
    ]
  },
  {
    "title": "DRGNN: Disentangled representation graph neural network for diverse category-level recommendations",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.09.008",
    "abstract": "Graph neural networks (GNNs) have significantly advanced recommender systems (RecSys) by enhancing their accuracy in complex collaborative filtering scenarios. However, this progress often comes at the cost of overlooking the diversity of recommendations, a factor in user satisfaction. Addressing this gap, this paper introduces the disentangled representation graph neural network (DRGNN). DRGNN integrates diversification into the candidate generation stage using two specialized modules. The first employs disentangled representation learning to separate item preferences from category preferences, thereby mitigating category bias in recommendations. The second module, focusing on positive sample selection, further reduces category bias. This approach not only maintains the high-order connectivity strengths of GNNs but also substantially improves the diversity of recommendations. Our extensive validation of DRGNN on three comprehensive web service datasets, Taobao, Amazon Beauty and MSD, shows that it not only matches the state-of-the-art methods in accuracy but also excels in achieving a balanced trade-off between accuracy and diversity in recommendations.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524002708",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Graph",
      "Law",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Sugiyama",
        "given_name": "Takuto"
      },
      {
        "surname": "Yoshida",
        "given_name": "Soh"
      },
      {
        "surname": "Muneyasu",
        "given_name": "Mitsuji"
      }
    ]
  },
  {
    "title": "Rethinking local-to-global representation learning for rotation-invariant point cloud analysis",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110624",
    "abstract": "Point cloud analysis has drawn much attention in recent years, whereas most existing point-based deep networks ignore the rotation-invariant property of the encoded features, which leads to poor performance given 3D shapes with arbitrary rotation. In this paper, we propose a novel rotation-invariant method that embeds both distinctive local and global rotation-invariant information. Specifically, we design a two-branch network that separately extracts purely local and global rotation-invariant features. In the global branch, we leverage canonical transformation to extract global representations, while in the local branch, we utilize hand-crafted geometric features (e.g., relative distances and angles) to embed local representations. To fuse the features from distinct branches, we introduce an attention-based fusion module to adaptively integrate the local-to-global representation by considering the geometry contexts of each point. Particularly, different from existing rotation-invariant works, we further introduce a self-attention unit into the global branch for embedding non-local information and also insert multiple fusion modules into the local branch to emphasize the global features. Extensive experiments on standard benchmarks show that our method achieves consistent and competitive performance on various downstream tasks, and also the best performance on the shape classification task on the ModelNet40 dataset with a 0.8% accuracy gain, compared to state-of-the-art methods. The code and pre-trained models are available at https://github.com/CentauriStar/Rotation-Invariant-Point-Cloud-Analysis.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324003753",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Embedding",
      "Invariant (physics)",
      "Leverage (statistics)",
      "Mathematical physics",
      "Mathematics",
      "Point cloud",
      "Rotation (mathematics)",
      "Theoretical computer science",
      "Topology (electrical circuits)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Zhaoxuan"
      },
      {
        "surname": "Yu",
        "given_name": "Yunlong"
      },
      {
        "surname": "Li",
        "given_name": "Xianzhi"
      }
    ]
  },
  {
    "title": "A unified framework to stereotyped behavior detection for screening Autism Spectrum Disorder",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.10.001",
    "abstract": "We propose a unified pipeline for the task of stereotyped behaviors detection for early diagnosis of Autism Spectrum Disorder (ASD). Current methods for analyzing autism-related behaviors of ASD children primarily focus on action classification tasks utilizing pre-trimmed video segments, limiting their real-world applicability. To overcome these challenges, we develop a two-stage network for detecting stereotyped behaviors: one for temporally localizing repetitive actions and another for classifying behavioral types. Specifically, building on the observation that stereotyped behaviors commonly manifest in various repetitive forms, our method proposes an approach to localize video segments where arbitrary repetitive behaviors are observed. Subsequently, we classify the detailed types of behaviors within these localized segments, identifying actions such as arm flapping, head banging, and spinning. Extensive experimental results on SSBD and ESBD datasets demonstrate that our proposed pipeline surpasses existing baseline methods, achieving a classification accuracy of 88.3% and 88.6%, respectively. The code and dataset will be publicly available at https://github.com/etri/AI4ASD/tree/main/pbr4RRB.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524002897",
    "keywords": [
      "Artificial intelligence",
      "Autism",
      "Autism spectrum disorder",
      "Cognitive psychology",
      "Computer science",
      "Developmental psychology",
      "Pattern recognition (psychology)",
      "Physics",
      "Psychology",
      "Quantum mechanics",
      "Spectrum (functional analysis)"
    ],
    "authors": [
      {
        "surname": "Yoo",
        "given_name": "Cheol-Hwan"
      },
      {
        "surname": "Yoo",
        "given_name": "Jang-Hee"
      },
      {
        "surname": "Back",
        "given_name": "Moon-Ki"
      },
      {
        "surname": "Wang",
        "given_name": "Woo-Jin"
      },
      {
        "surname": "Shin",
        "given_name": "Yong-Goo"
      }
    ]
  },
  {
    "title": "DDOWOD: DiffusionDet for open-world object detection",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.10.002",
    "abstract": "Open-world object detection (OWOD) poses a significant challenge in computer vision, requiring models to detect unknown objects and incrementally learn new categories. To explore this field, we propose the DDOWOD based on the DiffusionDet. It is more likely to cover unknown objects hidden in the background and can reduce the model’s bias towards known class objects during training due to its ability to randomly generate boxes and reconstruct the characteristics of the GT from them. Also, to improve the insufficient quality of pseudo-labels which leads to reduced accuracy in recognizing unknown classes, we use the Segment Anything Model (SAM) as the teacher model in distillation learning to endow DDOWOD with rich visual knowledge. Surprisingly, compared to other existing models, our DDOWOD is more suitable for using SAM as the teacher. Furthermore, we proposed the Stepwise distillation (SD) which is a new incremental learning method specialized for our DDOWOD to avoid catastrophic forgetting during the training. Our approach utilizes all previously trained models from past tasks rather than solely relying on the last one. DDOWOD has achieved excellent performance. U-Recall is 53.2, 51.5, 50.7 in OWOD split and U-AP is 21.9 in IntensiveSet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524002903",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Fan",
        "given_name": "Jiaqi"
      },
      {
        "surname": "Zhang",
        "given_name": "Enming"
      },
      {
        "surname": "Wei",
        "given_name": "Ying"
      },
      {
        "surname": "Wang",
        "given_name": "Yuefeng"
      },
      {
        "surname": "Xia",
        "given_name": "Jiakun"
      },
      {
        "surname": "Liu",
        "given_name": "Junwei"
      },
      {
        "surname": "Liu",
        "given_name": "Xinghong"
      },
      {
        "surname": "Ma",
        "given_name": "Shuailei"
      }
    ]
  },
  {
    "title": "A complex neural network model by Hilbert Transform",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.09.021",
    "abstract": "The phase information of the optical wave plays a vital role in processing wave-related signals. In deep learning fields, complex-valued neural networks are put forward on the concept of complex amplitudes for full utilization of phase information. To build a complex-valued neural network, the common way is to exploit Fourier Transform of the observed signal to extract amplitude and phase information. However, this will lead to spectrum waste for a real-valued signal by introducing negative frequencies that have no physical meaning. To this end, we attempt to use Hilbert Transform as an alternative to yield a single sideband spectrum and avoid negative frequencies from interacting with positive ones. On the other hand, Fourier transform is a global analysis thus it tells nothing about the time domain. As our key insight, we further explore the usage of instantaneous frequency calculated by Hilbert Transform and propose a new method of constructing complex input from a time–frequency angle. Simple pixel-wise classification experiments are carried out on two hyperspectral datasets and MNIST dataset. Experimental results have demonstrated that Hilbert Transform with instantaneous frequency performs better by a large margin than Fourier Transform owing to the additional time information.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524002873",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer vision",
      "Filter (signal processing)",
      "Hilbert spectral analysis",
      "Hilbert transform",
      "Mathematics",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Xinzhi"
      },
      {
        "surname": "Yu",
        "given_name": "Jun"
      },
      {
        "surname": "Kurihara",
        "given_name": "Toru"
      },
      {
        "surname": "Wu",
        "given_name": "Congzhong"
      },
      {
        "surname": "Zhang",
        "given_name": "Haiyan"
      },
      {
        "surname": "Zhan",
        "given_name": "Shu"
      }
    ]
  },
  {
    "title": "Joint-individual fusion structure with fusion attention module for multi-modal skin cancer classification",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110604",
    "abstract": "Many convolutional neural network (CNN) based approaches for skin cancer classification primarily rely on dermatological images, yielding commendable results in classification accuracy. However, leveraging patient metadata, a crucial source of clinical information for dermatologists, can further enhance accuracy. Current methodologies predominantly employ basic joint fusion structures (FS) and fusion modules (FMs) for multi-modal classification, leaving room for advancement in enhancing accuracy through exploration of more sophisticated FS and FM architectures. Thus, this paper introduces a novel fusion method that integrates dermatological images (dermoscopy images or clinical images) with patient metadata for skin cancer classification, focusing on enhancing FS and FM components. Initially, we propose a joint-individual fusion (JIF) structure that simultaneously learns shared features across multi-modality data while preserving specific characteristics. Subsequently, we introduce a multi-modal fusion attention (MMFA) module designed to amplify the most relevant image and metadata features through a combination of self and mutual attention mechanisms, thereby bolstering the decision-making pipeline. Our study compares the efficacy of the proposed JIF-MMFA method with other state-of-the-art fusion techniques across three distinct public datasets. Results demonstrate that the JIF-MMFA method consistently enhances classification outcomes across various CNN backbones, outperforming alternative fusion methodologies on all three datasets. These findings underscore the effectiveness and robustness of our proposed approach in skin cancer classification.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324003558",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Engineering",
      "Fusion",
      "Joint (building)",
      "Linguistics",
      "Materials science",
      "Modal",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Polymer chemistry",
      "Sensor fusion",
      "Structural engineering"
    ],
    "authors": [
      {
        "surname": "Tang",
        "given_name": "Peng"
      },
      {
        "surname": "Yan",
        "given_name": "Xintong"
      },
      {
        "surname": "Nan",
        "given_name": "Yang"
      },
      {
        "surname": "Hu",
        "given_name": "Xiaobin"
      },
      {
        "surname": "Menze",
        "given_name": "Bjoern H."
      },
      {
        "surname": "Krammer",
        "given_name": "Sebastian"
      },
      {
        "surname": "Lasser",
        "given_name": "Tobias"
      }
    ]
  },
  {
    "title": "TSAR-MVS: Textureless-aware segmentation and correlative refinement guided multi-view stereo",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110565",
    "abstract": "The reconstruction of textureless areas has long been a challenging problem in MVS due to lack of reliable pixel correspondences between images. In this paper, we propose the Textureless-aware Segmentation And Correlative Refinement guided Multi-View Stereo (TSAR-MVS), a novel method that effectively tackles challenges posed by textureless areas in 3D reconstruction through filtering, refinement and segmentation. First, we implement the joint hypothesis filtering, a technique that merges a confidence estimator with a disparity discontinuity detector to eliminate incorrect depth estimations. Second, to spread the pixels with confident depth, we introduce an iterative correlation refinement strategy that leverages RANSAC to generate 3D planes based on superpixels, succeeded by a weighted median filter for broadening the influence of accurately determined pixels. Finally, we present a textureless-aware segmentation method that leverages edge detection and line detection for accurately identify large textureless regions for further depth completion. Experiments on ETH3D, Tanks & Temples and Strecha datasets demonstrate the superior performance and strong generalization capability of our proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324003169",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Correlative",
      "Dilation (metric space)",
      "Discontinuity (linguistics)",
      "Estimator",
      "Filter (signal processing)",
      "Generalization",
      "Image (mathematics)",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pixel",
      "RANSAC",
      "Segmentation",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Yuan",
        "given_name": "Zhenlong"
      },
      {
        "surname": "Cao",
        "given_name": "Jiakai"
      },
      {
        "surname": "Wang",
        "given_name": "Zhaoqi"
      },
      {
        "surname": "Li",
        "given_name": "Zhaoxin"
      }
    ]
  },
  {
    "title": "Nonconvex submodule clustering via joint sliced sparse gradient and cluster-aware approach",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110619",
    "abstract": "Most existing subspace clustering methods preprocess image data by converting them into vectors, which lacks exploration of the spatial structure of high-dimensional data. Therefore, we proposes a nonconvex submodule clustering model (NSSGCA) via joint sliced sparse gradient and cluster-aware approach. NSSGCA arranges each 2D image as lateral slices of a 3rd-order tensor, and utilizes the t -product under the model of the union of free submodules to represent 3rd-order tensor samples, thereby exploring the latent spatial structure of samples. To more accurately approximate tensor rank, a nonconvex Schatten p -norm constraint is imposed on the rotated representation tensor. Under the submodule framework, a consistent gradient matrix is derived based on the δ -nearest neighbor adjacency graph to construct sliced sparse gradient (SSG) regularization, which is more conducive to clustering tasks. NSSGCA learns representation tensor with clearer block-like structure based on ℓ q norm and cluster-aware attention mechanism. The convergence of the constructed sequence to the stationary Karush–Kuhn–Tucker (KKT) point is proven. Experimental results on real-world image datasets confirm the effectiveness of NSSGCA.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324003704",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Cluster (spacecraft)",
      "Cluster analysis",
      "Computer science",
      "Engineering",
      "Joint (building)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Programming language",
      "Structural engineering"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Jingyu"
      },
      {
        "surname": "Deng",
        "given_name": "Tingquan"
      },
      {
        "surname": "Yang",
        "given_name": "Ming"
      }
    ]
  },
  {
    "title": "Self-ensembling depth completion via density-aware consistency",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110618",
    "abstract": "Depth completion can predict a dense depth map by taking a sparse depth map and the aligned RGB image as input, but the acquisition of ground truth annotations is labor-intensive and non-scalable. Therefore, we resort to semi-supervised learning, where we only need to annotate a few images and leverage massive unlabeled data without ground truth labels to facilitate model learning. In this paper, we propose SEED, a SElf-Ensembling Depth completion framework to enhance the generalization of the model on unlabeled data. Specifically, SEED contains a pair of the teacher and student models, which are given high-density and low-density sparse depth maps as input respectively. The main idea underpinning SEED is to enforce the density-aware consistency by encouraging consistent prediction across different-density input depth maps. One empirical challenge is that the pseudo-depth labels produced by the teacher model inevitably contain wrong depth values, which would mislead the convergence of the student model. To resist the noisy labels, we propose an automatic method to measure the reliability of the generated pseudo-depth labels adaptively. By leveraging the discrepancy of prediction distributions, we model the pixel-wise uncertainty map as the prediction variance and rectify the training process from noisy labels explicitly. To our knowledge, we are among the early semi-supervised attempts on the depth completion task. Extensive experiments on both outdoor and indoor datasets demonstrate that SEED consistently improves the performance of the baseline model by a large margin and even is on par with several fully-supervised methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324003698",
    "keywords": [
      "Artificial intelligence",
      "Completion (oil and gas wells)",
      "Computer science",
      "Consistency (knowledge bases)",
      "Geology",
      "Pattern recognition (psychology)",
      "Petroleum engineering"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Xuanmeng"
      },
      {
        "surname": "Zheng",
        "given_name": "Zhedong"
      },
      {
        "surname": "Jiang",
        "given_name": "Minyue"
      },
      {
        "surname": "Ye",
        "given_name": "Xiaoqing"
      }
    ]
  },
  {
    "title": "ClarityDiffuseNet: Enhancing fundus image quality under black shadows with diffusion model-based research",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.10.012",
    "abstract": "Deep learning models have achieved commendable success in the analysis of tasks related to fundus images. However, the performance of many models is affected by the quality of fundus images. A common quality issue observed in fundus images is the presence of severe black shadow artefact, primarily caused by opacities in the refractive media or due to insufficient or uneven illumination. Such low-quality images can compromise the model training and result in the models learning incorrect feature representations. The removal of black shadows can be regarded as a preprocessing problem in the enhancement of degraded images. Solutions typically involve either increasing the overall brightness of the image or restoring the dark shadowy areas. Prior work on increasing image brightness has often utilized generative adversarial networks (GANs), while restoration has been approached with autoencoders and variational autoencoders (VAEs). However, approaches that focus on brightening often fall short in properly addressing local degradations, and restoration techniques can lead to loss of details or over-smoothing in the shadow areas. In this study, we introduce a method named ClarityDiffuseNet, a model for restoring low-quality fundus images based on diffusion generative models targeting severe black shadows. Our method restores areas with black shadows from regions of high quality, enhancing the image to be richer in detail and visually closer to artefact-free images. Compared to models based on GANs and inpainting methods, our approach demonstrates superior performance on four benchmark public datasets, with Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity index (SSIM) indices significantly surpassing the state-of-the-art models by 7% and 9%, respectively. Our method demonstrates notable improvements in downstream tasks, such as disease diagnosis—evidenced by 9% increase in the area under the curve (AUC) metric when tested on low-quality datasets—in vessel segmentation, which saw about 6% improvement in the Dice coefficient under similar conditions. These outcomes underscore the substantial promise of diffusion generative models within the realm of fundus image restoration, highlighting their effectiveness in enhancing image quality for further analysis.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524003003",
    "keywords": [
      "Artificial intelligence",
      "Computer graphics (images)",
      "Computer science",
      "Computer vision",
      "Diffusion",
      "Fundus (uterus)",
      "Image (mathematics)",
      "Image quality",
      "Medicine",
      "Ophthalmology",
      "Physics",
      "Quality (philosophy)",
      "Quantum mechanics",
      "Thermodynamics"
    ],
    "authors": [
      {
        "surname": "Dong",
        "given_name": "Jiadi"
      },
      {
        "surname": "Qian",
        "given_name": "Tianwei"
      },
      {
        "surname": "Jiang",
        "given_name": "Yuxian"
      },
      {
        "surname": "Bi",
        "given_name": "Lei"
      },
      {
        "surname": "Kim",
        "given_name": "Jinman"
      },
      {
        "surname": "Wang",
        "given_name": "Lisheng"
      },
      {
        "surname": "Xu",
        "given_name": "Xun"
      }
    ]
  },
  {
    "title": "Feature selection for multilabel classification with missing labels via multi-scale fusion fuzzy uncertainty measures",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110580",
    "abstract": "Numerous high-dimension multilabel data are generated, posing a challenge for multilabel learning. Building effective learning models with discriminative features is essential to improve the performance of multilabel learning. Multilabel feature selection can filter out the discriminative features according to their contribution to classification. However, ambiguity, uncertainty, and missing labels coexist in real-life multilabel data, which brings adverse effects to multilabel feature selection. The multi-scale fuzzy rough set gives an effective way to mine intrinsic knowledge hidden in uncertain data. This paper first extends the multi-scale learning to multilabel data with missing labels and proposes a feature selection method for multilabel classification with missing labels via multi-scale fusion fuzzy uncertainty measures called FSMML. The missing label space construction and feature evaluation metric are carefully investigated in the framework of multi-scale learning. A multilabel multi-scale learning strategy is formalized with the fuzzy granularity cognitive mechanism as the core, and the multi-scale fusion fuzzy label learning is given to reconstruct the missing label space. Then, a novel multilabel multi-scale fuzzy rough sets with missing labels is developed, and the significance of each scale is quantified. Moreover, some multi-scale fusion fuzzy uncertainty measures are defined by capturing the sample fuzzy similarity in the feature and reconstructed label spaces. Accordingly, the relevance between features and label set and the interactivity and redundancy between features in feature evaluation are discussed. Finally, FSMML chooses high-quality features to maximize relevance and interactivity and minimize redundancy. Extensive experiments demonstrate the effectiveness of FSMML on fifteen datasets with missing labels.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324003315",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Discriminative model",
      "Feature (linguistics)",
      "Feature selection",
      "Fuzzy logic",
      "Linguistics",
      "Machine learning",
      "Missing data",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Yin",
        "given_name": "Tengyu"
      },
      {
        "surname": "Chen",
        "given_name": "Hongmei"
      },
      {
        "surname": "Wang",
        "given_name": "Zhihong"
      },
      {
        "surname": "Liu",
        "given_name": "Keyu"
      },
      {
        "surname": "Yuan",
        "given_name": "Zhong"
      },
      {
        "surname": "Horng",
        "given_name": "Shi-Jinn"
      },
      {
        "surname": "Li",
        "given_name": "Tianrui"
      }
    ]
  },
  {
    "title": "Visual-guided hierarchical iterative fusion for multi-modal video action recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.10.003",
    "abstract": "Vision-Language models(VLMs) have shown promising improvements on various visual tasks. Most existing VLMs employ two separate transformer-based encoders, each dedicated to modeling visual and language features independently. Because the visual features and language features are unaligned in the feature space, it is challenging for the multi-modal encoder to learn vision-language interactions. In this paper, we propose a Visual-guided Hierarchical Iterative Fusion (VgHIF) method for VLMs in video action recognition, which acquires more discriminative vision and language representation. VgHIF leverages visual features from different levels in visual encoder to interact with language representation. The interaction is processed by the attention mechanism to calculate the correlation between visual features and language representation. VgHIF learns grounded video-text representation and supports many different pre-trained VLMs in a flexible and efficient manner with a tiny computational cost. We conducted experiments on the Kinetics-400 Mini Kinetics 200 HMDB51, and UCF101 using VLMs: CLIP, X-CLIP, and ViFi-CLIP. The experiments were conducted under full supervision and few shot settings, and compared with the baseline multi-modal model without VgHIF, the Top-1 accuracy of the proposed method has been improved to varying degrees, and several groups of results have achieved comparable results with state-of-the-art performance, which strongly verified the effectiveness of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524002915",
    "keywords": [
      "Action (physics)",
      "Action recognition",
      "Algorithm",
      "Artificial intelligence",
      "Chemistry",
      "Class (philosophy)",
      "Computer science",
      "Computer vision",
      "Fusion",
      "Image (mathematics)",
      "Image fusion",
      "Iterative method",
      "Linguistics",
      "Modal",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Polymer chemistry",
      "Quantum mechanics"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Bingbing"
      },
      {
        "surname": "Zhang",
        "given_name": "Ying"
      },
      {
        "surname": "Zhang",
        "given_name": "Jianxin"
      },
      {
        "surname": "Sun",
        "given_name": "Qiule"
      },
      {
        "surname": "Wang",
        "given_name": "Rong"
      },
      {
        "surname": "Zhang",
        "given_name": "Qiang"
      }
    ]
  },
  {
    "title": "Special section: Best papers of the 15th Mexican conference on pattern recognition (MCPR) 2023",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.08.017",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524002496",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Engineering",
      "Engineering physics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Section (typography)",
      "Special section"
    ],
    "authors": [
      {
        "surname": "Rodríguez González",
        "given_name": "Ansel Y."
      },
      {
        "surname": "Perez-Espinosa",
        "given_name": "Humberto"
      },
      {
        "surname": "Carrasco Ochoa",
        "given_name": "Jesús Ariel"
      },
      {
        "surname": "Martínez Trinidad",
        "given_name": "José Francisco"
      },
      {
        "surname": "Olvera López",
        "given_name": "José Arturo"
      }
    ]
  },
  {
    "title": "DSFusion: Infrared and visible image fusion method combining detail and scene information",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110633",
    "abstract": "The goal of image fusion is to combine the complementary features of two images to generate an information-rich fused image. However, taking into account both detail and scene information is difficult for existing image fusion algorithms. Therefore, we propose an infrared and visible image fusion method combining details and scene information (DSFusion). Specifically, we first design a local attention module (LAM), which performs feature extraction on the source image from multiple perspectives in order to better preserve minute detail information. Moreover, in order to distinguish and highlight the differences between the two modal images, we improved the channel attention module. Finally, we design a new loss function that can effectively balance the detail and scene information of the fusion image. Extensive testing on publicly available datasets demonstrates that DSFusion surpasses state of the art in both qualitative and quantitative evaluation. Furthermore, promising outcomes have been obtained in generalization experiments by directly expanding the trained model to other datasets, indicating the model’s excellent generalization capability. The code is available at https://github.com/LKZ1584905069/DSFusion.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324003844",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Fusion",
      "Image (mathematics)",
      "Image fusion",
      "Infrared",
      "Linguistics",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Kuizhuang"
      },
      {
        "surname": "Li",
        "given_name": "Min"
      },
      {
        "surname": "Chen",
        "given_name": "Cheng"
      },
      {
        "surname": "Rao",
        "given_name": "Chengwei"
      },
      {
        "surname": "Zuo",
        "given_name": "Enguang"
      },
      {
        "surname": "Wang",
        "given_name": "Yunling"
      },
      {
        "surname": "Yan",
        "given_name": "Ziwei"
      },
      {
        "surname": "Wang",
        "given_name": "Bo"
      },
      {
        "surname": "Chen",
        "given_name": "Chen"
      },
      {
        "surname": "Lv",
        "given_name": "Xiaoyi"
      }
    ]
  },
  {
    "title": "Attribute disentanglement and re-entanglement for generalized zero-shot learning",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.09.007",
    "abstract": "The key challenge in zero-shot learning is inferring latent semantic knowledge between visual and attribute features of seen classes to achieve knowledge transfer to unseen classes. To address the limitation that local attribute features can only ensure attribute-level recognition rather than classification of an entire class, some methods incorporate global information into the process or results of local features extraction for classification. However, these approaches have not effectively addressed the issue. To address these issues, we propose an Attribute Disentanglement and Re-entanglement for Generalized Zero-Shot Learning. Our model no longer implicitly or explicitly incorporates global information into local attribute features for classification. Instead, we adjust local attribute features to make them more suitable for classification in re-entanglement phase, while ensuring the correct extraction of these features in disentanglement phase. We employ appropriate optimization loss functions and achieve significant improvements on three challenging benchmark datasets. Compared to other similar methods, our model exhibits strong competitiveness.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524002691",
    "keywords": [
      "Chemistry",
      "Computer science",
      "Engineering",
      "Linguistics",
      "Mathematics",
      "Mechanical engineering",
      "One shot",
      "Organic chemistry",
      "Philosophy",
      "Physics",
      "Quantum",
      "Quantum entanglement",
      "Quantum mechanics",
      "Shot (pellet)",
      "Zero (linguistics)"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Quan"
      },
      {
        "surname": "Liang",
        "given_name": "Yucuan"
      },
      {
        "surname": "Zhang",
        "given_name": "Zhenqi"
      },
      {
        "surname": "Cao",
        "given_name": "Wenming"
      }
    ]
  },
  {
    "title": "Text-free diffusion inpainting using reference images for enhanced visual fidelity",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.10.009",
    "abstract": "This paper presents a novel approach to subject-driven image generation that addresses the limitations of traditional text-to-image diffusion models. Our method generates images using reference images without relying on language-based prompts. We introduce a visual detail preserving module that captures intricate details and textures, addressing overfitting issues associated with limited training samples. The model's performance is further enhanced through a modified classifier-free guidance technique and feature concatenation, enabling the natural positioning and harmonization of subjects within diverse scenes. Quantitative assessments using CLIP, DINO and Quality scores (QS), along with a user study, demonstrate the superior quality of our generated images. Our work highlights the potential of pre-trained models and visual patch embeddings in subject-driven editing, balancing diversity and fidelity in image generation tasks. Our implementation is available at https://github.com/8eomio/Subject-Inpainting.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524002976",
    "keywords": [
      "Artificial intelligence",
      "Computer graphics (images)",
      "Computer science",
      "Computer vision",
      "Diffusion",
      "Fidelity",
      "Image (mathematics)",
      "Inpainting",
      "Pattern recognition (psychology)",
      "Physics",
      "Telecommunications",
      "Thermodynamics"
    ],
    "authors": [
      {
        "surname": "Kim",
        "given_name": "Beomjo"
      },
      {
        "surname": "Sohn",
        "given_name": "Kyung-Ah"
      }
    ]
  },
  {
    "title": "Discrete diffusion models with Refined Language-Image Pre-trained representations for remote sensing image captioning",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.09.019",
    "abstract": "RS image captioning (RSIC) utilizes natural language to provide a description of image content, assisting in the comprehension of object properties and relationships. Nonetheless, RS images are characterized by variations in object scales, distributions, and quantities, which make it challenging to obtain global semantic information and object connections. To enhance the accuracy of captions produced from RS images, this paper proposes a novel method referred to as Discrete Diffusion Models with Refined Language-Image Pre-trained representations (DDM-RLIP), leveraging an advanced discrete diffusion model (DDM) for nosing and denoising text tokens. DDM-RLIP is based on an advanced DDM-based method designed for natural pictures. The primary approach for refining image representations involves fine-tuning a CLIP image encoder on RS images, followed by adapting the transformer with an additional attention module to focus on crucial image regions and relevant words. Furthermore, experiments were conducted on three datasets, Sydney-Captions, UCM-Captions, and NWPU-Captions, and the results demonstrated the superior performance of the proposed method compared to conventional autoregressive models. On the NWPU-Captions dataset, the CIDEr score improved from 116.4 to 197.7, further validating the efficacy and potential of DDM-RLIP. The implementation codes for our approach DDM-RLIP are available at https://github.com/Leng-bingo/DDM-RLIP.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524002861",
    "keywords": [
      "Artificial intelligence",
      "Closed captioning",
      "Computer science",
      "Computer vision",
      "Diffusion",
      "Image (mathematics)",
      "Image processing",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Physics",
      "Speech recognition",
      "Thermodynamics"
    ],
    "authors": [
      {
        "surname": "Leng",
        "given_name": "Guannan"
      },
      {
        "surname": "Xiong",
        "given_name": "Yu-Jie"
      },
      {
        "surname": "Qiu",
        "given_name": "Chunping"
      },
      {
        "surname": "Guo",
        "given_name": "Congzhou"
      }
    ]
  },
  {
    "title": "Self-supervised feature-gate coupling for dynamic network pruning",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110594",
    "abstract": "Gating modules have been widely explored in dynamic network pruning (DNP) to reduce the run-time computational cost of deep neural networks while keeping the features representative. Despite the substantial progress, existing methods remain ignoring the consistency between feature and gate distributions, which may lead to distortion of gated features. In this paper, we propose a feature-gate coupling (FGC) approach aiming to align distributions of features and gates. FGC is a plug-and-play module that consists of two steps carried out in an iterative self-supervised manner. In the first step, FGC utilizes the k -Nearest Neighbor algorithm in the feature space to explore instance neighborhood relationships, which are treated as self-supervisory signals. In the second step, FGC exploits contrastive self-supervised learning (CSL) to regularize gating modules, leading to the alignment of instance neighborhood relationships within the feature and gate spaces. Experimental results validate that the proposed FGC method improves the baseline approach with significant margins, outperforming state-of-the-art methods with a better accuracy-computation trade-off. Code is publicly available at github.com/smn2010/FGC-PR.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324003455",
    "keywords": [
      "Agronomy",
      "Algorithm",
      "Amplifier",
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Biology",
      "Code (set theory)",
      "Computation",
      "Computer network",
      "Computer science",
      "Consistency (knowledge bases)",
      "Coupling (piping)",
      "Distortion (music)",
      "Engineering",
      "Feature (linguistics)",
      "Linguistics",
      "Mechanical engineering",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Pruning",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Shi",
        "given_name": "Mengnan"
      },
      {
        "surname": "Liu",
        "given_name": "Chang"
      },
      {
        "surname": "Jiao",
        "given_name": "Jianbin"
      },
      {
        "surname": "Ye",
        "given_name": "Qixiang"
      }
    ]
  },
  {
    "title": "Influence maximization for heterogeneous networks based on self-supervised clustered heterogeneous graph transformer",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110595",
    "abstract": "Influence maximization (IM) has drawn significant attention in recent years. Most existing IM methods primarily focus on homogeneous networks, and do not take into account the heterogeneity and the attributes of different types of nodes in heterogeneous networks. However, heterogeneous networks are ubiquitous in real world, encompassing rich semantics and complex structural information. Additionally, the clustering characteristics inherent in a network have a critical and substantial impact on the process of information diffusion, which is often overlooked in IM models designed for heterogeneous networks. To address the challenges posed by the heterogeneity and clustering structure in heterogeneous networks, we propose a novel deep learning framework based on a self-supervised clustered heterogeneous graph transformer for IM in heterogeneous networks, which we have named SCHGT-IM. SCHGT-IM aggregates the heterogeneity and clustering information in heterogeneous networks and incorporates a clustered cascade (CC) model as an information diffusion model to enhance the realism of simulations. We evaluate the performance of SCHGT-IM in comparison with that of state-of-the-art IM models using three academic heterogeneous networks extracted from the DBLP dataset. The experimental results on influence spread demonstrate that SCHGT-IM is superior to fourteen state-of-the-art algorithms and is highly effective in selecting influential seed nodes of different types from heterogeneous networks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324003467",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Electrical engineering",
      "Engineering",
      "Graph",
      "Mathematical optimization",
      "Mathematics",
      "Maximization",
      "Pattern recognition (psychology)",
      "Theoretical computer science",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Ying"
      },
      {
        "surname": "Li",
        "given_name": "Linlin"
      },
      {
        "surname": "Liu",
        "given_name": "Xiangyu"
      },
      {
        "surname": "Liu",
        "given_name": "Yijun"
      },
      {
        "surname": "Li",
        "given_name": "Qianqian"
      }
    ]
  },
  {
    "title": "Light dual hypergraph convolution for collaborative filtering",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110596",
    "abstract": "Recommender systems filter information to meet users’ personalized interests actively. Existing graph-based models typically extract users’ interests from a heterogeneous interaction graph. They do not distinguish learning between users and items, ignoring the heterogeneous property. In addition, the interaction sparsity and long-tail bias issues still limit the recommendation performance significantly. Fortunately, hidden homogeneous correlations that have a considerable volume can entangle abundant CF signals. In this paper, we propose a light dual hypergraph convolution (LDHC) for collaborative filtering, which designs a hypergraph to involve heterogeneous and homogeneous correlations with more CF signals confronting the challenges. Over the integrated hypergraph, a two-level interest propagation is performed within the heterogeneous interaction graph and between the homogeneous user/item graphs to model users’ interests, where learning on users and items is distinguished and collaborated by the homogeneous propagation. Specifically, hypergraph convolution is lightened by removing unnecessary parameters to propagate users’ interests. Extensive experiments on publicly available datasets demonstrate that the proposed LDHC outperforms the state-of-the-art baselines.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324003479",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Artificial neural network",
      "Collaborative filtering",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Convolution (computer science)",
      "Discrete mathematics",
      "Dual (grammatical number)",
      "Filter (signal processing)",
      "Graph",
      "Homogeneous",
      "Hypergraph",
      "Literature",
      "Machine learning",
      "Mathematics",
      "Recommender system",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Jian",
        "given_name": "Meng"
      },
      {
        "surname": "Lang",
        "given_name": "Langchen"
      },
      {
        "surname": "Guo",
        "given_name": "Jingjing"
      },
      {
        "surname": "Li",
        "given_name": "Zun"
      },
      {
        "surname": "Wang",
        "given_name": "Tuo"
      },
      {
        "surname": "Wu",
        "given_name": "Lifang"
      }
    ]
  },
  {
    "title": "Regional dynamic point cloud completion network",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.10.017",
    "abstract": "Point cloud completion network often encodes points into a global feature vector, then predicts the complete point cloud through the vector generation process. However, this method may not accurately capture complex shapes, as global feature vectors struggle to recover their detailed structure. In this paper, we present a novel shape completion network, namely RD-Net, that innovatively focuses on the interaction of information between points to provide both local and global information for generating fine-grained complete shape. Specifically, we propose a stored iteration-based method for point cloud sampling that quickly captures representative points within the point cloud. Subsequently, in order to better predict the shape and structure of the missing part, we design an iterative edge-convolution module. It uses a CNN-like hierarchy for feature extraction and learning context information. Moreover, we design a two-stage reconstruction process for latent vector decoding. We first employ a feature-points-based multi-scale generating decoder to estimate the missing point cloud hierarchically. This is followed by a self-attention mechanism that refines the generated shape and effectively generates structural details. By combining these innovations, RD-Net achieves a 2% reduction in CD error compared to the state-of-the-art method on the ShapeNet-part dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524003052",
    "keywords": [
      "Artificial intelligence",
      "Cloud computing",
      "Completion (oil and gas wells)",
      "Computer science",
      "Geology",
      "Geometry",
      "Mathematics",
      "Operating system",
      "Petroleum engineering",
      "Point (geometry)",
      "Point cloud"
    ],
    "authors": [
      {
        "surname": "Zhu",
        "given_name": "Liping"
      },
      {
        "surname": "Yang",
        "given_name": "Yixuan"
      },
      {
        "surname": "Liu",
        "given_name": "Kai"
      },
      {
        "surname": "Wu",
        "given_name": "Silin"
      },
      {
        "surname": "Wang",
        "given_name": "Bingyao"
      },
      {
        "surname": "Chang",
        "given_name": "Xianxiang"
      }
    ]
  },
  {
    "title": "LGNet: Local and global point dependency network for 3D object detection",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110585",
    "abstract": "3D object detection is a challenging task in autonomous driving industry scenarios. Many pre-existing methods employ the set-abstraction operation for generating key-point representations, which, however, cannot learn the long-range context dependency properly. In addition, the pooling operator, which only focuses on maximum channel response, is adopted to aggregate features of neighbor points without semantic information. To fix these issues, we propose LGNet, a new framework that simultaneously captures local and global point dependencies for enhancing 3D object detection. Specifically, we first introduce a new local point-graph pooling module to compute point-to-point correlations in a local region and aggregate features from neighboring points. To further capture the long-range dependency in a global context, we devised a global point-aware module to integrate local and global features at higher resolution. Experiments on the KITTI 3D detection dataset and Waymo Open Dataset benchmark show that LGNet achieves state-of-the-art performance in multiple classes. We will upload the code on https://github.com/MWPony/LGNet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324003364",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Dependency (UML)",
      "Geometry",
      "Mathematics",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Point (geometry)"
    ],
    "authors": [
      {
        "surname": "Ma",
        "given_name": "Jianwei"
      },
      {
        "surname": "Huang",
        "given_name": "Yan"
      },
      {
        "surname": "Qian",
        "given_name": "Cheng"
      },
      {
        "surname": "Kang",
        "given_name": "Jian"
      },
      {
        "surname": "Liu",
        "given_name": "Jiang"
      },
      {
        "surname": "Zhang",
        "given_name": "Hui"
      },
      {
        "surname": "Hong",
        "given_name": "Wei"
      }
    ]
  },
  {
    "title": "Module-based graph pooling for graph classification",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110606",
    "abstract": "Graph Neural Network (GNN) models are recently proposed to process the graph-structured data for the learning tasks on graphs, e.g., node classification, link prediction, and so on. This work focuses on the graph classification task, aiming to obtain the graph representation and predict the class label for a graph. Existing works proposed applying graph pooling to obtain graph embedding but still suffer from several issues. First, node embeddings are generated according to the topological information of the whole graph, but ignoring the local isomorphic substructures commonly seen in bioinformatics and chemistry. Another limitation arises when aggregating node embeddings. The hard assignment obtained through clustering algorithms, which rely on preset and fixed parameters instead of considering the graph’s properties adaptively, restricts the flexibility in handling graphs of varying scales. To address the above problems, a module-based graph pooling framework (MGPool) is proposed in this work. Inspired by the rules of bioinformatics, MGPool assumes that a graph consists of multiple modules (also known as sub structures), which are identified based on the natural organization of the graph rather than the hard allocation of nodes. Benefiting from the hypothesis, MGPool generates node embeddings from graph-view and module-view, which is capable to capture global graph information and local isomorphic information respectively. Then module-level pooling is used to capture the intra-module information, while the inter-module information in terms of the correlation between modules is obtained through graph-level pooling. Finally, an entropy-based weighting mechanism is proposed to adjust the modules’ weights for the graph aggregation. Experiments conducted on bioinformatics benchmark datasets demonstrate the effectiveness of MGPool by outperforming other state-of-the-art graph pooling methods. For social network datasets, MGPool also provides competitive performance. Moreover, the visualization of module entropy weights is given to reveal the interpretability of the model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324003571",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Graph",
      "Pattern recognition (psychology)",
      "Pooling",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Deng",
        "given_name": "Sucheng"
      },
      {
        "surname": "Yang",
        "given_name": "Geping"
      },
      {
        "surname": "Yang",
        "given_name": "Yiyang"
      },
      {
        "surname": "Gong",
        "given_name": "Zhiguo"
      },
      {
        "surname": "Chen",
        "given_name": "Can"
      },
      {
        "surname": "Chen",
        "given_name": "Xiang"
      },
      {
        "surname": "Hao",
        "given_name": "Zhifeng"
      }
    ]
  },
  {
    "title": "DeepMarkerNet: Leveraging supervision from the Duchenne Marker for spontaneous smile recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.09.015",
    "abstract": "Distinguishing between spontaneous and posed smiles from videos poses a significant challenge in pattern classification literature. Researchers have developed feature-based and deep learning-based solutions for this problem. To this end, deep learning outperforms feature-based methods. However, certain aspects of feature-based methods could improve deep learning methods. For example, previous research has shown that Duchenne Marker (or D-Marker) features from the face play a vital role in spontaneous smiles, which can be useful to improve deep learning performances. In this study, we propose a deep learning solution that leverages D-Marker features to improve performance further. Our multi-task learning framework, named DeepMarkerNet, integrates a transformer network with the utilization of facial D-Markers for accurate smile classification. Unlike past methods, our approach simultaneously predicts the class of the smile and associated facial D-Markers using two different feed-forward neural networks, thus creating a symbiotic relationship that enriches the learning process. The novelty of our approach lies in incorporating supervisory signals from the pre-calculated D-Markers (instead of as input in previous works), harmonizing the loss functions through a weighted average. In this way, our training utilizes the benefits of D-Markers, but the inference does not require computing the D-Marker. We validate our model’s effectiveness on four well-known smile datasets: UvA-NEMO, BBC, MMI facial expression, and SPOS datasets, and achieve state-of-the-art results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524002770",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Duchenne muscular dystrophy",
      "Internal medicine",
      "Medicine",
      "Pattern recognition (psychology)",
      "Psychology",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Hasan",
        "given_name": "Mohammad Junayed"
      },
      {
        "surname": "Rafat",
        "given_name": "Kazi"
      },
      {
        "surname": "Rahman",
        "given_name": "Fuad"
      },
      {
        "surname": "Mohammed",
        "given_name": "Nabeel"
      },
      {
        "surname": "Rahman",
        "given_name": "Shafin"
      }
    ]
  },
  {
    "title": "Blessing few-shot segmentation via semi-supervised learning with noisy support images",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110503",
    "abstract": "Mainstream few-shot segmentation methods meet performance bottleneck due to the data scarcity of novel classes with insufficient intra-class variations, which results in a biased model primarily favoring the base classes. Fortunately, owing to the evolution of the Internet, an extensive repository of unlabeled images has become accessible from diverse sources such as search engines and publicly available datasets. However, such unlabeled images are not a free lunch. There are noisy inter-class and intra-class samples causing severe feature bias and performance degradation. Therefore, we propose a semi-supervised few-shot segmentation framework named F4S, which incorporates a ranking algorithm designed to eliminate noisy samples and select superior pseudo-labeled images, thereby fostering the improvement of few-shot segmentation within a semi-supervised paradigm. The proposed F4S framework can not only enrich the intra-class variations of novel classes during the test phase, but also enhance meta-learning of the network during the training phase. Furthermore, it can be readily implemented with ease on any off-the-shelf few-shot segmentation methods. Additionally, based on a Structural Causal Model (SCM), we further theoretically explain why the proposed method can solve the noise problem: the severe noise effects are removed by cutting off the backdoor path between pseudo labels and noisy support images via causal intervention. On PASCAL-5 i and COCO-20 i datasets, we show that the proposed F4S can boost various popular few-shot segmentation methods to new state-of-the-art performances.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324002541",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Blessing",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Geography",
      "Image segmentation",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Segmentation",
      "Shot (pellet)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Runtong"
      },
      {
        "surname": "Zhu",
        "given_name": "Hongyuan"
      },
      {
        "surname": "Zhang",
        "given_name": "Hanwang"
      },
      {
        "surname": "Gong",
        "given_name": "Chen"
      },
      {
        "surname": "Zhou",
        "given_name": "Joey Tianyi"
      },
      {
        "surname": "Meng",
        "given_name": "Fanman"
      }
    ]
  },
  {
    "title": "Editorial for pattern recognition letters special issue on Advances in Disinformation Detection and Media Forensics",
    "journal": "Pattern Recognition Letters",
    "year": "2024",
    "doi": "10.1016/j.patrec.2024.09.004",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865524002605",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data science",
      "Disinformation",
      "Social media",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Amerini",
        "given_name": "Irene"
      },
      {
        "surname": "Sanchez",
        "given_name": "Victor"
      },
      {
        "surname": "Maiano",
        "given_name": "Luca"
      }
    ]
  },
  {
    "title": "Two-stage zero-shot sparse hashing with missing labels for cross-modal retrieval",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110717",
    "abstract": "Recently, zero-shot cross-modal hashing has gained significant popularity due to its ability to effectively realize the retrieval of emerging concepts within multimedia data. Although the existing approaches have shown impressive results, the following limitations still need to be solved: (1) Labels in large-scale multimodal datasets in real scenes are usually incomplete or partially missing. (2) The existing methods ignore the influence of features-wise low-level similarity and label distribution on retrieval performance. (3) The representation ability of dense hash codes limits its discriminative potential. To solve these issues, we introduce an effective cross-modal retrieval framework called two-stage zero-shot sparse hashing with missing labels (TZSHML). Specifically, we learn a classifier through the partially known labeled samples to predict the labels of unlabeled data. Then, we use the reliable information in the correctly marked labels to recover the missing labels. The predicted and recovered labels are combined to obtain more accurate labels for the samples with missing labels. In addition, we employ sample-wise fine-grained similarity and cluster-wise similarity to learn hash codes. Therefore, TZSHML ensures that more samples with similar semantics are clustered together. Besides, we apply high-dimensional sparse hash codes to explore richer semantic information. Finally, the drift and interaction terms are introduced into the learning of the hash function to further narrow the gap between different modalities. Extensive experimental results demonstrate the competitiveness of our approach over other state-of-the-art methods in zero-shot retrieval scenarios with missing labels. The source code of this paper can be obtained from https://github.com/szq0816/TZSHML.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324004680",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Computer security",
      "Data mining",
      "Discriminative model",
      "Double hashing",
      "Feature hashing",
      "Hash function",
      "Hash table",
      "Image (mathematics)",
      "Machine learning",
      "Missing data",
      "Pattern recognition (psychology)",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Yong",
        "given_name": "Kailing"
      },
      {
        "surname": "Shu",
        "given_name": "Zhenqiu"
      },
      {
        "surname": "Wang",
        "given_name": "Hongbin"
      },
      {
        "surname": "Yu",
        "given_name": "Zhengtao"
      }
    ]
  },
  {
    "title": "A safe screening rule with bi-level optimization of ν support vector machine",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110644",
    "abstract": "Support vector machine (SVM) has achieved many successes in machine learning, especially for a small sample problem. As a famous extension of the traditional SVM, the ν support vector machine ( ν -SVM) has shown outstanding performance due to its great model interpretability. However, it still faces challenges in training overhead for large-scale problems. To address this issue, we propose a safe screening rule with bi-level optimization for ν -SVM (SRBO- ν -SVM) which can screen out inactive samples before training and reduce the computational cost without sacrificing the prediction accuracy. Our SRBO- ν -SVM is strictly deduced by integrating the Karush–Kuhn–Tucker (KKT) conditions, the variational inequalities of convex problems and the ν -property. Furthermore, we develop an efficient dual coordinate descent method (DCDM) to further improve computational speed. Finally, a unified framework for SRBO is proposed to accelerate many SVM-type models, and it is successfully applied to one-class SVM. Experimental results on 6 artificial data sets and 30 benchmark data sets have verified the effectiveness and safety of our proposed methods in supervised and unsupervised tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324003959",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Geodesy",
      "Geography",
      "Interpretability",
      "Karush–Kuhn–Tucker conditions",
      "Machine learning",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Operating system",
      "Overhead (engineering)",
      "Pointwise",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Zhiji"
      },
      {
        "surname": "Chen",
        "given_name": "Wanyi"
      },
      {
        "surname": "Zhang",
        "given_name": "Huan"
      },
      {
        "surname": "Xu",
        "given_name": "Yitian"
      },
      {
        "surname": "Shi",
        "given_name": "Lei"
      },
      {
        "surname": "Zhao",
        "given_name": "Jianhua"
      }
    ]
  },
  {
    "title": "M 3 N e t : Movement Enhancement with Multi-Relation toward Multi-Scale video representation for Temporal Action Detection",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110702",
    "abstract": "Locating boundary is very important for Temporal Action Detection (TAD) and is a key factor affecting the performance of TAD. However, two factors lead to inaccurate boundary localization: the movement feature submergence and the existence of multi-scale actions. In this work, to address the submergence of movement feature, we design the Movement Enhance Module (MEM), in which the Movement Feature Extractor (MFE) and Multi-Relation Module (MRM) are used to highlight short-term and long-term movement information respectively. To address the characteristic of multi-scale actions, we propose a Scale Feature Pyramid Network (SFPN) to detect multi-scale actions and design a two-stage training strategy that makes each layer focus on a specific scale action. These tow modules are integrated as M 3 N e t , and extensive experiments demonstrate its effectiveness. M 3 N e t outperforms other representative TAD methods on ActivityNet-1.3 and THUMOS-14.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324004539",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Mathematics",
      "Scalable Vector Graphics",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Zixuan"
      },
      {
        "surname": "Wang",
        "given_name": "Dongqi"
      },
      {
        "surname": "Zhao",
        "given_name": "Xu"
      }
    ]
  },
  {
    "title": "ResPrune: An energy-efficient restorative filter pruning method using stochastic optimization for accelerating CNN",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110671",
    "abstract": "Convolutional Neural Networks (CNNs) are frequently employed for image pattern recognition and other computer vision tasks. When over-parameterized deep learning models are used for inference, resource-constrained edge devices may struggle. As a result, model compression, particularly filter pruning, has become critical. A reduction in model size might result in less calculation, resulting in faster hardware execution and lower energy consumption. One of the drawbacks of current pruning strategies is that once the filters are pruned, their weights are permanently lost. To address this constraint, we propose a unique two-phase pruning technique in which the filters to be pruned are selected using two criteria: l 2 -norm and redundancy. Second, rather than omitting the selected filters for all future epochs, we restore them to their original value with some stochasticity. Retaining the most optimal filter weights in earlier epochs enables the survival of the fittest filters, resulting in higher model convergence. Experiments on three benchmark datasets, CIFAR-10, CIFAR-100, and ILSVRC-2012, reveal that our strategy outperforms other state-of-the-art pruning methods by a minimum reduction of 57% FLOPs with an accuracy loss as minimal as 0.08 %.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324004229",
    "keywords": [
      "Agronomy",
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer vision",
      "Energy (signal processing)",
      "Filter (signal processing)",
      "Mathematical optimization",
      "Mathematics",
      "Pruning",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Jayasimhan",
        "given_name": "Anusha"
      },
      {
        "surname": "P.",
        "given_name": "Pabitha"
      }
    ]
  },
  {
    "title": "A unified framework for convolution-based graph neural networks",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110597",
    "abstract": "Graph Convolutional Networks (GCNs) have attracted a lot of research interest in machine learning, and many variants have been proposed recently. In this paper, we take a step forward to establish a unified framework for convolution-based graph neural networks, aiming to provide a systematic view of different GCN variants and deep understanding of the relations among them. Our key idea is formulating the basic graph convolution operation as an optimization problem in the graph Fourier space. Under this framework, a variety of popular GCN models, including vanilla-GCNs, attention-based GCNs and topology-based GCNs, can be interpreted as a similar optimization problem but with different regularizers. This novel perspective enables a better understanding of the similarities and differences among many widely used GCNs, and may inspire new model designs. As a showcase, we present a novel regularization technique under the proposed framework to tackle the oversmoothing problem in graph convolution. The effectiveness of newly designed model is validated empirically.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324003480",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Convolution (computer science)",
      "Graph",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Pan",
        "given_name": "Xuran"
      },
      {
        "surname": "Han",
        "given_name": "Xiaoyan"
      },
      {
        "surname": "Wang",
        "given_name": "Chaofei"
      },
      {
        "surname": "Li",
        "given_name": "Zhuo"
      },
      {
        "surname": "Song",
        "given_name": "Shiji"
      },
      {
        "surname": "Huang",
        "given_name": "Gao"
      },
      {
        "surname": "Wu",
        "given_name": "Cheng"
      }
    ]
  },
  {
    "title": "Reconstruction flow recurrent network for compressed video quality enhancement",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110638",
    "abstract": "We present a reconstruction flow for the task of compressed video quality enhancement (VQE). Compressed videos often suffer from various coding artifacts, such as blocking and blurring, especially under low bit-rate. VQE aims to suppress these artifacts to improve the visual quality. Frame similarity can be utilized to enhance low-quality frames given their neighboring high-quality frames, for which motion estimation becomes important. Previous approaches often calculate optical flow for the motion compensation. On the other hand, video coding contains a rich set of block motion vectors, forming a coding flow, which may or may not correspond to the scene motion, but to places that deliver the minimum compression error. In contrast, such a valuable coding flow has always been ignored in VQE previously. In this work, we combine these two motion sources into a new flow, namely reconstruction flow, for the purpose of high-quality VQE. Specifically, we estimate optical flows from RGB frames and extract coding flows from coding streams, which are then merged by a fusion module to generate reconstruction flow. Besides, our network is built upon a recurrent network to utilize global temporal information. The deep features are warped according to the reconstruction flow and fed into the subsequent reconstruction module with spatial-variant kernel attention. Our method is evaluated on the leading MFQE2.0 dataset, which demonstrates superior performances when compared to the existing state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324003893",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Flow (mathematics)",
      "Flow network",
      "Geometry",
      "Image (mathematics)",
      "Mathematical optimization",
      "Mathematics",
      "Optical flow"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Zhengning"
      },
      {
        "surname": "Liu",
        "given_name": "Xuhang"
      },
      {
        "surname": "Wang",
        "given_name": "Chuan"
      },
      {
        "surname": "Jiang",
        "given_name": "Ting"
      },
      {
        "surname": "Zeng",
        "given_name": "Tianjiao"
      },
      {
        "surname": "Zeng",
        "given_name": "Zhenni"
      },
      {
        "surname": "Wang",
        "given_name": "Guoqing"
      },
      {
        "surname": "Liu",
        "given_name": "Shuaicheng"
      }
    ]
  },
  {
    "title": "Compressing spectral kernels in Gaussian Process: Enhanced generalization and interpretability",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110642",
    "abstract": "The modeling capabilities of a Gaussian Process (GP), such as generalization, nonlinearity, and smoothness, are largely determined by the choice of its kernel. A popular family of kernels for GPs, the spectral mixture (SM) kernels, have the desirable property that with a large number of spectral components they can approximate any stationary kernel. However, using a large number of SM components increases the risk of overfitting and hinders interpretability. To overcome these challenges, we propose a compression algorithm incorporating component pruning and component merging for GPs. Here SM components with small signal variance are removed, and a moment-matching merge method is proposed to further reduce the number of SM components. The main novelty of the proposed method is a similarity measure between SM components based on their normalized cross-correlation, which is related to the Bhattacharyya coefficient. We derive a greedy GP compression algorithm and perform a comparative evaluation over various learning tasks in terms of forecasting performance and compression capability. Results substantiate the beneficial effect of the method, both in terms of generalization and interpretability. 1 1 Source code: https://github.com/ck2019ML/Gaussian-Process-Model-Compression.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324003935",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Chemistry",
      "Computational chemistry",
      "Computer science",
      "Gaussian",
      "Gaussian process",
      "Generalization",
      "Interpretability",
      "Mathematical analysis",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Process (computing)"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Kai"
      },
      {
        "surname": "van Laarhoven",
        "given_name": "Twan"
      },
      {
        "surname": "Marchiori",
        "given_name": "Elena"
      }
    ]
  },
  {
    "title": "Deep graph matching meets mixed-integer linear programming: Relax or not ?",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110697",
    "abstract": "Graph matching is an important problem that has received widespread attention, especially in the field of computer vision. Recently, state-of-the-art methods seek to incorporate graph matching with deep learning. However, there is no research to explain what role the graph matching algorithm plays in the model. Therefore, we propose an approach integrating a MILP formulation of the graph matching problem. This formulation is solved to optimal and it provides inherent baseline. Meanwhile, similar approaches are derived by releasing the optimal guarantee of the graph matching solver and by introducing a quality level. This quality level controls the quality of the solutions provided by the graph matching solver. In addition, several relaxations of the graph matching problem are put to the test. Our experimental evaluation gives several theoretical insights and guides the direction of deep graph matching methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324004485",
    "keywords": [
      "3-dimensional matching",
      "Algorithm",
      "Bipartite graph",
      "Blossom algorithm",
      "Computer science",
      "Factor-critical graph",
      "Graph",
      "Integer programming",
      "Line graph",
      "Linear programming",
      "Matching (statistics)",
      "Mathematical optimization",
      "Mathematics",
      "Optimal matching",
      "Solver",
      "Statistics",
      "Theoretical computer science",
      "Voltage graph"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Zhoubo"
      },
      {
        "surname": "Chen",
        "given_name": "Puqing"
      },
      {
        "surname": "Raveaux",
        "given_name": "Romain"
      },
      {
        "surname": "Yang",
        "given_name": "Xin"
      },
      {
        "surname": "Liu",
        "given_name": "Huadong"
      }
    ]
  },
  {
    "title": "A novel K-means and K-medoids algorithms for clustering non-spherical-shape clusters non-sensitive to outliers",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110639",
    "abstract": "Determination of the optimal number of clusters, the random selection of the initial centers, the non-detection of non-spherical clusters, and the negative impact of outliers are the main challenges of the K-means algorithm. In this paper, to tackle these issues three simple and intelligent algorithms are proposed by changing the structure of the K-means and K-medoids algorithms. The difference between these algorithms is in the selection of the initial centers and the stop condition. A method has been proposed to obtain the overlap space between the clusters. Using this method, a modified K-means algorithm is developed for the clustering of non-spherical data. These algorithms are designed in a way that they are not sensitive to outliers and can identify clusters having non-spherical shapes. The performance of the proposed methods is illustrated by applying the proposed algorithms to the different data sets and by comparing the results of the algorithms with other methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032400390X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "CURE data clustering algorithm",
      "Cluster analysis",
      "Computer science",
      "Correlation clustering",
      "Data mining",
      "Epistemology",
      "Medoid",
      "Outlier",
      "Philosophy",
      "Selection (genetic algorithm)",
      "Simple (philosophy)",
      "k-medoids"
    ],
    "authors": [
      {
        "surname": "Heidari",
        "given_name": "J."
      },
      {
        "surname": "Daneshpour",
        "given_name": "N."
      },
      {
        "surname": "Zangeneh",
        "given_name": "A."
      }
    ]
  },
  {
    "title": "Improving CNN-based semantic segmentation on structurally similar data using contrastive graph convolutional networks",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110622",
    "abstract": "Structurally similar data exist in most practical semantic segmentation applications. For example, objects can appear identical or positionally similar in many images, such as video frames. Objects with structural similarity in data samples can confuse deep neural networks (DNNs) in semantic segmentation applications. These challenges often lead to lower pixel classification accuracy of natural object segmentation. This study proposes a novel approach (S2-GCN) that enhances CNN-based semantic segmentation for structurally similar data using a contrastive graph convolutional network (GCN). By selecting specific label pairs and developing a customized GCN branch parallel to an encoder-decoder backbone, our method significantly improves accuracy, IoU, and F1-score, by up to 8 %, as demonstrated through an extensive evaluation of five datasets. Our findings show that the proposed method effectively addresses the structural similarity problem of CNN-based semantic segmentation and can be applied to a wide range of practical applications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032400373X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Encoder",
      "Graph",
      "Image (mathematics)",
      "Image segmentation",
      "Operating system",
      "Pattern recognition (psychology)",
      "Pixel",
      "Segmentation",
      "Similarity (geometry)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Ling"
      },
      {
        "surname": "Tang",
        "given_name": "Zedong"
      },
      {
        "surname": "Li",
        "given_name": "Hao"
      }
    ]
  },
  {
    "title": "MetaTKG++: Learning evolving factor enhanced meta-knowledge for temporal knowledge graph reasoning",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110629",
    "abstract": "Reasoning over Temporal Knowledge Graphs (TKGs) aims to predict future facts based on the given history. One of the key challenges for prediction is to analyze the evolution process of facts. Most existing works focus on exploring evolutionary information in history to obtain effective temporal embeddings for entities and relations, but they ignore the variation in evolution patterns of facts caused by numerous diverse entities and latent evolving factors, which makes them struggle to adapt to future data with different evolution patterns. Moreover, new entities continue to emerge along with the evolution of facts over time. Since existing models highly rely on historical information to learn embeddings for entities, they perform poorly on such entities with little historical information. To tackle these issues, we propose a novel evolving factor enhanced temporal meta-learner framework for TKG reasoning, MetaTKG++ for brevity. Specifically, we first propose a temporal meta-learner which regards TKG reasoning as many temporal meta-tasks for training. From the training process of each meta-task, the obtained meta-knowledge can guide backbones to adapt to future data exhibiting various evolution patterns and to effectively learn entities with little historical information. Then, we design an Evolving Factor Learning module, which aims to assist backbones in learning evolution patterns by modeling latent evolving factors. Meanwhile, during the training process with the proposed meta-learner, the learnable evolving factor can enhance the meta-knowledge with providing more comprehensive information on learning evolution patterns. Extensive experiments on five widely-used datasets and four backbones demonstrate that our method can greatly improve the performance on TKG prediction.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324003807",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Factor (programming language)",
      "Graph",
      "Knowledge graph",
      "Machine learning",
      "Programming language",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Xia",
        "given_name": "Yuwei"
      },
      {
        "surname": "Zhang",
        "given_name": "Mengqi"
      },
      {
        "surname": "Liu",
        "given_name": "Qiang"
      },
      {
        "surname": "Wang",
        "given_name": "Liang"
      },
      {
        "surname": "Wu",
        "given_name": "Shu"
      },
      {
        "surname": "Zhang",
        "given_name": "Xiaoyu"
      },
      {
        "surname": "Wang",
        "given_name": "Liang"
      }
    ]
  },
  {
    "title": "Toward a deeper understanding: RetNet viewed through Convolution",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110625",
    "abstract": "The success of Vision Transformer (ViT) has been widely reported on a wide range of image recognition tasks. ViT can learn global dependencies superior to CNN, yet CNN’s inherent locality can substitute for expensive training resources. Recently, the outstanding performance of RetNet in the field of language modeling has garnered attention, surpassing that of the Transformer with explicit local modeling, shifting researchers’ focus toward Transformers in the CV field. This paper investigates the effectiveness of RetNet from a CNN perspective and presents a variant of RetNet tailored to the visual domain. Similar to RetNet we improves ViT’s local modeling by applying a weight mask on the original self-attention matrix. A straightforward way to locally adapt the self-attention matrix can be realized by an Element-wise Learnable Mask (ELM), for which our preliminary results show promising results. However, the Element-wise Learnable Mask not only induces a non-trivial additional parameter overhead but also increases the optimization complexity. To this end, this work proposes a novel Gaussian mixture mask (GMM) in which one mask only has two learnable parameters and it can be conveniently used in any ViT variants whose attention mechanism allows the use of masks. Experimental results on multiple small datasets demonstrate that the effectiveness of our proposed Gaussian mask for boosting ViTs for free (almost zero additional parameter or computation cost). Our code is publicly available at https://github.com/CatworldLee/Gaussian-Mixture-Mask-Attention.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324003765",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Boosting (machine learning)",
      "Computation",
      "Computer engineering",
      "Computer science",
      "Gaussian",
      "Linguistics",
      "Locality",
      "Machine learning",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Chenghao"
      },
      {
        "surname": "Zhang",
        "given_name": "Chaoning"
      }
    ]
  },
  {
    "title": "IRCNN: A novel signal decomposition approach based on iterative residue convolutional neural network",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110670",
    "abstract": "The decomposition of non-stationary signals is an important and challenging task in the field of signal time–frequency analysis. In the recent two decades, many decomposition methods have been proposed, inspired by the empirical mode decomposition method, first published by Huang et al. in 1998. However, they still have some limitations. For example, they are generally prone to boundary and mode mixing effects and are not very robust to noise. Inspired by the successful applications of deep learning, and given the lack in the literature of works in which deep learning techniques are used directly to decompose non-stationary signals into simple oscillatory components, we use the convolutional neural network, residual structure and nonlinear activation function to compute in an innovative way the local average of the signal, and study a new non-stationary signal decomposition method under the framework of deep learning. We discuss the training process of the proposed model and study the convergence analysis of the learning algorithm. In the experiments, we evaluate the performance of the proposed model from two points of view: the calculation of the local average and the signal decomposition. Furthermore, we study the mode mixing, noise interference, and orthogonality properties of the decomposed components produced by the proposed method, and compare it with the state-of-the-art ones. All results show that the proposed model allows for better handling boundary effect, mode mixing effect, robustness, and the orthogonality of the decomposed components than existing methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324004217",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Convolutional neural network",
      "Decomposition",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Residue (chemistry)"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Feng"
      },
      {
        "surname": "Cicone",
        "given_name": "Antonio"
      },
      {
        "surname": "Zhou",
        "given_name": "Haomin"
      }
    ]
  },
  {
    "title": "GHOST: Graph-based higher-order similarity transformation for classification",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110623",
    "abstract": "Exploring and identifying a good feature representation to describe high-dimensional datasets is a challenge of prime importance. However, plenty of feature selection techniques and distance metrics exist, which entails an intricacy for identifying the one best suited to the task. This paper provides an algorithm to design high-order distance metrics over a sparse selection of features dedicated to classification. Our approach is based on Conditional Random Field (CRF) energy minimization and Dual Decomposition, which allow efficiency and great flexibility in the considered features. The optimization technique ensures the tractability of high-dimensionality problems using hundreds of features and samples. Our approach is evaluated on synthetic data as well as on Covid-19 patient stratification. Comparisons with state-of-the-art baselines and our proposed method on different classification results prove the learned metric’s relevance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324003741",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Curse of dimensionality",
      "Data mining",
      "Dimensionality reduction",
      "Economics",
      "Feature selection",
      "Machine learning",
      "Metric (unit)",
      "Operations management",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Battistella",
        "given_name": "Enzo"
      },
      {
        "surname": "Vakalopoulou",
        "given_name": "Maria"
      },
      {
        "surname": "Paragios",
        "given_name": "Nikos"
      },
      {
        "surname": "Deutsch",
        "given_name": "Éric"
      }
    ]
  },
  {
    "title": "A novel hybrid decoding neural network for EEG signal representation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110726",
    "abstract": "In this paper, we proposed a novel hybrid decoding model that combines the superiority of CNNs and multi-head self-attention mechanisms, called HCANN, to finely characterizing EEG features. Depthwise separable convolution with multi-scale factors efficiently decouples temporal relevant information between brain-computer interface (BCI) tasks and EEG signals. Multi-head mechanism adaptively modified for EEG focuses on brain spatial activation patterns and extracts complementary spatial representation information from multiple subspaces. The proposed HCANN decodes the intent information of EEG recorded by three BCI paradigms, including one active and two passive BCI paradigms: rapid serial visual presentation, motor imagery, and imagined speech. We evaluated HCANN by comparing with the current state-of-the-art methods. The experimental results demonstrated that HCANN can effectively decode EEG and improves classification performance for all three BCI tasks. In addition, the visualization of spatial-temporal features at different decoding stages demonstrated that the proposed HCANN gradually extracts effective features related to the BCI tasks. The code of HCANN is publicly available at https://github.com/youshuoji/HCANN.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324004771",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Decoding methods",
      "Electroencephalography",
      "Law",
      "Neural decoding",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Programming language",
      "Psychology",
      "Representation (politics)",
      "SIGNAL (programming language)",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Ji",
        "given_name": "Youshuo"
      },
      {
        "surname": "Li",
        "given_name": "Fu"
      },
      {
        "surname": "Fu",
        "given_name": "Boxun"
      },
      {
        "surname": "Zhou",
        "given_name": "Yijin"
      },
      {
        "surname": "Wu",
        "given_name": "Hao"
      },
      {
        "surname": "Li",
        "given_name": "Yang"
      },
      {
        "surname": "Li",
        "given_name": "Xiaoli"
      },
      {
        "surname": "Shi",
        "given_name": "Guangming"
      }
    ]
  },
  {
    "title": "GLNAS: Greedy Layer-wise Network Architecture Search for low cost and fast network generation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110730",
    "abstract": "The process of applying machine learning algorithms to practical problems can be a challenging and tedious task for non-experts. Previous research has sought to alleviate this burden by introducing automated machine learning techniques, including Network Architecture Search (NAS) and Differentiable Architecture Search (DARTS). However, these methods use a fixed number of layers and predefined skip connections which impose limitations on the generation of an optimal network architecture. In this paper, we propose a novel approach called Greedy Layer-wise Network Architecture Search (GLNAS), which trains network layers one after another and evaluates the network’s performance after each layer is added. GLNAS also assesses the effectiveness of skip connections between layers by testing various outputs of previous layers as an input to the current layer. Our experiment results demonstrate that the network generated by GLNAS requires fewer parameters (i.e., 3.5 millions in both CIFAR-10 and CIFAR-100 datasets) and GPU resources during the searching phase (i.e., 0.17 and 0.24 GPU days in CIFAR-10 and CIFAR-100 datasets respectively) than many existing methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324004813",
    "keywords": [
      "Algorithm",
      "Architecture",
      "Art",
      "Artificial intelligence",
      "Chemistry",
      "Computer network",
      "Computer science",
      "Engineering",
      "Greedy algorithm",
      "Layer (electronics)",
      "Machine learning",
      "Network architecture",
      "Operating system",
      "Organic chemistry",
      "Process (computing)",
      "Systems engineering",
      "Task (project management)",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Ho",
        "given_name": "Jiacang"
      },
      {
        "surname": "Park",
        "given_name": "Kyongseok"
      },
      {
        "surname": "Kang",
        "given_name": "Dae-Ki"
      }
    ]
  },
  {
    "title": "A cross-network node classification method in open-set scenario",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110718",
    "abstract": "Cross-network node classification aims to classify the nodes of unlabeled target network using a labeled source network. Existing methods introduce domain adaptation to address representation discrepancy in closed-set scenario. However, the open-set scenario is widespread in applications, in which, the coexistence and interaction of representation discrepancy and label discrepancy pose a great challenge. To this end, we make the first attempt for cross-network node classification in open-set scenario and propose a novel method based on reconstruction. Firstly, the pseudo unknown class nodes from target network are reconstructed into source network, which addresses label discrepancy by transforming open-set into closed-set with K+1 classes. Secondly, the contrastive-center loss is introduced to enhance the node representations, which aims to identify the unknown nodes from known nodes in networks. And then the invariant representations are learned better to address representation discrepancy. Extensive experiments demonstrate the effectiveness of our method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324004692",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Engineering",
      "Node (physics)",
      "Programming language",
      "Set (abstract data type)",
      "Structural engineering"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Yuhong"
      },
      {
        "surname": "Ji",
        "given_name": "Yunlong"
      },
      {
        "surname": "Yu",
        "given_name": "Kui"
      },
      {
        "surname": "Hu",
        "given_name": "Xuegang"
      },
      {
        "surname": "Wu",
        "given_name": "Xindong"
      }
    ]
  },
  {
    "title": "Prototype learning for adversarial domain adaptation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110653",
    "abstract": "Adversarial learning has been widely used in recent years to address the issue of domain shift in domain adaptation. However, this approach focuses on global cross-domain alignment and overlooks the alignment of class boundaries. To tackle this limitation, we introduce a novel method called PLADA. PLADA leverages prototype learning to align category distributions across domains. The prototypes in PLADA represent the source category distribution, which is constructed using labelled data and transferred to the target domain. In addition to adversarial learning for global domain-invariant feature learning, we propose the weighted prototype loss (WPL) to embed prototype information. WPL transforms the local category distribution alignment problem into a distance measurement between the prediction and prototypes, resulting in a more discriminative representation. Experimental results demonstrate that our proposed model performs comparably well on multiple classic domain adaptation tasks, showcasing the potential of PLADA.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324004047",
    "keywords": [
      "Adaptation (eye)",
      "Adversarial system",
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Discriminative model",
      "Domain (mathematical analysis)",
      "Domain adaptation",
      "Feature (linguistics)",
      "Feature learning",
      "Invariant (physics)",
      "Law",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematical physics",
      "Mathematics",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Political science",
      "Politics",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Fang",
        "given_name": "Yuchun"
      },
      {
        "surname": "Chen",
        "given_name": "Chen"
      },
      {
        "surname": "Zhang",
        "given_name": "Wei"
      },
      {
        "surname": "Wu",
        "given_name": "Jiahua"
      },
      {
        "surname": "Zhang",
        "given_name": "Zhaoxiang"
      },
      {
        "surname": "Xie",
        "given_name": "Shaorong"
      }
    ]
  },
  {
    "title": "Dual feature disentanglement for face anti-spoofing",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110656",
    "abstract": "Domain generalization for Face Anti-Spoofing (FAS) is increasingly crucial with face attacks across unseen domains. Some existing face anti-spoofing methods aim to disentangle spoof feature for both seen and unseen scenarios. However, it is still a challenging problem to capture spoof feature from facial image, because of spoof pattern are often mixed with various facial attributes such as identity, expression, age and gender. To solve the above problem, we propose a Dual Feature Disentanglement Network (DFDN), which leverages feature projection scheme based on domain-invariant feature in conjunction with rearranging the facial structure to learn spoof feature jointly. Specifically, DFDN introduces the local mask module and domain discriminator to enhance the domain-invariant feature. Based on this, we employ the geometry projection relations to achieve the refined spoof pattern via the feature projection. Meanwhile, we disrupt facial structure to weaken face attributes feature learning and guide CNNs to learn spoof feature. Subsequently, consistent regularization is developed to reduce the gap between the above two kinds of spoof pattern by introducing optimal transport and cosine similarity. Extensive experiments and visualizations demonstrate the advantages of our DFDN over the state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324004072",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Dual (grammatical number)",
      "Face (sociological concept)",
      "Feature (linguistics)",
      "Linguistics",
      "Literature",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Social science",
      "Sociology",
      "Spoofing attack"
    ],
    "authors": [
      {
        "surname": "Ma",
        "given_name": "Yimei"
      },
      {
        "surname": "Qian",
        "given_name": "Jianjun"
      },
      {
        "surname": "Li",
        "given_name": "Jun"
      },
      {
        "surname": "Yang",
        "given_name": "Jian"
      }
    ]
  },
  {
    "title": "Global self-sustaining and local inheritance for source-free unsupervised domain adaptation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110679",
    "abstract": "In this paper, we investigate a practical problem called source-free unsupervised domain adaptation, which adapts a source-trained model to the target domain with unlabeled target data. To address this problem, we propose a novel GlObal self-sustAining and Local inheritance (GOAL) method. GOAL contains three components. (1) A backbone follows a mean teacher scheme. The teacher model serves as a smoothing functionality, facilitating a more consistent convergence of the student model. This capability alleviates the student model’s sensitivity to minor input data variations and enhances the overall robustness of the model. Additionally, disparities in predictions between the student and teacher models can be leveraged to identify potential noise in the data. (2) A Global Consistency Self-Sustaining mechanism for learning a stable, discriminative, and diverse prediction space. On the one hand, we employ neighbor samples and mean-teacher schemes to enhance the discriminability and stability of model predictions. On the other hand, non-neighbor samples are leveraged to augment the diversity of model predictions. Furthermore, to mitigate the impact of potential negative neighbors, we derive a weighting factor by incorporating both neighbor entropy and the top- n d similarity of features. (3) A Local Topology Inheritance mechanism to improve the semantic structure of the feature space. We construct a semantic topology graph based on the output predictions of the teacher model and subsequently transmit the teacher topology to the feature space of the student utilizing a local topology inheritance loss. Combining these three components, GOAL can effectively solve the source-free unsupervised domain adaptation. To the best of our knowledge, GOAL is the first attempt to perform topology inheritance for global consistency domain adaptation. Comprehensive experiments illustrate the effectiveness and superiority of GOAL in addressing source-free unsupervised domain adaptation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324004308",
    "keywords": [
      "Adaptation (eye)",
      "Artificial intelligence",
      "Biology",
      "Classifier (UML)",
      "Computer science",
      "Domain (mathematical analysis)",
      "Domain adaptation",
      "Gene",
      "Genetics",
      "Inheritance (genetic algorithm)",
      "Mathematical analysis",
      "Mathematics",
      "Neuroscience"
    ],
    "authors": [
      {
        "surname": "Peng",
        "given_name": "Lin"
      },
      {
        "surname": "He",
        "given_name": "Yuhang"
      },
      {
        "surname": "Wang",
        "given_name": "Shaokun"
      },
      {
        "surname": "Song",
        "given_name": "Xiang"
      },
      {
        "surname": "Dong",
        "given_name": "Songlin"
      },
      {
        "surname": "Wei",
        "given_name": "Xing"
      },
      {
        "surname": "Gong",
        "given_name": "Yihong"
      }
    ]
  },
  {
    "title": "AMANet: An Adaptive Memory Attention Network for video cloud detection",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110616",
    "abstract": "According to their orbits, meteorological satellites can be divided into polar-orbiting satellites and geostationary satellites. Existing cloud detection methods mainly focus on polar-orbiting satellite datasets. The geostationary satellite datasets contain, in contrast, time-continuous frames of particular locations. The temporal consistent information in these consecutive frames aid to increase the detection accuracy, but is challenging to be exploited. Besides, powered by the advanced technology of satellites, the onboard cloud detection application becomes a trend. Considering that satellites have resource limitations on energy and storage, applications deployed on them should be lightweight enough. However, the existing cloud detection models never concentrated on this lightweight video cloud detection task before. In this task, the temporal consistent features provided by time-continuous frames should be exploited for accuracy enhancement with low resource consumption. To tackle this problem, we design a lightweight deep learning video cloud detection model: Adaptive Memory Attention Network (AMANet). The proposed network is based on the encoder–decoder structure. The encoder consists of two branches. In the main branch, spatial and semantic features of the current frame are extracted. In the TemporalAttentionFlow branch, the proposed PyramidEncodingModule adaptively extracts context information from frames in sequence based on their distance to the current frame. In addition, in the proposed AdaptiveMemoryAttentionModule, the temporal relation among frames is extracted and propagated adaptively. The lightweight decoder is designed to gradually recover the cloud masks to the same scale as the input image. Experiments on a Video Cloud Detection dataset based on the dataset Fengyun4aCloud demonstrate that the designed AMANet achieves a remarkable balance between accuracy and resource consumption in comparison with current cloud detection methods, lightweight semantic segmentation methods, and video semantic segmentation methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324003674",
    "keywords": [
      "Artificial intelligence",
      "Cloud computing",
      "Computer science",
      "Computer vision",
      "Operating system"
    ],
    "authors": [
      {
        "surname": "Luo",
        "given_name": "Chen"
      },
      {
        "surname": "Feng",
        "given_name": "Shanshan"
      },
      {
        "surname": "Quan",
        "given_name": "YingLing"
      },
      {
        "surname": "Ye",
        "given_name": "Yunming"
      },
      {
        "surname": "Xu",
        "given_name": "Yong"
      },
      {
        "surname": "Li",
        "given_name": "Xutao"
      },
      {
        "surname": "Zhang",
        "given_name": "Baoquan"
      }
    ]
  },
  {
    "title": "Semantic-embedded similarity prototype for scene recognition",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110725",
    "abstract": "Due to the high inter-class similarity caused by the complex composition and the co-existing objects across scenes, numerous studies have explored object semantic knowledge within scenes to improve scene recognition. However, a resulting challenge emerges as object information extraction techniques require heavy computational costs, thereby burdening the network considerably. This limitation often renders object-assisted approaches incompatible with edge devices in practical deployment. In contrast, this paper proposes a semantic knowledge-based similarity prototype, which can help the scene recognition network achieve superior accuracy without increasing the computational cost in practice. It is simple and can be plug-and-played into existing pipelines. More specifically, a statistical strategy is introduced to depict semantic knowledge in scenes as class-level semantic representations. These representations are used to explore correlations between scene classes, ultimately constructing a similarity prototype. Furthermore, we propose to leverage the similarity prototype to support network training from the perspective of Gradient Label Softening and Batch-level Contrastive Loss, respectively. Comprehensive evaluations on multiple benchmarks show that our similarity prototype enhances the performance of existing networks, all while avoiding any additional computational burden in practical deployments. Code and the statistical similarity prototype will be available at https://github.com/ChuanxinSong/SimilarityPrototype.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032400476X",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Cognitive neuroscience of visual object recognition",
      "Computer science",
      "Data mining",
      "Executable",
      "Image (mathematics)",
      "Information retrieval",
      "Leverage (statistics)",
      "Machine learning",
      "Object (grammar)",
      "Operating system",
      "Semantic similarity",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Song",
        "given_name": "Chuanxin"
      },
      {
        "surname": "Wu",
        "given_name": "Hanbo"
      },
      {
        "surname": "Ma",
        "given_name": "Xin"
      },
      {
        "surname": "Li",
        "given_name": "Yibin"
      }
    ]
  },
  {
    "title": "Multi-label feature selection using self-information in divergence-based fuzzy neighborhood rough sets",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110684",
    "abstract": "Multi-label feature selection corresponds to pattern recognition and knowledge mining, and its application has been expanded to different scenarios. As an excellent processing platform for uncertain and ambiguous information, divergence-based fuzzy rough sets (Div-FRSs) have been proposed and applied to feature selection. However, there are three critical problems to be solved when applying Div-FRSs to multi-label learning. The first is how to effectively dispose the noise produced by features in multi-label data. The second is how to synthetically consider the relevance among all labels. The last is how to thoroughly mine the uncertainty brought by upper approximations neglected in Div-FRSs existing researches. To address these issues, this study presents a new divergence-based fuzzy neighborhood rough set model (Div-FNRSs) for multi-label learning using self-information. First, the divergence-based fuzzy neighborhood relation and class are gradually raised to manage the noise in multi-label data, and fuzzy decision is introduced to dispose all labels as a whole. Combining them together, a new model Div-FNRSs is constructed. Then, divergence-based fuzzy neighborhood self-information containing upper approximations and lower approximations is designed to depict distinguishing ability of features through three-level uncertainty measure establishment and granulation property exploration. Furthermore, feature significance for choosing the optimal features is given and it motivates a heuristic feature-selection algorithm DivFNSI-FS. Finally, data experiments are completed to validate DivFNSI-FS effectiveness with six state-of-the-art multi-label feature selection approaches on fourteen multi-label datasets. A conclusion can be drawn that DivFNSI-FS outperforms existing algorithms to obtain better performance on eight commonly-used evaluation indexes.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324004357",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Divergence (linguistics)",
      "Feature (linguistics)",
      "Feature selection",
      "Fuzzy logic",
      "Fuzzy set",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Rough set",
      "Selection (genetic algorithm)"
    ],
    "authors": [
      {
        "surname": "Jiang",
        "given_name": "Jiefang"
      },
      {
        "surname": "Zhang",
        "given_name": "Xianyong"
      },
      {
        "surname": "Yuan",
        "given_name": "Zhong"
      }
    ]
  },
  {
    "title": "Graph Attentive Dual Ensemble learning for Unsupervised Domain Adaptation on point clouds",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110690",
    "abstract": "Due to the annotation difficulty of point clouds, Unsupervised Domain Adaptation (UDA) is a promising direction to address unlabeled point cloud classification and segmentation. Recent works show that adding a self-supervised learning branch for target domain training consistently boosts UDA point cloud tasks. However, most of these works simply resort to geometric deformation, which ignores semantic information and is hard to bridge the domain gap. In this paper, we propose a novel self-learning strategy for UDA on point clouds, termed as Graph Attentive Dual Ensemble learning (GRADE), which delivers semantic information directly. Specifically, with a pre-training process on the source domain, GRADE further builds dual collaborative training branches on the target domain, where each of them constructs a temporal average teacher model and distills its pseudo labels to the other branch. To achieve faithful labels from each teacher model, we improve the popular DGCNN architecture by introducing a dynamic graph attentive module to mine the relation between local neighborhood points. We conduct extensive experiments on several UDA point cloud benchmarks, and the results demonstrate that our GRADE method outperforms the state-of-the-art methods on both classification and segmentation tasks with clear margins.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324004412",
    "keywords": [
      "Adaptation (eye)",
      "Art",
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Domain (mathematical analysis)",
      "Domain adaptation",
      "Dual (grammatical number)",
      "Ensemble learning",
      "Graph",
      "Literature",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Point cloud",
      "Psychology",
      "Theoretical computer science",
      "Unsupervised learning"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Qing"
      },
      {
        "surname": "Yan",
        "given_name": "Chuan"
      },
      {
        "surname": "Hao",
        "given_name": "Qi"
      },
      {
        "surname": "Peng",
        "given_name": "Xiaojiang"
      },
      {
        "surname": "Liu",
        "given_name": "Li"
      }
    ]
  },
  {
    "title": "Large-scale multi-view clustering via matrix factorization of consensus graph",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110716",
    "abstract": "Recently, anchors-based multi-view clustering methods have been widely concerned for they can not only significantly reduce the time complexity but also have good interpretability. However, the time consumption of optimization and spectral embedding with Singular Value Decomposition (SVD) is expensive for large-scale multi-view clustering due to the large-scale consensus graph. This paper proposes to factorize the consensus graph to directly obtain the low-dimension consensus embedding matrix by optimizing the objective function. Specifically, the consensus graph is factorized into a transition matrix and a low-dimension embedding matrix. Among them, the transition matrix is used to prevent the clustering time consumption from increasing significantly with the number of anchors, and the low-dimension embedding matrix is used to mine the low-dimension consensus information of each view. The proposed method demonstrates its superiority by outperforming eight multi-view clustering algorithms on nine datasets, as evidenced by the clustering results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324004679",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Cluster analysis",
      "Combinatorics",
      "Computer science",
      "Correlation clustering",
      "Dimension (graph theory)",
      "Dimensionality reduction",
      "Eigenvalues and eigenvectors",
      "Embedding",
      "Graph",
      "Mathematics",
      "Matrix decomposition",
      "Non-negative matrix factorization",
      "Physics",
      "Quantum mechanics",
      "Singular value decomposition",
      "Spectral clustering",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Zengbiao"
      },
      {
        "surname": "Tan",
        "given_name": "Yihua"
      },
      {
        "surname": "Yang",
        "given_name": "Tao"
      }
    ]
  },
  {
    "title": "Underwater object detection in noisy imbalanced datasets",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110649",
    "abstract": "Class imbalance occurs in the datasets with a disproportionate ratio of observations. The class imbalance problem drives the detection and classification systems to be more biased towards the over-represented classes while the under-represented classes may not receive sufficient learning. Previous works often deploy distribution based re-balancing approaches to address this problem. However, these established techniques do not work properly for underwater object detection where label noise commonly exists. In our experiments, we observe that the imbalanced detection problem may be caused by imbalance data distributions or label noise. To deal with these challenges, we first propose a noise removal (NR) algorithm to remove label noise in the datasets, and then propose a factor-agnostic gradient re-weighting algorithm (FAGR) to address the imbalanced detection problem. FAGR provides a rebalanced gradient to each class, which encourages the detection network to treat all the classes equally whilst minimising the detection discrepancy. Our proposed NR+FAGR framework achieves state-of-the-art (SOAT) performance on three underwater object datasets due to its high capacity in handling the class imbalance and noise issues. The source code will be made available at: https://github.com/IanDragon.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032400400X",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Geography",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Remote sensing",
      "Underwater"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Long"
      },
      {
        "surname": "Li",
        "given_name": "Tengyue"
      },
      {
        "surname": "Zhou",
        "given_name": "Andy"
      },
      {
        "surname": "Wang",
        "given_name": "Shengke"
      },
      {
        "surname": "Dong",
        "given_name": "Junyu"
      },
      {
        "surname": "Zhou",
        "given_name": "Huiyu"
      }
    ]
  },
  {
    "title": "A lie group semi-supervised FCM clustering method for image segmentation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110681",
    "abstract": "As an unsupervised clustering method with low overhead, Fuzzy C-means (FCM) clustering has been widely used in a variety of image segmentation tasks. However, existing FCM clustering methods are sensitive to image noises and are either suffer from losing of image detail or falling into local optima in identifying cluster centers. Aiming at these problems, this paper proposes a Lie group semi-supervised FCM (LieSSFCM) clustering method for image segmentation. The method maps the input image from Euclidean space to Lie group manifold by representing each image pixel as a matrix Lie group and calculates geodesic distances between group elements and cluster centers on Lie group manifold. Prior information of the image and neighborhood relationships of pixels are used to guide the initialization and constrain the update of cluster centers and the corresponding fuzzy membership matrix. The proposed LieSSFCM has been validated against two medical image datasets and was compared with seven FCM clustering methods. Experimental results along with a systematic evaluation demonstrated that the method was superior in segmentation accuracy both visually and statistically, robustness to noises, adaptability to different tasks, and stability while maintaining a moderate computational complexity.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324004321",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Computer vision",
      "Group (periodic table)",
      "Image (mathematics)",
      "Image segmentation",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Sun",
        "given_name": "Haocheng"
      },
      {
        "surname": "Liu",
        "given_name": "Li"
      },
      {
        "surname": "Li",
        "given_name": "Fanzhang"
      }
    ]
  },
  {
    "title": "Self-supervised learning for RGB-D object tracking",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110543",
    "abstract": "Recently, there has been a growing interest in RGB-D object tracking thanks to its promising performance achieved by combining visual information with auxiliary depth cues. However, the limited volume of annotated RGB-D tracking data for offline training has hindered the development of a dedicated end-to-end RGB-D tracker design. Consequently, the current state-of-the-art RGB-D trackers mainly rely on the visual branch to support the appearance modelling, with the depth map utilised for elementary information fusion or failure reasoning of online tracking. Despite the achieved progress, the current paradigms for RGB-D tracking have not fully harnessed the inherent potential of depth information, nor fully exploited the synergy of vision-depth information. Considering the availability of ample unlabelled RGB-D data and the advancement in self-supervised learning, we address the problem of self-supervised learning for RGB-D object tracking. Specifically, an RGB-D backbone network is trained on unlabelled RGB-D datasets using masked image modelling. To train the network, the masking mechanism creates a selective occlusion of the input visible image to force the corresponding aligned depth map to help with discerning and learning vision-depth cues for the reconstruction of the masked visible image. As a result, the pre-trained backbone network is capable of cooperating with crucial visual and depth features of the diverse objects and background in the RGB-D image. The intermediate RGB-D features output by the pre-trained network can effectively be used for object tracking. We thus embed the pre-trained RGB-D network into a transformer-based tracking framework for stable tracking. Comprehensive experiments and the analysis of the results obtained on several RGB-D tracking datasets demonstrate the effectiveness and superiority of the proposed RGB-D self-supervised learning framework and the following tracking approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324002942",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Backbone network",
      "BitTorrent tracker",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Eye tracking",
      "Pedagogy",
      "Psychology",
      "RGB color model",
      "Supervised learning",
      "Tracking (education)"
    ],
    "authors": [
      {
        "surname": "Zhu",
        "given_name": "Xue-Feng"
      },
      {
        "surname": "Xu",
        "given_name": "Tianyang"
      },
      {
        "surname": "Atito",
        "given_name": "Sara"
      },
      {
        "surname": "Awais",
        "given_name": "Muhammad"
      },
      {
        "surname": "Wu",
        "given_name": "Xiao-Jun"
      },
      {
        "surname": "Feng",
        "given_name": "Zhenhua"
      },
      {
        "surname": "Kittler",
        "given_name": "Josef"
      }
    ]
  },
  {
    "title": "Linear Centroid Encoder for Supervised Principal Component Analysis",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110634",
    "abstract": "We propose a new supervised dimensionality reduction technique called Supervised Linear Centroid-Encoder (SLCE), a linear counterpart of the nonlinear Centroid-Encoder (CE) (Ghosh and Kirby, 2022). SLCE works by mapping the samples of a class to its class centroid using a linear transformation. The transformation is a projection that reconstructs a point such that its distance from the corresponding class centroid, i.e., centroid-reconstruction loss, is minimized in the ambient space. We derive a closed-form solution using an eigendecomposition of a symmetric matrix. We did a detailed analysis and presented some crucial mathematical properties of the proposed approach. We establish a connection between the eigenvalues and the centroid-reconstruction loss. In contrast to Principal Component Analysis (PCA) which reconstructs a sample in the ambient space, the transformation of SLCE uses the instances of a class to rebuild the corresponding class centroid. Therefore the proposed method can be considered a form of supervised PCA. Experimental results show the performance advantage of SLCE over other supervised methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324003856",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Centroid",
      "Chemistry",
      "Computer science",
      "Dimensionality reduction",
      "Eigenvalues and eigenvectors",
      "Encoder",
      "Gene",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Principal component analysis",
      "Projection (relational algebra)",
      "Quantum mechanics",
      "Statistics",
      "Transformation (genetics)"
    ],
    "authors": [
      {
        "surname": "Ghosh",
        "given_name": "Tomojit"
      },
      {
        "surname": "Kirby",
        "given_name": "Michael"
      }
    ]
  },
  {
    "title": "Few-shot image classification via hybrid representation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110640",
    "abstract": "Few-shot image classification aims to learn an embedding model on the base datasets and design a base learner to recognize novel categories. The few-shot image classification framework is a two-phase process. First, the pre-train phase utilizes the base data to train a CNN-based feature extractor. Next, in the meta-test phase, the frozen feature extractor is applied to novel data with categories different from the base data. A base learner is then designed for recognition. Several simple base learners, including nearest neighbor, support vector machine, and logistic regression classifiers, have been recently introduced for few-shot learning tasks. However, these base learners are separately designed to consider specific representations (e.g., the class center) or shared representations (e.g., the boundaries). This paper mainly focuses on exploring the representation-residual base learners, which aim to represent a query sample with the support set and predict the query sample’s label based on the minimal residual error. We first introduce two representation-residual base learners: a specific representation base learner and a shared representation base learner. Then, we propose a novel hybrid representation base learner that combines both base learners to generate competitive representation. Additionally, we extend our approach by incorporating a self-training framework to utilize the query data fully. We evaluate our proposed method on several benchmark few-shot image classification datasets, such as miniImageNet, tieredImageNet, CIFAR-FS, FC100, and CUB datasets. The experimental results indicate that our proposed approach shows a significant performance improvement.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324003911",
    "keywords": [
      "Artificial intelligence",
      "Base (topology)",
      "Benchmark (surveying)",
      "Class (philosophy)",
      "Computer science",
      "Data mining",
      "Feature (linguistics)",
      "Feature extraction",
      "Geodesy",
      "Geography",
      "Law",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Programming language",
      "Representation (politics)",
      "Set (abstract data type)",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Bao-Di"
      },
      {
        "surname": "Shao",
        "given_name": "Shuai"
      },
      {
        "surname": "Zhao",
        "given_name": "Chunyan"
      },
      {
        "surname": "Xing",
        "given_name": "Lei"
      },
      {
        "surname": "Liu",
        "given_name": "Weifeng"
      },
      {
        "surname": "Cao",
        "given_name": "Weijia"
      },
      {
        "surname": "Zhou",
        "given_name": "Yicong"
      }
    ]
  },
  {
    "title": "Boosting edge detection via Fusing Spatial and Frequency Domains",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110699",
    "abstract": "Deep learning-based edge detection methods have shown great advantages and obtained promising performance. However, most of the current methods only extract features from the spatial (RGB) domain for edge detection and the information that can be mined is limited. As a result, they could not work well for the scenarios where the object is similar in color to the background. To combat this challenge, we propose a novel edge detection method by incorporating the features of both spatial and frequency domains, named Fusing Spatial and Frequency Domains (FSFD). A Frequency Perception (FP) module is constructed to extract the edges of objects in the frequency domain, which can avoid the indistinguishable situation in the spatial domain due to similar colors. A Multi-Scale Enhancement (MSE) module is designed to learn multi-scale feature in spatial domain, enabling the model to perceive edges of small objects. Spatial-Frequency Fusion (S2F) module is further introduced to fuse the features of spatial and frequency domains using an online learning manner. Adequate experiments are conducted on popular BSDS500, NYUDV2, and Multicue datasets. The results show that our method can outperform other methods as the state-of-the-art when dealing with the problem of edge detection. The codes will be released on https://github.com/JingDongdong/FSFD.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324004503",
    "keywords": [
      "Artificial intelligence",
      "Boosting (machine learning)",
      "Computer science",
      "Computer vision",
      "Domain (mathematical analysis)",
      "Edge detection",
      "Electrical engineering",
      "Engineering",
      "Enhanced Data Rates for GSM Evolution",
      "Feature (linguistics)",
      "Frequency domain",
      "Fuse (electrical)",
      "Image (mathematics)",
      "Image processing",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Object detection",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "RGB color model",
      "Spatial analysis",
      "Spatial frequency",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Jing",
        "given_name": "Dongdong"
      },
      {
        "surname": "Shao",
        "given_name": "Huikai"
      },
      {
        "surname": "Zhong",
        "given_name": "Dexing"
      }
    ]
  },
  {
    "title": "EDMD: An Entropy based Dissimilarity measure to cluster Mixed-categorical Data",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110674",
    "abstract": "The effectiveness of clustering techniques is significantly influenced by proximity measures irrespective of type of data and categorical data is no exception. Most of the existing proximity measures for categorical data assume that all attributes contribute equally to the distance measurement which is not true. Usually, frequency or probability-based approaches are better equipped in principle to counter this issue by appropriately weighting the attributes based on the intra-attribute statistical information. However, owing to the qualitative nature of categorical features, the intra-attribute disorder is not captured effectively by the popularly used continuum form of entropy known as Shannon or information entropy. If the categorical data contains ordinal features, then the problem multiplies because the existing measures treat all attributes as nominal. To address these issues, we propose a new Entropy-based Dissimilarity measure for Mixed categorical Data (EDMD) composed of both nominal and ordinal attributes. EDMD treats both nominal and ordinal attributes separately to capture the intrinsic information from the values of two different attribute types. We apply Boltzmann’s definition of entropy, which is based on the principle of counting microstates, to exploit the intra-attribute statistical information of nominal attributes while preserving the order relationships among ordinal values in distance formulation. Additionally, the statistical significance of different attributes of the data towards dissimilarity computation is taken care of through attribute weighting. The proposed measure is free from any user-defined or domain-specific parameters and there is no prior assumption about the distribution of the data sets. Experimental results demonstrate the efficacy of EDMD in terms of cluster quality, accuracy, cluster discrimination ability, and execution time to handle mixed categorical data sets of different characteristics.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324004254",
    "keywords": [
      "Artificial intelligence",
      "Categorical variable",
      "Cluster (spacecraft)",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Entropy (arrow of time)",
      "Mathematics",
      "Measure (data warehouse)",
      "Pattern recognition (psychology)",
      "Physics",
      "Programming language",
      "Quantum mechanics",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Kar",
        "given_name": "Amit Kumar"
      },
      {
        "surname": "Akhter",
        "given_name": "Mohammad Maksood"
      },
      {
        "surname": "Mishra",
        "given_name": "Amaresh Chandra"
      },
      {
        "surname": "Mohanty",
        "given_name": "Sraban Kumar"
      }
    ]
  },
  {
    "title": "The spiking neural network based on fMRI for speech recognition",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110672",
    "abstract": "The structure of the human brain has evolved to achieve extraordinary computing power through continuous refinement by natural selection. At present, the topology of brain-like model lacks biological plausibility. In this paper, a new brain-like model is proposed, called fMRI-SNN, which is a spiking neural network (SNN) constrained by the topology of a functional brain network from human functional Magnetic Resonance Imaging (fMRI) data. To verify its performance, this fMRI-SNN is applied to speech recognition. Our results indicate that the recognition accuracy of fMRI-SNN is superior to that of other SNNs and reported methods, and exhibits stronger performance on more difficult speech recognition tasks. Our discussion on recognition mechanism finds the advantage of fMRI-SNN is that the differences in its neuronal firing patterns are greater than those of other SNNs, since it has better information transmission ability.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324004230",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Pattern recognition (psychology)",
      "Speech recognition",
      "Spiking neural network"
    ],
    "authors": [
      {
        "surname": "Song",
        "given_name": "Yihua"
      },
      {
        "surname": "Guo",
        "given_name": "Lei"
      },
      {
        "surname": "Man",
        "given_name": "Menghua"
      },
      {
        "surname": "Wu",
        "given_name": "Youxi"
      }
    ]
  },
  {
    "title": "Exploring target-related information with reliable global pixel relationships for robust RGB-T tracking",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110707",
    "abstract": "RGB-T Siamese trackers have drawn continuous interest in recent years due to their proper trade-off between accuracy and speed. However, they are sensitive to the background distractors in some challenging cases, thereby inducing unreliable response positions. To overcome such drawbacks, we advance a new RGB-T Siamese tracker, named SiamTIH, which will advance the RGB-T Siamese trackers’ discriminability against distractors by exploiting target-related information and reliable global pixel relationships within multi-modal data. Specifically, we propose a target-related feature enhancement module (TFE) to highlight such areas in the detection branch that are similar to the templates and suppress those background distractor regions that are significantly different from the templates but are greatly informative. Then, we propose an intra- and mutual-modal attention based multi-modal feature fusion module (IMA-MF) to capture the reliable global pixel relationships within multi-modal data. Especially, the intra-modal attention is used to capture the global pixel relationships within each single modality data, and the mutual-modal attention is utilized to enhance the feature representation of the current modality by overall pixel relationships as well as modality-specific relationships. Finally, we propose a hard-focused online classifier (HFOC) that combines an offline classifier and an online classifier to further improve the robustness of our tracker. Besides, the proposed framework is further extended to a Transformer based tracker to verify its generality. Extensive experiments on three RGB-T benchmarks demonstrate that our new RGB-T tracker outperforms the existing ones and maintains real-time performance, exceeding on average 30 frames per second (FPS). The code will be available at https://github.com/Tianlu-Zhang/SiamTIH.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324004588",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Pattern recognition (psychology)",
      "Pedagogy",
      "Pixel",
      "Psychology",
      "RGB color model",
      "Tracking (education)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Tianlu"
      },
      {
        "surname": "He",
        "given_name": "Xiaoyi"
      },
      {
        "surname": "Luo",
        "given_name": "Yongjiang"
      },
      {
        "surname": "Zhang",
        "given_name": "Qiang"
      },
      {
        "surname": "Han",
        "given_name": "Jungong"
      }
    ]
  },
  {
    "title": "Advancing Supervised Learning with the Wave Loss Function: A Robust and Smooth Approach",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110637",
    "abstract": "Loss function plays a vital role in supervised learning frameworks. The selection of the appropriate loss function holds the potential to have a substantial impact on the proficiency attained by the acquired model. The training of supervised learning algorithms inherently adheres to predetermined loss functions during the optimization process. In this paper, we present a novel contribution to the realm of supervised machine learning: an asymmetric loss function named wave loss. It exhibits robustness against outliers, insensitivity to noise, boundedness, and a crucial smoothness property. Theoretically, we establish that the proposed wave loss function manifests the essential characteristic of being classification-calibrated. Leveraging this breakthrough, we incorporate the proposed wave loss function into the least squares setting of support vector machines (SVM) and twin support vector machines (TSVM), resulting in two robust and smooth models termed as Wave-SVM and Wave-TSVM, respectively. To address the optimization problem inherent in Wave-SVM, we utilize the adaptive moment estimation (Adam) algorithm, which confers multiple benefits, including the incorporation of adaptive learning rates, efficient memory utilization, and faster convergence during training. It is noteworthy that this paper marks the first instance of Adam’s application to solve an SVM model. Further, we devise an iterative algorithm to solve the optimization problems of Wave-TSVM. To empirically showcase the effectiveness of the proposed Wave-SVM and Wave-TSVM, we evaluate them on benchmark UCI and KEEL datasets (with and without feature noise) from diverse domains. Moreover, to exemplify the applicability of Wave-SVM in the biomedical domain, we evaluate it on the Alzheimer’s Disease Neuroimaging Initiative (ADNI) dataset. The experimental outcomes unequivocally reveal the prowess of Wave-SVM and Wave-TSVM in achieving superior prediction accuracy against the baseline models. The source codes of the proposed models are publicly available at https://github.com/mtanveer1/Wave-SVM.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324003881",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Evolutionary biology",
      "Function (biology)"
    ],
    "authors": [
      {
        "surname": "Akhtar",
        "given_name": "Mushir"
      },
      {
        "surname": "Tanveer",
        "given_name": "M."
      },
      {
        "surname": "Arshad",
        "given_name": "Mohd."
      }
    ]
  },
  {
    "title": "Incomplete RGB-D salient object detection: Conceal, correlate and fuse",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110700",
    "abstract": "Integrating RGB and depth information has advanced salient object detection but low-quality depth maps lead to inaccurate results. The current methods address this issue by employing either a weighting approach or by estimating depth images directly from RGB images. However, these methods face limitations in low-contrast RGB scenarios and fluctuating illumination conditions. To overcome these limitations, a new model has been proposed that discards low-quality depth images and formulates an incomplete multi-modality salient object detection learning. To the best of our knowledge, this is the first incomplete multi-modality salient object detection model that is capable of describing the common latent multi-modality correlation representation between RGB and depth modalities. The model acquires a resilient representation of multiple modalities even when some depth samples are missing due to noise or data scarcity. The proposed approach follows a three-step process: concealing modality-specific representation, correlating common latent representation, and fusing multilevel representation. We processed shallow and deep features separately in Shallow Common Latent Representation (SCLR) block and Deep Common Latent Representation (DCLR) block, respectively. The model outperforms 14 state-of-the-art saliency detectors on 6 benchmark datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324004515",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Electrical engineering",
      "Engineering",
      "Fuse (electrical)",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "RGB color model",
      "Salient"
    ],
    "authors": [
      {
        "surname": "Kanwal",
        "given_name": "Samra"
      },
      {
        "surname": "Taj",
        "given_name": "Imtiaz Ahmad"
      }
    ]
  },
  {
    "title": "MixingMask: A contour-aware approach for joint object detection and instance segmentation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110620",
    "abstract": "Remarkable achievements have been made in object detection and segmentation tasks. However, there remains a noticeable scarcity of methodologies that can achieve satisfactory results in both tasks simultaneously. To address this problem, we present a solution called MixingMask, in which we have a key insight to provide specific attention to boundary features by leveraging contour-based segmentation methods. Specifically, our approach commences with a novel contour deformation module that employs mixing operations coupled with the proposed adaptive feature sampling. Successively, the contours are encoded using a decoupled vector for bounding box regression, thereby effectively associating the contour shape with its scale and position. Lastly, we incorporate the proposed contour regression module into the baseline method to achieve specialized attention to boundary features. Such a design not only successfully remedies the prevailing disregard towards boundary features but also forms an implicit liaison between object detection and instance segmentation tasks. Comprehensive experimental assessments validate the superior performance of the proposed method in both object detection and instance segmentation tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324003716",
    "keywords": [
      "Artificial intelligence",
      "Boundary (topology)",
      "Bounding overwatch",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Image (mathematics)",
      "Image segmentation",
      "Linguistics",
      "Machine learning",
      "Margin (machine learning)",
      "Mathematical analysis",
      "Mathematics",
      "Minimum bounding box",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Ouyang",
        "given_name": "Wenzhe"
      },
      {
        "surname": "Xu",
        "given_name": "Zenglin"
      },
      {
        "surname": "Xu",
        "given_name": "Jing"
      },
      {
        "surname": "Wang",
        "given_name": "Qifan"
      },
      {
        "surname": "Xu",
        "given_name": "Yong"
      }
    ]
  },
  {
    "title": "Conditional advancement of machine learning algorithm via fuzzy neural network",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110732",
    "abstract": "Improving overall performance is the ultimate goal of any machine learning (ML) algorithm. While it is a trivial task to explore multiple individual validation measurements, evaluating and monitoring overall performance can be complicated due to the highly nonlinear nature of the functions describing the relationships among different validation metrics, such as the Dice Similarity Coefficient (DSC) and Jaccard Index (JI). Therefore, it is naturally desirable to have a reliable validation algorithm or model that can integrate all existing validation metrics into a single value. This consolidated metric would enable straightforward assessment of an ML algorithm’s performance and identify areas for improvement. To deal with such a complex nonlinear problem, this study suggests a novel parameterized model named Adaptive Neuro-Fuzzy Inference Systems (ANFIS), which takes any set of input–output precise-imprecise data and uses a neuro-adaptive learning strategy to tune the parameters of the pre-defined membership functions. Our method can be accepted as an elegant and the state-of-the-art method for the nonlinear function approximation, which could be added directly to any convolutional neural networks (CNN) loss functions as the regularization term to generate a constrained-CNN-FUZZY model optimization. To demonstrate the ability of the purposed method and provide a practical explanation of the capability of ANFIS, we use deep CNN as a testing platform to consider the fact that one of the biggest challenges CNN-developers faced today is to reduce the mismatching between the provided input data and the predicted results monitored by different validation metrics. We first create a toy dataset using MNIST and investigate the properties of the proposed model. We then use a medical dataset to demonstrate our method’s efficacy on brain lesion segmentation. In both datasets, our method shows reliable validation results to guide researchers towards choosing performance metrics in a problem-aware manner, especially when the results of different validation metrics are too similar among models to determine the best one.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324004837",
    "keywords": [
      "Adaptive neuro fuzzy inference system",
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Convolutional neural network",
      "Economics",
      "Fuzzy control system",
      "Fuzzy logic",
      "Jaccard index",
      "Machine learning",
      "Metric (unit)",
      "Nonlinear system",
      "Operations management",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics"
    ],
    "authors": [
      {
        "surname": "Bronik",
        "given_name": "Kevin"
      },
      {
        "surname": "Zhang",
        "given_name": "Le"
      }
    ]
  },
  {
    "title": "An empirical study on the robustness of the segment anything model (SAM)",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110685",
    "abstract": "The Segment Anything Model (SAM) is a foundation model for general image segmentation. Although it exhibits impressive performance predominantly on natural images, understanding its robustness against various image perturbations and domains is critical for real-world applications where such challenges frequently arise. In this study we conduct a comprehensive robustness investigation of SAM under diverse real-world conditions. Our experiments encompass a wide range of image perturbations. Our experimental results demonstrate that SAM’s performance generally declines under perturbed images, with varying degrees of vulnerability across different perturbations. By customizing prompting techniques and leveraging domain knowledge based on the unique characteristics of each dataset, the model’s resilience to these perturbations can be enhanced, addressing dataset-specific challenges. This work sheds light on the limitations and strengths of SAM in real-world applications, promoting the development of more robust and versatile image segmentation solutions. Our code is available at https://github.com/EternityYW/SAM-Robustness/.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324004369",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Gene",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Yuqing"
      },
      {
        "surname": "Zhao",
        "given_name": "Yun"
      },
      {
        "surname": "Petzold",
        "given_name": "Linda"
      }
    ]
  },
  {
    "title": "Hybrid-context-based multi-prior entropy modeling for learned lossless image compression",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110632",
    "abstract": "Lossless image compression is an essential aspect of image processing, particularly in many fields that require high information fidelity. In recent years, learned lossless image compression methods have shown promising results. However, many of these methods do not make optimal use of available information, leading to sub-optimal performance. This paper proposes a multi-prior entropy model for lossless image compression, which effectively leverages available information to achieve better compression performance. The proposed multi-prior comprises a cross-channel prior, hybrid local context, and hyperprior, allowing it to effectively utilize all available information. To remove redundancy across color channels, the original image is first losslessly transformed into YUV color space. The network then learns priors from the original image, the prior-coding channels, and the local context, which are fused to form the multi-prior used for GMM parameters estimation. Moreover, to capture the features of different images, a hybrid local context is abstracted using different kernel sizes of mask convolutions in a local context. The experimental results on several datasets demonstrate that our algorithm outperforms several existing learning-based image compression methods and traditional methods, such as JPEG2000, WebP, and FLIF.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324003832",
    "keywords": [
      "Algorithm",
      "Archaeology",
      "Artificial intelligence",
      "Composite material",
      "Compression (physics)",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Data compression",
      "Entropy (arrow of time)",
      "Entropy encoding",
      "Geography",
      "Image (mathematics)",
      "Image compression",
      "Image processing",
      "Lossless compression",
      "Materials science",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics"
    ],
    "authors": [
      {
        "surname": "Fu",
        "given_name": "Chuan"
      },
      {
        "surname": "Du",
        "given_name": "Bo"
      },
      {
        "surname": "Zhang",
        "given_name": "Liangpei"
      }
    ]
  },
  {
    "title": "Guided contrastive boundary learning for semantic segmentation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110723",
    "abstract": "Semantic segmentation, a fundamental task in environmental understanding, aims to assign each image pixel to a specific class. Despite recent progress, segmentation accuracy in boundary regions remains suboptimal. This paper introduces Guided Contrastive Boundary Learning (GCBL), a novel framework designed to enhance feature representation learning, thereby improving boundary segmentation performance. Unlike conventional contrastive learning, GCBL guides inter-class representation learning by weighting pixel contributions based on their estimated probabilities. For intra-class learning, it leverages the neural collapse phenomenon, encouraging representations to align with last-layer classifier weights. Additionally, an asymmetric distance boundary pixel search strategy ensures a more reasonable selection of contrastive pairs. To prevent weight collapse in learning, a regularization term is applied to the last-layer classifier’s weights. The GCBL method is readily integrable into existing and future segmentation frameworks. Extensive experiments on the Cityscapes, ADE20K, and S3DIS datasets demonstrate the effectiveness and generalizability of our approach. Code is available at https://github.com/skyshoumeng/GCBL.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324004746",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Qiu",
        "given_name": "Shoumeng"
      },
      {
        "surname": "Chen",
        "given_name": "Jie"
      },
      {
        "surname": "Zhang",
        "given_name": "Haiqiang"
      },
      {
        "surname": "Wan",
        "given_name": "Ru"
      },
      {
        "surname": "Xue",
        "given_name": "Xiangyang"
      },
      {
        "surname": "Pu",
        "given_name": "Jian"
      }
    ]
  },
  {
    "title": "G2-SCANN: Gaussian-kernel graph-based SLD clustering algorithm with natural neighbourhood",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110682",
    "abstract": "For most clustering methods, not only the number of clusters must be set in advance, but also various hyperparameters such as initial centroids, number of nearest neighbours, the minimum number of points, neighbourhood radius, and cutoff distance all require pre-specification. As one of the most promising unsupervised learning methods in machine intelligence, existing clustering methods cannot simultaneously handle datasets with arbitrary shapes, different densities, distinct sizes, and overlapping. Background outliers and high dimensionality make clustering problems more challenging. In this paper, we propose a novel universal clustering methodology, called G2-SCANN, which yields the best clustering performance for all 30 synthetic and real datasets without any hyperparameter tuning if the exact number of clusters is known. Firstly, the shortest path length (SPL) in complex network or graph-based geodesic distance is used to give a locally backbone-structured description of graph vertex similarity. Accordingly, SPL-weighted local degree (SLD) is defined as vertex attributes of a SPL-weighted graph expressed by G2-SPL adjacency matrix with ε-natural neighbourhood. Secondly, the process of calculating SLD for every data point in a bottom-up way directly leads to division from a complete graph constituted by all data points to a group of SLD trees. This brings the interpretability and the elimination of lone trees. Thirdly, contrastive learning of largest SLD values for finding root vertices of each divisive tree is conducted and top-down category message is then transmitted from the root vertices to all the leaf ones of a SLD tree. It eventually produces tree-like clusters. Totally, the proposed G2-SCANN method leverages both local neighbouring similarity of data points and global information about data distribution and makes it perform better than other methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324004333",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Cluster analysis",
      "Combinatorics",
      "Computer science",
      "Graph",
      "Kernel (algebra)",
      "Mathematical analysis",
      "Mathematics",
      "Neighbourhood (mathematics)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Deng",
        "given_name": "Zhidong"
      },
      {
        "surname": "Wang",
        "given_name": "Jingyi"
      }
    ]
  },
  {
    "title": "Consistency approximation: Incremental feature selection based on fuzzy rough set theory",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110652",
    "abstract": "Fuzzy Rough Set Theory (FRST)-based feature selection has been widely used as a preprocessing step to handle dynamic and large datasets. However, large-scale or high-dimensional datasets remain intractable for FRST-based feature selection approaches due to high space complexity and unsatisfactory classification performance. To overcome these challenges, we propose a Consistency Approximation (CA)-based framework for incremental feature selection. By exploring CA, we introduce a novel significance measure and a tri-accelerator. The CA-based significance measure provides a mechanism for each sample in the universe to keep members with different class labels within its fuzzy neighbourhood as far as possible, while keeping members with the same label as close as possible. Furthermore, our tri-accelerator reduces the search space and decreases the computational space with a theoretical lower bound. The experimental results demonstrate the superiority of our proposed algorithm compared to state-of-the-art methods on efficiency and classification accuracy, especially for large-scale and high-dimensional datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324004035",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Consistency (knowledge bases)",
      "Data mining",
      "Feature (linguistics)",
      "Feature selection",
      "Fuzzy logic",
      "Linguistics",
      "Machine learning",
      "Measure (data warehouse)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Preprocessor",
      "Rough set"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Jie"
      },
      {
        "surname": "Wu",
        "given_name": "Daiyang"
      },
      {
        "surname": "Wu",
        "given_name": "JiaXin"
      },
      {
        "surname": "Ye",
        "given_name": "Wenhao"
      },
      {
        "surname": "Huang",
        "given_name": "Faliang"
      },
      {
        "surname": "Wang",
        "given_name": "Jiahai"
      },
      {
        "surname": "See-To",
        "given_name": "Eric W.K."
      }
    ]
  },
  {
    "title": "Global–local consistent semi-supervised segmentation of histopathological image with different perturbations",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110696",
    "abstract": "A histopathological image is a microscopic image applied to examine cellular and tissue structures and identify any abnormalities or disease processes. Histopathological image segmentation is a prerequisite step for analyzing histopathological images that can divide an image into meaningful regions or objects to accurately classify and analyze tissue structures, cellular regions, or particular histological entities. However, the existing deep learning based pathological image segmentation methods require huge annotation efforts from the pathologists, which is labor-intensive and time-consuming. In this scenario, it has become a hotspot to leverage abundantly available unlabeled data to help learn segmentation models given limited labeled data. In this paper, we propose a global–local consistent semi-supervised segmentation (GLCS) model that enforce the consistency of the segmentation results with weak and strong perturbations on unlabeled data. In GLCS, we firstly generate different weak perturbations for each unlabeled sample, and then add a regularization term to ensure the segmentation consistency among different weak perturbations. Next, different from the existing studies applying the regression methods to match the segmentation results among different perturbations, our methods are based on the generative adversarial learning that can keep the global structure consistency among unlabeled data with different strength of perturbations. Finally, we also add a patch-correlation based regularization term to preserve the local structure similarity among different perturbations images. We validate our GLCS on three datasets, i.e. Glas, Crag and MoNuSeg. The experimental results show that our method can achieve to the dice ratio of 90.35, 82.61 and 81.60 with 1:1 proportion of labeled data, which are significantly superior to the state-of-the-art semi-supervised histopathological image segmentation methods. Our code is public available at https://github.com/ISBELLAG/GLCS.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324004473",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Consistency (knowledge bases)",
      "Image segmentation",
      "Leverage (statistics)",
      "Pattern recognition (psychology)",
      "Regularization (linguistics)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Guan",
        "given_name": "Xi"
      },
      {
        "surname": "Zhu",
        "given_name": "Qi"
      },
      {
        "surname": "Sun",
        "given_name": "Liang"
      },
      {
        "surname": "Zhao",
        "given_name": "Junyong"
      },
      {
        "surname": "Zhang",
        "given_name": "Daoqiang"
      },
      {
        "surname": "Wan",
        "given_name": "Peng"
      },
      {
        "surname": "Shao",
        "given_name": "Wei"
      }
    ]
  },
  {
    "title": "Robust image hiding network with Frequency and Spatial Attentions",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110691",
    "abstract": "Convert Image Communication (CIC) is a promising technology to protect the privacy of images. Recently, the emergence of robust CIC resistant to JPEG compression has gained due to the widespread use of JPEG compression in image communication. This paper introduces a R ̲ obust image hiding network with F ̲ requency and S ̲ patial A ̲ ttentions (RFSA) to implement robust CIC. RFSA can hide an image within another image with high robust. It incorporates multiple image attentions corresponding to imperceptibility, recovered image quality, and resistance to JPEG compression, which ensure that secret images are hidden within regions that cause little distortion and can well withstand JPEG compression. Additionally, two encoders, that is, a frequency encoder and a spatial encoder, are mixed to adaptively embed secret images across both frequency and spatial domains. Experimental results demonstrate that the proposed scheme not only maintains high image quality and capacity but also exhibits exceptional resistance to JPEG compression compared to other state-of-the-art image hiding methods. The average Peak Signal-to-Noise Ratio (PSNR) of the recovered image remains at 24.96 dB even under JPEG compression with a quality factor of 55.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324004424",
    "keywords": [
      "Amplifier",
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Computer science",
      "Computer vision",
      "Data compression",
      "Distortion (music)",
      "Encoder",
      "Image (mathematics)",
      "Image compression",
      "Image processing",
      "Image quality",
      "JPEG",
      "JPEG 2000",
      "Lossy compression",
      "Operating system",
      "Peak signal-to-noise ratio",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Zeng",
        "given_name": "Xiaobin"
      },
      {
        "surname": "Feng",
        "given_name": "Bingwen"
      },
      {
        "surname": "Xia",
        "given_name": "Zhihua"
      },
      {
        "surname": "Peng",
        "given_name": "Zecheng"
      },
      {
        "surname": "Qin",
        "given_name": "Tiewei"
      },
      {
        "surname": "Lu",
        "given_name": "Wei"
      }
    ]
  },
  {
    "title": "Few-shot intent detection with self-supervised pretraining and prototype-aware attention",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110641",
    "abstract": "Few-shot intent detection is a more challenging application. However, traditional prototypical networks based on averaging often suffer from issues such as missing key information, poor generalization capabilities. In previous work, using three-dimensional convolutional neural networks (3DCNN) to generate prototype representations faces challenges with long-distance dependencies. Furthermore, a pretrained encoder’s performance in a specific domain is often suboptimal because its knowledge of the specific domain is fragmented. Therefore, in this paper, we propose a simple yet effective two-stage learning strategy to address these issues. In the first stage, we propose a self-supervised multi-task pretraining (SMTP) strategy. SMTP utilizes unlabeled data from the current domain to help the pretrained encoder learn the semantic information of the text and implicitly distinguish semantically similar text representations without using any labels. SMTP aims to enhance the representation capability of the pretrained encoder in a specific domain. In the second stage, we propose a prototype-aware attention (PaAT) model to generate prototype representations of the same class. PaAT generates prototype representations by calculating the attention between class texts, which can effectively solve the long-distance dependence problem of 3DCNN. PaAT is a siamese architecture that can simultaneously generate prototype representations and sentence-level representations of unseen data. In addition, to prevent overfitting in few-shot learning, we introduce an unsupervised contrastive regularization term to constrain PaAT. Our method achieves state-of-the-art performance on four public datasets. 1 1 Our code is available: https://github.com/YS19999/SMTP-PaAT. .",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324003923",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Class (philosophy)",
      "Computer science",
      "Convolutional neural network",
      "Domain (mathematical analysis)",
      "Encoder",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Natural language processing",
      "Operating system",
      "Overfitting",
      "Pattern recognition (psychology)",
      "Regularization (linguistics)",
      "Sentence"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Shun"
      },
      {
        "surname": "Du",
        "given_name": "YaJun"
      },
      {
        "surname": "Zheng",
        "given_name": "Xin"
      },
      {
        "surname": "Li",
        "given_name": "XianYong"
      },
      {
        "surname": "Chen",
        "given_name": "XiaoLiang"
      },
      {
        "surname": "Li",
        "given_name": "YanLi"
      },
      {
        "surname": "Xie",
        "given_name": "ChunZhi"
      }
    ]
  },
  {
    "title": "DGRM: Diffusion-GAN recommendation model to alleviate the mode collapse problem in sparse environments",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110692",
    "abstract": "Generative adversarial network (GAN) has been widely adopted in recommender systems (RSs) to improve the recommendation accuracy. However, existing GAN-based models often suffer from the mode collapse problem in sparse environments and fail to adequately capture the complexity of user preferences and behaviors, which affects recommendation performance. To address these issues, we introduce a diffusion model (DM) into the GAN framework, proposing an efficient Diffusion-GAN recommendation model (DGRM) to achieve mutual enhancement between the two generative models. This model first utilizes the forward process of DM to generate conditional vectors that guide the training of the GAN generator. Subsequently, the backward process of DM assists the GAN discriminator using Wasserstein distance during adversarial training. The Wasserstein distance is adopted to solve the asymmetry of Kullback-Leibler (KL) divergence as a loss function in traditional GANs. Experiments on multiple datasets demonstrate that the proposed model effectively alleviates mode collapse and surpasses other state-of-the-art (SOTA) methods in various evaluation metrics.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324004436",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Deep learning",
      "Detector",
      "Diffusion",
      "Discriminator",
      "Divergence (linguistics)",
      "Generative adversarial network",
      "Generator (circuit theory)",
      "Linguistics",
      "Machine learning",
      "Mode (computer interface)",
      "Operating system",
      "Philosophy",
      "Physics",
      "Power (physics)",
      "Process (computing)",
      "Quantum mechanics",
      "RSS",
      "Recommender system",
      "Telecommunications",
      "Thermodynamics"
    ],
    "authors": [
      {
        "surname": "Jiangzhou",
        "given_name": "Deng"
      },
      {
        "surname": "Songli",
        "given_name": "Wang"
      },
      {
        "surname": "Jianmei",
        "given_name": "Ye"
      },
      {
        "surname": "Lianghao",
        "given_name": "Ji"
      },
      {
        "surname": "Yong",
        "given_name": "Wang"
      }
    ]
  },
  {
    "title": "Multi-modal interaction with token division strategy for RGB-T tracking",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110626",
    "abstract": "RGB-T tracking takes visible and infrared images as inputs, which is an extended application of multi-modal fusion in the field of visual object tracking. The complementarity between visible and infrared modalities can enhance the robustness of tracker in complex scenes. Cross-modal interaction can facilitate the fusion and synergy of different modalities, but most previous methods lack clear target information in multi-modal fusion, leading to some undesired cross-relation in interaction. To reduce these undesired cross-relations, we propose a Multi-modal Interaction scheme Guided by Token Division strategy (MIGTD). This scheme divides the input multi-modal tokens into several categories and restricts the interaction between tokens by setting different rules. The above operation is implemented in parallel through an attention masking strategy. To accurately classify search tokens, an instance segmentation task with box-supervised loss is employed. We conduct extensive experiments on three popular benchmark datasets, RGBT234, LasHeR and VTUAV. The experimental results indicate that the tracker proposed in this article reach the world’s advanced level in performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324003777",
    "keywords": [
      "Arithmetic",
      "Artificial intelligence",
      "Chemistry",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Division (mathematics)",
      "Mathematics",
      "Modal",
      "Pedagogy",
      "Polymer chemistry",
      "Psychology",
      "RGB color model",
      "Security token",
      "Tracking (education)"
    ],
    "authors": [
      {
        "surname": "Cai",
        "given_name": "Yujue"
      },
      {
        "surname": "Sui",
        "given_name": "Xiubao"
      },
      {
        "surname": "Gu",
        "given_name": "Guohua"
      },
      {
        "surname": "Chen",
        "given_name": "Qian"
      }
    ]
  },
  {
    "title": "Token-word mixer meets object-aware transformer for referring image segmentation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110719",
    "abstract": "Referring image segmentation aims to generate a binary mask of the target object according to a referring expression. Some recent works argue that post-fusion paradigm may result in inconsistency and insufficiency issue and propose to integrate textual features during the visual encoding process. Although effective, they do fusion in a single way at each stage of encoder, e.g. utilizing cross attention mechanisms. This single fusion method ignores local and detailed image information correlated with language due to the incapability of attention in capturing high-frequencies information. To address this issue, we propose a Token-Word Mixer, which takes into consideration the characteristics of convolution and attention, and achieves more comprehensive interactions and alignments of multi-modal features through a mix operation. Furthermore, existing methods that rely solely on grid features lack perception of the target object and inference of relationships between objects, making it difficult to associate and align semantic information of target objects during multi-modal fusion when referring expressions or image scenes are complex. Therefore, we propose to incorporates object-level information by exploiting a DETR-based detector to provide region features, and the Object-Aware Transformer encoder with an additional learnable token is proposed to perceive effective information associated with the target object. Based on the enhanced cross-modal features and the aggregated token, we adopt query-based mask generation method instead of pixel classification framework for referring image segmentation. Extensive experiments and ablation studies indicate the effectiveness of our proposed methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324004709",
    "keywords": [
      "Artificial intelligence",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Electrical engineering",
      "Engineering",
      "Image segmentation",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Security token",
      "Segmentation",
      "Speech recognition",
      "Text segmentation",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Zhenliang"
      },
      {
        "surname": "Teng",
        "given_name": "Zhu"
      },
      {
        "surname": "Fan",
        "given_name": "Jack"
      },
      {
        "surname": "Zhang",
        "given_name": "Baopeng"
      },
      {
        "surname": "Fan",
        "given_name": "Jianping"
      }
    ]
  },
  {
    "title": "TSANet: Forecasting traffic congestion patterns from aerial videos using graphs and transformers",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110721",
    "abstract": "Forecasting traffic congestion patterns in lane-less traffic scenarios is a complex task because of the combination of high & irregular vehicle densities, fluctuating speeds, and the presence of environmental obstacles. Existing techniques like vehicle counting and density prediction, which successfully estimate congestion in lane-based traffic, are unsuitable for lane-less traffic scenarios due to the irregular and unpredictable nature of traffic density patterns. To overcome these challenges, we propose traffic states to measure congestion patterns in lane-less traffic scenarios. Each traffic state is characterized by the spatio-temporal distribution of neighbouring road users, including vehicles and motorcyclists. We employ traffic graphs to capture the spatial distribution of neighbouring road users. Also, we propose a novel method for the automated construction of traffic graphs by leveraging the detection and tracking of individual road users in aerial videos. Further, in order to incorporate the temporal distribution, we utilize a transformer model to capture the evolution of spatial traffic graphs over time. This enables us to forecast future spatio-temporal distributions and their associated traffic states. Our proposed model, named Traffic State Anticipation Network (TSANet), can effectively forecast future traffic states by analysing sequences of current traffic graphs, thereby enhancing our understanding of evolving traffic patterns in lane-less scenarios. Also, to address the lack of publicly available lane-less traffic datasets, we introduce EyeonTraffic (EoT), a large-scale lane-less traffic dataset containing three hours of aerial videos captured at three busy intersections in Ahmedabad city, India. Experimental results on the EoT dataset demonstrate the efficacy of our proposed TSANet in effectively anticipating traffic states across diverse spatial regions within an intersection. In addition, we also show that TSANet generalizes well for previously unseen intersections, making it suitable for analysing various traffic scenarios without the need for explicit training, thereby enhancing its practical applicability.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324004722",
    "keywords": [
      "Computer science",
      "Engineering",
      "Floating car data",
      "Real-time computing",
      "Traffic congestion",
      "Traffic congestion reconstruction with Kerner's three-phase theory",
      "Traffic generation model",
      "Transport engineering"
    ],
    "authors": [
      {
        "surname": "Kumar",
        "given_name": "K. Naveen"
      },
      {
        "surname": "Roy",
        "given_name": "Debaditya"
      },
      {
        "surname": "Suman",
        "given_name": "Thakur Ashutosh"
      },
      {
        "surname": "Vishnu",
        "given_name": "Chalavadi"
      },
      {
        "surname": "Mohan",
        "given_name": "C. Krishna"
      }
    ]
  },
  {
    "title": "DIVA: Deep unfolded network from quantum interactive patches for image restoration",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110676",
    "abstract": "This paper presents a deep neural network called DIVA unfolding a baseline adaptive denoising algorithm (DeQuIP), relying on the theory of quantum many-body physics. Furthermore, it is shown that with very slight modifications, this network can be enhanced to solve more challenging image restoration tasks such as image deblurring, super-resolution and inpainting. Despite a compact and interpretable (from a physical perspective) architecture, the proposed deep learning network outperforms several recent algorithms from the literature, designed specifically for each task. The key ingredients of the proposed method are on one hand, its ability to handle non-local image structures through the patch-interaction term and the quantum-based Hamiltonian operator, and, on the other hand, its flexibility to adapt the hyperparameters patch-wisely, due to the training process.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324004278",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Computer graphics (images)",
      "Computer science",
      "Computer vision",
      "Diva",
      "Image (mathematics)",
      "Physics",
      "Quantum",
      "Quantum mechanics",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Dutta",
        "given_name": "Sayantan"
      },
      {
        "surname": "Basarab",
        "given_name": "Adrian"
      },
      {
        "surname": "Georgeot",
        "given_name": "Bertrand"
      },
      {
        "surname": "Kouamé",
        "given_name": "Denis"
      }
    ]
  },
  {
    "title": "Multi-view reduced dimensionality K-means clustering with σ -norm and Schatten p -norm",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110675",
    "abstract": "Recently, multi-view high dimensional data obtained from diverse domains or various feature extractors has drawn great attention due to its reflection of different properties or distributions. In this paper, we propose a novel unsupervised multi-view clustering method, which is called Multi-View Reduced Dimensionality K-means clustering (MRDKM) and integrates the dimension reduction mechanism, σ -norm, Schatten p-norm, and multi-view K-means clustering. Moreover, an unsupervised optimization scheme was proposed to solve the minimization problem with good convergence properties. Comprehensive evaluations of five benchmark datasets and comparisons with several multi-view clustering algorithms demonstrate the superiority of the proposed work.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324004266",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Cluster analysis",
      "Clustering high-dimensional data",
      "Computer science",
      "Curse of dimensionality",
      "Dimensionality reduction",
      "Geodesy",
      "Geography",
      "Law",
      "Mathematics",
      "Norm (philosophy)",
      "Political science"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Xiangdong"
      },
      {
        "surname": "Li",
        "given_name": "Fangfang"
      },
      {
        "surname": "Shi",
        "given_name": "Zhaoyang"
      },
      {
        "surname": "Yang",
        "given_name": "Ming"
      }
    ]
  },
  {
    "title": "A multi-modal extraction integrated model for neuropsychiatric disorders classification",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110646",
    "abstract": "Convolutional neural networks (CNNs) provide high-precision automatic classification of neuropsychiatric disorders based on images. However, the “black box” nature leads to poor interpretability of CNN. This study constructs an integrated model for neuropsychiatric disorders classification from multi-modal data. The proposed model consists of a novel multi-scale image features extraction neural network (MSFM) and a XGBoost. The proposed MSFM extracts the pixel context semantic information from fMRI images with different scales, which employs token and channel-mixing strategy to enhance the information communication between context semantic information. XGBoost is used to extract phenotypic feature from phenotypic records. Based on the integration of phenotypic and image features, a comparative interpretable classification of mental disorders can be achieved. The overall accuracy, sensitivity, and recall of the binary classification (healthy controls & neuropsychiatric disorders) of the integrated model are 90.23%, 91.08%, and 89.33%, respectively. The visualization of image features and the phenotypic features present consistency in the brain regions, increasing the interpretability of the MSFM. Especially, through visual statistical analysis of the test set, it was found that there are differences in the distribution of ADHD, BD, and SD in the brain regions. Our solution may provide psychiatrists with ideas for comparative examinations and diagnosis.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324003972",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Extraction (chemistry)",
      "Materials science",
      "Modal",
      "Pattern recognition (psychology)",
      "Polymer chemistry"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Liangliang"
      },
      {
        "surname": "Liu",
        "given_name": "Zhihong"
      },
      {
        "surname": "Chang",
        "given_name": "Jing"
      },
      {
        "surname": "Xu",
        "given_name": "Xue"
      }
    ]
  },
  {
    "title": "Toward comprehensive and effective palmprint reconstruction attack",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110655",
    "abstract": "The challenge posed by the template-based reconstruction attack significantly impacts the security and privacy of biometric systems. Current reconstruction techniques rely on extensive training data or encounter limitations in adaptability, resulting in subpar reconstruction performance. In this paper, we propose a black-box palmprint template reconstruction method based on the modified Progressive GAN (ProGAN), which achieves a substantial success rate in attacking deep-learning-based and hand-crafted-based templates. Our approach incorporates the dropout mechanism into the generator of ProGAN and introduces a Double Reuse Training Strategy to enable effective training of the reconstruction network despite limited data. Furthermore, we devise a novel Scale-Adaptive Multi-Texture Complementarity loss, enhancing the texture quality of reconstructed images. We conduct extensive experiments on diverse palmprint recognition techniques. The resulting reconstructed images exhibit exceptional image quality. Additionally, we thoroughly examine the security and privacy aspects of the palmprint recognition algorithm based on the insights gained from the reconstruction attacks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324004060",
    "keywords": [
      "Artificial intelligence",
      "Biometrics",
      "Computer science",
      "Computer security",
      "Computer vision"
    ],
    "authors": [
      {
        "surname": "Yan",
        "given_name": "Licheng"
      },
      {
        "surname": "Wang",
        "given_name": "Fei"
      },
      {
        "surname": "Leng",
        "given_name": "Lu"
      },
      {
        "surname": "Teoh",
        "given_name": "Andrew Beng Jin"
      }
    ]
  },
  {
    "title": "Dynamic and static fusion mechanisms of infrared and visible images",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110689",
    "abstract": "This paper propose a dynamic fusion mechanism of infrared and visible images, named DIFM, capable of solving the static fusion optimization problem. The DIFM correlates the image fusion quality with the image restoration quality to construct a unified optimization loss function. According to the DIFM, a dynamic image fusion network of infrared and visible images is constructed and is therefore denoted with DF-Net. Specifically, the DF-Net comprises two modules, i.e., the dynamic fusion module (DFM) and the self-learning dynamic restoration module (SLDRM). In order to solve the static fusion problem of existing methods, the DFM is proposed to learn the fusion weight dynamically. Specifically, the DFM comprises a classification module (CM) and an image fusion module (IFM), which determine whether and how to fuse source images. In addition, a unified fusion loss function is introduced to obtain more hidden features of infrared and visible images in complex environments. Therefore, the stumbling block of deep learning in image fusion, i.e., static fusion, is significantly mitigated. Extensive experiments demonstrate that the dynamic fusion optimization method neatly outperforms the state-of-the-art methods in most metrics.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324004400",
    "keywords": [
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Computer science",
      "Computer vision",
      "Design for manufacturability",
      "Electrical engineering",
      "Engineering",
      "Fuse (electrical)",
      "Fusion",
      "Fusion mechanism",
      "Fusion rules",
      "Geometry",
      "Image (mathematics)",
      "Image fusion",
      "Linguistics",
      "Lipid bilayer fusion",
      "Mathematics",
      "Mechanical engineering",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Fang",
        "given_name": "Aiqing"
      },
      {
        "surname": "Li",
        "given_name": "Ying"
      }
    ]
  },
  {
    "title": "Contrastive cross-modal clustering with twin network",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110645",
    "abstract": "Cross-modal clustering (CMC) methods explore the correlation information between multiple modalities to improve clustering performance. However, the obvious differences between heterogeneous modalities make it difficult to obtain the correlation information directly. In this paper, we propose a novel Contrastive Cross-modal Clustering with Twin Network (3CTnet) for CMC, which contrasts the differences of multiple modalities to fully mine the correlation information. The 3CTnet contains two modal-special encoders and an attention-based correlation propagate module (CPM). First, the modal-special encoders are trained by pseudo-labels to learn the clustering structure and feature of single modality. Then we contrast the clustering structures and features of different modalities to explore the inter-cluster and inter-feature correlation information simultaneously. Finally, the CPM is designed to propagate the learned correlation information among modal-special encoders to further optimize the learning of features and clustering structures. The experiments show that 3CTnet outperforms the state-of-the-art CMC methods on six large datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324003960",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Cluster analysis",
      "Computer science",
      "Modal",
      "Pattern recognition (psychology)",
      "Polymer chemistry"
    ],
    "authors": [
      {
        "surname": "Mao",
        "given_name": "Yiqiao"
      },
      {
        "surname": "Yan",
        "given_name": "Xiaoqiang"
      },
      {
        "surname": "Hu",
        "given_name": "Shizhe"
      },
      {
        "surname": "Ye",
        "given_name": "Yangdong"
      }
    ]
  },
  {
    "title": "Cross-lingual font style transfer with full-domain convolutional attention",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110709",
    "abstract": "In this paper, we propose a new cross-lingual font style transfer model, FCAGAN, which enables font style transfer between different languages by observing a small number of samples. Most previous work has been on style transfer of different fonts for single language content, but in our task we can learn the font style of one language and migrate it to another. We investigated the drawbacks of related studies and found that existing cross-lingual approaches cannot perfectly learn styles from other languages and maintain the integrity of their own content. Therefore, we designed a new full-domain convolutional attention (FCA) module in combination with other modules to better learn font styles, and a multi-layer perceptual discriminator to ensure character integrity. Experiments show that using this model provides more satisfying results than the current cross-lingual font style transfer methods. Code can be found at https://github.com/jtlxlf/FCAGAN.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324004606",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Character (mathematics)",
      "Code (set theory)",
      "Computer science",
      "Domain (mathematical analysis)",
      "Economics",
      "Font",
      "Geometry",
      "History",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Natural language processing",
      "Parallel computing",
      "Programming language",
      "Set (abstract data type)",
      "Speech recognition",
      "Style (visual arts)",
      "Task (project management)",
      "Transfer (computing)"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Hui-huang"
      },
      {
        "surname": "Ji",
        "given_name": "Tian-le"
      },
      {
        "surname": "Rosin",
        "given_name": "Paul L."
      },
      {
        "surname": "Lai",
        "given_name": "Yu-Kun"
      },
      {
        "surname": "Meng",
        "given_name": "Wei-liang"
      },
      {
        "surname": "Wang",
        "given_name": "Yao-nan"
      }
    ]
  },
  {
    "title": "Efficient and robust clustering based on backbone identification",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110635",
    "abstract": "Clustering is the process of grouping similar data objects into different subsets based on their similarities. Inspired by the concept of the popularity of individuals in a community, we rate the popularity of each sample which reflects the centrality of that sample in the dataset. With the aim of identifying clusters with arbitrary shapes and varying densities, we propose a clustering approach that divides samples into separate population groups. This approach is based on identifying the backbone of data, characterized by a set of popular points surrounded by less popular points. To distinguish poorly separated clusters, a proximity measure is defined based on the popularity of samples. We also use the popularity of samples to assign halo points to clusters and calculate cohesion between clusters. The proposed clustering method can detect arbitrary-shaped clusters with varying densities without requiring to specify the number of clusters. Outliers are also identified according to popularity. We demonstrate the effectiveness of the approach on synthetic and real-world datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324003868",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Identification (biology)",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Motallebi",
        "given_name": "Hassan"
      }
    ]
  },
  {
    "title": "AdaptBIR: Adaptive Blind Image Restoration with latent diffusion prior for higher fidelity",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110659",
    "abstract": "This work aims to help diffusion models get their footing in the low-level vision field, solving the pain point of insufficient fidelity. Specifically, we propose an Adaptive Blind Image Restoration framework with latent diffusion prior — AdaptBIR, which can adaptively distinguish and address various ranges of degradations. First, we quantitatively categorize images through an Image Quality Assessment (IQA) method. Then, a dual-encoder degradation removal module is employed with the guidance of IQA scores to reach better information preservation. Lastly, we utilize a two-phase controller to handle the reconstruction process in an organized manner. Extensive experiments show that applying such an adaptive framework achieves better performance on both fidelity and perceptual metrics. In this way, AdaptBIR represents more than just a novel framework, it paves the way for a broader application of the diffusion model in blind image restoration tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324004102",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Fidelity",
      "Image (mathematics)",
      "Image processing",
      "Image restoration",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Yingqi"
      },
      {
        "surname": "He",
        "given_name": "Jingwen"
      },
      {
        "surname": "Liu",
        "given_name": "Yihao"
      },
      {
        "surname": "Lin",
        "given_name": "Xinqi"
      },
      {
        "surname": "Yu",
        "given_name": "Fanghua"
      },
      {
        "surname": "Hu",
        "given_name": "Jinfan"
      },
      {
        "surname": "Qiao",
        "given_name": "Yu"
      },
      {
        "surname": "Dong",
        "given_name": "Chao"
      }
    ]
  },
  {
    "title": "A Transformer-based visual object tracker via learning immediate appearance change",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110705",
    "abstract": "Transformer has shown its great strength in visual object tracking due to its effective attention mechanism, but most prevailing transformer-based trackers only explore temporal information frame by frame, thus overlooking the rich context information inherent in videos. To alleviate this problem, we propose a transformer-based tracker via learning immediate appearance change information in videos, called IAC-tracker. The proposed tracker enhances the perception of the immediate motion state to improve the performance of single target tracking. IAC-tracker contains three key components: a spatial information extractor (SIE) with a superior attention mechanism to progressively extract spatial information, a temporal information extractor (TIE) with a designed temporal attention mechanism to progressively learn target immediate appearance change, and a novel spatial–temporal context enhanced fusion module integrating the information from SIE and TIE to prepare for the final prediction head. Comparison experiments with state-of-the-art trackers on six challenging datasets demonstrate the superior performance of IAC-tracker with real-time running speed.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324004564",
    "keywords": [
      "Artificial intelligence",
      "BitTorrent tracker",
      "Computer science",
      "Computer vision",
      "Electrical engineering",
      "Engineering",
      "Eye tracking",
      "Object (grammar)",
      "Transformer",
      "Video tracking",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Yifan"
      },
      {
        "surname": "Liu",
        "given_name": "Xiaotao"
      },
      {
        "surname": "Yuan",
        "given_name": "Dian"
      },
      {
        "surname": "Wang",
        "given_name": "Jiaoying"
      },
      {
        "surname": "Wu",
        "given_name": "Peng"
      },
      {
        "surname": "Liu",
        "given_name": "Jing"
      }
    ]
  },
  {
    "title": "AMMD: Attentive maximum mean discrepancy for few-shot image classification",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110680",
    "abstract": "Metric-based methods have attained promising performance for few-shot image classification. Maximum Mean Discrepancy (MMD) is a typical distance between distributions, requiring to compute expectations w.r.t. data distributions. In this paper, we propose Attentive Maximum Mean Discrepancy (AMMD) to measure the distances between query images and support classes for few-shot classification. Each query image is classified as the support class with minimal AMMD distance. The proposed AMMD assists MMD with distributions adaptively estimated by an Attention-based Distribution Generation Module (ADGM). ADGM is learned to put more mass on more discriminative features, which makes the proposed AMMD distance emphasize discriminative features and overlook spurious features. Extensive experiments show that our AMMD achieves competitive or state-of-the-art performance on multiple few-shot classification benchmark datasets. Code is available at https://github.com/WuJi1/AMMD.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032400431X",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Engineering",
      "Image (mathematics)",
      "Mathematics",
      "Mechanical engineering",
      "One shot",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Shot (pellet)"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Ji"
      },
      {
        "surname": "Wang",
        "given_name": "Shipeng"
      },
      {
        "surname": "Sun",
        "given_name": "Jian"
      }
    ]
  },
  {
    "title": "View-unaligned clustering with graph regularization",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110706",
    "abstract": "In current multi-view clustering modeling scenarios, the cross-view correspondence of the data is generally presumed in advance. However, this assumption is inevitably violated in practical applications as each view is independently processed during data collection and transmission, thus resulting in the view-unaligned problem (VuP). The absence of cross-view correspondence between the data renders most existing multi-view clustering methods ineffective in addressing the VuP. To address this problem, we propose a novel view-unaligned clustering method, termed View-unaligned Clustering with Graph regularization (VuCG), which performs latent embedding learning on manifold, latent embedding alignment and partition generation in a one-stage manner. Specifically, we implement the latent embedding learning on manifold by seeking a matrix factorization respecting the graph structure, and perform latent embedding alignment by identifying the counterpart from a selected template of the shuffled embedding using global and local information of the data in the latent space. Additionally, we obtain the partition of the aligned data by applying the relaxed k -means algorithm to the aligned embeddings. Extensive experimental results on four practical datasets demonstrate the effectiveness of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324004576",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Clustering high-dimensional data",
      "Combinatorics",
      "Computer science",
      "Data mining",
      "Dimensionality reduction",
      "Embedding",
      "Graph",
      "Mathematics",
      "Nonlinear dimensionality reduction",
      "Partition (number theory)",
      "Pattern recognition (psychology)",
      "Regularization (linguistics)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Cao",
        "given_name": "Junfeng"
      },
      {
        "surname": "Dong",
        "given_name": "Wenhua"
      },
      {
        "surname": "Chen",
        "given_name": "Jing"
      }
    ]
  },
  {
    "title": "Degradation-removed multiscale fusion for low-light salient object detection",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110650",
    "abstract": "Low-light is practical in real-life applications and leads to decreased performance of visual perception. For example, the challenges significantly burden the applications of salient object detection (SOD). Existing methods primarily focus on SOD under normal-light conditions. Although some forward-looking work has attempted to address the problem, the model design fails to directly target the physical degradation factor of darkness. Introducing extra sensors (e.g., depth or infrared) may supplement more necessary saliency information, however, acquisition costs and computational overhead will be involved. To improve the SOD under low-light condition, we devise a new image enhancement method and integrate it into the SOD network to form a new learning framework. Specifically, we employ Retinex-guided self-enhancement in combination with multiscale cross-channel detection, effectively mitigating the influence of factors such as image dark degradation and low contrast. This approach enhances the detection performance without incurring additional costs. Additionally, to promote efforts towards this task, we construct a comprehensive low-light SOD dataset benchmark, named YLLSOD. Finally, we conduct extensive comparative experiments between our proposed method and the state-of-the-art single-modal methods, validating the competitiveness of our approach. Comparative experiments with some representative bi-modal methods further illustrate the advantages of our proposed method. Our new dataset will be available at https://github.com/ynn1030/YLLSOD.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324004011",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Degradation (telecommunications)",
      "Fusion",
      "Linguistics",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Salient",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Yu",
        "given_name": "Nana"
      },
      {
        "surname": "Wang",
        "given_name": "Jie"
      },
      {
        "surname": "Shi",
        "given_name": "Hong"
      },
      {
        "surname": "Zhang",
        "given_name": "Zihao"
      },
      {
        "surname": "Han",
        "given_name": "Yahong"
      }
    ]
  },
  {
    "title": "Linear Gaussian bounding box representation and ring-shaped rotated convolution for oriented object detection",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110677",
    "abstract": "In oriented object detection, current representations of oriented bounding boxes (OBBs) often suffer from the boundary discontinuity problem. Methods of designing continuous regression losses do not essentially solve this problem. Although Gaussian bounding box (GBB) representation avoids this problem, directly regressing GBB is susceptible to numerical instability. We propose linear GBB (LGBB), a novel OBB representation. By linearly transforming the elements of GBB, LGBB avoids the boundary discontinuity problem and has high numerical stability. In addition, existing convolution-based rotation-sensitive feature extraction methods only have local receptive fields, resulting in slow feature aggregation. We propose ring-shaped rotated convolution (RRC), which adaptively rotates feature maps to arbitrary orientations to extract rotation-sensitive features under a ring-shaped receptive field, rapidly aggregating features and contextual information. Experimental results demonstrate that LGBB and RRC achieve state-of-the-art performance. Furthermore, integrating LGBB and RRC into various models effectively improves detection accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032400428X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Bounding overwatch",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Convolution (computer science)",
      "Gaussian",
      "Image (mathematics)",
      "Law",
      "Mathematics",
      "Minimum bounding box",
      "Object (grammar)",
      "Object detection",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Physics",
      "Political science",
      "Politics",
      "Quantum mechanics",
      "Representation (politics)",
      "Ring (chemistry)"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Zhen"
      },
      {
        "surname": "Ma",
        "given_name": "Yunkai"
      },
      {
        "surname": "Fan",
        "given_name": "Junfeng"
      },
      {
        "surname": "Liu",
        "given_name": "Zhaoyang"
      },
      {
        "surname": "Jing",
        "given_name": "Fengshui"
      },
      {
        "surname": "Tan",
        "given_name": "Min"
      }
    ]
  },
  {
    "title": "Discovering attention-guided cross-modality correlation for visible–infrared person re-identification",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110643",
    "abstract": "Visible–infrared person re-identification (VI Re-ID) is an essential and challenging task. Existing studies mainly focus on learning the unified modality-invariant representations directly from visible and infrared images. However, it is hard to obtain the identity-aware patterns due to the co-existence of inter- and intra-modality discrepancies. In this paper, we propose a novel attention-guided cross-modality correlation method (AGCC) to achieve the modality-invariant and identity-discriminative representations for visible–infrared person Re-ID. Specifically, we introduce a modality-aware attention (MAA) mechanism to model the inter- and intra-modality variations, which generates attention masks of two modalities for preserving the most significant region and obtaining the discriminative patterns in each identity. Further, we present an attention-guided channel and spatial correlation scheme (AGCSC) to establish the attention-guided cross-modality correlation, which can bridge the gap between inter- and intra-modalities. Moreover, a novel joint-modality learning head (JMLH) is developed to promote the metric and mutual learning from both feature distribution and classification logit levels. Extensive experiments on two public SYSU-MM01 and RegDB datasets demonstrate the remarkable superiority of our method over the state of the arts. The implementation codes will be made available soon.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324003947",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Computer science",
      "Correlation",
      "Geometry",
      "Identification (biology)",
      "Infrared",
      "Mathematics",
      "Modality (human–computer interaction)",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Psychology"
    ],
    "authors": [
      {
        "surname": "Yu",
        "given_name": "Hao"
      },
      {
        "surname": "Cheng",
        "given_name": "Xu"
      },
      {
        "surname": "Cheng",
        "given_name": "Kevin Ho Man"
      },
      {
        "surname": "Peng",
        "given_name": "Wei"
      },
      {
        "surname": "Yu",
        "given_name": "Zitong"
      },
      {
        "surname": "Zhao",
        "given_name": "Guoying"
      }
    ]
  },
  {
    "title": "Discriminative action tubelet detector for weakly-supervised action detection",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110704",
    "abstract": "We propose a novel framework for spatiotemporal action detection using only video-level class labels as weak supervision. Traditional fully-supervised approaches rely on labor-intensive manual annotation of bounding boxes for each frame. In contrast, collecting video-level class labels is significantly less tedious and more feasible compared to annotating frame-level sequences with bounding boxes. To address this challenge, we propose a discriminative action tubelet detector, called DAT-detector, designed to discern discriminative tubelets from action tubelet proposals (ATPs). Whereas the previous approaches have only focused on tubelet selection among the predefined object proposals, our DAT-detector prioritizes the generation of more precise action tubelets using regression and attention modules. Moreover, we introduce an ATP generation method that enhances the quality of tubelet proposals. Our approach achieves state-of-the-art performance on several benchmarks, and also demonstrates competitive performance even with fully-supervised approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324004552",
    "keywords": [
      "Action (physics)",
      "Annotation",
      "Artificial intelligence",
      "Bounding overwatch",
      "Class (philosophy)",
      "Computer science",
      "Detector",
      "Discriminative model",
      "Frame (networking)",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Selection (genetic algorithm)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Lee",
        "given_name": "Jiyoung"
      },
      {
        "surname": "Kim",
        "given_name": "Seungryong"
      },
      {
        "surname": "Kim",
        "given_name": "Sunok"
      },
      {
        "surname": "Sohn",
        "given_name": "Kwanghoon"
      }
    ]
  },
  {
    "title": "DH-GAN: Image manipulation localization via a dual homology-aware generative adversarial network",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110658",
    "abstract": "Image manipulation localization is a binary segmentation task that sensitive to the tampered artifacts other than awareness of the object. Thus, both traditional and learning-based methods highly rely on hand-crafted features. However, these specifically-defined features limit the ability of the network for general scenes. To tackle this problem, we propose a dual homology-aware generative adversarial network (DH-GAN), a novel GAN-based framework to localize the manipulated region. Firstly, we localize the forgery region via re-calibrating the multi-scale encoded features with a selective pyramid generator. Then, we perform the homology identification in the discriminator. The proposed homology-aware discriminators contain a stack of masked convolution (MConv) layers and learn to identify the real/fake of the segmented pixels on the predicted/target masked image in a hard-gating manner. Overall, the networks are optimized under a standard GAN. Experiments show that the proposed method outperforms other state-of-the-art algorithms on four popular image manipulation datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324004096",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Biology",
      "Computational biology",
      "Computer science",
      "Computer vision",
      "Dual (grammatical number)",
      "Gene",
      "Generative adversarial network",
      "Generative grammar",
      "Genetics",
      "Homology (biology)",
      "Image (mathematics)",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Weihuang"
      },
      {
        "surname": "Cun",
        "given_name": "Xiaodong"
      },
      {
        "surname": "Pun",
        "given_name": "Chi-Man"
      }
    ]
  },
  {
    "title": "Evaluating brain group structure methods using hierarchical dynamic models",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110687",
    "abstract": "In graph theory, complex structures are studied, as well as the dynamics of the connectivity strength of this structure. However, in the estimation procedure, particular characteristics need to be considered at some level as informative when estimating the characteristics of a group. This work proposes a model that provides dynamic estimation of the network structure based on a model that makes it possible to incorporate hierarchy (individual information) in the process. In addition, we show the feasibility of modeling a complex structure by levels, exemplifying this by cluster analysis as a visualization of the embedding projection reduction space. Our case study is a neuroscience experiment, which needs to estimate the brain connectivity map, that is, to study the information flow of the brain in resting-stage subjects. Methods for estimating group networks can be grouped into the following 4 categories: group-structure (GS), virtual-typical-subject (VTS), common-structure (CS), and individual-structure (IS). These four group-structure estimation methods were compared in the context of the Multiregression Dynamic Models. Results showed that the proposed Bayesian Network Structure Dynamic estimation, using GS and hierarchical dynamic models, accommodates the latent/personal information in the estimation process by extracting the pattern shared between them. Moreover, the cluster analysis estimation corroborates the empirical results and expert judgments.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324004382",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Bayesian probability",
      "Biology",
      "Cluster analysis",
      "Computer science",
      "Context (archaeology)",
      "Data mining",
      "Dynamic Bayesian network",
      "Hierarchical clustering",
      "Machine learning",
      "Paleontology",
      "Projection (relational algebra)",
      "Visualization"
    ],
    "authors": [
      {
        "surname": "Costa",
        "given_name": "Lilia"
      },
      {
        "surname": "Anacleto",
        "given_name": "Osvaldo"
      },
      {
        "surname": "Nascimento",
        "given_name": "Diego C."
      },
      {
        "surname": "Smith",
        "given_name": "James Q."
      },
      {
        "surname": "Queen",
        "given_name": "Catriona M."
      },
      {
        "surname": "Louzada",
        "given_name": "Francisco"
      },
      {
        "surname": "Nichols",
        "given_name": "Thomas"
      }
    ]
  },
  {
    "title": "MOVES: Movable and moving LiDAR scene segmentation in label-free settings using static reconstruction",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110651",
    "abstract": "Accurate static structure reconstruction and segmentation of non-stationary objects is of vital importance for autonomous navigation applications. These applications assume a LiDAR scan to consist of only static structures. In the real world however, LiDAR scans consist of non-stationary dynamic structures — moving and movable objects. Current solutions use segmentation information to isolate and remove moving structures from LiDAR scan. This strategy fails in several important use-cases where segmentation information is not available. In such scenarios, moving objects and objects with high uncertainty in their motion i.e. movable objects, may escape detection. This violates the above assumption. We present MOVES , a novel GAN based adversarial model that segments out moving as well as movable objects in the absence of segmentation information. We achieve this by accurately transforming a dynamic LiDAR scan to its corresponding static scan. This is obtained by replacing dynamic objects and corresponding occlusions with static structures which were occluded by dynamic objects. We leverage corresponding static-dynamic LiDAR pairs. We design a novel discriminator, coupled with a contrastive loss on a smartly selected LiDAR scan triplet. For datasets lacking paired information, we propose MOVES-MMD that integrates Unsupervised Domain Adaptation into the network. We perform rigorous experiments to demonstrate state of the art dynamic to static translation performance on a sparse real world industrial dataset, an urban and a simulated dataset. MOVES also segments out movable and moving objects without using segmentation information. Without utilizing segmentation labels, MOVES performs better than segmentation based navigation baseline in highly dynamic and long LiDAR sequences. The code is available here.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324004023",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Geology",
      "Lidar",
      "Remote sensing",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Kumar",
        "given_name": "Prashant"
      },
      {
        "surname": "Makwana",
        "given_name": "Dhruv"
      },
      {
        "surname": "Susladkar",
        "given_name": "Onkar"
      },
      {
        "surname": "Mittal",
        "given_name": "Anurag"
      },
      {
        "surname": "Kalra",
        "given_name": "Prem Kumar"
      }
    ]
  },
  {
    "title": "A novel active contour model based on features for image segmentation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110673",
    "abstract": "Active contour model is an extraordinarily valuable technique in image segmentation, which is essential for image analysis and understanding. Active contour model has been widely studied because it delineates closed and smooth contours or surfaces of target objects. However, traditional active contour models underperform on complex natural images. To tackle this problem, we propose a novel active contour model framework, called FeaACM. We introduce the feature energy function into the conventional energy functional to minimize the energy functional to maintain the consistency of the object region and account for different distributions of objects and backgrounds in the feature space. To demonstrate the advantages of our method, we compare our method with the state-of-the-art methods, and show that our method achieves competitive performance. In addition, we utilize AutoEncoder technology to extract the feature of the image verifying the generality of our framework. Extensive and numerous experiments indicate that our method can segment complex natural images effectively. Our code is available at https://github.com/xuepeng1234/FeaACM.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324004242",
    "keywords": [
      "Active contour model",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Image (mathematics)",
      "Image segmentation",
      "Pattern recognition (psychology)",
      "Scale-space segmentation",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Xue",
        "given_name": "Peng"
      },
      {
        "surname": "Niu",
        "given_name": "Sijie"
      }
    ]
  },
  {
    "title": "Semantic attention-based heterogeneous feature aggregation network for image fusion",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110728",
    "abstract": "Infrared and visible image fusion aims to generate a comprehensive image that retains both salient targets of the infrared image and texture details of the visible image. However, existing methods overlook the differences in attention to semantic information among different fused images. To address this issue, we propose a semantic attention-based heterogeneous feature aggregation network for image fusion. The key component of our network is the semantic attention-based fusion module, which leverages the weights derived from semantic feature maps to dynamically adjust the significance of various semantic objects within the fusion feature maps. By using semantic weights as guidance, our fusion process concentrates on regions with crucial semantics, resulting in a more focused fusion that preserves rich semantic information. Moreover, we propose an innovative component called the attentive dense block. This block effectively filters out irrelevant features during extraction, accentuates essential features to their maximum potential, and enhances the visual quality of the fused images. Importantly, our network demonstrates strong generalization capabilities. Extensive experiments validate the superiority of our proposed network over current state-of-the-art techniques in terms of both visual appeal and semantics-driven evaluation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324004795",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Feature (linguistics)",
      "Fusion",
      "Image (mathematics)",
      "Linguistics",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Semantic feature"
    ],
    "authors": [
      {
        "surname": "Ruan",
        "given_name": "Zhiqiang"
      },
      {
        "surname": "Wan",
        "given_name": "Jie"
      },
      {
        "surname": "Xiao",
        "given_name": "Guobao"
      },
      {
        "surname": "Tang",
        "given_name": "Zhimin"
      },
      {
        "surname": "Ma",
        "given_name": "Jiayi"
      }
    ]
  },
  {
    "title": "D-Net: A dual-encoder network for image splicing forgery detection and localization",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110727",
    "abstract": "Many detection methods based on convolutional neural networks (CNNs) have been proposed for image splicing forgery detection. Most of these methods focus on validating local patches or local objects. We regard image splicing forgery detection as a binary classification task that distinguishes tampered and non-tampered regions by forensic fingerprints rather than semantic features. As the network goes deep, its representation ability becomes strong. However, the non-semantic forensic fingerprints can hardly be retained by normal CNNs in deep layers. We proposed a novel dual-encoder network (D-Net) for image splicing forgery detection to resolve these issues, employing an unfixed and a fixed encoder. The unfixed encoder autonomously learns the image fingerprints that differentiate between the tampered and non-tampered regions, whereas the fixed encoder intentionally provides structural information that assists the learning and detection of the forgeries. This dual-encoder is followed by a spatial pyramid global-feature extraction module that expands the global insight of D-Net for classifying the tampered and non-tampered regions more accurately. In an experimental comparison study of D-Net and state-of-the-art methods, D-Net, without pre-training or training on a large number of forgery images, outperformed the other methods in pixel-level forgery detection. Moreover, it is stably robust to different anti-forensic attacks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324004783",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer vision",
      "Dual (grammatical number)",
      "Encoder",
      "Gene",
      "Genetics",
      "Geometry",
      "Image (mathematics)",
      "Literature",
      "Mathematics",
      "Net (polyhedron)",
      "Operating system",
      "Pattern recognition (psychology)",
      "RNA",
      "RNA splicing"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Zonglin"
      },
      {
        "surname": "Liu",
        "given_name": "Bo"
      },
      {
        "surname": "Bi",
        "given_name": "Xiuli"
      },
      {
        "surname": "Xiao",
        "given_name": "Bin"
      },
      {
        "surname": "Li",
        "given_name": "Weisheng"
      },
      {
        "surname": "Wang",
        "given_name": "Guoyin"
      },
      {
        "surname": "Gao",
        "given_name": "Xinbo"
      }
    ]
  },
  {
    "title": "Pseudo-set Frequency Refinement architecture for fine-grained few-shot class-incremental learning",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110686",
    "abstract": "Few-shot class-incremental learning was introduced to solve the model adaptation problem for new incremental classes with only a few examples while still remaining effective for old data. Although recent state-of-the-art methods make some progress in improving system robustness on common datasets, they fail to work on fine-grained datasets where inter-class differences are small. The problem is mainly caused by: (1) the overlapping of new data and old data in the feature space during incremental learning, which means old samples can be falsely classified as newly introduced classes and induce catastrophic forgetting phenomena; (2) lacking discriminative feature learning ability to identify fine-grained objects. In this paper, a novel Pseudo-set Frequency Refinement (PFR) architecture is proposed to tackle these problems. We design a pseudo-set training strategy to mimic the incremental learning scenarios so that the model can better adapt to novel data in future incremental sessions. Furthermore, separate adaptation tasks are developed by utilizing frequency-based information to refine the original features and address the above challenging problems. More specifically, the high and low-frequency components of the images are employed to enrich the discriminative feature analysis ability and incremental learning ability of the model respectively. The refined features are used to perform inter-class and inter-set analyses. Extensive experiments show that the proposed method consistently outperforms the state-of-the-art methods on four fine-grained datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324004370",
    "keywords": [
      "Algorithm",
      "Architecture",
      "Art",
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Materials science",
      "Metallurgy",
      "Programming language",
      "Set (abstract data type)",
      "Shot (pellet)",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Pan",
        "given_name": "Zicheng"
      },
      {
        "surname": "Zhang",
        "given_name": "Weichuan"
      },
      {
        "surname": "Yu",
        "given_name": "Xiaohan"
      },
      {
        "surname": "Zhang",
        "given_name": "Miaohua"
      },
      {
        "surname": "Gao",
        "given_name": "Yongsheng"
      }
    ]
  },
  {
    "title": "Multi-granularity relationship reasoning network for high-fidelity 3D shape reconstruction",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110647",
    "abstract": "Monocular image-based 3D reconstruction is widely used in virtual reality, augmented reality, and autonomous driving, which benefits from the rapid development of deep learning approaches. Most of the available methods focused on reconstructing the overall shape of the object while ignoring some fine-grained details. Moreover, these methods make it hard to exactly reconstruct complex topological structures. In this paper, we propose a multi-granularity relationship reasoning network (MGRRNet), which aims to recover 3D shapes with high fidelity and rich details via the relationship reasoning between different granularity information. Specifically, our model captures the discriminative and detailed features at different granularities for extracting attentional regions. Then we perform the relationship reasoning between different granularities to reinforce the multi-granularity consistency and inter-granularity correlation. By doing this, our network is able to achieve robust feature representation and fine reconstruction. During the learning process, we jointly optimize procedures of different granularity feature representations via a sequence of inter-granularity cycle loss iterations. Extensive experimental results on two publicly available datasets justify that our approach achieves competitive performance compared to the state-of-the-art methods. Codes and all resources will be publicly available at https://github.com/Ray-tju/MGRRNet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324003984",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Electrical engineering",
      "Engineering",
      "Fidelity",
      "Granularity",
      "High fidelity",
      "Operating system",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Lei"
      },
      {
        "surname": "Zhou",
        "given_name": "Zhiyuan"
      },
      {
        "surname": "Wu",
        "given_name": "Suping"
      },
      {
        "surname": "Li",
        "given_name": "Pan"
      },
      {
        "surname": "Zhang",
        "given_name": "Boyang"
      }
    ]
  },
  {
    "title": "GLAN: A graph-based linear assignment network",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110694",
    "abstract": "Differentiable solvers for the linear assignment problem (LAP) have attracted much research attention in recent years, which are usually embedded into learning frameworks as components. However, previous algorithms, with or without learning strategies, usually suffer from the degradation of the optimality with the increment of the problem size. In this paper, we propose a learnable linear assignment solver based on deep graph networks. Specifically, we first transform the cost matrix to a bipartite graph and convert the assignment task to the problem of selecting reliable edges from the constructed graph. Subsequently, a deep graph network is developed to aggregate and update the features of nodes and edges. Finally, the network predicts a label for each edge that indicates the assignment relationship. The experimental results on a synthetic dataset reveal that our method outperforms state-of-the-art baselines and achieves consistently high accuracy with the increment of the problem size. Furthermore, we also embed the proposed solver, in comparison with state-of-the-art baseline solvers, into a popular multi-object tracking (MOT) framework to train the tracker in an end-to-end manner. The experimental results on MOT benchmarks illustrate that the proposed LAP solver improves the tracker by the largest margin.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032400445X",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Graph",
      "Mathematics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "He"
      },
      {
        "surname": "Wang",
        "given_name": "Tao"
      },
      {
        "surname": "Lang",
        "given_name": "Congyan"
      },
      {
        "surname": "Feng",
        "given_name": "Songhe"
      },
      {
        "surname": "Jin",
        "given_name": "Yi"
      },
      {
        "surname": "Li",
        "given_name": "Yidong"
      }
    ]
  },
  {
    "title": "GoogLeNet-AL: A fully automated adaptive model for lung cancer detection",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110657",
    "abstract": "As lung cancer has emerged as the top contributor to cancer-related fatalities, efficient and precise diagnostic methods are essential for efficient diagnosis. This research introduces a novel CNN architecture GoogLeNet with Adaptive Layers (GoogLeNet-AL) for lung cancer detection. The GoogLeNet-AL architecture integrates innovative features such as squeeze-and-excitation blocks, dilated convolutions, depthwise separable convolutions, group convolutions, non-local blocks, octave convolutions, inverted Residuals, and ghost convolutions in the inception layers to boost the potential of GoogLeNet-AL to capture multi-scale features efficiently. The GoogLeNet-AL model has been implemented in the PyTorch 1.8.1 platform and trained using publicly accessible datasets IQ-OTH/NCCD and Chest CT-Scan for comprehensive performance evaluation. Additionally, we employ data augmentation, stratified sampling, and fairness-aware training to enhance robustness and mitigate biases. The experimental assessment demonstrate that the GoogLeNet-AL method achieves an accuracy of 98.74 %, an F1-score of 98.96 %, and a precision of 99.74 % in lung cancer detection and also demonstrates its superior performance by outperforming traditional GoogLeNet and other baseline models. Overall, the proposed architecture enhanced the detection and categorization of lung nodules by reducing false positives and negatives, thus offering a valuable tool for combating lung cancer.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324004084",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Biology",
      "Computer science",
      "False positive paradox",
      "Gene",
      "Lung cancer",
      "Medicine",
      "Pathology",
      "Pattern recognition (psychology)",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Ma",
        "given_name": "Lei"
      },
      {
        "surname": "Wu",
        "given_name": "Huiqun"
      },
      {
        "surname": "Samundeeswari",
        "given_name": "P."
      }
    ]
  },
  {
    "title": "Graph domain adaptation with localized graph signal representations",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110628",
    "abstract": "In this paper we propose a domain adaptation algorithm designed for graph domains. Given a source graph with many labeled nodes and a target graph with few or no labeled nodes, we aim to estimate the target labels by making use of the similarity between the characteristics of the variation of the label functions on the two graphs. Our assumption about the source and the target domains is that the local behavior of the label function, such as its spread and speed of variation on the graph, bears resemblance between the two graphs. We estimate the unknown target labels by solving an optimization problem where the label information is transferred from the source graph to the target graph based on the prior that the projections of the label functions onto localized graph bases be similar between the source and the target graphs. In order to efficiently capture the local variation of the label functions on the graphs, spectral graph wavelets are used as the graph bases. Experimentation on various data sets shows that the proposed method yields quite satisfactory classification accuracy compared to reference domain adaptation methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324003790",
    "keywords": [
      "Algorithm",
      "Computer science",
      "Graph",
      "Line graph",
      "Mathematics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Pilavcı",
        "given_name": "Yusuf Yiğit"
      },
      {
        "surname": "Güneyi",
        "given_name": "Eylem Tuğçe"
      },
      {
        "surname": "Cengiz",
        "given_name": "Cemil"
      },
      {
        "surname": "Vural",
        "given_name": "Elif"
      }
    ]
  },
  {
    "title": "Dual space-based fuzzy graphs and orthogonal basis clustering for unsupervised feature selection",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110683",
    "abstract": "Unsupervised feature selection (UFS) takes an important position because gaining the class labels is laborious or even impossible. In the domain of UFS, clustering is a major means to exploit label information. The existing methods either could not model a clear clustering structure or could not utilize the local structure of data. Consequently, a new clustering method in combination with graph learning is proposed in this work. Specifically, for clustering, orthogonal basis clustering is introduced, where orthogonal constraints are imposed on the cluster center matrix and the clustering matrix. The clustering indicator matrix is also imposed by a non-negative constraint. A clearer clustering structure and more independent clustering centers are obtained through these constraints. For local preservation, given that traditional graphs for keeping the local manifold are faced with the problem of imbalanced neighbors, the fuzzy graph is introduced to acquire a robust structure, which is applied to both data space and feature space. The topological structure in these spaces can be well maintained. For the choice of salient features, ℓ 2 , 0 -norm regularization is imposed on the projection matrix. The object function is solved alternately. Then, a feature selection algorithm is designed. Experiments are designed and performed on nine real-world data sets. The results attest to the effectiveness of the proposed algorithm compared with other relevant algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324004345",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Basis (linear algebra)",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Dual (grammatical number)",
      "Feature (linguistics)",
      "Feature selection",
      "Feature vector",
      "Fuzzy clustering",
      "Geometry",
      "Linguistics",
      "Literature",
      "Mathematics",
      "Orthogonal basis",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Selection (genetic algorithm)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Duanzhang"
      },
      {
        "surname": "Chen",
        "given_name": "Hongmei"
      },
      {
        "surname": "Mi",
        "given_name": "Yong"
      },
      {
        "surname": "Luo",
        "given_name": "Chuan"
      },
      {
        "surname": "Horng",
        "given_name": "Shi-Jinn"
      },
      {
        "surname": "Li",
        "given_name": "Tianrui"
      }
    ]
  },
  {
    "title": "Dynamic weighted knowledge distillation for brain tumor segmentation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110731",
    "abstract": "Automatic 3D MRI brain tumor segmentation holds a crucial position in the field of medical image analysis, contributing significantly to the clinical diagnosis and treatment of brain tumors. However, traditional 3D brain tumor segmentation methods often entail extensive parameters and computational demands, posing substantial challenges in model training and deployment. To overcome these challenges, this paper introduces a brain tumor segmentation framework based on knowledge distillation. This framework includes training a lightweight network by extracting knowledge from a well-established brain tumor segmentation network. Firstly, this framework replaces the conventional static knowledge distillation (SKD) with the proposed dynamic weighted knowledge distillation (DWKD). DWKD dynamically adjusts the distillation loss weights for each pixel based on the learning state of the student network. Secondly, to enhance the student network's generalization capability, this paper customizes a loss function for DWKD, known as regularized cross-entropy (RCE). RCE introduces controlled noise into the model, enhancing its robustness and diminishing the risk of overfitting. This controlled injection of noise aids in fortifying the model's robustness. Lastly, Empirical validation of the proposed methodology is conducted using two distinct backbone networks, namely Attention U-Net and Residual U-Net. Rigorous experimentation is executed across the BraTS 2019, BraTS 2020, and BraTS 2021 datasets. Experimental results demonstrate that DWKD exhibits significant advantages over SKD in enhancing the segmentation performance of the student network. Furthermore, when dealing with limited training data, the RCE method can further improve the student network's segmentation performance. Additionally, this paper quantitatively analyzes the number of concept detectors identified in network dissection. It assesses the impact of DWKD on model interpretability and finds that compared to SKD, DWKD can more effectively enhance model interpretability. The source code is available at https://github.com/YuBinLab-QUST/DWKD/.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324004825",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Distillation",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "An",
        "given_name": "Dianlong"
      },
      {
        "surname": "Liu",
        "given_name": "Panpan"
      },
      {
        "surname": "Feng",
        "given_name": "Yan"
      },
      {
        "surname": "Ding",
        "given_name": "Pengju"
      },
      {
        "surname": "Zhou",
        "given_name": "Weifeng"
      },
      {
        "surname": "Yu",
        "given_name": "Bin"
      }
    ]
  },
  {
    "title": "Discrete online cross-modal hashing with consistency preservation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110688",
    "abstract": "Online cross-modal hashing has attracted widespread attention with the rapid expansion of large-scale streaming data, which can reduce storage requirements and enhance efficiency for online cross-modal retrieval. However, despite promising progress, existing methods still suffer from defective accuracy in a way, primarily attributed to two issues: insufficient semantic information exploitation and mismatched training-retrieval process. To address these challenges, we propose a novel supervised hashing method with dual consistency preservation, called Discrete Online Cross-Modal Hashing (DOCMH). On the one hand, we design more informative continuous semantic labels and fine-grained similarity graphs to preserve semantic consistency across different streaming data chunks and modality representations. On the other hand, we propose an effective modality deviation calibration mechanism for preserving learning process consistency between the training and retrieval phases. Extensive experiments on three widely used benchmark datasets demonstrate the superior performance of the proposed DOCMH under various scenarios.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324004394",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Computer security",
      "Consistency (knowledge bases)",
      "Consistent hashing",
      "Double hashing",
      "Dynamic perfect hashing",
      "Hash function",
      "Hash table",
      "K-independent hashing",
      "Mathematics",
      "Modal",
      "Pattern recognition (psychology)",
      "Polymer chemistry",
      "Universal hashing"
    ],
    "authors": [
      {
        "surname": "Kang",
        "given_name": "Xiao"
      },
      {
        "surname": "Liu",
        "given_name": "Xingbo"
      },
      {
        "surname": "Xue",
        "given_name": "Wen"
      },
      {
        "surname": "Zhang",
        "given_name": "Xuening"
      },
      {
        "surname": "Nie",
        "given_name": "Xiushan"
      },
      {
        "surname": "Yin",
        "given_name": "Yilong"
      }
    ]
  },
  {
    "title": "Robust Self-expression Learning with Adaptive Noise Perception",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110695",
    "abstract": "Self-expression learning methods often obtain a coefficient matrix to measure the similarity between pairs of samples. However, directly using the raw data to represent each sample under the self-expression framework may not be ideal, as noise points are inevitably involved in the process of representing clean samples. To address this issue, this work proposes a novel self-expression model called robust Self-Expression learning with adaptive Noise Perception (SENP). SENP decomposes each sample into a clean part and a noisy part, and samples with large self-expression losses can be recognized as the noise points. A reliable coefficient matrix can then be learned by using only the clean points to reconstruct the clean part of each sample. By simultaneously detecting the noisy part of each sample and noise points, and adaptively mitigating their negative impacts, the representative ability of the generated coefficient matrix is improved. Moreover, inspired by the solution of non-negative matrix factorization (NMF), an effective algorithm is formed to optimize SENP. Extensive experiments on well-known benchmark datasets demonstrate the superiority of SENP compared to several state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324004461",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Expression (computer science)",
      "Image (mathematics)",
      "Neuroscience",
      "Noise (video)",
      "Pattern recognition (psychology)",
      "Perception",
      "Perceptual learning",
      "Programming language",
      "Psychology",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Yangbo"
      },
      {
        "surname": "Zhou",
        "given_name": "Jie"
      },
      {
        "surname": "Lu",
        "given_name": "Jianglin"
      },
      {
        "surname": "Wan",
        "given_name": "Jun"
      },
      {
        "surname": "Gao",
        "given_name": "Can"
      },
      {
        "surname": "Lin",
        "given_name": "Qingshui"
      }
    ]
  },
  {
    "title": "Many birds, one stone: Medical image segmentation with multiple partially labeled datasets",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110636",
    "abstract": "Medical image segmentation is fundamental in the field of medical image analysis and has wide clinical applications in disease diagnosis and surgical planning etc. Current prevalent solution is to train a deep network in a fully supervised way with a large-scale fully labeled dataset. However, due to the high labor cost and requirement on medical expertise, such dataset is always absent. Instead, there are multiple partially labeled datasets which are originally established for specific purposes. To make full use of these partially labeled datasets, we propose a novel partially supervised segmentation network, named PSSNet, which consists of a task-specific feature learning network followed by a cross-task attention module (xTA) to exploit task dependencies to enhance task-specific features. To solve the challenges raised by unlabeled classes and domain shift across datasets, we propose an adversarial self-training strategy. We conduct experiments on two medical image segmentation tasks. One is the fine-grained fundus image segmentation aiming to simultaneously segment four-class lesions, OD and OC, and vessels. Validation on seven datasets demonstrates that our PSSNet performs the best among three baselines and three state-of-the-arts. The other is the multiple abdominal organ segmentation in CT images. Our PSSNet is trained on three partially labeled datasets, i.e., LiTS, KiTS and Spleen. Validation on one fully labeled dataset, i.e., BTCV, demonstrates that our PSSNet achieves better performances than four state-of-the-arts. The code is publicly available at https://github.com/CVIU-CSU/PSSNet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032400387X",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Code (set theory)",
      "Computer science",
      "Deep learning",
      "Economics",
      "Feature (linguistics)",
      "Field (mathematics)",
      "Image (mathematics)",
      "Image segmentation",
      "Linguistics",
      "Machine learning",
      "Management",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Pure mathematics",
      "Segmentation",
      "Set (abstract data type)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Qing"
      },
      {
        "surname": "Zeng",
        "given_name": "Hailong"
      },
      {
        "surname": "Sun",
        "given_name": "Zhaodong"
      },
      {
        "surname": "Li",
        "given_name": "Xiaobai"
      },
      {
        "surname": "Zhao",
        "given_name": "Guoying"
      },
      {
        "surname": "Liang",
        "given_name": "Yixiong"
      }
    ]
  },
  {
    "title": "Beyond local patches: Preserving global–local interactions by enhancing self-attention via 3D point cloud tokenization",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110712",
    "abstract": "Transformer-based architectures have recently shown impressive performance on various point cloud understanding tasks such as 3D object shape classification and semantic segmentation. Particularly, this can be attributed to their self-attention mechanism, which has the ability to capture long-range dependencies. However, current methods have constrained it to operate in local patches due to its quadratic memory constraints. This hinders their generalization ability and scaling capacity due to the loss of non-locality in early layers. To tackle this issue, we propose a window-based transformer architecture that captures long-range dependencies while aggregating information in the local patches. We do this by interacting each window with a set of global point cloud tokens — a representative subset of the entire scene — and augmenting the local geometry through a 3D Histogram of Oriented Gradients (HOG) descriptor. Through a series of experiments on segmentation and classification tasks, we show that our model exceeds the state-of-the-art on S3DIS semantic segmentation (+1.67% mIoU), ShapeNetPart part segmentation (+1.03% instance mIoU) and performs competitively on ScanObjectNN 3D object classification. 1 1 The code and trained models shall be made publicly available.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324004631",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Linguistics",
      "Locality",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Point cloud",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Khan",
        "given_name": "M.Q."
      },
      {
        "surname": "Shahzad",
        "given_name": "M."
      },
      {
        "surname": "Khan",
        "given_name": "S.A."
      },
      {
        "surname": "Fraz",
        "given_name": "M.M."
      },
      {
        "surname": "Zhu",
        "given_name": "X.X."
      }
    ]
  },
  {
    "title": "CMIGNet: Cross-Modal Inverse Guidance Network for RGB-Depth salient object detection",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110693",
    "abstract": "Currently, the majority of RGB-Depth salient object detection (SOD) methods utilize the encoder–decoder architecture. However, they often fail to utilize the encoding and decoding features fully. This paper rethinks the differences and correlations between them and proposes the Cross-Modal Inverse Guidance Network (CMIGNet) for SOD. Specifically, a Multi-level Feature Guidance Enhancement (MFGE) module is integrated into every layer of the foundational network. It employs a high-level decoding feature to guide the low-level RGB and depth encoding features, facilitating the rapid identification of salient regions and noise removal. The dual-stream encoding features guided by the MFGE module are combined using the proposed Dual-Stream Interactive Fusion (DSIF) module. It could simultaneously reduce dependence on two modal features during the fusion process. Thus, the impact on the results can be reduced in complex scenes when one modality is absent or confusing. Finally, the edge information is supplemented using the proposed Edge Refinement Awareness (ERA) module to generate the final salient map. Comparisons on seven widely used and one latest challenging RGB-D datasets show that the performance of the proposed CMIGNet is highly competitive with the state-of-the-art RGB-Depth SOD models. Additionally, our model is lighter and faster.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324004448",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Geometry",
      "Inverse",
      "Materials science",
      "Mathematics",
      "Modal",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Polymer chemistry",
      "RGB color model",
      "Salient"
    ],
    "authors": [
      {
        "surname": "Zhu",
        "given_name": "Hegui"
      },
      {
        "surname": "Ni",
        "given_name": "Jia"
      },
      {
        "surname": "Yang",
        "given_name": "Xi"
      },
      {
        "surname": "Zhang",
        "given_name": "Libo"
      }
    ]
  },
  {
    "title": "Prompt-guided DETR with RoI-pruned masked attention for open-vocabulary object detection",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110648",
    "abstract": "Prompt-OVD is an efficient and effective DETR-based framework for open-vocabulary object detection that utilizes class embeddings from CLIP as prompts, guiding the Transformer decoder to detect objects in base and novel classes. Additionally, our RoI-pruned masked attention helps leverage the zero-shot classification ability of the Vision Transformer-based CLIP, resulting in improved detection performance at a minimal computational cost. Our experiments on the OV-COCO and OV-LVIS datasets demonstrate that Prompt-OVD achieves an impressive 21.2 times faster inference speed than the first end-to-end open-vocabulary detection method (OV-DETR), while also achieving higher APs than four two-stage methods operating within similar inference time ranges. We release the code at https://github.com/DISL-Lab/Prompt-OVD.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324003996",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Linguistics",
      "Natural language processing",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Region of interest",
      "Vocabulary"
    ],
    "authors": [
      {
        "surname": "Song",
        "given_name": "Hwanjun"
      },
      {
        "surname": "Bang",
        "given_name": "Jihwan"
      }
    ]
  },
  {
    "title": "A generalizable framework for low-rank tensor completion with numerical priors",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110678",
    "abstract": "Low-Rank Tensor Completion, a method which exploits the inherent structure of tensors, has been studied extensively as an effective approach to tensor completion. Whilst such methods attained great success, none have systematically considered exploiting the numerical priors of tensor elements. Ignoring numerical priors causes loss of important information regarding the data, and therefore prevents the algorithms from reaching optimal accuracy. Despite the existence of some individual works which consider ad hoc numerical priors for specific tasks, no generalizable frameworks for incorporating numerical priors have appeared. We present the Generalized CP Decomposition Tensor Completion (GCDTC) framework, the first generalizable framework for low-rank tensor completion that takes numerical priors of the data into account. We test GCDTC by further proposing the Smooth Poisson Tensor Completion (SPTC) algorithm, an instantiation of the GCDTC framework, whose performance exceeds current state-of-the-arts by considerable margins in the task of non-negative tensor completion, exemplifying GCDTC’s effectiveness. Our code is open-source.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324004291",
    "keywords": [
      "Algorithm",
      "Applied mathematics",
      "Artificial intelligence",
      "Bayesian probability",
      "Combinatorics",
      "Computer science",
      "Geometry",
      "Mathematical optimization",
      "Mathematics",
      "Prior probability",
      "Rank (graph theory)",
      "Tensor (intrinsic definition)"
    ],
    "authors": [
      {
        "surname": "Yuan",
        "given_name": "Shiran"
      },
      {
        "surname": "Huang",
        "given_name": "Kaizhu"
      }
    ]
  },
  {
    "title": "A multi-resolution self-supervised learning framework for semantic segmentation in histopathology",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110621",
    "abstract": "Modern whole slide imaging technique together with supervised deep learning approaches have been advancing the field of histopathology, enabling accurate analysis of tissues. These approaches use whole slide images (WSIs) at various resolutions, utilising low-resolution WSIs to identify regions of interest in the tissue and high-resolution for detailed analysis of cellular structures. Due to the labour-intensive process of annotating gigapixels WSIs, accurate analysis of WSIs remains challenging for supervised approaches. Self-supervised learning (SSL) has emerged as an approach to build efficient and robust models using unlabelled data. It has been successfully used to pre-train models to learn meaningful image features which are then fine-tuned with downstream tasks for improved performance compared to training models from scratch. Yet, existing SSL methods optimised for WSI are unable to leverage the multi-resolutions and instead, work only in an individual resolution neglecting the hierarchical structure of multi-resolution inputs. This limitation prevents from the effective utilisation of complementary information between different resolutions, hampering discriminative WSI representation learning. In this paper we propose a Multi-resolution SSL Framework for WSI semantic segmentation (MSF-WSI) that effectively learns histopathological features. Our MSF-WSI learns complementary information from multiple WSI resolutions during the pre-training stage; this contrasts with existing works that only learn between the resolutions at the fine-tuning stage. Our pre-training initialises the model with a comprehensive understanding of multi-resolution features which can lead to improved performance in the subsequent tasks. To achieve this, we introduced a novel Context-Target Fusion Module (CTFM) and a masked jigsaw pretext task to facilitate the learning of multi-resolution features. Additionally, we designed Dense SimSiam Learning (DSL) strategy to maximise the similarities of image features from early model layers to enable discriminative learned representations. We evaluated our method using three public datasets on breast and liver cancer segmentation tasks. Our experiment results demonstrated that our MSF-WSI surpassed the accuracy of other state-of-the-art methods in downstream fine-tuning and semi-supervised settings.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324003728",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Histopathology",
      "Image segmentation",
      "Machine learning",
      "Medicine",
      "Natural language processing",
      "Pathology",
      "Pattern recognition (psychology)",
      "Resolution (logic)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Hao"
      },
      {
        "surname": "Ahn",
        "given_name": "Euijoon"
      },
      {
        "surname": "Kim",
        "given_name": "Jinman"
      }
    ]
  },
  {
    "title": "Federated zero-shot learning with mid-level semantic knowledge transfer",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110824",
    "abstract": "Conventional centralized deep learning paradigms are not feasible when data from different sources cannot be shared due to data privacy or transmission limitation. To resolve this problem, federated learning has been introduced to transfer knowledge across multiple sources (clients) with non-shared data while optimizing a globally generalized central model (server). Existing federated learning paradigms mostly focus on transmitting image encoders that take instance-sensitive images as input, making them less generalizable and vulnerable to privacy inference attacks. In contrast, in this work, we consider transferring mid-level semantic knowledge (such as attribute) which is not sensitive to specific objects of interest and therefore is more privacy-preserving and general. To this end, we formulate a new Federated Zero-Shot Learning (FZSL) paradigm to learn mid-level semantic knowledge at multiple local clients with non-shared local data and cumulatively aggregate a globally generalized central model for deployment. To improve model discriminative ability, we explore semantic knowledge available from either a language or a vision-language foundation model in order to enrich the mid-level semantic space in FZSL. Extensive experiments on five zero-shot learning benchmark datasets validate the effectiveness of our approach for optimizing a generalizable federated learning model with mid-level semantic knowledge transfer.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324005752",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Computer security",
      "Discriminative model",
      "Encoder",
      "Geodesy",
      "Geography",
      "Inference",
      "Key (lock)",
      "Knowledge management",
      "Knowledge transfer",
      "Machine learning",
      "Natural language processing",
      "Operating system",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Sun",
        "given_name": "Shitong"
      },
      {
        "surname": "Si",
        "given_name": "Chenyang"
      },
      {
        "surname": "Wu",
        "given_name": "Guile"
      },
      {
        "surname": "Gong",
        "given_name": "Shaogang"
      }
    ]
  },
  {
    "title": "Sentiment analysis based on text information enhancement and multimodal feature fusion",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110847",
    "abstract": "Rapid advancements in multimedia technology have created explosive growth in sentiment data generated across various social media platforms. While previous research on sentiment analysis has shifted from analyzing single data types to incorporating multimodal data, current studies face certain limitations. These include overlooking the impact of redundant information within feature sequences of each modality, failing to account for the complementarity between modality data, and neglecting the varying significance of different modalities in conveying sentiments. This paper introduces a sentiment analysis framework designed for text information enhancement and multimodal feature fusion. The text modality is central to this framework, around which an attention mechanism augments emotional correlations between modalities. An expanded sentiment lexicon refines the representation of multimodal features, thus capturing emotional information more accurately. Experimental evaluations conducted on two standard datasets, CMU-MOSI and CMU-MOSEI, show that the accuracy of the proposed method in multimodal emotion recognition tasks reaches 85.7% and 85.8% respectively, at 1.6% and 1.8% higher than the baseline methods. Thus, it demonstrates robust regression and classification performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324005983",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Feature (linguistics)",
      "Fusion",
      "Information fusion",
      "Linguistics",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Sentiment analysis"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Zijun"
      },
      {
        "surname": "Cai",
        "given_name": "Li"
      },
      {
        "surname": "Yang",
        "given_name": "Wenjie"
      },
      {
        "surname": "Liu",
        "given_name": "Junhui"
      }
    ]
  },
  {
    "title": "Explainable time series anomaly detection using masked latent generative modeling",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110826",
    "abstract": "We present a novel time series anomaly detection method that achieves excellent detection accuracy while offering a superior level of explainability. Our proposed method, TimeVQVAE-AD, leverages masked generative modeling adapted from the cutting-edge time series generation method known as TimeVQVAE. The prior model is trained on the discrete latent space of a time–frequency domain. Notably, the dimensional semantics of the time–frequency domain are preserved in the latent space, enabling us to compute anomaly scores across different frequency bands, which provides a better insight into the detected anomalies. Additionally, the generative nature of the prior model allows for sampling likely normal states for detected anomalies, enhancing the explainability of the detected anomalies through counterfactuals. Our experimental evaluation on the UCR Time Series Anomaly archive demonstrates that TimeVQVAE-AD significantly surpasses the existing methods in terms of detection accuracy and explainability. We provide our implementation on GitHub: https://github.com/ML4ITS/TimeVQVAE-AnomalyDetection.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324005776",
    "keywords": [
      "Anomaly (physics)",
      "Anomaly detection",
      "Artificial intelligence",
      "Computer science",
      "Condensed matter physics",
      "Generative grammar",
      "Generative model",
      "Geology",
      "Machine learning",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Physics",
      "Series (stratigraphy)",
      "Time series"
    ],
    "authors": [
      {
        "surname": "Lee",
        "given_name": "Daesoo"
      },
      {
        "surname": "Malacarne",
        "given_name": "Sara"
      },
      {
        "surname": "Aune",
        "given_name": "Erlend"
      }
    ]
  },
  {
    "title": "Dehazing & Reasoning YOLO: Prior knowledge-guided network for object detection in foggy weather",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110756",
    "abstract": "Fast and accurate object detection in foggy weather is crucial for visual tasks such as autonomous driving and video surveillance. Existing methods typically preprocess images with enhancement techniques before the object detector, so that the real-time performance of object detection decreases to some extent. Meanwhile, many popular object detection models rely solely on visual features for localization and classification. When fog is present, visual features would be so adversely impacted that the detection accuracy sharply decreases. Therefore, we propose an end-to-end prior knowledge-guided network called DR-YOLO for object detection in foggy weather. DR-YOLO integrates the atmospheric scattering model and the co-occurrence relation graph as prior knowledge into the entire training process of the detector. Firstly, Restoration Subnet Module (RSM) is designed to employ the atmospheric scattering model to guide the learning direction of the detector for dehazing features. Specifically, it is only adopted during the training process and does not increase the time cost of detection process. Secondly, for guiding the detector to pay more attention to potential co-occurring objects in the same scene, we introduce Relation Reasoning Attention Module (RRAM) that utilizes the co-occurrence relation graph to supplement deficient visual features in foggy weather. In addition, DR-YOLO employs Adaptive Feature Fusion Module (AFFM) to effectively merge the key features from the backbone and neck for the needs of RRAM and RSM. Finally, we conduct experiments on clear, synthetic and real-world foggy datasets to demonstrate the effectiveness of DR-YOLO. The source code is available at https://github.com/wenxinss/DR-YOLO.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324005077",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Geography",
      "Meteorology",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Weather prediction"
    ],
    "authors": [
      {
        "surname": "Zhong",
        "given_name": "Fujin"
      },
      {
        "surname": "Shen",
        "given_name": "Wenxin"
      },
      {
        "surname": "Yu",
        "given_name": "Hong"
      },
      {
        "surname": "Wang",
        "given_name": "Guoyin"
      },
      {
        "surname": "Hu",
        "given_name": "Jun"
      }
    ]
  },
  {
    "title": "Scalable compressive sampling network with progressive hierarchical subspace learning",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110769",
    "abstract": "Traditional compressive sampling does not sufficiently exploit the sparsity of signals to learn the sampling matrix adaptively. Moreover, they do not independently sample different frequency bands, which makes them ineffective in utilizing information from specific frequency bands. The existing deep learning-based compressive sensing methods achieve good performance with high model complexity, which limits their application to devices with low computing resources or small storage space. To address the above issues and improve the compressive sensing performance of natural images, we propose a novel scalable compressive sampling network with progressive hierarchical subspace learning (called SPHSL-CSNet) in an end-to-end mode. Specifically, the progressive hierarchical sampling strategy based on a three-level wavelet transform is presented, achieving band-separated sampling by extracting the low frequency, low-medium frequency, low-mid-second high frequency and the whole wavelet frequency band of the wavelet transform. This enables our model to obtain more image information with fewer sampling measurements and pay more attention to the reconstruction of texture details. The independent sampling of specific frequency bands is realized through the band-aware mask, which effectively reduces the parameter quantity of the sampling matrix and easier to deploy terminal devices in resource-limited scenarios. Extensive experiments on widely used benchmark datasets not only demonstrate that the proposed SPHSL-CSNet outperforms state-of-the-art performance under the premise of being lightweight, but also effective for the multispectral image compression. Furthermore, SPHSL-CSNet achieves excellent performance on antinoise performance with respect to the existing deep learning-based image CS method in most cases.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032400520X",
    "keywords": [
      "Artificial intelligence",
      "Compressed sensing",
      "Computer science",
      "Computer vision",
      "Database",
      "Filter (signal processing)",
      "Pattern recognition (psychology)",
      "Sampling (signal processing)",
      "Scalability",
      "Subspace topology"
    ],
    "authors": [
      {
        "surname": "Yin",
        "given_name": "Zhu"
      },
      {
        "surname": "Wu",
        "given_name": "Zhongcheng"
      },
      {
        "surname": "Shi",
        "given_name": "Wuzhen"
      }
    ]
  },
  {
    "title": "MS-TCRNet: Multi-Stage Temporal Convolutional Recurrent Networks for action segmentation using sensor-augmented kinematics",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110778",
    "abstract": "Action segmentation is a challenging task in high-level process analysis, typically performed on video or kinematic data obtained from various sensors. This work presents two contributions related to action segmentation on kinematic data. Firstly, we introduce two versions of Multi-Stage Temporal Convolutional Recurrent Networks (MS-TCRNet), specifically designed for kinematic data. The architectures consist of a prediction generator with intra-stage regularization and Bidirectional LSTM or GRU-based refinement stages. Secondly, we propose two new data augmentation techniques, World Frame Rotation and Hand Inversion, which utilize the strong geometric structure of kinematic data to improve algorithm performance and robustness. We evaluate our models on three datasets of surgical suturing tasks: the Variable Tissue Simulation (VTS) Dataset and the newly introduced Bowel Repair Simulation (BRS) Dataset, both of which are open surgery simulation datasets collected by us, as well as the JHU-ISI Gesture and Skill Assessment Working Set (JIGSAWS), a well-known benchmark in robotic surgery. Our methods achieved state-of-the-art performance. code: https://github.com/AdamGoldbraikh/MS-TCRNet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324005296",
    "keywords": [
      "Action (physics)",
      "Artificial intelligence",
      "Classical mechanics",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Geology",
      "Kinematics",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Segmentation",
      "Stage (stratigraphy)"
    ],
    "authors": [
      {
        "surname": "Goldbraikh",
        "given_name": "Adam"
      },
      {
        "surname": "Shubi",
        "given_name": "Omer"
      },
      {
        "surname": "Rubin",
        "given_name": "Or"
      },
      {
        "surname": "Pugh",
        "given_name": "Carla M."
      },
      {
        "surname": "Laufer",
        "given_name": "Shlomi"
      }
    ]
  },
  {
    "title": "Discovering an inference recipe for weakly-supervised object localization",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110838",
    "abstract": "Most existing weakly-supervised object localization (WSOL) methods have improved training procedures for better localization performance. However, the inference procedure has been overlooked. We observe that the useful information for localization is missed by the current inference practice of WSOL. To address this limitation, we propose a new test-time ingredient for WSOL: binarizing the penultimate feature map and their corresponding weights of the last linear layer. With this simple remedy, the proposed method consistently improves the localization performance of the existing training methods for WSOL. Extensive evaluation including with three different backbone networks on three different WSOL benchmarks validates its effectiveness. In addition, we demonstrate our method is also able to improve weakly-supervised semantic segmentation performances on PASCAL VOC dataset. Lastly, since our method is only applied during the testing phase, our performance gain comes with negligible computational overheads.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324005892",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Computer science",
      "Geography",
      "Inference",
      "Machine learning",
      "Natural language processing",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Recipe"
    ],
    "authors": [
      {
        "surname": "Lee",
        "given_name": "Sanghuk"
      },
      {
        "surname": "Mun",
        "given_name": "Cheolhyun"
      },
      {
        "surname": "Uh",
        "given_name": "Youngjung"
      },
      {
        "surname": "Choe",
        "given_name": "Junsuk"
      },
      {
        "surname": "Byun",
        "given_name": "Hyeran"
      }
    ]
  },
  {
    "title": "ReViT: Enhancing vision transformers feature diversity with attention residual connections",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110853",
    "abstract": "Vision Transformer (ViT) self-attention mechanism is characterized by feature collapse in deeper layers, resulting in the vanishing of low-level visual features. However, such features can be helpful to accurately represent and identify elements within an image and increase the accuracy and robustness of vision-based recognition systems. Following this rationale, we propose a novel residual attention learning method for improving ViT-based architectures, increasing their visual feature diversity and model robustness. In this way, the proposed network can capture and preserve significant low-level features, providing more details about the elements within the scene being analyzed. The effectiveness and robustness of the presented method are evaluated on five image classification benchmarks, including ImageNet1k, CIFAR10, CIFAR100, Oxford Flowers-102, and Oxford-IIIT Pet, achieving improved performances. Additionally, experiments on the COCO2017 dataset show that the devised approach discovers and incorporates semantic and spatial relationships for object detection and instance segmentation when implemented into spatial-aware transformer models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324006046",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Electrical engineering",
      "Engineering",
      "Feature (linguistics)",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Residual",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Diko",
        "given_name": "Anxhelo"
      },
      {
        "surname": "Avola",
        "given_name": "Danilo"
      },
      {
        "surname": "Cascio",
        "given_name": "Marco"
      },
      {
        "surname": "Cinque",
        "given_name": "Luigi"
      }
    ]
  },
  {
    "title": "Multi-threshold deep metric learning for facial expression recognition",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110711",
    "abstract": "Feature representations generated through triplet-based deep metric learning offer significant advantages for facial expression recognition (FER). Each threshold in triplet loss inherently shapes a distinct distribution of inter-class variations, leading to unique representations of expression features. Nonetheless, pinpointing the optimal threshold for triplet loss presents a formidable challenge, as the ideal threshold varies not only across different datasets but also among classes within the same dataset. In this paper, we propose a novel multi-threshold deep metric learning approach that bypasses the complex process of threshold validation and markedly improves the effectiveness in creating expression feature representations. Instead of choosing a single optimal threshold from a valid range, we comprehensively sample thresholds throughout this range, which ensures that the representation characteristics exhibited by the thresholds within this spectrum are fully captured and utilized for enhancing FER. Specifically, we segment the embedding layer of the deep metric learning network into multiple slices, with each slice representing a specific threshold sample. We subsequently train these embedding slices in an end-to-end fashion, applying triplet loss at its associated threshold to each slice, which results in a collection of unique expression features corresponding to each embedding slice. Moreover, we identify the issue that the traditional triplet loss may struggle to converge when employing the widely-used Batch Hard strategy for mining informative triplets, and introduce a novel loss termed dual triplet loss to address it. Extensive evaluations demonstrate the superior performance of the proposed approach on both posed and spontaneous facial expression datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032400462X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Engineering",
      "Facial expression",
      "Facial expression recognition",
      "Facial recognition system",
      "Metric (unit)",
      "Operations management",
      "Pattern recognition (psychology)",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Wenwu"
      },
      {
        "surname": "Yu",
        "given_name": "Jinyi"
      },
      {
        "surname": "Chen",
        "given_name": "Tuo"
      },
      {
        "surname": "Liu",
        "given_name": "Zhenguang"
      },
      {
        "surname": "Wang",
        "given_name": "Xun"
      },
      {
        "surname": "Shen",
        "given_name": "Jianbing"
      }
    ]
  },
  {
    "title": "Source-free domain adaptation via dynamic pseudo labeling and Self-supervision",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110793",
    "abstract": "Recently, unsupervised domain adaptation (UDA) has attracted extensive interest in relieving the greedy requirement of vanilla deep learning for labeled data. It seeks for a solution to adapt the knowledge from a well-labeled training dataset (source domain) to another unlabeled target dataset (target domain). However, in some practical scenarios, the source domain data is inaccessible for a variety of reasons, and only a model trained on it can be provided, thus deriving a more challenging task, i.e., source-free unsupervised domain adaptation (SFUDA). Some pseudo labeling-based methods have been proposed to solve it by predicting pseudo labels for the unlabeled target domain data. Nevertheless, incorrectly designated pseudo labels will impose an adverse impact on the network adaptation. To alleviate this issue, we propose a dynamic confidence-based pseudo labeling strategy for SFUDA in this paper. Unlike those methods that first rigidly assign pseudo labels to all target domain data and then try to weaken the effect of incorrect pseudo labels in training, we proactively label the target samples with higher confidence in a dynamic manner. To further relieve the impact of incorrect pseudo labels, we harness the collaborative learning to constrain the consistency of the network and impose an additional soft supervision. Besides, we also investigate the possible problem brought by our labeling strategy, i.e., the neglect of wavering samples near the decision boundary, and solve it by injecting the self-supervised learning into our model. Experiments on three UDA benchmark datasets demonstrate the state-of-the-art performance of our proposed method. The code is publicly available at https://github.com/meowpass/DPLS.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324005442",
    "keywords": [
      "Adaptation (eye)",
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Domain (mathematical analysis)",
      "Domain adaptation",
      "Mathematical analysis",
      "Mathematics",
      "Neuroscience",
      "Psychology"
    ],
    "authors": [
      {
        "surname": "Ma",
        "given_name": "Qiankun"
      },
      {
        "surname": "Zeng",
        "given_name": "Jie"
      },
      {
        "surname": "Zhang",
        "given_name": "Jianjia"
      },
      {
        "surname": "Zu",
        "given_name": "Chen"
      },
      {
        "surname": "Wu",
        "given_name": "Xi"
      },
      {
        "surname": "Zhou",
        "given_name": "Jiliu"
      },
      {
        "surname": "Chen",
        "given_name": "Jie"
      },
      {
        "surname": "Wang",
        "given_name": "Yan"
      }
    ]
  },
  {
    "title": "Hunt-inspired Transformer for visual object tracking",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110703",
    "abstract": "This paper presents a hunt-inspired Transformer for visual object tracking, dubbed as HuntFormer. The HuntFormer focuses on robust target detection and identification, simulating natural hunting processes. Specifically, the HuntFormer comprises two essential module designs including a predictor for detection and a verifier for identification. The predictor emulates the detection stage by designing a motion trajectory guided particle filter, which identifies potential target locations by predicting the motion state within a particle filtering framework. The predictor utilizes spatio-temporal correlation scores between dynamic target templates and the search region to guide the learning process to generate a set of reliable particles. This enables the base tracker to narrow its search range to focus on the target, and swiftly re-detect the target in case of model drift. Once the target is re-detected, the verifier assesses the detection result as a reliable tracked item. The verifier initially maintains a dynamic memory that stores reliable target templates and their corresponding locations in the motion trajectory. It then models the uncertainty of appearance information within this memory probabilistically. The output uncertainty score determines whether the memory gets updated or not. Ultimately, the predictor and the verifier collaborate, ensuring a robust tracking outcome. Extensive evaluations on six challenging benchmark datasets demonstrate HuntFormer’s favorable performance against various state-of-the-art trackers. Notably, in the VOT-LT2022 tracking challenge, the HuntFormer won the third place with an F-score of 0.598, closely competing for the second place with an F-score of 0.600.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324004540",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Electrical engineering",
      "Engineering",
      "Eye tracking",
      "Object (grammar)",
      "Transformer",
      "Video tracking",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Zhibin"
      },
      {
        "surname": "Xue",
        "given_name": "Wanli"
      },
      {
        "surname": "Zhou",
        "given_name": "Yuxi"
      },
      {
        "surname": "Zhang",
        "given_name": "Kaihua"
      },
      {
        "surname": "Chen",
        "given_name": "Shengyong"
      }
    ]
  },
  {
    "title": "A sparse transformer generation network for brain imaging genetic association",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110845",
    "abstract": "Brain imaging and genetic data are commonly utilized to investigate brain aging and diseases, particularly Alzheimer’s Disease (AD). Imaging genetics analyze the associations between neuroimaging and genetic data to reveal potential pathological mechanisms for more accurate diagnosis. Many existing methods have limitations in performing fine-grained association analysis between genetic data and phenotypic features extracted from predefined regions of interest in imaging data. To address this issue, this paper proposes a sparse transformer association analysis (STAA) framework that integrates phenotype and genotype feature extraction, identification, and association analysis into a unified model. A key component of the framework is a cross-modal generation network that connects genetic variants with imaging data, enhancing the understanding of the genetic associations underlying imaging patterns in AD and brain aging. Validated using the simulated data and real ADNI dataset, STAA shows superior performance in age regression and AD diagnosis, identifying key genetic and imaging biomarkers and providing association analysis of genetic imaging expression patterns at the voxel level.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032400596X",
    "keywords": [
      "Artificial intelligence",
      "Association (psychology)",
      "Computer science",
      "Pattern recognition (psychology)",
      "Psychology",
      "Psychotherapist"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Hongrui"
      },
      {
        "surname": "Gui",
        "given_name": "Yuanyuan"
      },
      {
        "surname": "Lu",
        "given_name": "Hui"
      },
      {
        "surname": "Liu",
        "given_name": "Manhua"
      }
    ]
  },
  {
    "title": "Poisson tensor completion with transformed correlated total variation regularization",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110735",
    "abstract": "Tensor completion involves recovering the underlying tensor from partial observations, and in this paper we focus on the point that these observations obey the Poisson distribution. To contend with this problem, we adopt a popular method that minimizes the sum of the data-fitting term and the regularization term under a uniform sampling mechanism. Specifically, we consider the negative logarithmic maximum likelihood estimate of the Poisson distribution as the data-fitting term. To effectively characterize the intrinsic structure of the tensor data, we propose a parameter-free regularization term that can simultaneously capture the low rankness and local smoothness of the underlying tensor. Here, the transformed tensor nuclear norm is used to explore the low rankness under suitable unitary transformations. We present theoretical derivations to demonstrate the feasibility of the proposed model. Furthermore, we develop an algorithm based on the alternating direction multiplier method (ADMM) to efficiently solve the proposed optimization problem, with its overall convergence being established. A series of numerical experiments show that proposed model yields a pleasing accuracy over several state-of-the-art models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324004862",
    "keywords": [
      "Algorithm",
      "Applied mathematics",
      "Artificial intelligence",
      "Astrophysics",
      "Computer science",
      "Image (mathematics)",
      "Mathematics",
      "Physics",
      "Poisson distribution",
      "Pure mathematics",
      "Regularization (linguistics)",
      "Statistics",
      "Tensor (intrinsic definition)",
      "Total variation denoising",
      "Variation (astronomy)"
    ],
    "authors": [
      {
        "surname": "Feng",
        "given_name": "Qingrong"
      },
      {
        "surname": "Hou",
        "given_name": "Jingyao"
      },
      {
        "surname": "Kong",
        "given_name": "Weichao"
      },
      {
        "surname": "Xu",
        "given_name": "Chen"
      },
      {
        "surname": "Wang",
        "given_name": "Jianjun"
      }
    ]
  },
  {
    "title": "Back-to-Bones: Rediscovering the role of backbones in domain generalization",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110762",
    "abstract": "Domain Generalization (DG) studies the capability of a deep learning model to generalize to out-of-training distributions. In the last decade, literature has been massively filled with training methodologies that claim to obtain more abstract and robust data representations to tackle domain shifts. Recent research has provided a reproducible benchmark for DG, pointing out the effectiveness of naive empirical risk minimization (ERM) over existing algorithms. Nevertheless, researchers persist in using the same outdated feature extractors, and little to no attention has been given to the effects of different backbones yet. In this paper, we go “back to the backbones”, proposing a comprehensive analysis of their intrinsic generalization capabilities, which so far have been overlooked by the research community. We evaluate a wide variety of feature extractors, from standard residual solutions to transformer-based architectures, finding an evident linear correlation between large-scale single-domain classification accuracy and DG capability. Our extensive experimentation shows that by adopting competitive backbones in conjunction with effective data augmentation, plain ERM outperforms recent DG solutions and achieves state-of-the-art accuracy. Moreover, our additional qualitative studies reveal that novel backbones give more similar representations to same-class samples, separating different domains in the feature space. This boost in generalization capabilities leaves marginal room for DG algorithms. It suggests a new paradigm for investigating the problem, placing backbones in the spotlight and encouraging the development of consistent algorithms on top of them. The code is available at https://github.com/PIC4SeR/Back-to-Bones.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324005132",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Domain (mathematical analysis)",
      "Epistemology",
      "Generalization",
      "Mathematical analysis",
      "Mathematics",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Angarano",
        "given_name": "Simone"
      },
      {
        "surname": "Martini",
        "given_name": "Mauro"
      },
      {
        "surname": "Salvetti",
        "given_name": "Francesco"
      },
      {
        "surname": "Mazzia",
        "given_name": "Vittorio"
      },
      {
        "surname": "Chiaberge",
        "given_name": "Marcello"
      }
    ]
  },
  {
    "title": "CL-TransFER: Collaborative learning based transformer for facial expression recognition with masked reconstruction",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110741",
    "abstract": "Facial expression recognition (FER) has attracted intensive attention due to its critical role in various computer vision tasks. However, existing FER approaches suffer from either noisy annotations or expression ambiguity (high inter-class and low intra-class similarity), limiting the FER performance. To this end, we propose a robust end-to-end collaborative learning based transformer for FER (CL-TransFER) in this paper. Specifically, CL-TransFER co-trains a CNN feature extractor and a transformer feature extractor jointly to extract both rich local semantic features as well as global structural information from facial images. By enforcing the consensus between the predictions of two extractors, the CL-TransFER could suppress the influence of noisy annotations. To further tackle the expression ambiguity problem, we design a simple yet efficient self-supervised masked reconstruction (SSMR) task to pre-train the transformer feature extractor of CL-TransFER. This enhances the model's capability of learning fine-grained discriminative representations. Extensive experiments on three popular benchmarks have demonstrated the effectiveness and superiority of our method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324004928",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Electrical engineering",
      "Engineering",
      "Facial expression",
      "Facial expression recognition",
      "Facial recognition system",
      "Pattern recognition (psychology)",
      "Speech recognition",
      "Transfer of learning",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Yujie"
      },
      {
        "surname": "Hu",
        "given_name": "Lin"
      },
      {
        "surname": "Zu",
        "given_name": "Chen"
      },
      {
        "surname": "Zhang",
        "given_name": "Jianjia"
      },
      {
        "surname": "Hou",
        "given_name": "Yun"
      },
      {
        "surname": "Chen",
        "given_name": "Ying"
      },
      {
        "surname": "Zhou",
        "given_name": "Jiliu"
      },
      {
        "surname": "Zhou",
        "given_name": "Luping"
      },
      {
        "surname": "Wang",
        "given_name": "Yan"
      }
    ]
  },
  {
    "title": "Few-shot classification with Fork Attention Adapter",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110805",
    "abstract": "Few-shot learning aims to transfer the knowledge learned from seen categories to unseen categories with a few references. It is also an essential challenge to bridge the gap between humans and deep learning models in real-world applications. Despite extensive previous efforts to tackle this problem by finding an appropriate similarity function, we emphasize that most existing methods have merely considered a single low-resolution representation pair utilized in similarity calculations between support and query samples. Such representational limitations could induce the instability of category predictions. To better achieve metric learning stabilities, we present a novel method dubbed Fork Attention Adapter (FA-adapter), which can seamlessly establish the dense feature similarity with the newly generated nuanced features. The utility of the proposed method is more performant and efficient via the two-stage training phase. Extensive experiments demonstrate consistent and substantial accuracy gains on the fine-grained CUB, Aircraft, non-fine-grained mini-ImageNet, and tiered-ImageNet benchmarks. By comprehensively studying and visualizing the learned knowledge from different source domains, we further present an extension version termed FA-adapter++ to boost the performance in fine-grained scenarios.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324005569",
    "keywords": [
      "Adapter (computing)",
      "Artificial intelligence",
      "Chemistry",
      "Computer hardware",
      "Computer science",
      "Fork (system call)",
      "Operating system",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Shot (pellet)"
    ],
    "authors": [
      {
        "surname": "Sun",
        "given_name": "Jieqi"
      },
      {
        "surname": "Li",
        "given_name": "Jian"
      }
    ]
  },
  {
    "title": "Denoising diffusion post-processing for low-light image enhancement",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110799",
    "abstract": "Low-light image enhancement (LLIE) techniques attempt to increase the visibility of images captured in low-light scenarios. However, as a result of enhancement, a variety of image degradations such as noise and color bias are revealed. Furthermore, each particular LLIE approach may introduce a different form of flaw within its enhanced results. To combat these image degradations, post-processing denoisers have widely been used, which often yield oversmoothed results lacking detail. We propose using a diffusion model as a post-processing approach, and we introduce Low-light Post-processing Diffusion Model (LPDM) in order to model the conditional distribution between under-exposed and normally-exposed images. We apply LPDM in a manner which avoids the computationally expensive generative reverse process of typical diffusion models, and post-process images in one pass through LPDM. Extensive experiments demonstrate that our approach outperforms competing post-processing denoisers by increasing the perceptual quality of enhanced low-light images on a variety of challenging low-light datasets. Source code is available at https://github.com/savvaki/LPDM.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324005508",
    "keywords": [
      "Anisotropic diffusion",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Diffusion",
      "Image (mathematics)",
      "Image denoising",
      "Image enhancement",
      "Image processing",
      "Multiview Video Coding",
      "Noise reduction",
      "Non-local means",
      "Pattern recognition (psychology)",
      "Physics",
      "Thermodynamics",
      "Video denoising",
      "Video processing",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Panagiotou",
        "given_name": "Savvas"
      },
      {
        "surname": "Bosman",
        "given_name": "Anna S."
      }
    ]
  },
  {
    "title": "Guided-attention and gated-aggregation network for medical image segmentation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110812",
    "abstract": "Recently, transformers have been widely used in medical image segmentation to capture long-range and global dependencies using self-attention. However, they often struggle to learn the local details which limit their ability to capture irregular shapes and sizes of the tissues and indistinct boundaries between the tissues, which are critical for accurate segmentation. To alleviate this issue, we propose a network named GA2Net, which comprises an encoder, a bottleneck, and a decoder. The encoder computes multi-scale features. In the bottleneck, we propose a hierarchical-gated features aggregation (HGFA) which introduces a novel spatial gating mechanism to enrich the multi-scale features. To effectively learn the shapes and sizes of the tissues, we apply deep supervision in the bottleneck. GA2Net proposes to use adaptive aggregation (AA) within the decoder, to adjust the receptive fields for each location in the feature map, by replacing the traditional concatenation/summation operations in skip connections in U-Net like architecture. Furthermore, we propose mask-guided feature attention (MGFA) modules within the decoder which strives to learn the salient features using foreground priors to adequately grasp the intricate structural and contour information of the tissues. We also apply intermediate supervision for each stage of the decoder, which further improves the capability of the model to better locate the boundaries of the tissues. Our extensive experimental results illustrate that our GA2-Net significantly outperforms the existing state-of-the-art methods over eight medical image segmentation datasets i.e., five polyps, a skin lesion, a multiple myeloma cell segmentation, and a cardiac MRI scan datasets. We then perform an extensive ablation study to validate the capabilities of our method. Code is available at https://github.com/mustansarfiaz/ga2net.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324005636",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Image (mathematics)",
      "Image segmentation",
      "Pattern recognition (psychology)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Fiaz",
        "given_name": "Mustansar"
      },
      {
        "surname": "Noman",
        "given_name": "Mubashir"
      },
      {
        "surname": "Cholakkal",
        "given_name": "Hisham"
      },
      {
        "surname": "Anwer",
        "given_name": "Rao Muhammad"
      },
      {
        "surname": "Hanna",
        "given_name": "Jacob"
      },
      {
        "surname": "Khan",
        "given_name": "Fahad Shahbaz"
      }
    ]
  },
  {
    "title": "Table representation learning using heterogeneous graph embedding",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110734",
    "abstract": "Tables, especially when having complex layouts, contain rich semantic information. However, effectively learning from tables to uncover such semantic information remains challenging. The rapid progress in natural language processing does not necessarily correspond to equivalent advancements in table parsing, which often requires joint visual and language modeling. Indeed, humans can quickly derive semantic meaning from table entries by associating them with corresponding column and/or row headers. Motivated by this observation, we propose a new heterogeneous Graph-based Table Representation Learning (GTRL) framework. GTRL combines graph-based visual modeling with sequence-based language modeling to learn granular per-cell embeddings that are sensitive to the semantic meaning of cells within their corresponding table context. We systematically evaluate the proposed GTRL framework using two datasets: a new adhesive table benchmark comprising complex tables extracted from industrial documents for learning per-entry semantics, and a publicly available large-scale dataset that enables learning header semantics from column tables. Experimental results demonstrate the competitive performance of the proposed GTRL, which often exhibits reduced computational complexity compared to state-of-the-art table representation learning models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324004850",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Embedding",
      "Graph",
      "Law",
      "Mathematics",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Table (database)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Tchuitcheu",
        "given_name": "Willy Carlos"
      },
      {
        "surname": "Lu",
        "given_name": "Tan"
      },
      {
        "surname": "Dooms",
        "given_name": "Ann"
      }
    ]
  },
  {
    "title": "Exposure difference network for low-light image enhancement",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110796",
    "abstract": "Low-light image enhancement aims to simultaneously improve the brightness and contrast of low-light images and recover the details of the visual content. This is a challenging task that makes typical data-driven methods suffer, especially when faced with severe information loss in extreme low-light conditions. In this work, we approach this task by proposing a novel exposure difference network. The proposed network generates a set of possible exposure corrections derived from the differences between synthesized images under different exposure levels, which are fused and adaptively combined with the raw input for light compensation. By modeling the intermediate exposure differences, our model effectively eliminates the redundancy existing in the synthesized data and offers the flexibility to handle image quality degradation resulting from varying levels of inadequate illumination. To further enhance the naturalness of the output image, we propose a global-aware color calibration module to derive low-frequency global information from inputs, which is further converted into a projection matrix to calibrate the RGB output. Extensive experiments show that our method can achieve competitive light enhancement performance both quantitatively and qualitatively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324005478",
    "keywords": [
      "Artificial intelligence",
      "Brightness",
      "Computer science",
      "Computer vision",
      "Image (mathematics)",
      "Image quality",
      "Luminance",
      "Naturalness",
      "Offset (computer science)",
      "Operating system",
      "Optics",
      "Physics",
      "Programming language",
      "Quantum mechanics",
      "RGB color model",
      "Redundancy (engineering)"
    ],
    "authors": [
      {
        "surname": "Jiang",
        "given_name": "Shengqin"
      },
      {
        "surname": "Mei",
        "given_name": "Yongyue"
      },
      {
        "surname": "Wang",
        "given_name": "Peng"
      },
      {
        "surname": "Liu",
        "given_name": "Qingshan"
      }
    ]
  },
  {
    "title": "Learning latent disentangled embeddings and graphs for multi-view clustering",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110839",
    "abstract": "Graph based methods have recently attracted much attention for multi-view clustering. Most existing methods seek the latent shared embeddings to learn a unified similarity graph or fuse multiple view-specific graphs to a consensus one for clustering, which may not sufficiently explore the common and complementary information among views. Besides, the high-order inter-view correlations are not fully investigated. To address these issues, this paper proposes a latent Disentangled Embeddings and GRaphs based multi-viEw clustEring (DEGREE) method, which considers the common and view-specific information in a latent subspace by explicit embedding disentanglement and multiple graphs learning. We assume that each view can be generated from a shared latent embedding and a corresponding view-specific embedding, which model the common information and exclusive complementary information among views, respectively. The intra-view and inter-view exclusivities among embeddings are encouraged by an orthogonality regularizer. To fully use the underlying information, we excavate the pairwise instance relations in both shared embedding and diverse view-specific embeddings by learning multiple graphs. Besides, a tensor singular value decomposition (t-SVD) based tensor nuclear norm regularizer is imposed on view-specific graphs, which helps to explore the high-order inter-view correlations. An alternative optimization algorithm is designed to solve the proposed model. Experimental evaluations on several popular datasets demonstrate that our DEGREE method outperforms the state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324005909",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Chao"
      },
      {
        "surname": "Chen",
        "given_name": "Haoxing"
      },
      {
        "surname": "Li",
        "given_name": "Huaxiong"
      },
      {
        "surname": "Chen",
        "given_name": "Chunlin"
      }
    ]
  },
  {
    "title": "Vision-language pre-training via modal interaction",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110809",
    "abstract": "Existing vision-language pre-training models typically extract region features and conduct fine-grained local alignment based on masked image/text completion or object detection methods. However, these models often design independent subtasks for different modalities, which may not adequately leverage interactions between modalities, requiring large datasets to achieve optimal performance. To address these limitations, this paper introduces a novel pre-training approach that facilitates fine-grained vision-language interaction. We propose two new subtasks — image filling and text filling — that utilize data from one modality to complete missing parts in another, enhancing the model’s ability to integrate multi-modal information. A selector mechanism is also developed to minimize semantic overlap between modalities, thereby improving the efficiency and effectiveness of the pre-trained model. Our comprehensive experimental results demonstrate that our approach not only fosters better semantic associations among different modalities but also achieves state-of-the-art performance on downstream vision-language tasks with significantly smaller datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324005600",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Geography",
      "Human–computer interaction",
      "Meteorology",
      "Modal",
      "Natural language processing",
      "Polymer chemistry",
      "Training (meteorology)"
    ],
    "authors": [
      {
        "surname": "Cheng",
        "given_name": "Hang"
      },
      {
        "surname": "Ye",
        "given_name": "Hehui"
      },
      {
        "surname": "Zhou",
        "given_name": "Xiaofei"
      },
      {
        "surname": "Liu",
        "given_name": "Ximeng"
      },
      {
        "surname": "Chen",
        "given_name": "Fei"
      },
      {
        "surname": "Wang",
        "given_name": "Meiqing"
      }
    ]
  },
  {
    "title": "Incorporating texture and silhouette for video-based person re-identification",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110759",
    "abstract": "Silhouette is an effective modality in video-based person re-identification (ReID) since it contains features (e.g., stature and gait) complementary to the RGB modality. However, recent silhouette-assisted methods have not fully explored the spatial–temporal relations within each modality or considered the cross-modal complementarity in fusion. To address these two issues, we propose a Complete Relational Framework that includes two key components. The first component, Spatial-Temporal Relational Module (STRM), explores the spatiotemporal relations. STRM decomposes the video’s spatiotemporal context into local/fine-grained and global/semantic aspects, modeling them sequentially to enhance the representation of each modality. The second component, Modality-Channel Relational Module (MCRM), explores the complementarity between RGB and silhouette videos. MCRM aligns two modalities semantically and multiplies them to capture complementary interrelations. With these two modules focusing on intra- and cross-modal relationships, our method achieves superior results across multiple benchmarks with minimal additional parameters and FLOPs. Code and models are available at https://github.com/baist/crf.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324005107",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Chemistry",
      "Complementarity (molecular biology)",
      "Component (thermodynamics)",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Genetics",
      "Law",
      "Modal",
      "Modality (human–computer interaction)",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Physics",
      "Political science",
      "Politics",
      "Polymer chemistry",
      "RGB color model",
      "Representation (politics)",
      "Silhouette",
      "Thermodynamics"
    ],
    "authors": [
      {
        "surname": "Bai",
        "given_name": "Shutao"
      },
      {
        "surname": "Chang",
        "given_name": "Hong"
      },
      {
        "surname": "Ma",
        "given_name": "Bingpeng"
      }
    ]
  },
  {
    "title": "ITFuse: An interactive transformer for infrared and visible image fusion",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110822",
    "abstract": "Infrared and visible image fusion (IVIF) has attracted increasing attention from the community because of its pleasing results in downstream applications. However, most existing deep fusion models are either feature–level fusion or image–level fusion, leading to information loss. In this paper, we propose an interactive transformer for IVIF, termed ITFuse. In contrast to previous algorithms, ITFuse consists of feature interactive modules (FIMs) and a feature reconstruction module (FRM) to alternatively extract and integrate important features. Specifically, to adequately exploit the common properties of different source images, we design a residual attention block (RAB) for mutual feature representation. To aggregate the distinct characteristics that existed in the corresponding input images, we leverage interactive attention (ITA) to incorporate the complementary information for comprehensive feature preservation and interaction. In addition, cross-modal attention (CMA) and transformer block (TRB) are presented to fully merge the capitalized features and construct long-range relationships. Furthermore, we devise a pixel loss and a structural loss to train the proposed deep fusion model in an unsupervised manner for excess performance amelioration. Massive experiments on popular databases illustrate that our ITFuse performs better than other representative state-of-the-art methods in terms of both qualitative and quantitative assessments. The source code of the proposed method is available at https://github.com/tthinking/ITFuse.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324005739",
    "keywords": [
      "Artificial intelligence",
      "Computer graphics (images)",
      "Computer science",
      "Computer vision",
      "Electrical engineering",
      "Engineering",
      "Fusion",
      "Infrared",
      "Linguistics",
      "Optics",
      "Philosophy",
      "Physics",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Tang",
        "given_name": "Wei"
      },
      {
        "surname": "He",
        "given_name": "Fazhi"
      },
      {
        "surname": "Liu",
        "given_name": "Yu"
      }
    ]
  },
  {
    "title": "Triadic temporal-semantic alignment for weakly-supervised video moment retrieval",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110819",
    "abstract": "Video Moment Retrieval (VMR) aims to identify specific event moments within untrimmed videos based on natural language queries. Existing VMR methods have been criticized for relying heavily on moment annotation bias rather than true multi-modal alignment reasoning. Weakly supervised VMR approaches inherently overcome this issue by training without precise temporal location information. However, they struggle with fine-grained semantic alignment and often yield multiple speculative predictions with prolonged video spans. In this paper, we take a step forward in the context of weakly supervised VMR by proposing a triadic temporal-semantic alignment model. Our proposed approach augments weak supervision by comprehensively addressing the multi-modal semantic alignment between query sentences and videos from both fine-grained and coarse-grained perspectives. To capture fine-grained cross-modal semantic correlations, we introduce a concept-aspect alignment strategy that leverages nouns to select relevant video clips. Additionally, an action-aspect alignment strategy with verbs is employed to capture temporal information. Furthermore, we propose an event-aspect alignment strategy that focuses on event information within coarse-grained video clips, thus mitigating the tendency towards long video span predictions during coarse-grained cross-modal semantic alignment. Extensive experiments conducted on the Charades-CD and ActivityNet-CD datasets demonstrate the superior performance of our proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324005703",
    "keywords": [
      "Artificial intelligence",
      "Classical mechanics",
      "Computer science",
      "Computer vision",
      "Moment (physics)",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Physics"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Jin"
      },
      {
        "surname": "Xie",
        "given_name": "JiaLong"
      },
      {
        "surname": "Zhou",
        "given_name": "Fengyu"
      },
      {
        "surname": "He",
        "given_name": "Shengfeng"
      }
    ]
  },
  {
    "title": "Hierarchical mixture of discriminative Generalized Dirichlet classifiers",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110789",
    "abstract": "This paper presents a discriminative classifier for compositional data. This classifier is based on the posterior distribution of the Generalized Dirichlet which is the discriminative counterpart of Generalized Dirichlet mixture model. Moreover, following the mixture of experts paradigm, we proposed a hierarchical mixture of this classifier. In order to learn the models parameters, we use a variational approximation by deriving an upper-bound for the Generalized Dirichlet mixture. To the best of our knownledge, this is the first time this bound is proposed in the literature. Experimental results are presented for spam detection and color space identification.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324005405",
    "keywords": [
      "Applied mathematics",
      "Artificial intelligence",
      "Boundary value problem",
      "Classifier (UML)",
      "Computer science",
      "Dirichlet distribution",
      "Discriminative model",
      "Latent Dirichlet allocation",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Mixture model",
      "Pattern recognition (psychology)",
      "Topic model",
      "Upper and lower bounds"
    ],
    "authors": [
      {
        "surname": "Togban",
        "given_name": "Elvis"
      },
      {
        "surname": "Ziou",
        "given_name": "Djemel"
      }
    ]
  },
  {
    "title": "Detect-order-construct: A tree construction based approach for hierarchical document structure analysis",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110836",
    "abstract": "Document structure analysis (aka document layout analysis) is crucial for understanding the physical layout and logical structure of documents, with applications in information retrieval, document summarization, knowledge extraction, etc. In this paper, we concentrate on Hierarchical Document Structure Analysis (HDSA) to explore hierarchical relationships within structured documents created using authoring software employing hierarchical schemas, such as LaTeX, Microsoft Word, and HTML. To comprehensively analyze hierarchical document structures, we propose a tree construction based approach that addresses multiple subtasks concurrently, including page object detection (Detect), reading order prediction of identified objects (Order), and the construction of intended hierarchical structure (Construct). We present an effective end-to-end solution based on this framework to demonstrate its performance. To assess our approach, we develop a comprehensive benchmark called Comp-HRDoc, which evaluates the above subtasks simultaneously. Our end-to-end system achieves state-of-the-art performance on two large-scale document layout analysis datasets (PubLayNet and DocLayNet), a high-quality hierarchical document structure reconstruction dataset (HRDoc), and our Comp-HRDoc benchmark. The Comp-HRDoc benchmark is publicly available at .",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324005879",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Binary tree",
      "Combinatorics",
      "Computer science",
      "Construct (python library)",
      "Data mining",
      "Economics",
      "Finance",
      "Mathematics",
      "Order (exchange)",
      "Pattern recognition (psychology)",
      "Programming language",
      "Tree (set theory)",
      "Tree structure"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Jiawei"
      },
      {
        "surname": "Hu",
        "given_name": "Kai"
      },
      {
        "surname": "Zhong",
        "given_name": "Zhuoyao"
      },
      {
        "surname": "Sun",
        "given_name": "Lei"
      },
      {
        "surname": "Huo",
        "given_name": "Qiang"
      }
    ]
  },
  {
    "title": "PPM: A boolean optimizer for data association in multi-view pedestrian detection",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110807",
    "abstract": "To accurately localize occluded people in a crowd is a challenging problem in video surveillance. Existing end-to-end deep multi-camera detectors rely heavily on pre-training with the same multiview datasets used for testing, which compromises their real-world applications. An alternative approach presented here is to project the torso lines of the instance segmentation masks from multiple views to the ground plane and propose pedestrian candidates at the intersection points. The candidate selection process is, for the first time, formulated as a logic minimization problem in Boolean algebra. A probabilistic Petrick’s method (PPM) is proposed to seek the minimum number of candidates to account for all the foreground masks while maximizing the joint occupancy likelihoods in multiple views. Experiments on benchmark video datasets have demonstrated the much improved performance of this approach in comparison with the benchmark deep or non-deep algorithms for multiview pedestrian detection.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324005582",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Association (psychology)",
      "Computer science",
      "Data association",
      "Data mining",
      "Geography",
      "Pattern recognition (psychology)",
      "Pedestrian",
      "Pedestrian detection",
      "Probabilistic logic",
      "Psychology",
      "Psychotherapist"
    ],
    "authors": [
      {
        "surname": "Qiu",
        "given_name": "Rui"
      },
      {
        "surname": "Xu",
        "given_name": "Ming"
      },
      {
        "surname": "Yan",
        "given_name": "Yuyao"
      },
      {
        "surname": "Smith",
        "given_name": "Jeremy S."
      },
      {
        "surname": "Ling",
        "given_name": "Yuchen"
      }
    ]
  },
  {
    "title": "A restarted large-scale spectral clustering with self-guiding and block diagonal representation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110746",
    "abstract": "Spectral clustering, a prominent unsupervised machine learning method, involves a critical task of constructing a similarity matrix. In existing approaches, this matrix is either computed once for all or updated alternatively. However, the former struggles to capture comprehensive relationships among data points, and the latter is often impractical for large-scale problems. This study introduces a new clustering framework with self-guidance and a block diagonal representation, retaining valuable information from previous cycles. To our knowledge, this is the first application of such a framework to spectral clustering, with a key distinction being the reclassification of samples in each cycle. To reduce computational overhead, we employ a block diagonal representation with Nyström approximation for constructing the similarity matrix. Theoretical results justify the rationality of approximate computations in spectral clustering. Comprehensive experiments on benchmark datasets demonstrate the superiority of our proposed algorithms over state-of-the-art methods for large-scale clustering. Notably, our framework has the potential to enhance clustering algorithms, performing well even with a randomly chosen initial guess.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324004977",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Block (permutation group theory)",
      "Block matrix",
      "Cluster analysis",
      "Computer science",
      "Correlation clustering",
      "Data mining",
      "Diagonal",
      "Eigenvalues and eigenvectors",
      "Geodesy",
      "Geography",
      "Geometry",
      "Image (mathematics)",
      "Law",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Political science",
      "Politics",
      "Quantum mechanics",
      "Representation (politics)",
      "Similarity (geometry)",
      "Spectral clustering"
    ],
    "authors": [
      {
        "surname": "Guo",
        "given_name": "Yongyan"
      },
      {
        "surname": "Wu",
        "given_name": "Gang"
      }
    ]
  },
  {
    "title": "Efficient neural implicit representation for 3D human reconstruction",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110758",
    "abstract": "High-fidelity digital human representations are increasingly in demand in the digital world, particularly for interactive telepresence, AR/VR, 3D graphics, and the rapidly evolving metaverse. Even though they work well in small spaces, conventional methods for reconstructing 3D human motion frequently require the use of expensive hardware and have high processing costs. This study presents HumanAvatar, an innovative approach that efficiently reconstructs precise human avatars from monocular video sources. At the core of our methodology, we integrate the pre-trained HuMoR, a model celebrated for its proficiency in human motion estimation. This is adeptly fused with the cutting-edge neural radiance field technology, Instant-NGP, and the state-of-the-art articulated model, Fast-SNARF, to enhance the reconstruction fidelity and speed. By combining these two technologies, a system is created that can render quickly and effectively while also providing estimation of human pose parameters that are unmatched in accuracy. We have enhanced our system with an advanced posture-sensitive space reduction technique, which optimally balances rendering quality with computational efficiency. In our detailed experimental analysis using both artificial and real-world monocular videos, we establish the advanced performance of our approach. HumanAvatar consistently equals or surpasses contemporary leading-edge reconstruction techniques in quality. Furthermore, it achieves these complex reconstructions in minutes, a fraction of the time typically required by existing methods. Our models achieve a training speed that is 110 × faster than that of State-of-The-Art (SoTA) NeRF-based models. Our technique performs noticeably better than SoTA dynamic human NeRF methods if given an identical runtime limit. HumanAvatar can provide effective visuals after only 30 s of training. Please visit https://github.com/HZXu-526/Human-Avatar for further demo results and code.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324005090",
    "keywords": [
      "Artificial intelligence",
      "Computer graphics",
      "Computer science",
      "Computer vision",
      "Electrical engineering",
      "Engineering",
      "Fidelity",
      "High fidelity",
      "Monocular",
      "Rendering (computer graphics)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Zexu"
      },
      {
        "surname": "Erfani",
        "given_name": "Sarah Monazam"
      },
      {
        "surname": "Lu",
        "given_name": "Siying"
      },
      {
        "surname": "Gong",
        "given_name": "Mingming"
      }
    ]
  },
  {
    "title": "LightCM-PNet: A lightweight pyramid network for real-time prostate segmentation in transrectal ultrasound",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110776",
    "abstract": "The accurate and real-time automatic segmentations of prostate transrectal ultrasound (TRUS) images enable the fusion of magnetic resonance imaging (MRI) and TRUS image to guide robotic prostate biopsy systems. This segmentation, applied to intraoperative TRUS images, is crucial for accurate image registration and automatic localization of the biopsy target. However, the blurred imaging and uneven intensity distribution in TRUS make accurate prostate segmentation still challenging. Most deep learning-based image segmentation methods, like convolutional neural network (CNN) and transformer, are often characterized by large parameters, computational complexity, and slow inference speed. Therefore, we propose a lightweight and accurate segmentation network. We combine CNN and tokenized multilayer perceptron (MLP) as the backbone of feature extraction, and build a pyramid structure network for feature encoding and decoding. This structure can effectively reduce the number of parameters and computational complexity, and effectively use global context information. Additionally, we introduce an interactive hybrid attention module that sequentially derives attention maps along the channel and spatial dimensions. The attention module interacts with the encoded to focus on the prostate region in the output features. We then incorporate a feature fusion module to construct reverse attention features, which enhances areas with weak responses in the foreground. To further improve the accuracy and smoothness of the segmentation result, we include a boundary constraint term in the loss function. Experimental results demonstrate that the proposed network achieves better performance in prostate TRUS image segmentation, with fewer parameters and faster inference speed.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324005272",
    "keywords": [
      "Artificial intelligence",
      "Cancer",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Deep learning",
      "Feature (linguistics)",
      "Feature extraction",
      "Image segmentation",
      "Internal medicine",
      "Linguistics",
      "Medicine",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Prostate",
      "Prostate biopsy",
      "Pyramid (geometry)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Weirong"
      },
      {
        "surname": "Pan",
        "given_name": "Bo"
      },
      {
        "surname": "Ai",
        "given_name": "Yue"
      },
      {
        "surname": "Li",
        "given_name": "Gonghui"
      },
      {
        "surname": "Fu",
        "given_name": "Yili"
      },
      {
        "surname": "Liu",
        "given_name": "Yanjie"
      }
    ]
  },
  {
    "title": "Auxiliary audio–textual modalities for better action recognition on vision-specific annotated videos",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110808",
    "abstract": "Most current audio–visual datasets are class-relevant, where audio and visual modalities are annotated. Thus, current audio–visual recognition methods apply cross-modality attention or modality fusion. However, leveraging the audio modality effectively in vision-specific videos for human activity recognition is of particular challenge. We address this challenge by proposing a novel audio–visual recognition framework that effectively leverages audio modality in any vision-specific annotated dataset. The proposed framework employs language models (e.g., GPT-3, CPT-text, BERT) for building a semantic audio–video label dictionary (SAVLD) that serves as a bridge between audio and video datasets by mapping each video label to its most K-relevant audio labels. Then, SAVLD along with a pre-trained audio multi-label model are used to estimate the audio–visual modality relevance. Accordingly, we propose a novel learnable irrelevant modality dropout (IMD) to completely drop the irrelevant audio modality and fuse only the relevant modalities. Finally, for the efficiency of the proposed multimodal framework, we present an efficient two-stream video Transformer to process the visual modalities (i.e., RGB frames and optical flow). The final predictions are re-ranked with GPT-3 recommendations of the human activity classes. GPT-3 provides high-level recommendations using the labels of the detected visual objects and the audio predictions of the input video. Our framework demonstrated a remarkable performance on the vision-specific annotated datasets Kinetics400 and UCF-101 by outperforming most relevant human activity recognition methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324005594",
    "keywords": [
      "Action (physics)",
      "Action recognition",
      "Artificial intelligence",
      "Audio visual",
      "Class (philosophy)",
      "Computer science",
      "Computer vision",
      "Modalities",
      "Modality (human–computer interaction)",
      "Multimedia",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Social science",
      "Sociology",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Alfasly",
        "given_name": "Saghir"
      },
      {
        "surname": "Lu",
        "given_name": "Jian"
      },
      {
        "surname": "Xu",
        "given_name": "Chen"
      },
      {
        "surname": "Li",
        "given_name": "Yu"
      },
      {
        "surname": "Zou",
        "given_name": "Yuru"
      }
    ]
  },
  {
    "title": "Shadow-aware decomposed transformer network for shadow detection and removal",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110771",
    "abstract": "Shadow detection and removal are important yet challenging computer vision tasks. Existing methods simultaneously contend with the brightness and color information of input image while treating different regions of input images equally. We argue that these operations fail to effectively extract the relationship between shadow and non-shadow regions. To relieve these problems, this paper proposes a shadow-aware decomposed transformer network for shadow detection and removal. The network decomposes the input image into brightness and color maps using its bright channel, which are concatenated with the original image as the input data, enabling the network to pay balanced attention to the brightness and color information when calculating the relationship between regions. Additionally, given that the correlation matrix of the transformer measures the relative dependency between regions, the proposed shadow-aware transformer block can extract the relationship between shadow and non-shadow regions more effectively by retaining the specific elements of the correlation matrix. We conduct extensive experiments on three shadow detection benchmark datasets and two shadow removal benchmark datasets. Experimental results show that the proposed method performs favorably against state-of-the-art methods. Codes have been made available at https://github.com/XIAOWANG914/SADT.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324005223",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Electrical engineering",
      "Engineering",
      "Pattern recognition (psychology)",
      "Psychology",
      "Psychotherapist",
      "Shadow (psychology)",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Xiao"
      },
      {
        "surname": "Yao",
        "given_name": "Siyuan"
      },
      {
        "surname": "Tang",
        "given_name": "Yong"
      },
      {
        "surname": "Yang",
        "given_name": "Sili"
      },
      {
        "surname": "Liu",
        "given_name": "Zhenbao"
      }
    ]
  },
  {
    "title": "Robust multilayer bootstrap networks in ensemble for unsupervised representation learning and clustering",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110739",
    "abstract": "It is known that unsupervised nonlinear learning is sensitive to the selection of hyperparameters, which hinders its practical use. How to determine the optimal hyperparameter setting that may be dramatically different across applications is a hard issue. In this paper, we aim to address this issue for multilayer bootstrap networks (MBN), a recent unsupervised model, in a way as simple as possible. Specifically, we first propose an MBN ensemble (MBN-E) algorithm which concatenates the sparse outputs of a set of MBN base models with different network structures into a new representation. Then, we take the new representation produced by MBN-E as a reference for selecting the optimal MBN base models. Moreover, we propose a fast version of MBN-E (fMBN-E), which is not only theoretically even faster than a single standard MBN but also does not increase the estimation error of MBN-E. Empirically, comparing to a number of advanced clustering methods, the proposed methods reach reasonable performance in their default settings. fMBN-E is empirically hundreds of times faster than MBN-E without suffering performance degradation. The applications to image segmentation and graph data mining further demonstrate the advantage of the proposed methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324004904",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Ensemble learning",
      "Feature learning",
      "Law",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Unsupervised learning"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Xiao-Lei"
      },
      {
        "surname": "Li",
        "given_name": "Xuelong"
      }
    ]
  },
  {
    "title": "Global contrast-masked autoencoders are powerful pathological representation learners",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110745",
    "abstract": "Using digital pathology slide scanning technology, artificial intelligence algorithms, particularly deep learning, have achieved significant results in the field of computational pathology. Compared to other medical images, pathology images are more difficult to annotate, and thus, there is an extreme lack of available datasets for conducting supervised learning to train robust deep learning models. In this paper, we introduce a self-supervised learning (SSL) model, the Global Contrast-masked Autoencoder (GCMAE), designed to train encoders to capture both local and global features of pathological images and significantly enhance the performance of transfer learning across datasets. Our study demonstrates the capability of the GCMAE to learn transferable representations through extensive experiments on three distinct disease-specific hematoxylin and eosin (H&E)-stained pathology datasets: Camelyon16, NCT-CRC, and BreakHis. Moreover, we propose an effective automated pathology diagnosis process based on the GCMAE for clinical applications. The source code of this paper is publicly available at https://github.com/StarUniversus/gcmae.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324004965",
    "keywords": [
      "Artificial intelligence",
      "Autoencoder",
      "Code (set theory)",
      "Computer science",
      "Contrast (vision)",
      "Deep learning",
      "Digital pathology",
      "Encoder",
      "Field (mathematics)",
      "Law",
      "Machine learning",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Programming language",
      "Pure mathematics",
      "Representation (politics)",
      "Set (abstract data type)",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Quan",
        "given_name": "Hao"
      },
      {
        "surname": "Li",
        "given_name": "Xingyu"
      },
      {
        "surname": "Chen",
        "given_name": "Weixing"
      },
      {
        "surname": "Bai",
        "given_name": "Qun"
      },
      {
        "surname": "Zou",
        "given_name": "Mingchen"
      },
      {
        "surname": "Yang",
        "given_name": "Ruijie"
      },
      {
        "surname": "Zheng",
        "given_name": "Tingting"
      },
      {
        "surname": "Qi",
        "given_name": "Ruiqun"
      },
      {
        "surname": "Gao",
        "given_name": "Xinghua"
      },
      {
        "surname": "Cui",
        "given_name": "Xiaoyu"
      }
    ]
  },
  {
    "title": "HTQ: Exploring the High-Dimensional Trade-Off of mixed-precision quantization",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110788",
    "abstract": "Mixed-precision quantization, where more sensitive layers are kept at higher precision, can achieve the trade-off between accuracy and complexity of neural networks. However, the search space for mixed-precision grows exponentially with the number of layers, making the brute force approach infeasible on deep networks. To reduce this exponential search space, recent efforts use Pareto frontier or integer linear programming to select the bit-precision of each layer. Unfortunately, we find that these prior works rely on a single constraint. In practice, model complexity includes space complexity and time complexity, and the two are weakly correlated, thus using simply one as a constraint leads to sub-optimal results. Besides this, they require manually set constraints, making them only pseudo-automatic. To address the above issues, we propose High-dimensional Trade-off Quantization (HTQ), which automatically determines the bit-precision in the high-dimensional space of model accuracy, space complexity, and time complexity without any manual intervention. Specifically, we use the saliency criterion based on connection sensitivity to indicate the accuracy perturbation after quantization, which performs similarly to Hessian information but can be calculated quickly (more than 1000 × speedup). The bit-precision is then automatically selected according to the three-dimensional (3D) Pareto frontier of the total perturbation, model size, and bit operations (BOPs) without manual constraints. Moreover, HTQ allows for the joint optimization of weights and activations, and thus the bit-precisions of both can be computed concurrently. Compared to state-of-the-art methods, HTQ achieves higher accuracy and lower space/time complexity on various model architectures for image classification and object detection tasks. Code is available at: https://github.com/zkkli/HTQ.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324005399",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Zhikai"
      },
      {
        "surname": "Long",
        "given_name": "Xianlei"
      },
      {
        "surname": "Xiao",
        "given_name": "Junrui"
      },
      {
        "surname": "Gu",
        "given_name": "Qingyi"
      }
    ]
  },
  {
    "title": "DM-GAN: CNN hybrid vits for training GANs under limited data",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110810",
    "abstract": "Generative adversarial network (GAN) training demands substantial data and computational resources. This paper aims to explore an economical approach for generating novel images with limited image data, addressing the challenge of data scarcity. Our contributions involve resolving the few-shot image generation challenge through the development of an unsupervised hybrid generative adversarial network named DM-GAN. We introduce a lightweight hybrid module (DC-Vit) comprising convolution and visual transformation, merging local and global features to enhance image perception, expressiveness, and ensure stable image generation. Additionally, a multi-scale adaptive skip connection module is incorporated to effectively mitigate the feature loss problem arising from inter-layer jumps, thereby producing more complete and regular images. To enhance the texture learning process and improve the quality and realism of synthesized images, we integrate the gray conjugate matrix into the loss function. Empirical evaluations are conducted on small sample datasets at various resolutions, including publicly accessible collections of art paintings, real-life photographs, and proprietary artifact image datasets. The experimental results unequivocally demonstrate the qualitative and quantitative superiority of our model over existing methods, underscoring its efficacy and robustness.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324005612",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Meteorology",
      "Pattern recognition (psychology)",
      "Physics",
      "Training (meteorology)",
      "Training set"
    ],
    "authors": [
      {
        "surname": "Yan",
        "given_name": "Longquan"
      },
      {
        "surname": "Yan",
        "given_name": "Ruixiang"
      },
      {
        "surname": "Chai",
        "given_name": "Bosong"
      },
      {
        "surname": "Geng",
        "given_name": "Guohua"
      },
      {
        "surname": "Zhou",
        "given_name": "Pengbo"
      },
      {
        "surname": "Gao",
        "given_name": "Jian"
      }
    ]
  },
  {
    "title": "Feature selections based on two-type overlap degrees and three-view granulation measures for k -nearest-neighbor rough sets",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110837",
    "abstract": "Feature selections facilitate data learning, and they effectively develop by combining the overlap degree and dependency degree related to k -nearest-neighbor (KNN) rough sets. However, the overlap degree neglects the fusion centrality while the dependency degree adopts only the algebraic perspective, so the corresponding feature selection has advance space. In this paper, the overlap degree is improved by fusion priority while the dependency degree is enriched by informational and dual viewpoints, so systematic feature selections are two-dimensionally established to generate multiple improved algorithms. At first, an improved type of overlap degrees is proposed by operationally exchanging the integration summation and fusion division, thus better motivating feature sorting. Then based on KNN granulation, informational, joint, conditional entropies are constructed, and they derive relative entropies by combining the dependency degree; corresponding size relationships, system equations, and granulation monotonicity are acquired. Furthermore, three-view granulation measures (i.e., the dependency degree, conditional entropy, relative conditional entropy) determine three-view attribute reducts based on feature significances; after pre-sorting deletion features based on two-type overlap degrees, 2 × 3 = 6 heuristic reduction algorithms are systematically established to extend recent algorithm OD&KNN. Finally, relevant uncertainty measures and feature selections are validated through data experiments, and five improved selection algorithms achieve better classification performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324005880",
    "keywords": [
      "Algorithm",
      "Biology",
      "Combinatorics",
      "Computer science",
      "Discrete mathematics",
      "Ecology",
      "Feature (linguistics)",
      "Linguistics",
      "Mathematics",
      "Philosophy",
      "Scalable Vector Graphics",
      "Type (biology)",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Jiang"
      },
      {
        "surname": "Zhang",
        "given_name": "Xianyong"
      },
      {
        "surname": "Yuan",
        "given_name": "Zhong"
      }
    ]
  },
  {
    "title": "Learning to match features with discriminative sparse graph neural network",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110784",
    "abstract": "We propose a cluster-based sparse graph network to improve the efficiency of image feature matching. This architecture clusters keypoints with high correlations into the same subgraphs, where each keypoint interacts only with others within the same subgraph. This strategy effectively reduces the spread of redundant messages and boosts the efficiency of message transmission. A unique coarse-to-fine paradigm is proposed for the incremental construction of sparse graphs, facilitating the evolution of subgraphs from coarse to fine, which enhances keypoint correlation and reduces misclassification. Additionally, the introduction of global tokens within each subgraph enables the learning of global information through interactions with a limited number of global tokens, further minimizing the impact of misclassification by broadening the scope of learning beyond the limits of individual subgraphs. The methodology demonstrates competitive performance in a range of vision tasks, including pose estimation, visual localization, and homography estimation. Compared to complete graph networks, it reduces time and memory consumption by 91% and 46%, respectively, during dense matching. Moreover, building on this foundational architecture, we introduce a novel hierarchical approach for visual localization, utilizing a two-stage sparse-to-dense matching process, achieves a substantial 31.8% decrease in time consumption while maintains competitive accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324005351",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Discriminative model",
      "Graph",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Shi",
        "given_name": "Yan"
      },
      {
        "surname": "Cai",
        "given_name": "Jun-Xiong"
      },
      {
        "surname": "Fan",
        "given_name": "Mingyu"
      },
      {
        "surname": "Feng",
        "given_name": "Wensen"
      },
      {
        "surname": "Zhang",
        "given_name": "Kai"
      }
    ]
  },
  {
    "title": "CFNet: An infrared and visible image compression fusion network",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110774",
    "abstract": "Image fusion aims to acquire a more complete image representation within a limited physical space to more effectively support practical vision applications. Although the currently popular infrared and visible image fusion algorithms take practical applications into consideration. However, they did not fully consider the redundancy and transmission efficiency of image data. To address this limitation, this paper proposes a compression fusion network for infrared and visible images based on joint CNN and Transformer, termed CFNet. First of all, the idea of variational autoencoder image compression is introduced into the image fusion framework, achieving data compression while maintaining image fusion quality and reducing redundancy. Moreover, a joint CNN and Transformer network structure is proposed, which comprehensively considers the local information extracted by CNN and the global long-distance dependencies emphasized by Transformer. Finally, multi-channel loss based on region of interest is used to guide network training. Not only can color visible and infrared images be fused directly but more bits can be allocated to the foreground region of interest, resulting in a superior compression ratio. Extensive qualitative and quantitative analyses affirm that the proposed compression fusion algorithm achieves state-of-the-art performance. In particular, rate–distortion performance experiments demonstrate the great advantages of the proposed algorithm for data storage and transmission. The source code is available at https://github.com/Xiaoxing0503/CFNet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324005259",
    "keywords": [
      "Artificial intelligence",
      "Composite material",
      "Compression (physics)",
      "Computer science",
      "Computer vision",
      "Fusion",
      "Image (mathematics)",
      "Image compression",
      "Image fusion",
      "Image processing",
      "Infrared",
      "Linguistics",
      "Materials science",
      "Optics",
      "Philosophy",
      "Physics"
    ],
    "authors": [
      {
        "surname": "Xing",
        "given_name": "Mengliang"
      },
      {
        "surname": "Liu",
        "given_name": "Gang"
      },
      {
        "surname": "Tang",
        "given_name": "Haojie"
      },
      {
        "surname": "Qian",
        "given_name": "Yao"
      },
      {
        "surname": "Zhang",
        "given_name": "Jun"
      }
    ]
  },
  {
    "title": "A Neuroinspired Contrast Mechanism enables Few-Shot Object Detection",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110766",
    "abstract": "Deep learning-based object detectors often demand abundant annotated data for training. However, in practice, only limited training data are available, making Few-Shot Object Detection (FSOD) an attractive research topic. Existing two-stage proposal-based Faster R-CNN detectors for FSOD struggle to match the performance of models trained on large datasets. We argue that detectors trained with limited samples cannot establish robust comparison-based relationships. Additionally, FSOD methods only explore these relationships during the training phase. To address these issues, we draw inspiration from neuroscience studies and propose Residual Contrast Faster R-CNN (RcFRCN). RcFRCN incorporates two novel customized contrast blocks: a Residual Spatial Contrast Block and a Residual Proposal Contrast Block. These blocks capture cross-spatial and cross-proposal contrast information, enhancing both training and testing phases. We conduct comprehensive experiments on two FSOD benchmarks: PASCAL VOC and MS-COCO. Our RcFRCN achieves a mAP of 21.9 under a 30-shot setting on MS-COCO. It also achieves AP scores of 69.1, 55.8, and 64.0 under 10-shot settings of different splits on PASCAL VOC, respectively. Moreover, we apply RcFRCN on remote sensing and use our contrast blocks for open-vocabulary detection. Experiment results on these tasks also demonstrate the robustness and generalization ability of our methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032400517X",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Contrast (vision)",
      "Mechanism (biology)",
      "Object (grammar)",
      "Object detection",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Shot (pellet)"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Lingxiao"
      },
      {
        "surname": "Chen",
        "given_name": "Dapeng"
      },
      {
        "surname": "Chen",
        "given_name": "Yifei"
      },
      {
        "surname": "Peng",
        "given_name": "Wei"
      },
      {
        "surname": "Xie",
        "given_name": "Xiaohua"
      }
    ]
  },
  {
    "title": "Category-agnostic semantic edge detection by measuring neural representation randomness",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110820",
    "abstract": "Edge detection plays a fundamental role in computer vision tasks and gains wide applications. In particular, semantic edge detection recently draws more attention due to the high demand for a fine-grained understanding of visual scenes. However, detecting high-level semantic edges hidden in visual scenes is quite challenging. Existing semantic edge detection methods focus on category-aware semantic edges and require elaborate category annotations. Instead, we first propose the category-agnostic semantic edge detection task without additional semantic category annotations. To achieve this goal, we propose to utilize only edge position annotations and leverage the information randomness of semantic edges. Specifically, we align semantic edge positions to the ground truth by maximizing randomness on edge regions and minimizing randomness on non-edge regions in the training process. In the inference process, we first obtain neural representations by the trained network, and then generate semantic edges by measuring neural randomness. We evaluate our method by comparisons with alternative methods on two well-known datasets: Cityscapes (Cordts et al., 2016) and SBD (Hariharan et al., 2014). The results demonstrate our superiority over the alternatives, which is more significant under weak annotations. We also provide comprehensive mechanism studies to verify the generalizability, rationality, and validity of our working mechanism.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324005715",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Edge detection",
      "Enhanced Data Rates for GSM Evolution",
      "Image (mathematics)",
      "Image processing",
      "Law",
      "Mathematics",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Randomness",
      "Representation (politics)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Pan",
        "given_name": "Zhiyi"
      },
      {
        "surname": "Jiang",
        "given_name": "Peng"
      },
      {
        "surname": "Zeng",
        "given_name": "Qiong"
      },
      {
        "surname": "Li",
        "given_name": "Ge"
      },
      {
        "surname": "Tu",
        "given_name": "Changhe"
      }
    ]
  },
  {
    "title": "Semantic segmentation for large-scale point clouds based on hybrid attention and dynamic fusion",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110798",
    "abstract": "This paper investigates the semantic segmentation problem for large-scale point clouds. Recent segmentation methods usually employ an encoder–decoder architecture. However, these methods may not effectively extract neighboring information in the encoder. Additionally, they typically use nearest neighbor interpolation and skip connections in the decoder, overlooking the semantic gap between encoder and decoder features. To resolve these issues, we propose HADF-Net, which consists of a Hybrid Attention Encoder (HAE), an Edge Dynamic Fusion module (EDF), and a Dynamic Cross-attention Decoder (DCD). HAE leverages the distinctive properties of geometric and semantic relations to aggregate local features at different stages. EDF aims to alleviate information loss during decoder upsampling by dynamically integrating the neighboring information. DCD employs an enhanced fusion mechanism with spatial-wise cross-attention to bridge the semantic gap between encoder and decoder features. Experimental results on 4 datasets demonstrate that our HADF-Net achieves superior performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324005491",
    "keywords": [
      "Artificial intelligence",
      "Cartography",
      "Computer science",
      "Computer vision",
      "Fusion",
      "Geography",
      "Geometry",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Point (geometry)",
      "Point cloud",
      "Scale (ratio)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Ce"
      },
      {
        "surname": "Shu",
        "given_name": "Zhaokun"
      },
      {
        "surname": "Shi",
        "given_name": "Li"
      },
      {
        "surname": "Ling",
        "given_name": "Qiang"
      }
    ]
  },
  {
    "title": "Meta-collaborative comparison for effective cross-domain few-shot learning",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110790",
    "abstract": "Recent advancements in cross-domain few-shot learning (CD-FSL) primarily focus on learning to compare global representations between query and support images for classification. However, due to the notorious cross-domain semantic gap, the ideal global representations can be totally different across domains, thereby solely learning to compare global representations is not sufficient to achieve effective generalization in challenging cases. To mitigate this problem, we present a Meta-collaborative Comparison Network (MeCo-Net) for CD-FSL, which imitates humans to recognize unfamiliar objects through collaborative comparison on both global and local representations. Following this idea, paralleling with a conventional global comparison branch, we additionally feed random crops of both query and support images into a feature encoder to separately extract their local representations. Subsequently, we associate these local representations across images through bipartite graph matching for local comparison. Thanks to the complementary global and local comparisons, we can obtain a more generalizable classifier for CD-FSL by meta-integrating them for final prediction. Experimental results on eight benchmarks demonstrate that the proposed model generalizes to multiple target domains with state-of-the-art performance without the need for fine-tuning.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324005417",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Domain (mathematical analysis)",
      "Machine learning",
      "Materials science",
      "Mathematical analysis",
      "Mathematics",
      "Metallurgy",
      "Shot (pellet)"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Fei"
      },
      {
        "surname": "Wang",
        "given_name": "Peng"
      },
      {
        "surname": "Zhang",
        "given_name": "Lei"
      },
      {
        "surname": "Wei",
        "given_name": "Wei"
      },
      {
        "surname": "Zhang",
        "given_name": "Yanning"
      }
    ]
  },
  {
    "title": "Boosted multilayer feedforward neural network with multiple output layers",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110740",
    "abstract": "This research introduces the Boosted Ensemble deep Multi-Layer Layer Perceptron (EdMLP) architecture with multiple output layers, a novel enhancement for the traditional Multi-Layer Perceptron (MLP). By adopting a layer-wise training approach, EdMLP enables the integration of boosting techniques within a single model, treating each layer as a weak learner, resulting in substantial performance gains. Additionally, the inclusion of layer-wise hyperparameter tuning allows optimization of individual layers thereby reducing the tuning time. Furthermore, the ensemble deep architecture’s versatility can be extended to other neural network-based models, such as the Self Normalized Network (SNN) where experiments demonstrate substantial performance enhancements yielded by the EdSNN compared to the standard original SNN model. This research underscores the potential of the EdMLP, and the Ed architecture in general as a powerful tool for improving the performance of various multilayer feedforward neural network models. The source code of this work is publicly accessible from the authors GitHub.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324004916",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Control engineering",
      "Engineering",
      "Feed forward",
      "Feedforward neural network",
      "Pattern recognition (psychology)",
      "Probabilistic neural network",
      "Time delay neural network"
    ],
    "authors": [
      {
        "surname": "Aly",
        "given_name": "Hussein"
      },
      {
        "surname": "Al-Ali",
        "given_name": "Abdulaziz K."
      },
      {
        "surname": "Suganthan",
        "given_name": "Ponnuthurai Nagaratnam"
      }
    ]
  },
  {
    "title": "Structured pruning adapters",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110724",
    "abstract": "Adapters are a parameter-efficient alternative to fine-tuning, which augment a frozen base network to learn new tasks. Yet, the inference of the adapted model is often slower than the corresponding fine-tuned model. To improve on this, we introduce the concept of Structured Pruning Adapters (SPAs), a family of compressing, task-switching network adapters, that accelerate and specialize networks using tiny parameter sets and structured pruning. Specifically, we propose the Structured Pruning Low-rank Adapter (SPLoRA) and the Structured Pruning Residual Adapter (SPPaRA) and evaluate them on a suite of pruning methods, architectures, and image recognition benchmarks. Compared to regular structured pruning with fine-tuning, SPLoRA improves image recognition accuracy by 6.9% on average for ResNet50 while using half the parameters at 90% pruned weights. Alternatively, a SPLoRA augmented model can learn adaptations with 17 × fewer parameters at 70% pruning with 1.6% lower accuracy. For ViT-b/16 models, SPLoRA improves accuracy by an average of 43%-points at 75% pruned weights while learning 6.8 × fewer parameters. Our experimental code and Python library of adapters are available at www.github.com/lukashedegaard/structured-pruning-adapters.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324004758",
    "keywords": [
      "Adapter (computing)",
      "Agronomy",
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Inference",
      "Machine learning",
      "Operating system",
      "Pattern recognition (psychology)",
      "Programming language",
      "Pruning",
      "Python (programming language)",
      "Residual",
      "Structured prediction"
    ],
    "authors": [
      {
        "surname": "Hedegaard",
        "given_name": "Lukas"
      },
      {
        "surname": "Alok",
        "given_name": "Aman"
      },
      {
        "surname": "Jose",
        "given_name": "Juby"
      },
      {
        "surname": "Iosifidis",
        "given_name": "Alexandros"
      }
    ]
  },
  {
    "title": "Text-guided distillation learning to diversify video embeddings for text-video retrieval",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110754",
    "abstract": "Conventional text-video retrieval methods typically match a video with a text on a one-to-one manner. However, a single video can contain diverse semantics, and text descriptions can vary significantly. Therefore, such methods fail to match a video with multiple texts simultaneously. In this paper, we propose a novel approach to tackle this one-to-many correspondence problem in text-video retrieval. We devise diverse temporal aggregation and a multi-key memory to address temporal and semantic diversity, consequently constructing multiple video embedding paths from a single video. Additionally, we introduce text-guided distillation learning that enables each video path to acquire meaningful distinct competencies in representing varied semantics. Our video embedding approach is text-agnostic, allowing the prepared video embeddings to be used continuously for any new text query. Experiments show our method outperforms existing methods on four datasets. We further validate the effectiveness of our designs with ablation studies and analyses on diverse video embeddings.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324005053",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Embedding",
      "Information retrieval",
      "Key (lock)",
      "Natural language processing",
      "Programming language",
      "Semantics (computer science)",
      "Video processing",
      "Video retrieval",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Lee",
        "given_name": "Sangmin"
      },
      {
        "surname": "Kim",
        "given_name": "Hyung-Il"
      },
      {
        "surname": "Ro",
        "given_name": "Yong Man"
      }
    ]
  },
  {
    "title": "High-frequency and low-frequency dual-channel graph attention network",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110795",
    "abstract": "Most existing graph convolution layers use learnable or fixed weights to sum up neighbor features to aggregate neighbor information. Since the attention values are always positive, these graph convolution layers perform as low-pass filters, which may result in their poor performance on heterophilic graphs. In this paper, two graph convolutional layers are proposed, HLGAT and NGAT. NGAT is a convolution network using only negative attention values, which only make the aggregation of high-frequency information of neighbor nodes. HLGAT makes the aggregation of low-frequency and high-frequency information by two channels, respectively, and fuses two outputs by using a learnable way. On node-classification task, both NGAT and HLGAT offer significant performance improvement compared to existing methods. The results clearly show that: (1) High-frequency information of neighborhoods plays a decisive role in heterophilic graphs. (2) The aggregation of low-frequency and high-frequency information of neighbor nodes can significantly improve the performance on heterophilic graphs.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324005466",
    "keywords": [
      "Algorithm",
      "Art",
      "Computer science",
      "Dual (grammatical number)",
      "Frequency assignment",
      "Graph",
      "Literature",
      "Low frequency",
      "Telecommunications",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Sun",
        "given_name": "Yukuan"
      },
      {
        "surname": "Duan",
        "given_name": "Yutai"
      },
      {
        "surname": "Ma",
        "given_name": "Haoran"
      },
      {
        "surname": "Li",
        "given_name": "Yuelong"
      },
      {
        "surname": "Wang",
        "given_name": "Jianming"
      }
    ]
  },
  {
    "title": "Flexible density peak clustering for real-world data",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110772",
    "abstract": "In density based clustering, the density peak algorithm has attracted much attention due to its effectiveness and simplicity, and a vast amount of clustering approaches have been proposed based on this algorithm. Some of these works require manual selection of cluster centers with a decision graph, where human involvement leads to uncertainty in clustering results. In order to avoid human involvement, some other algorithms depend on user-specified number of clusters to determine cluster centers automatically. However, it is well known that accurate estimation of number of clusters is a long-standing difficulty in data clustering. In this paper we present a sequential density peak clustering algorithm to extract clusters one by one, thereby determining the number of clusters automatically and avoiding manual selection of cluster centers in the meanwhile. Starting from a density peak, our algorithm generates an initial cluster surrounding the density peak in the first step, and then obtains the final cluster by expanding the initial cluster based on the relative density relationship among neighboring data points. With a peeling-off strategy, we obtain all the clusters sequentially. Our algorithm works well with clusters of Gaussian distribution and is therefore potential for clustering of real-world data. Experiments with a large number of synthetic and real datasets and comparisons with existing algorithms demonstrate the effectiveness of the proposed algorithm.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324005235",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Hou",
        "given_name": "Jian"
      },
      {
        "surname": "Lin",
        "given_name": "Houshen"
      },
      {
        "surname": "Yuan",
        "given_name": "Huaqiang"
      },
      {
        "surname": "Pelillo",
        "given_name": "Marcello"
      }
    ]
  },
  {
    "title": "Learning real-world heterogeneous noise models with a benchmark dataset",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110823",
    "abstract": "Noise modeling is an important research field in computer vision; the design of an accurate model for imaging sensor noise depends on not only a comprehensive benchmark dataset of the real world, but also a precise design of the noise modeling algorithm. However, due to the inaccurate estimation method of noise-free images and limited shooting scenes, the current realistic datasets could not describe the diverse noise properties sufficiently. Moreover, popular parametric noise models are not sophisticated enough to characterize the real-world noise exactly. In this work, we first construct a more comprehensive dataset of the real world by capturing more indoor and outdoor scenes under different lighting conditions using diverse smartphones, then we propose a non-parametric noise estimation method capable of modeling the spatial heterogeneity of real-world noise patterns. Specifically, in order to characterize the spatial heterogeneity of real-world noise, we assume a non-i.i.d Gaussian distribution and propose a deep convolutional neural network (DCNN)-based approach for learning pixel-wise noise variance maps. To learn the pixel-wise variance map, we have constructed a variance estimation network mapping from the conditional signals (clean image, ISO, and camera model) to surrogate labels obtained from the nonlocal search of similar patches from the clean-noisy image pair. Finally, we conducted denoising and classification experiments using different kinds of simulated noisy images, compared to the Poisson-Gaussian and Noise Flow noise models, the proposed method achieves denoising performance improvements (PSNR) of 1.13 dB and 2.51 dB respectively on the proposed real-world test dataset, denoising and classification results on the real noisy data captured by mobile phones have verified that our approach is more accurate than current noise modeling methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324005740",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Cartography",
      "Computer science",
      "Data mining",
      "Geography",
      "Image (mathematics)",
      "Machine learning",
      "Noise (video)",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Sun",
        "given_name": "Lu"
      },
      {
        "surname": "Lin",
        "given_name": "Jie"
      },
      {
        "surname": "Dong",
        "given_name": "Weisheng"
      },
      {
        "surname": "Li",
        "given_name": "Xin"
      },
      {
        "surname": "Wu",
        "given_name": "Jinjian"
      },
      {
        "surname": "Shi",
        "given_name": "Guangming"
      }
    ]
  },
  {
    "title": "LiDARCapV2: 3D human pose estimation with human–object interaction from LiDAR point clouds",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110848",
    "abstract": "Human–object interactions in open environments are common in the real world. Estimating 3D human pose from data where objects occlude the human is a challenging task in biometrics. However, existing LiDAR-based human motion capture datasets lack occlusion scenarios between humans and objects. To overcome this limitation, we propose LiDARHuman51M, a new human–object interaction dataset captured by LiDAR in long-range outdoor scene. It includes human motion labels acquired by an IMU system and synchronous RGB images. Additionally, we present an occlusion-aware method, LiDARCapV2, for capturing human motion from LiDAR point clouds under human–object interaction settings. Our key insight is to overcome object interference in human feature extraction by introducing a module called AgNoise-Segment. A noise augmentation strategy introduced in the AgNoise-Segment module alleviates the dependency of the segmentation accuracy on the effectiveness of 3D human pose estimations. Furthermore, we propose a skeleton extraction module that integrates features learned from the AgNoise-Segment module and predicts the skeleton locations. Quantitative and qualitative experiments demonstrate that LiDARCapV2 can capture high-quality 3D human motion under human–object interaction settings. Experiments on the KITTI and Waymo datasets demonstrate that our method can be generalized to real-world open scenarios.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324005995",
    "keywords": [
      "3D pose estimation",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Geography",
      "Geometry",
      "Lidar",
      "Mathematics",
      "Object (grammar)",
      "Point (geometry)",
      "Point cloud",
      "Pose",
      "Remote sensing"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Jingyi"
      },
      {
        "surname": "Mao",
        "given_name": "Qihong"
      },
      {
        "surname": "Shen",
        "given_name": "Siqi"
      },
      {
        "surname": "Wen",
        "given_name": "Chenglu"
      },
      {
        "surname": "Xu",
        "given_name": "Lan"
      },
      {
        "surname": "Wang",
        "given_name": "Cheng"
      }
    ]
  },
  {
    "title": "Discriminative atoms embedding relation dual network for classification of choroidal neovascularization in OCT images",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110757",
    "abstract": "Choroidal neovascularization (CNV) is an eye disease that can cause vision loss. Automatic CNV classification in OCT images is crucial in the treatment of CNV. However, two problems arise for CNV classification in OCT images. The subtle visual differences between different CNV types render classification difficult. Additionally, it is difficult to obtain sufficient labeled data, which results in performance degradation. In order to solve these two problems, a discriminative atom-embedding relation dual network is proposed in this paper. Considering that semi-supervised learning (SSL) is an effective machine learning framework to make full use of limited labeled data and a large amount of unlabeled data, the proposed network is developed within an SSL framework. To capture the visual differences, novel discriminative atoms are first introduced to mine discriminative information between different CNV types. Subsequently, a relation module is incorporated to embed the learned discriminative atom information into the features. This makes the learned features capable of distinguishing between different CNV types. Moreover, a novel relation consistency loss is proposed to further improve the robustness of the learned features. Experimental results on private and public datasets demonstrate the effectiveness of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324005089",
    "keywords": [
      "Artificial intelligence",
      "Choroidal neovascularization",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Discriminative model",
      "Dual (grammatical number)",
      "Embedding",
      "Linguistics",
      "Medicine",
      "Ophthalmology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Relation (database)",
      "Retinal"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Ruifeng"
      },
      {
        "surname": "Zhang",
        "given_name": "Guang"
      },
      {
        "surname": "Xi",
        "given_name": "Xiaoming"
      },
      {
        "surname": "Xu",
        "given_name": "Longsheng"
      },
      {
        "surname": "Nie",
        "given_name": "Xiushan"
      },
      {
        "surname": "Nie",
        "given_name": "Jianhua"
      },
      {
        "surname": "Meng",
        "given_name": "Xianjing"
      },
      {
        "surname": "Zhang",
        "given_name": "Yanwei"
      },
      {
        "surname": "Chen",
        "given_name": "Xinjian"
      },
      {
        "surname": "Yin",
        "given_name": "Yilong"
      }
    ]
  },
  {
    "title": "FMGNet: An efficient feature-multiplex group network for real-time vision task",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110698",
    "abstract": "Lightweight network design is crucial for optimizing speed and accuracy in computer vision tasks on mobile platforms with limited resources. Widely adopted models, such as EfficientNet and RegNet have achieved significant success by integrating key elements like Pointwise Convolutions (PWConvs) and Squeeze-and-Excitation (SE) blocks. However, a notable observation is that the output feature of the PWConv closely resembles its input, particularly in the absence of an activation function. This similarity and redundancy lead to wasted computational complexity and adversely affect the inference speed. To address these issues, we propose an efficient lightweight network called Efficient Feature-Multiplex Group Network (FMGNet). FMGNet is composed of two key components: the Cross-layer Feature-multiplex Group (CFG) block and the CFG-aligned Cross-layer Attention (CCA) block. The CFG block enables more compact feature learning with fewer parameters by multiplexing the input features of the PWConv. Meanwhile, the CCA block leverages the pre-modified features derived from the CFG block’s PWConv, allowing for simultaneous and parallel channel attention modeling. Our extensive experiments across various tasks, including image classification (ImageNet), object detection (PASCAL VOC), human pose estimation (MPII), person re-identification (Market-1501, DukeMTMC-ReID, CUHK03-NP), and semantic segmentation (Cityscapes), indicate that FMGNet achieves comparable performance to state-of-the-art lightweight convolutional neural networks, offering faster inference times. Remarkably, FMGNet even surpasses recent transformer-based models, such as SwiftFormer and EfficientFormerV2, achieving superior results with lower inference latency.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324004497",
    "keywords": [
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Feature (linguistics)",
      "Geometry",
      "Inference",
      "Linguistics",
      "Mathematics",
      "Object detection",
      "Pascal (unit)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pose",
      "Programming language"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Hao"
      },
      {
        "surname": "Ma",
        "given_name": "Yongqiang"
      },
      {
        "surname": "Zhang",
        "given_name": "Kaipeng"
      },
      {
        "surname": "Zheng",
        "given_name": "Nanning"
      },
      {
        "surname": "Lai",
        "given_name": "Shenqi"
      }
    ]
  },
  {
    "title": "HFGN: High-Frequency residual Feature Guided Network for fast MRI reconstruction",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110801",
    "abstract": "Magnetic Resonance Imaging (MRI) is a valuable medical imaging technology, while it suffers from a long acquisition time. Various methods have been proposed to reconstruct sharp images from undersampled k-space data to reduce imaging time. However, these methods hardly reconstruct high-quality aliasing-free Magnetic Resonance (MR) images with clear structures, especially in high-frequency components. To address this problem, we propose a High-Frequency residual feature Guided Network (HFGN) for fast MRI reconstruction. HFGN uses a sub-network, High-Frequency Extraction Network (HFEN), to learn the difference between the U-Net reconstruction result and the ground truth, then uses the learned features to guide the reconstruction of the network. In the reconstruction network, we propose Residual Channel and Spatial Attention block (RCSA), which uses frequency domain and image domain convolution branching to learn the global and local features of the image simultaneously. The experiment results under different acceleration rates on different datasets demonstrate that our proposed method surpasses the existing state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324005521",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Residual"
    ],
    "authors": [
      {
        "surname": "Fang",
        "given_name": "Faming"
      },
      {
        "surname": "Hu",
        "given_name": "Le"
      },
      {
        "surname": "Liu",
        "given_name": "Jinhao"
      },
      {
        "surname": "Yi",
        "given_name": "Qiaosi"
      },
      {
        "surname": "Zeng",
        "given_name": "Tieyong"
      },
      {
        "surname": "Zhang",
        "given_name": "Guixu"
      }
    ]
  },
  {
    "title": "Precise facial landmark detection by Dynamic Semantic Aggregation Transformer",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110827",
    "abstract": "At present, deep neural network methods have played a dominant role in face alignment field. However, they generally use predefined network structures to predict landmarks, which tends to learn general features and leads to mediocre performance, e.g., they perform well on neutral samples but struggle with faces exhibiting large poses or occlusions. Moreover, they cannot effectively deal with semantic gaps and ambiguities among features at different scales, which may hinder them from learning efficient features. To address the above issues, in this paper, we propose a Dynamic Semantic-Aggregation Transformer (DSAT) for more discriminative and representative feature (i.e., specialized feature) learning. Specifically, a Dynamic Semantic-Aware (DSA) model is first proposed to partition samples into subsets and activate the specific pathways for them by estimating the semantic correlations of feature channels, making it possible to learn specialized features from each subset. Then, a novel Dynamic Semantic Specialization (DSS) model is designed to mine the homogeneous information from features at different scales for eliminating the semantic gap and ambiguities and enhancing the representation ability. Finally, by integrating the DSA model and DSS model into our proposed DSAT in both dynamic architecture and dynamic parameter manners, more specialized features can be learned for achieving more precise face alignment. It is interesting to show that harder samples can be handled by activating more feature channels. Extensive experiments on popular face alignment datasets demonstrate that our proposed DSAT outperforms state-of-the-art models in the literature. Our code is available at https://github.com/GERMINO-LiuHe/DSAT.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324005788",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Electrical engineering",
      "Engineering",
      "Landmark",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Wan",
        "given_name": "Jun"
      },
      {
        "surname": "Liu",
        "given_name": "He"
      },
      {
        "surname": "Wu",
        "given_name": "Yujia"
      },
      {
        "surname": "Lai",
        "given_name": "Zhihui"
      },
      {
        "surname": "Min",
        "given_name": "Wenwen"
      },
      {
        "surname": "Liu",
        "given_name": "Jun"
      }
    ]
  },
  {
    "title": "Efficient and lightweight convolutional neural network architecture search methods for object classification",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110752",
    "abstract": "Determining the architecture of deep learning models is a complex task. Several automated search techniques have been proposed, but these methods typically require high-performance graphics processing units (GPUs), manual parameter adjustments, and specific training approaches. This study introduces an efficient, lightweight convolutional neural network architecture search approach tailored for object classification. It features an optimized search space design and a novel controller design. This study introduces a refined search space design incorporating optimizations in both spatial and operational aspects. The focus is on the synergistic integration of convolutional units, dimension reduction units, and the stacking of Convolutional Neural Network (CNN) architectures. To enhance the search space, ShuffleNet modules are integrated, reducing the number of parameters and training time. Additionally, BlurPool is implemented in the dimension reduction unit operation to achieve translational invariance, alleviate the gradient vanishing problem, and optimize unit compositions. Moreover, an innovative controller model, Stage LSTM, is proposed based on Long Short-Term Memory (LSTM) to generate lightweight architectural sequences. In conclusion, the refined search space design and the Stage LSTM controller model are synergistically combined to establish an efficient and lightweight architecture search technique termed Stage and Lightweight Network Architecture Search (SLNAS). The experimental results highlight the superior performance of the optimized search space design, primarily when implemented with the Stage LSTM controller model. This approach shows significantly improved accuracy and stability compared to random, traditional LSTM, and Genetic Algorithm (GA) controller models, with statistically significant differences. Notably, the Stage LSTM controller excels in accuracy while producing models with fewer parameters within the expanded architecture search space. The study adopts the Stage LSTM controller model due to its ability to approximate optimal sequence structures, particularly when combined with the optimized search space design, referred to as SLNAS. SLNAS's performance is evaluated through experiments and comparisons with other Neural Architecture Search (NAS) and object classification methods from different researchers. These experiments consider model parameters, hardware resources, model stability, and multiple datasets. The results show that SLNAS achieves a low error rate of 2.86 % on the CIFAR-10 dataset after just 0.2 days of architecture search, matching the performance of manually designed models but using only 2 % of the parameters. SLNAS consistently demonstrates robust performance across various image classification domains, with an approximate parameter count 700,000. To summarize, SLNAS emerges as a highly effective automated network architecture search method tailored for image classification. It streamlines the model design process, making it accessible to researchers without specialized knowledge in deep learning. Optimizing this method unlocks the full potential of deep learning across diverse research areas. Interested parties can publicly access the source code and pre-trained models through the following link: https://github.com/huanyu-chen/LNASG-and-SLNAS-model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032400503X",
    "keywords": [
      "Architecture",
      "Art",
      "Artificial intelligence",
      "Computer architecture",
      "Computer network",
      "Computer science",
      "Convolutional neural network",
      "Machine learning",
      "Network architecture",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Lin",
        "given_name": "Chuen-Horng"
      },
      {
        "surname": "Chen",
        "given_name": "Tsung-Yi"
      },
      {
        "surname": "Chen",
        "given_name": "Huan-Yu"
      },
      {
        "surname": "Chan",
        "given_name": "Yung-Kuan"
      }
    ]
  },
  {
    "title": "Class-imbalanced semi-supervised learning for large-scale point cloud semantic segmentation via decoupling optimization",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110701",
    "abstract": "Semi-supervised learning (SSL), thanks to the significant reduction of data annotation costs, has been an active research topic for large-scale 3D scene understanding. However, the existing SSL-based methods suffer from severe training bias, mainly due to class imbalance and long-tail distributions of the point cloud data. As a result, they lead to a biased prediction for the tail class segmentation. In this paper, we introduce a new decoupling optimization framework, which disentangles feature representation learning and classifier in an alternative optimization manner to shift the bias decision boundary effectively. In particular, we first employ two-round pseudo-label generation to select unlabeled points across head-to-tail classes. We further introduce multi-class imbalanced focus loss to adaptively pay more attention to feature learning across head-to-tail classes. We fix the backbone parameters after feature learning and retrain the classifier using ground-truth points to update its parameters. Extensive experiments demonstrate the effectiveness of our method outperforming previous state-of-the-art methods on both indoor and outdoor 3D point cloud datasets (i.e., S3DIS, ScanNet-V2, Semantic3D, and SemanticKITTI) using 1% and 1pt evaluation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324004527",
    "keywords": [
      "Artificial intelligence",
      "Cartography",
      "Class (philosophy)",
      "Computer science",
      "Control engineering",
      "Decoupling (probability)",
      "Engineering",
      "Geography",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Point cloud",
      "Scale (ratio)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Mengtian"
      },
      {
        "surname": "Lin",
        "given_name": "Shaohui"
      },
      {
        "surname": "Wang",
        "given_name": "Zihan"
      },
      {
        "surname": "Shen",
        "given_name": "Yunhang"
      },
      {
        "surname": "Zhang",
        "given_name": "Baochang"
      },
      {
        "surname": "Ma",
        "given_name": "Lizhuang"
      }
    ]
  },
  {
    "title": "Tensorial bipartite graph clustering based on logarithmic coupled penalty",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110860",
    "abstract": "The graph-based multi-view clustering method has gained considerable attention in recent years. However, due to its large time complexity, it is limited to handling small-scale clustering datasets. Moreover, most existing models only consider the similarity within views and do not leverage the correlation between views and use the tensor nuclear norm (TNN) as a convex approximation to the tensor rank function. The TNN treats each singular value equally, leading to suboptimal results. To address this issue, this paper proposes a tensorial multi-view clustering model based on bipartite graphs. This paper first introduces a new non-convex logarithmic coupled penalty (LCP) function that treats different singular values differently and preserves the useful structural information required. Additionally, a tensorial bipartite graph clustering model based on logarithmic coupled penalty (LCP-TBGC) is proposed along with a corresponding solution algorithm. The paper also presents a theoretical proof that the obtained resulting sequence converges to the Karush–Kuhn–Tucker (KKT) point. Finally, to validate the effectiveness and superiority of the proposed model, experiments were conducted on eight datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324006113",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Bipartite graph",
      "Cluster analysis",
      "Combinatorics",
      "Complete bipartite graph",
      "Computer science",
      "Graph",
      "Logarithm",
      "Mathematical analysis",
      "Mathematics"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Chang"
      },
      {
        "surname": "Zhang",
        "given_name": "Hongbing"
      },
      {
        "surname": "Fan",
        "given_name": "Hongtao"
      },
      {
        "surname": "Li",
        "given_name": "Yajing"
      }
    ]
  },
  {
    "title": "GR-GAN: A unified adversarial framework for single image glare removal and denoising",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110815",
    "abstract": "In this paper, we introduce the single image glare removal (SIGR) task. SIGR aims to eliminate glare or light caused by external environment in lighting scene. To efficiently improve natural image quality and usability, many research tasks, such as image deraining and shadow removal, have been investigated a lot. However, SIGR is still underexplored. Therefore, we propose to construct a dataset and explore deep learning-based models for SIGR. Our contributions can be summarized as follows: (1) We establish a new benchmark dataset for SIGR, termed De-Glare, aiming to propel research of SIGR. This dataset comprises pairs of {glare, glare-free} images sourced from both real-world and synthetic data, utilized for training and evaluating models. (2) We conduct a comprehensive benchmarking of extensive state-of-the-art (SoTA) methods on the constructed De-Glare dataset and provide insightful analyses based on the results. (3) An innovative approach for single image glare removal employing a multi-scale generative adversarial network (GR-GAN) model is proposed. Glare images typically exhibit irregular glare shapes and cluttered backgrounds. To address these irregular glare patterns, we introduce a deformable convolution-based glare attention detector (GAD) designed to generate an attention map which specifics glare spots or rays in the input image. In pursuit of enhancing the perceptual quality of output image, GR-GAN adaptively filters out irrelevant noises and enhances salient features through a generator with cascaded pyramid neck (CPN) network. This work can provide useful insights for developing better SIGR models. Without specific tuning, our method achieves the SoTA results on multiple computer vision tasks, including the image deraining and image shadow removal.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324005661",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "GLARE",
      "Image (mathematics)",
      "Image quality",
      "Layer (electronics)",
      "Organic chemistry"
    ],
    "authors": [
      {
        "surname": "Niu",
        "given_name": "Cong"
      },
      {
        "surname": "Li",
        "given_name": "Ke"
      },
      {
        "surname": "Wang",
        "given_name": "Di"
      },
      {
        "surname": "Zhu",
        "given_name": "Wenxuan"
      },
      {
        "surname": "Xu",
        "given_name": "Haojie"
      },
      {
        "surname": "Dong",
        "given_name": "Jinhui"
      }
    ]
  },
  {
    "title": "Multi-scale task-aware structure graph modeling for few-shot image recognition",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110855",
    "abstract": "The Few-shot image recognition attempts to recognize images from a novel class with only a limited number of labeled training images, which is a few-shot learning (FSL) task. FSL is very challenging. Limited labeled training samples cannot adequately represent the distribution of classes, and the base and novel classes in the training and testing stages do not intersect and have different distributions, leading to a domain shift problem in generalizing the learned model to the novel class dataset. In this paper, we propose multi-scale task-aware structure graph modeling for few-shot image recognition. We train a meta-filter learner to generate task-aware local structure filters for each scale and adaptively capture the local structures at each scale. Moreover, we introduce a novel multi-scale graph attention network (MGAT) module to model the multi-scale local structures of an image, fully exploring the correlations between different local structures at all scales of the image. Finally, we leverage the attention mechanism of graph attention network to achieve information aggregation and propagation, aiming to obtain more representative and discriminative local structure features that integrate both local and global information. We conducted comprehensive experiments on four benchmark datasets widely adopted in FSL tasks. The experimental results demonstrate that the MTSGM obtains state-of-the-art performance, which validates the effectiveness of MTSGM.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032400606X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Graph",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Scale (ratio)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Peng"
      },
      {
        "surname": "Ye",
        "given_name": "Zilong"
      },
      {
        "surname": "Wang",
        "given_name": "Liang"
      },
      {
        "surname": "Liu",
        "given_name": "Huiting"
      },
      {
        "surname": "Ji",
        "given_name": "Xia"
      }
    ]
  },
  {
    "title": "Fine-grained recognition via submodular optimization regulated progressive training",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110849",
    "abstract": "Progressive training has unfolded its superiority on a wide range of downstream tasks. However, it may fail in fine-grained recognition (FGR) due to special challenges with high intra-class and low inter-class variances. In this paper, we propose an active self-pace learning method to exploit the full potential of progressive training strategy in FGR. The key innovation of our design is to integrate submodular optimization and self-pace learning into a maximum–minimum optimization framework. The submodular optimization is regarded as a dynamic regularization to select active sample groups in each training round for restricting the search space of self-pace optimization. This can overcome the limitation of traditional self-pace learning that is easily trapped into local minimums when facing challenging samples. Extensive experiments on three public FGR datasets show that the proposed method can win at least 1.5% performance gain in various kinds of network backbones including swin-transformer.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324006009",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Geography",
      "Mathematical optimization",
      "Mathematics",
      "Meteorology",
      "Pattern recognition (psychology)",
      "Submodular set function",
      "Training (meteorology)"
    ],
    "authors": [
      {
        "surname": "Kang",
        "given_name": "Bin"
      },
      {
        "surname": "Du",
        "given_name": "Songlin"
      },
      {
        "surname": "Liang",
        "given_name": "Dong"
      },
      {
        "surname": "Wu",
        "given_name": "Fan"
      },
      {
        "surname": "Li",
        "given_name": "Xin"
      }
    ]
  },
  {
    "title": "Multi-schema prompting powered token-feature woven attention network for short text classification",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110782",
    "abstract": "Short text classification task poses challenges in natural language processing due to insufficient contextual information. This task is typically approached by extracting rich semantic features in the text and encoding it as a sentence-level representation using deep neural networks. The self-attention mechanism has emerged as one of the primary methods to tackle this problem. However, traditional attention methods only focus on the interactions between tokens, neglecting the semantic relationships between features. We propose a novel attention-based module, called token-feature woven attention fusion (TFWAF) network for sentence-level representation information aggregation, which leverages the self-attention mechanism from both token and feature perspectives. Moreover, we design a multi-schema prompting approach within machine reading comprehension and prompt learning paradigms to better utilize prior knowledge in a pre-trained language model and recognize enhanced textual semantic representation. Experimental results show our model achieves state-of-the-art performance compared to existing baselines on eight benchmark datasets in the context of short text classification. The source code is available in https://github.com/Aaronzijingcai/MP-TFWA.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324005338",
    "keywords": [
      "Artificial intelligence",
      "Computer network",
      "Computer science",
      "Feature (linguistics)",
      "Information retrieval",
      "Linguistics",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Schema (genetic algorithms)",
      "Security token"
    ],
    "authors": [
      {
        "surname": "Cai",
        "given_name": "Zijing"
      },
      {
        "surname": "Zhang",
        "given_name": "Hua"
      },
      {
        "surname": "Zhan",
        "given_name": "Peiqian"
      },
      {
        "surname": "Jia",
        "given_name": "Xiaohui"
      },
      {
        "surname": "Yan",
        "given_name": "Yongjian"
      },
      {
        "surname": "Song",
        "given_name": "Xiawen"
      },
      {
        "surname": "Xie",
        "given_name": "Bo"
      }
    ]
  },
  {
    "title": "Mask-DerainGAN: Learning to remove rain streaks by learning to generate rainy images",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110840",
    "abstract": "Image deraining with unpaired data has been a challenging problem. Previous methods suffer from either the color distortion artifacts, due to the pixel-level cycle consistency loss, or the time-consuming training process. To address these problems, in this paper, we propose a novel method for rain removal based on using unpaired data. First, we obtain a rain streak mask from the derained result, which serves as a guidance for generating rainy images. Both the mask and the rain-free image are then fed into the proposed generator to obtain a high-quality rainy image, which implicitly helps improve the rain removal performance. In this way, the proposed learning framework simultaneously learns rain removal and rain generation in order to produce high-quality rain-free images and rainy images. Second, we propose a contrastive learning generator to preserve background texture details and ensure semantic consistency between the generated rain-free image and the original input. Experimental results demonstrate that our method surpasses most state-of-the-art unsupervised methods on multiple benchmark synthetic and real datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324005910",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Environmental science",
      "Geology",
      "Pattern recognition (psychology)",
      "Remote sensing"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Pengjie"
      },
      {
        "surname": "Wang",
        "given_name": "Pei"
      },
      {
        "surname": "Chen",
        "given_name": "Miaomiao"
      },
      {
        "surname": "Lau",
        "given_name": "Rynson W.H."
      }
    ]
  },
  {
    "title": "Early gesture detection in untrimmed streams: A controlled CTC approach for reliable decision-making",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110733",
    "abstract": "This paper focuses on the problem of online action detection for interactive systems, with a special emphasis on earliness. Online Action Detection (OAD) refers to the challenging task of recognizing gestures in untrimmed, streaming videos where the actions occur in unpredictable orders and durations. To address these challenges, we present a skeleton-based system for OAD incorporating a decision mechanism to accurately detect ongoing gestures. This allows us to provide instance-level output, achieving a high level of stream understanding. This mechanism relies on a novel Connectionist Temporal Classification (CTC) loss design that restricts the path possibilities according to the action boundaries. We also present a mechanism to tune the trade-off between accuracy and earliness according to the needs of the interactive system using a weighted label prior. This system includes a 3D CNN network, referred to as DOLT-C3D, exploiting the spatial–temporal information provided by the euclidean skeleton representation. We extensively evaluate our approach on eight publicly available datasets, demonstrating its superior performance compared to state-of-the-art methods in terms of both accuracy and earliness. We also successfully applied our approach to early 2D gestures detection. Furthermore, our system shows real-time performance, making it a suitable choice for interactive systems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324004849",
    "keywords": [
      "Artificial intelligence",
      "Computer network",
      "Computer science",
      "Gesture",
      "Machine learning",
      "Pattern recognition (psychology)",
      "STREAMS"
    ],
    "authors": [
      {
        "surname": "Mocaër",
        "given_name": "William"
      },
      {
        "surname": "Anquetil",
        "given_name": "Eric"
      },
      {
        "surname": "Kulpa",
        "given_name": "Richard"
      }
    ]
  },
  {
    "title": "Unsupervised outlier detection using random subspace and subsampling ensembles of Dirichlet process mixtures",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110846",
    "abstract": "Probabilistic mixture models are recognized as effective tools for unsupervised outlier detection owing to their interpretability and global characteristics. Among these, Dirichlet process mixture models stand out as a strong alternative to conventional finite mixture models for both clustering and outlier detection tasks. Unlike finite mixture models, Dirichlet process mixtures are infinite mixture models that automatically determine the number of mixture components based on the data. Despite their advantages, the adoption of Dirichlet process mixture models for unsupervised outlier detection has been limited by challenges related to computational inefficiency and sensitivity to outliers in the construction of outlier detectors. Additionally, Dirichlet process Gaussian mixtures struggle to effectively model non-Gaussian data with discrete or binary features. To address these challenges, we propose a novel outlier detection method that utilizes ensembles of Dirichlet process Gaussian mixtures. This unsupervised algorithm employs random subspace and subsampling ensembles to ensure efficient computation and improve the robustness of the outlier detector. The ensemble approach further improves the suitability of the proposed method for detecting outliers in non-Gaussian data. Furthermore, our method uses variational inference for Dirichlet process mixtures, which ensures both efficient and rapid computation. Empirical analyses using benchmark datasets demonstrate that our method outperforms existing approaches in unsupervised outlier detection.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324005971",
    "keywords": [
      "Algorithm",
      "Anomaly detection",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Cluster analysis",
      "Computer science",
      "Dirichlet process",
      "Gene",
      "Inference",
      "Mathematics",
      "Mixture model",
      "Outlier",
      "Pattern recognition (psychology)",
      "Robustness (evolution)",
      "Subspace topology"
    ],
    "authors": [
      {
        "surname": "Kim",
        "given_name": "Dongwook"
      },
      {
        "surname": "Park",
        "given_name": "Juyeon"
      },
      {
        "surname": "Chung",
        "given_name": "Hee Cheol"
      },
      {
        "surname": "Jeong",
        "given_name": "Seonghyun"
      }
    ]
  },
  {
    "title": "Cluster prototype earth mover’s distance adapters and alignment-guided prompt learning for vision–language models",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110861",
    "abstract": "Vision–language pre-trained models have focused on fine-tuning methods to enhance generalization in downstream tasks, such as CLIP. Recent research proposes fine-tuning these models using prompting and adapter techniques. However, prompting methods tend to overfit class-specific data distributions, and most adapters ignore the prototype representations as the weights, resulting in poor generalization. To tackle these challenges, we propose a novel method that integrates the cluster prototype Earth Mover’s Distance adapters and alignment-guided prompt learning (CPAAP) for vision–language models. Specifically, our adapter designs three components: graph convolutional network-based clusters, hierarchy prototype representations, and Earth Mover’s Distance similarity, providing a robust metric for measuring feature representations. Additionally, alignment-guided prompt learning enforces a constraint in the prediction of trainable and frozen models, enabling effective adaptation to downstream tasks. Extensive experiments are conducted on few-shot classification, base-to-novel generalization, cross-dataset evaluation, and domain generalization. In practice, CPAAP outperforms state-of-the-art methods on zero-shot tasks, achieving an absolute gain of 0.46% on the harmonic mean across 11 popular datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324006125",
    "keywords": [
      "Adapter (computing)",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Earth mover's distance",
      "Generalization",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Operating system",
      "Overfitting"
    ],
    "authors": [
      {
        "surname": "Dong",
        "given_name": "Mengping"
      },
      {
        "surname": "Li",
        "given_name": "Fei"
      },
      {
        "surname": "Li",
        "given_name": "Zhenbo"
      },
      {
        "surname": "Liu",
        "given_name": "Xue"
      }
    ]
  },
  {
    "title": "Rethink video retrieval representation for video captioning",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110744",
    "abstract": "Video captioning, a challenging task targeting the automatic generation of accurate and comprehensive descriptions based on video content, has witnessed substantial success recently driven by bridging video representations and textual semantics. Inspired by the nature of the video retrieval task, which learns visual features strongly related to text queries, we propose to take advantage of visual representation learning from the video retrieval framework to tackle video captioning tasks and construct adequate multi-grained cross-modal matching while extracting visual features. However, a simple direct application of recent video retrieval models fails to capture sufficient temporal details and the rich visual features of local patch tokens of video frames lack semantic information essential for captioning tasks. These deficiencies are primarily due to these models lack fine-grained interactions between video frames and offer only weak textual supervision over frame patch tokens. To increase the attention on temporal details, we propose a learnable token shift module, which flexibly captures subtle movements in local regions across the temporal sequence. Furthermore, we devise a Refineformer, which learns to integrate local video patch tokens strongly related to desired captions via a cross-attention mechanism. Extensive experiments on MSVD, MSR-VTT and VATEX demonstrate the favorable performance of our method. Code will be available at https://github.com/tiesanguaixia/IVRC.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324004953",
    "keywords": [
      "Artificial intelligence",
      "Closed captioning",
      "Computer science",
      "Computer vision",
      "Image (mathematics)",
      "Information retrieval",
      "Law",
      "Multimedia",
      "Natural language processing",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Video retrieval"
    ],
    "authors": [
      {
        "surname": "Tian",
        "given_name": "Mingkai"
      },
      {
        "surname": "Li",
        "given_name": "Guorong"
      },
      {
        "surname": "Qi",
        "given_name": "Yuankai"
      },
      {
        "surname": "Wang",
        "given_name": "Shuhui"
      },
      {
        "surname": "Sheng",
        "given_name": "Quan Z."
      },
      {
        "surname": "Huang",
        "given_name": "Qingming"
      }
    ]
  },
  {
    "title": "CORE: Learning consistent ordinal representations with convex optimization for image ordinal estimation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110748",
    "abstract": "Image ordinal estimation is to estimate the ordinal label of a given image. Existing methods primarily rely on ordinal regression, mapping feature representations directly to ordinal labels. However, these methods often struggle to preserve the inherent order within the learned feature representations. To this end, this paper proposes learning intrinsic Consistent Ordinal REpresentations (CORE), a novel approach that learns intrinsic ordinal relationships directly from ground-truth labels. First, it constructs an ordinal manifold using an ordinal totally ordered set (toset) distribution (OTD), capturing the inherent order of labels while regularizing feature embeddings. Second, the CORE leverages the toset distribution to convert both feature representations and labels into a unified embedding space, enabling consistent manifold alignment. Third, CORE employs an ordinal prototype-constrained convex programming formulation with dual decomposition, minimizing the Kullback–Leibler (KL) divergence between the toset distributions of labels and feature representations. Extensive experiments demonstrate that CORE, when combined with existing deep ordinal regression methods, significantly improves their performance in preserving ordinal relationships and achieves superior quantitative results across four real-world scenarios.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324004990",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Core (optical fiber)",
      "Divergence (linguistics)",
      "Embedding",
      "Feature (linguistics)",
      "Geometry",
      "Image (mathematics)",
      "Linguistics",
      "Mathematics",
      "Ordinal data",
      "Ordinal optimization",
      "Ordinal regression",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Regular polygon",
      "Statistics",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Lei",
        "given_name": "Yiming"
      },
      {
        "surname": "Li",
        "given_name": "Zilong"
      },
      {
        "surname": "Li",
        "given_name": "Yangyang"
      },
      {
        "surname": "Zhang",
        "given_name": "Junping"
      },
      {
        "surname": "Shan",
        "given_name": "Hongming"
      }
    ]
  },
  {
    "title": "Graph semantic information for self-supervised monocular depth estimation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110770",
    "abstract": "Self-supervised monocular depth estimation has garnered significant attention in recent years due to its practical value in applications, as it eliminates the need for ground truth depth maps during training. However, its performance usually drops when estimating weakly textured regions and boundary regions, primarily due to the limited depth representation capability of traditional Convolutional Neural Networks (CNNs) that do not support topology. To address these issues, we propose a Graph Semantic Model (GSM) to improve self-supervised monocular depth estimation by utilizing graph learning and semantic information. Our focus is on improving feature representation through graph semantic information. Therefore, we incorporate semantic segmentation and depth estimation into one framework and enhance the interaction of different modal information through the Inter-Directed Graph Reasoning (IDGR) module. In addition, we design the Semantic-Guided Edge Graph Reasoning (SGEGR) module, aiming to boost the network's ability to perceive local depth. Extensive experiments on the KITTI dataset show that our method outperforms the state-of-the-art methods, particularly in accurately estimating depth within weakly textured regions and boundary regions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324005211",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Graph",
      "Machine learning",
      "Monocular",
      "Pattern recognition (psychology)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Dongdong"
      },
      {
        "surname": "Wang",
        "given_name": "Chunping"
      },
      {
        "surname": "Wang",
        "given_name": "Huiying"
      },
      {
        "surname": "Fu",
        "given_name": "Qiang"
      }
    ]
  },
  {
    "title": "CarvingNet: Point cloud completion by stepwise refining multi-resolution features",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110780",
    "abstract": "In the field of 3D vision, 3D point cloud completion is a crucial task in many practical applications. Current methods use Transformer's Encoder-Decoder framework to predict the missing part of the point cloud features at low resolution, which does not fully utilize the feature information at multiple resolutions and can result in the loss of the object's geometric details. In this paper, we present a novel point cloud completion method, CarvingNet, which, to the best of our knowledge, is the first to apply the U-Net architecture to the point cloud completion task by operating directly on unordered point cloud features at multiple resolutions. Firstly, we gradually expand the receptive field and use cross-attention to purify the features of the missing part of the point cloud at each resolution and to generate the contour features of the complete point cloud at the last obtained resolution. Then, we gradually reduce the receptive field and use cross-attention to refine the features of the complete point cloud at each resolution and to generate the features of the complete point cloud with rich details at the last obtained resolution. To obtain point cloud features at different resolutions, we specifically design the up-sampling module and down-sampling module for disordered point cloud features. Furthermore, we improve the FoldingNet network to make it more suitable for generating high-quality dense point clouds. The experimental results demonstrate that our proposed CarvingNet achieves the performance of existing state-of-the-art methods on the ShapeNet-55, ShapeNet-34, and KITTI benchmarks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324005314",
    "keywords": [
      "Artificial intelligence",
      "Cloud computing",
      "Computer science",
      "Computer vision",
      "Geometry",
      "Materials science",
      "Mathematics",
      "Metallurgy",
      "Operating system",
      "Point (geometry)",
      "Point cloud",
      "Refining (metallurgy)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Liangliang"
      },
      {
        "surname": "Liu",
        "given_name": "Guihua"
      },
      {
        "surname": "Xu",
        "given_name": "Feng"
      },
      {
        "surname": "Deng",
        "given_name": "Lei"
      }
    ]
  },
  {
    "title": "Multi-view clustering via dynamic unified bipartite graph learning",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110715",
    "abstract": "Multi-view clustering algorithms based on graph learning have the ability to extract the potential association between data samples, which has been a concern of many researchers in recent years. However, existing algorithms have two limitations: (1) they directly learn from the raw graph, which includes noise and outliers, and they construct the graph filter statically, biasing the clustering results; (2) during graph construction, they mainly use the information of a single structure and fail to fully extract the multi-granular structural information among the data. To address these issues, this paper proposes a novel multi-view clustering method via dynamic unified bipartite graph learning. Specifically, a learnable graph filter is first refined to dynamically filter the original data feature space, gradually filtering out the undesirable high-frequency noise and achieving a clustering-friendly smooth representation. Second, a unified bipartite graph is constructed by combining the multi-granular structural information of different views to better explore the distinct and common information of each view. In one framework, the dynamic filter and multi-granular structure information are combined to iteratively learn the unified bipartite graph. An efficient iterative algorithm is designed to decompose the objective function into small-scale subproblems for solving. Extensive experiments on benchmark datasets show the superiority of the proposed algorithm over several existing state-of-the-art multi-view clustering algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324004667",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Bipartite graph",
      "Cluster analysis",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Filter (signal processing)",
      "Graph",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Xingwang"
      },
      {
        "surname": "Wang",
        "given_name": "Shujun"
      },
      {
        "surname": "Liu",
        "given_name": "Xiaolin"
      },
      {
        "surname": "Liang",
        "given_name": "Jiye"
      }
    ]
  },
  {
    "title": "A2GCN: Graph Convolutional Networks with Adaptive Frequency and Arbitrary Order",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110764",
    "abstract": "Graph Neural Networks (GNNs) gain remarkable success in various graph learning tasks under homophily graph assumption. This assumption is extremely fragile since real-world graphs with heterophily are ubiquitous. Under this circumstance, existing GNNs attempt to design or learn graph spectral filters for observed graphs. The representation ability of them, however, is limited due to: (1) Constant frequency response is incapable of simulating complicated filters that real-world applications require. (2) Fixed polynomial order fails to effectively uncover the node label patterns concealing different order neighborhoods. To this end, we propose a novel G raph C onvolutional N etworks with A daptive Frequency and A rbitrary Order (A2GCN) to learn various graph spectral filters suitable for distinct networks. Specifically, a simple but elegant filter with adaptive frequency response is designed to span across multiple layers for capturing different frequency components hiding in varying orders, producing A2GCN filter bases. Afterward, the coefficients of the A2GCN basis for each node are learned to achieve A2GCN filters with arbitrary polynomial order. The resulting A2GCN filters possess flexible frequency response that can automatically adapt to the node label pattern, as such, it empowers A2GCN with stronger expressiveness and naturally alleviates the over-smoothing problem. Theoretical analysis is provided to show the superiority of the proposed A2GCN. Additionally, extensive experiments on both node-level and graph-level tasks validate that the proposed A2GCN accomplishes highly competitive performance and improves classification accuracy. Codes are available at https://github.com/AIG22/A2GCN.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324005156",
    "keywords": [
      "Algorithm",
      "Computer science",
      "Graph",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Ai",
        "given_name": "Guoguo"
      },
      {
        "surname": "Yan",
        "given_name": "Hui"
      },
      {
        "surname": "Wang",
        "given_name": "Huan"
      },
      {
        "surname": "Li",
        "given_name": "Xin"
      }
    ]
  },
  {
    "title": "Multimodal explainability via latent shift applied to COVID-19 stratification",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110825",
    "abstract": "We are witnessing a widespread adoption of artificial intelligence in healthcare. However, most of the advancements in deep learning in this area consider only unimodal data, neglecting other modalities. Their multimodal interpretation necessary for supporting diagnosis, prognosis and treatment decisions. In this work we present a deep architecture, which jointly learns modality reconstructions and sample classifications using tabular and imaging data. The explanation of the decision taken is computed by applying a latent shift that, simulates a counterfactual prediction revealing the features of each modality that contribute the most to the decision and a quantitative score indicating the modality importance. We validate our approach in the context of COVID-19 pandemic using the AIforCOVID dataset, which contains multimodal data for the early identification of patients at risk of severe outcome. The results show that the proposed method provides meaningful explanations without degrading the classification performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324005764",
    "keywords": [
      "2019-20 coronavirus outbreak",
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Computer science",
      "Coronavirus disease 2019 (COVID-19)",
      "Disease",
      "Dormancy",
      "Germination",
      "Infectious disease (medical specialty)",
      "Internal medicine",
      "Medicine",
      "Outbreak",
      "Seed dormancy",
      "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)",
      "Stratification (seeds)",
      "Virology"
    ],
    "authors": [
      {
        "surname": "Guarrasi",
        "given_name": "Valerio"
      },
      {
        "surname": "Tronchin",
        "given_name": "Lorenzo"
      },
      {
        "surname": "Albano",
        "given_name": "Domenico"
      },
      {
        "surname": "Faiella",
        "given_name": "Eliodoro"
      },
      {
        "surname": "Fazzini",
        "given_name": "Deborah"
      },
      {
        "surname": "Santucci",
        "given_name": "Domiziana"
      },
      {
        "surname": "Soda",
        "given_name": "Paolo"
      }
    ]
  },
  {
    "title": "Feature-semantic augmentation network for few-shot open-set recognition",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110781",
    "abstract": "Few-shot open-set recognition (FSOR) represents a relatively underexplored area of research. The primary challenge encountered by FSOR methods lies in recognizing known classes while simultaneously rejecting unknown classes utilizing only limited samples. Current FSOR methods predominantly rely on the visual information extracted from images to establish class representations, aiming to derive distinguishable classification scores for both known and unknown classes. However, these methods often overlook the benefits of leveraging semantic information derived from class names associated with images, which could provide valuable auxiliary learning insights. This study introduces a feature-semantic augmentation network to improve FSOR performance utilizing multimodal information. Specifically, we augment the class-specific features of closed-set prototypes by integrating visual and textual features from known class names across both local and global feature spaces. To facilitate prototype learning, We introduce a refinement and fusion module. Among these, the former leverages the similarity between prototype and target features at both channel and spatial dimensions to calibrate targets relative to their relevant prototypes. Meanwhile, the latter employs additional classification targets generated by the fusion module to provide learning sources from different classes. Experimental results on various few-shot learning benchmarks show that the proposed method significantly outperforms current state-of-the-art methods across both closed- and open-set scenarios.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324005326",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Discrete mathematics",
      "Feature (linguistics)",
      "Linguistics",
      "Mathematics",
      "Open set",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Semantic feature",
      "Set (abstract data type)",
      "Shot (pellet)"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Xilang"
      },
      {
        "surname": "Choi",
        "given_name": "Seon Han"
      }
    ]
  },
  {
    "title": "BMPCN: A Bigraph Mutual Prototype Calibration Net for few-shot classification",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110841",
    "abstract": "In recent studies on few-shot classification, most of the existing methods utilized word embeddings as prior knowledge to adjust the distribution of visual prototypes. However, this straightforward fusion of visual and semantic features profoundly alters the feature distribution in the original feature space, rendering it unable to effectively calibrate feature distribution through mutual guidance of cross-modal information. To address this problem, we propose a novel Bigraph Mutual Prototype Calibration Network (BMPCN) for few-shot learning in this paper, in which we not only update the distribution of class features based on prototype-level similarity in both visual and semantic spaces but also facilitate the mutual guidance of visual and semantic feature updates through instance-level similarity. In the BMPCN, a bigraph mutual promotion structure is proposed, wherein a visual graph is constructed with visual features as nodes and the similarity between visual features as edges. Simultaneously, the semantic feature nodes are automatically generated from images, and the class-level prior knowledge is leveraged to correct these automatically generated semantic nodes. To better update the bigraph mutual promotion structure, we propose a Bigraph Interactive Augmentation Module (BIAM), a Nearest Neighbor Proto-level Similarity Promotion Module (NN-PSP), and a Proto-level Similarity Promotion Module (PK-PSP) based on original knowledge augmentation to perform the bigraph update. For inter-graph updating, we use the prototype-level similarity obtained from the NN-PSP and PK-PSP modules to fully learn task-level information, thus enabling task-specific prototype updates. For intra-graph updating, our visual and semantic graphs use instance-level similarity analysis to extract potential correlations between different feature domains and implement mutual guidance in the BIAM module to correct the feature distribution of visual and semantic features. Experiments on three widely used benchmarks illustrated that our proposed method obtains excellent performance based on the backbone Conv-4, and the results outperform state-of-the-art methods by about 8% on miniImageNet, tieredImageNet, and CUB-200-2011. Code has been available at https://github.com/cmzHome/BMPCN-MASTER.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324005922",
    "keywords": [
      "Artificial intelligence",
      "Business",
      "Calibration",
      "Computer science",
      "Geometry",
      "International trade",
      "Mathematics",
      "Mutual information",
      "Mutual recognition",
      "Natural language processing",
      "Net (polyhedron)",
      "Pattern recognition (psychology)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Jing"
      },
      {
        "surname": "Chen",
        "given_name": "Mingzhe"
      },
      {
        "surname": "Hu",
        "given_name": "Yunzuo"
      },
      {
        "surname": "Zhang",
        "given_name": "Xinzhou"
      },
      {
        "surname": "Wang",
        "given_name": "Zhe"
      }
    ]
  },
  {
    "title": "Semi-supervised clustering guided by pairwise constraints and local density structures",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110751",
    "abstract": "Clustering based on local density peaks and graph cut (LDP-SC) is one of the state-of-the-art algorithms in unsupervised clustering, which first divides the data set to be multiple local trees, and then aggregates these local trees to obtain the final clustering result. However, for complex data sets, there might exist data points from different classes in the same local tree. In this article, we use pairwise constraint information to resolve this issue and propose a semi-supervised local density peaks and graph cut based clustering algorithm (SLDPC). In particular, SLDPC proposes intra-cluster conflict resolution and inter-cluster conflict resolution steps to split the local trees which are inconsistent with the provided pairwise constraint information. Theoretically, we show that the two steps will finish in a finite number of operations and the split local trees will be consistent with the pairwise constraint information. Subsequently, root node redirection and noise filtering steps are designed to avoid the local trees becoming too fragmented. Finally, we exploit the E2CP algorithm to further improve the similarity matrix between local trees using the pairwise constraint information, and the spectral clustering algorithm is adopted to obtain the clustering result. Experiments on multiple widely used synthetic and real-world data sets show that SLDPC is superior to LDP-SC and several other semi-supervised prominent clustering algorithms for most of the cases.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324005028",
    "keywords": [
      "Artificial intelligence",
      "CURE data clustering algorithm",
      "Canopy clustering algorithm",
      "Cluster analysis",
      "Computer science",
      "Constrained clustering",
      "Constraint (computer-aided design)",
      "Correlation clustering",
      "Data mining",
      "Geometry",
      "Graph",
      "Mathematics",
      "Pairwise comparison",
      "Pattern recognition (psychology)",
      "Single-linkage clustering",
      "Spectral clustering",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Long",
        "given_name": "Zhiguo"
      },
      {
        "surname": "Gao",
        "given_name": "Yang"
      },
      {
        "surname": "Meng",
        "given_name": "Hua"
      },
      {
        "surname": "Chen",
        "given_name": "Yuxu"
      },
      {
        "surname": "Kou",
        "given_name": "Hui"
      }
    ]
  },
  {
    "title": "Partial label feature selection based on noisy manifold and label distribution",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110791",
    "abstract": "In partial label learning, each training object is assigned a valid label and pseudo-labels, and a multi-class classifier is derived with inaccurate supervision. However, ambiguous labeling information adversely affects the performance of the classifier. Partial label feature selection has been shown efficiently improve the generalization performance of classifiers. Traditional manifold learning can employ intrinsic geometric information to identify discriminative features, while it is challenging due to the noisy manifold caused by pseudo-labels. Consequently, this paper proposes an embedding partial label feature selection based on noisy manifold and label distribution, which exploits feature dependency, label correlation, and instance relevance. Specifically, a linear regression function projects the feature space to the low-dimensional manifold space, which can avoid the influence of pseudo-labels affected by direct projection to the label space. The feature dependency and label correlation are obtained by manifold regularization in the feature and label space to reflect the feature significance. During optimization, instance similarity constraints variable iteration. Label distribution obtained through feature significance and instance relevance guides label space updates and reduces the impact of noise in the manifold. The effectiveness and robustness of the proposed algorithm are corroborated through experiments with three classifiers and five comparison methods on twelve datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324005429",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Distribution (mathematics)",
      "Engineering",
      "Feature (linguistics)",
      "Feature selection",
      "Linguistics",
      "Manifold (fluid mechanics)",
      "Mathematical analysis",
      "Mathematics",
      "Mechanical engineering",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Selection (genetic algorithm)"
    ],
    "authors": [
      {
        "surname": "Qian",
        "given_name": "Wenbin"
      },
      {
        "surname": "Liu",
        "given_name": "Jiale"
      },
      {
        "surname": "Yang",
        "given_name": "Wenji"
      },
      {
        "surname": "Huang",
        "given_name": "Jintao"
      },
      {
        "surname": "Ding",
        "given_name": "Weiping"
      }
    ]
  },
  {
    "title": "Visual multi-object tracking with re-identification and occlusion handling using labeled random finite sets",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110785",
    "abstract": "This paper proposes an online visual multi-object tracking (MOT) algorithm that resolves object appearance–reappearance and occlusion. Our solution is based on the labeled random finite set (LRFS) filtering approach, which in principle, addresses disappearance, appearance, reappearance, and occlusion via a single Bayesian recursion. However, in practice, existing numerical approximations cause reappearing objects to be initialized as new tracks, especially after long periods of being undetected. In occlusion handling, the filter’s efficacy is dictated by trade-offs between the sophistication of the occlusion model and computational demand. Our contribution is a novel modeling method that exploits object features to address reappearing objects whilst maintaining a linear complexity in the number of detections. Moreover, to improve the filter’s occlusion handling, we propose a fuzzy detection model that takes into consideration the overlapping areas between tracks and their sizes. We also develop a fast version of the filter to further reduce the computational time.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324005363",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Computer science",
      "Computer vision",
      "Eye tracking",
      "Identification (biology)",
      "Mathematics",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Pedagogy",
      "Psychology",
      "Tracking (education)",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Van Ma",
        "given_name": "Linh"
      },
      {
        "surname": "Nguyen",
        "given_name": "Tran Thien Dat"
      },
      {
        "surname": "Shim",
        "given_name": "Changbeom"
      },
      {
        "surname": "Kim",
        "given_name": "Du Yong"
      },
      {
        "surname": "Ha",
        "given_name": "Namkoo"
      },
      {
        "surname": "Jeon",
        "given_name": "Moongu"
      }
    ]
  },
  {
    "title": "D2GL: Dual-level dual-scale graph learning for sketch-based 3D shape retrieval",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110768",
    "abstract": "Sketch-based 3D shape retrieval (SBSR) is an active research area in the computer vision community, but it is still very challenging. One main reason is that existing deep learning-based methods usually treat sketches as 2D images, neglecting the sparsity and diversity. In this paper, we propose a novel Dual-level Dual-scale Graph Learning (D2GL) method to effectively enhance structural information and produce robust representations for sparse and diverse hand-drawn sketches. Specifically, in addition to the traditional branches for SBSR, we introduce a Dual-level Dual-scale Graph Self-attention (DLDS-GSA) as an auxiliary branch. DLDS-GSA further consists of two levels of encoders, i.e., a local structural encoder and a dual-scale global structural encoder, to capture both local discriminative and multi-scale global structures while minimizing the impact of various sketch drawing details. Comprehensive experiments on SHREC’13 and SHREC’14 datasets demonstrate the superiority of D2GL for SBSR, with extended experiments on PART-SHREC’14 confirming its generalization for unseen classes in SBSR.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324005193",
    "keywords": [
      "Algorithm",
      "Art",
      "Artificial intelligence",
      "Cartography",
      "Computer science",
      "Computer vision",
      "Dual (grammatical number)",
      "Dual graph",
      "Geography",
      "Graph",
      "Literature",
      "Pattern recognition (psychology)",
      "Planar graph",
      "Scale (ratio)",
      "Sketch",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Wenjing"
      },
      {
        "surname": "Bai",
        "given_name": "Jing"
      },
      {
        "surname": "Zheng",
        "given_name": "Hu"
      }
    ]
  },
  {
    "title": "A survey of dialogic emotion analysis: Developments, approaches and perspectives",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110794",
    "abstract": "Dialogic emotion analysis is an emerging and important research field in natural language processing. It aims to understand and process emotions in various forms of dialogue, such as human-human conversations, human–machine interactions, and chatbot responses. However, dialogic emotion analysis faces many challenges, such as the diversity of dialogue genres, the complexity of emotional expressions, and the difficulty of capturing the emotional needs of dialogue participants. Moreover, the current dialogue systems lack the ability to analyze emotions effectively and appropriately in different dialogue contexts. Therefore, a comprehensive review of the existing research on dialogic emotion analysis is needed. This survey aims to review dialogic emotion analysis methods based on natural language processing from 2017 to 2024. The review process follows the Preferred Reporting Items for Systematic reviews and Meta-Analyses (PRISMA). We summarize the research methods and emphasize their main research contributions. In addition, we also discuss current research trends and possible future research directions, as well as the impact of personal traits on emotions and potential ethical issues.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324005454",
    "keywords": [
      "Archaeology",
      "Computer science",
      "Dialogic",
      "Field (mathematics)",
      "History",
      "Mathematics",
      "Natural (archaeology)",
      "Operating system",
      "Pedagogy",
      "Process (computing)",
      "Psychology",
      "Pure mathematics"
    ],
    "authors": [
      {
        "surname": "Gan",
        "given_name": "Chenquan"
      },
      {
        "surname": "Zheng",
        "given_name": "Jiahao"
      },
      {
        "surname": "Zhu",
        "given_name": "Qingyi"
      },
      {
        "surname": "Cao",
        "given_name": "Yang"
      },
      {
        "surname": "Zhu",
        "given_name": "Ye"
      }
    ]
  },
  {
    "title": "Multi-query and multi-level enhanced network for semantic segmentation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110777",
    "abstract": "Plain transformer-based methods have achieved promising performance on semantic segmentation recently. These methods adopt a single set of class queries to predict masks of different semantic categories based on multi-level feature maps. We argue that this single-query design cannot fully exploit diverse information of different levels for improved semantic segmentation. To address this issue, we propose a multi-query and multi-level enhanced network for semantic segmentation (named QLSeg). Our QLSeg first performs multi-level feature enhancement on plain transformer to improve feature discriminability. Afterwards, we introduce multi-query decoder to respectively extract feature embeddings and predict mask logits at different levels, where feature embeddings are adaptively merged for classification and mask logits are summed for output masks. In addition, we introduce masked attention-to-mask to focus on local regions with the same class. We perform the experiments on three widely-used semantic segmentation datasets: ADE20K, COCO-Stuff-10K, and PASCAL-Context. Our proposed QLSeg achieves competitive results on all these three datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324005284",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Information retrieval",
      "Natural language processing",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Xie",
        "given_name": "Bin"
      },
      {
        "surname": "Cao",
        "given_name": "Jiale"
      },
      {
        "surname": "Anwer",
        "given_name": "Rao Muhammad"
      },
      {
        "surname": "Xie",
        "given_name": "Jin"
      },
      {
        "surname": "Nie",
        "given_name": "Jing"
      },
      {
        "surname": "Yang",
        "given_name": "Aiping"
      },
      {
        "surname": "Pang",
        "given_name": "Yanwei"
      }
    ]
  },
  {
    "title": "Finding score-based representative samples for cancer risk prediction",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110767",
    "abstract": "Finding representative samples is important for predicting cancer risk. In particular, it is crucial to identify each representative sample as responsible for the prediction performance. In this article, we present a general framework for finding representative samples by explicitly estimating their inherit contribution levels (or scores). By leveraging explainable models as our score functions such as Shapley value, LIME and influence function, our framework can quantitatively identify the representative level of each sample in cancer risk prediction. Furthermore, a score ensembler is introduced to integrate these scores obtained from various score functions with an additional vector of weight variables optimized by the Fast Iterative Shrinkage-Thresholding Algorithm. Empirical evaluations on four cancer risk datasets with different challenges by using five classifiers suggest that our approach significantly outperforms the state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324005181",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Mathematics",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Liao",
        "given_name": "Jun"
      },
      {
        "surname": "Luo",
        "given_name": "Hao"
      },
      {
        "surname": "Yan",
        "given_name": "Xuewen"
      },
      {
        "surname": "Ye",
        "given_name": "Ting"
      },
      {
        "surname": "Huang",
        "given_name": "Shanshan"
      },
      {
        "surname": "Liu",
        "given_name": "Li"
      }
    ]
  },
  {
    "title": "FairScene: Learning unbiased object interactions for indoor scene synthesis",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110737",
    "abstract": "In this paper, we propose an unbiased graph neural network learning method called FairScene for indoor scene synthesis. Conventional methods directly apply graphical models to represent the correlation of objects for subsequent furniture insertion. However, due to the object category imbalance in dataset collection and complex object entanglement with implicit confounders, these methods usually generate significantly biased scenes. Moreover, the performance of these methods varies greatly for different indoor scenes. To address this, we propose a framework named FairScene which can fully exploit unbiased object interactions through causal reasoning, so that fair scene synthesis is achieved by calibrating the long-tailed category distribution and mitigating the confounder effects. Specifically, we remove the long-tailed object priors subtract the counterfactual prediction obtained from default input, and intervene in the input feature by cutting off the causal link to confounders based on the causal graph. Extensive experiments on the 3D-FRONT dataset show that our proposed method outperforms the state-of-the-art indoor scene generation methods and enhances vanilla models on a wide variety of vision tasks including scene completion and object recognition.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324004886",
    "keywords": [
      "Artificial intelligence",
      "Bayesian probability",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Counterfactual thinking",
      "Epistemology",
      "Exploit",
      "Feature (linguistics)",
      "Feature learning",
      "Graph",
      "Linguistics",
      "Machine learning",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Prior probability",
      "Rendering (computer graphics)",
      "Scene graph",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Zhenyu"
      },
      {
        "surname": "Wang",
        "given_name": "Ziwei"
      },
      {
        "surname": "Liu",
        "given_name": "Shengyu"
      },
      {
        "surname": "Luo",
        "given_name": "Hao"
      },
      {
        "surname": "Lu",
        "given_name": "Jiwen"
      },
      {
        "surname": "Yan",
        "given_name": "Haibin"
      }
    ]
  },
  {
    "title": "FeverNet: Enabling accurate and robust remote fever screening",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110755",
    "abstract": "Remote human fever screening via thermal infrared imaging helps reduce the risk of respiratory disease transmission and plays an important role in public health monitoring. However, the accuracy of such systems often falls prey to variations in measurement distance and environment temperature. Most previous methods tend to employ sensors to overcome these variations, which are expensive schemes and have limited performance improvement. To address above problems, this paper presents a novel and robust remote fever screening framework named FeverNet. Specifically, FeverNet introduces depth estimation network and temperature distribution constraints across time periods to reduce the influence of distance variations and environment temperature changes. The fever attention module is thus proposed to enhance feature representation and expand the difference between fever faces and normal ones. In addition, we provide the Extended Thermal Infrared Face dataset (ETIF), which further gives visible images (paired with thermal infrared images) for depth estimation and improve the fever face generated method based on the maximum temperature of the face. Extensive experiments on ETIF demonstrate the advantages of our FeverNet over the state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324005065",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Face (sociological concept)",
      "Feature (linguistics)",
      "Geography",
      "Infrared",
      "Law",
      "Linguistics",
      "Machine learning",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Political science",
      "Politics",
      "Remote sensing",
      "Representation (politics)",
      "Social science",
      "Sociology",
      "Telecommunications",
      "Thermal infrared",
      "Thermography",
      "Transmission (telecommunications)"
    ],
    "authors": [
      {
        "surname": "Yan",
        "given_name": "Mengkai"
      },
      {
        "surname": "Qian",
        "given_name": "Jianjun"
      },
      {
        "surname": "Shao",
        "given_name": "Hang"
      },
      {
        "surname": "Luo",
        "given_name": "Lei"
      },
      {
        "surname": "Yang",
        "given_name": "Jian"
      }
    ]
  },
  {
    "title": "Momentum recursive DARTS",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110710",
    "abstract": "DARTS has emerged as a popular method for neural architecture search (NAS) owing to its efficiency and simplicity. It employs gradient-based bi-level optimization to iteratively optimize the upper-level architecture parameters and lower-level super-network weights. The key challenge in DARTS is the accurate estimation of gradients for two-level object functions, leading to significant errors in gradient approximation. To address this issue, we propose a new approach, MR-DARTS, that incorporates a momentum term and a recursive scheme to improve gradient estimation. Specifically, we leverage historical information by using a running average of past observed gradients to enhance the quality of current gradient estimation in both upper-level and lower-level functions. Our theoretical analysis shows that the variance of our estimated gradient decreases with each iteration. By utilizing momentum and a recursive scheme, MR-DARTS effectively controls the error in stochastic gradient updates that result from inaccurate gradient estimation. Furthermore, we utilize the Neumann series approximation and Hessian Vector Product scheme to reduce computational requirements and memory usage. We evaluate our proposed method on several benchmarks and demonstrate its effectiveness through comprehensive experiments.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324004618",
    "keywords": [
      "Algorithm",
      "Applied mathematics",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Gradient descent",
      "Hessian matrix",
      "Leverage (statistics)",
      "Mathematical optimization",
      "Mathematics"
    ],
    "authors": [
      {
        "surname": "Ma",
        "given_name": "Benteng"
      },
      {
        "surname": "Zhang",
        "given_name": "Yanning"
      },
      {
        "surname": "Xia",
        "given_name": "Yong"
      }
    ]
  },
  {
    "title": "EdVAE: Mitigating codebook collapse with evidential discrete variational autoencoders",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110792",
    "abstract": "Codebook collapse is a common problem in training deep generative models with discrete representation spaces like Vector Quantized Variational Autoencoders (VQ-VAEs). We observe that the same problem arises for the alternatively designed discrete variational autoencoders (dVAEs) whose encoder directly learns a distribution over the codebook embeddings to represent the data. We hypothesize that using the softmax function to obtain a probability distribution causes the codebook collapse by assigning overconfident probabilities to the best matching codebook elements. In this paper, we propose a novel way to incorporate evidential deep learning (EDL) through a hierarchical Bayesian modeling instead of softmax to combat the codebook collapse problem of dVAE. We evidentially monitor the significance of attaining the probability distribution over the codebook embeddings, in contrast to softmax usage. Our experiments using various datasets show that our model, called EdVAE, mitigates codebook collapse while improving the reconstruction performance, and enhances the codebook usage compared to dVAE and VQ-VAE based models. Our code can be found at https://github.com/ituvisionlab/EdVAE.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324005430",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Codebook",
      "Computer science",
      "Mathematics",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Baykal",
        "given_name": "Gulcin"
      },
      {
        "surname": "Kandemir",
        "given_name": "Melih"
      },
      {
        "surname": "Unal",
        "given_name": "Gozde"
      }
    ]
  },
  {
    "title": "SLSG: Industrial image anomaly detection with improved feature embeddings and one-class classification",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110862",
    "abstract": "Industrial image anomaly detection under the setting of one-class classification has significant practical value. However, most existing models face challenges in extracting separable feature representations when performing feature embedding and in constructing compact descriptions of normal features when performing one-class classification. One direct consequence is that most models perform poorly in detecting logical anomalies that violate contextual relationships. Focusing on more effective and comprehensive anomaly detection, we propose a network based on self-supervised learning and self-attentive graph convolution (SLSG). SLSG uses a generative pre-training network to assist the encoder in learning the embedding of normal patterns and the reasoning of positional relationships. Subsequently, we introduce pseudo-prior knowledge of anomalies in SLSG using simulated abnormal samples. By comparing the simulated anomalies, SLSG can better summarize the normal patterns and narrow the hypersphere used for one-class classification. In addition, with the construction of a more general graph structure, SLSG comprehensively models the dense and sparse relationships among the elements in an image, which further strengthens the detection of logical anomalies. Extensive experiments on benchmark datasets show that SLSG achieves superior anomaly detection performance, demonstrating the effectiveness of our method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324006137",
    "keywords": [
      "Anomaly (physics)",
      "Anomaly detection",
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Computer vision",
      "Condensed matter physics",
      "Contextual image classification",
      "Feature (linguistics)",
      "Image (mathematics)",
      "Linguistics",
      "Mathematics",
      "One-class classification",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Minghui"
      },
      {
        "surname": "Liu",
        "given_name": "Jing"
      },
      {
        "surname": "Yang",
        "given_name": "Zhiwei"
      },
      {
        "surname": "Wu",
        "given_name": "Zhaoyang"
      }
    ]
  },
  {
    "title": "Few-shot relational triple extraction with hierarchical prototype optimization",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110779",
    "abstract": "Relational Triple Extraction (RTE) aims to extract relations and entities from unstructured text. Current RTE models using supervised learning require a large amount of labeled data, which presents a challenge for real-world applications. Therefore, the research work on Few-Shot Relational Triple Extraction (FS-RTE) has been proposed. However, the existing work cannot effectively construct accurate prototypes from a small number of samples, and it is difficult to model the dependencies between entities and relations, resulting in poor performance in relational triple extraction. In this paper, we propose a Hierarchical Prototype Optimized FS-RTE method (HPO). In particular, to mitigate prototype bias built on a small number of samples, HPO uses prompt learning to merge the information of relational labels into the text. Then, the entity-level prototypes are constructed using a span encoder to avoid label dependency between entity tokens. Finally, the hierarchical contrastive learning (HCL) method is introduced to improve the metric space between the prototypes of entities and relations, respectively. Experiments conducted on two public datasets show that HPO can significantly outperform previous state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324005302",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Computer vision",
      "Extraction (chemistry)",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Shot (pellet)"
    ],
    "authors": [
      {
        "surname": "Gao",
        "given_name": "Chen"
      },
      {
        "surname": "Zhang",
        "given_name": "Xuan"
      },
      {
        "surname": "Jin",
        "given_name": "Zhi"
      },
      {
        "surname": "Shang",
        "given_name": "Weiyi"
      },
      {
        "surname": "Ma",
        "given_name": "Yubin"
      },
      {
        "surname": "Li",
        "given_name": "Linyu"
      },
      {
        "surname": "Ding",
        "given_name": "Zishuo"
      },
      {
        "surname": "Liang",
        "given_name": "Yuqin"
      }
    ]
  },
  {
    "title": "Cascaded learning with transformer for simultaneous eye landmark, eye state and gaze estimation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110760",
    "abstract": "Eye tracking have garnered attention in human–machine interaction, disease monitoring, biometrics, etc. Existing investigations for eye tracking have predominantly concentrated on individual task for pupil detection or gaze estimation, overlooking the implicit relationships that exist among different tasks for eye tracking. In this work, we introduce a cascaded framework with transformer to collaboratively realize eye landmark detection, eye state detection and gaze estimation. Within our framework, we leverage Transformer to capture long dependencies with explicit eye-related structural information and implicit correlation among different tasks. Furthermore, the proposed cascade iteration framework alternatively optimize each task and boost the overall performance for pupil center, eye state and gaze estimation simultaneously. To address the problem of manual annotation, we further introduce the Control-Eye Diffusion Model (CEDM), a controllable eye image generation method conditioned on a simple contour with structure information. The proposed methods are evaluated on challenging datasets such as GI4E, BioID and MPIIGaze, and the results show that our methods outperform state-of-the-art methods in several tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324005119",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Electrical engineering",
      "Engineering",
      "Eye movement",
      "Eye tracking",
      "Gaze",
      "Landmark",
      "Pattern recognition (psychology)",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Gou",
        "given_name": "Chao"
      },
      {
        "surname": "Yu",
        "given_name": "Yuezhao"
      },
      {
        "surname": "Guo",
        "given_name": "Zipeng"
      },
      {
        "surname": "Xiong",
        "given_name": "Chen"
      },
      {
        "surname": "Cai",
        "given_name": "Ming"
      }
    ]
  },
  {
    "title": "Local context attention learning for fine-grained scene graph generation",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110708",
    "abstract": "Fine-grained scene graph generation aims to parse the objects and their fine-grained relationships within scenes. Despite the significant progress in recent years, their performance is still limited by two major issues: (1) ambiguous perception under a global view; (2) the lack of reliable, fine-grained annotations. We argue that understanding the local context is important in addressing the two issues. However, previous works often overlook it, which limits their effectiveness in fine-grained scene graph generation. To tackle this challenge, we introduce a Local-context Attention Learning method that concentrates on local context and can generate high-reliability, fine-grained annotations. It comprises two components: (1) The Fine-grained Location Attention Network (FLAN), a multi-branch network that encompasses global and local branches, can attend to local informative context and perceive granularity levels in different regions, thereby adaptively enhancing the learning of fine-grained locations. (2) The Fine-grained Location Label Transfer (FLLT) method identifies coarse-grained labels inconsistent with the local context and determines which labels should be transferred through the global confidence thresholding strategy, finally transferring them to reliable local context-consistent fine-grained ones. Experiments conducted on the Visual Genome, OpenImage, and GQA-200 datasets show that the proposed methods achieve significant improvements on the fine-grained scene graph generation task. By addressing the challenge mentioned above, our method also achieves state-of-the-art performances on the three datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032400459X",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Granularity",
      "Graph",
      "Machine learning",
      "Operating system",
      "Paleontology",
      "Rendering (computer graphics)",
      "Scene graph",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Zhu",
        "given_name": "Xuhan"
      },
      {
        "surname": "Wang",
        "given_name": "Ruiping"
      },
      {
        "surname": "Lan",
        "given_name": "Xiangyuan"
      },
      {
        "surname": "Wang",
        "given_name": "Yaowei"
      }
    ]
  },
  {
    "title": "A knowledge graph completion model based on triple level interaction and contrastive learning",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110783",
    "abstract": "Knowledge graphs provide credible and structured knowledge for downstream tasks such as information retrieval. Nevertheless, the ubiquitous incompleteness of knowledge graphs often limits the performance of applications. To address the incompleteness, people have proposed the knowledge graph completion task to supplement the facts of incomplete triplets. Recently, researchers have proposed introducing text descriptions to enrich entity representations. Existing methods based on triple decoupling with text description solve the combinatorial explosion problem well. Nevertheless, they still suffer from a lack of global characteristics of factual triples. In addition, the success of contrastive learning research has improved such methods, but they are still limited by existing negative sampling, which is usually more costly than embedding-based methods. In order to solve these limitations, this paper proposes an innovative triple-level interaction model for knowledge graph completion named InCL-KGC. Concretely, the proposed model employs an on-verge interaction method to reduce text redundancy information for entity representation and capture the global semantics of factual triplets. Furthermore, we design an effective hard negative sampling strategy to improve contrast learning. Additionally, we perform an improved Harbsort algorithm for the purpose of reducing the adverse impact of candidate entity sparsity on inference. Extensive experiment consequences exhibit that our model transcends recent baselines with MRR, Hit@3, and Hits@10 increased by 1.2%, 3.2%, and 6.8% on WN18RR, while the index MRR, Hit@1, Hit@3, and Hits@10 were enhanced by 2.8%, 1%, 3.3%, 4.3% on FB15K-237.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032400534X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Graph",
      "Knowledge graph",
      "Natural language processing",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Hu",
        "given_name": "Jie"
      },
      {
        "surname": "Yang",
        "given_name": "Hongqun"
      },
      {
        "surname": "Teng",
        "given_name": "Fei"
      },
      {
        "surname": "Du",
        "given_name": "Shengdong"
      },
      {
        "surname": "Li",
        "given_name": "Tianrui"
      }
    ]
  },
  {
    "title": "Few-shot learning with long-tailed labels",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110806",
    "abstract": "Few-Shot Learning (FSL) is a challenging classification task in machine learning, and it aims to recognize unseen examples of new classes with only a few labeled reference examples (i.e., the support set). The training phase of FSL typically requires a large amount of labeled examples (i.e., the base set) to effectively learn transferable knowledge, but it is usually difficult to obtain sufficient data annotation in practical applications. Existing semi-supervised FSL approaches can learn generalizable representations from partly labeled data, yet they do not sufficiently consider the real distribution of those labeled data. In this paper, we propose a new problem setting termed Few-Shot Learning with Long-Tailed Labels (FSL-LTL) to further consider a more practical semi-supervised scenario where the labeled examples are long-tailed. To effectively address this new problem, we build a novel two-stage training framework dubbed Reweighted Contrastive Embedding (RCE). In the first stage of RCE, we adopt the popular contrastive learning framework to pre-train a reliable network in a self-supervised manner. In the second stage, we integrate the semi-supervised empirical risk into a Weighted Random Sampling (WRS) strategy to fine-tune the pre-trained backbone with the aid of a consistency regularization. Experimental results demonstrate the feasibility of the proposed FSL-LTL problem setting and the superiority of our new RCE method over existing FSL approaches and semi-supervised learning methods. These results also suggest that the RCE approach is a promising solution for addressing the new FSL-LTL problem.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324005570",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Machine learning",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Shot (pellet)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Hongliang"
      },
      {
        "surname": "Chen",
        "given_name": "Shuo"
      },
      {
        "surname": "Luo",
        "given_name": "Lei"
      },
      {
        "surname": "Yang",
        "given_name": "Jiang"
      }
    ]
  },
  {
    "title": "CTNeRF: Cross-time Transformer for dynamic neural radiance field from monocular video",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110729",
    "abstract": "The goal of our work is to generate high-quality novel views from monocular videos of complex and dynamic scenes. Prior methods, such as DynamicNeRF, have shown impressive performance by leveraging time-varying dynamic radiation fields. However, these methods have limitations when it comes to accurately modeling the motion of complex objects, which can lead to inaccurate and blurry renderings of details. To address this limitation, we propose a novel approach that builds upon a recent generalization NeRF, which aggregates nearby views onto new viewpoints. However, such methods are typically only effective for static scenes. To overcome this challenge, we introduce a module that operates in both the time and frequency domains to aggregate the features of object motion. This allows us to learn the relationship between frames and generate higher-quality images. Our experiments demonstrate significant improvements over state-of-the-art methods on dynamic scene datasets. Specifically, our approach outperforms existing methods in terms of both the accuracy and visual quality of the synthesized views. Our code is available on https://github.com/xingy038/CTNeRF.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324004801",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Electrical engineering",
      "Engineering",
      "Geology",
      "Monocular",
      "Radiance",
      "Remote sensing",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Miao",
        "given_name": "Xingyu"
      },
      {
        "surname": "Bai",
        "given_name": "Yang"
      },
      {
        "surname": "Duan",
        "given_name": "Haoran"
      },
      {
        "surname": "Wan",
        "given_name": "Fan"
      },
      {
        "surname": "Huang",
        "given_name": "Yawen"
      },
      {
        "surname": "Long",
        "given_name": "Yang"
      },
      {
        "surname": "Zheng",
        "given_name": "Yefeng"
      }
    ]
  },
  {
    "title": "Self-Training-Transductive-Learning Broad Learning System (STTL-BLS): A model for effective and efficient image classification",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110747",
    "abstract": "A novel model called Self-Training-Transductive-Learning Broad Learning System (STTL-BLS) is proposed for image classification. The model consists of two key blocks: Feature Block (FB) and Enhancement Block (EB). The FB utilizes the Proportion of Large Values Attention (PLVA) technique and an Encoder for feature extraction. Multiple FBs are cascaded in the model to learn discriminative features. The Enhancement Block (EB) enhances feature learning and prevents under-fitting on complex datasets. Additionally, an architecture that combines characteristics of Broad Learning System (BLS) and gradient descent is designed for STTL-BLS, enabling the model to leverage the advantages of both BLS and Convolutional Neural Networks (CNNs). Moreover, a training algorithm (STTL) that combines self-training and transductive learning is presented for the model to improve its generalization ability. Experimental results demonstrate that the accuracy of the proposed model surpasses all compared BLS variants and performs comparably or even superior to deep networks: on small-scale datasets, STTL-BLS has an average accuracy improvement of 14.82 percentage points compared to other models; on large-scale datasets, 12.95 percentage points. Notably, the proposed model exhibits low time complexity, particularly with the shortest testing time on the small-scale datasets among all compared models: it has an average testing time of 46.4 s less than other models. It proves to be an additional valuable solution for image classification tasks on both small- and large-scale datasets. The source code for this paper can be accessed at https://github.com/threedteam/sttl_bls.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324004989",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Image (mathematics)",
      "Image manipulation",
      "Machine learning",
      "Meteorology",
      "Pattern recognition (psychology)",
      "Physics",
      "Training (meteorology)"
    ],
    "authors": [
      {
        "surname": "Yi",
        "given_name": "Lin"
      },
      {
        "surname": "Lv",
        "given_name": "Di"
      },
      {
        "surname": "Liu",
        "given_name": "Dinghao"
      },
      {
        "surname": "Li",
        "given_name": "Suhuan"
      },
      {
        "surname": "Liu",
        "given_name": "Ran"
      }
    ]
  },
  {
    "title": "Heterogeneous domain adaptation via incremental discriminative knowledge consistency",
    "journal": "Pattern Recognition",
    "year": "2024",
    "doi": "10.1016/j.patcog.2024.110857",
    "abstract": "Heterogeneous domain adaptation is a challenging problem in transfer learning since samples from the source and target domains reside in different feature spaces with different feature dimensions. The key problem is how to minimize some gaps (e.g., data distribution mismatch) presented in two heterogeneous domains and produce highly discriminative representations for the target domain. In this paper, we attempt to address these challenges with the proposed incremental discriminative knowledge consistency (IDKC) method, which integrates cross-domain mapping, distribution matching, discriminative knowledge preservation, and domain-specific geometry structure consistency into a unified learning model. Specifically, we attempt to learn a domain-specific projection to project original samples into a common subspace in which the marginal distribution is well aligned and the discriminative knowledge consistency is preserved by leveraging the labeled samples from both domains. Moreover, domain-specific structure consistency is enforced to preserve the data manifold from the original space to the common feature space in each domain. Meanwhile, we further apply pseudo labeling to unlabeled target samples based on the feature correlation and retain pseudo labels with high feature correlation coefficients for the next iterative learning. Our pseudo-labeling strategy expands the number of labeled target samples in each category and thus enforces class-discriminative knowledge consistency to produce more discriminative feature representations for the target domain. Extensive experiments on several standard benchmarks for object recognition, cross-language text classification, and digit classification tasks verify the effectiveness of our method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320324006083",
    "keywords": [
      "Adaptation (eye)",
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Consistency (knowledge bases)",
      "Data mining",
      "Discriminative model",
      "Domain adaptation",
      "Domain knowledge",
      "Machine learning",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Psychology"
    ],
    "authors": [
      {
        "surname": "Lu",
        "given_name": "Yuwu"
      },
      {
        "surname": "Lin",
        "given_name": "Dewei"
      },
      {
        "surname": "Wen",
        "given_name": "Jiajun"
      },
      {
        "surname": "Shen",
        "given_name": "Linlin"
      },
      {
        "surname": "Li",
        "given_name": "Xuelong"
      },
      {
        "surname": "Wen",
        "given_name": "Zhenkun"
      }
    ]
  }
]